FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Zhang, LL
   Luo, MN
   Liu, J
   Chang, XJ
   Yang, Y
   Hauptmann, AG
AF Zhang, Lingling
   Luo, Minnan
   Liu, Jun
   Chang, Xiaojun
   Yang, Yi
   Hauptmann, Alexander G.
TI Deep Top-k Ranking for Image-Sentence Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image-sentence matching; cross-modal retrieval; deep learning; top-k
   ranking
ID CROSS; FUSION
AB Image-sentence matching is a challenging task for the heterogeneity-gap between different modalities. Ranking-based methods have achieved excellent performance in this task in past decades. Given an image query, these methods typically assume that the correct matched image-sentence pair must rank before all other mismatched ones. However, this assumption may be too strict and prone to the overfitting problem, especially when some sentences in a massive database are similar and confusable with one another. In this paper, we relax the traditional ranking loss and propose a novel deep multi-modal network with a top-k ranking loss to mitigate the data ambiguity problem. With this strategy, query results will not be penalized unless the index of ground truth is outside the range of top-k query results. Considering the non-smoothness and non-convexity of the initial top-k ranking loss, we exploit a tight convex upper bound to approximate the loss and then utilize the traditional back-propagation algorithm to optimize the deep multi-modal network. Finally, we apply the method on three benchmark datasets, namely, Flickr8k, Flickr30k, and MSCOCO. Empirical results on metrics R@K (K = 1, 5, 10) show that our method achieves comparable performance in comparison to state-of-the-art methods.
C1 [Zhang, Lingling] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Luo, Minnan; Liu, Jun] Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China.
   [Chang, Xiaojun] Monash Univ, Fac Informat Technol, Clayton Vic 3800, Australia.
   [Yang, Yi] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Ultimo, NSW 2007, Australia.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Monash University;
   University of Technology Sydney; Carnegie Mellon University
RP Luo, MN (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Big Data Analyt, Xian 710049, Peoples R China.
EM zhanglingling@stu.xjtu.edu.cn; minnluo@mail.xjtu.edu.cn;
   liukeen@mail.xjtu.edu.cn; cxj273@gmail.com; yee.i.yang@gmail.com;
   alex@cs.cmu.edu
RI zhang, lingling/HDM-2189-2022; Chang, Xiaojun/A-2055-2015; Lang,
   Ming/HIK-0758-2022; yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022;
   Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022
OI Chang, Xiaojun/0000-0002-7778-8807; Yang, Yi/0000-0002-0512-880X; 
FU National Key Research and Development Program of China [2016YFB1000903];
   National Natural Science Foundation of China [61532004, 61532015,
   61672418, 61672419, 61877050]; Innovative Research Group of the National
   Natural Science Foundation of China [61721002]; Innovation Research Team
   of Ministry of Education [IRT_17R86]; Project of China Knowledge Centre
   for Engineering Science and Technology, Intelligence Advanced Research
   Projects Activity via Department of Interior/Interior Business Center
   [D17PC00340]; Australian Research Council Discovery Early Career
   Researcher Award [DE190100626]; Defense Advanced Research Projects
   Agency [DARPA-BAA-HR0011-18-S-0044]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1000903; in part by the
   National Natural Science Foundation of China under Grants 61532004,
   61532015, 61672418, 61672419, and 61877050; in part by the Innovative
   Research Group of the National Natural Science Foundation of China under
   Grant 61721002; in part by the Innovation Research Team of Ministry of
   Education under Grant IRT_17R86; in part by the Project of China
   Knowledge Centre for Engineering Science and Technology, Intelligence
   Advanced Research Projects Activity via Department of Interior/Interior
   Business Center under Contract D17PC00340; in part by the Australian
   Research Council Discovery Early Career Researcher Award DE190100626;
   and in part by the Defense Advanced Research Projects Agency under Grant
   DARPA-BAA-HR0011-18-S-0044.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2014, BMVC
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Arora R, 2013, INT CONF ACOUST SPEE, P7135, DOI 10.1109/ICASSP.2013.6639047
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chang XJ, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P75, DOI 10.1145/3097983.3097991
   Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Gupta MR, 2014, J MACH LEARN RES, V15, P1461
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Huang YF, 2017, IEEE INT CONF COMP V, P2313, DOI 10.1109/ICCVW.2017.273
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy Andrej, 2014, Advances in neural information processing systems, P1889
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Klein Benjamin, 2014, ARXIV14117399
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai P L, 2000, Int J Neural Syst, V10, P365, DOI 10.1016/S0129-0657(00)00034-X
   Lapin M, 2016, PROC CVPR IEEE, P1468, DOI 10.1109/CVPR.2016.163
   Lapin Maksim, 2015, ADV NEURAL INFORM PR, V1, P325
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu L-P, 2016, INT JOINT C ARTIFICI, P1781
   Lu XY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P433
   Luo F., 2016, P 25 INT JOINT C ART, P1802, DOI [10.5555/3060832.3060873, DOI 10.5555/3060832.3060873]
   Luo MN, 2017, COMPUT VIS IMAGE UND, V163, P67, DOI 10.1016/j.cviu.2017.07.001
   Luo MN, 2017, NEURAL COMPUT, V29, P1124, DOI 10.1162/NECO_a_00937
   Lux M, 2004, LECT NOTES ARTIF INT, V3336, P343
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Ma Z, 2015, PR MACH LEARN RES, V37, P169
   Mao Junhua, 2014, CoRR
   Montanarella L, 1995, RAPID COMMUN MASS SP, V9, P1589, DOI 10.1002/rcm.1290091525
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   RIEDMILLER M, 1994, COMP STAND INTER, V16, P265, DOI 10.1016/0920-5489(94)90017-5
   Rudzicz F., 2010, P 2010 IEEE INT C AC, V1, P4198
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Wang L., 2018, PATTERN ANAL MACH IN
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wang Z., 2013, SCI WORLD J, V2013
   Weston J, 2011, IJCAI
   Yan CX, 2018, INFORM SCIENCES, V432, P479, DOI 10.1016/j.ins.2017.08.004
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang X., 2017, ARXIV170104207
NR 55
TC 29
Z9 29
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 775
EP 785
DI 10.1109/TMM.2019.2931352
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700016
DA 2024-07-18
ER

PT J
AU Ma, GX
   Chen, CLZ
   Li, S
   Peng, C
   Hao, AM
   Qin, H
AF Ma, Guangxiao
   Chen, Chenglizhao
   Li, Shuai
   Peng, Chong
   Hao, Aimin
   Qin, Hong
TI Salient Object Detection via Multiple Instance Joint Re-Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Saliency detection; Visualization;
   Deep learning; Correlation; Topology; Salient Object Detection
   Inter-image Corres-pondence; Multiple Instance Learning; Joint
   Re-Learning
ID VIDEO; OPTIMIZATION; DRIVEN; FUSION; MODEL
AB In recent years deep neural networks have been widely applied to visual saliency detection tasks with remarkable detection performance improvements. As for the salient object detection in single image, the automatically computed convolutional features frequently demonstrate high discriminative power to distinguish salient foregrounds from its non-salient surroundings in most cases. Yet, the obstinate feature conflicts still persist, which naturally gives rise to the learning ambiguity, arriving at massive failure detections. To solve such problem, we propose to jointly re-learn common consistency of inter-image saliency and then use it to boost the detection performance. Its core rationale is to utilize the easy-to-detect cases to re-boost much harder ones. Compared with the conventional methods, which focus on their problem domain within the single image scope, our method attempts to utilize those beyond-scope information to facilitate the current salient object detection. To validate our new approach, we have conducted a comprehensive quantitative comparisons between our approach and 13 state-of-the-art methods over 5 publicly available benchmarks, and all the results suggest the advantage of our approach in terms of accuracy, reliability, and versatility.
C1 [Ma, Guangxiao; Chen, Chenglizhao; Hao, Aimin] Beihang Univ, Qingdao Res Inst, Qingdao 266100, Peoples R China.
   [Chen, Chenglizhao; Peng, Chong] Qingdao Univ, Qingdao 266071, Peoples R China.
   [Li, Shuai; Hao, Aimin] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Beihang University; Qingdao University; Beihang University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Chen, CLZ (corresponding author), Beihang Univ, Qingdao Res Inst, Qingdao 266100, Peoples R China.; Chen, CLZ; Peng, C (corresponding author), Qingdao Univ, Qingdao 266071, Peoples R China.
EM mgx@buaa.edu.cn; cclz123@163.com; lishuai@buaa.edu.cn;
   pchong1991@163.com; ham@buaa.edu.cn; qin@cs.stonybrook.edu
FU National Key RAMP;D Program of China [2017YFF0106407]; National Natural
   Science Foundation of China [61802215, 61806106]; Natural Science
   Foundation of Shandong Province [ZR2019BF011, ZR2019QF009]; National
   Science Foundation of USA [IIS-1715985, IIS-0949467, IIS-1047715,
   IIS1049448]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2017YFF0106407, in part by the National Natural Science
   Foundation of China under Grants 61802215 and 61806106, in part by the
   Natural Science Foundation of Shandong Province under Grants ZR2019BF011
   and ZR2019QF009, and in part by the National Science Foundation of USA
   under Grants IIS-1715985, IIS-0949467, IIS-1047715, and IIS1049448. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jian Zhang. (GuangxiaoMa,
   Chenglizhao Chen, and Shuai Li contributed equally to this work.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Bi S., 2014, J IMAGE GRAPH, V2
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Chen CLZ, 2019, IEEE ACCESS, V7, P79770, DOI 10.1109/ACCESS.2019.2899351
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CLZ, 2018, IEEE SIGNAL PROC LET, V25, P154, DOI 10.1109/LSP.2017.2775212
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li GB, 2018, PROC CVPR IEEE, P3243, DOI 10.1109/CVPR.2018.00342
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2016, IEEE T IMAGE PROCESS, V25, P5012, DOI 10.1109/TIP.2016.2602079
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li X, 2016, SPRINGER COMPLEX, P341, DOI 10.1007/978-3-662-47824-0_13
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Peng C, 2019, PROC CVPR IEEE, P7309, DOI 10.1109/CVPR.2019.00749
   Peng C, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3200488
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Song WF, 2019, IEEE J BIOMED HEALTH, V23, P1215, DOI 10.1109/JBHI.2018.2852718
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LZ, 2019, IEEE T PATTERN ANAL, V41, P1734, DOI 10.1109/TPAMI.2018.2846598
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang W., 2019, ARXIV190409146
   Wang WG, 2019, INSTITUTIONAL ACTIVISM IN CORPORATE GOVERNANCE: QUALIFIED FOREIGN INSTITUTIONAL INVESTORS IN CHINA, P1
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2016, IEEE T IMAGE PROCESS, V25, P5025, DOI 10.1109/TIP.2016.2601784
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2016, IEEE SIGNAL PROC LET, V23, P838, DOI 10.1109/LSP.2016.2558489
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhou F, 2014, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2014.429
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhou XF, 2018, J VIS COMMUN IMAGE R, V51, P131, DOI 10.1016/j.jvcir.2018.01.014
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 68
TC 45
Z9 45
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 324
EP 336
DI 10.1109/TMM.2019.2929943
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300004
DA 2024-07-18
ER

PT J
AU Zhang, CX
   Ge, LY
   Chen, Z
   Li, M
   Liu, W
   Chen, H
AF Zhang, Congxuan
   Ge, Liyue
   Chen, Zhen
   Li, Ming
   Liu, Wen
   Chen, Hao
TI Refined TV-<i>L</i><SUP>1</SUP> Optical Flow Estimation Using Joint
   Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical flow; TV-L-1 model; joint filtering; mutual-structure;
   edge-preserving
ID QUALITY ASSESSMENT; ALGORITHM; OCCLUSION
AB Though the accuracy and robustness of optical flow has been dramatically enhanced over the past few years, the issue of edge-blurring near the image and motion boundaries has remained a challenge in flow field estimation. In this paper, we propose a refined total variation with L-1 norm (TV-L-1) optical flow estimation approach using joint filtering, named JOF. First, we divide the image into three categorized regions: mutual-structure regions, inconsistent structure regions, and smooth regions. The mutual-structure guided filter for optical flow estimation is constructed by extracting the mutual-structure regions of the flow field. Second, the refined TV-L-1 optical flow model is proposed by incorporating the non-local term and mutual-structure guided filter objective function into the classical TV-L-1 energy function. Furthermore, the novel TV-L-1 optical flow objective function is minimized using a joint filtering program composed of a weighted median filter and a mutual-structure guided filter to optimize the estimated flow field during the coarse-to-fine optical flow computation scheme. Finally, we compare the proposed JOF method with several state-of-the-art approaches including variational and deep learning based optical flow models using the Middlebury, MPI-Sintel, and UCF101 test databases. The evaluation results indicate that the proposed method has high accuracy and good robustness for flow field computation and, especially, the significant benefit of edge-preserving.
C1 [Zhang, Congxuan; Ge, Liyue; Chen, Zhen] Nanchang Hangkong Univ, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
   [Zhang, Congxuan] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Li, Ming; Chen, Hao] Nanchang Hangkong Univ, Key Lab Jiangxi Prov Image Proc & Pattern Recogni, Nanchang 330063, Jiangxi, Peoples R China.
   [Liu, Wen] Univ Kansas, Bioengn Program, Lawrence, KS 66045 USA.
   [Liu, Wen] Univ Kansas, Dept Phys Therapy & Rehabil Sci, Lawrence, KS 66045 USA.
C3 Nanchang Hangkong University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Nanchang Hangkong University; University of Kansas;
   University of Kansas
RP Chen, Z (corresponding author), Nanchang Hangkong Univ, Key Lab Nondestruct Testing, Minist Educ, Nanchang 330063, Jiangxi, Peoples R China.
EM zcxdsg@163.com; lygeah@163.com; dr_chenzhen@163.com; liming@nchu.edu.cn;
   WLIU@kumc.edu; chenhaoshl@nchu.edu.cn
OI Zhang, Congxuan/0000-0003-1356-1205; Liu, Wen/0000-0002-4762-3104
FU National Natural Science Foundation of China [61866026, 61772255,
   61866025]; Aeronautical Science Foundation of China [2018ZC56008];
   Natural Science Foundation of Jiangxi Province [20171BAB212012,
   20192BCB23011]; China Postdoctoral Science Foundation [2019M650894];
   Advantage SubjectTeam Project of Jiangxi Province [20165BCB19007,
   20152BCB24004]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61866026, 61772255, and 61866025, in
   part by theAdvantage SubjectTeam Project of Jiangxi Province under
   Grants 20165BCB19007 and 20152BCB24004, in part by the Aeronautical
   Science Foundation of China under Grant 2018ZC56008, in part by the
   Natural Science Foundation of Jiangxi Province under Grants
   20171BAB212012 and 20192BCB23011, and in part by the China Postdoctoral
   Science Foundation under Grant 2019M650894. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. David Crandall.
CR Ali S, 2016, COMPUT VIS IMAGE UND, V145, P95, DOI 10.1016/j.cviu.2015.12.003
   Alvarez L., 1999, PROC CONGRESO ECUACI, P1349
   Amiaz T, 2007, PATTERN RECOGN, V40, P2496, DOI 10.1016/j.patcog.2006.09.011
   Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Black MJ, 1996, COMPUT VIS IMAGE UND, V63, P75, DOI 10.1006/cviu.1996.0006
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Bruhn A, 2005, INT J COMPUT VISION, V61, P211, DOI 10.1023/B:VISI.0000045324.43199.43
   Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44
   Chambolle A, 2004, J MATH IMAGING VIS, V20, P89
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen ZY, 2013, PROC CVPR IEEE, P2443, DOI 10.1109/CVPR.2013.316
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Drulea M, 2011, IEEE INT C INTELL TR, P318, DOI 10.1109/ITSC.2011.6082986
   Gilliam C, 2018, IEEE T IMAGE PROCESS, V27, P1010, DOI 10.1109/TIP.2017.2765822
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hsieh CK, 2009, IEEE T MULTIMEDIA, V11, P600, DOI 10.1109/TMM.2009.2017606
   Hu Y., 2017, P IEEE C COMP VIS PA, P481
   Hu YL, 2016, PROC CVPR IEEE, P5704, DOI 10.1109/CVPR.2016.615
   Hu YL, 2016, IMAGE VISION COMPUT, V52, P167, DOI 10.1016/j.imavis.2016.06.004
   Hur Junhwa, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P163, DOI 10.1007/978-3-319-46604-0_12
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Kennedy R, 2015, LECT NOTES COMPUT SC, V8932, P364, DOI 10.1007/978-3-319-14612-6_27
   Lan ZZ, 2014, MULTIMED TOOLS APPL, V71, P333, DOI 10.1007/s11042-013-1391-2
   Li YY, 2009, COMMUN MATH SCI, V7, P741
   Liu SC, 2014, PROC CVPR IEEE, P4209, DOI 10.1109/CVPR.2014.536
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   McGuire K, 2017, IEEE ROBOT AUTOM LET, V2, P1070, DOI 10.1109/LRA.2017.2658940
   Mohamed MA, 2014, IEEE T CIRC SYST VID, V24, P1499, DOI 10.1109/TCSVT.2014.2308628
   Monzón N, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2526903
   NAGEL HH, 1986, IEEE T PATTERN ANAL, V8, P565, DOI 10.1109/TPAMI.1986.4767833
   Papenberg N, 2006, INT J COMPUT VISION, V67, P141, DOI 10.1007/s11263-005-3960-y
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Shen XY, 2017, INT J COMPUT VISION, V125, P19, DOI 10.1007/s11263-017-1021-y
   Soomro K., 2012, ARXIV12120402CS
   Sun DQ, 2008, LECT NOTES COMPUT SC, V5304, P83
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tretiak O., 1984, Seventh International Conference on Pattern Recognition (Cat. No. 84CH2046-1), P16
   Tu ZG, 2017, PATTERN RECOGN, V65, P11, DOI 10.1016/j.patcog.2016.10.027
   Tu ZG, 2016, SIGNAL PROCESS, V127, P253, DOI 10.1016/j.sigpro.2016.02.018
   Vaquero V, 2017, IEEE IMAGE PROC, P2558, DOI 10.1109/ICIP.2017.8296744
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wedel A, 2009, LECT NOTES COMPUT SC, V5604, P23, DOI 10.1007/978-3-642-03061-1_2
   Werlberger M., 2009, P BMVC
   Werlberger M, 2010, PROC CVPR IEEE, P2464, DOI 10.1109/CVPR.2010.5539945
   Wu GL, 2017, IEEE T MULTIMEDIA, V19, P1730, DOI 10.1109/TMM.2017.2691538
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Yang QX, 2012, PROC CVPR IEEE, P1402, DOI 10.1109/CVPR.2012.6247827
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zhang CX, 2018, IEEE ACCESS, V6, P26958, DOI 10.1109/ACCESS.2018.2831920
   Zhang CX, 2017, IEEE T IMAGE PROCESS, V26, P4055, DOI 10.1109/TIP.2017.2712279
   Zhang SH, 2011, IEEE T MULTIMEDIA, V13, P1286, DOI 10.1109/TMM.2011.2165052
   Zimmer H, 2011, INT J COMPUT VISION, V93, P368, DOI 10.1007/s11263-011-0422-6
NR 61
TC 18
Z9 21
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 349
EP 364
DI 10.1109/TMM.2019.2929934
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300006
DA 2024-07-18
ER

PT J
AU Wang, DZ
   Mao, KZ
AF Wang, Dongzhe
   Mao, Kezhi
TI Learning Semantic Text Features for Web Text-Aided Image Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic matching neural network; semantic filter; image classification;
   text representation; web image search
ID LOW-RANK; SELECTION; SCENE
AB The good generalization performance of conventional pattern classifiers often relies on the size of training data labeled by costly human labor. These days, publicly available web resources grow explosively, and this allows us to easily obtain abundant and cheap web data. Yet, web data are usually not as cooperative as human labeled data. In this paper, we explore the use of web text data to aid image classification. Without requiring the previous collection of auxiliary data from the web, we directly retrieve the web text information with the aid of the powerful reverse image search engine. We develop a novel textual modeling method named semantic matching neural network (SMNN) that is capable of learning semantic features from the associated text of web images. The SMNN text features have improved reliability and applicability, compared to the text features obtained from other methods. The SMNN text features and convolutional neural network (CNN) visual features are merged into a shared representation, which learns to capture the correlations between the two modalities. Experimental results on benchmark UIUC-Sports, Scene-15, Caltech-256, and Pascal VOC-2012 data sets show that the visual and text modalities of data from different sources are remarkably complementary and the fusion of them achieves substantial performance improvement.
C1 [Wang, Dongzhe; Mao, Kezhi] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Mao, KZ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM dwang015@e.ntu.edu.sg; ekzmao@ntu.edu.sg
RI Mao, Kezhi/A-5025-2011
OI Mao, Kezhi/0000-0002-9191-8604
CR [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2007, P 24 INT C MACHINE L, DOI DOI 10.1145/1273496.1273592
   [Anonymous], 2014, EFFECTIVE USE WORD O
   Bergamo Alessandro, 2010, ADV NEURAL INFORM PR, V23
   Bird S., 2004, P ACL INTERACTIVE PO, P214
   Chen Jiawei., 2014, Proceedings of International Conference on Multimedia Retrieval, P1
   Chen X, 2015, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2015.7298856
   Cheng DS, 2015, COMPUT VIS IMAGE UND, V131, P56, DOI 10.1016/j.cviu.2014.07.005
   CHOLLET F, 2017, PROC CVPR IEEE, P1800, DOI DOI 10.1109/CVPR.2017.195
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dai W., 2008, NIPS, P353, DOI DOI 10.5555/2981780.2981825
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Dos Santos C., 2014, Coling, P69
   Durand T, 2016, PROC CVPR IEEE, P4743, DOI 10.1109/CVPR.2016.513
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2016, LECT NOTES COMPUT SC, V9907, P849, DOI 10.1007/978-3-319-46487-9_52
   Gao SH, 2016, IEEE T CIRC SYST VID, V26, P494, DOI 10.1109/TCSVT.2015.2389413
   Gao Shenghua, 2010, P INT C MULT, P1115
   Griffin G., 2007, CALTECH 256 OBJECT C
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hu B, 2014, INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONIC ENGINEERING (EEE 2014), P42
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Koskela M, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1169, DOI 10.1145/2647868.2655024
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Kuzborskij I, 2016, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2016.231
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Li L.-j., 2010, NIPS
   Li LJ, 2007, LECT NOTES ARTIF INT, V4456, P1
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Mao KZ, 2005, IEEE T NEURAL NETWOR, V16, P1531, DOI 10.1109/TNN.2005.853575
   Mao KZ, 2002, IEEE T NEURAL NETWOR, V13, P1218, DOI 10.1109/TNN.2002.1031954
   Mikolov T, 2013, P WORKSHOP ICLR 2013
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pu YC, 2016, JMLR WORKSH CONF PRO, V51, P741
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI DOI 10.1109/CVPR.2013.49
   Tian LX, 2013, INT J INTELL SYST, V28, P242, DOI 10.1002/int.21567
   Tran TA, 2016, PROC INT CONF ADV, P17, DOI 10.1109/ATC.2016.7764768
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang DZ, 2015, 2015 18TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P823
   Wang DZ, 2016, 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P114
   Wang G, 2009, PROC CVPR IEEE, P1367, DOI 10.1109/CVPRW.2009.5206816
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yang JK, 2017, MATER LETT, V200, P1, DOI 10.1016/j.matlet.2017.04.090
   Yang Q., 2009, P JOINT C 47 ANN M A, V1, P1
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang ZJ, 2016, ADV SOC SCI EDUC HUM, V64, P1480
   Yao Y, 2018, J COASTAL RES, P851, DOI 10.2112/SI85-171.1
   Yao YZ, 2019, IEEE T IMAGE PROCESS, V28, P436, DOI 10.1109/TIP.2018.2869721
   Yuan Lin, 2010, Proceedings of the 12th Asia Pacific Web Conference (APWEB 2010), P267, DOI 10.1109/APWeb.2010.49
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang CJ, 2014, COMPUT VIS IMAGE UND, V123, P14, DOI 10.1016/j.cviu.2014.02.013
   Zhang CJ, 2011, PROC CVPR IEEE, P1673, DOI 10.1109/CVPR.2011.5995484
   Zhang Y, 2015, ARXIV PREPRINT ARXIV
   Zhu Q., 2006, Proceedings of ACM MM'06, ACM, P211
   Zhu Xiaojin, 2010, Encyclopedia of Machine Learning, P892, DOI DOI 10.1007/978-0-387-30164-8_749
   Zhu Y., 2011, P NAT C ART INT
   Zuo Z, 2015, PATTERN RECOGN, V48, P3004, DOI 10.1016/j.patcog.2015.02.003
NR 79
TC 10
Z9 12
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 2985
EP 2996
DI 10.1109/TMM.2019.2920620
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200002
DA 2024-07-18
ER

PT J
AU Guo, Y
   Chen, Q
   Chen, J
   Wu, QY
   Shi, QF
   Tan, MK
AF Guo, Yong
   Chen, Qi
   Chen, Jian
   Wu, Qingyao
   Shi, Qinfeng
   Tan, Mingkui
TI Auto-Embedding Generative Adversarial Networks For High Resolution Image
   Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Generative models; adversarial learning; low-dimensional
   embedding; autoencoder
AB Generating images via a generative adversarial network (GAN) has attracted much attention recently. However, most of the existing GAN-based methods can only produce low-resolution images of limited quality. Directly generating high-resolution images using GANs is nontrivial, and often produces problematic images with incomplete objects. To address this issue, we develop a novel GAN called auto-embedding generative adversarial network, which simultaneously encodes the global structure features and captures the fine-grained details. In our network, we use an autoencoder to learn the intrinsic high-level structure of real images and design a novel denoiser network to provide photo-realistic details for the generated images. In the experiments, we are able to produce $512 \times 512$ images of promising quality directly from the input noise. The resultant images exhibit better perceptual photo-realism, that is, with sharper structure and richer details, than other baselines on several datasets, including Oxford-102 Flowers, Caltech-UCSD Birds (CUB), High-Quality Large-scale CelebFaces Attributes (CelebA-HQ), Large-scale Scene Understanding (LSUN), and ImageNet.
C1 [Guo, Yong; Chen, Qi; Chen, Jian; Wu, Qingyao; Shi, Qinfeng; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510630, Guangdong, Peoples R China.
   [Shi, Qinfeng] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Shi, Qinfeng] Australian Ctr Robot Vis, Brisbane, Qld 4000, Australia.
C3 South China University of Technology; University of Adelaide; Australian
   Centre for Robotic Vision
RP Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510630, Guangdong, Peoples R China.
EM guo.yong@mail.scut.edu.cn; sechenqi@mail.scut.edu.cn;
   ellachen@scut.edu.cn; qyw@scut.edu.cn; javen.shi@adelaide.edu.au;
   mingkuitan@scut.edu.cn
OI Chen, Qi/0000-0001-8732-8049; Shi, Javen Qinfeng/0000-0002-9126-2107
FU National Natural Science Foundation of China [61602185, 61876208,
   61502177]; Recruitment Program for Young Professionals, Guangdong
   Provincial Scientific and Technological Fund [2017B090901008,
   2017A010101011, 2017B090910005]; Pearl River S&T Nova Program of
   Guangzhou [201806010081]; CCF-Tencent Open Research Fund [RAGR20170105];
   Program for Guangdong Introducing Innovative and Enterpreneurial Teams
   [2017ZT07X183]; Scientific and Technological Planning on Provincial Key
   Research and Development [2018B01010700]; Guangdong Special Branch Plans
   Young Talent with Scientific and Technological Innovation
   [2016TQ03X445]; Guangzhou Science and Technology Planning Project
   [2019-03-01-06-3002-0003]; Guangzhou Tianhe District Science and
   Technology Planning Project [201702YH112]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61602185, Grant 61876208, and Grant
   61502177, in part by the Recruitment Program for Young Professionals,
   Guangdong Provincial Scientific and Technological Fund under Grant
   2017B090901008, Grant 2017A010101011, and Grant 2017B090910005, in part
   by Pearl River S&T Nova Program of Guangzhou under Grant 201806010081,
   in part by CCF-Tencent Open Research Fund under Grant RAGR20170105, in
   part by the Program for Guangdong Introducing Innovative and
   Enterpreneurial Teams under Grant 2017ZT07X183, in part by the
   Scientific and Technological Planning on Provincial Key Research and
   Development under Grant 2018B01010700, in part by the Guangdong Special
   Branch Plans Young Talent with Scientific and Technological Innovation
   under Grant 2016TQ03X445, in part by the Guangzhou Science and
   Technology Planning Project 2019-03-01-06-3002-0003, and in part by the
   Guangzhou Tianhe District Science and Technology Planning Project
   201702YH112.
CR [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], P CVPR2018
   [Anonymous], 2017, BEGAN BOUNDARY EQUIL
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   [Anonymous], INT C MACH LEARN ATL
   [Anonymous], ICLR2019
   [Anonymous], 2017, ARXIV171010916
   [Anonymous], 2018, ICML
   [Anonymous], 2018, ICLR
   [Anonymous], P ECCV18 WORKSH
   [Anonymous], RBM
   [Anonymous], ICLR2019
   [Anonymous], 2018, ARXIV180107892
   [Anonymous], ATTNGAN FINE GRAINED
   [Anonymous], 2017, P INT C LEARN REPR
   [Anonymous], P NIPS WORKSH ADV TR
   [Anonymous], 2018, ARXIV180900219
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2017, ARXIV170400028CSLG
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Cao J., 2018, INT C MACHINE LEARNI, P707
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Dolhansky B, 2018, PROC CVPR IEEE, P7902, DOI 10.1109/CVPR.2018.00824
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kheirkhah P, 2017, IEEE IJCNN, P4467, DOI 10.1109/IJCNN.2017.7966422
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee HJ, 2017, IEEE T MULTIMEDIA, V19, P1921, DOI 10.1109/TMM.2017.2687759
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Odena A, 2017, PR MACH LEARN RES, V70
   Radford A, 2016, 4 INT C LEARNING REP
   Ranzato M., 2014, ARXIV14126604
   Reed S, 2016, PR MACH LEARN RES, V48
   Rosca M., 2017, Variational approaches for auto-encoding generative adversarial networks
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2007, AISTATS, V2, P412
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241
   Sun YP, 2016, IEEE T MULTIMEDIA, V18, P171, DOI 10.1109/TMM.2015.2496246
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Ulyanov D., 2016, P 33 INT C INT C MAC, V48, P1349
   Ulyanov D, 2018, AAAI CONF ARTIF INTE, P1250
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Yang C., 2017, P IEEE C COMP VIS PA, P6721, DOI DOI 10.1109/CVPR.2017.434
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Yin Wotao., 2008, SIAGOPT VIEWS AND NE, V19, P11
   Yu X, 2018, PROC CVPR IEEE, P908, DOI 10.1109/CVPR.2018.00101
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
NR 61
TC 46
Z9 49
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2726
EP 2737
DI 10.1109/TMM.2019.2908352
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, TY
   Lin, GS
   Cai, JF
   Shen, T
   Shen, CH
   Kot, AC
AF Zhang, Tianyi
   Lin, Guosheng
   Cai, Jianfei
   Shen, Tong
   Shen, Chunhua
   Kot, Alex C.
TI Decoupled Spatial Neural Attention for Weakly Supervised Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Semantics; Detectors; Training; Task analysis;
   Pipelines; Object recognition; Semantic segmentation; deep convolutional
   neural network (DCNN); weakly-supervised learning
AB Weakly supervised semantic segmentation receives much research attention since it alleviates the need to obtain a large amount of dense pixel-wise ground-truth annotations for the training images. Compared with other forms of weak supervision, image labels are quite efficient to obtain. In this paper, we focus on the weakly supervised semantic segmentation with image label annotations. Recent progress for this task has been largely dependent on the quality of generated pseudo-annotations. In this paper, inspired by spatial neural-attention for image captioning, we propose a decoupled spatial neural attention network for generating pseudo-annotations. Our decoupled attention structure could simultaneously identify the object regions and localize the discriminative parts, which generates high-quality pseudo-annotations in one forward path. The generated pseudo-annotations lead to the segmentation results that achieve the state of the art in weakly supervised semantic segmentation.
C1 [Zhang, Tianyi] Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
   [Zhang, Tianyi; Lin, Guosheng; Cai, Jianfei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Shen, Tong] JD AI Res, Beijing 100101, Peoples R China.
   [Shen, Chunhua] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
   [Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Nanyang Technological University;
   University of Adelaide; Nanyang Technological University
RP Lin, GS (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM zh0023yi@e.ntu.edu.sg; gslin@ntu.edu.sg; asjfcai@ntu.edu.sg;
   shentong7@jd.com; chunhua.shen@adelaide.edu.au; eackot@ntu.edu.sg
RI Zhang, Tianyi/AAG-6220-2021; Cai, Jianfei/A-3691-2011
OI Shen, Chunhua/0000-0002-8648-8718; Cai, Jianfei/0000-0002-9444-3763;
   Kot, Alex/0000-0001-6262-8125
FU National Research Foundation, Singapore; Infocomm Media Development
   Authority, Singapore; MoE Tier-2 Grant of Singapore [2016-T2-2-065]; MoE
   Tier-1 Grant of Singapore [RG28/18]; National Research Foundation
   Singapore under its AI Singapore Programme [AISG-RP-2018-003]; MOE
   Tier-1 research Grant [RG126/17 (S)]
FX This research was carried out at the Rapid-Rich Object Search (ROSE) Lab
   and the Interdisciplinary Graduate School at the Nanyang Technological
   University, Singapore. The ROSE Lab was supported by the National
   Research Foundation, Singapore, and the Infocomm Media Development
   Authority, Singapore. This work was also partly supported by MoE Tier-2
   Grant (2016-T2-2-065) and MoE Tier-1 Grant (RG28/18) of Singapore. The
   work of G. Lin was supported in part by the National Research Foundation
   Singapore under its AI Singapore Programme (AISG-RP-2018-003) and in
   part by the MOE Tier-1 research Grant [RG126/17 (S)].
CR [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], P INT JOINT C ART IN
   [Anonymous], P BRIT MACH VIS C MA
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], P INT C LEARN REPR W
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34
   Chaudhry A., 2017, P BRIT MACH VIS C
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   Hong S, 2016, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2016.349
   Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Li XX, 2017, PROC CVPR IEEE, P6459, DOI 10.1109/CVPR.2017.684
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang-Chieh Chen, 2015, INT C LEARN REPR
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780
   Qi XJ, 2016, LECT NOTES COMPUT SC, V9912, P90, DOI 10.1007/978-3-319-46484-8_6
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Roy A, 2017, PROC CVPR IEEE, P7282, DOI 10.1109/CVPR.2017.770
   Saleh F, 2016, LECT NOTES COMPUT SC, V9912, P413, DOI 10.1007/978-3-319-46484-8_25
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Shimoda W, 2016, LECT NOTES COMPUT SC, V9908, P218, DOI 10.1007/978-3-319-46493-0_14
   Torr P. H, 2017, EMMCVPR, P263
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yunchao Wei, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6488, DOI 10.1109/CVPR.2017.687
   Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
   Zhuang BH, 2017, PROC CVPR IEEE, P2915, DOI 10.1109/CVPR.2017.311
NR 54
TC 52
Z9 53
U1 2
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2930
EP 2941
DI 10.1109/TMM.2019.2914870
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mitrica, I
   Mercier, E
   Ruellan, C
   Fiandrotti, A
   Cagnazzo, M
   Pesquet-Popescu, B
AF Mitrica, Iulia
   Mercier, Eric
   Ruellan, Christophe
   Fiandrotti, Attilio
   Cagnazzo, Marco
   Pesquet-Popescu, Beatrice
TI Very Low Bitrate Semantic Compression of Airplane Cockpit Screen Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HEVC; screen content coding; cockpit content coding; low bitrate;
   character recognition; semantic video coding; convolutional neural
   networks; compound video; compound images
ID VIDEO; IMAGE; ALGORITHM; LINES
AB This paper addresses the problem of encoding the video generated by the screen of an airplane cockpit. As other computer screens, cockpit screens consist of computer-generated graphics often atop a natural background. Existing screen content coding schemes fail notably in preserving the readability of textual information at the low bitrates required in avionic applications. We propose a screen coding scheme where textual information is encoded according to the relative semantics rather than in the pixel domain. The encoder localizes textual information, and the semantics of each character are extracted with a convolutional neural network and predictively encoded. Text is then removed via inpainting, and the residual background video is compressed with a standard codec and transmitted to the receiver together with the text semantics. At the decoder side, text is synthesized using the decoded semantics and superimposed over the decoded residual video recovering the original frame. Our proposed scheme offers two key advantages over a semantics-unaware scheme that encodes text in the pixel domain. First, the text readability at the decoder is not compromised by compression artifacts, whereas the relative bitrate is negligible. Second, removal of high-frequency transform coefficients associated with the inpainted text drastically reduces the bitrate of the residual ideo. Experiments with real cockpit video sequences show BD-rate gains up to 82% and 69% over a reference H.265/HEVC encoder and its screen content coding extension. Moreover, our scheme achieves quasi-errorless character recognition already at very low bitrates, whereas even HEVC-SCC needs at least three or four times more bitrate to achieve a comparable error rate.
C1 [Mitrica, Iulia; Mercier, Eric; Ruellan, Christophe] Zodiac Data Syst, F-78373 Plaisir, France.
   [Mitrica, Iulia] Telecom ParisTech, F-75013 Paris, France.
   [Fiandrotti, Attilio; Cagnazzo, Marco; Pesquet-Popescu, Beatrice] Univ Paris Saclay, Telecom ParisTech, Informat Proc & Commun Lab, F-75013 Paris, France.
C3 Zodiac Aerospace; IMT - Institut Mines-Telecom; Institut Polytechnique
   de Paris; Telecom Paris; Universite Paris Cite; Universite Paris Saclay;
   IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris
RP Mitrica, I (corresponding author), Zodiac Data Syst, F-78373 Plaisir, France.
EM iulia.mitrica@telecom-paristech.fr; merciere@free.fr;
   christophe.ruellan@zodiacaerospace.com; attilio.fiandrotti@gmail.com;
   cagnazzo@telecom-paristech.fr; beatrice.pesquet@telecom-paristech.fr
RI Cagnazzo, Marco/AAZ-3881-2020
OI Cagnazzo, Marco/0000-0001-6731-3755; Iulia, Mitrica/0000-0002-0648-3440;
   Fiandrotti, Attilio/0000-0002-9991-6822; Mercier,
   Eric/0000-0002-8973-227X
CR [Anonymous], PG252 XIL
   [Anonymous], 2007, P 10 INT C INF FUS
   [Anonymous], 2016, DEEP LEARNING
   [Anonymous], 2018, TECH REP
   [Anonymous], 2018, WP504 XIL
   [Anonymous], VESA DISPLAY STREAM
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bjotegaard G., 2001, VCEGM33
   Cagnazzo M, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/78323
   de Queiroz R, 1998, P SOC PHOTO-OPT INS, V3653, P1106, DOI 10.1117/12.334618
   de Queiroz R. L., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P209, DOI 10.1109/ICIP.1999.821599
   de Queiroz RL, 2000, IEEE T IMAGE PROCESS, V9, P1461, DOI 10.1109/83.862619
   Décombas M, 2015, APSIPA TRANS SIGNAL, V4, DOI 10.1017/ATSIP.2015.4
   Ding WP, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P809, DOI 10.1109/ICME.2006.262624
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Feng G, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON SOLID-STATE AND INTEGRATED CIRCUIT TECHNOLOGY (ICSICT), P624, DOI 10.1109/ICSICT.2016.7998996
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   Haffner P., 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P625, DOI 10.1109/ICDAR.1999.791865
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Kang JW, 2016, IEEE T MULTIMEDIA, V18, P2054, DOI 10.1109/TMM.2016.2595259
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo HC, 2012, IEEE T MULTIMEDIA, V14, P500, DOI 10.1109/TMM.2012.2191945
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin T, 2005, IEEE T IMAGE PROCESS, V14, P993, DOI 10.1109/TIP.2005.849776
   Matas J, 2000, COMPUT VIS IMAGE UND, V78, P119, DOI 10.1006/cviu.1999.0831
   Mrak M, 2012, EUR SIGNAL PR CONF, P1209
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Peng WH, 2016, IEEE J EM SEL TOP C, V6, P393, DOI 10.1109/JETCAS.2016.2608971
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Said A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P229, DOI 10.1109/ICIP.1999.821603
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   WELCH TA, 1984, COMPUTER, V17, P8, DOI 10.1109/MC.1984.1659158
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yin XC, 2016, IEEE T IMAGE PROCESS, V25, P2752, DOI 10.1109/TIP.2016.2554321
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 43
TC 5
Z9 7
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2157
EP 2170
DI 10.1109/TMM.2019.2900168
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200001
OA Green Published
DA 2024-07-18
ER

PT J
AU Galteri, L
   Seidenari, L
   Bertini, M
   Del Bimbo, A
AF Galteri, Leonardo
   Seidenari, Lorenzo
   Bertini, Marco
   Del Bimbo, Alberto
TI Deep Universal Generative Adversarial Compression Artifact Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compression; image restoration; object detection
ID IMAGE QUALITY ASSESSMENT; SUPERRESOLUTION; DEBLOCKING; FRAMEWORK;
   ALGORITHM
AB Image compression is a need that arises in many circumstances. Unfortunately, whenever a lossy compression algorithm is used, artifacts will manifest. Image artifacts, caused by compression tend to eliminate higher frequency details and, in certain cases, may add noise or small image structures. There are two main drawbacks of this phenomenon. First, images appear much less pleasant to the human eye. Second, computer vision algorithms, such as object detectors, may be hindered and their performance reduced. Removing such artifacts means recovering the original image from a perturbed version of it. This means that one ideally should invert the compression process through a complicated nonlinear image transformation. We propose an image transformation approach based on a feedforward fully convolutional residual network model. We show that this model can be optimized either traditionally, directly optimizing an image similarity loss (SSIM), or using a generative adversarial approach (GAN). Our GAN is able to produce images with more photorealistic details than SSIM-based networks. We describe a novel training procedure based on subpatches and devise a novel testing protocol to evaluate restored images quantitatively. We show that our approach can be used as a preprocessing step for different computer vision tasks in case images are degraded by compression to a point that state-of-the art algorithms fail. In this case, our GAN-based approach obtains better performance than MSE or SSIM trained networks. Different from previously proposed approaches, we are able to remove artifacts generated at any QF by inferring the image quality directly from data.
C1 [Galteri, Leonardo; Seidenari, Lorenzo; Bertini, Marco; Del Bimbo, Alberto] Univ Firenze, MICC, I-50134 Florence, Italy.
C3 University of Florence
RP Seidenari, L (corresponding author), Univ Firenze, MICC, I-50134 Florence, Italy.
EM leonardo.galteri@unifi.it; lorenzo.seidenari@unifi.it;
   marco.bertini@unifi.it; alberto.delbimbo@unifi.it
RI Galteri, Leonardo/HSI-0092-2023; Galteri, Leonardo/AAB-1635-2020;
   Bertini, Marco/X-1325-2019; Seidenari, Lorenzo/AAA-1848-2020
OI Galteri, Leonardo/0000-0002-7247-9407; Bertini,
   Marco/0000-0002-1364-218X; Seidenari, Lorenzo/0000-0003-4816-0268
FU NVIDIA Corporation
FX This work was supported by the NVIDIA Corporation through the donation
   of the Titan X Pascal GPUs. The associate editor coordinating the review
   of this manuscript and approving it for publication was. Prof. Yongdong
   Zhang. (Corresponding author: Lorenzo Seidenari.)
CR [Anonymous], 2017, P IEEE CVF C COMP VI
   [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2014, LIVE IMAGE QUALITY A
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2015, ARXIV150806576V2
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, P NEUR INF PROC SYST
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], METH SUBJ ASS QUAL T
   [Anonymous], P WSCG C COMP GRAPH
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2016, DISTILL, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Balle J., 2018, P INT C LEARN REPR
   Bruna J., 2016, ICLR, P1
   Cavigelli L, 2017, IEEE IJCNN, P752, DOI 10.1109/IJCNN.2017.7965927
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581
   Dar Y, 2016, IEEE T IMAGE PROCESS, V25, P3044, DOI 10.1109/TIP.2016.2558825
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang LW, 2015, IEEE T MULTIMEDIA, V17, P921, DOI 10.1109/TMM.2015.2434216
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   King DB, 2015, ACS SYM SER, V1214, P1
   Kumar N, 2018, IEEE T MULTIMEDIA, V20, P298, DOI 10.1109/TMM.2017.2729021
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Nuutinen M, 2016, BEHAV RES METHODS, V48, P138, DOI 10.3758/s13428-014-0555-y
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Rippel O., 2017, P 34 INT C MACH LEAR, P2922
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, INT CONF ACOUST SPEE, P3313
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Winkler S, 2009, INT WORK QUAL MULTIM, P139, DOI 10.1109/QOMEX.2009.5246961
   Wong TS, 2009, IEEE T IMAGE PROCESS, V18, P2518, DOI 10.1109/TIP.2009.2028252
   Yang S, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P869, DOI 10.1109/ICIP.2000.899594
   Yim C, 2011, IEEE T IMAGE PROCESS, V20, P88, DOI 10.1109/TIP.2010.2061859
   Yoo J, 2018, PROC CVPR IEEE, P6684, DOI 10.1109/CVPR.2018.00699
   Zhang J, 2016, IEEE T IMAGE PROCESS, V25, P1246, DOI 10.1109/TIP.2016.2515985
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang XF, 2013, IEEE T IMAGE PROCESS, V22, P4613, DOI 10.1109/TIP.2013.2274386
NR 53
TC 56
Z9 60
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2131
EP 2145
DI 10.1109/TMM.2019.2895280
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700019
DA 2024-07-18
ER

PT J
AU Zhou, ML
   Wei, XK
   Wang, SQ
   Kwong, S
   Fong, CK
   Wong, PHW
   Yuen, WYF
   Gao, W
AF Zhou, Mingliang
   Wei, Xuekai
   Wang, Shiqi
   Kwong, Sam
   Fong, Chi-Keung
   Wong, Peter H. W.
   Yuen, Wilson Y. F.
   Gao, Wei
TI SSIM-Based Global Optimization for CTU-Level Rate Control in HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human visual perception; rate control; rate-distortion optimization;
   global optimization
ID RATE CONTROL ALGORITHM; BIT ALLOCATION; GAME-THEORY; VIDEO; MODELS;
   SCHEME
AB In this paper, we propose a coding tree unit (CTU)-level rate control scheme from the perspective of SSIM-based rate-distortion optimization to improve the coding efficiency. First, we establish the SSIM-based rate-distortion model based on the divisive normalization scheme, which characterizes the relationship between the local visual quality and the coding bits. Then, the establishedmodel is applied to the CTU-level rate control and transformed into a global optimization problem solved by convex optimization. Finally, a new model parameter updating strategy for the CTU-level rate control is presented that is robust to scene variations. Our algorithm can achieve optimal CTU-level bit allocation given the bit-rate budget. The experimental results show that our algorithm substantially enhances the coding performance and consistently outperforms both the rate control scheme in the HEVC reference software and existing algorithms in terms of rate-perceptual distortion performance using different test configurations.
C1 [Zhou, Mingliang; Wei, Xuekai; Wang, Shiqi; Kwong, Sam; Gao, Wei] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Fong, Chi-Keung; Wong, Peter H. W.; Yuen, Wilson Y. F.] TFI Digital Media Ltd, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM minzhou@cityu.edu.hk; xuekaiwei2-c@my.cityu.edu.hk;
   shiqwang@cityu.edu.hk; cssamk@cityu.edu.hk; calvin.fong@tfidm.com;
   peter.wong@tfidm.com; wilson@tfidm.com; gaowei262@126.com
RI Zhou, Mingliang/HPC-0298-2023; Wong, Peter/AAA-3481-2022; Kwong,
   Sam/C-9319-2012
OI Wong, Peter/0000-0003-0373-933X; Kwong, Sam/0000-0001-7484-7261; Zhou,
   Mingliang/0000-0002-1874-3641; WEI, Xuekai/0000-0002-3761-1759
FU Hong Kong ITF UICP [9440174]; Natural Science Foundation of China
   [61801303, 61672443]; Hong Kong RGC General Research Fund [9042489,
   CityU 11206317, 9042322, CityU 11200116]; National Natural Science Funds
   of China [61502277]; Hong Kong RGC Early Career Scheme [9048122, CityU
   21211018]; City University of Hong Kong [7200539/CS]; startup project of
   Shenzhen University [2018069]
FX This work was supported in part by Hong Kong ITF UICP under Grant
   9440174, in part by the Natural Science Foundation of China under Grant
   61801303 and Grant 61672443, in part by Hong Kong RGC General Research
   Fund 9042489 under Grant CityU 11206317, in part by Hong Kong RGC
   General Research Fund 9042322 under Grant CityU 11200116, in part by the
   National Natural Science Funds of China under Grant 61502277, in part by
   the Hong Kong RGC Early Career Scheme under Grant 9048122 (CityU
   21211018), in part by the City University of Hong Kong under Grant
   7200539/CS, and in part by the startup project of Shenzhen University
   (2018069). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yonghong Tian.
   (Corresponding author: Sam Kwong.)
CR [Anonymous], 2012, 2012 VISUAL COMMUNIC
   [Anonymous], 2012, International Conference on Indoor Positioning and Indoor Navigation (IPIN), DOI DOI 10.1109/IPIN.2012.6418880
   [Anonymous], 2013, PROC VIS COMMUN IMAG
   [Anonymous], 2018, HMREFERENCE SOFTWARE
   Aswathappa Babu Hemanth Kumar, 2010, 2010 42nd Southeastern Symposium on System Theory (SSST 2010), P367, DOI 10.1109/SSST.2010.5442789
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bross B., 2012, P 8 JCTVC M SAN JOS, P1
   Choi H.-J., 2012, SEMICONDUCTOR NANOST, P1, DOI DOI 10.1109/OCEANS-YEOSU.2012.6263424
   Dong JP, 2009, IEEE T CIRC SYST VID, V19, P1108, DOI 10.1109/TCSVT.2009.2020338
   Gao W, 2017, IEEE T IMAGE PROCESS, V26, P6074, DOI 10.1109/TIP.2017.2745099
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   He ZH, 2008, IEEE T MULTIMEDIA, V10, P1237, DOI 10.1109/TMM.2008.2004903
   Karczewicz M., 2013, P 13 M JOINT COLL TE, P1
   Lee B, 2014, IEEE T CIRC SYST VID, V24, P465, DOI 10.1109/TCSVT.2013.2276880
   Li BF, 2013, ASIA PAC J OPER RES, V29, P1
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li SX, 2017, IEEE T CIRC SYST VID, V27, P2409, DOI 10.1109/TCSVT.2016.2589878
   Li ZG, 2006, J VIS COMMUN IMAGE R, V17, P376, DOI 10.1016/j.jvcir.2005.04.004
   Lin WY, 2008, IEEE T CIRC SYST VID, V18, P1128, DOI 10.1109/TCSVT.2008.927111
   Liu Y., 2006, P INT C AC SPEECH SI, V2
   Luo ZY, 2013, IEEE T CIRC SYST VID, V23, P935, DOI 10.1109/TCSVT.2013.2240919
   Ma SW, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P793
   Mansour H, 2011, IEEE T MULTIMEDIA, V13, P165, DOI 10.1109/TMM.2010.2099648
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang MH, 2016, IEEE T IMAGE PROCESS, V25, P2943, DOI 10.1109/TIP.2016.2552646
   Wang MH, 2015, IEEE SIGNAL PROC LET, V22, P896, DOI 10.1109/LSP.2014.2377032
   Wang N, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P1370
   Wang S, 2014, ROUTL ADV GEOGR, V11, P1
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Wang SQ, 2017, IEEE T CIRC SYST VID, V27, P2189, DOI 10.1109/TCSVT.2016.2580398
   Wang SQ, 2013, IEEE T IMAGE PROCESS, V22, P1418, DOI 10.1109/TIP.2012.2231090
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yan B, 2009, IEEE SIGNAL PROC LET, V16, P145, DOI 10.1109/LSP.2008.2010813
   Yang CL, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING AND INTELLIGENT SYSTEMS, PROCEEDINGS, VOL 4, P291, DOI 10.1109/ICICISYS.2009.5357689
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Zhao DD, 2011, COMPUT ELECTR ENG, V37, P550, DOI 10.1016/j.compeleceng.2011.04.009
   Zhou M., 2014, Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific, P1, DOI 10.1109/OCEANS.2014.7003239
   Zhou ML, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107616
NR 40
TC 45
Z9 46
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1921
EP 1933
DI 10.1109/TMM.2019.2895281
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700003
DA 2024-07-18
ER

PT J
AU Gu, XL
   Wong, YK
   Shou, LD
   Peng, P
   Chen, G
   Kankanhalli, MS
AF Gu, Xiaoling
   Wong, Yongkang
   Shou, Lidan
   Peng, Pai
   Chen, Gang
   Kankanhalli, Mohan S.
TI Multi-Modal and Multi-Domain Embedding Learning for Fashion Retrieval
   and Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-view embedding learning; heterogeneous similarity; homogeneous
   similarity; fashion analysis
ID IMAGES
AB Big data analytics has been revolutionizing the fashion industry in recent years. This is evidenced by the fact that popular fashion brands and designers have relied on big data analytics to trace fashion trends and predict market patterns. In this paper, we propose learning a common latent feature representation from heterogeneous fashion data. Specifically, we design a multi-modal and multi-domain embedding learning framework for fashion analysis and data retrieval. Unlike most of the existing multi-view embedding methods, which only consider the heterogeneous similarity constraint, our proposed framework jointly considers both the homogeneous and heterogeneous similarity constraints to capture cross-view similarity and preserve the similarity of the same view. The proposed framework is comprised of two projection steps. In the first projection, a quintuplet-based ranking loss is proposed for multi-domain fashion data to preserve the homogeneous similarity. In the second projection, a cross-view similarity ranking loss is designed for multi-modal fashion data to capture heterogeneous similarity. By utilizing the learned common latent feature representation, the distance between any vector pairs from same or different modalities can reflect its semantic similarity. Quantitative evaluation on a new large-scale dataset and a fashion analysis case study demonstrate the effectiveness of our proposed method.
C1 [Gu, Xiaoling] Hangzhou Dianzi Univ, Key Lab Complex Syst Modeling & Simulat, Sch Comp Sci & Technol, Hangzhou 310005, Zhejiang, Peoples R China.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Shou, Lidan; Chen, Gang] Zhejiang Univ, Database Lab, Dept Comp Sci, Hangzhou 310027, Zhejiang, Peoples R China.
   [Peng, Pai] Tencent Technol Shanghai Co Ltd, Youtu Lab, Shanghai 200233, Peoples R China.
C3 Hangzhou Dianzi University; National University of Singapore; Zhejiang
   University
RP Gu, XL (corresponding author), Hangzhou Dianzi Univ, Key Lab Complex Syst Modeling & Simulat, Sch Comp Sci & Technol, Hangzhou 310005, Zhejiang, Peoples R China.
EM guxl@hdu.edu.cn; yongkang.wong@nus.edu.sg; should@zju.edu.cn;
   popeyepeng@tencent.com; cg@zju.edu.cn; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428
FU National Basic Research Program (973 Program) [2015CB352400]; National
   Science Foundation of China [61802100, 61836002, 61622205, 61472110,
   61672455, 61528207, 61472348]; Natural Science Foundation of Zhejiang
   Province of China [LY18F020005]; National Research Foundation, Prime
   Minister's Office, Singapore under its International Research Centre in
   Singapore Funding Initiative
FX This work was supported in part by the National Basic Research Program
   (973 Program, 2015CB352400), in part by the National Science Foundation
   of China under Grants 61802100, 61836002, 61622205, 61472110, 61672455,
   61528207, and 61472348, in part by the Natural Science Foundation of
   Zhejiang Province of China (LY18F020005), and in part by the National
   Research Foundation, Prime Minister's Office, Singapore under its
   International Research Centre in Singapore Funding Initiative. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Bertini.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], P 11 AS C COMP VIS
   [Anonymous], 2011, P ICML
   [Anonymous], P ADV NEURAL INFORM
   [Anonymous], 2014, ABS14112539 CORR
   Bell S, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766959
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen KT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P177, DOI 10.1145/2733373.2809930
   Davidson I, 2005, LECT NOTES ARTIF INT, V3721, P59
   Fang HG, 2012, INT CONF COMP SCI ED, P1333, DOI 10.1109/ICCSE.2012.6295311
   Feng YJ, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL ELECTROMAGNETICS (ICCEM), P15, DOI 10.1109/COMPEM.2017.7912749
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   Hidayati SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P197, DOI 10.1145/2647868.2656405
   Hu XF, 2014, IEEE IJCNN, P7, DOI 10.1109/IJCNN.2014.6889605
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Jiao H, 2015, ASIA PAC J MANAG, V32, P1083, DOI 10.1007/s10490-015-9427-y
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma Z, 2015, PR MACH LEARN RES, V37, P169
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Ng A. Y., 2002, Advances in Neural Information Processing Systems, P1473
   Quadrianto Novi., 2011, ICML, P425
   Shao D, 2017, INT C MANAGE SCI ENG, P190, DOI 10.1109/ICMSE.2017.8574440
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun BW, 2014, ACTA POLYM SIN, P649, DOI 10.3724/SP.J.1105.2014.13342
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Vittayakorn S, 2015, IEEE WINT CONF APPL, P951, DOI 10.1109/WACV.2015.131
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang Liu., 2006, Distance metric learning: A comprehensive survey, V2
   Zhai DM, 2012, ACM T INTEL SYST TEC, V3, DOI 10.1145/2168752.2168767
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhang ML, 2014, IEEE T KNOWL DATA EN, V26, P1819, DOI 10.1109/TKDE.2013.39
   Zhang XF, 2016, PROC CVPR IEEE, P1114, DOI 10.1109/CVPR.2016.126
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
NR 50
TC 27
Z9 27
U1 2
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1524
EP 1537
DI 10.1109/TMM.2018.2876822
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400015
DA 2024-07-18
ER

PT J
AU Yang, M
   Zhao, W
   Xu, W
   Feng, YB
   Zhao, Z
   Chen, XJ
   Lei, K
AF Yang, Min
   Zhao, Wei
   Xu, Wei
   Feng, Yabing
   Zhao, Zhou
   Chen, Xiaojun
   Lei, Kai
TI Multitask Learning for Cross-Domain Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multitask learning; image captioning; image synthesis; dual learning;
   reinforcement learning
ID REPRESENTATION
AB Recent artificial intelligence research has witnessed great interest in automatically generating text descriptions of images, which are known as the image captioning task. Remarkable success has been achieved on domains where a large number of paired data in multimedia are available. Nevertheless, annotating sufficient data is labor-intensive and time-consuming, establishing significant barriers for adapting the image captioning systems to new domains. In this study, we introduc a novel Multitask Learning Algorithm for cross-Domain Image Captioning (MLADIC). MLADIC is a multitask system that simultaneously optimizes two coupled objectives via a dual learning mechanism: image captioning and text-to-image synthesis, with the hope that by leveraging the correlation of the two dual tasks, we are able to enhance the image captioning performance in the target domain. Concretely, the image captioning task is trained with an encoder-decoder model (i.e., CNN-LSTM) to generate textual descriptions of the input images. The image synthesis task employs the conditional generative adversarial network (C-GAN) to synthesize plausible images based on text descriptions. In C-GAN, a generative model G synthesizes plausible images given text descriptions, and a discriminative model D tries to distinguish the images in training data from the generated images by G. The adversarial process can eventually guide G to generate plausible and high-quality images. To bridge the gap between different domains, a two-step strategy is adopted in order to transfer knowledge from the source domains to the target domains. First, we pre-train the model to learn the alignment between the neural representations of images and that of text data with the sufficient labeled source domain data. Second, we fine-tune the learned model by leveraging the limited image-text pairs and unpaired data in the target domain. We conduct extensive experiments to evaluate the performance of MLADIC by using the MSCOCO as the source domain data, and using Flickr30k and Oxford-102 as the target domain data. The results demonstrate that MLADIC achieves substantially better performance than the strong competitors for the cross-domain image captioning task.
C1 [Yang, Min; Zhao, Wei] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Xu, Wei; Feng, Yabing] Tencent, Shenzhen 518057, Peoples R China.
   [Zhao, Zhou] Zhejiang Univ, Sch Comp Sci, Hangzhou 310058, Zhejiang, Peoples R China.
   [Chen, Xiaojun] Shenzhen Univ, Sch Comp Sci, Shenzhen 518060, Peoples R China.
   [Lei, Kai] Peking Univ, Sch Elect & Comp Engn, Shenzhen Key Lab Informat Centr Networking & Bloc, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Tencent; Zhejiang University; Shenzhen University; Peking
   University
RP Lei, K (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Key Lab Informat Centr Networking & Bloc, Shenzhen 518055, Peoples R China.
EM min.yang@siat.ac.cn; wei.zhao@siat.ac.cn; weizierxu@tencent.com;
   jessefeng@tencent.com; zhaozhou@zju.edu.cn; xjchen@szu.edu.cn;
   leik@pkusz.edu.cn
RI Lei, Kai/W-5518-2019; zhao, zhao/JAC-1686-2023; Zhao, zhuo/JYO-7894-2024
OI Lei, Kai/0000-0001-9197-895X; 
FU Shenzhen Key Fundamental Research Project [JCYJ20151030154330711];
   Guangdong Natural Science Fund Project [2018A030313017]
FX This work was supported in part by Shenzhen Key Fundamental Research
   Project JCYJ20151030154330711 and Guangdong Natural Science Fund
   Project: 2018A030313017. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Benoit Huet.
   (Corresponding author: Kai Lei.)
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], PROC CVPR IEEE
   [Anonymous], T ASS COMPUT LINGUIS
   [Anonymous], 2016, Proceedings of the 24th ACM international conference on Multimedia
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 1998, INTRO REINFORCEMENT
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2004, P WORKSHOP TEXT SUMM
   [Anonymous], 2018, P IEEE C COMP VIS PA
   [Anonymous], 2013, P 2013 C EMP METH NA
   [Anonymous], CVPR
   [Anonymous], 2017, P AAAI
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   Bengio S, 2015, ADV NEUR IN, V28
   Bernardi R, 2016, J ARTIF INTELL RES, V55, P409, DOI 10.1613/jair.4900
   Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He D, 2016, ADV NEUR IN, V29
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P, 2015, International Conference on Learning Representations
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   LeCun Y., 2016, P 4 INT C LEARN REPR, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S., 2016, EMNLP, P1961
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Wu XX, 2016, IEEE T MULTIMEDIA, V18, P1305, DOI 10.1109/TMM.2016.2557722
   Xu K., 2015, COMPUTER SCI, P2048
   Yang J., 2015, Advances in Neural Information Processing Systems, P1099
   Yang M, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1021, DOI 10.1145/3077136.3080706
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yuan Y, 2016, IEEE T CYBERNETICS, V46, P2966, DOI 10.1109/TCYB.2015.2484324
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 59
TC 82
Z9 84
U1 0
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 1047
EP 1061
DI 10.1109/TMM.2018.2869276
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700019
DA 2024-07-18
ER

PT J
AU Yang, X
   Mei, HY
   Zhang, JQ
   Xu, K
   Yin, BC
   Zhang, Q
   Wei, XP
AF Yang, Xin
   Mei, Haiyang
   Zhang, Jiqing
   Xu, Ke
   Yin, Baocai
   Zhang, Qiang
   Wei, Xiaopeng
TI DRFN: Deep Recurrent Fusion Network for Single-Image Super-Resolution
   With Large Factors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image super-resolution; transposed convolution; deep recurrent network;
   multi-level fusion structure; large factors
ID QUALITY ASSESSMENT
AB Recently, single-image super-resolution has made great progress due to the development of deep convolutional neural networks (CNNs). The vast majority of CNN-based models use a predefined upsampling operator, such as bicubic interpolation, to upscale input low-resolution images to the desired size and learn nonlinear mapping between the interpolated image and ground truth high-resolution (HR) image. However, interpolation processing can lead to visual artifacts as details are over smoothed, particularly when the super-resolution factor is high. In this paper, we propose a deep recurrent fusion network (DRFN), which utilizes transposed convolution instead of bicubic interpolation for upsampling and integrates different-level features extracted from recurrent residual blocks to reconstruct the final HR images. We adopt a deep recurrence learning strategy and, thus, have a larger receptive field, which is conducive to reconstructing an image more accurately. Furthermore, we show that the multilevel fusion structure is suitable for dealing with image super-resolution problems. Extensive benchmark evaluations demonstrate that the proposed DRFN performs better than most current deep learning methods in terms of accuracy and visual effects, especially for large-scale images, while using fewer parameters.
C1 [Yang, Xin; Mei, Haiyang; Zhang, Jiqing; Xu, Ke; Yin, Baocai; Zhang, Qiang; Wei, Xiaopeng] Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Dalian University of Technology
RP Zhang, Q; Wei, XP (corresponding author), Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
EM xinyang@dlut.edu.cn; mhy845879017@gmail.com;
   zhangjiqing@mail.dlut.edu.cn; kkangwing@mail.dlut.edu.cn;
   ybc@dlut.edu.cn; zhangq@dlut.edu.cn; xpwei@dlut.edu.cn
RI Zhang, Qiang/IWU-5000-2023; Zhang, Jiqing/AIC-1975-2022; Jiang,
   Tao/IWM-7503-2023; Mei, Haiyang/AAA-1479-2020; wei, xiao/ISB-6027-2023;
   zhang, qiang/HZJ-9551-2023; jiang, lei/IWE-1124-2023; ,
   kk/AAH-8764-2020; Zhang, Qiang/GXF-3105-2022
OI Zhang, Jiqing/0000-0002-0061-5465; Mei, Haiyang/0000-0003-3549-9684; ,
   Xin/0000-0002-8046-722X; Zhang, Qiang/0000-0003-3776-9799
FU National Natural Science Foundation of China [91748104, 61632006,
   61425002, U1708263]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 91748104, Grant 61632006, Grant
   61425002, and Grant U1708263. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Raouf Hamzaoui.
CR [Anonymous], STUD APPL MATH
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Cui Z, 2014, LECT NOTES COMPUT SC, V8693, P49, DOI 10.1007/978-3-319-10602-1_4
   Dai D, 2015, COMPUT GRAPH FORUM, V34, P95, DOI 10.1111/cgf.12544
   Dai S., 2007, CVPR, P1, DOI DOI 10.1109/CVPR.2007.383028
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Efrat N, 2013, IEEE I CONF COMP VIS, P2832, DOI 10.1109/ICCV.2013.352
   Fattal R, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239502, 10.1145/1276377.1276441]
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Lai W-S, 2017, PROC CVPR IEEE, P624, DOI DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Martin D., 2001, P ICCV, P416, DOI [DOI 10.1109/ICCV.2001.937655, 10.1109/ICCV.2001.937655]
   Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121
   Nah S., 2016, P IEEE C COMP VIS PA, V1, P3
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi Y, 2017, IEEE T MULTIMEDIA, V19, P2804, DOI 10.1109/TMM.2017.2711263
   Singh A, 2015, LECT NOTES COMPUT SC, V9004, P552, DOI 10.1007/978-3-319-16808-1_37
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu K, 2018, VISUAL COMPUT, V34, P1065, DOI 10.1007/s00371-018-1554-2
   Yang CY, 2013, IEEE I CONF COMP VIS, P561, DOI 10.1109/ICCV.2013.75
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang YB, 2016, IEEE T MULTIMEDIA, V18, P405, DOI 10.1109/TMM.2015.2512046
NR 44
TC 88
Z9 95
U1 2
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 328
EP 337
DI 10.1109/TMM.2018.2863602
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, CG
   Xie, HT
   Chen, JJ
   Zha, ZJ
   Hao, XH
   Zhang, YD
   Dai, QH
AF Yan, Chenggang
   Xie, Hongtao
   Chen, Jianjun
   Zha, Zhengjun
   Hao, Xinhong
   Zhang, Yongdong
   Dai, Qionghai
TI A Fast Uyghur Text Detector for Complex Background Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Uyghur text localization; FASTroke keypoint extractor; Uyghur sence
   image
ID SCENE; RECOGNITION
AB Uyghur text localization in images with complex backgrounds is a challenging yet important task for many applications. Generally, Uyghur characters in images consist of strokes with uniform features, and they are distinct from backgrounds in color, intensity, and texture. Based on these differences, we propose a FASTroke keypoint extractor, which is fast and stroke-specific. Compared with the commonly used MSER detector, FASTroke produces less than twice the amount of components and recognizes at least 10% more characters. While the characters in a line usually have uniform features such as size, color, and stroke width, a component similarity based clustering is presented without component-level classification. It incurs no extra errors by incorporating a component-level classifier while the computing cost is drastically reduced. The experiments show that the proposed method can achieve the best performance on the UICBI-500 benchmark dataset.
C1 [Yan, Chenggang; Xie, Hongtao; Zha, Zhengjun; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Inst Informat & Control, Hangzhou 310018, Zhejiang, Peoples R China.
   [Chen, Jianjun] Chinese Acad Sci, Natl Engn Lab Informat Secur Technol, Inst Informat Engn, Sch Cyber Secur, Beijing 100093, Peoples R China.
   [Hao, Xinhong] Beijing Inst Technol, Sci & Technol Mechatron Dynam Control Lab, Beijing 100081, Peoples R China.
   [Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Hangzhou Dianzi University; Chinese Academy of Sciences;
   Beijing Institute of Technology; Tsinghua University
RP Xie, HT (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM cgyan@hdu.edu.cn; htxie@ustc.edu.cn; chenjianjun@iie.ac.cn;
   haoxinhong@bit.edu.cn; zhyd73@ustc.edu.cn; qhdai@tsinghua.edu.cn
RI Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020; Dai,
   Qionghai/ABD-5298-2021
OI Zha, Zheng-Jun/0000-0003-2510-8993; Dai, Qionghai/0000-0001-7043-3061
FU National Nature Science Foundation of China [61525206, 61771468,
   61622211, 61472392, 61620106009]; National Key Research and Development
   Program of China [2017YFC0820600]; Youth Innovation Promotion
   Association Chinese Academy of Sciences [2017209]
FX This work was supported in part by the National Nature Science
   Foundation of China (61525206, 61771468, 61622211, 61472392 and
   61620106009), in part by the National Key Research and Development
   Program of China (2017YFC0820600), and in part by the Youth Innovation
   Promotion Association Chinese Academy of Sciences (2017209). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yonggang Wen. (Corresponding
   author: Hongtao Xie.)
CR Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Busta M, 2015, IEEE I CONF COMP VIS, P1206, DOI 10.1109/ICCV.2015.143
   Chen JJ, 2018, LECT NOTES COMPUT SC, V10704, P565, DOI 10.1007/978-3-319-73603-7_46
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Fang S, 2017, MULTIMEDIA TOOLS APP, V76, P1
   Hanif Shehzad Muhammad, 2009, 2009 10th International Conference on Document Analysis and Recognition (ICDAR), P1, DOI 10.1109/ICDAR.2009.172
   He T, 2016, IEEE T IMAGE PROCESS, V25, P2529, DOI 10.1109/TIP.2016.2547588
   Huang R, 2013, PROC INT CONF DOC, P462, DOI 10.1109/ICDAR.2013.99
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jianjun Chen, 2016, Advances in Multimedia Information Processing - PCM 2016. 17th Pacific-Rim Conference on Multimedia. Proceedings: LNCS 9917, P406, DOI 10.1007/978-3-319-48896-7_40
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Koo HI, 2013, IEEE T IMAGE PROCESS, V22, P2296, DOI 10.1109/TIP.2013.2249082
   Lee JJ, 2011, PROC INT CONF DOC, P429, DOI 10.1109/ICDAR.2011.93
   Liu S, 2017, LECT NOTES COMPUT SC, V10132, P490, DOI 10.1007/978-3-319-51811-4_40
   Mancas-Thillou C, 2007, COMPUT VIS IMAGE UND, V107, P97, DOI 10.1016/j.cviu.2006.11.010
   Mancas-Thillou C, 2006, IEEE IMAGE PROC, P985, DOI 10.1109/ICIP.2006.312653
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mokrzycki W. S., 2011, Machine Graphics & Vision, V20, P383
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Neumann L, 2011, PROC INT CONF DOC, P687, DOI 10.1109/ICDAR.2011.144
   Nikolaou N, 2009, INT J IMAG SYST TECH, V19, P14, DOI 10.1002/ima.20174
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Park J, 2010, PATTERN RECOGN LETT, V31, P1728, DOI 10.1016/j.patrec.2010.05.024
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Shivakumara P., 2008, Pattern recognition, P1
   Song Y, 2017, MACH VISION APPL, V28, P755, DOI 10.1007/s00138-017-0837-3
   Sung MC, 2015, PROC INT CONF DOC, P426, DOI 10.1109/ICDAR.2015.7333797
   Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xiaodong Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3216, DOI 10.1109/ICPR.2010.786
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765
   Yi Chucai, 2011, IEEE Trans Image Process, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yin XC, 2015, IEEE T PATTERN ANAL, V37, P1930, DOI 10.1109/TPAMI.2014.2388210
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zamberletti A, 2015, LECT NOTES COMPUT SC, V9009, P91, DOI 10.1007/978-3-319-16631-5_7
   Zhao X, 2011, IEEE T IMAGE PROCESS, V20, P790, DOI 10.1109/TIP.2010.2068553
NR 44
TC 155
Z9 156
U1 5
U2 71
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3389
EP 3398
DI 10.1109/TMM.2018.2838320
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600017
DA 2024-07-18
ER

PT J
AU Jerripothula, KR
   Cai, JF
   Yuan, JS
AF Jerripothula, Koteswar Rao
   Cai, Jianfei
   Yuan, Junsong
TI Quality-Guided Fusion-Based Co-Saliency Estimation for Image
   Co-Segmentation and Colocalization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-saliency; co-segmentation; co-localization; fusion; foreground;
   quality
ID OBJECT DETECTION; COSEGMENTATION
AB Despite the advantage of exploiting interimage information by performing joint processing of images for co-saliency, co-segmentation, or co-localization, it introduces a few drawbacks: 1) its necessity in scenarios where the joint processing might not perform better than individual image processing; 2) increased complexity over individual image processing; and 3) complex parameter tuning. In this paper, we propose a simple co-saliency estimation method where we fuse saliency maps of different images using the dense correspondence technique. More important, the co-saliency estimation is guided by our proposed quality measurement that helps decide whether the saliency fusion really improves the quality of the saliency map or not. Our basic idea for developing the quality metric is that a high-quality saliency map should have well-separated foreground and background, as well as a concentrated foreground like ground-truths. Extensive experiments on several benchmark datasets including the large-scale dataset, ImageNet, for the applications of foreground co-segmentation and co-localization show that our proposed framework is able to achieve very competitive results.
C1 [Jerripothula, Koteswar Rao] Graph Era Univ, Dehra Dun 248002, Uttar Pradesh, India.
   [Cai, Jianfei; Yuan, Junsong] Nanyang Technol Univ, Singapore 639798, Singapore.
C3 Graphic Era University; Nanyang Technological University
RP Jerripothula, KR (corresponding author), Graph Era Univ, Dehra Dun 248002, Uttar Pradesh, India.
EM krjimp@geu.ac.in; asjfcai@ntu.edu.sg; jsyuan@ntu.edu.sg
RI Jerripothula, Koteswar Rao/N-6980-2015; Yuan, Junsong/R-4352-2019; Cai,
   Jianfei/A-3691-2011
OI Jerripothula, Koteswar Rao/0000-0002-3507-3731; Yuan,
   Junsong/0000-0002-7901-8793; Cai, Jianfei/0000-0002-9444-3763
FU Infocomm Media Development Authority, Singapore
FX This researchwas carried out at the Rapid-Rich Object Search (ROSE) Lab
   at the Nanyang Technological University, Singapore. The ROSE Lab is
   supported by the Infocomm Media Development Authority, Singapore.
CR [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.570
   [Anonymous], 2010, P ACM MULTIMEDIA
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chai YN, 2011, IEEE I CONF COMP VIS, P2579, DOI 10.1109/ICCV.2011.6126546
   Chen HT, 2010, IEEE IMAGE PROC, P1117, DOI 10.1109/ICIP.2010.5650014
   Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724
   Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Fu HZ, 2015, IEEE T IMAGE PROCESS, V24, P3415, DOI 10.1109/TIP.2015.2442915
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   FU HZ, 2015, PROC CVPR IEEE, P4428, DOI DOI 10.1109/CVPR.2015
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Guillaumin M, 2012, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2012.6248055
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Jacobs D.E., 2010, ACM S USER INTERFACE, P219
   Jain SD, 2016, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2016.313
   Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413
   Jerripothula KR, 2016, LECT NOTES COMPUT SC, V9911, P187, DOI 10.1007/978-3-319-46478-7_12
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jerripothula KR, 2015, 2015 VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2015.7457899
   Jerripothula KR, 2015, IEEE IMAGE PROC, P4639, DOI 10.1109/ICIP.2015.7351686
   Jerripothula KR, 2014, IEEE IMAGE PROC, P3277, DOI 10.1109/ICIP.2014.7025663
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17
   Joulin A, 2012, PROC CVPR IEEE, P542, DOI 10.1109/CVPR.2012.6247719
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Mai L, 2014, LECT NOTES COMPUT SC, V8691, P76, DOI 10.1007/978-3-319-10578-9_6
   Meng FM, 2016, COMPUT VIS IMAGE UND, V146, P67, DOI 10.1016/j.cviu.2016.02.004
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Shotton J, 2006, LECT NOTES COMPUT SC, V3951, P1
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tao ZQ, 2017, AAAI CONF ARTIF INTE, P4285
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Vezhnevets A, 2014, PROC CVPR IEEE, P1987, DOI 10.1109/CVPR.2014.255
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Yuan JS, 2012, IEEE T IMAGE PROCESS, V21, P2207, DOI 10.1109/TIP.2011.2181952
   ZHANG D, 2018, ACM T INTEL SYST TEC, V9, P1
   Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012
NR 56
TC 36
Z9 37
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2466
EP 2477
DI 10.1109/TMM.2018.2798294
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200018
DA 2024-07-18
ER

PT J
AU Cheung, CH
   Ngan, KN
   Sheng, L
AF Cheung, Chi Ho
   Ngan, King Ngi
   Sheng, Lu
TI Spatio-Temporal Disocclusion Filling Using Novel Sprite Cells
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Disocclusion filling; depth image-based rendering; 3-D; inpainting
ID VIEW SYNTHESIS; IMAGE; VIDEO
AB Depth image-based rendering is an important technique for virtual view synthesis with limited 3-D data. However, occluded areas result in disocclusions in synthesized images. Filling of the disocclusions in a plausible manner is a critical task in virtual view synthesis. In addition to spatial consistency, temporal consistency of the filled regions also affects the visual quality. In this paper, we propose a novel codebook method called sprite cell for filling the disocclusions with high spatial and temporal consistency. Each codeword consists of a color vector, depth value, frame log, and the confidence score of the corresponding pixel. In contrast with the existing methods that reuse the filling results of previous frames without considering their accuracy, the proposed method estimates the confidence scores of the filling results to prevent temporal continuation of filling errors. Moreover, we introduce a method to correct the luminance of filled disocclusions that compensates for the change of scenes. The experimental results show that the proposed method achieves both objective and subjective improvements over the state-of-the-art methods. The sequences synthesized with the proposed method have higher spatio-temporal consistency.
C1 [Cheung, Chi Ho; Ngan, King Ngi; Sheng, Lu] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
   [Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610051, Sichuan, Peoples R China.
C3 Chinese University of Hong Kong; University of Electronic Science &
   Technology of China
RP Cheung, CH (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM chcheung@ee.cuhk.edu.hk; knngan@ee.cuhk.edu.hk; lsheng@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014; Sheng, Lu/V-2526-2019
OI Ngan, N/0000-0003-1946-3235; Sheng, Lu/0000-0002-8525-9163
CR Ahn I, 2013, IEEE T BROADCAST, V59, P614, DOI 10.1109/TBC.2013.2281658
   [Anonymous], 2011, 3DTV C TRUE VIS CAPT
   [Anonymous], 2008, ETRI 3DV DAT
   [Anonymous], 3DV SEQ HHI
   [Anonymous], BASLER TIME OF FLIGH
   [Anonymous], 2003, P IIASTED VIIP
   [Anonymous], BLUETECHNIX TIME OF
   [Anonymous], ISO IEC JTC1 SC29 WG
   [Anonymous], 3DV SEQ ETRI GIST
   [Anonymous], P 3DTV C TRUE VIS CA
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Cheng C.-M., 2008, PROC 19 INT C PATTER, P1
   Cheung C. H., 2015, P IEEE INT C MULT EX, P1
   Choi S, 2013, IEEE T IMAGE PROCESS, V22, P2429, DOI 10.1109/TIP.2013.2251646
   Criminisi A, 2003, PROC CVPR IEEE, P721
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Doan HN, 2015, LECT NOTES COMPUT SC, V9315, P598, DOI 10.1007/978-3-319-24078-7_61
   Grimson WEL, 1998, PROC CVPR IEEE, P22, DOI 10.1109/CVPR.1998.698583
   Guillemot C, 2014, IEEE SIGNAL PROC MAG, V31, P127, DOI 10.1109/MSP.2013.2273004
   Hsu HA, 2014, IEEE T CIRC SYST VID, V24, P74, DOI 10.1109/TCSVT.2013.2276699
   Kim HG, 2015, IEEE IMAGE PROC, P3136, DOI 10.1109/ICIP.2015.7351381
   Kim S., 2007, Proceedings of the 70th Annual Meeting of the American Society for Information Science and Technology, V44, P1
   Köppel M, 2016, IEEE T BROADCAST, V62, P457, DOI 10.1109/TBC.2016.2529287
   Le Meur O, 2013, IEEE T IMAGE PROCESS, V22, P3779, DOI 10.1109/TIP.2013.2261308
   Lee PJ, 2011, IEEE T MULTIMEDIA, V13, P246, DOI 10.1109/TMM.2010.2100372
   Luo GB, 2016, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2016.197
   McMillan Leonard, 1997, THESIS
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Plath N, 2013, IEEE T IMAGE PROCESS, V22, P3420, DOI 10.1109/TIP.2013.2268940
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   Smolic A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2161, DOI 10.1109/ICME.2006.262683
   Smolic A, 2011, PATTERN RECOGN, V44, P1958, DOI 10.1016/j.patcog.2010.09.005
   Tanimoto M, 2012, P IEEE, V100, P905, DOI 10.1109/JPROC.2011.2182101
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Vazquez C., 2006, P SPIE C 3D TV VID D, p63 920D
   Wang LF, 2011, P AM MATH SOC, V139, P3679, DOI 10.1090/S0002-9939-2011-10758-5
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhang L, 2005, IEEE T BROADCAST, V51, P191, DOI 10.1109/TBC.2005.846190
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 43
TC 4
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1376
EP 1391
DI 10.1109/TMM.2017.2772442
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400008
DA 2024-07-18
ER

PT J
AU Li, N
   Xu, YF
   Wang, C
AF Li, Nan
   Xu, Yifang
   Wang, Chao
TI Quasi-Homography Warps in Image Stitching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image stitching; image warping; natural-looking; projective distortion;
   perspective distortion
ID PANORAMAS; ALIGNMENT; REGION; SYSTEM; SCENES
AB The naturalness of warps is gaining extensive attention in image stitching. Recent warps, such as SPHP and AANAP, use global similarity warps to mitigate projective distortion (which enlarges regions); however, they necessarily bring in perspective distortion (which generates inconsistencies). In this paper, we propose a novel quasi-homography warp, which effectively balances the perspective distortion against the projective distortion in the non-overlapping region to create a more natural-looking panorama. Our approach formulates the warp as the solution of a bivariate system, where perspective distortion and projective distortion are characterized as slope preservation and scale linearization, respectively. Because our proposed warp only relies on a global homography, it is thus totally parameter free. A comprehensive experiment shows that a quasi-homography warp outperforms some state-of-the-art warps in urban scenes, including homography, AutoStitch and SPHP. A user study demonstrates that it wins most users' favor, compared to homography and SPHP.
C1 [Li, Nan] Tianjin Univ, Ctr Appl Math, Tianjin 300072, Peoples R China.
   [Xu, Yifang] Nankai Univ, Ctr Combinator, Tianjin 300071, Peoples R China.
   [Wang, Chao] Nankai Univ, Dept Software, Tianjin 300071, Peoples R China.
C3 Tianjin University; Nankai University; Nankai University
RP Xu, YF (corresponding author), Nankai Univ, Ctr Combinator, Tianjin 300071, Peoples R China.
EM nan@tju.edu.cn; xyf@mail.nankai.edu.cn; wangchao@nankai.edu.cn
RI Xu, Yifang/IWL-8258-2023; Xu, Yifang/AAY-4429-2020
OI Xu, Yifang/0009-0001-3738-789X; Xu, Yifang/0000-0002-7569-1956; Li,
   Nan/0000-0001-5973-2829
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   BURT PJ, 1983, ACM T GRAPHIC, V2, P217, DOI 10.1145/245.247
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen Q, 2013, IEEE T MULTIMEDIA, V15, P521, DOI 10.1109/TMM.2012.2236306
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Davis J, 1998, PROC CVPR IEEE, P354, DOI 10.1109/CVPR.1998.698630
   Duplaquet ML, 1998, P SOC PHOTO-OPT INS, V3387, P369, DOI 10.1117/12.316427
   Eden A., 2006, P 2006 IEEE COMP SOC, VVolume 2, P2498
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Gao J., 2013, Eurographics (Short Papers), P45, DOI DOI 10.2312/CONF/EG2013/SHORT/045-048
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Kushnir M, 2014, IEEE T PATTERN ANAL, V36, P2381, DOI 10.1109/TPAMI.2014.2339862
   Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264
   Levin A, 2004, LECT NOTES COMPUT SC, V2034, P377
   Li YF, 2018, MULTIMED TOOLS APPL, V77, P3279, DOI 10.1007/s11042-017-5072-4
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin WY, 2016, LECT NOTES COMPUT SC, V9905, P562, DOI 10.1007/978-3-319-46448-0_34
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2052, DOI 10.1109/TMM.2014.2346476
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mills A, 2009, IMAGE VISION COMPUT, V27, P1593, DOI 10.1016/j.imavis.2009.03.004
   PELEG S, 1981, COMPUT VISION GRAPH, V16, P90, DOI 10.1016/0146-664X(81)90094-0
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Tran QH, 2012, LECT NOTES COMPUT SC, V7575, P274, DOI 10.1007/978-3-642-33765-9_20
   Raguram R, 2013, IEEE T PATTERN ANAL, V35, P2022, DOI 10.1109/TPAMI.2012.257
   Shum HY, 2005, IEEE T MULTIMEDIA, V7, P85, DOI 10.1109/TMM.2004.840591
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tang WK, 2005, IEEE T MULTIMEDIA, V7, P280, DOI 10.1109/TMM.2005.843811
   Tzavidas S, 2005, IEEE T MULTIMEDIA, V7, P880, DOI 10.1109/TMM.2005.854430
   Xinding Sun, 2005, IEEE Transactions on Multimedia, V7, P981, DOI 10.1109/TMM.2005.854388
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Zaragoza J, 2014, IEEE T PATTERN ANAL, V36, P1285, DOI 10.1109/TPAMI.2013.247
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P3099, DOI 10.1109/TIP.2016.2535225
   Zhao Q, 2013, IEEE T MULTIMEDIA, V15, P1745, DOI 10.1109/TMM.2013.2280249
NR 43
TC 69
Z9 77
U1 1
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1365
EP 1375
DI 10.1109/TMM.2017.2771566
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, X
   Shen, B
   Liu, BD
   Zhang, YJ
AF Li, Xue
   Shen, Bin
   Liu, Bao-Di
   Zhang, Yu-Jin
TI Ranking-Preserving Low-Rank Factorization for Image Annotation With
   Missing Labels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic image annotation; tag ranking; low-rank matrix factorization;
   missing labels
ID TAG COMPLETION
AB Automatic image annotation has been extensively studied in the recent decades. Nevertheless, existing methods usually assume a properly labeled training set, which greatly inhibits their application to real-world datasets with incomplete labels. Due to their lack of special treatments for noisy data, most existing methods simply consider the missing labels as strictly negative ones, leading to the degradation of tagging accuracy. In light of such challenges, we propose a novel model in this paper, called ranking-preserving low-rank factorization. Specifically, we construct a local training set for each test image, and conduct low-rank matrix factorization on the model coefficient matrix, to simultaneously capture the label dependency and reduce the model complexity. Furthermore, to alleviate the ambiguity introduced by missing labels, the prediction model is learnt via tag ranking regularized by sample similarities and tag correlations, and both regularization terms are incorporated into our factorization scheme. By assembling all the aforementioned components together, our method obviates the need for making binary decisions based on unreliable data, and thus is more robust towards missing labels. Extensive empirical evaluations conducted on four datasets demonstrate the effectiveness of the proposed method.
C1 [Li, Xue; Zhang, Yu-Jin] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Shen, Bin] Purdue Univ, W Lafayette, IN 47907 USA.
   [Shen, Bin] Google New York, New York, NY 10011 USA.
   [Liu, Bao-Di] China Univ Petr Huadong, Dept Informat & Control Engn, Qingdao 266580, Peoples R China.
C3 Tsinghua University; Purdue University System; Purdue University; Google
   Incorporated; China University of Petroleum
RP Shen, B (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.; Shen, B (corresponding author), Google New York, New York, NY 10011 USA.
EM lixue421@gmail.com; stanshen-bin@gmail.com; thu.liubaodi@gmail.com;
   zhang-yj@tsinghua.edu.cn
FU National Nature Science Foundation of China [61171118]
FX This work was supported by the National Nature Science Foundation of
   China under Grant 61171118.
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], 2014, INT C MACH LEARN
   [Anonymous], 2003, Journal of machine learning research
   [Anonymous], 2010, P ACM MULTIMEDIA
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Bucak S. S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2801, DOI 10.1109/CVPR.2011.5995734
   Burges C., 2005, ICML, P89
   Cabral R, 2015, IEEE T PATTERN ANAL, V37, P121, DOI 10.1109/TPAMI.2014.2343234
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chatzilari E, 2016, IEEE T MULTIMEDIA, V18, P1488, DOI 10.1109/TMM.2016.2565440
   Chen A., 2013, ICML, P1274
   Feng SL, 2004, PROC CVPR IEEE, P1002
   Feng SH, 2015, IEEE T IMAGE PROCESS, V24, P1223, DOI 10.1109/TIP.2015.2395816
   Goldberg A. B., 2010, Advances in Neural Information Processing Systems, P757
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Izadinia H, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P13, DOI 10.1145/2814815.2814821
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kalayeh MM, 2014, PROC CVPR IEEE, P184, DOI 10.1109/CVPR.2014.31
   Lan T, 2013, PROC CVPR IEEE, P3103, DOI 10.1109/CVPR.2013.399
   Li X, 2015, JMLR WORKSH CONF PRO, V38, P635
   Li X, 2016, IEEE T MULTIMEDIA, V18, P474, DOI 10.1109/TMM.2016.2518478
   Li X, 2016, NEUROCOMPUTING, V173, P425, DOI 10.1016/j.neucom.2014.12.121
   Lin ZJ, 2014, COMPUT VIS IMAGE UND, V124, P42, DOI 10.1016/j.cviu.2014.03.012
   Lin ZJ, 2013, PROC CVPR IEEE, P1618, DOI 10.1109/CVPR.2013.212
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P702, DOI 10.1109/TMM.2011.2134078
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu X., 2012, ACM T MULTIM COMPUT, V8
   Makadia A, 2008, LECT NOTES COMPUT SC, V5304, P316, DOI 10.1007/978-3-540-88690-7_24
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Simonyan K., 2014, 14091556 ARXIV
   Verma Y, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.25
   Verma Y, 2012, LECT NOTES COMPUT SC, V7574, P836, DOI 10.1007/978-3-642-33712-3_60
   Wang QF, 2014, LECT NOTES COMPUT SC, V8690, P425, DOI 10.1007/978-3-319-10605-2_28
   Weston J, 2011, IJCAI
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Xu LL, 2014, IEEE DATA MINING, P1067, DOI 10.1109/ICDM.2014.125
   Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869
   Yang C., 2006, P IEEE INT C COMPUTE, P2057
   Yang KY, 2011, IEEE T MULTIMEDIA, V13, P662, DOI 10.1109/TMM.2011.2147777
   Yang Liu, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P417, DOI 10.1109/ICDM.2011.141
   Yunbo Cao, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186
   Zheng M, 2011, IEEE T IMAGE PROCESS, V20, P1327, DOI 10.1109/TIP.2010.2090535
NR 45
TC 18
Z9 23
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1169
EP 1178
DI 10.1109/TMM.2017.2761985
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400012
DA 2024-07-18
ER

PT J
AU Meng, FM
   Li, HL
   Wu, QB
   Ngan, KN
   Cai, JF
AF Meng, Fanman
   Li, Hongliang
   Wu, Qingbo
   Ngan, King Ngi
   Cai, Jianfei
TI Seeds-Based Part Segmentation by Seeds Propagation and Region Convexity
   Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive segmentation; part segmentation; weakly supervised
   segmentation
ID CO-SEGMENTATION; SHAPE; COSEGMENTATION; IMAGENET
AB Object part segmentation is an important and challenging task in computer vision. The existing supervised part segmentation methods need pixel level training data, which leads to a huge workload for the user. In this paper, a weakly supervised part segmentation method is proposed, which segments part regions from multiple images by only several seeds on an image. Two aspects such as seed propagation among multiple images and part generation from seeds are considered. The first aspect is to generate part seeds in each image in terms of seed propagation, which is accomplished by part matching combined with latent object regions. We fuse the local part matching and global shape cosegmentation to avoid the noise propagation. The second aspect is to segment part regions from object regions and part seeds, which is formulated as the object shape decomposition model. The shape convexity analysis and seed location are fused to accomplish the decomposition and the final part segmentation. The proposed method is verified on the PASCAL 2010 dataset, Bird dataset, Cat-Dog dataset, and UCF Sports Actions dataset. Experimental results demonstrate the effectiveness of the proposed method with larger intersection over union (IOU) values compared with existing weakly supervised part generation methods.
C1 [Meng, Fanman; Li, Hongliang; Wu, Qingbo; Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Hong Kong, Peoples R China.
   [Cai, Jianfei] Nanyang Technol Univ, Sch Comp Engn, Singapore 639668, Singapore.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong; Nanyang Technological University
RP Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Sichuan, Peoples R China.
EM fmmeng@uestc.edu.cn; hlli@uestc.edu.cn; qbwu@uestc.edu.cn;
   knngan@ee.cuhk.edu.hk; asjfcai@ntu.edu.sg
RI Ngan, N/E-8240-2014; Wu, Qingbo/AAF-6872-2019; Cai, Jianfei/A-3691-2011
OI Ngan, N/0000-0003-1946-3235; Wu, Qingbo/0000-0003-2936-6340; Li,
   Hongliang/0000-0002-7481-095X; Cai, Jianfei/0000-0002-9444-3763
FU National Natural Science Foundation of China [61502084, 61525102,
   61601102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61502084, Grant 61525102, and Grant
   61601102. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Honggang Wang.
   (Corresponding author: Fanman Meng.)
CR AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681
   Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Del Pero L, 2016, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2016.84
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   FU HZ, 2015, PROC CVPR IEEE, P4428, DOI DOI 10.1109/CVPR.2015
   Gopalan R, 2010, LECT NOTES COMPUT SC, V6313, P286
   Gorelick L, 2017, IEEE T PATTERN ANAL, V39, P258, DOI 10.1109/TPAMI.2016.2547399
   Guillaumin M, 2014, INT J COMPUT VISION, V110, P328, DOI 10.1007/s11263-014-0713-9
   Guo G, 2014, INT J COMPUT VISION, V108, P241, DOI 10.1007/s11263-014-0705-9
   Irani M., 2013, P IEEE C COMP VIS DE, P10
   Izadinia H, 2015, IEEE I CONF COMP VIS, P10, DOI 10.1109/ICCV.2015.10
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Kuettel D, 2012, LECT NOTES COMPUT SC, V7578, P459, DOI 10.1007/978-3-642-33786-4_34
   Li HL, 2014, IEEE T IMAGE PROCESS, V23, P3545, DOI 10.1109/TIP.2014.2330759
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu HR, 2010, PROC CVPR IEEE, P97, DOI 10.1109/CVPR.2010.5540225
   Luo B, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1187, DOI 10.1145/2733373.2806313
   Luo L, 2015, IEEE T IMAGE PROCESS, V24, P273, DOI 10.1109/TIP.2014.2376188
   Luo P, 2013, IEEE I CONF COMP VIS, P2648, DOI 10.1109/ICCV.2013.329
   Ma C, 2013, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2013.113
   Meng FM, 2016, COMPUT VIS IMAGE UND, V146, P67, DOI 10.1016/j.cviu.2016.02.004
   Meng FM, 2015, IEEE T CIRC SYST VID, V25, P1735, DOI 10.1109/TCSVT.2015.2402891
   Meng FM, 2013, IEEE T MULTIMEDIA, V15, P2186, DOI 10.1109/TMM.2013.2280893
   Meng FM, 2013, IEEE T IMAGE PROCESS, V22, P4809, DOI 10.1109/TIP.2013.2278461
   Meng FM, 2013, IEEE T CYBERNETICS, V43, P725, DOI 10.1109/TSMCB.2012.2215316
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   Rantalankila P, 2014, PROC CVPR IEEE, P2417, DOI 10.1109/CVPR.2014.310
   Ren Z, 2013, IEEE T PATTERN ANAL, V35, P2546, DOI 10.1109/TPAMI.2013.67
   Ren Z, 2011, IEEE I CONF COMP VIS, P303, DOI 10.1109/ICCV.2011.6126256
   Taniai T, 2016, PROC CVPR IEEE, P4246, DOI 10.1109/CVPR.2016.460
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788
   Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184
   Wang Y, 2013, PROC CVPR IEEE, P3135, DOI 10.1109/CVPR.2013.403
   Yang JM, 2015, PROC CVPR IEEE, P1770, DOI 10.1109/CVPR.2015.7298786
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 43
TC 15
Z9 16
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 310
EP 322
DI 10.1109/TMM.2017.2739919
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200005
DA 2024-07-18
ER

PT J
AU Zhao, N
   Zhang, HW
   Hong, RC
   Wang, M
   Chua, TS
AF Zhao, Na
   Zhang, Hanwang
   Hong, Richang
   Wang, Meng
   Chua, Tat-Seng
TI VIDEOWHISPER: Toward Discriminative Unsupervised Video Feature Learning
   With Attention-Based Recurrent Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recurrent neural networks; sequence learning; unsupervised feature
   learning; video features
ID RECOGNITION
AB We present VIDEOWHISPER, a novel approach for unsupervised video representation learning. Based on the observation that the frame sequence encodes the temporal dynamics of a video (e.g., object movement and event evolution), we treat the frame sequential order as a self-supervision to learn video representations. Unlike other unsupervised video feature learning methods based on frame-level feature reconstruction that is sensitive to visual variance, VIDEOWHISPER is driven by a novel video "sequence-to-whisper" learning strategy. Specifically, for each video sequence, we use a prelearned visual dictionary to generate a sequence of high-level semantics, dubbed "whisper," which can be considered as the language describing the video dynamics. In this way, we model VIDEOWHISPER as an end-to-end sequence-to-sequence learning model using attention-based recurrent neural networks. This model is trained to predict the whisper sequence and hence it is able to learn the temporal structure of videos. We propose two ways to generate video representation from the model. Through extensive experiments on two real-world video datasets, we demonstrate that video representation learned by VIDEOWHISPER is effective to boost fundamental multimedia applications such as video retrieval and event classification.
C1 [Zhao, Na; Zhang, Hanwang; Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Hong, Richang; Wang, Meng] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
C3 National University of Singapore; Hefei University of Technology
RP Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Anhui, Peoples R China.
EM zhaona311@gmail.com; hanwangzhang@gmail.com; hongrc@hfut.edu.cn;
   mengwang@gmail.com; chuats@comp.nus.edu.sg
RI Zhao, Na/AIA-4986-2022; Wang, Meng/ITR-8699-2023
OI Zhao, Na/0000-0003-2329-7014; Zhang, Hanwang/0000-0001-7374-8739
FU National Research Foundation, Prime Minister's Office, Singapore, under
   its IRC@SG Funding Initiative
FX This work was part of the NExT++ project, supported by the National
   Research Foundation, Prime Minister's Office, Singapore, under its
   IRC@SG Funding Initiative. The guest editor coordinating the review of
   this manuscript and approving it for publication was Prof. Qi Tian.
   (Corresponding author: Richang Hong.)
CR [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2015, P ICLR
   [Anonymous], 2009, MOSIFT RECOGNIZING H
   [Anonymous], 2015, CoRR
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, CORR
   Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang YG, 2015, IEEE T IMAGE PROCESS, V24, P3781, DOI 10.1109/TIP.2015.2456412
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Pang B, 2005, P 43 ANN M ASS COMP, P115, DOI [10.3115/1219840.1219855, DOI 10.3115/1219840.1219855]
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K., 2014, CORR
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sutskever I, 2014, ADV NEUR IN, V27
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xu K., 2015, COMPUTER SCI, P2048
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Zhang HW, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P325, DOI 10.1145/2911451.2911502
   Zhang Hanwang, 2017, P IEEE C COMP VIS PA, P5532, DOI DOI 10.48550/ARXIV.1702.08319
   Zhang XD, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1315, DOI 10.1145/2733373.2806338
   Zhao N, 2017, IEEE INT CON MULTI, P277, DOI 10.1109/ICME.2017.8019344
NR 37
TC 20
Z9 20
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2080
EP 2092
DI 10.1109/TMM.2017.2722687
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200011
DA 2024-07-18
ER

PT J
AU Karaoglu, S
   Tao, R
   Gevers, T
   Smeulders, AWM
AF Karaoglu, Sezer
   Tao, Ran
   Gevers, Theo
   Smeulders, Arnold W. M.
TI Words Matter: Scene Text for Image Classification and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Professional communication; image retrieval; computers and information
   processing; image analysis; image classification; text recognition;
   object detection
AB Text in natural images typically adds meaning to an object or scene. In particular, text specifies which business places serve drinks (e.g., cafe, teahouse) or food (e.g., restaurant, pizzeria), and what kind of service is provided (e.g., massage, repair). The mere presence of text, its words, and meaning are closely related to the semantics of the object or scene. This paper exploits textual contents in images for fine-grained business place classification and logo retrieval. There are four main contributions. First, we show that the textual cues extracted by the proposed method are effective for the two tasks. Combining the proposed textual and visual cues outperforms visual only classification and retrieval by a large margin. Second, to extract the textual cues, a generic and fully unsupervised word box proposal method is introduced. The method reaches state-of-the-art word detection recall with a limited number of proposals. Third, contrary to what is widely acknowledged in text detection literature, we demonstrate that high recall in word detection is more important than high f-score at least for both tasks considered in this work. Last, this paper provides a large annotated text detection dataset with 10 K images and 27 601 word boxes.
C1 [Karaoglu, Sezer; Gevers, Theo] Univ Amsterdam, Comp Vis Lab, NL-1098 XH Amsterdam, Netherlands.
   [Tao, Ran; Smeulders, Arnold W. M.] Univ Amsterdam, Intelligent Sensory Informat Syst Lab, NL-1098 XH Amsterdam, Netherlands.
   [Gevers, Theo] Univ Autonoma Barcelona, Comp Vis Ctr, E-08193 Barcelona, Spain.
C3 University of Amsterdam; University of Amsterdam; Centre de Visio per
   Computador (CVC); Autonomous University of Barcelona
RP Karaoglu, S (corresponding author), Univ Amsterdam, Comp Vis Lab, NL-1098 XH Amsterdam, Netherlands.
EM s.karaoglu@uva.nl; r.tao@uva.nl; th.gevers@uva.nl;
   a.w.m.smeulders@uva.nl
FU Dutch National Program COMMIT/
FX This work was supported by the Dutch National Program COMMIT/. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tao Mei. (Sezer Karaoglu and Ran
   Tao contributed equally to this work.)
CR Almazán J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814
   [Anonymous], 2013, CORR
   [Anonymous], 2013, Proceedings of the 21st ACM International Conference on Multimedia
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], CORR
   [Anonymous], 2010, CALTECH UCSD BIRDS 2
   [Anonymous], 2009, P 17 ACM INT C MULTI, DOI DOI 10.1145/1631272.1631361
   [Anonymous], 2014, CORR
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], J VISION
   [Anonymous], 2011, P 1 ACM INT C MULT R, DOI DOI 10.1145/1991996.1992021
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Breiman L., 2001, Mach. Learn., V45, P5
   Chen H., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2609, DOI 10.1109/ICIP.2011.6116200
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Everts I, 2012, LECT NOTES COMPUT SC, V7577, P172, DOI 10.1007/978-3-642-33783-3_13
   Ezaki N, 2004, INT C PATT RECOG, P683, DOI 10.1109/ICPR.2004.1334351
   Fernando Basura., 2010, International Conference on Machine Vision (ICMV), P144
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Goel V, 2013, PROC INT CONF DOC, P398, DOI 10.1109/ICDAR.2013.87
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z
   Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Karaoglu S, 2012, LECT NOTES COMPUT SC, V7585, P456, DOI 10.1007/978-3-642-33885-4_46
   Li Y, 2014, IEEE T IMAGE PROCESS, V23, P1666, DOI 10.1109/TIP.2014.2302896
   Liu JX, 2012, LECT NOTES COMPUT SC, V7572, P172, DOI 10.1007/978-3-642-33718-5_13
   Lu SJ, 2015, INT J DOC ANAL RECOG, V18, P125, DOI 10.1007/s10032-015-0237-z
   Lu T, 2014, ADV COMPUT VIS PATT, P221, DOI 10.1007/978-1-4471-6515-6_9
   Mao J., 2013, Proceedings of ACM MM'13, ACM, P1007
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mishra A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.127
   Mishra A, 2013, IEEE I CONF COMP VIS, P3040, DOI 10.1109/ICCV.2013.378
   Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990
   Movshovitz-Attias Y, 2015, PROC CVPR IEEE, P1693, DOI 10.1109/CVPR.2015.7298778
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Neumann L, 2011, LECT NOTES COMPUT SC, V6494, P770, DOI 10.1007/978-3-642-19318-7_60
   Neumann L, 2011, PROC INT CONF DOC, P687, DOI 10.1109/ICDAR.2011.144
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54
   Romberg S., 2013, Proceedings of the 3rd ACM conference on International conference on multimedia retrieval, P113
   Rusiñol M, 2014, INT J DOC ANAL RECOG, V17, P331, DOI 10.1007/s10032-014-0225-8
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tolias G., 2016, Conference Track Proceedings,
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   Vincent L, 1993, IEEE T IMAGE PROCESS, V2, P176, DOI 10.1109/83.217222
   Wang G, 2009, PROC CVPR IEEE, P1367, DOI 10.1109/CVPRW.2009.5206816
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yi CC, 2011, IEEE T IMAGE PROCESS, V20, P2594, DOI 10.1109/TIP.2011.2126586
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang Y., 2015, CORR
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhu Q., 2006, Proceedings of ACM MM'06, ACM, P211
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 67
TC 73
Z9 76
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1063
EP 1076
DI 10.1109/TMM.2016.2638622
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000014
OA Green Published
DA 2024-07-18
ER

PT J
AU Ren, XH
   Zhou, Y
   He, JH
   Chen, K
   Yang, XK
   Sun, J
AF Ren, Xiaohang
   Zhou, Yi
   He, Jianhua
   Chen, Kai
   Yang, Xiaokang
   Sun, Jun
TI A Convolutional Neural Network-Based Chinese Text Detection Algorithm
   via Text Structure Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Chinese text detection; convolutional neural network (CNN); text
   structure detector; unsupervised learning
ID READING TEXT; SCENE
AB Text detection in a natural environment plays an important role in many computer vision applications. While existing text detection methods are focused on English characters, there are strong application demands on text detection in other languages, such as Chinese. In this paper, we present a novel text detection algorithm for Chinese characters based on a specific designed convolutional neural network (CNN). The CNN contains a text structure component detector layer, a spatial pyramid layer, and a multi-input-layer deep belief network (DBN). The CNN is pre-trained via a convolutional sparse auto-encoder, specifically designed for extracting complex features from Chinese characters. In particular, the text structure component detectors enhance the accuracy and uniqueness of feature descriptors by extracting multiple text structure components in various ways. The spatial pyramid layer enhances the scale invariability of the CNN for detecting texts in multiple scales. Finally, the multi-input-layer DBN replaces the fully connected layers in the CNN to ensure features from multiple scales are comparable. A multilingual text detection dataset, in which texts in Chinese, English, and digits are labeled separately, is set up to evaluate the proposed text detection algorithm. The proposed algorithm shows a significant performance improvement over the baseline CNN algorithms. In addition the proposed algorithm is evaluated over a public multilingual benchmark and achieves state-of-the-art result under multiple languages. Furthermore, a simplified version of the proposed algorithm with only general components is evaluated on the ICDAR 2011 and 2013 datasets, showing comparable detection performance to the existing general text detection algorithms.
C1 [Ren, Xiaohang; Zhou, Yi; Chen, Kai; Yang, Xiaokang; Sun, Jun] Shanghai Jiao Tong Univ, Dept Elect Engn, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [He, Jianhua] Aston Univ, Sch Engn & Appl Sci, Birmingham B4 7ET, W Midlands, England.
C3 Shanghai Jiao Tong University; Aston University
RP Ren, XH (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM xiaomu@sjtu.edu.cn; zy_21th@sjtu.edu.cn; j.he7@aston.ac.uk;
   kchen@sjtu.edu.cn; xkyang@sjtu.edu.cn; junsun@sjtu.edu.cn
RI Yang, Xiaokang/C-6137-2009
OI Yang, Xiaokang/0000-0003-4029-3322; He, Jianhua/0000-0002-5738-8507
FU National Key Research and Development Program of China [2016YFB1001003];
   National Natural Science Foundation of China [61521062, 61527804];
   Shanghai Science and Technology Committees of Scientific Research
   Project [14XD1402100, 15JC1401700]; 111 Program [B07022]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001003, in part by the
   National Natural Science Foundation of China under Grant 61521062 and
   Grant 61527804, in part by Shanghai Science and Technology Committees of
   Scientific Research Project under Grant 14XD1402100 and Grant
   15JC1401700, and in part by the 111 Program (B07022).
CR Bai B, 2013, PROC INT CONF DOC, P1380, DOI 10.1109/ICDAR.2013.279
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bouman KL, 2011, IEEE T MULTIMEDIA, V13, P922, DOI 10.1109/TMM.2011.2154317
   Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57
   Chalasani R., 2013, The 2013 International Joint Conference on Neural Networks (IJCNN), P1, DOI [10.1109/IJCNN.2013.6706854, DOI 10.1109/IJCNN.2013.6706854]
   Chen XR, 2004, PROC CVPR IEEE, P366
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Guo XC, 2014, CHEMOSPHERE, V112, P1, DOI 10.1016/j.chemosphere.2014.03.068
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jaderberg M., 2014, CORR
   Jung C, 2009, PATTERN RECOGN LETT, V30, P114, DOI 10.1016/j.patrec.2008.05.014
   Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221
   Li XJ, 2008, IEEE IMAGE PROC, P969, DOI 10.1109/ICIP.2008.4711918
   Liu J, 2011, PROC INT CONF DOC, P1044, DOI 10.1109/ICDAR.2011.211
   Liu XQ, 2012, IEEE T MULTIMEDIA, V14, P482, DOI 10.1109/TMM.2011.2177646
   Liu Y, 2013, PROC INT CONF DOC, P1355, DOI 10.1109/ICDAR.2013.274
   Lucas SM, 2005, PROC INT CONF DOC, P80, DOI 10.1109/ICDAR.2005.231
   Lucas SM, 2003, PROC INT CONF DOC, P682
   Neumann L, 2013, PROC INT CONF DOC, P523, DOI 10.1109/ICDAR.2013.110
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Pan YF, 2011, IEEE T IMAGE PROCESS, V20, P800, DOI 10.1109/TIP.2010.2070803
   Sermanet P, 2013, PROC CVPR IEEE, P3626, DOI 10.1109/CVPR.2013.465
   Shahab A, 2011, PROC INT CONF DOC, P1491, DOI 10.1109/ICDAR.2011.296
   Shivakumara P, 2011, IEEE T PATTERN ANAL, V33, P412, DOI 10.1109/TPAMI.2010.166
   Shivakumara P, 2010, PATTERN RECOGN, V43, P2165, DOI 10.1016/j.patcog.2010.01.009
   Tsai S., 2012, P AS C COMP VIS, V12, P13
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang T, 2012, INT C PATT RECOG, P3304
   Wu L, 2015, IEEE T MULTIMEDIA, V17, P1137, DOI 10.1109/TMM.2015.2443556
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Ye QX, 2005, IMAGE VISION COMPUT, V23, P565, DOI 10.1016/j.imavis.2005.01.004
   Yin XC, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P1091
   Yin XC, 2014, IEEE T PATTERN ANAL, V36, P970, DOI 10.1109/TPAMI.2013.182
   Zhang Z, 2015, PROC CVPR IEEE, P2558, DOI 10.1109/CVPR.2015.7298871
   Zhao X, 2011, IEEE T IMAGE PROCESS, V20, P790, DOI 10.1109/TIP.2010.2068553
   Zhou X., 2009, LINGUISTIC RES, V01, P62
NR 38
TC 39
Z9 40
U1 4
U2 65
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 506
EP 518
DI 10.1109/TMM.2016.2625259
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400007
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Moon, SE
   Lee, JS
AF Moon, Seong-Eun
   Lee, Jong-Seok
TI Implicit Analysis of Perceptual Multimedia Experience Based on
   Physiological Response: A Review
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Implicit analysis; multimedia; perceptual experience; physiological
   signal
ID VISUAL FATIGUE; EMOTION RECOGNITION; VIRTUAL-REALITY; BRAIN ACTIVITY;
   EEG; MUSIC; QUALITY; COMPLEXITY; P300; AESTHETICS
AB The exponential growth of popularity of multimedia has led to needs for user-centric adaptive applications that manage multimedia content more effectively. Implicit analysis, which examines users' perceptual experience of multimedia by monitoring physiological or behavioral cues, has potential to satisfy such demands. Particularly, physiological signals categorized into cerebral physiological signals (electroencephalography, functional magnetic resonance imaging, and functional near-infrared spectroscopy) and peripheral physiological signals (heart rate, respiration, skin temperature, etc.) have recently received attention along with notable development of wearable physiological sensors. In this paper, we review existing studies on physiological signal analysis exploring perceptual experience of multimedia. Furthermore, we discuss current trends and challenges.
C1 [Moon, Seong-Eun; Lee, Jong-Seok] Yonsei Univ, Sch Integrated Technol, Incheon 406840, South Korea.
   [Moon, Seong-Eun; Lee, Jong-Seok] Yonsei Univ, Yonsei Inst Convergence Technol, Incheon 406840, South Korea.
C3 Yonsei University; Yonsei University
RP Moon, SE (corresponding author), Yonsei Univ, Sch Integrated Technol, Incheon 406840, South Korea.; Moon, SE (corresponding author), Yonsei Univ, Yonsei Inst Convergence Technol, Incheon 406840, South Korea.
EM se.moon@yonsei.ac.kr; jong-seok.lee@yonsei.ac.kr
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425
FU Ministry of Science, ICT and Future Planning, Korea, under the "IT
   Consilience Creative Program" [IITP-R0346-16-1008]
FX This work was supported by the Ministry of Science, ICT and Future
   Planning, Korea, under the "IT Consilience Creative Program"
   (IITP-R0346-16-1008), supervised by the Institute for Information and
   Communications Technology Promotion.
CR Abadi MK, 2015, IEEE T AFFECT COMPUT, V6, P209, DOI 10.1109/TAFFC.2015.2392932
   Anguera JA, 2013, NATURE, V501, P97, DOI 10.1038/nature12486
   [Anonymous], 2003, GEN METH SUBJ ASS SO
   [Anonymous], 2015, P 7 INT WORKSH QUAL
   [Anonymous], 1978, CRITIQUE JUDGMENT
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2011, P ICML
   [Anonymous], VISUAL SIGNAL QUALIT
   [Anonymous], 2013, EUROPEAN NETWORK QUA
   [Anonymous], P INT WORKSH PERC QU
   [Anonymous], FACIAL EXPRESSION EM
   [Anonymous], SOCIAL MEDIA RETRIEV
   [Anonymous], SUBJ METH ASS STER 3
   [Anonymous], 2015, NEUROSCIENCE EXPLORI
   [Anonymous], ACM T INTERACTIVE IN
   [Anonymous], P 4 INT C INT ENV
   [Anonymous], 2013, IEEE INT S BROADB MU
   [Anonymous], 2006, VOC PERF QUAL SERV
   [Anonymous], 1922, The emotions
   Antons JN, 2014, T-LAB SER TELECOMMUN, P109, DOI 10.1007/978-3-319-02681-7_8
   Antons JN, 2013, INT CONF ACOUST SPEE, P3672, DOI 10.1109/ICASSP.2013.6638343
   Antons JN, 2012, IEEE J-STSP, V6, P721, DOI 10.1109/JSTSP.2012.2191936
   Arndt S, 2014, IEEE J-STSP, V8, P366, DOI 10.1109/JSTSP.2014.2313026
   Arndt S, 2013, INT WORK QUAL MULTIM, P152, DOI 10.1109/QoMEX.2013.6603229
   Berking M, 2012, CURR OPIN PSYCHIATR, V25, P128, DOI 10.1097/YCO.0b013e3283503669
   Berlyne D.E., 1971, Aesthetics and Psychobiology
   BOITEN FA, 1994, INT J PSYCHOPHYSIOL, V17, P103, DOI 10.1016/0167-8760(94)90027-2
   Cabanac M, 2002, BEHAV PROCESS, V60, P69, DOI 10.1016/S0376-6357(02)00078-5
   Cannon WB, 1927, AM J PSYCHOL, V39, P106, DOI 10.2307/1415404
   Cela-Conde CJ, 2004, P NATL ACAD SCI USA, V101, P6321, DOI 10.1073/pnas.0401427101
   Chanel G, 2006, LECT NOTES COMPUT SC, V4105, P530
   Chanel G, 2011, IEEE T SYST MAN CY A, V41, P1052, DOI 10.1109/TSMCA.2011.2116000
   Chen CX, 2015, BIOMED ENG ONLINE, V14, DOI 10.1186/1475-925X-14-S1-S12
   Chen M, 2015, INT CONF AFFECT, P63, DOI 10.1109/ACII.2015.7344552
   Cho BH, 2002, P IEEE VIRT REAL ANN, P156, DOI 10.1109/VR.2002.996518
   Clerico A, 2015, I IEEE EMBS C NEUR E, P914, DOI 10.1109/NER.2015.7146774
   COHEN D, 1968, SCIENCE, V161, P784, DOI 10.1126/science.161.3843.784
   Costa T, 2006, NEUROSCI LETT, V406, P159, DOI 10.1016/j.neulet.2006.06.039
   Croft RJ, 2000, NEUROPHYSIOL CLIN, V30, P5, DOI 10.1016/S0987-7053(00)00055-1
   Daimi SN, 2014, EXPERT SYST APPL, V41, P6057, DOI 10.1016/j.eswa.2014.03.050
   Davis S, 2011, INT WORK QUAL MULTIM, P143, DOI 10.1109/QoMEX.2011.6065693
   Dolan RJ, 2002, SCIENCE, V298, P1191, DOI 10.1126/science.1076358
   Duchowski AT, 2002, BEHAV RES METH INS C, V34, P455, DOI 10.3758/BF03195475
   DUNCANJOHNSON CC, 1982, BIOL PSYCHOL, V14, P1, DOI 10.1016/0301-0511(82)90016-3
   EKMAN P, 1987, J PERS SOC PSYCHOL, V53, P712, DOI 10.1037/0022-3514.53.4.712
   Falk TH, 2012, INT WORK QUAL MULTIM, P146, DOI 10.1109/QoMEX.2012.6263874
   Falk TH, 2011, IEEE T NEUR SYS REH, V19, P136, DOI 10.1109/TNSRE.2010.2078516
   FARLEY FH, 1980, B PSYCHONOMIC SOC, V15, P194
   Frantzidis CA, 2010, IEEE T INF TECHNOL B, V14, P589, DOI 10.1109/TITB.2010.2041553
   Frey J, 2016, PHYSICS: PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON PHYSIOLOGICAL COMPUTING SYSTEMS, P105, DOI 10.5220/0005954501050114
   FRIJDA NH, 1988, AM PSYCHOL, V43, P349, DOI 10.1037/0003-066X.43.5.349
   Friston KJ, 2011, BRAIN CONNECT, V1, P13, DOI 10.1089/brain.2011.0008
   Gerson AD, 2006, IEEE T NEUR SYS REH, V14, P174, DOI 10.1109/TNSRE.2006.875550
   Gray HM, 2004, J EXP SOC PSYCHOL, V40, P216, DOI 10.1016/S0022-1031(03)00092-1
   Güçlütürk Y, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00112
   Gupta Rishabh, 2015, 2015 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). Proceedings, P1, DOI 10.1109/WASPAA.2015.7336888
   Gupta R, 2016, NEUROCOMPUTING, V174, P875, DOI 10.1016/j.neucom.2015.09.085
   Haas L F, 2003, J Neurol Neurosurg Psychiatry, V74, P9, DOI 10.1136/jnnp.74.1.9
   Hillebrand A, 2005, HUM BRAIN MAPP, V25, P199, DOI 10.1002/hbm.20102
   HOSHI Y, 1993, J APPL PHYSIOL, V75, P1842, DOI 10.1152/jappl.1993.75.4.1842
   Imamoglu Ç, 2000, J ENVIRON PSYCHOL, V20, P5, DOI 10.1006/jevp.1999.0155
   Jatupaiboon N, 2013, SCI WORLD J, DOI 10.1155/2013/618649
   Jerritta S., 2011, 2011 Proceedings of IEEE 7th International Colloquium on Signal Processing & its Applications (CSPA 2011), P410, DOI 10.1109/CSPA.2011.5759912
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Juslin PN, 2003, ANN NY ACAD SCI, V1000, P279, DOI 10.1196/annals.1280.025
   Kawabata H, 2004, J NEUROPHYSIOL, V91, P1699, DOI 10.1152/jn.00696.2003
   Key APF, 2005, DEV NEUROPSYCHOL, V27, P183, DOI 10.1207/s15326942dn2702_1
   Khalili Z., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1571, DOI 10.1109/IJCNN.2009.5178854
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kim DI, 2011, J IND ENG CHEM, V17, P1, DOI 10.1016/j.jiec.2010.12.010
   Kim J, 2008, IEEE T PATTERN ANAL, V30, P2067, DOI 10.1109/TPAMI.2008.26
   Kim Y.J., 2011, Int. Conference on Human-Comp. Interaction, V2, P289
   Kirk U, 2009, NEUROIMAGE, V44, P1125, DOI 10.1016/j.neuroimage.2008.10.009
   Ko KE, 2009, INT J CONTROL AUTOM, V7, P865, DOI 10.1007/s12555-009-0521-0
   Kober SE, 2012, INT J HUM-COMPUT ST, V70, P577, DOI 10.1016/j.ijhcs.2012.03.004
   Koelsch S, 2006, HUM BRAIN MAPP, V27, P239, DOI 10.1002/hbm.20180
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kreibig SD, 2007, PSYCHOPHYSIOLOGY, V44, P787, DOI 10.1111/j.1469-8986.2007.00550.x
   Kroupi E, 2014, IEEE INT CON MULTI
   Kroupi E, 2014, BRAIN-COMPUT INTERFA, V1, P85, DOI 10.1080/2326263X.2014.912882
   Kroupi E, 2014, EUR SIGNAL PR CONF, P2135
   Kroupi E, 2013, IEEE ENG MED BIO, P2911, DOI 10.1109/EMBC.2013.6610149
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lanteaume L, 2007, CEREB CORTEX, V17, P1307, DOI 10.1093/cercor/bhl041
   Lécuyer A, 2008, COMPUTER, V41, P66, DOI 10.1109/MC.2008.410
   Lee JS, 2008, IEEE T MULTIMEDIA, V10, P767, DOI 10.1109/TMM.2008.922789
   Lee JS, 2012, IEEE COMMUN MAG, V50, P38, DOI 10.1109/MCOM.2012.6178832
   Lee YH, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108772
   Lin YP, 2010, IEEE T BIO-MED ENG, V57, P1798, DOI 10.1109/TBME.2010.2048568
   Lindemann Lea., 2011, Image Processing (ICIP), 2011 18th IEEE International Conference on, P3109, DOI DOI 10.1109/ICIP.2011.6116324
   Martínez HP, 2013, IEEE COMPUT INTELL M, V8, P20, DOI 10.1109/MCI.2013.2247823
   Menkovski V., 2009, PROCEED INGS 7 INT C, P52
   Mikkelsen KB, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00438
   MONTAGU JD, 1966, PSYCHOL BULL, V65, P261, DOI 10.1037/h0023204
   Moon SE, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P987, DOI 10.1145/2733373.2806382
   Moon SE, 2015, IEEE T AUTON MENT DE, V7, P236, DOI 10.1109/TAMD.2015.2449553
   Mustafa M, 2012, ACM T APPL PERCEPT, V9, DOI 10.1145/2325722.2325725
   Muthukumaraswamy SD, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00138
   OGAWA S, 1990, P NATL ACAD SCI USA, V87, P9868, DOI 10.1073/pnas.87.24.9868
   OGAWA S, 1992, P NATL ACAD SCI USA, V89, P5951, DOI 10.1073/pnas.89.13.5951
   Olivetti BelardinelliM., 2004, Cognitive Processing, V5, P167
   Olofsson JK, 2008, BIOL PSYCHOL, V77, P247, DOI 10.1016/j.biopsycho.2007.11.006
   Park S, 2014, INT J PSYCHOPHYSIOL, V92, P42, DOI 10.1016/j.ijpsycho.2014.02.003
   Patel Salil H, 2005, Int J Med Sci, V2, P147
   Perrin AF, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1007, DOI 10.1145/2733373.2806387
   Polich J, 2007, CLIN NEUROPHYSIOL, V118, P2128, DOI 10.1016/j.clinph.2007.04.019
   Reisenzein R, 2013, EMOT REV, V5, P16, DOI 10.1177/1754073912457228
   Richman LS, 2005, HEALTH PSYCHOL, V24, P422, DOI 10.1037/0278-6133.24.4.422
   Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003
   SAKLOFSKE DH, 1975, PERCEPT MOTOR SKILL, V41, P363, DOI 10.2466/pms.1975.41.2.363
   Sanei S, 2013, EEG Signal Processing
   Savran A., 2006, P ENTERFACE 2006 WOR
   SCHACHTER S, 1962, PSYCHOL REV, V69, P379, DOI 10.1037/h0046234
   Schäfer A, 2013, INT J CARDIOL, V166, P15, DOI 10.1016/j.ijcard.2012.03.119
   Scholler S, 2012, IEEE T IMAGE PROCESS, V21, P2619, DOI 10.1109/TIP.2012.2187672
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Soleymani M, 2008, IEEE INT SYM MULTIM, P228, DOI 10.1109/ISM.2008.14
   Song W, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P267, DOI 10.1145/2647868.2654923
   Sur Shravani, 2009, Ind Psychiatry J, V18, P70, DOI 10.4103/0972-6748.57865
   SUTTON S, 1965, SCIENCE, V150, P1187, DOI 10.1126/science.150.3700.1187
   Ukai K, 2008, DISPLAYS, V29, P106, DOI 10.1016/j.displa.2007.09.004
   van den Broek SP, 1998, ELECTROEN CLIN NEURO, V106, P522, DOI 10.1016/S0013-4694(97)00147-8
   Wang Dan, 2013, Int J Inf Educ Technol, V3, P505, DOI 10.7763/IJIET.2013.V3.326
   Wang DH, 2015, NAT NEUROSCI, V18, P1853, DOI 10.1038/nn.4164
   Wang XY, 2012, NEUROREPORT, V23, P862, DOI 10.1097/WNR.0b013e3283587161
   Winkielman P, 2004, CURR DIR PSYCHOL SCI, V13, P120, DOI 10.1111/j.0963-7214.2004.00288.x
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yazdani A, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133373
   Yazdani J.-S. Lee., 2009, Proc. SIGMM Workshop on Social media, P81
   Zhao CL, 2012, ACCIDENT ANAL PREV, V45, P83, DOI 10.1016/j.aap.2011.11.019
   Zheng WL, 2015, IEEE T AUTON MENT DE, V7, P162, DOI 10.1109/TAMD.2015.2431497
   Zhuang XD, 2014, 2014 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI), P736, DOI 10.1109/BHI.2014.6864469
NR 133
TC 31
Z9 33
U1 0
U2 64
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 340
EP 353
DI 10.1109/TMM.2016.2614880
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tang, YX
   Wang, XF
   Dellandréa, E
   Chen, LM
AF Tang, Yuxing
   Wang, Xiaofang
   Dellandrea, Emmanuel
   Chen, Liming
TI Weakly Supervised Learning of Deformable Part-Based Models for Object
   Detection via Region Proposals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deformable part-based models ( DPMs); object detection; region
   proposals; weakly supervised learning
ID LOCALIZATION; HISTOGRAMS; GRADIENTS
AB The success of deformable part-based models (DPMs) for visual object detection relies on a large number of labeled bounding boxes. With only image-level annotations, our goal is to propose a model enhancing the weakly supervised DPMs by emphasizing the importance of location and size of the initial class- specific root filter. To adaptively select a discriminative set of candidate bounding boxes as this root filter estimate, first, we explore the generic objectness measurement to combine the most salient regions and "good" region proposals. Second, we propose learning of the latent class label of each candidate window as a binary classification problem, by training category- specific classifiers used to coarsely classify a candidate window into either a target object or a nontarget class. Finally, we design a flexible enlarging-and-shrinking postprocessing procedure to modify the DPMs outputs, which can effectively match the approximative object aspect ratios and further improve final accuracy. Extensive experimental results on the challenging PASCAL Visual Object Class 2007 and the Microsoft Common Objects in Context 2014 dataset demonstrate that our proposed framework is effective for initialization of the DPM's root filter. It also shows competitive final localization performance with state-of-the-art weakly supervised object detectionmethods, particularly for the object categories that are relatively salient in the images and deformable in structures.
C1 [Tang, Yuxing; Wang, Xiaofang; Dellandrea, Emmanuel; Chen, Liming] Ecole Cent Lyon, LIRIS, CNRS, UMR 5205, F-69134 Ecully, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Ecole Centrale de Lyon
RP Tang, YX (corresponding author), Ecole Cent Lyon, LIRIS, CNRS, UMR 5205, F-69134 Ecully, France.
EM yuxing.tang@ec-lyon.fr; xiaofang.wang@ec-lyon.fr;
   emmanuel.dellandrea@ec-lyon.fr; liming.chen@ec-lyon.fr
RI Lu, Rui/KCJ-8212-2024; Tang, Yuxing/I-8413-2019
OI Tang, Yuxing/0000-0002-6452-2751
FU French Research Agency, Agence Nationale de Recherche (ANR) [2009 CORD
   026 02, ANR-12-CHRI-0002-04]; Agence Nationale de la Recherche (ANR)
   [ANR-12-CHRI-0002] Funding Source: Agence Nationale de la Recherche
   (ANR)
FX This work was supported by the French Research Agency, Agence Nationale
   de Recherche (ANR) in part through the VideoSense Project under Grant
   2009 CORD 026 02 and in part through the Visen project under Grant
   ANR-12-CHRI-0002-04 within the framework of the ERA-Net CHIST-ERA.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2014, BMVC
   [Anonymous], 2012, European conference on computer vision, DOI 10.1007/978-3-642-33712-3_25
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2014, Advances in Neural Information Processing Systems
   [Anonymous], COMPUT VIS PATTERN R
   [Anonymous], 2014, P BMVC 2014
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Azizpour H, 2012, LECT NOTES COMPUT SC, V7572, P836, DOI 10.1007/978-3-642-33718-5_60
   Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711
   Carreira J, 2012, IEEE T PATTERN ANAL, V34, P1312, DOI 10.1109/TPAMI.2011.231
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cinbis RG, 2014, PROC CVPR IEEE, P2409, DOI 10.1109/CVPR.2014.309
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Foresti GL, 2002, IEEE T MULTIMEDIA, V4, P459, DOI 10.1109/TMM.2002.802024
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R. B., 2011, Advances in Neural Information Processing Systems, P442
   Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hoffman J., 2014, NIPS (News Physiol. Sci.), P3536
   Jia Yangqing, 2014, ARXIV14085093, DOI [10.1145/2647868.2654889, DOI 10.1145/2647868.2654889]
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341
   Nguyen MH, 2009, IEEE I CONF COMP VIS, P1925, DOI 10.1109/ICCV.2009.5459426
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Ren S., P IEEE T PATT AN MAC
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XF, 2013, PROC CVPR IEEE, P3246, DOI 10.1109/CVPR.2013.417
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi ZY, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.78
   Shi Z, 2013, IEEE I CONF COMP VIS, P2984, DOI 10.1109/ICCV.2013.371
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siva P, 2012, LECT NOTES COMPUT SC, V7574, P594, DOI 10.1007/978-3-642-33712-3_43
   Siva P, 2011, IEEE I CONF COMP VIS, P343, DOI 10.1109/ICCV.2011.6126261
   Song HO, 2014, PR MACH LEARN RES, V32, P1611
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tang YX, 2014, IEEE IMAGE PROC, P4072, DOI 10.1109/ICIP.2014.7025827
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang C, 2015, IEEE T IMAGE PROCESS, V24, P1371, DOI 10.1109/TIP.2015.2396361
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
   Zhu YK, 2014, IEEE T MULTIMEDIA, V16, P1585, DOI 10.1109/TMM.2014.2321534
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 59
TC 36
Z9 36
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 393
EP 407
DI 10.1109/TMM.2016.2614862
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, H
   Zheng, QH
   Zhang, WZ
   Du, B
   Li, HF
AF Zhao, Hui
   Zheng, Qinghua
   Zhang, Weizhan
   Du, Biao
   Li, Haifei
TI A Segment-Based Storage and Transcoding Trade-off Strategy for
   Multi-version VoD Systems in the Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud; multi-version video-on-demand (VoD); segment-based; storage and
   transcoding trade-off; transcoding weighted graph
ID VIDEO; ALGORITHMS; NETWORK
AB Multi-version video-on-demand (VoD) providers either store multiple versions of the same video or transcode video to multiple versions in real time to offer multiple-bitrate streaming services to heterogeneous clients. However, this could incur tremendous storage cost or transcoding computation cost. There have been some works regarding trading off between transcoding and storing whole videos, but they did not take into account video segmentation and internal popularity. As a result, they were not cost-efficient. This paper introduces video segmentation and proposes a segment-based storage and transcoding trade-off strategy for multi-version VoD systems in the cloud. First, we split each video into multiple segments depending on the video internal popularity. Second, we describe the transcoding relationships among versions using a transcoding weighted graph, which can be used to calculate the version-aware transcoding cost from one version to another. Third, we take the video segmentation, version-aware transcoding weighted graph, and video internal popularity into account to propose a storage and transcoding trade-off strategy, which stores multiple versions of popular segments and transcodes unpopular segments. We then formulate it as an optimization problem and present a heuristic divide-and-conquer algorithm to get an approximate optimal solution. Finally, we conduct extensive simulations to evaluate the solution; the results show that it can significantly lower the storage and transcoding cost of multi-version VoD systems.
C1 [Zhao, Hui; Zheng, Qinghua; Zhang, Weizhan; Du, Biao] Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian 710049, Peoples R China.
   [Zhao, Hui; Zheng, Qinghua; Zhang, Weizhan; Du, Biao] Xi An Jiao Tong Univ, Dept Comp Sci & Technol, Xian 710049, Peoples R China.
   [Li, Haifei] Union Univ, Dept Comp Sci, Jackson, TN 38305 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Union University
RP Zhao, H (corresponding author), Xi An Jiao Tong Univ, MOEKLINNS Lab, Xian 710049, Peoples R China.; Zhao, H (corresponding author), Xi An Jiao Tong Univ, Dept Comp Sci & Technol, Xian 710049, Peoples R China.
EM huizhao@stu.xjtu.edu.cn; qhzheng@mail.xjtu.edu.cn;
   zhangwzh@mail.xjtu.edu.cn; dubiao@stu.xjtu.edu.cn; hli@uu.edu
FU National Science Foundation of China [61221063, 61472317, 61472315,
   61428206, 91118005, 91218301]; MOE Innovation Research Team [IRT13035];
   National High Technology Research and Development Program 863 of China
   [2012AA011003]; National Key Technologies R&D Program of China
   [2013BAK09B01]; Coordinator Innovation Project for the Key Lab of
   Shaanxi Province [2013SZS05-Z01]; Changjiang Scholars Program of MOE;
   MOE Online Education Research Fund [2016YB165, 2016YB169]; Fundamental
   Research Funds for the Central Universities; China Scholarship Council
FX This work was supported in part by the National Science Foundation of
   China under Grant 61221063, Grant 61472317, Grant 61472315, Grant
   61428206, Grant 91118005, and Grant 91218301, in part by the MOE
   Innovation Research Team No. IRT13035, in part by the National High
   Technology Research and Development Program 863 of China under Grant
   2012AA011003, in part by the National Key Technologies R&D Program of
   China under Grant 2013BAK09B01, in part by the Coordinator Innovation
   Project for the Key Lab of Shaanxi Province under Grant 2013SZS05-Z01,
   in part by the Changjiang Scholars Program of MOE, in part by the MOE
   Online Education Research Fund 2016YB165 and Fund 2016YB169, in part by
   the Fundamental Research Funds for the Central Universities, and in part
   by the China Scholarship Council. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shiwen Mao.
CR [Anonymous], P 2009 C HOT TOP CLO
   [Anonymous], 2008, P 2008 ACM IEEE C SU, DOI DOI 10.1109/SC.2008.5217932
   [Anonymous], 2014, 2014 IEEE INT C MULT, DOI DOI 10.1109/ICME.2014.6890255
   [Anonymous], 2012, MATH PROB ENG
   Balachandran A., 2013, Proceedings of the 2013 conference on Internet measurement conference, P43
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Bzoch P, 2013, 2013 IEEE EUROCON, P679, DOI 10.1109/EUROCON.2013.6625054
   Chang ZY, 2015, IEEE T MULTIMEDIA, V17, P723, DOI 10.1109/TMM.2015.2416636
   Chen SQ, 2005, IEEE MULTIMEDIA, V12, P59, DOI 10.1109/MMUL.2005.56
   Cherkasova L, 2004, IEEE ACM T NETWORK, V12, P781, DOI 10.1109/TNET.2004.836125
   Choi K, 2015, ETRI J, V37, P685, DOI 10.4218/etrij.15.0114.1278
   Ciullo D, 2013, IEEE INFOCOM SER, P2724
   Cong X, 2014, PEER PEER NETW APPL, V7, P175, DOI 10.1007/s12083-012-0193-z
   Finamore A., 2011, ACM IMC, P345, DOI DOI 10.1145/2068816.2068849
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Garcia-Dorado J., IEEE T CLOU IN PRESS
   Guo L, 2008, PODC'08: PROCEEDINGS OF THE 27TH ANNUAL ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P283, DOI 10.1145/1400751.1400789
   He J, 2013, IEEE T CIRC SYST VID, V23, P1717, DOI 10.1109/TCSVT.2013.2255423
   Hsu CH, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/1925101.1925103
   Hsu TH, 2011, EXPERT SYST APPL, V38, P3467, DOI 10.1016/j.eswa.2010.08.134
   Jokhio Fareed, 2013, 2013 39th Euromicro Conference on Software Engineering and Advanced Applications (SEAA), P365, DOI 10.1109/SEAA.2013.17
   Kao CF, 2007, IEEE T MULTIMEDIA, V9, P221, DOI 10.1109/TMM.2006.886259
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Li XL, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1181, DOI 10.1109/ICME.2008.4607651
   MEEYOUNG C, 2009, IEEE ACM T NETWORK, V17, P1357
   Miranda LCO, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1085
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Song M, 2009, ETRI J, V31, P333, DOI 10.4218/etrij.09.0208.0405
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1227, DOI 10.1109/TCSVT.2007.905519
   Woeginger GJ, 2003, LECT NOTES COMPUT SC, V2570, P185
   Wolf T, 2010, IEEE NETWORK, V24, P6, DOI 10.1109/MNET.2010.5510912
   XJTU, 2016, SKYCLASS
   Xu K, 2014, IEEE T NETW SERV MAN, V11, P472, DOI 10.1109/TNSM.2014.2360772
   Yin H, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823750
   Yuan D, 2013, P IEEE INT C E-SCI, P285, DOI 10.1109/eScience.2013.34
   Zhang D, 2015, IEEE T CIRC SYST VID, V25, P300, DOI 10.1109/TCSVT.2014.2352499
   Zhang G, 2015, IEEE T MULTIMEDIA, V17, P229, DOI 10.1109/TMM.2014.2383617
   Zhang JH, 2012, PROCEEDINGS OF 2012 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND NETWORK TECHNOLOGY (ICCSNT 2012), P1, DOI 10.1109/ICCSNT.2012.6525878
   Zhao H, 2015, 2015 IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATION (ISCC), P943, DOI 10.1109/ISCC.2015.7405635
   Zheng Qingji., 2012, CODASPY '12: ACM conference on Data and Application Security and Privacy, P1
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
NR 41
TC 35
Z9 37
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 149
EP 159
DI 10.1109/TMM.2016.2612123
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200012
DA 2024-07-18
ER

PT J
AU Scott, MJ
   Guntuku, SC
   Lin, WS
   Ghinea, G
AF Scott, Michael James
   Guntuku, Sharath Chandra
   Lin, Weisi
   Ghinea, Gheorghita
TI Do Personality and Culture Influence Perceived Video Quality and
   Enjoyment?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Big-5; culture; enjoyment; Hofstede; multimedia; personality;
   perception; quality; quality of experiece (QoE); video
ID MULTIMEDIA; EXPERIENCE; IMPACT; FILMS
AB The interplay between system, context, and human factors is important in perception of multimedia quality. However, studies on human factors are very limited in comparison to those for system and context factors. This article presents an attempt to explore the influence of personality and cultural traits on perception of multimedia quality. As a first step, a database consisting of 144 video sequences from 12 short movie excerpts has been assembled and rated by 114 participants from a cross-cultural population, thereby providing a useful ground-truth for this (as well as future) study. As a second step, three statistical models are compared: (i) a baseline model to only consider system factors; (ii) an extended model to include personality and culture; and (iii) an optimistic model in which each participant is modeled. As a third step, predictive models based on content, affect, system, and human factors are trained to generalize the statistical findings. As shown by statistical analysis, personality and cultural traits represent 9.3% of the variance attributable to human factors, and human factors overall predict an equal or higher proportion of variance compared to system factors. Moreover, the quality-enjoyment correlation varies across the excerpts. Predictive models trained by including human factors demonstrate about 3% and 9% improvement over models trained solely based on system factors for predicting perceived quality and enjoyment. As evidenced by this, human factors indeed are important in perceptual multimedia quality, but the results suggest further investigation of moderation effects and a broader range of human factors is necessary.
C1 [Scott, Michael James] Falmouth Univ, Games Acad, Falmouth TR10 9FE, Cornwall, England.
   [Guntuku, Sharath Chandra; Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
C3 Nanyang Technological University; Brunel University
RP Guntuku, SC (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM michael.scott@falmouth.ac.uk; sharathc001@e.ntu.edu.sg;
   wslin@ntu.edu.sg; george.ghinea@brunel.ac.uk
RI Lin, Weisi/A-3696-2011; Scott, Michael/AAY-3110-2021; Guntuku, Sharath
   Chandra/U-6314-2019; Lin, Weisi/A-8011-2012; Ghinea,
   Gheorghita/AAG-6770-2020; Scott, Michael/D-5833-2014
OI Lin, Weisi/0000-0001-9866-1947; Guntuku, Sharath
   Chandra/0000-0002-2929-0035; Ghinea, Gheorghita/0000-0003-2578-5580;
   Scott, Michael/0000-0002-6803-1490
FU Singapore MoE Tier 1 Project [M4011379, RG141/14]
FX The work of W. Lin was supported by the Singapore MoE Tier 1 Project
   M4011379, RG141/14. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Klara Nahrstedt.
   (Michael James Scott and Sharath Chandra Guntuku contributed equally to
   this work.) (Corresponding author: Sharath Chandra Guntuku.)
CR [Anonymous], 2015, PROC 7 INT WORKSHOP
   [Anonymous], CULTURAL EFFECTS VIS
   [Anonymous], 2011, P IWSDS WORKSH PAR I, DOI DOI 10.1007/978-1-4614-1335-6_19
   [Anonymous], 2010, Tech. Rep.
   [Anonymous], ONLINE READINGS PSYC
   [Anonymous], 2011, INT J ASIAN LANG PRO
   [Anonymous], P IEEE GLOBECOM
   [Anonymous], CULTURE ANAL VERSUS
   [Anonymous], 2011, MM 11
   [Anonymous], 2009, 2009 3 INT C AFF COM, DOI [10.1109/ACII.2009.5349551, DOI 10.1109/ACII.2009.5349551]
   [Anonymous], 2007, MIR
   [Anonymous], PROC ACM INT CONF MU
   [Anonymous], 2010, CULTURES ORG SOFTWAR
   [Anonymous], 2011, P INT C MUSIC INFORM
   [Anonymous], D44 MOBILE3DTV
   [Anonymous], 2014, ACMMM
   Bhattacharya S., 2013, Proc. 21st ACM Internat. Conf. Multimedia (ACM, P361
   Bilandzic H, 2011, COMMUNICATIONS-GER, V36, P29, DOI 10.1515/COMM.2011.002
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Bosker R.J., 2012, MULTILEVEL ANAL INTR
   Carey MJ, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1800, DOI 10.1109/ICSLP.1996.607979
   Cholakkal H, 2016, PROC CVPR IEEE, P5278, DOI 10.1109/CVPR.2016.570
   Cristani M., 2013, P 21 ACM INT C MULT, P213
   Eyben F., 2010, IEEE Netw, V24, P36
   Fiedler M, 2010, IEEE NETWORK, V24, P36, DOI 10.1109/MNET.2010.5430142
   Gao XP, 2007, COLOR RES APPL, V32, P223, DOI 10.1002/col.20321
   Ghinea G, 2006, MULTIMEDIA SYST, V11, P271, DOI 10.1007/s00530-005-0007-8
   Ghinea G, 2003, BRIT J EDUC TECHNOL, V34, P393, DOI 10.1111/1467-8535.00337
   Ghinea G., 1998, Proceedings ACM Multimedia 98, P49, DOI 10.1145/290747.290754
   GOLDBERG LR, 1990, J PERS SOC PSYCHOL, V59, P1216, DOI 10.1037/0022-3514.59.6.1216
   Goldstein E., 2013, SENSATION PERCEPTION
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Gulliver SR, 2006, ACM T MULTIM COMPUT, V2, P241, DOI 10.1145/1201730.1201731
   Guntuku Sharath Chandra, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015. Proceedings: LNCS 8936, P171, DOI 10.1007/978-3-319-14442-9_15
   Guntuku S.C., 2015, Proc. 1st Int. Workshop on Affect Sentiment in Multimedia, P21, DOI 10.1145/2813524.2813528
   Guntuku SC, 2015, INT CONF AFFECT, P236, DOI 10.1109/ACII.2015.7344577
   Hands DS, 2004, IEEE T MULTIMEDIA, V6, P806, DOI 10.1109/TMM.2004.837233
   Hofstede G., 2013, Values survey module 2013 manual
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Hyder M, 2012, COMM COM INF SC, V281, P200
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jumisko-Pyykko S., 2005, 13th Annual ACM International Conference on Multimedia, P535, DOI 10.1145/1101149.1101270
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Manav B, 2007, COLOR RES APPL, V32, P144, DOI 10.1002/col.20294
   Matthews G., 2003, PERSONALITY TRAITS
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Naccari M, 2009, IEEE T MULTIMEDIA, V11, P932, DOI 10.1109/TMM.2009.2021785
   Nisbett RE, 2005, TRENDS COGN SCI, V9, P467, DOI 10.1016/j.tics.2005.08.004
   Paudyal Pradip, 2014, 2014 5 EUR WORKSH VI, P1
   Plutchik R., 1980, EMOTION PSYCHOEVOLUT
   Rainer B, 2012, INT WORK QUAL MULTIM, P278, DOI 10.1109/QoMEX.2012.6263842
   Redi JudithA., 2015, Visual Signal Quality Assessment, P31
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Reiter U, 2014, T-LAB SER TELECOMMUN, P55, DOI 10.1007/978-3-319-02681-7_4
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Schaefer A, 2010, COGNITION EMOTION, V24, P1153, DOI 10.1080/02699930903274322
   Scott MJ, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P481, DOI 10.1145/2733373.2806254
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Siahaan Ernestasia, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P245, DOI 10.1109/QoMEX.2014.6982326
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   TRUNK GV, 1979, IEEE T PATTERN ANAL, V1, P306, DOI 10.1109/TPAMI.1979.4766926
   Visch VT, 2010, COGNITION EMOTION, V24, P1439, DOI 10.1080/02699930903498186
   Winkler S, 2006, IEEE T MULTIMEDIA, V8, P973, DOI 10.1109/TMM.2006.879871
   Xu QQ, 2014, IEEE T MULTIMEDIA, V16, P373, DOI 10.1109/TMM.2013.2292568
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu Y, 2015, COMPUT HUM BEHAV, V49, P412, DOI 10.1016/j.chb.2015.02.054
   Zhu Y, 2015, PROC SPIE, V9394, DOI 10.1117/12.2085002
NR 76
TC 29
Z9 30
U1 5
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1796
EP 1807
DI 10.1109/TMM.2016.2574623
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800010
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, HL
   Kong, DH
   Wang, SF
   Yin, BC
AF Liu, Honglin
   Kong, Dehui
   Wang, Shaofan
   Yin, Baocai
TI Sparse Pose Regression via Componentwise Clustering Feature Point
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Componentwise clustering feature point representation (CCFPR); pose
   estimation; sparse regression
ID BODY POSE; RECONSTRUCTION; TRACKING
AB We propose two-dimensional pose estimation from a single range image of the human body, using sparse regression with a componentwise clustering feature point representation (CCFPR) model. CCFPR includes primary feature points and secondary feature points. The primary feature points consist of the torso center and five extremal points of human body, and further serve to classify all body pixels as the points of six body components. The secondary feature points are given by the cluster centers of each of the five components other than the torso, using K-means cluster. The human pose is obtained by learning a sparse projection matrix, which maps CCFPR to the skeleton points of human body, based on the assumption that each skeleton point be represented by a combination of a few feature points of associated body components. Experimental results on both virtual data and real data show that, under the sparse regression model with a suitably selected cluster number, CCFPR outperforms the random decision forest approach and prediction results of KINECT SENSOR V2.
C1 [Liu, Honglin; Kong, Dehui; Wang, Shaofan] Beijing Univ Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing Key Lab Multimedia & Intelligent Software, Coll Metropolitan Transportat, Beijing 100124, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
C3 Beijing University of Technology; Dalian University of Technology
RP Liu, HL (corresponding author), Beijing Univ Technol, Beijing Adv Innovat Ctr Future Internet Technol, Beijing Key Lab Multimedia & Intelligent Software, Coll Metropolitan Transportat, Beijing 100124, Peoples R China.
EM liuhonglin@emails.bjut.edu.cn; kdh@bjut.edu.cn; wangshaofan@bjut.edu.cn;
   ybc@dlut.edu.cn
RI liu, honglin/HDN-0409-2022
OI liu, honglin/0000-0002-1254-3163
FU Natural Science Foundation of China [61227004, 61370120, 61390510,
   61300065]; Beijing Natural Science Foundation [4162009, 4152009,
   4162010, 4142010]; Beijing Municipal Commission of Education
   [km201410005013, km201510005025]; Jing-Hua Talents Project of the
   Beijing University of Technology; Project for Academic Human Resources
   Development in Institutions of Higher Learning Under the Jurisdiction of
   Beijing Municipality
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61227004, Grant 61370120, Grant 61390510, and Grant
   61300065, in part by the Beijing Natural Science Foundation under Grant
   4162009, Grant 4152009, Grant 4162010, and Grant 4142010, in part by the
   Beijing Municipal Commission of Education under Grant km201410005013 and
   Grant km201510005025, in part by the Jing-Hua Talents Project of the
   Beijing University of Technology, and in part by the Funding Project for
   Academic Human Resources Development in Institutions of Higher Learning
   Under the Jurisdiction of Beijing Municipality. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jing-Ming Guo.
CR [Anonymous], 2010, NOTE GROUP LASSO SPA, DOI [10.1007/978-1-0716-2237-7_18, DOI 10.1007/978-1-0716-2237-7_18]
   [Anonymous], 2012, PROC CVPR IEEE
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.241
   Baak A, 2011, IEEE I CONF COMP VIS, P1092, DOI 10.1109/ICCV.2011.6126356
   Bregler C, 2004, INT J COMPUT VISION, V56, P179, DOI 10.1023/B:VISI.0000011203.00237.9b
   Buys K, 2014, J VIS COMMUN IMAGE R, V25, P39, DOI 10.1016/j.jvcir.2013.03.011
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   Handrich S, 2013, IEEE SYS MAN CYBERN, P906, DOI 10.1109/SMC.2013.159
   Holt B., 2013, P IEEE C WORKSH AUT, P1
   Jiu MY, 2014, PATTERN RECOGN LETT, V50, P122, DOI 10.1016/j.patrec.2013.09.021
   Jung HY, 2015, PROC CVPR IEEE, P2467, DOI 10.1109/CVPR.2015.7298861
   Lallemand J, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P271, DOI 10.1109/3DV.2013.43
   Plagemann C, 2010, IEEE INT CONF ROBOT, P3108, DOI 10.1109/ROBOT.2010.5509559
   Schwarz LA, 2012, IMAGE VISION COMPUT, V30, P217, DOI 10.1016/j.imavis.2011.12.001
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Shum HPH, 2013, IEEE T CYBERNETICS, V43, P1357, DOI 10.1109/TCYB.2013.2275945
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207
   Yang HD, 2007, PATTERN RECOGN, V40, P3120, DOI 10.1016/j.patcog.2007.01.033
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Ye M, 2011, IEEE I CONF COMP VIS, P731, DOI 10.1109/ICCV.2011.6126310
   Youding Zhu, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P267
   Youding Zhu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563163
   Zhang Z, 2013, IEEE T MULTIMEDIA, V15, P106, DOI 10.1109/TMM.2012.2225040
   Zuffi S, 2013, IEEE I CONF COMP VIS, P3312, DOI 10.1109/ICCV.2013.411
NR 28
TC 8
Z9 8
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1233
EP 1244
DI 10.1109/TMM.2016.2556859
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600001
DA 2024-07-18
ER

PT J
AU Hu, YC
   Niu, D
   Li, ZP
AF Hu, Yaochen
   Niu, Di
   Li, Zongpeng
TI A Geometric Approach to Server Selection for Interactive Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE End-to-end delay; geometric optimization; interactive video streaming;
   server selection
ID ALGORITHM
AB Many distributed interactive multimedia applications, such as live video conferencing and video sharing, require each participating client to transmit its captured video stream to other clients via relay servers. We consider connecting multiple clients through multiple relay servers and study the server selection problem from a dense pool of content delivery network edge locations and datacenters to reduce the end-to-end delays between clients. To achieve scalability in the presence of a large number of candidate servers, we formulate server selection as a geometric problem in a delay space instead of in a graph, which turns out to be an extension of the well-known Euclidean k-median problem. We propose practical approximation schemes when using only one or two servers with theoretical worst-case guarantees as well as fast heuristics when using k servers. We demonstrate the benefit of our optimized multiserver selection schemes through extensive evaluation based on real-world traces collected from the PlanetLab and Seattle platforms, containing personal mobile devices as well as real network experiments based on a prototype implementation.
C1 [Hu, Yaochen; Niu, Di] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 1H9, Canada.
   [Li, Zongpeng] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Alberta; University of Calgary
RP Hu, YC; Niu, D (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 1H9, Canada.; Li, ZP (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM yaochen@ualberta.ca; dniu@ualberta.ca; zongpeng@ucalgary.ca
RI Hu, Yaochen/AAQ-6915-2021
CR Arefin A, 2013, IEEE MULTIMEDIA, V20, P38, DOI 10.1109/MMUL.2012.59
   Baset S., 2006, IEEE INT C COMPUTER, P1, DOI DOI 10.1109/INFOCOM.2006.312
   BONFIGLIO D., 2008, INFOCOM, P261, DOI [DOI 10.1109/INF0C0M.2008.61, 10.1109/INFOCOM.2008.61, DOI 10.1109/INFOCOM.2008.61]
   Cameron C. W., 2002, Performance Evaluation Review, V30, P152, DOI 10.1145/511399.511354
   Cappos Justin, 2009, SIGCSE Bulletin, V41, P111, DOI 10.1145/1539024.1508905
   CHANDRASEKARAN R, 1989, MATH PROGRAM, V44, P293, DOI 10.1007/BF01587094
   Chen X., 2011, 19 INT C ACM MULT SC
   Dabek F, 2004, ACM SIGCOMM COMP COM, V34, P15, DOI 10.1145/1030194.1015471
   De Cicco L, 2011, COMPUT NETW, V55, P558, DOI 10.1016/j.comnet.2010.09.010
   Ding C., 2012, PROC IEEE 20 INT WOR, P1
   Donnet B, 2010, IEEE COMMUN SURV TUT, V12, P488, DOI 10.1109/SURV.2010.032810.00007
   Ledlie J., 2007, C NETW SYST DES IMPL
   Lee KW, 2005, COMPUT NETW, V49, P84, DOI 10.1016/j.comnet.2005.04.006
   Liang C, 2011, IEEE ACM T NETWORK, V19, P1704, DOI 10.1109/TNET.2011.2141680
   Liao J., 2012, P IEEE INT C AC SPEE, P2321
   Lu Y, 2010, LECT NOTES COMPUT SC, V6091, P96, DOI 10.1007/978-3-642-12963-6_8
   Qiu LL, 2001, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2001.916655
   Rhea S., 2004, USENIX ANN TECH C BO
   ROSEN JB, 1992, OPER RES, V40, P188, DOI 10.1287/opre.40.1.188
   Si XB, 2012, IEEE INT WORKS INFOR, P1, DOI 10.1109/WIFS.2012.6412616
   Singh K., 2001, INT TEL WORKSH NEW Y
   Vazirani V.V., 2001, Approximation algorithms, V1
   Xu Y., 2012, P 2012 INTERNET MEAS, P371, DOI DOI 10.1145/2398776.2398816
   Yu CG, 2014, IEEE INFOCOM SER, P1456, DOI 10.1109/INFOCOM.2014.6848080
   Zhang XG, 2012, IEEE INFOCOM SER, P621, DOI 10.1109/INFCOM.2012.6195805
   Zheng HY, 2013, INT CON DISTR COMP S, P500, DOI 10.1109/ICDCS.2013.44
NR 26
TC 14
Z9 15
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 840
EP 851
DI 10.1109/TMM.2016.2538721
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200005
DA 2024-07-18
ER

PT J
AU Usman, M
   He, XJ
   Lam, KM
   Xu, M
   Bokhari, SMM
   Chen, JJ
AF Usman, Muhammad
   He, Xiangjian
   Lam, Kin-Man
   Xu, Min
   Bokhari, Syed Mohsin Matloob
   Chen, Jinjun
TI Frame Interpolation for Cloud-Based Mobile Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block Matching Algorithm (BMA); Error Concealment (EC); H.265/HEVC; High
   Definition (HD); Intraframe; Motion Vectors (MVs); packet loss rate;
   packet retransmission; Quality of Experience (QoE); Quantization
   Parameters (QPs)
ID TEMPORAL ERROR CONCEALMENT; MOTION VECTOR RECOVERY; TRANSMISSION; QOE
AB Cloud-based High Definition (HD) video streaming is becoming popular day by day. On one hand, it is important for both end users and large storage servers to store their huge amount of data at different locations and servers. On the other hand, it is becoming a big challenge for network service providers to provide reliable connectivity to the network users. There have been many studies over cloud-based video streaming for Quality of Experience (QoE) for services like YouTube. Packet losses and bit errors are very common in transmission networks, which affect the user feedback over cloud-based media services. To cover up packet losses and bit errors, Error Concealment (EC) techniques are usually applied at the decoder/receiver side to estimate the lost information. This paper proposes a time-efficient and quality-oriented EC method. The proposed method considers H.265/HEVC based intra-encoded videos for the estimation of whole intra-frame loss. The main emphasis in the proposed approach is the recovery of Motion Vectors (MVs) of a lost frame in real-time. To boost-up the search process for the lost MVs, a bigger block size and searching in parallel are both considered. The simulation results clearly show that our proposed method outperforms the traditional Block Matching Algorithm (BMA) by approximately 2.5 dB and Frame Copy (FC) by up to 12 dB at a packet loss rate of 1%, 3%, and 5% with different Quantization Parameters (QPs). The computational time of the proposed approach outperforms the BMA by approximately 1788 seconds.
C1 [Usman, Muhammad; He, Xiangjian; Xu, Min; Chen, Jinjun] Univ Technol Sydney, Sch Comp & Commun, Global Big Data Technol Ctr, Sydney, NSW 2007, Australia.
   [Bokhari, Syed Mohsin Matloob] Univ Engn & Technol, Dept Elect Engn, Peshawar 25120, Pakistan.
   [Lam, Kin-Man] Hong Kong Polytech Univ, Elect & Informat Engn Dept, Hong Kong, Hong Kong, Peoples R China.
C3 University of Technology Sydney; University of Engineering & Technology
   Peshawar; Hong Kong Polytechnic University
RP He, XJ (corresponding author), Univ Technol Sydney, Sch Comp & Commun, Global Big Data Technol Ctr, Sydney, NSW 2007, Australia.
EM Muhammad.Usman-2@student.uts.edu.au; Xiangjian.He@uts.edu.au;
   enkmlam@polyu.edu.hk; Min.Xu@uts.edu.au;
   Mohsin.Bokhari@uetpeshawar.edu.pk; Jinjun.Chen@uts.edu.au
RI usman, muhammad/KMX-2101-2024; He, Xiangjian/CAA-1461-2022; Chen,
   Jinjun/AAP-2361-2020
OI Chen, Jinjun/0000-0003-1677-9525; /0000-0003-2165-4575; Xu,
   Min/0000-0001-9581-8849; He, Xiangjian/0000-0001-8962-540X
CR Alasaad A, 2015, IEEE T PARALL DISTR, V26, P1021, DOI 10.1109/TPDS.2014.2316827
   [Anonymous], 2015, CISC VIS NETW IND FO
   Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672
   Asheri H, 2012, IEEE T CONSUM ELECTR, V58, P880, DOI 10.1109/TCE.2012.6311331
   Brent R., 2010, MODERN COMPUTER ARIT
   Carle G, 1997, IEEE NETWORK, V11, P24, DOI 10.1109/65.642357
   Choi WI, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P371
   Hadizadeh H, 2013, IEEE T MULTIMEDIA, V15, P2099, DOI 10.1109/TMM.2013.2281024
   He J, 2014, IEEE T CIRC SYST VID, V24, P669, DOI 10.1109/TCSVT.2013.2283430
   He J, 2013, IEEE T CIRC SYST VID, V23, P1717, DOI 10.1109/TCSVT.2013.2255423
   Hoque MM, 2014, ELECTRON LETT, V50, P996, DOI 10.1049/el.2014.1055
   Hossfeld T, 2012, IEEE COMMUN MAG, V50, P28, DOI 10.1109/MCOM.2012.6178831
   Koloda J, 2014, IEEE T MULTIMEDIA, V16, P1729, DOI 10.1109/TMM.2014.2330314
   Kumwilaisak W, 2011, J VIS COMMUN IMAGE R, V22, P164, DOI 10.1016/j.jvcir.2010.12.002
   Kung WY, 2006, IEEE T CIRC SYST VID, V16, P789, DOI 10.1109/TCSVT.2006.877391
   Lie WN, 2014, IEEE T MULTIMEDIA, V16, P216, DOI 10.1109/TMM.2013.2281587
   Lin YD, 2015, IEEE SYST J, V9, P393, DOI 10.1109/JSYST.2013.2289556
   Liu XG, 2014, J SUPERCOMPUT, V70, P1180, DOI 10.1007/s11227-014-1167-0
   Metkar S., 2013, SPRINGER BRIEFS APPL
   Nightingale J, 2014, IEEE T CONSUM ELECTR, V60, P242, DOI 10.1109/TCE.2014.6852000
   Smolka B, 2015, J REAL-TIME IMAGE PR, V10, P289, DOI 10.1007/s11554-012-0307-0
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tsai WJ, 2010, IEEE T CIRC SYST VID, V20, P1822, DOI 10.1109/TCSVT.2010.2087816
   Usman M, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P233, DOI 10.1109/PCS.2015.7170081
   Wang L, 2012, IEEE T PARALL DISTR, V23, P296, DOI 10.1109/TPDS.2011.144
   Wenger S., 2012, JCTVH0072
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yang M, 2014, IEEE T BROADCAST, V60, P385, DOI 10.1109/TBC.2014.2321676
   Yang SH, 2015, MULTIMED TOOLS APPL, V74, P10785, DOI 10.1007/s11042-014-2206-9
   Zhang XY, 2014, IEEE T PARALL DISTR, V25, P363, DOI 10.1109/TPDS.2013.48
   Zhang YB, 2012, IEEE T CIRC SYST VID, V22, P12, DOI 10.1109/TCSVT.2011.2130450
   Zhou J, 2011, IEEE T BROADCAST, V57, P75, DOI 10.1109/TBC.2010.2086771
   Zhou Y, 2015, IEEE SENS J, V15, P1892, DOI 10.1109/JSEN.2014.2366511
   Zhu J, 2012, IEEE T CIRC SYST VID, V22, P855, DOI 10.1109/TCSVT.2011.2179453
   Zhu LL, 2011, J SUPERCOMPUT, V58, P96, DOI 10.1007/s11227-010-0535-7
NR 35
TC 32
Z9 32
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 831
EP 839
DI 10.1109/TMM.2016.2537200
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200004
OA Green Published
DA 2024-07-18
ER

PT J
AU Van, LP
   De Praeter, J
   Van Wallendael, G
   Van Leuven, S
   De Cock, J
   Van de Walle, R
AF Luong Pham Van
   De Praeter, Johan
   Van Wallendael, Glenn
   Van Leuven, Sebastiaan
   De Cock, Jan
   Van de Walle, Rik
TI Efficient Bit Rate Transcoding for High Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate adaptation; complexity scalable transcoding; high efficiency
   video coding (HEVC); machine learning
ID CU SIZE DECISION; MODE REFINEMENT ALGORITHM; HEVC; COMPLEXITY; H.264/AVC
AB High efficiency video coding (HEVC) shows a significant advance in compression efficiency and is considered to be the successor of H.264/AVC. To incorporate the HEVC standard into real-life network applications and a diversity of other applications, efficient bit rate adaptation (transrating) algorithms are required. A current problem of transrating for HEVC is the high computational complexity associated with the encoder part of such a cascaded pixel domain transcoder. This paper focuses on deriving an optimal strategy for reducing the transcoding complexity with a complexity-scalable scheme. We propose different transcoding techniques which are able to reduce the transcoding complexity in both CU and PU optimization levels. At the CU level, CUs can be evaluated in top-to-bottom or bottom-to-top flows, in which the coding information of the input video stream is utilized to reduce the number of evaluations or to early terminate certain evaluations. At the PU level, the PU candidates are adaptively selected based on the probability of PU sizes and the co-located input PU partitioning. Moreover, with the use of different proposed methods, a complexity-scalable transrating scheme can be achieved. Furthermore, the transcoding complexity can be effectively controlled by the machine learning based approach. Simulations show that the proposed techniques provide a superior transcoding performance compared to the state-of-the-art related works. Additionally, the proposed methods can achieve a range of trade-offs between transrating complexity and coding performance. From the proposed schemes, the fastest approach is able to reduce the complexity by 82% while keeping the bitrate loss below 3%.
C1 [Luong Pham Van; De Praeter, Johan; Van Wallendael, Glenn; Van Leuven, Sebastiaan; De Cock, Jan; Van de Walle, Rik] Ghent Univ iMinds, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
C3 Ghent University; IMEC
RP Van, LP; De Praeter, J; Van Wallendael, G; Van Leuven, S; De Cock, J; Van de Walle, R (corresponding author), Ghent Univ iMinds, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
EM luong.phamvan@ugent.be; johan.depraeter@ugent.be;
   glenn.vanwallendael@ugent.be; sebasti-aan.vanleuven@ugent.be;
   jan.decock@ugent.be; rik.vandewalle@ugent.be
RI Van Wallendael, Glenn/H-8315-2015
OI Van Wallendael, Glenn/0000-0001-9530-3466; De Praeter,
   Johan/0000-0003-4687-2659
FU Ghent University-iMinds, Agency for Innovation by Science and Technology
   (IWT); Fund for Scientific Research-Flanders (FWO-Flanders); European
   Union
FX This work was supported by Ghent University-iMinds, Agency for
   Innovation by Science and Technology (IWT) Ph.D. and post-doctoral
   fellow grants, the Fund for Scientific Research-Flanders (FWO-Flanders),
   and the European Union. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Yap-Peng Tan.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   [Anonymous], 2012, JCTVCI1100
   [Anonymous], 2001, VCEGM33 ITUT
   [Anonymous], 2012, JTC1SC29WG11
   [Anonymous], 2012, JCTVC1002
   [Anonymous], 2013, 230082 ISOIECHEVC
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   De Cock J, 2010, SIGNAL PROCESS-IMAGE, V25, P235, DOI 10.1016/j.image.2010.01.006
   De Praeter J, 2014, IEEE IMAGE PROC, P2487, DOI 10.1109/ICIP.2014.7025503
   Deknudt C, 2010, IEEE T CONSUM ELECTR, V56, P2430, DOI 10.1109/TCE.2010.5681124
   Eleftheriadis A, 2006, IEEE T MULTIMEDIA, V8, P297, DOI 10.1109/TMM.2005.864346
   Hait N, 2009, IEEE T CIRC SYST VID, V19, P1129, DOI 10.1109/TCSVT.2009.2020334
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Lee J, 2015, IEEE T CIRC SYST VID, V25, P411, DOI 10.1109/TCSVT.2014.2339612
   Lefol D, 2006, IEEE T CONSUM ELECTR, V52, P215
   Lefol D, 2006, IEEE INT SYMP CIRC S, P4459
   Lefol D, 2006, IEEE IMAGE PROC, P845, DOI 10.1109/ICIP.2006.312534
   Van LP, 2013, IEEE IMAGE PROC, P1573, DOI 10.1109/ICIP.2013.6738324
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Peixoto E, 2014, IEEE T CIRC SYST VID, V24, P99, DOI 10.1109/TCSVT.2013.2273651
   Quinlan J.R.C., 1993, C4 5 PROGRAMS MACHIN
   Seo KD, 2000, IEEE T CONSUM ELECTR, V46, P1128, DOI 10.1109/30.920469
   Shanableh T, 2013, IEEE T CIRC SYST VID, V23, P1191, DOI 10.1109/TCSVT.2013.2241352
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2014, IEEE T IMAGE PROCESS, V23, P4232, DOI 10.1109/TIP.2014.2341927
   Shen LQ, 2013, IEEE T CONSUM ELECTR, V59, P207, DOI 10.1109/TCE.2013.6490261
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen XL, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-4
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Van Wallendael G, 2012, IEEE IMAGE PROC, P733, DOI 10.1109/ICIP.2012.6466964
   Witten I. H., 2005, DATA MINING PRACTICA
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
NR 35
TC 31
Z9 31
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 364
EP 378
DI 10.1109/TMM.2015.2512231
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600005
OA Green Published
DA 2024-07-18
ER

PT J
AU Song, JK
   Shen, HT
   Wang, JF
   Huang, Z
   Sebe, N
   Wang, JD
AF Song, Jingkuan
   Shen, Heng Tao
   Wang, Jianfeng
   Huang, Zi
   Sebe, Nicu
   Wang, Jingdong
TI A Distance-Computation-Free Search Scheme for Binary Code Databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes indexing; Hamming distance search
ID QUANTIZATION
AB Recently, binary codes have been widely used in many multimedia applications to approximate high-dimensional multimedia features for practical similarity search due to the highly compact data representation and efficient distance computation. While the majority of the hashing methods aim at learning more accurate hash codes, only a few of them focus on indexing methods to accelerate the search for binary code databases. Among these indexing methods, most of them suffer from extremely high memory cost or extensive Hamming distance computations. In this paper, we propose a new Hamming distance search scheme for large scale binary code databases, which is free of Hamming distance computations to return the exact results. Without the necessity to compare database binary codes with queries, the search performance can be improved and databases can be externally maintained. More specifically, we adopt the inverted multi-index data structure to index binary codes. Importantly, the Hamming distance information embedded in the structure is utilized in the designed search scheme such that the verification of exact results no longer relies on Hamming distance computations. As a step further, we optimize the performance of the inverted multi-index structure by taking the code distributions among different bits into account for index construction. Empirical results on large-scale binary code databases demonstrate the superiority of our method over existing approaches in terms of both memory usage and search efficiency.
C1 [Song, Jingkuan; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-4067 Trento, Italy.
   [Shen, Heng Tao; Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Wang, Jianfeng] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
   [Wang, Jingdong] Microsoft Res Asia, Media Comp Grp, Beijing 100190, Peoples R China.
C3 University of Trento; University of Queensland; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Microsoft
   Research Asia; Microsoft
RP Song, JK; Sebe, N (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-4067 Trento, Italy.; Shen, HT; Huang, Z (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.; Wang, JF (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.; Wang, JD (corresponding author), Microsoft Res Asia, Media Comp Grp, Beijing 100190, Peoples R China.
EM jingkuan.song@unitn.it; shenht@itee.uq.edu.au; wjf2006@mail.ustc.edu.cn;
   huang@itee.uq.edu.au; sebe@disi.unitn.it; jingdw@microsoft.com
RI Wang, Jingdong/E-9920-2017; Sebe, Niculae/KEC-2000-2024; Shen, Heng
   Tao/ABD-5331-2021
OI Wang, Jingdong/0000-0002-4888-4445; Sebe, Niculae/0000-0002-6597-7248;
   HUANG, ZI/0000-0002-9738-4949
CR [Anonymous], DIAGN THER
   [Anonymous], 2008, Proceedings of the 21st International Conference on Neural Information Processing Systems
   [Anonymous], ACM MULTIMEDIA
   Arietta SM, 2011, IEEE COMPUT GRAPH, V31, P9, DOI 10.1109/MCG.2010.105
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Charikar Moses S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965
   Duan LY, 2015, IEEE T MULTIMEDIA, V17, P828, DOI 10.1109/TMM.2015.2419973
   Flum J., 2006, TEXT THEORET COMP S
   Gao LL, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P903, DOI 10.1145/2733373.2806360
   Gao LL, 2015, PROC CVPR IEEE, P4371, DOI 10.1109/CVPR.2015.7299066
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Haitsma J., 2011, U. S. Patent, Patent No. [7,921,296, 7921296]
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hinton G., 2007, INT J APPROX REASON, V50, P969
   Jégou H, 2011, INT CONF ACOUST SPEE, P861
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Leutenegger S, 2011, IEEE I CONF COMP VIS, P2548, DOI 10.1109/ICCV.2011.6126542
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu AX, 2011, PROC INT CONF DATA, P553, DOI 10.1109/ICDE.2011.5767831
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   Ma YP, 2015, KSII T INTERNET INF, V9, P2599, DOI 10.3837/tiis.2015.07.015
   Malekesmaeili M., 2012, IEEE T PATTERN ANAL, V34, P2481
   Manku GS, 2007, P 16 INT C WORLD WID, P141, DOI DOI 10.1145/1242572.1242592
   Norouzi M.E., 2011, ICML
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Plateau M., 2005, P ROADEF, P14
   Shen FM, 2015, IEEE I CONF COMP VIS, P4148, DOI 10.1109/ICCV.2015.472
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang M., 2015, Edbt, P361
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhang YD, 2014, IEEE T MULTIMEDIA, V16, P1127, DOI 10.1109/TMM.2014.2306392
   Zhou WG, 2014, IEEE T MULTIMEDIA, V16, P601, DOI 10.1109/TMM.2014.2301979
   Zou FH, 2015, IEEE T MULTIMEDIA, V17, P1006, DOI 10.1109/TMM.2015.2425651
NR 42
TC 44
Z9 44
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 484
EP 495
DI 10.1109/TMM.2016.2515990
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600014
DA 2024-07-18
ER

PT J
AU Wang, Z
   Hu, RM
   Liang, C
   Yu, Y
   Jiang, JJ
   Ye, M
   Chen, J
   Leng, QM
AF Wang, Zheng
   Hu, Ruimin
   Liang, Chao
   Yu, Yi
   Jiang, Junjun
   Ye, Mang
   Chen, Jun
   Leng, Qingming
TI Zero-Shot Person Re-identification via Cross-View Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-view consistency; data-driven distance metric; person
   re-identification (re-id)
ID IDENTIFICATION; TRACKING
AB Person re-identification, aiming to identify images of the same person from various cameras configured in different places, has attracted much attention in the multimedia retrieval community. In this problem, choosing a proper distance metric is a crucial aspect, and many classic methods utilize a uniform learnt metric. However, their performance is limited due to ignoring the zero-shot and fine-grained characteristics presented in real person re-identification applications. In this paper, we investigate two consistencies across two cameras, which are cross-view support consistency and cross-view projection consistency. The philosophy behind it is that, in spite of visual changes in two images of the same person under two camera views, the support sets in their respective views are highly consistent, and after being projected to the same view, their context sets are also highly consistent. Based on the above phenomena, we propose a data-driven distance metric (DDDM) method, re-exploiting the training data to adjust the metric for each query-gallery pair. Experiments conducted on three public data sets have validated the effectiveness of the proposed method, with a significant improvement over three baseline metric learning methods. In particular, on the public VIPeR dataset, the proposed method achieves an accuracy rate of 42.09% at rank-1, which outperforms the state-of-the-art methods by 4.29%.
C1 [Wang, Zheng; Hu, Ruimin; Liang, Chao; Ye, Mang; Chen, Jun] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
   [Wang, Zheng; Hu, Ruimin; Liang, Chao; Ye, Mang; Chen, Jun] Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Peoples R China.
   [Yu, Yi] Natl Inst Informat, Tokyo 1018430, Japan.
   [Jiang, Junjun] China Univ Geosci, Sch Comp, Wuhan 430074, Peoples R China.
   [Leng, Qingming] Jiujiang Univ, Sch Informat Sci & Technol, Jiujiang 332005, Peoples R China.
C3 Wuhan University; Research Organization of Information & Systems (ROIS);
   National Institute of Informatics (NII) - Japan; China University of
   Geosciences; Jiujiang University
RP Wang, Z; Hu, RM; Liang, C; Ye, M; Chen, J (corresponding author), Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.; Wang, Z; Hu, RM; Liang, C; Ye, M; Chen, J (corresponding author), Collaborat Innovat Ctr Geospatial Technol, Wuhan 430079, Peoples R China.; Yu, Y (corresponding author), Natl Inst Informat, Tokyo 1018430, Japan.; Jiang, JJ (corresponding author), China Univ Geosci, Sch Comp, Wuhan 430074, Peoples R China.; Leng, QM (corresponding author), Jiujiang Univ, Sch Informat Sci & Technol, Jiujiang 332005, Peoples R China.
EM wangzwhu@whu.edu.cn; hurm1964@gmail.com; cliang@whu.edu.cn;
   yi.yu.yy@gmail.com; junjun0595@163.com; yemang@whu.edu.cn;
   chenj@whu.edu.cn; lengqingming@126.com
RI Wang, Zheng/AAQ-8628-2020; Chen, Jun/AAD-8167-2022; Wang,
   Zheng/ABC-6029-2020; Ye, Mang/HCJ-0591-2022; Jiang, Junjun/L-7087-2019
OI Wang, Zheng/0000-0003-3846-9157; Wang, Zheng/0000-0003-3846-9157; Ye,
   Mang/0000-0003-3989-7655; Jiang, Junjun/0000-0002-5694-505X
FU National Nature Science Foundation of China [61231015, 61172173,
   61303114, 61170023, 61501413, 61562048]; National High Technology
   Research and Development Program of China [2015AA016306]; Internet of
   Things Development Funding Project of Ministry of Industry [25];
   Technology Research Project of Ministry of Public Security
   [2014JSYJA016]; Major Science and Technology Innovation Plan of Hubei
   Province [2013AAA020]; Nature Science Foundation of Hubei Province
   [2014CFB712]; Nature Science Foundation of Jiangxi Province
   [20151BAB217013]; Fundamental Research Funds for the Central
   Universities [2042014kf0250]
FX This work was supported by the National Nature Science Foundation of
   China under Grant 61231015, Grant 61172173, Grant 61303114, Grant
   61170023, Grant 61501413, and Grant 61562048, by the National High
   Technology Research and Development Program of China under Grant
   2015AA016306, by the Internet of Things Development Funding Project of
   Ministry of Industry in 2013 under Grant 25, by the Technology Research
   Project of Ministry of Public Security under Grant 2014JSYJA016, by the
   Major Science and Technology Innovation Plan of Hubei Province under
   Grant 2013AAA020, by the Nature Science Foundation of Hubei Province
   under Grant 2014CFB712, by the Nature Science Foundation of Jiangxi
   Province under Grant 20151BAB217013, and by the Fundamental Research
   Funds for the Central Universities under Grant 2042014kf0250. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. K. Selcuk Candan.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   [Anonymous], 2013, P IEEE INT MULT EXP
   [Anonymous], 2009, Proceedings of the 26th Annual International Conference on Machine Learning
   [Anonymous], 2009, Advances in neural information processing systems
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2012, BRIT MACH VIS C
   [Anonymous], 2010, P BRIT MACH VIS C
   [Anonymous], P IEEE INT WORKSH PE
   [Anonymous], 2010, Asian Conference on Computer Vision
   Ben Shitrit H, 2011, IEEE I CONF COMP VIS, P137, DOI 10.1109/ICCV.2011.6126235
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gong S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hirzer M, 2012, IEEE IMAGE PROC, P1617, DOI 10.1109/ICIP.2012.6467185
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Jiang JJ, 2015, IEEE PHOTONICS J, V7
   Jiang JJ, 2014, IEEE T IMAGE PROCESS, V23, P4220, DOI 10.1109/TIP.2014.2347201
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Kviatkovsky Igor, 2013, IEEE Trans Pattern Anal Mach Intell, V35, P1622, DOI 10.1109/TPAMI.2012.246
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu J., 2013, SLEP: Sparse Learning with Efficient Projections
   Lo Presti L, 2012, IEEE T MULTIMEDIA, V14, P346, DOI 10.1109/TMM.2011.2173323
   Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pedagadi S, 2013, PROC CVPR IEEE, P3318, DOI 10.1109/CVPR.2013.426
   Tao DP, 2013, IEEE T CIRC SYST VID, V23, P1675, DOI 10.1109/TCSVT.2013.2255413
   Vezzani R, 2013, ACM COMPUT SURV, V46, DOI 10.1145/2543581.2543596
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang YM, 2014, IEEE T CIRC SYST VID, V24, P1350, DOI 10.1109/TCSVT.2014.2305519
   Wang Z., 2014, PROC PACIFICRIM C MU, P1
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
NR 48
TC 128
Z9 131
U1 1
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 260
EP 272
DI 10.1109/TMM.2015.2505083
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400010
DA 2024-07-18
ER

PT J
AU Cho, K
   Courville, A
   Bengio, Y
AF Cho, Kyunghyun
   Courville, Aaron
   Bengio, Yoshua
TI Describing Multimedia Content Using Attention-Based Encoder-Decoder
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; deep learning; recurrent neural networks
AB Whereas deep neural networks were first mostly used for classification tasks, they are rapidly expanding in the realm of structured output problems, where the observed target is composed of multiple random variables that have a rich joint distribution, given the input. In this paper we focus on the case where the input also has a rich structure and the input and output structures are somehow related. We describe systems that learn to attend to different places in the input, for each element of the output, for a variety of tasks: machine translation, image caption generation, video clip description, and speech recognition. All these systems are based on a shared set of building blocks: gated recurrent neural networks and convolutional neural networks, along with trained attention mechanisms. We report on experimental results with these systems, showing impressively good performance and the advantage of the attention mechanism.
C1 [Cho, Kyunghyun; Courville, Aaron; Bengio, Yoshua] Univ Montreal, Informat & Operat Res Dept, Montreal, PQ H3T 1J4, Canada.
   [Cho, Kyunghyun] NYU, Dept Comp Sci, New York, NY 10012 USA.
C3 Universite de Montreal; New York University
RP Cho, K (corresponding author), Univ Montreal, Informat & Operat Res Dept, Montreal, PQ H3T 1J4, Canada.
EM kyunghyun.cho@nyu.edu; aaron.courville@umontreal.ca;
   yoshua.bengio@umontreal.ca
FU NSERC; FRQNT; Calcul Quebec; Compute Canada; Canada Research Chairs;
   CIFAR; Samsung
FX This work was supported by NSERC, FRQNT, Calcul Quebec, Compute Canada,
   the Canada Research Chairs, CIFAR, and Samsung. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR [Anonymous], CORR
   [Anonymous], 2014, CORR
   [Anonymous], 2015, P ICLR
   [Anonymous], 2014, CORR
   [Anonymous], 2015, Show, attend and tell: neural image caption generation with visual attention. Paper presented at: International Conference on Machine Learning
   [Anonymous], 2013, EMNLP
   [Anonymous], 2015, 3 INT C LEARN REPR I
   [Anonymous], 2014, 2 INT C LEARN REPR I
   [Anonymous], 2014, ARXIV14105401
   [Anonymous], 2014, CORR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, CoRR
   [Anonymous], CORR
   [Anonymous], 2014, P NIPS DEEP LEARN WO
   [Anonymous], P EACL WORKSH STAT M
   [Anonymous], 2014, P ICLR
   [Anonymous], P ICLR
   [Anonymous], 2014, P ICML
   [Anonymous], 2015, CoRR
   [Anonymous], 2014, CoRR
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2013, CORR
   Boulanger-Lewandowski N., 2013, ISMIR, P335, DOI 10.5281/zenodo.1418319
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Cho K., 2014, P 8 WORKSH SYNT SEM, P103
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chorowski J, 2015, ADV NEUR IN, V28
   Denil M., 2014, Extraction of salient sentences from labelled documents
   Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312
   Donahue Jeff., 2014, CoRR
   Dulac-Arnold G., 2014, P INT C LEARN REPR
   Fang H., 2014, CoRR
   Garofolo J. S., 1993, 4930 NASA STI NISTIR, V93
   Graves A., 2013, Generating sequences with recurrent neural networks
   Graves A., 2012, P ICML WORKSH REPR L
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Gregor K., 2015, CORR
   Gulcehre C., 2015, CoRR
   Heafield, 2014, P 9 WORKSH STAT MACH, P97
   Hinton G. E., 1986, Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations, V1, P2
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hochreiter S., 2000, Field Guide to Dynamical Recurrent Networks
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Larochelle Hugo, 2010, ADV NEURAL INFORM PR, V23
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Mnih A., 2007, P 24 INT C MACH LEAR, P641, DOI [DOI 10.1145/1273496.1273577, 10.1145/1273496.1273577]
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Raiko T., 2015, P ICLR
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sutskever I., 2014, 28 C NEUR INF PROC S
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Yichuan, 2013, P NIPS
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Toth Laszlo, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P190, DOI 10.1109/ICASSP.2014.6853584
   Vedantam R., 2014, ARXIV14115726
   Venugopalan S., 2014, CoRR, Vabs/1412.4729
   Vinyals O., 2014, CORR
   Vinyals O., CORR IN PRESS
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575
   Yao L., 2015, P INT C COM IN PRESS
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zheng Y, 2015, INT J COMPUT VISION, V113, P67, DOI 10.1007/s11263-014-0765-x
NR 75
TC 267
Z9 296
U1 6
U2 96
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1875
EP 1886
DI 10.1109/TMM.2015.2477044
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400002
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Ma, B
   Shen, JB
   Liu, YB
   Hu, HW
   Shao, L
   Li, XL
AF Ma, Bo
   Shen, Jianbing
   Liu, Yangbiao
   Hu, Hongwei
   Shao, Ling
   Li, Xuelong
TI Visual Tracking Using Strong Classifier and Structural Local Sparse
   Descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Local descriptor; sparse representation; strong classifier; visual
   tracking
ID OBJECT TRACKING; ROBUST TRACKING
AB Sparse coding methods have achieved great success in visual tracking, and we present a strong classifier and structural local sparse descriptors for robust visual tracking. Since the summary features considering the sparse codes are sensitive to occlusion and other interfering factors, we extract local sparse descriptors from a fraction of all patches by performing a pooling operation. The collection of local sparse descriptors is combined into a boosting-based strong classifier for robust visual tracking using a discriminative appearance model. Furthermore, a structural reconstruction error based weight computation method is proposed to adjust the classification score of each candidate for more precise tracking results. To handle appearance changes during tracking, we present an occlusion-aware template update scheme. Comprehensive experimental comparisons with the state-of-the-art algorithms demonstrated the better performance of the proposed method.
C1 [Ma, Bo; Shen, Jianbing; Liu, Yangbiao; Hu, Hongwei] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
   [Shao, Ling] Northumbria Univ, Dept Comp Sci & Digital Technol, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Ctr Opti Imagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
C3 Beijing Institute of Technology; Northumbria University; Chinese Academy
   of Sciences; Xi'an Institute of Optics & Precision Mechanics, CAS; State
   Key Laboratory of Transient Optics & Photonics
RP Ma, B (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing Lab Intelligent Informat Technol, Beijing 100081, Peoples R China.
EM bma000@bit.edu.cn; shenjianbing@bit.edu.cn; ling.shao@ieee.org;
   xuelong_li@opt.ac.cn
RI Shao, Ling/D-3535-2011; Li, Xuelong/Z-3785-2019; li,
   xiang/GWM-6319-2022; Shen, Jianbing/U-8796-2019; Li,
   Xuelong/ABF-3381-2020
OI Shen, Jianbing/0000-0002-4109-8353; Shao, Ling/0000-0002-8264-6117; Li,
   Xuelong/0000-0002-0019-4197
FU National Natural Science Foundation of China [61472036, 61272359];
   National Basic Research Program of China (973 Program) [2013CB328805];
   Chinese Academy of Sciences [KGZD-EW-T03]; Fok Ying-Tong Education
   Foundation for Young Teachers' Specialized Fund for Joint Building
   Program of Beijing Municipal Education Commission
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472036 and Grant 61272359, by the
   National Basic Research Program of China (973 Program) under Grant
   2013CB328805, by the Key Research Program of the Chinese Academy of
   Sciences under Grant KGZD-EW-T03, and by the Fok Ying-Tong Education
   Foundation for Young Teachers' Specialized Fund for Joint Building
   Program of Beijing Municipal Education Commission. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Eckehard Steinbach.
CR [Anonymous], P INT C COMP VIS
   [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2003, IEEE COMPUT SOC C CO
   [Anonymous], 2006, P 2006 IEEE COMP SOC
   [Anonymous], 2009, P ADV NEUR INF PROC
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bai YC, 2012, PROC CVPR IEEE, P1854, DOI 10.1109/CVPR.2012.6247884
   Bailer C, 2014, LECT NOTES COMPUT SC, V8695, P170, DOI 10.1007/978-3-319-10584-0_12
   Bao CL, 2012, PROC CVPR IEEE, P1830, DOI 10.1109/CVPR.2012.6247881
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dinh T.B., 2011, P IEEE WORKSH APPL C, P642, DOI DOI 10.1109/WACV.2011.5711565
   Dinh TB, 2011, PROC CVPR IEEE, P1177, DOI 10.1109/CVPR.2011.5995733
   Fan JL, 2012, IEEE T PATTERN ANAL, V34, P1633, DOI 10.1109/TPAMI.2011.257
   Fan JL, 2010, LECT NOTES COMPUT SC, V6311, P480
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2010, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2010.5540231
   Kwon J, 2011, IEEE I CONF COMP VIS, P1195, DOI 10.1109/ICCV.2011.6126369
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li X, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2508037.2508039
   Liu BY, 2011, PROC CVPR IEEE, P1313, DOI 10.1109/CVPR.2011.5995730
   Liu R, 2009, IEEE I CONF COMP VIS, P1459, DOI 10.1109/ICCV.2009.5459285
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Pérez P, 2002, LECT NOTES COMPUT SC, V2350, P661
   Qing Wang, 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P425, DOI 10.1109/WACV.2012.6162999
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Stalder Severin, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1409, DOI 10.1109/ICCVW.2009.5457445
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang D, 2013, IEEE T IMAGE PROCESS, V22, P314, DOI 10.1109/TIP.2012.2202677
   Wang NY, 2013, IEEE I CONF COMP VIS, P657, DOI 10.1109/ICCV.2013.87
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P3296, DOI 10.1109/TIP.2012.2190085
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2012, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2012.6247878
   Yang F, 2014, IEEE T IMAGE PROCESS, V23, P1639, DOI 10.1109/TIP.2014.2300823
   Yang HX, 2011, NEUROCOMPUTING, V74, P3823, DOI 10.1016/j.neucom.2011.07.024
   Yao R, 2013, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2013.306
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Zhang KH, 2013, IEEE T IMAGE PROCESS, V22, P4664, DOI 10.1109/TIP.2013.2277800
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhang SP, 2013, PATTERN RECOGN, V46, P1772, DOI 10.1016/j.patcog.2012.10.006
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou XZ, 2015, IEEE T MULTIMEDIA, V17, P145, DOI 10.1109/TMM.2014.2380914
   Zhou XZ, 2010, PROC CVPR IEEE, P1847, DOI 10.1109/CVPR.2010.5539856
   Zhuang BH, 2014, IEEE T IMAGE PROCESS, V23, P1872, DOI 10.1109/TIP.2014.2308414
NR 55
TC 104
Z9 112
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1818
EP 1828
DI 10.1109/TMM.2015.2463221
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400011
DA 2024-07-18
ER

PT J
AU Sun, C
   Bao, BK
   Xu, CS
AF Sun, Chao
   Bao, Bing-Kun
   Xu, Changsheng
TI Knowing Verb From Object: Retagging With Transfer Learning on
   Verb-Object Concept Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian; retagging; verb-object image
ID CONTEXT; MODELS
AB Image retagging is significant and essential for tag-based applications, such as search and browsing. However, most existing image retagging approaches are typically based on enriching-and-removing and/or reranking strategies, which lead to two drawbacks: 1) since the object and/or human appeared in the images are tagged as individuals, the meanings represented by the mutual context of object and human are ignored and not tagged, and 2) some images which are visually dissimilar but semantically similar could be filtered incorrectly, as they are conflict with the content consistency rule. These two defects are distinct especially when images with human-object interactions are retagged. To tackle these defects, in this paper we propose a Bayesian approach to jointly consider the human and object in an image and retag it properly. In our approach, human and objects in images are detected and their interrelationships are taken into account. Tags which represent the mutual context of human and objects are then mapped to those interrelationships by a probabilistic graphical model. For a new image which lacks the tag representing the interaction between human and object, our model can correctly retag it for the interaction. In this paper, those images involving human-object interactions are called verb-object concept images, and experiments on a 60-class dataset demonstrate the capacity of our Bayesian retagging approach of verb-object concept images (BRVOI).
C1 [Sun, Chao; Bao, Bing-Kun; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Sun, C (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM csun@nlpr.ia.ac.cn; bingkunbao@gmail.com; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61201374, 61432019];
   Beijing Natural Science Foundation [4152053]; State Key Laboratory for
   Novel Software Technology of Nanjing University, China
FX This work was supported in part by National Basic Research Program of
   China under Grant 2012CB316304, by the National Natural Science
   Foundation of China under Grant 61225009, Grant 61201374, and Grant
   61432019, by the Beijing Natural Science Foundation Grant 4152053, and
   by the open project from State Key Laboratory for Novel Software
   Technology of Nanjing University, China. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek.
CR [Anonymous], P 2009 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2009.5206557
   [Anonymous], 2001, COMPUT SCI STAT
   [Anonymous], 2010, P ACM MULTIMEDIA
   Bao BK, 2013, IEEE T IMAGE PROCESS, V22, P860, DOI 10.1109/TIP.2012.2219543
   Bart E, 2005, PROC CVPR IEEE, P672
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Delaitre V., 2011, P INT C NEUR INF PRO, P1503
   Desai C, 2011, INT J COMPUT VISION, V95, P1, DOI 10.1007/s11263-011-0439-x
   Desai Chaitanya., 2010, Ieee computer society conference on computer vision and pattern recognition workshops, P9
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Filipovych R., 2008, PROC IEEE C COMPUT V, P1
   Gallese V, 1996, BRAIN, V119, P593, DOI 10.1093/brain/119.2.593
   Gupta A., 2007, PROC IEEE C COMPUT V, P1
   Gupta A., 2008, PROC IEEE C COMPUT V, P1
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Larochelle H., 2008, AAAI C ARTIFICIAL IN, V1, P2
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li T, 2011, IEEE T IMAGE PROCESS, V20, P2301, DOI 10.1109/TIP.2010.2103081
   Liu D, 2011, MULTIMED TOOLS APPL, V51, P723, DOI 10.1007/s11042-010-0647-3
   Liu Dong., 2010, Proceedings of the International Conference on Multimedia, P491
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Weinberger K.Q., 2008, Proceedings of Conference on Multimedia, P111
   Wilson AD, 1999, IEEE T PATTERN ANAL, V21, P884, DOI 10.1109/34.790429
   Yao BP, 2012, IEEE T PATTERN ANAL, V34, P1691, DOI 10.1109/TPAMI.2012.67
   Yao BP, 2010, PROC CVPR IEEE, P9, DOI 10.1109/CVPR.2010.5540234
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Yohan Jin, 2005, 13th Annual ACM International Conference on Multimedia, P706
NR 38
TC 7
Z9 8
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1747
EP 1759
DI 10.1109/TMM.2015.2463218
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400006
DA 2024-07-18
ER

PT J
AU Yao, SS
   Wang, YS
   Niu, BN
AF Yao, Shanshan
   Wang, Yunsheng
   Niu, Baoning
TI An Efficient Cascaded Filtering Retrieval Method for Big Audio Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio middle fingerprint; big audio data; cascade filtering retrieval;
   content-based retrieval; Philips audio fingerprint
AB Fast audio retrieval is crucial for many important applications and yet demanding due to the high dimension nature and increasingly larger volume of audios on the Internet. Although audio fingerprinting can greatly reduce its dimension while keeping audio identifiable, the dimension for audio fingerprints is still too high to scale up for big audio data. The tradeoff between accuracy (measured by precision and recall rate) and efficiency (measured by retrieval time) prevents further reduction in the dimension of fingerprints. This paper shows that a multi-stage filtering strategy can achieve both speedup and high accuracy, with the beginning stages focusing on speedup and the end stage emphasizing accuracy. With this strategy, an efficient cascaded filtering retrieval method is proposed that consists of filtering with Fibonacci hashing, the middle fingerprint, thresholds to quickly select candidate audios, and refining with an accurate and robust fingerprint on the candidate audios. Experiments with 500 000 audios show that the proposed method can achieve a speed gain more than 28 K times that of the Fibonacci hashing retrieval. After applying MP3 conversion, resampling, white noise addition, and background noise addition, the recall rates of the method are all above 99.45%, and the precision is the same as the Philips audio fingerprint, which is close to 100%.
C1 [Yao, Shanshan; Wang, Yunsheng; Niu, Baoning] Taiyuan Univ Technol, Dept Comp Sci & Technol, Taiyuan 030024, Peoples R China.
C3 Taiyuan University of Technology
RP Niu, BN (corresponding author), Taiyuan Univ Technol, Dept Comp Sci & Technol, Taiyuan 030024, Peoples R China.
EM yaoshanshan0273@link.tyut.edu.cn; wang_yun_sheng@163.com;
   niubaoning@tyut.edu.cn
RI Yao, Shanshan/JJC-4929-2023; Niu, Baoning/ACC-8776-2022
OI Niu, Baoning/0000-0002-7924-3384
FU National Key Technology Support Program of China [2012BAH04F02,
   2015BAH37F02]
FX This work was supported in part by the National Key Technology Support
   Program of China under Grant 2012BAH04F02 and Grant 2015BAH37F02. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. Yonghong Tian. (Corresponding author: Baoning
   Niu.)
CR BARNEA DI, 1972, IEEE T COMPUT, VC 21, P179, DOI 10.1109/TC.1972.5008923
   Chen M., 2013, P ICAIEES, P219
   Diamantaras K., 1996, Principal component neural networks: theory and applications
   Grosche P., 2012, Multimodal Music Processing, V3, P157
   Haitsma J, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P178
   Haitsma J., 2002, P ISMIR 2002 3 INT C, P107
   Hu Y. G., 2006, COMPUT APPL, V26, P2250
   KURTH F, 2002, P AUD ENG SOC CONV, P1
   Kurth F, 2008, IEEE T AUDIO SPEECH, V16, P382, DOI 10.1109/TASL.2007.911552
   Panagiotou V., 2013, P DSP, P1
   Precioso F, 2011, IEEE IMAGE PROC, P109, DOI 10.1109/ICIP.2011.6115618
   Shen L, 2009, 2009 INTERNATIONAL CONFERENCE ON NETWORKING AND DIGITAL SOCIETY, VOL 2, PROCEEDINGS, P259, DOI [10.1109/ICNDS.2009.144, 10.1109/CSIE.2009.236]
   Shibuya T, 2013, P ICME, P1
   Vitola C. P. J., 2013, P STSIVA, P1
   Yao S., 2015, P BIGMM, P105
   Zheng Guibin, 2009, Proceedings of the 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing. IIH-MSP 2009, P1156, DOI 10.1109/IIH-MSP.2009.178
NR 16
TC 11
Z9 11
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1450
EP 1459
DI 10.1109/TMM.2015.2460121
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000006
DA 2024-07-18
ER

PT J
AU Zhou, Z
   Shi, F
   Xiao, JJ
   Wu, W
AF Zhou, Zhong
   Shi, Feng
   Xiao, Jiangjian
   Wu, Wei
TI Non-Rigid Structure-From-Motion on Degenerate Deformations With Low-Rank
   Shape Deformation Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Degenerate deformations; low-rank shape deformation model; non-rigid
   structure from motion; 3D reconstruction
ID ACTIVE APPEARANCE MODELS; FACTORIZATION; RECOVERY
AB Non-rigid structure-from-motion (NRSfM) is the process of recovering time-varying 3D structures and poses of a deformable object from an uncalibrated monocular video sequence. Currently, most NRSfM algorithms utilize a non-degenerate assumption for non-rigid object deformations whereby the 3D structures of a non-rigid object can be assumed to be a linear combination of basis shapes with full rank three. Unfortunately, this assumption will produce extra degrees-of-freedom when the non-rigid object has some degenerate deformations with shape bases of rank less than three. These extra degrees-of-freedom will yield spurious shape deformations due to non-negligible noise in real applications, which will cause substantial reconstruction errors. To solve this problem, we propose a low-rank shape deformation model to represent 3D structures of degenerate deformations. When modeling degenerate deformations, the proposed model exploits the rank-deficient nature of degenerate deformations in addition to the low-rank property of non-rigid objects' trajectories, thus providing a more accurate and compact representation compared with existing models. Based on this model, we formulate the NRSfM problem as two coherent optimization problems. These problems are solved with iterative non-linear optimization algorithms. Experiments on synthetic and motion capture data are conducted. The results exhibit the significant advantages of our approach over state-of-the-art NRSfM algorithms for the 3D recovery of non-rigid objects with degenerate deformations.
C1 [Zhou, Zhong; Shi, Feng; Wu, Wei] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Xiao, Jiangjian] Chinese Acad Sci, Ningbo Ind Technol Res Inst, Ningbo 315201, Zhejiang, Peoples R China.
C3 Beihang University; Chinese Academy of Sciences
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zz@buaa.edu.cn; supersf2008@hotmail.com; xiaojj@nimte.ac.cn;
   wuwei@buaa.edu.cn
RI Shi, Feng/G-3247-2012
FU National 863 Program of China [2012AA011801, 2012AA011803]; Natural
   Science Foundation of China [61170188]
FX This work was supported by the National 863 Program of China under Grant
   2012AA011801 and Grant 2012AA011803 and by the Natural Science
   Foundation of China under Grant 61170188. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jing-Ming Guo.
CR Agudo A, 2014, PROC CVPR IEEE, P1558, DOI 10.1109/CVPR.2014.202
   Akhter I., 2008, ADV NEURAL INFORM PR, P41
   Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201
   Akhter I, 2009, PROC CVPR IEEE, P1534, DOI 10.1109/CVPRW.2009.5206620
   Angst R, 2012, LECT NOTES COMPUT SC, V7577, P682, DOI 10.1007/978-3-642-33783-3_49
   [Anonymous], P EUR C COMP VIS
   Brand M, 2005, PROC CVPR IEEE, P122
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Dai YC, 2012, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2012.6247905
   Del Bue A, 2013, INT J COMPUT VISION, V103, P226, DOI 10.1007/s11263-012-0577-9
   Ding LY, 2009, IMAGE VISION COMPUT, V27, P1826, DOI 10.1016/j.imavis.2009.02.005
   Gotardo P. F. U., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3065, DOI 10.1109/CVPR.2011.5995560
   Gotardo PFU, 2011, IEEE I CONF COMP VIS, P802, DOI 10.1109/ICCV.2011.6126319
   Gotardo PFU, 2011, IEEE T PATTERN ANAL, V33, P2051, DOI 10.1109/TPAMI.2011.50
   Khan I, 2014, IEEE T MULTIMEDIA, V16, P1350, DOI 10.1109/TMM.2014.2308415
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   Magnus J.R., 1999, MATRIX DIFFERENTIAL
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   Olsen SI, 2008, J MATH IMAGING VIS, V31, P233, DOI 10.1007/s10851-007-0060-3
   Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602
   Paladini M, 2010, LECT NOTES COMPUT SC, V6312, P15, DOI 10.1007/978-3-642-15552-9_2
   Park HS, 2010, LECT NOTES COMPUT SC, V6313, P158
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rabaud Vincent, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2427, DOI 10.1109/CVPRW.2009.5206628
   Rengier F, 2010, INT J COMPUT ASS RAD, V5, P335, DOI 10.1007/s11548-010-0476-x
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tao LL, 2013, PROC CVPR IEEE, P1530, DOI 10.1109/CVPR.2013.201
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Tron R., 2007, PROC IEEE C COMPUT V, P1
   Xiao J, 2004, PROC CVPR IEEE, P668
   Zaheer A, 2011, IEEE I CONF COMP VIS, P2447, DOI 10.1109/ICCV.2011.6126529
   Zhou HY, 2012, IEEE T MULTIMEDIA, V14, P168, DOI 10.1109/TMM.2011.2170406
   Zhu JK, 2006, LECT NOTES COMPUT SC, V3951, P186
NR 35
TC 11
Z9 15
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 171
EP 185
DI 10.1109/TMM.2014.2384396
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500003
OA Bronze
DA 2024-07-18
ER

PT J
AU Xiong, J
   Li, HL
   Meng, FM
   Zhu, SY
   Wu, QB
   Zeng, B
AF Xiong, Jian
   Li, Hongliang
   Meng, Fanman
   Zhu, Shuyuan
   Wu, Qingbo
   Zeng, Bing
TI MRF-Based Fast HEVC Inter CU Decision With the Variance of Absolute
   Differences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph cut; H.264; HEVC; inter prediction; MRF; video coding
ID RANDOM-FIELDS; ALGORITHM; SEGMENTATION
AB The newly developed High Efficiency Video Coding (HEVC) Standard has improved video coding performance significantly in comparison to its predecessors. However, more intensive computation complexity is introduced by implementing a number of new coding tools. In this paper, a fast coding unit (CU) decision based on Markov random field (MRF) is proposed for HEVC inter frames. First, it is observed that the variance of the absolute difference (VAD) is proportional with the rate-distortion (R-D) cost. The VAD based feature is designed for the CU selection. Second, the decision of CU splittings is modeled as an MRF inference problem, which can be optimized by the Graphcut algorithm. Third, a maximum a posteriori (MAP) approach based on the R-D cost is conducted to evaluate whether the unsplit CUs should be further split or not. Experimental results show that the proposed algorithm can achieve about 53% reduction of the coding time with negligible coding performance degradation, which outperforms the state-of-the-art algorithms significantly.
C1 [Xiong, Jian; Li, Hongliang; Meng, Fanman; Zhu, Shuyuan; Wu, Qingbo; Zeng, Bing] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Xiong, J (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 611731, Peoples R China.
EM jxiong219@gmail.com; hlli@uestc.edu.cn; fanmanmeng@gmail.com;
   eezsy@uestc.edu.cn; wqb.uestc@gmail.com; eezeng@uestc.edu.cn
RI Wu, Qingbo/M-5065-2015; Wu, Qingbo/AAF-6872-2019
OI Wu, Qingbo/0000-0003-2936-6340; Li, Hongliang/0000-0002-7481-095X;
   Xiong, Jian/0000-0002-8346-178X; Xiong, Jian/0000-0002-4720-4102
FU Major State Basic Research Development Program of China (973 Program)
   [2015CB351804]; National Science Foundation of China [61271289,
   61300091]; The Ph.D. Programs Foundation of Ministry of Education of
   China [20110185110002]; The Program for Young Scholars Innovative
   Research Team of Sichuan Province, China [2014TD0006]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2015CB351804, the
   National Science Foundation of China under Grants 61271289 and 61300091,
   The Ph.D. Programs Foundation of Ministry of Education of China under
   Grant 20110185110002, and by The Program for Young Scholars Innovative
   Research Team of Sichuan Province, China, under Grant 2014TD0006. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yiannis Andreopoulos.
CR [Anonymous], 2001, SC16Q6 ITUT
   Bossen F., 2012, P 10 JCT VC M STOCKH, P1
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y, 2006, INT J COMPUT VISION, V70, P109, DOI 10.1007/s11263-006-7934-5
   Bross B., 2012, P 9 M JCTVC 11003 AP, P4
   Cassa MB, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P493, DOI 10.1109/PCS.2012.6213262
   GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x
   Han JN, 2010, INT CONF ACOUST SPEE, P726, DOI 10.1109/ICASSP.2010.5495043
   Jie Leng, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P56, DOI 10.1109/CMSP.2011.167
   Kim I.-K., 2012, P 9 M JCTVC 11002 GE, P7
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Lan CL, 2011, IEEE J-STSP, V5, P1298, DOI 10.1109/JSTSP.2011.2165273
   Lee CH, 2005, LECT NOTES COMPUT SC, V3765, P469
   Li HL, 2004, IEEE T MULTIMEDIA, V6, P624, DOI [10.1109/TMM.2004.830812, 10.1109/tmm.2004.830812]
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   LIU C., 2009, Ph.D. Thesis
   Maji S., 2008, CVPR
   MAUERSBERGER W, 1979, ELECTRON LETT, V15, P664, DOI 10.1049/el:19790472
   Meng FM, 2013, IEEE T MULTIMEDIA, V15, P2186, DOI 10.1109/TMM.2013.2280893
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Moser G, 2013, IEEE T GEOSCI REMOTE, V51, P2734, DOI 10.1109/TGRS.2012.2211882
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen XL, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P453, DOI 10.1109/PCS.2012.6213252
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu Q, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P659
   Xiong J., P IEEE INT IN PRESS
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Zhang YF, 2013, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2013.13
NR 29
TC 64
Z9 66
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2141
EP 2153
DI 10.1109/TMM.2014.2356795
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300006
DA 2024-07-18
ER

PT J
AU Samanta, S
   Chanda, B
AF Samanta, Soumitra
   Chanda, Bhabatosh
TI Space-Time Facet Model for Human Activity Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-words; facet model; human activity; space time interest point
ID HUMAN ACTION CATEGORIES; RECOGNITION; SCALE; POINTS
AB This paper presents a novel space-time feature-based human activity analysis system. We detect Space Time Interest Points (STIP) and generate their description based on the facet model. The proposed approach detects interest points in video data using the three-dimensional facet model efficiently. Then we describe each interest point by three-dimensional Haar wavelet transform and time derivatives of different order obtained from said facet model. Here we represent each video clip following the bag-of-words approach by learning feature specific dictionary. Finally, classification is done using non-linear SVM with chi(2)-kernel. We evaluate the performance of our system on standard datasets like Weizmann, KTH, UCF sports, ICD, UCF YouTube, and UCF50 and get better, or at least comparable results compared to other state-of-the-art systems.
C1 [Samanta, Soumitra; Chanda, Bhabatosh] Indian Stat Inst, ECSU, Kolkata 700108, India.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata
RP Samanta, S (corresponding author), Indian Stat Inst, ECSU, Kolkata 700108, India.
OI Samanta, Soumitra/0000-0003-2200-3061
FU DST, India [NRDMS/11/1586/09/Phase-I/Project, 9]
FX This work was supported in part by DST, India under Grant
   NRDMS/11/1586/09/Phase-I/Project No. 9. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang.
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Ali A, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P28, DOI 10.1109/EVENT.2001.938863
   [Anonymous], PATTERN RECOG LETT
   [Anonymous], P ICVGIP
   [Anonymous], 2009, P BMVC
   [Anonymous], IEEE I CONF COMP VIS
   [Anonymous], P ECCV
   [Anonymous], P ECCV
   [Anonymous], P ECCV
   [Anonymous], 1988, P ALV VIS C MANCH UK
   [Anonymous], 2008, P IEEE INT C COMP VI
   [Anonymous], 2008, P BMVC
   [Anonymous], P NIPS
   [Anonymous], 2007, P ACM INT C MULT
   [Anonymous], P ISVC
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bregonzio M, 2012, PATTERN RECOGN, V45, P1220, DOI 10.1016/j.patcog.2011.08.014
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Efros AA, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P726
   Elgammal A, 2003, PROC CVPR IEEE, P571
   Everts I, 2013, PROC CVPR IEEE, P2850, DOI 10.1109/CVPR.2013.367
   Fathi A., 2008, PROC IEEE C COMPUT V, P1
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Haralick R. M., 1992, COMPUTER ROBOT VISIO
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Ikizler N., Proceedings from International Conference on Pattern Recognition, 2008, P1
   Ji Q, 2002, PATTERN RECOGN, V35, P689, DOI 10.1016/S0031-3203(01)00035-8
   Jiang ZL, 2012, IEEE T PATTERN ANAL, V34, P533, DOI 10.1109/TPAMI.2011.147
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Ke Y, 2005, IEEE I CONF COMP VIS, P166
   Ke Y., 2007, PROC IEEE C COMPUT V, P1
   Kliper-Gross O., 2012, P ECCV
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li H, 2005, IEEE I CONF COMP VIS, P236
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Liu JG, 2009, PROC CVPR IEEE, P1996
   [刘军 LIU Jun], 2008, [高分子通报, Polymer Bulletin], P1, DOI 10.1145/1509315.1509331
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Natarajan P, 2010, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR.2010.5539876
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Nowozin S, 2007, IEEE I CONF COMP VIS, P1727
   Oikonomopoulos A, 2006, IEEE T SYST MAN CY B, V36, P710, DOI 10.1109/TSMCB.2005.861864
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Rodriguez M. D., 2008, PROC IEEE C COMPUT V, P1
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Samanta S., 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P265, DOI 10.1109/WACV.2012.6163050
   Savarese S., 2008, PROC IEEE WORKSHOP M, P1
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shechtman E, 2007, IEEE T PATTERN ANAL, V29, P2045, DOI 10.1109/TPAMI.2007.1119
   Sheikh Y, 2005, IEEE I CONF COMP VIS, P144
   Shen Y., 2008, IEEE C COMPUTER VISI, P1, DOI DOI 10.1109/CVPR.2008.4587755
   Shi F, 2013, PROC CVPR IEEE, P2595, DOI 10.1109/CVPR.2013.335
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wang Y, 2009, IEEE T PATTERN ANAL, V31, P1762, DOI 10.1109/TPAMI.2009.43
   Yan Zhu, 2010, Computer Vision - ACCV 2010. 10th Asian Conference on Computer Vision. Revised Selected Papers, P660, DOI 10.1007/978-3-642-19309-5_51
   Yilmaz A, 2005, PROC CVPR IEEE, P984
   Yuan JS, 2009, PROC CVPR IEEE, P2442, DOI [10.1109/CVPRW.2009.5206671, 10.1109/CVPR.2009.5206671]
NR 69
TC 32
Z9 34
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1525
EP 1535
DI 10.1109/TMM.2014.2326734
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200003
DA 2024-07-18
ER

PT J
AU Zhu, YK
   Zhu, J
   Zhang, R
AF Zhu, Yukun
   Zhu, Jun
   Zhang, Rui
TI Contextual Object Detection With Spatial Context Prototypes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering methods; context modeling; object detection
ID MODELS
AB Contextual information is widely exploited in state-of-the-art object detection systems, most of which utilize pre-defined spatial relationships (e. g., above, below, next to, etc.). However, we observe that the spatial arrangement manifests heterogeneous statistical distributions for different object class pairs, which suggests mining class-specified prototypes of spatial contexts in a data-driven manner. This paper proposes a novel contrast K-Means clustering algorithm for automatically discovering spatial context prototypes to beyond the pre-defined spatial relationship representation in literature. Based on the learned prototypes, we further construct the spatial context features by using a simple localized soft assignment quantization method. Besides, considering the large number of real object categories that might lead to over-complicated spatial context features, we propose a feature refinement method based on the number of context occurrences and K-L divergence to efficiently reduce the complexity of our contextual model. The experiment results on PASCAL VOC dataset and SUN 09 dataset demonstrate that our method can effectively capture meaningful spatial context prototypes as well as most contributing contextual features for different object class pairs and thus boost recognition performance on object detection task.
C1 [Zhu, Yukun; Zhu, Jun; Zhang, Rui] Shanghai Jiao Tong Univ, Inst Image Transmiss & Informat Proc, Shanghai 200030, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhu, YK (corresponding author), Shanghai Jiao Tong Univ, Inst Image Transmiss & Informat Proc, Shanghai 200030, Peoples R China.
EM lrnczhu@gmail.com; zhujun.sjtu@gmail.com; zhang_rui@sjtu.edu.cn
RI zhang, ruigang/H-7317-2014; Zhu, Jun/Q-3667-2016
FU NSFC [61071155, 61221001]; 973 Program [2010CB731401, 2010CB731406];
   STCSM [l2DZ2272600]
FX This work was supported by the NSFC under Grants 61071155 and 61221001,
   the 973 Program under Grants 2010CB731401 and 2010CB731406, and the
   STCSM under Grant l2DZ2272600. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cees
   G. M. Snoek.
CR Amit Y, 2007, INT J COMPUT VISION, V75, P267, DOI 10.1007/s11263-006-0033-9
   [Anonymous], 2008, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2008.4587799, DOI 10.1109/CVPR.2008.4587799]
   [Anonymous], 2008, Proceedings of the Twenty-Fifth International Conference on Machine Learning
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2008, IEEE C COMP VIS PATT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bar M, 2004, NAT REV NEUROSCI, V5, P617, DOI 10.1038/nrn1476
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Carbonetto P, 2004, LECT NOTES COMPUT SC, V3021, P350
   Choi MJ, 2010, PROC CVPR IEEE, P129, DOI 10.1109/CVPR.2010.5540221
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Desai C, 2009, IEEE I CONF COMP VIS, P229, DOI 10.1109/ICCV.2009.5459256
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Felzenszwalb P., 2008, PROC 2008 IEEE C COM, P1
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Goh JOS, 2004, J NEUROSCI, V24, P10223, DOI 10.1523/JNEUROSCI.3373-04.2004
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Heitz G, 2008, LECT NOTES COMPUT SC, V5302, P30, DOI 10.1007/978-3-540-88682-2_4
   Hoiem D., 2006, CVPR
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kumar S, 2005, IEEE I CONF COMP VIS, P1284
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Singhal A, 2003, PROC CVPR IEEE, P235
   Song Z, 2011, PROC CVPR IEEE, P1585, DOI 10.1109/CVPR.2011.5995330
   Torralba A, 2003, J OPT SOC AM A, V20, P1407, DOI 10.1364/JOSAA.20.001407
   Torralba A, 2006, PSYCHOL REV, V113, P766, DOI 10.1037/0033-295X.113.4.766
   Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang Y, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON MATERIAL SCIENCE AND ENVIRONMENTAL ENGINEERING (MSEE 2013), P6
NR 38
TC 9
Z9 9
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1585
EP 1596
DI 10.1109/TMM.2014.2321534
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200008
DA 2024-07-18
ER

PT J
AU Rhee, CE
   Kim, TS
   Lee, HJ
AF Rhee, Chae Eun
   Kim, Tae Sung
   Lee, Hyuk-Jae
TI An H.264 High-Profile Intra-Prediction with Adaptive Selection Between
   the Parallel and Pipelined Executions of Prediction Modes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Early termination; H.264/AVC; high-profile; intra-prediction; pipeline
   schedule
ID DECISION ALGORITHM; VLSI ARCHITECTURE; ENCODER; DESIGN; BLOCK
AB A high-profile H. 264 intra-frame encoder is suitable for low-cost and low-power applications and capable of providing enhanced compression efficiency. The high-profile is targeting the high-resolution videos. Thus, the encoding speed should be faster than or comparable to the baseline-profile. In previous work related to a hardware-based baseline-profile intra-frame encoder, a speed-up is achieved by the early termination of the intra modes and by an increase in the rate of hardware utilization only under one of the serialized and parallel schedules. This paper proposes a novel pipeline schedule for a hardware-based high-profile intraprediction scheme in which the 8 x 8 prediction is performed in Stage 1 and, 4 x 4, 16 x 16 and chroma predictions are executed during Stage 2. The processing time of Stage 2 is efficiently accelerated based on the result of the 8 x 8 prediction in Stage 1. According to the distribution of each mode, the schedule is adaptively selected between parallel and pipeline schedules. To increase the hardware utilization of the 8 x 8 prediction, the order of prediction modes and the inverse vertical transform is adaptively adjusted. In addition, early termination of the prediction modes is employed for a fast 8 x 8 prediction. The proposed 8 x 8 intra-prediction is implemented and verified as an entire intra-frame encoder. Experimental results show that the average number of cycles necessary to process one MB for videos with resolutions of 1920 x 1080 and 3840 x 2160 are only 269 and 253 cycles, respectively. Compared to JM13.2, the bitrate is increased by 1.13% on average with a small PSNR degradation of 0.06 dB. The difference in the rate-distortion performance between the proposed high-profile intra-prediction scheme and JM 13.2 is not significant, whereas the achieved speed-up due to the proposed schemes is considerable compared to the conventional hardware-based intra-prediction encoders.
C1 [Rhee, Chae Eun] Inha Univ, Dept Informat & Commun Engn, Inchon 402751, South Korea.
   [Kim, Tae Sung; Lee, Hyuk-Jae] Seoul Natl Univ, Dept Elect Engn, Interuniv Semicond Res Ctr, Seoul 151742, South Korea.
C3 Inha University; Seoul National University (SNU)
RP Rhee, CE (corresponding author), Inha Univ, Dept Informat & Commun Engn, Inchon 402751, South Korea.
EM chae.rhee@inha.ac.kr; tskim@capp.snu.ac.kr; hyuk_jae_lee@capp.snu.ac.kr
FU INHA UNIVERSITY - Ministry of Trade, Industry and Energy (MOTIE, Korea)
   [INHA-47293, 10039188]; MSIP (Ministry of Science, ICT & Future
   Planning), Korea under the ITRC (Information Technology Research Center)
   support program [NIPA-2013-H0301-13-1011]
FX This work was supported by INHA UNIVERSITY Research Grant. (INHA-47293)
   and by the Technology Innovation Program (10039188, Development of
   multimedia convergence programmable platform for smart vehicles) funded
   by the Ministry of Trade, Industry and Energy (MOTIE, Korea). This work
   was also supported by the MSIP (Ministry of Science, ICT & Future
   Planning), Korea under the ITRC (Information Technology Research Center)
   support program supervised by the NIPA (National IT Industry Promotion
   Agency) (NIPA-2013-H0301-13-1011). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Zhihai (Henry) He.
CR [Anonymous], 2001, P 13 VCEG M33 M AUST
   Bharanitharan K, 2008, IEEE T MULTIMEDIA, V10, P1250, DOI 10.1109/TMM.2008.2004904
   Cheng CC, 2006, IEEE INT SYMP CIRC S, P5335
   Ding LF, 2010, IEEE J SOLID-ST CIRC, V45, P46, DOI 10.1109/JSSC.2009.2031787
   Feng HX, 2008, IEEE I C EMBED SOFTW, P187, DOI 10.1109/ICESS.2008.16
   Huang YH, 2010, IEEE T CIRC SYST VID, V20, P1122, DOI 10.1109/TCSVT.2010.2057018
   Huang YW, 2005, IEEE T CIRC SYST VID, V15, P378, DOI 10.1109/TCSVT.2004.842620
   Jung JS, 2011, J SIGNAL PROCESS SYS, V64, P161, DOI 10.1007/s11265-010-0574-6
   Jung JS, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/542735
   Li HL, 2008, IEEE T CIRC SYST VID, V18, P756, DOI 10.1109/TCSVT.2008.918778
   Lin HY, 2010, IEEE T CIRC SYST VID, V20, P894, DOI 10.1109/TCSVT.2010.2046059
   Lin Y. K., 2007, IEEE T CIRCUITS SYST, V19, P432
   Lin YK, 2008, DES AUT CON, P78
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Puri A, 2004, SIGNAL PROCESS-IMAGE, V19, P793, DOI 10.1016/j.image.2004.06.003
   Rhee CE, 2012, IEEE T CIRC SYST VID, V22, P403, DOI 10.1109/TCSVT.2011.2163977
   Rhee CE, 2010, IEEE T CIRC SYST VID, V20, P1848, DOI 10.1109/TCSVT.2010.2087834
   Tsai AC, 2008, IEEE T CIRC SYST VID, V18, P975, DOI 10.1109/TCSVT.2008.920742
   Tsai AC, 2008, IEEE T CIRC SYST VID, V18, P694, DOI 10.1109/TCSVT.2008.919113
   Tsai AC, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1587
   Wang JC, 2007, IEEE T CIRC SYST VID, V17, P1414, DOI 10.1109/TCSVT.2007.903786
   Wei ZY, 2007, IEEE INT SYMP CIRC S, P3630, DOI 10.1109/ISCAS.2007.378539
   Yang CL, 2004, IEEE IMAGE PROC, P461
   Yu Y, 2008, INT CONF ACOUST SPEE, P681
   Zhang K, 2007, 9TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY: TOWARD NETWORK INNOVATION BEYOND EVOLUTION, VOLS 1-3, P673
NR 25
TC 8
Z9 8
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 947
EP 959
DI 10.1109/TMM.2014.2306396
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800005
DA 2024-07-18
ER

PT J
AU Wang, S
   Ma, ZG
   Yang, Y
   Li, X
   Pang, CY
   Hauptmann, AG
AF Wang, Sen
   Ma, Zhigang
   Yang, Yi
   Li, Xue
   Pang, Chaoyi
   Hauptmann, Alexander G.
TI Semi-Supervised Multiple Feature Analysis for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human action recognition; multiple feature learning; semi-supervised
   learning; shared structural analysis
ID IMAGE ANNOTATION; FRAMEWORK; WEB
AB This paper presents a semi-supervised method for categorizing human actions using multiple visual features. The proposed algorithm simultaneously learns multiple features from a small number of labeled videos, and automatically utilizes data distributions between labeled and unlabeled data to boost the recognition performance. Shared structural analysis is applied in our approach to discover a common subspace shared by each type of feature. In the subspace, the proposed algorithm is able to characterize more discriminative information of each feature type. Additionally, data distribution information of each type of feature has been preserved. The aforementioned attributes make our algorithm robust for action recognition, especially when only limited labeled training samples are provided. Extensive experiments have been conducted on both the choreographed and the realistic video datasets, including KTH, Youtube action and UCF50. Experimental results show that our method outperforms several state-of-the-art algorithms. Most notably, much better performances have been achieved when there are only a few labeled training samples.
C1 [Wang, Sen; Yang, Yi; Li, Xue] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
   [Ma, Zhigang] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.
   [Li, Xue] Chongqing Univ, Key Lab Dependable Serv Comp, Cyber Phys Soc, Chongqing 630044, Peoples R China.
   [Pang, Chaoyi] CSIRO, Australian E Hlth Res Ctr, Brisbane, Qld, Australia.
   [Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
C3 University of Queensland; Carnegie Mellon University; Chongqing
   University; Commonwealth Scientific & Industrial Research Organisation
   (CSIRO); Carnegie Mellon University
RP Wang, S (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
EM sen.wang@uq.edu.au; kevinma@cs.cmu.edu; yi.yang@uq.edu.au;
   xueli@itee.uq.edu.au; chaoyi.pang@csiro.au; alex@cs.cmu.edu
RI Unankard, Sayan/M-4084-2016; wu, sen/HKE-6181-2023; Yang,
   Yi/B-9273-2017; Lang, Ming/HIK-0758-2022; yang, yang/GWB-9426-2022;
   yang, yang/GVT-5210-2022; Ma, Zhigang/H-3543-2015; Pang,
   Chaoyi/JQX-1513-2023; yang, yang/HGT-7999-2022
OI Yang, Yi/0000-0002-0512-880X; LI, Xue/0000-0002-4515-6792; Wang,
   Sen/0000-0002-5414-8276
FU U. S. Army Research Office [W911NF-13-1-0277]; Australian Research
   Council [DE130101311];  [DP 130104614]; Australian Research Council
   [DE130101311] Funding Source: Australian Research Council
FX The work was supported in part by the U. S. Army Research Office
   (W911NF-13-1-0277), in part by the Australian Research Council the
   Discovery Early Career Researcher Award No. DE130101311, and in part by
   the Discover Project No. DP 130104614. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the authors and do not necessarily reflect the views of ARO and
   Australian Research Council. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Xiao-Ping Zhang.
CR Ando RK, 2005, J MACH LEARN RES, V6, P1817
   [Anonymous], P NIPS
   [Anonymous], 2009, MOSIFT RECOGNIZING H
   [Anonymous], 2010, SDM
   [Anonymous], 2009, P BRIT MACH VIS C
   Farquhar JDR, 2005, P NIPS
   Feng Y., 2012, P 11 ASIAN C COMPUTE, P343
   Golub G. H., 1996, MATRIX COMPUTATIONS, V4
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lu ZW, 2010, IEEE T MULTIMEDIA, V12, P194, DOI 10.1109/TMM.2010.2041100
   Ma Z., 2011, Proc. 19th ACM Int'l Conf. Multimedia, P283, DOI DOI 10.1145/2072298.2072336
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Saberian M. J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2929, DOI 10.1109/CVPR.2011.5995605
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wang L, 2006, INT C PATT RECOG, P1266
   Wang S, 2012, PROC CVPR IEEE, P1370, DOI 10.1109/CVPR.2012.6247823
   Wang Y., P CVPR, P872
   Weinland D., 2008, CVPR, P1
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Y, 2012, IEEE T IMAGE PROCESS, V21, P1339, DOI 10.1109/TIP.2011.2169269
   Yu SI, 2013, PROC CVPR IEEE, P3714, DOI 10.1109/CVPR.2013.476
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zheng J., 2012, P BMVC
   Zhu X., 2005, Time-sensitive Dirichlet process mixture models
NR 38
TC 58
Z9 61
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 289
EP 298
DI 10.1109/TMM.2013.2293060
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800001
DA 2024-07-18
ER

PT J
AU Yin, H
   Zhang, X
   Zhan, TY
   Zhang, Y
   Min, GY
   Wu, DO
AF Yin, Hao
   Zhang, Xu
   Zhan, Tongyu
   Zhang, Ying
   Min, Geyong
   Wu, Dapeng Oliver
TI NetClust: A Framework for Scalable and Pareto-Optimal Media Server
   Placement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Server placement; network coordinate; multimedia network
AB Effective media server placement strategies are critical for the quality and cost of multimedia services. Existing studies have primarily focused on optimization-based algorithms to select server locations from a small pool of candidates based on the entire topological information and thus these algorithms are not scalable due to unavailability of the small pool of candidates and low-efficiency of gathering the topological information in large-scale networks. To overcome this limitation, a novel scalable framework called NetClust is proposed in this paper. NetClust takes advantage of the latest network coordinate technique to reduce the workloads when obtaining the global network information for server placement, adopts a new Kappa -means-clustering-based algorithm to select server locations and identify the optimal matching between clients and servers. The key contribution of this paper is that the proposed framework optimizes the trade-off between the service delay performance and the deployment cost under the constraints of client location distribution and the computing/storage/bandwidth capacity of each server simultaneously. To evaluate the performance of the proposed framework, a prototype system is developed and deployed in a real-world large-scale Internet. Experimental results demonstrate that 1) NetClust achieves the lower deployment cost and lower delay compared to the traditional server selection method; and 2) NetClust offers a practical and feasible solution for multimedia service providers.
C1 [Yin, Hao; Zhang, Xu; Zhan, Tongyu] Tsinghua Univ, RIIT, Beijing 100084, Peoples R China.
   [Zhang, Ying] Ericsson Res, San Jose, CA 95134 USA.
   [Min, Geyong] Univ Exeter, Coll Engn Math & Phys Sci, Exeter EX4 4QJ, Devon, England.
   [Wu, Dapeng Oliver] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 Tsinghua University; Ericsson; University of Exeter; State University
   System of Florida; University of Florida
RP Yin, H (corresponding author), Tsinghua Univ, RIIT, Beijing 100084, Peoples R China.
EM h-yin@mail.tsinghua.edu.cn; xzhang12@mails.tsinghua.edu.cn;
   zhanty@outlook.com; ying.zhang@ericsson.com; g.min@exeter.ac.uk;
   wu@ece.ufl.edu
RI Sui, Yanwei/AAH-9928-2021
OI Min, Geyong/0000-0003-1395-7314; Wu, Dapeng/0000-0003-1755-0183
CR [Anonymous], CISC VIS NETW IND FO
   [Anonymous], 2006, P 25 IEEE INT C COMP
   Buchholz S., 2004, P 2004 ACM S APPL CO, P1705
   Charikar M., 1999, PROC 40 ANN IEEE S F, P378
   Chen Y, 2002, LECT NOTES COMPUT SC, V2429, P306
   CNNIC, STAT SURV REP INT DE
   Cohen R, 2007, IEEE INFOCOM SER, P1793, DOI 10.1109/INFCOM.2007.209
   Costa M, 2004, INT CON DISTR COMP S, P178, DOI 10.1109/ICDCS.2004.1281582
   Dabek F, 2004, ACM SIGCOMM COMP COM, V34, P15, DOI 10.1145/1030194.1015471
   Dahlin M, 2003, IEEE ACM T NETWORK, V11, P300, DOI 10.1109/TNET.2003.810312
   Deb K., 2001, WIL INT S SYS OPT
   Jamin S, 2001, IEEE INFOCOM SER, P31, DOI 10.1109/INFCOM.2001.916684
   Laoutaris N, 2007, IEEE INFOCOM SER, P2144, DOI 10.1109/INFCOM.2007.248
   Li B, 1999, IEEE INFOCOM SER, P1282, DOI 10.1109/INFCOM.1999.752146
   MCVITIE DG, 1971, COMMUN ACM, V14, P486, DOI 10.1145/362619.362631
   Neves T.A., 2010, ELECT NOTES DISCRETE, V36, P89
   Ng ISE, 2002, IEEE INFOCOM SER, P170, DOI 10.1109/INFCOM.2002.1019258
   Oppenheimer D., 2006, P ANN TECH C USENIX
   Padmanabhan VN, 2001, ACM SIGCOMM COMP COM, V31, P173, DOI 10.1145/964723.383073
   Pias M, 2003, LECT NOTES COMPUT SC, V2735, P278
   Qiu LL, 2001, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2001.916655
   Radoslavov P, 2002, COMPUT COMMUN, V25, P384, DOI 10.1016/S0140-3664(01)00410-8
   Ramasubramanian V, 2009, PERF E R SI, V37, P61
   Shavitt Y, 2004, IEEE ACM T NETWORK, V12, P993, DOI 10.1109/TNET.2004.838597
   Shim J., 2010, WEB ENG PEER TO PEER, V2376, P83
   Song S, 2011, IEEE INFOCOM SER, P6, DOI 10.1109/INFCOM.2011.5935251
   Song SH, 2011, INT CON DISTR COMP S, P655, DOI 10.1109/ICDCS.2011.69
   Wu JJ, 2011, J PARALLEL DISTR COM, V71, P62, DOI 10.1016/j.jpdc.2010.08.008
   Xu JL, 2002, IEEE J SEL AREA COMM, V20, P1383, DOI 10.1109/JSAC.2002.802068
   Yin H, 2010, IEEE NETWORK, V24, P52, DOI 10.1109/MNET.2010.5510919
NR 30
TC 38
Z9 43
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2114
EP 2124
DI 10.1109/TMM.2013.2280557
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900031
DA 2024-07-18
ER

PT J
AU Sang, JT
   Mei, T
   Xu, YQ
   Zhao, C
   Xu, CS
   Li, SP
AF Sang, Jitao
   Mei, Tao
   Xu, Ying-Qing
   Zhao, Chen
   Xu, Changsheng
   Li, Shipeng
TI Interaction Design for Mobile Visual Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interaction design; interactive search; mobile visual search; user
   interfaces
AB Mobile devices are becoming ubiquitous. People take pictures via their phone cameras to explore the world on the go. In many cases, they are concerned with the picture-related information. Understanding user intent conveyed by those pictures therefore becomes important. Existing mobile applications employ visual search to connect the captured picture with the physical world. However, they only achieve limited success due to the ambiguity nature of user intent in the picture-one picture usually contains multiple objects. By taking advantage of multitouch interactions on mobile devices, this paper presents a prototype of interactive mobile visual search, named TapTell, to help users formulate their visual intent more conveniently. This kind of search leverages limited yet natural user interactions on the phone to achieve more effective visual search while maintaining a satisfying user experience. We make three contributions in this work. First, we conduct a focus study on the usage patterns and concerned factors for mobile visual search, which in turn leads to the interactive design of expressing visual intent by gesture. Second, we introduce four modes of gesture-based interactions (crop, line, lasso, and tap) and develop a mobile prototype. Third, we perform an in-depth usability evaluation on these different modes, which demonstrates the advantage of interactions and shows that lasso is the most natural and effective interaction mode. We show that TapTell provides a natural user experience to use phone camera and gesture to explore the world. Based on the observation and conclusion, we also suggest some design principles for interactive mobile visual search in the future.
C1 [Sang, Jitao; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Mei, Tao; Li, Shipeng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Xu, Ying-Qing] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Zhao, Chen] Microsoft Corp, Redmond, WA 98052 USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Microsoft
   Research Asia; Microsoft; Tsinghua University; Microsoft
RP Sang, JT (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jtsang@nlpr.ia.ac.cn; tmei@microsoft.com; yqxu@tsinghua.edu.cn;
   chzha@microsoft.com; csxu@nlpr.ia.ac.cn; spli@microsoft.com
RI Mei, Tao/GQZ-0596-2022; Li, Shipeng/AAA-3374-2020; xu, cj/HJZ-3488-2023
OI Mei, Tao/0000-0002-5990-7307; Li, Shipeng/0000-0001-5368-4256
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61232013, 61225009]
FX This work was supported in part by the National Basic Research Program
   of China under Grant2012CB316304 and National Natural Science Foundation
   of China under Grants 61232013 and 61225009. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Nicu Sebe.
CR [Anonymous], 2010, P 18 ACM INT C MULT
   [Anonymous], IJCAI P INT JOINT C
   Bahamondez EDV, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P935
   BAILEY RA, 2009, DESIGN COMP EXPT
   Broder A., 2002, SIGIR Forum, V36, P3, DOI 10.1145/792550.792552
   Brooke J., 1996, USABILITY EVALUATION, P6
   Chen DM, 2010, IEEE DATA COMPR CONF, P525, DOI 10.1109/DCC.2010.53
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Church K., 2009, P WORKSH VIS INT SOC, P1
   Church K., 2008, Proceedings of the 10th international conference on Human computer interaction with mobile devices and services, P13, DOI DOI 10.1145/1409240.1409243
   Dey A, 2010, IEEE PERVAS COMPUT, V9, P11, DOI 10.1109/MPRV.2010.10
   Girod B, 2011, IEEE MULTIMEDIA, V18, P86, DOI 10.1109/MMUL.2011.48
   Gong J., 2004, Proceedings of the 2004 DSI Annual Meeting, P3751
   Guerreiro T.J. V., 2010, Proceedings of the 12th International Conference on Human Computer Interaction with Mobile Devices and Services, P31, DOI DOI 10.1145/1851600.1851608
   Huber J., 2010, 18th ACM International Conference on Multimedia (MM'10), P341, DOI DOI 10.1145/1873951
   Jokela T, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P63
   Kane SK, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P413
   Likert R., 1932, TECHNIQUE MEASUREMEN, DOI 1933-01885-001
   Nister David, 2006, CVPR
   Petersen M. G., 2010, P 12 INT C HUM COMP, P265, DOI [https://doi.org/10.1145/1851600.1851646, DOI 10.1145/1851600.1851646]
   Schreiber D., 2011, P 13 INT C HUM COMP, P251
   Wang Y, 2011, PROCEEDINGS OF THE 4TH CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE AND SYSTEMS DYNAMICS, SSMSSD10, VOL 4, P73
   Yu F.X., 2011, Proceedings_of_the_19th_ACM_International_Conference_on_Multimedia, MM'11, P3
   Zhang N., 2011, Power and Energy Society General Meeting, 2011 IEEE, P1
   Zhang N, 2012, IEEE INT WORKSH MULT, P238, DOI 10.1109/MMSP.2012.6343447
   Zimmerman J, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P493
NR 26
TC 24
Z9 26
U1 3
U2 69
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1665
EP 1676
DI 10.1109/TMM.2013.2268052
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800017
DA 2024-07-18
ER

PT J
AU Maugey, T
   Frossard, P
AF Maugey, Thomas
   Frossard, Pascal
TI Interactive Multiview Video System With Low Complexity 2D Look Around at
   Decoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 2D look around viewing; interactivity; multiview video coding; view
   synthesis
ID LAYERED DEPTH VIDEO; DISPLAYS; FRAME; 3DTV
AB Multiview video with interactive 2D look around at the receiver is a challenging application with several issues in terms of effective use of storage and bandwidth resources, reactivity of the system, quality of the viewing experience and system complexity. The impression of 3D immersion is highly dependent on the smoothness of the navigation and thus on the number of 2D viewpoints. The classical decoding system for generating virtual views first projects a reference or encoded frame to a given viewpoint and then fills in the holes due to potential occlusions. This last step still constitutes a complex operation with specific software or hardware at the receiver and requires a certain quantity of information from the neighboring frames for ensuring consistency between the virtual images. In this work we propose a new approach that shifts most of the burden due to interactivity from the decoder to the encoder, by anticipating the navigation of the decoder and sending auxiliary information that guarantees temporal and interview consistency. This leads to an additional cost in terms of transmission rate and storage, which we minimize by using optimization techniques based on the user behavior modeling. We show by experiments that the proposed system represents a valid solution for interactive multiview systems with classical decoders.
C1 [Maugey, Thomas; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Inst Elect Engn, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Maugey, T (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Inst Elect Engn, CH-1015 Lausanne, Switzerland.
EM thomas.maugey@epfl.ch; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
FU Swiss National Science Foundation [200021-126894]; Swiss National
   Science Foundation (SNF) [200021_126894] Funding Source: Swiss National
   Science Foundation (SNF)
FX This work was supported in part by the Swiss National Science
   Foundation, under grant 200021-126894. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Zhihai (Henry) He.
CR [Anonymous], 2007, JOINT SCAL VID MOD J
   Bartczak B, 2011, IEEE T BROADCAST, V57, P477, DOI 10.1109/TBC.2011.2120790
   Benzie P, 2007, IEEE T CIRC SYST VID, V17, P1647, DOI 10.1109/TCSVT.2007.905377
   Bjontegaard G., 2001, P 13 VCEG M33 M
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cheung G., 2009, P INT PACK VID WORKS
   Cheung G., 2008, P IEEE INT WORKSH MU
   Cheung G., 2009, P IEEE INT C IM PROC
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung N., 2006, P SPIE VISUAL COMMUN
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Daribo I., 2010, P IEEE INT WORKSH MU
   Daribo I, 2011, IEEE T BROADCAST, V57, P533, DOI 10.1109/TBC.2011.2125110
   Davidoiu V., 2010, P INT C AC SPEECH SI
   de With P. H. N., 2007, P IEEE INT C IM PROC
   Holliman NS, 2011, IEEE T BROADCAST, V57, P362, DOI 10.1109/TBC.2011.2130930
   Huang Z., 2011, P ACM C MULT SYST MM
   *ISO IEC MPEG ITU, 2007, JOINT MULT VID MOD J
   Jian-Guang Lou, 2005, 13th Annual ACM International Conference on Multimedia, P161
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kim WS, 2010, P SPIE INT SOC OPTIC
   Kimata H., 2004, NTT TECHNICAL REV, V2, P21
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Liu YQ, 2010, J INSECT SCI, V10, P1, DOI 10.1673/031.010.14140
   Maugey T., 2011, P IEEE INT C IM PROC
   Merkle P., 2008, P 3D TV C IST TURK M
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Müller K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/438148
   Oh H, 2006, LECT NOTES COMPUT SC, V4319, P898
   Oh K., 2009, P PICT COD S PCS CHI
   Petrazzuoli G., 2011, P IEEE INT C IM PROC
   Shao F., 2011, OPT ENG, V50
   Shi S., 2010, P ACM INT C MULT FIR
   Shi S., 2009, P ACM INT C MULT BEI
   Shimizu S, 2007, IEEE T CIRC SYST VID, V17, P1485, DOI 10.1109/TCSVT.2007.903773
   Smolic A, 2005, P IEEE, V93, P98, DOI 10.1109/JPROC.2004.839608
   Tekalp AM, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.905878
   Tian D., 2009, P SPIE INT SOC OPTIC, V7443
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiu X., 2011, P IEEE INT C IM PROC
NR 43
TC 19
Z9 19
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1070
EP 1082
DI 10.1109/TMM.2013.2246147
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600011
OA Green Published
DA 2024-07-18
ER

PT J
AU Dong, W
   Lepri, B
   Pianesi, F
   Pentland, A
AF Dong, Wen
   Lepri, Bruno
   Pianesi, Fabio
   Pentland, Alex
TI Modeling Functional Roles Dynamics in Small Group Interactions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Functional roles; influence model; non-linguistic behavior; multimodal
   analysis
ID AGREEMENT
AB The paper addresses the automatic recognition of social and task-oriented functional roles in small-group meetings, focusing on several properties: a) the importance of non-linguistic behaviors, b) the relative time-consistency of the social roles played by a given person during the course of a meeting, and c) the interplays and mutual constraints among the roles enacted by the different participants in a social encounter. In particular, this paper proposes that the Influence Model framework can address these properties of functional roles, and compares the performance obtained by this framework to the performances of models that consider only property (a) (SVM), and to those that address both (a) and (b) (HMM). The results obtained confirm our expectations: the classification of social functional roles improves if models account for temporal dependencies among the roles played by the same subject, for the time properties of the roles played by each individual, and for the mutual constraints among the roles of different group members. The two versions of the Influence Model (IM and newIM), which encode all three properties together, outperform both the SVM and the HMM on most of the figures of merit used. Of particular interest is the capability of the Influence Model to obtain good or very good results on the less-populated classes-Orienteer and Seeker for the task area, and Attacker and Supporter for the socio-emotional area.
C1 [Dong, Wen; Lepri, Bruno; Pentland, Alex] MIT Media Lab, Cambridge, MA 02139 USA.
   [Dong, Wen] Northeastern Univ, Boston, MA 02115 USA.
   [Lepri, Bruno; Pianesi, Fabio] FBK, I-38100 Trento, Italy.
C3 Massachusetts Institute of Technology (MIT); Northeastern University;
   Fondazione Bruno Kessler
RP Dong, W (corresponding author), MIT Media Lab, Cambridge, MA 02139 USA.
EM wdong@media.mit.edu; lepri@fbk.eu; pianesi@fbk.eu; sandy@media.mit.edu
RI Dong, Wen/AAQ-6057-2021
OI Dong, Wen/0000-0001-8923-2227
FU UE under the CHIL (FP6) project; PERSI project
FX This work was supported in part by the UE under the CHIL (FP6) project.
   The work of B. Lepri was supported by PERSI project inside the Marie
   Curie COFUND D 7th Framework. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Eckehard G. Steinbach.
CR [Anonymous], 2008, P 10 INT C MULT INT
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   [Anonymous], 2008, Proc. ACM Multimedia, DOI [10.1145/1459359.1459462, DOI 10.1145/1459359.1459462]
   [Anonymous], 539 MIT MED LAB VIS
   [Anonymous], 1950, AM SOCIOL REV
   [Anonymous], 2009, P 17 ACM INT C MULT
   Banerjee S., 2004, P 8 INT C SPOK LANG, V2189-2192
   Barzilay R, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P679
   BASU S, 2002, THESIS MIT CAMBRIDGE
   Benne KD, 1948, J SOC ISSUES, V4, P41, DOI 10.1111/j.1540-4560.1948.tb01783.x
   Biddle B. J., 2013, Role theory: Expectations, identities, and behaviors
   Boersma P., 2018, Praat: doing phonetics by computer, DOI DOI 10.1097/AUD.0B013E31821473F7
   Caputo B., 2002, Proceedings of NIPS workshop on Statistical methods for computational experiments in visual processing and computer vision
   Carli G., 1992, P ICSPAT, P1011
   Chippendale P, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P487
   Choudhury T., 2004, P NIPS
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Curhan JR, 2007, J APPL PSYCHOL, V92, P802, DOI 10.1037/0021-9010.92.3.802
   Davis JW, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P39, DOI 10.1109/EVENT.2001.938864
   Dong W., 2009, P AAAI SPRING S 2009
   Dong W, 2007, LECT NOTES COMPUT SC, V4451, P170
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   Doyle M., 1982, MAKE MEETINGS WORK N, P1976
   Eagle N., 2005, J PERSONAL UBIQ COMP
   Favre S., 2009, P 17 ACM INT C MULT, P585
   Favre S., 2008, Proc. ACM International Conference on Multimodal Interfaces, P29
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Hardin JW, 2003, GEN ESTIMATING EQUAT
   HARE AP, 1994, SMALL GR RES, V25, P433, DOI 10.1177/1046496494253005
   Heerey EA, 2007, J ABNORM PSYCHOL, V116, P125, DOI 10.1037/0021-843X.116.1.125
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   Katz D., 1978, The Social Psychology of Organizations II
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   Lepri B., 2010, P INT C MULT INT
   Lepri B, 2009, LECT NOTES COMPUT SC, V5535, P114, DOI 10.1007/978-3-642-02247-0_13
   Maskey S., 2003, P INTERSPEECH
   MATENA L, 2008, P MOBILEHCI 2008 10, P503
   McGrath J.E., 1984, GROUPS INTERACTION P
   Pan W, 2012, IEEE SIGNAL PROC MAG, V29, P77, DOI 10.1109/MSP.2011.942737
   Pianesi F, 2008, PERS UBIQUIT COMPUT, V12, P181, DOI 10.1007/s00779-007-0144-5
   Pianesi F, 2007, LANG RESOUR EVAL, V41, P409, DOI 10.1007/s10579-007-9060-6
   Raducanu B, 2009, INT CONF ACOUST SPEE, P1949, DOI 10.1109/ICASSP.2009.4959992
   Salamin H., 2010, Proceedings of the international conference on Multimedia, P847
   Salazar AJ, 1996, SMALL GR RES, V27, P475, DOI 10.1177/1046496496274001
   Sanchez-Cortes D., IEEE T MULT IN PRESS
   The American Heritage&REG;, 2012, DICT ENGL LANG
   Vinciarelli A, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1551
   Vinciarelli A, 2006, 2006 IEEE International Conference on Multimedia and Expo - ICME 2006, Vols 1-5, Proceedings, P1801, DOI 10.1109/ICME.2006.262902
   Vinciarelli A, 2011, IEEE SYS MAN CYBERN, P374, DOI 10.1109/ICSMC.2011.6083694
   Vinciarelli Alessandro., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P261, DOI DOI 10.1145/1291233.1291287
   Weng CY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1403
   Wilson T., 2011, P 16 INT C INT US IN, P419
   Yngve VictorH., 1970, PAPERS 6 REGIONAL M, V6, P567
   Zancanaro M., 2006, Proceedings of the 8th international conference on Multimodal interfaces, P28
NR 54
TC 24
Z9 25
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 83
EP 95
DI 10.1109/TMM.2012.2225039
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600007
OA Green Published
DA 2024-07-18
ER

PT J
AU Lin, CH
   Shieh, CK
   Hwang, WS
AF Lin, Cheng-Han
   Shieh, Ce-Kuen
   Hwang, Wen-Shyang
TI An Access Point-Based FEC Mechanism for Video Transmission Over Wireless
   LANs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE FEC; video quality; wireless access point
ID SATURATION THROUGHPUT ANALYSIS; LAYER ERROR CONTROL; PERFORMANCE;
   TRANSPORT; PROTOCOL
AB Forward Error Correction (FEC) is one of the most common means of performing packet error recovery in data transmissions. FEC schemes typically tune the FEC rate in accordance with feedback information provided by the receiver. However, the feedback and FEC rate calculation processes inevitably have a finite duration, and thus the FEC rate implemented at the sender may not accurately reflect the current state of the network. Thus, this paper proposes an Enhanced Random Early Detection Forward Error Correction (ERED-FEC) mechanism to improve the quality of video transmissions over Wireless Local Area Networks (WLANs). In contrast to most FEC schemes, the FEC redundancy rate is calculated directly at the Access Point (AP). Moreover, the redundancy rate is tuned in accordance with both the wireless channel condition (as indicated by the number of packet retransmissions) and the network traffic load (as indicated by the AP queue length). The experimental results show that the proposed ERED-FEC mechanism achieves a significant improvement in the video quality compared to existing FEC schemes without introducing an excessive number of redundant packets into the network.
C1 [Lin, Cheng-Han; Shieh, Ce-Kuen] Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
   [Hwang, Wen-Shyang] Natl Kaohsiung Univ Appl Sci, Dept Elect Engn, Kaohsiung 807, Taiwan.
C3 National Cheng Kung University; National Kaohsiung University of Science
   & Technology
RP Lin, CH (corresponding author), Natl Cheng Kung Univ, Dept Elect Engn, Tainan 701, Taiwan.
EM jhlin5@hpds.ee.ncku.edu.tw; shieh@ee.ncku.edu.tw;
   wsh-wang@mail.ee.kuas.edu.tw
FU National Science Council (NSC), Taiwan [NSC 100-2221-E-151-036]
FX This work was supported by the National Science Council (NSC), Taiwan
   under Contract No. NSC 100-2221-E-151-036. The corresponding author is
   W.-S. Hwang. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhihai (Henry) He.
CR [Anonymous], 802162004 IEEE
   Argyriou A, 2008, IEEE T MULTIMEDIA, V10, P1121, DOI 10.1109/TMM.2008.2001371
   Bajic IV, 2007, IEEE T BROADCAST, V53, P276, DOI 10.1109/TBC.2006.889211
   Chan SHG, 2006, IEEE T MULTIMEDIA, V8, P370, DOI 10.1109/TMM.2005.864340
   Deng DJ, 2009, INT J COMMUN SYST, V22, P119, DOI 10.1002/dac.962
   Deng DJ, 2008, IEEE T WIREL COMMUN, V7, P5129, DOI 10.1109/T-WC.2008.071259
   Dong XJ, 2005, IEEE COMMUN LETT, V9, P100, DOI 10.1109/LCOMM.2005.02011
   Du HQ, 2009, 2009 5TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, NETWORKING AND MOBILE COMPUTING, VOLS 1-8, P4808
   Guruswami V, 1999, IEEE T INFORM THEORY, V45, P1757, DOI 10.1109/18.782097
   Han L., 2009, KSII INT C INT DEC, P209
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P1051, DOI 10.1109/TCSVT.2006.881198
   IEEE, 2005, 80211E2005 IEEE
   Jurca D, 2009, IEEE T CIRC SYST VID, V19, P1315, DOI 10.1109/TCSVT.2009.2022800
   Ke C. H., 2007, J MOBILE MULTIMEDIA, V3
   Kim JG, 2000, IEEE ACM T NETWORK, V8, P337, DOI 10.1109/90.851980
   Koumaras H, 2010, J VIS COMMUN IMAGE R, V21, P139, DOI 10.1016/j.jvcir.2009.07.005
   Lin CH, 2009, TELECOMMUN SYST, V42, P223, DOI 10.1007/s11235-009-9182-9
   Lin C. H., 2006, P INT C NETW SERV IC
   Lin CH, 2008, IEEE T BROADCAST, V54, P517, DOI 10.1109/TBC.2008.2001713
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Mitchell J.L., 1996, MPEG VIDEO COMPRESSI
   Nafaa A, 2005, IEEE ICC, P1390
   Nafaa A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1431, DOI 10.1109/ICC.2004.1312748
   Ni Q, 2005, WIREL COMMUN MOB COM, V5, P945, DOI 10.1002/wcm.358
   Paavola J, 2007, IEEE T BROADCAST, V53, P263, DOI 10.1109/TBC.2007.891694
   Park KH, 1998, IEEE IC COMP COM NET, P196, DOI 10.1109/ICCCN.1998.998777
   Po-Chang Huang, 2009, Proceedings of the 2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing. IIH-MSP 2009, P989, DOI 10.1109/IIH-MSP.2009.287
   ROCA V, 2004, RR5225 INRIA
   Takahata K, 2003, 23RD INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING SYSTEMS WORKSHOPS, P594
   Tun M, 2007, IEEE T BROADCAST, V53, P649, DOI 10.1109/LPT.2007.903636
   Wu HT, 2002, IEEE INFOCOM SER, P599, DOI 10.1109/INFCOM.2002.1019305
   Yang XK, 2005, IEEE T MULTIMEDIA, V7, P753, DOI 10.1109/TMM.2005.846782
NR 32
TC 21
Z9 24
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 195
EP 206
DI 10.1109/TMM.2012.2225028
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600016
DA 2024-07-18
ER

PT J
AU Jiao, BX
   Yang, LJ
   Xu, JZ
   Tian, Q
   Wu, F
AF Jiao, Binxing
   Yang, Linjun
   Xu, Jizheng
   Tian, Qi
   Wu, Feng
TI Visually Summarizing Web Pages Through Internal and External Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual summarization; web page summarization
AB Visually summarizing web pages is an attractive approach that provides users an effective and friendly interface to identify desired contents at a first glance for search and re-finding tasks. Using dominant images in web pages is generally reliable for this purpose. However, dominant images are often unavailable in many web pages. To solve this problem, we first propose a new approach to summarize those web pages without any dominant images by retrieving relevant external images from the Internet. However, relevant external images are sometimes unreliable. To take the advantages of these two kinds of images, we further propose a clustering based algorithm to select the best summarization among all of internal and external images. This algorithm leverages relevance and dominance of images as the prior information. Experimental results show that our approach achieves 0.098 and 0.082 NDCG(1) gain on a human labeled data set, compared with relevant external image and dominant image, respectively. Our user study also indicates that the images selected by our algorithm are useful as the summarization of web pages.
C1 [Jiao, Binxing] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Jiao, Binxing; Yang, Linjun; Xu, Jizheng; Wu, Feng] Microsoft Res Asia, Beijing 100081, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia; University of Texas
   System; University of Texas at San Antonio (UTSA)
RP Jiao, BX (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM bxjiao@gmail.com; linjuny@microsoft.com; jzxu@microsoft.com;
   qitian@cs.utsa.edu; fengwu@mi-crosoft.com
RI Xu, Jizheng/JDD-5152-2023; Wu, Feng/KCY-3017-2024
OI Jiao, Binxing/0000-0003-4710-0095
FU NSFC [60933013]; NHTRDPC [2010ZX03004-003]; ARO [W911BF-12-1-0057]; NSF
   IIS [1052851]; Google; FXPAL; NEC Laboratories of America; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1052851] Funding Source: National Science Foundation
FX This work was supported in part by the NSFC (No. 60933013) and the
   NHTRDPC (No. 2010ZX03004-003). The work of Q. Tian was supported in part
   by ARO grant W911BF-12-1-0057, NSF IIS 1052851, Faculty Research Awards
   by Google, FXPAL, and NEC Laboratories of America, respectively. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Jiangchuan (JC) Liu.
CR [Anonymous], 2009, Search engines: Information retrieval in practice
   [Anonymous], 2009, P 17 ACM INT C MULTI
   Aula A., 2010, P INT C WORLD WIDE W, P51
   Barsalou L.W., 1987, Concepts and conceptual development: Ecological and intellectual factors in categorization, P101
   Brin S, 1998, COMPUT NETWORKS ISDN, V30, P107, DOI 10.1016/S0169-7552(98)00110-X
   Chen M, 2005, P 14 ACM INT C INF K, P277
   Cockburn A, 2001, INT J HUM-COMPUT ST, V54, P903, DOI 10.1006/ijhc.2001.0459
   Cockburn A., 1999, WEBVIEW GRAPHICAL AI
   Dziadosz S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P365
   EROL B., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, P231
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Jiao BX, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P499
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kaasten S, 2002, BCS CONF SERIES, P247
   Kopetzky T, 1999, COMPUT NETW, V31, P1525, DOI 10.1016/S1389-1286(99)00050-X
   Lam Heidi., 2005, P SIGCHI C HUMAN FAC, P681
   Li Z., 2008, Proceeding of the 17th international conference on World Wide Web, WWW '08, P21, DOI DOI 10.1145/1367497.1367501
   Liu R, 2010, LECT NOTES COMPUT SC, V5996, P485
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Maekawa Takuya, 2006, P 15 INT C WORLD WID, P43, DOI [10.1145/1135777.1135789, DOI 10.1145/1135777.1135789]
   Mei T., 2007, MSRA USTC SJTU TRECV
   Obendorf H, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P597
   Rosch Eleanor, 1973, Cognitive Development and the Acquisition of Language, P111
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Teevan J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P2023
   Woodruff A., 2001, CHI 2001 Conference Proceedings. Conference on Human Factors in Computing Systems, P198, DOI 10.1145/365024.365098
   Woodruff A, 2002, J AM SOC INF SCI TEC, V53, P172, DOI 10.1002/asi.10029
   Yu Q, 2007, LECT NOTES COMPUT SC, V4425, P645
NR 29
TC 3
Z9 3
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1673
EP 1683
DI 10.1109/TMM.2012.2198457
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400016
DA 2024-07-18
ER

PT J
AU Dai, R
   Wang, P
   Akyildiz, IF
AF Dai, Rui
   Wang, Pu
   Akyildiz, Ian F.
TI Correlation-Aware QoS Routing With Differential Coding for Wireless
   Video Sensor Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality-of-service (QoS); routing; spatial correlation; video
   compression; wireless video sensor networks
ID QUALITY; LAYER
AB The spatial correlation of visual information retrieved from distributed camera sensors leads to considerable data redundancy in wireless video sensor networks, resulting in significant performance degradation in energy efficiency and quality-of-service (QoS) satisfaction. In this paper, a correlation-aware QoS routing algorithm (CAQR) is proposed to efficiently deliver visual information under QoS constraints by exploiting the correlation of visual information observed by different camera sensors. First, a correlation-aware inter-node differential coding scheme is designed to reduce the amount of traffic in the network. Then, a correlation-aware load balancing scheme is proposed to prevent network congestion by splitting the correlated flows that cannot be reduced to different paths. Finally, the correlation-aware schemes are integrated into an optimization QoS routing framework with an objective to minimize energy consumption subject to delay and reliability constraints. Simulation results demonstrate that the proposed routing algorithm achieves efficient delivery of visual information under QoS constraints in wireless video sensor networks.
C1 [Dai, Rui; Wang, Pu; Akyildiz, Ian F.] Georgia Inst Technol, Sch Elect & Comp Engn, Broadband Wireless Networking Lab, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Dai, R (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Broadband Wireless Networking Lab, Atlanta, GA 30332 USA.
EM rui.dai@gatech.edu; pwang40@ece.gatech.edu; ian@ece.gatech.edu
RI Wang, Pu/ABD-6654-2021; Akyildiz, Ian/ABD-5310-2021; Akyildiz, Ian
   F/G-7136-2011
OI Dai, Rui/0000-0001-6620-7862; Wang, Pu/0000-0003-1988-5016
FU U.S. National Science Foundation (NSF) [ECCS-0701559]
FX This work was supported by the U.S. National Science Foundation (NSF)
   under Grant No. ECCS-0701559.
CR Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   Chen M, 2007, COMPUT COMMUN, V30, P3368, DOI 10.1016/j.comcom.2007.01.016
   Chipara O, 2006, INT WORKSH QUAL SERV, P83, DOI 10.1109/IWQOS.2006.250454
   Cristescu R, 2004, IEEE INFOCOM SER, P2571
   Dai R, 2009, IEEE T MULTIMEDIA, V11, P1148, DOI 10.1109/TMM.2009.2026100
   Devarajan D, 2008, P IEEE, V96, P1625, DOI 10.1109/JPROC.2008.928759
   Ergen S.C., 2005, TDMA SCHEDULING ALGO
   Felemban E, 2006, IEEE T MOBILE COMPUT, V5, P738, DOI 10.1109/TMC.2006.79
   Feng W., 2003, P ACM MULT BERK CA N
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   Goto K, 2006, IEEE WRK SIG PRO SYS, P101, DOI 10.1109/SIPS.2006.352563
   He Tian., 2003, Proceedings of the 23rd International Conference on Distributed Computing Systems, Providence, Rhode Island, P46
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Kulkarni P., 2005, P ACM MULT SING NOV
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   Liu JN, 2006, MOBICOM 2006, P310
   Liu Yang, 2005, P IEEE INT C MOB ADH
   Luo H, 2006, IEEE T MOBILE COMPUT, V5, P1620, DOI 10.1109/TMC.2006.171
   Melodia T, 2008, IEEE INFOCOM SER, P121
   Pattem S, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P28
   Seeling P, 2004, IEEE COMMUN SURV TUT, V6, P58, DOI 10.1109/COMST.2004.5342293
   Soro S, 2009, ADV MULTIMED, V2009, DOI 10.1155/2009/640386
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang A, 2002, IEEE SIGNAL PROC MAG, V19, P68, DOI 10.1109/MSP.2002.1012351
   Wang P., 2010, P IEEE INFOCOM 2010
   Ziviani A, 2005, MULTIMED TOOLS APPL, V26, P59, DOI 10.1007/s11042-005-6849-4
NR 27
TC 48
Z9 57
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1469
EP 1479
DI 10.1109/TMM.2012.2194992
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600008
DA 2024-07-18
ER

PT J
AU Schier, M
   Welzl, M
AF Schier, Michael
   Welzl, Michael
TI Optimizing Selective ARQ for H. 264 Live Streaming: A Novel Method for
   Predicting Loss-Impact in Real Time
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE End-to-end distortion modeling; error propagation; H.264/AVC; live video
   streaming; packet loss; selective automatic repeat request (ARQ)
ID VIDEO TRANSMISSION; DISTORTION; MODE; PACKETIZATION; ALGORITHM;
   DECISION; CODES
AB This work proposes a quality-oriented, real-time capable prioritization technique for media units of H.264/AVC video streams. The derivation of estimates is based on the analysis of the macroblock partitioning, the spatial extents of temporal dependencies, and the length and strength of prediction chains existing among macroblocks, thus incorporating the expected impact of error propagation. It is demonstrated how the prioritization scheme can be beneficially integrated into live streaming systems which are characterized by tight timing constraints, with the focus on content-aware selective automatic repeat request mechanisms. Additionally, it is shown how potentially limited feedback can be used to adapt the estimation process to leverage prediction preciseness. The approach is compared against existing techniques in terms of practicability and efficiency, and tested under independent and bursty loss conditions in a wired and a wireless test setup. Moreover, the performance is examined when low-latency and constant bitrate video settings are enforced by using x264's novel encoding feature periodic-intra-refresh. Results of both experiments and simulations indicate that the proposed technique outperforms all reference techniques in nearly all test cases, and that the video quality can be further improved by incorporating receiver feedback.
C1 [Schier, Michael] Univ Innsbruck, Inst Comp Sci, A-6020 Innsbruck, Austria.
   [Welzl, Michael] Univ Oslo, Dept Informat, N-0316 Oslo, Norway.
C3 University of Innsbruck; University of Oslo
RP Schier, M (corresponding author), Univ Innsbruck, Inst Comp Sci, A-6020 Innsbruck, Austria.
EM michael.schier@uibk.ac.at; michawe@ifi.uio.no
RI Welzl, Michael/AAR-9946-2020
OI Welzl, Michael/0000-0001-8179-599X
CR [Anonymous], P INT S CIRC SYST VA
   [Anonymous], 6 MOSC STAT U GRAPH
   [Anonymous], HINDAWI J ADV MULTIM
   [Anonymous], P LIN C AUSTR APR
   [Anonymous], P IEEE WOWMOM LUCC I
   [Anonymous], INT PACK VID WORKSH
   [Anonymous], P INT PACK VID WORKS
   [Anonymous], P IEEE GLOB C HON HI
   [Anonymous], P IEEE INF C NEW YOR
   [Anonymous], P INT PACK VID WORKS
   [Anonymous], IEEE COMPUTER COMMUN
   [Anonymous], H264 ITU
   [Anonymous], HINDAWI J ADV MU JUN
   [Anonymous], P IEEE INF MOVID RIO
   Babich F, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324290
   Baldi M, 2008, IEEE T BROADCAST, V54, P542, DOI 10.1109/TBC.2008.2000553
   Begen AC, 2007, IEEE T MULTIMEDIA, V9, P332, DOI 10.1109/TMM.2006.886282
   Bucciol P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1288, DOI 10.1109/ICC.2004.1312720
   Chang SH, 2007, IEEE T BROADCAST, V53, P79, DOI 10.1109/TBC.2006.887170
   Chen WT, 2008, IEEE WCNC, P3133
   De Vito F, 2004, 2004 IEEE 6TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P79
   ELLIOTT EO, 1963, AT&T TECH J, V42, P1977, DOI 10.1002/j.1538-7305.1963.tb00955.x
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Huang YH, 2010, IEEE T CIRC SYST VID, V20, P1122, DOI 10.1109/TCSVT.2010.2057018
   Huo LS, 2007, IEEE T CONSUM ELECTR, V53, P417, DOI 10.1109/TCE.2007.381710
   Huszák A, 2008, COMPUT COMMUN, V31, P2676, DOI 10.1016/j.comcom.2008.02.033
   Korhonen J, 2009, SIGNAL PROCESS-IMAGE, V24, P229, DOI 10.1016/j.image.2008.12.005
   Lee KK, 2004, IEEE T VEH TECHNOL, V53, P929, DOI 10.1109/TVT.2004.825769
   Li ZC, 2009, IEEE T CIRC SYST VID, V19, P917, DOI 10.1109/TCSVT.2009.2022806
   Loguinov D, 2002, IEEE INFOCOM SER, P723, DOI 10.1109/INFCOM.2002.1019318
   Masala E, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P345
   Masala E, 2009, IEEE T MULTIMEDIA, V11, P972, DOI 10.1109/TMM.2009.2021784
   Paxson Vern., 2000, RFC 2988: Computing TCP's retransmission timer
   Rhee I, 2000, IEEE J SEL AREA COMM, V18, P1033, DOI 10.1109/49.848254
   Schier M., 2011, 2011 IEEE Consumer Communications and Networking Conference (CCNC 2011), P525, DOI 10.1109/CCNC.2011.5766530
   Schier Michael, 2010, 2010 International Conference on Multimedia Computing and Information Technology (MCIT 2010), P53, DOI 10.1109/MCIT.2010.5444855
   Shen YS, 2006, IEEE T IMAGE PROCESS, V15, P273, DOI 10.1109/TIP.2005.860598
   Sun G, 2008, 2008 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2008), VOLS 1-4, P1735, DOI 10.1109/APCCAS.2008.4746375
   Talari A., 2009, PROC IEEE MILCOM C, P1
   Tourrilhes Jean., WIRELESS TOOLS LINUX
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Yu ACW, 2008, IEEE T CIRC SYST VID, V18, P186, DOI 10.1109/TCSVT.2007.913970
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang WZ, 2009, IEEE T CONSUM ELECTR, V55, P1982, DOI 10.1109/TCE.2009.5373759
   Zorzi M, 2001, IEEE T VEH TECHNOL, V50, P12, DOI 10.1109/25.917864
NR 47
TC 13
Z9 18
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 415
EP 430
DI 10.1109/TMM.2011.2178235
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xing, LY
   You, JY
   Ebrahimi, T
   Perkis, A
AF Xing, Liyuan
   You, Junyong
   Ebrahimi, Touradj
   Perkis, Andrew
TI Assessment of Stereoscopic Crosstalk Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crosstalk perception; objective metric; perceptual attribute; subjective
   evaluation
AB Stereoscopic three-dimensional (3-D) services do not always prevail when compared with their two-dimensional (2-D) counterparts, though the former can provide more immersive experience with the help of binocular depth. Various specific 3-D artefacts might cause discomfort and severely degrade the Quality of Experience (QoE). In this paper, we analyze one of the most annoying artefacts in the visualization stage of stereoscopic imaging, namely, crosstalk, by conducting extensive subjective quality tests. A statistical analysis of the subjective scores reveals that both scene content and camera baseline have significant impacts on crosstalk perception, in addition to the crosstalk level itself. Based on the observed visual variations during changes in significant factors, three perceptual attributes of crosstalk are summarized as the sensorial results of the human visual system (HVS). These are shadow degree, separation distance, and spatial position of crosstalk. They are classified into two categories: 2-D and 3-D perceptual attributes, which can be described by a Structural SIMilarity (SSIM) map and a filtered depth map, respectively. An objective quality metric for predicting crosstalk perception is then proposed by combining the two maps. The experimental results demonstrate that the proposed metric has a high correlation (over 88%) when compared with subjective quality scores in a wide variety of situations.
C1 [Xing, Liyuan; You, Junyong; Perkis, Andrew] Norwegian Univ Sci & Technol NTNU, Ctr Quantifiable Qual Serv Commun Syst Q2S, N-7491 Trondheim, Norway.
   [Ebrahimi, Touradj] Ecole Polytech Fed Lausanne, Multimedia Signal Proc Grp MMSPG, CH-1015 Lausanne, Switzerland.
C3 Norwegian University of Science & Technology (NTNU); Swiss Federal
   Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne
RP Xing, LY (corresponding author), Norwegian Univ Sci & Technol NTNU, Ctr Quantifiable Qual Serv Commun Syst Q2S, N-7491 Trondheim, Norway.
EM liyuan@q2s.ntnu.no; junyong.you@ieee.org; touradj.ebrahimi@epfl.ch;
   andrew@iet.ntnu.no
RI Perkis, Andrew/AAI-4792-2020
OI Perkis, Andrew/0000-0003-1414-2870; Ebrahimi,
   Touradj/0000-0002-9900-3687
FU Research Council of Norwegian University of Science and Technology;
   UNINETT
FX Manuscript received March 24, 2011; revised July 20, 2011; accepted
   October 01, 2011. Date of publication October 18, 2011; date of current
   version March 21, 2012. This work was supported by the Research Council
   of Norwegian University of Science and Technology and UNINETT. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Weisi Lin.
CR [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], P INT WORKSH VID PRO
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], 2000, SUB ASS STER TEL PIC
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Boev A., SOFTWARE SIMULATION
   Boev A., 2009, P STEREOSCOPIC DISPL, p72371F
   Goldmann L, 2010, PROC SPIE, V7526, DOI 10.1117/12.839438
   Gorley P., 2008, P C STER DISPL APPL, V6803
   Huang KC, 2003, P SOC PHOTO-OPT INS, V5006, P247, DOI 10.1117/12.474139
   Kim D., 2009, P STER DISPL APPL 20, V7237
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lipton L., 1987, Proceedings of the SPIE - The International Society for Optical Engineering, V761, P75, DOI 10.1117/12.940123
   Olsson R., 2007, P 3DTV C TRU VIS CAP
   Pastoor S., 1995, Proceedings of 2nd Int. Display Workshop, P69
   Sazzad Z. M. P., 2009, P INT WORKSH QUAL MU
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Seuntiëns PJH, 2005, DISPLAYS, V26, P177, DOI 10.1016/j.displa.2005.06.005
   SSIM Implementation, SSIM IMPL
   Tanimoto M., 2008, JTC1SC29WG11 ISOIEC
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woods A., 2010, P C 3 DIM SYST APPL
   Xing LY, 2010, PROC SPIE, V7744, DOI 10.1117/12.863365
   Xing LY, 2010, IEEE IMAGE PROC, P4033, DOI 10.1109/ICIP.2010.5649402
   You J., 2010, HIGH QUALITY VISUAL
NR 26
TC 41
Z9 46
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 326
EP 337
DI 10.1109/TMM.2011.2172402
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500008
DA 2024-07-18
ER

PT J
AU Merler, M
   Huang, B
   Xie, LX
   Hua, G
   Natsev, A
AF Merler, Michele
   Huang, Bert
   Xie, Lexing
   Hua, Gang
   Natsev, Apostol
TI Semantic Model Vectors for Complex Video Event Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complex video events; event recognition; high-level descriptor
ID SCENE
AB We propose semantic model vectors, an intermediate level semantic representation, as a basis for modeling and detecting complex events in unconstrained real-world videos, such as those from YouTube. The semantic model vectors are extracted using a set of discriminative semantic classifiers, each being an ensemble of SVM models trained from thousands of labeled web images, for a total of 280 generic concepts. Our study reveals that the proposed semantic model vectors representation outperforms-and is complementary to-other low-level visual descriptors for video event modeling. We hence present an end-to-end video event detection system, which combines semantic model vectors with other static or dynamic visual descriptors, extracted at the frame, segment, or full clip level. We perform a comprehensive empirical study on the 2010 TRECVID Multimedia Event Detection task (http://www.nist.gov/itl/iad/mig/med10.cfm), which validates the semantic model vectors representation not only as the best individual descriptor, outperforming state-of-the-art global and local static features as well as spatio-temporal HOG and HOF descriptors, but also as the most compact. We also study early and late feature fusion across the various approaches, leading to a 15% performance boost and an overall system performance of 0.46 mean average precision. In order to promote further research in this direction, we made our semantic model vectors for the TRECVID MED 2010 set publicly available for the community to use (http://www1.cs.columbia.edu/similar to mmerler/SMV.html).
C1 [Merler, Michele] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Huang, Bert] Univ Maryland, Dept Comp Sci, College Pk, MD 20740 USA.
   [Xie, Lexing] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT 0200, Australia.
   [Hua, Gang] Stevens Inst Technol, Dept Comp Sci, Hoboken, NJ 07030 USA.
   [Natsev, Apostol] IBM TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
C3 Columbia University; University System of Maryland; University of
   Maryland College Park; Australian National University; Stevens Institute
   of Technology; International Business Machines (IBM)
RP Merler, M (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
EM mmerler@cs.columbia.edu; bert@cs.umd.edu; lexing.xie@anu.edu.au;
   ghua@stevens.edu; natsev@us.ibm.com
RI ; Huang, Bert/E-2576-2016
OI Xie, Lexing/0000-0001-8319-0118; Merler, Michele/0000-0002-4358-8671;
   Huang, Bert/0000-0002-8548-7246
CR [Anonymous], COMPUT VIS IMAGE UND
   [Anonymous], COMPUTER VI IN PRESS
   [Anonymous], 2009, Proc. ACM International Confence on Multimedia
   [Anonymous], 2010, P INT C COMP VIS PAT
   [Anonymous], 2010, Cisco Visual Networking Index: Forecast and Methodology, 2009-2014
   [Anonymous], GREAT SCOTT 35 HOURS
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], P ACM SIGMM INT WORK
   [Anonymous], P INT C COMP VIS PAT
   [Anonymous], P INT C COMP VIS PAT
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2009, P BRIT MACH VIS C BM
   [Anonymous], P INT C COMP VIS ICC
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], 2007, COLUMBIA U BASELINE
   [Anonymous], P INT C COMP VIS PAT
   [Anonymous], ICGTR0108
   [Anonymous], ONTOLOGIES VIDEO EVE
   [Anonymous], P INT C COMP VIS PAT
   [Anonymous], P AAAI SPRING S INT
   Ballan L, 2010, MULTIMED TOOLS APPL, V48, P313, DOI 10.1007/s11042-009-0342-4
   Ballan L, 2010, MULTIMED TOOLS APPL, V48, P69, DOI 10.1007/s11042-009-0351-3
   Boiman O, 2007, INT J COMPUT VISION, V74, P17, DOI 10.1007/s11263-006-0009-9
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Duan LX, 2010, PROC CVPR IEEE, P1959, DOI 10.1109/CVPR.2010.5539870
   Ebadollahi S, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P881, DOI 10.1109/ICME.2006.262691
   Gupta A., 2007, PROC INT C COMPUTER, P1
   Hakeem A, 2007, ARTIF INTELL, V171, P586, DOI 10.1016/j.artint.2007.04.002
   Hu YX, 2009, IEEE I CONF COMP VIS, P128, DOI 10.1109/ICCV.2009.5459153
   Jiang Y., 2011, P ACM INT C MULT RET
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li L: J., 2007, PROC INT C COMPUTER, P1
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Natsev A.P., 2004, PROC 10 ACM SIGKDD I, P641
   Niebles JC, 2008, LECT NOTES COMPUT SC, V5305, P527, DOI 10.1007/978-3-540-88693-8_39
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Ryoo MS, 2009, IEEE I CONF COMP VIS, P1593, DOI 10.1109/ICCV.2009.5459361
   Ryoo M.S., 2007, IEEE C COMPUTER VISI, P1
   Schindler Grant, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4562960
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Smith JR, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P445
   Song DJ, 2010, IEEE T IMAGE PROCESS, V19, P174, DOI 10.1109/TIP.2009.2032939
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Wang F, 2008, ADVANCES IN MATRIX THEORY AND ITS APPLICATIONS, VOL 1, P238
   Wang J., 2007, P INT WORKSHOP WORKS, P217
   Wang T., 2006, Computer Vision and Pattern Recognition Workshop, P109
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu J, 2007, PROCEEDINGS OF THE 2007 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS, P7, DOI 10.1109/SOLI.2007.4383891
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Yan R, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P834
   Yin J, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1321
   Yongzhen Huang, 2009, Computer Vision - ACCV 2009. 9th Asian Conference on Computer Vision. Revised Selected Papers, P180
   Zelnik-Manor L, 2006, IEEE T PATTERN ANAL, V28, P1530, DOI 10.1109/TPAMI.2006.194
   Zhang D, 2005, PROC CVPR IEEE, P611
   Zhou X., 2008, MM 08 P 2008 ACM INT, P229, DOI DOI 10.1145/1459359.1459391.ISBN
   Zhu Guangyu., 2009, Proceedings of the 17th acm international conference on multimedia, P165
NR 65
TC 86
Z9 96
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 88
EP 101
DI 10.1109/TMM.2011.2168948
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100009
DA 2024-07-18
ER

PT J
AU Chen, KW
   Lin, CW
   Chiu, TH
   Chen, MYY
   Hung, YP
AF Chen, Kuan-Wen
   Lin, Chih-Wei
   Chiu, Tzu-Hsuan
   Chen, Mike Yen-Yang
   Hung, Yi-Ping
TI Multi-Resolution Design for Large-Scale and High-Resolution Monitoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hybrid dual-camera system; multi-resolution; steerable focus; user
   study; visual monitoring
ID VISION; CAMERA
AB Large-scale and high-resolution monitoring systems are ideal for many visual surveillance applications. However, existing approaches have insufficient resolution and low frame rate per second, or have high complexity and cost. We take inspiration from the human visual system and propose a multi-resolution design, e-Fovea, which provides peripheral vision with a steerable fovea that is in higher resolution. In this paper, we firstly present two user studies, with a total of 36 participants, to compare e-Fovea to two existing multi-resolution visual monitoring designs. The user study results show that for visual monitoring tasks, our e-Fovea design with steerable focus is significantly faster than existing approaches and preferred by users. We then present our design and implementation of e-Fovea, which combines both multi-resolution camera input and multi-resolution steerable projector output. Finally, we present our deployment of e-Fovea in three installations to demonstrate its feasibility.
C1 [Chen, Kuan-Wen; Lin, Chih-Wei; Chen, Mike Yen-Yang; Hung, Yi-Ping] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
   [Chiu, Tzu-Hsuan; Chen, Mike Yen-Yang; Hung, Yi-Ping] Natl Taiwan Univ, Inst Networking & Multimedia, Taipei 10764, Taiwan.
   [Hung, Yi-Ping] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Hung, Yi-Ping] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10764, Taiwan.
C3 National Taiwan University; National Taiwan University; Academia Sinica
   - Taiwan; National Taiwan University
RP Chen, KW (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10764, Taiwan.
EM kuanwenchen@ntu.edu.tw; mikechen@csie.ntu.edu.tw; hung@csie.ntu.edu.tw
RI Chen, Kuan-Wen/AAJ-3870-2020
OI Chen, Kuan-Wen/0000-0002-4159-201X; CHEN, YEN-YANG/0000-0001-5410-652X
FU National Science Council, Taiwan [NSC 98-2221-E-002-127-MY3]; Ministry
   of Economic Affairs, Taiwan [99-EC-17-A-02-S1-032]
FX Manuscript received April 10, 2011; revised July 16, 2011, July 25,
   2011, and July 28, 2011; accepted July 31, 2011. Date of publication
   August 18, 2011; date of current version November 18, 2011. This work
   was supported in part by the National Science Council, Taiwan, under
   Grants NSC 98-2221-E-002-127-MY3, and by the Ministry of Economic
   Affairs, Taiwan, under Grant 99-EC-17-A-02-S1-032. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Monica Aguilar.
CR Ahlborn BA, 2006, P IEEE VIRT REAL ANN, P281, DOI 10.1109/VR.2006.7
   [Anonymous], 2008, P WORKSH MULT MULT S
   [Anonymous], PENPOWER TRACKIN IDV
   Baudisch P., 2001, 01UIST. Proceedings of the 14th Annual ACM Symposium on User Interface Software and Technology, P31, DOI 10.1145/502348.502354
   Baudisch P., 2002, Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2002, P259, DOI 10.1145/503376.503423
   BROWN M, 2003, P IEEE INT C COMP VI
   Cao X, 2007, UIST 2007: PROCEEDINGS OF THE 20TH ANNUAL ACM SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P43
   Cha Zhang, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P28, DOI 10.1109/MMSP.2008.4665044
   Chan L. W., 2006, P INT C UB INT COMP
   Chan L.W., 2010, Proceedings of the 23rd Annual ACM Symposium on User Interface Software and Technology, P263, DOI [10.1145/1866029.1866072, DOI 10.1145/1866029.1866072]
   Chen IH, 2007, IEEE T AUTOM SCI ENG, V4, P286, DOI 10.1109/TASE.2006.884040
   Chen K. W., 2010, P ACM MULT OCT, P311
   Feiner S., 1991, UIST Fourth Annual Symposium on User Interface Software and Technology. Proceedings of the ACM Symposium on User Interface Software and Technology, P9, DOI 10.1145/120782.120783
   Full Vision Industry Co. Ltd., SPEC FVIP150H IP CAM
   Geisler J., 2007, P IEEE INT WORKSH HU, P278
   Hsiao C. H., 2009, P ACM C HUM FACT COM
   Hu T. T., 2008, P IEEE INT WORKSH TA
   Jessell TM., 2000, PRINCIPLES NEURAL SC
   LALONDE M, 2007, SPIE, V6575
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Marchesotti L., 2003, P INT C IM PROC ICIP
   Palmer S., 1999, VISION SCI PHOTONS P
   PLAISANT C, 1995, IEEE SOFTWARE, V12, P21, DOI 10.1109/52.368260
   Rivlin E, 2000, INT J COMPUT VISION, V39, P81, DOI 10.1023/A:1008166825510
   Sanneblad Johan, 2006, P WORKING C ADV VISU, P373, DOI [10.1145/1133265.1133343, DOI 10.1145/1133265.1133343]
   Sansoni G, 1999, APPL OPTICS, V38, P6565, DOI 10.1364/AO.38.006565
   Szeliski R., 2004, MSRTR200492
   Wandell B. A, 1995, Foundations of vision
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 29
TC 13
Z9 13
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1256
EP 1268
DI 10.1109/TMM.2011.2165055
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400007
DA 2024-07-18
ER

PT J
AU Cleju, N
   Thomos, N
   Frossard, P
AF Cleju, Nicolae
   Thomos, Nikolaos
   Frossard, Pascal
TI Selection of Network Coding Nodes for Minimal Playback Delay in
   Streaming Overlays
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay minimization; network coding; overlay networks; throughput
   maximization
ID INFORMATION-FLOW; MULTICAST
AB Network coding permits to deploy distributed packet delivery algorithms that locally adapt to the network availability in media streaming applications. However, it may also increase delay and computational complexity if it is not implemented efficiently. We address here the effective placement of a limited number of nodes that implement randomized network coding in overlay networks, so that the goodput is kept high while the delay for decoding stays small in streaming applications. We first estimate the decoding delay at each client, which depends on the innovative rate in the network. This estimation permits to identify the nodes that have to perform coding in order to reduce the decoding delay. We then propose two iterative algorithms for selecting the nodes that should perform network coding. The first algorithm relies on the knowledge of the full network statistics. The second algorithm uses only local network statistics at each node. Simulation results show that large performance gains can be achieved with the selection of only a few network coding nodes. Moreover, the second algorithm performs very closely to the central estimation strategy, which demonstrates that the network coding nodes can be selected efficiently with help of a distributed innovative flow rate estimation solution. Our solution provides large gains in terms of throughput, delay, and video quality in realistic overlay networks when compared to methods that employ traditional streaming strategies as well as random network coding nodes selection algorithms.
C1 [Cleju, Nicolae] Gheorghe Asachi Tech Univ Iasi, Iasi 700506, Romania.
   [Thomos, Nikolaos; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Lausanne, Switzerland.
C3 GH Asachi Technical University; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP Cleju, N (corresponding author), Gheorghe Asachi Tech Univ Iasi, Iasi 700506, Romania.
EM nikcleju@etti.tuiasi.ro; nikolaos.thomos@epfl.ch;
   pascal.frossard@epfl.ch
RI Thomos, Nikolaos/AAU-2328-2020; Frossard, Pascal/AAF-2268-2019; Cleju,
   Nicolae/C-3174-2015
OI Thomos, Nikolaos/0000-0001-7266-2642; Cleju, Nicolae/0000-0002-9861-0872
FU Swiss National Science Foundation [PZ00P2-121906]; Swiss National
   Science Foundation (SNF) [PZ00P2_121906] Funding Source: Swiss National
   Science Foundation (SNF)
FX Manuscript received January 18, 2011; revised April 28, 2011; accepted
   June 18, 2011. Date of publication July 12, 2011; date of current
   version September 16, 2011. This work was supported in part by the Swiss
   National Science Foundation under grant PZ00P2-121906. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], P 41 ALL ANN C COMM
   [Anonymous], JVT REFERENCE SOFTWA
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], P 41 ALL C COMM CONT
   BHATTAD K, 2005, P IEEE INT S INF THE
   CANNONS J, 2008, IEEE T INFORM THEORY, V54, P2398
   CLEJU N, 2010, P INT C COMM ICC 10
   Fragouli C, 2006, IEEE T INFORM THEORY, V52, P829, DOI 10.1109/TIT.2005.864435
   FRAGOULI C, 2004, P C INF SCI SYST CIS
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   *ISO IEC FDIS, 2003, 1449610 ISOIEC FDIS
   Langberg M, 2009, IEEE T INFORM THEORY, V55, P147, DOI 10.1109/TIT.2008.2008135
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   TAVORY A, 2003, P EL C COMP COMPL EC
   THOMOS N, 2010, P INT C AC SPEECH SI
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wu MQ, 2005, SIGNAL PROCESS-IMAGE, V20, P728, DOI 10.1016/j.image.2005.05.002
   Wu YN, 2006, IEEE T INFORM THEORY, V52, P2398, DOI 10.1109/TIT.2006.874430
   YALAGANDULA P, 2006, P SIGCOMM WORKSH INT
NR 22
TC 10
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1103
EP 1115
DI 10.1109/TMM.2011.2161448
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300022
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Hung, H
   Gatica-Perez, D
AF Hung, Hayley
   Gatica-Perez, Daniel
TI Estimating Cohesion in Small Groups Using Audio-Visual Nonverbal
   Behavior
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cohesion estimation; industrial psychology; meetings; nonverbal
   communication
ID PERFORMANCE; COHESIVENESS; TASK
AB Cohesiveness in teams is an essential part of ensuring the smooth running of task-oriented groups. Research in social psychology and management has shown that good cohesion in groups can be correlated with team effectiveness or productivity, so automatically estimating group cohesion for team training can be a useful tool. This paper addresses the problem of analyzing group behavior within the context of cohesion. Four hours of audio-visual group meeting data were used for collecting annotations on the cohesiveness of four-participant teams. We propose a series of audio and video features, which are inspired by findings in the social sciences literature. Our study is validated on a set of 61 2-min meeting segments which showed high agreement amongst human annotators when asked to identify meetings that have high or low cohesion.
C1 [Hung, Hayley] Univ Amsterdam, NL-1098 XG Amsterdam, Netherlands.
   [Gatica-Perez, Daniel] Idiap Res Inst, CH-1920 Martigny, Switzerland.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 University of Amsterdam; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Hung, H (corresponding author), Univ Amsterdam, NL-1098 XG Amsterdam, Netherlands.
RI Hung, Hayley/AAS-2215-2021
OI Hung, Hayley/0000-0003-0719-8948
FU European Project AMIDA (Augmented Multi-party Interaction with Distance
   Access); Swiss National Science Foundation through the NCCR IM2
   (Interactive MultiModal Information Management)
FX Manuscript received December 14, 2009; revised March 31, 2010; accepted
   May 13, 2010. Date of current version September 15, 2010. This work was
   supported in part by the European Project AMIDA (Augmented Multi-party
   Interaction with Distance Access) and the Swiss National Science
   Foundation through the NCCR IM2 (Interactive MultiModal Information
   Management). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Nicu Sebe.
CR Ambady N, 2002, J PERS SOC PSYCHOL, V83, P947, DOI 10.1037//0022-3514.83.4.947
   [Anonymous], 2008, HONEST SIGNALS, DOI DOI 10.7551/MITPRESS/8022.001.0001
   Bailenson JN, 2008, COMPUT HUM BEHAV, V24, P66, DOI 10.1016/j.chb.2007.01.015
   Basu S., 2001, LEARNING HUMAN INTER
   BOLLEN KA, 1990, SOC FORCES, V69, P479, DOI 10.2307/2579670
   BRAATEN LJ, 1991, GROUP, V15, P39, DOI 10.1007/BF01419845
   CAMPBELL N, 2007, LECT NOTES COMPUTER, V5042, P107
   Campbell N, 2008, PROCEEDINGS OF THE SECOND INTERNATIONAL SYMPOSIUM ON UNIVERSAL COMMUNICATION, P12, DOI 10.1109/ISUC.2008.36
   CANEEL R, P AUG COGN AUGCOG, V2005
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Carron A.V., 1998, Advances in Sport and Exercise Psychology Measurement, P213
   Carron AV, 2000, SMALL GR RES, V31, P89, DOI 10.1177/104649640003100105
   Casey-Campbell M, 2009, INT J MANAG REV, V11, P223, DOI 10.1111/j.1468-2370.2008.00239.x
   Cassell Justine, 2007, P WORKSHOP EMBODIED, P41
   CHIN WW, 2009, SMALL GR RES, V30, P751
   COHEN J, 1968, PSYCHOL BULL, V70, P213, DOI 10.1037/h0026256
   CONDON WS, 1966, J NERV MENT DIS, V143, P338, DOI 10.1097/00005053-196610000-00005
   Cover T. M., 1991, ELEMENTS INFORM THEO
   DINES J, P INTERSPEECH, V2006
   Dunbar NE, 2005, J SOC PERS RELAT, V22, P207, DOI 10.1177/0265407505050944
   EVANS NJ, 1980, SMALL GROUP BEHAV, V11, P359, DOI 10.1177/104649648001100401
   Friedkin NE, 2004, ANNU REV SOCIOL, V30, P409, DOI 10.1146/annurev.soc.30.012703.110625
   Gatica-Perez D, 2005, INT CONF ACOUST SPEE, P489
   Gratch J, 2006, LECT NOTES ARTIF INT, V4133, P14
   Griffith J, 2007, ARMED FORCES SOC, V34, P138, DOI 10.1177/0095327X06294620
   Jayagopi D., 2008, IEEE T AUDIO SPEECH
   Kidwell RE, 1997, J MANAGE, V23, P775, DOI 10.1016/S0149-2063(97)90029-5
   Krämer NC, 2007, LECT NOTES ARTIF INT, V4722, P238
   Lakin JL, 2003, PSYCHOL SCI, V14, P334, DOI 10.1111/1467-9280.14481
   Mast MS, 2002, HUM COMMUN RES, V28, P420, DOI 10.1111/j.1468-2958.2002.tb00814.x
   Mikalachki A., 1969, GROUP COHESION RECON
   MORGON N, 2009, MRATE ESTIMATER
   MULLEN B, 1994, PSYCHOL BULL, V115, P210, DOI 10.1037/0033-2909.115.2.210
   Seashore S.E., 1954, GROUP COHESIVENESS I
   Siebold GL, 1999, MIL PSYCHOL, V11, P5, DOI 10.1207/s15327876mp1101_2
   TANNEN D, 1993, GENDER DISCOURSE, P53
   West C., 1983, Language, gender, and society, P103
   YEO C, 2008, UCBEECS200879
   ZACCARO SJ, 1988, J APPL SOC PSYCHOL, V18, P837, DOI 10.1111/j.1559-1816.1988.tb01178.x
   ZACCARO SJ, 1988, J SOC PSYCHOL, V128, P547, DOI 10.1080/00224545.1988.9713774
NR 40
TC 103
Z9 113
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 563
EP 575
DI 10.1109/TMM.2010.2055233
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900010
OA Green Published
DA 2024-07-18
ER

PT J
AU Choi, BY
   Song, S
   Wang, Y
   Park, EK
AF Choi, Baek-Young
   Song, Sejun
   Wang, Yue
   Park, Eun Kyo
TI Using RTT Variability for Adaptive Cross-Layer Approach to Multimedia
   Delivery in Heterogeneous Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer approach; heterogenous networks; round-trip time (RTT)
   variability
ID ERROR CONTROL; WIRELESS; VIDEO; QOS
AB A holistic approach should be made for a wider adoption of a cross-layer approach. A cross-layer design on a wireless network assumed with a certain network condition, for instance, can have a limited usage in heterogeneous environments with diverse access network technologies and time varying network performance. The first step toward a cross-layer approach is an automatic detection of the underlying access network type, so that appropriate schemes can be applied without manual configurations. To address the issue, we investigate the characteristics of round-trip time (RTT) on wireless and wired networks. We conduct extensive experiments from diverse network environments and perform quantitative analyses on RTT variability. We show that RTT variability on a wireless network exhibits greatly larger mean, standard deviation, and min-to-high percentiles at least 10 ms bigger than those of wired networks due to the MAC layer retransmissions. We also find that the impact of packet size on wireless channel is particularly significant. Thus through a simple set of testing, one can accurately classify whether or not there has been a wireless network involved. We then propose effective adaptive cross-layer schemes for multimedia delivery over error-prone links. They include limiting the MAC layer retransmissions, controlling the application layer forward error correction (FEC) level, and selecting an optimal packet size. We conduct an analysis on the interplay of those adaptive parameters given a network condition. It enables us to find optimal cross-layer adaptive parameters when they are used concurrently.
C1 [Choi, Baek-Young; Wang, Yue; Park, Eun Kyo] Univ Missouri, Kansas City, MO 64110 USA.
   [Song, Sejun] Texas A&M Univ, College Stn, TX 77843 USA.
C3 University of Missouri System; University of Missouri Kansas City; Texas
   A&M University System; Texas A&M University College Station
RP Choi, BY (corresponding author), Univ Missouri, Kansas City, MO 64110 USA.
EM choiby@umkc.edu; song@entc.tamu.edu; wangx82@gmail.com; ekpark@umkc.edu
OI Choi, Baek-Young/0000-0003-4449-2425
FU U.S. National Science Foundation [0729197]; Direct For Computer & Info
   Scie & Enginr; Division of Computing and Communication Foundations
   [0729197] Funding Source: National Science Foundation
FX This work was supported in part by the U.S. National Science Foundation
   under Grant No. 0729197.
CR Aikat J., 2003, P 3 ACM SIGCOMM C IN, P279, DOI DOI 10.1145/948205.948241
   [Anonymous], P IEEE INFOCOM SAN F
   CASETTI C, 2002, 020024 UCLA CSD
   CHAUDHARY R, 2003, P IEEE ICCCN
   CHENG YC, 2007, P ACM SIGCOMM AUG
   CHINCHILLA F, 2004, P IEEE INFOCOM HONG
   Choi BY, 2007, COMPUT NETW, V51, P2701, DOI 10.1016/j.comnet.2006.11.023
   Floyd S., 2000, P ACM SIGCOMM
   Gretarsson J., 2005, P 1 ACM WORKSHOP WIR, P123
   Handley M., 2003, TCP FRIENDLY RATE CO
   HUANG L, 2002, P ACM WOWMOM
   Larzon LA., 2004, The lightweight user datagram protocol (UDP-Lite)
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   LOW S, 2000, UNDERSTANDING TCP VE
   MA H, 1999, P PACK VID WORKSH AP
   Mano CD, 2008, ACM T INFORM SYST SE, V11, DOI 10.1145/1330332.1330334
   Mascolo C., 2001, P 7 ANN INT C MOB CO, P287
   Padhye J, 2000, IEEE ACM T NETWORK, V8, P133, DOI 10.1109/90.842137
   PLANET A, 2006, TRUE PING OPEN SOURC
   Pyun JY, 2003, IEEE T CONSUM ELECTR, V49, P614, DOI 10.1109/TCE.2003.1233784
   Sessini Phillipa., 2006, OBSERVATIONS ROUND T
   SYSTEMS IC, 2008, LINKSYS 802 11G AP M
   Tian Y., 2005, IEEE COMMUN MAG, V43, pS27
   VEAL B, 2005, P PASS ACT MEAS C
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   WEI W, 2005, P IEEE INFOCOM MIAM
   WEI W, 2006, P IEEE INFOCOM BARC
   WEI W, 2007, P 7 ACM SIGCOMM C IN, P365
   WICKER SB, 1999, REEDSOLOMON CODES TH
   Xu K, 2004, IEEE J SEL AREA COMM, V22, P747, DOI 10.1109/JSAC.2004.825989
   YEO J, 2004, P WISE OCT
   YUAN W, 2003, P SPIE ACM MULT COMP
   ZENG W, 2003, P ASI EXCH VANC BC C
   Zheng H, 2000, IEEE WCNC, P191, DOI 10.1109/WCNC.2000.904625
   ZHOU B, 2007, P IEEE INT C NETW PR
   Zorzi M, 2001, IEEE T VEH TECHNOL, V50, P12, DOI 10.1109/25.917864
NR 36
TC 12
Z9 14
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1194
EP 1203
DI 10.1109/TMM.2009.2026103
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700015
OA Green Published
DA 2024-07-18
ER

PT J
AU Lu, L
   Hanjalic, A
AF Lu, Lie
   Hanjalic, Alan
TI Text-Like Segmentation of General Audio for Content-Based Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio element; audio scene; audio scene segmentation; content-based
   audio analysis
AB Automatic detection of (semantically) meaningful audio segments, or audio scenes, is an important step in high-level semantic inference from general audio signals, and can benefit various content-based applications involving both audio and multimodal (multimedia) data sets. Motivated by the known limitations of traditional low-level feature-based approaches, we propose in this paper a novel approach to discover audio scenes, based on an analysis of audio elements and key audio elements, which can be seen as equivalents to the words and keywords in a text document, respectively. In the proposed approach, an audio track is seen as a sequence of audio elements, and the presence of an audio scene boundary at a given time stamp is checked based on pair-wise measuring the semantic affinity between different parts of the analyzed audio stream surrounding that time stamp. Our proposed model for semantic affinity exploits the proven concepts from text document analysis, and is introduced here as a function of the distance between the audio parts considered, and the co-occurrence statistics and the importance weights of the audio elements contained therein. Experimental evaluation performed on a representative data set consisting of 5 h of diverse audio data streams indicated that the proposed approach is more effective than the traditional low-level feature-based approaches in solving the posed audio scene segmentation problem.
C1 [Lu, Lie] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
C3 Microsoft Research Asia; Microsoft; Delft University of Technology
RP Lu, L (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM llu@microsoft.com; A.Hanjalic@tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
CR [Anonymous], 1998, P BROADC NEWS TRANSC
   Cai R, 2006, IEEE T AUDIO SPEECH, V14, P1026, DOI 10.1109/TSA.2005.857575
   CAI R, 2005, P ACM MULT 05, P628
   Cai R, 2008, IEEE T MULTIMEDIA, V10, P596, DOI 10.1109/TMM.2008.921739
   Ellis D. R., 2004, Proceedings of a Workshop on Equine Recurrent Laryngeal Neuropathy, Stratford-upon-Avon, UK, 7-10 September, 2003, P39, DOI 10.1145/1026653.1026659
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   FERRET O, 1998, P 36 ANN M ASS COMP, V2, P1481
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   HANJALIC A, 2005, P 2 EUR WORKSH INT K
   KASS RE, 1995, J AM STAT ASSOC, V90, P773, DOI 10.1080/01621459.1995.10476572
   Kender JR, 1998, PROC CVPR IEEE, P367, DOI 10.1109/CVPR.1998.698632
   Lu L., 2006, P ACM MULT 06, P825
   LU L, 2005, P 30 IEEE INT C AC S, V2, P1069
   Lu L, 2008, IEEE T MULTIMEDIA, V10, P74, DOI 10.1109/TMM.2007.911304
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Sundaram H, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1145, DOI 10.1109/ICME.2000.871563
   Sundaram H, 2000, INT CONF ACOUST SPEE, P2441, DOI 10.1109/ICASSP.2000.859335
   VENUGOPAL S, 1999, P MMSP99, P191
   Wang D, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P468
   Xu M, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P281
NR 20
TC 2
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 658
EP 669
DI 10.1109/TMM.2009.2017607
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900008
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Xu, CS
   Cheng, J
   Lu, HQ
   Ma, SD
AF Zhang, Xiaoyu
   Xu, Changsheng
   Cheng, Jian
   Lu, Hanqing
   Ma, Songde
TI Effective Annotation and Search for Video Blogs with Integration of
   Context and Content Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotation expansion; ranking; saliency-based matching; vlog annotation;
   vlog search
AB In recent years, weblogs (or blogs) have received great popularity worldwide, among which video blogs (or vlogs) are playing an increasingly important role. However, research on vlog analysis is still in the early stage, and how to manage vlogs effectively so that they can be more easily accessible is a challenging problem. In this paper, we propose a novel vlog management model which is comprised of automatic vlog annotation and user-oriented vlog search. For vlog annotation, we extract informative keywords from both the target vlog itself and relevant external resources; besides semantic annotation, we perform sentiment analysis on comments to obtain the overall evaluation. For vlog search, we present saliency-based matching to simulate human perception of similarity, and organize the results by personalized ranking and category-based clustering. An evaluation criterion is also proposed for vlog annotation, which assigns a score to an annotation according to its accuracy and completeness in representing the vlog's semantics. Experimental results demonstrate the effectiveness of the proposed management model for vlogs.
C1 [Zhang, Xiaoyu; Xu, Changsheng; Cheng, Jian; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Zhang, Xiaoyu; Xu, Changsheng; Cheng, Jian; Lu, Hanqing] China Singapore Inst Digital Media, Singapore, Singapore.
   [Ma, Songde] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Zhang, XY (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xyzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jcheng@nlpr.ia.ac.cn;
   luhq@nlpr.ia.ac.cn; masd@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Zhang, Xiaoyu/ISV-0984-2023; zhang,
   xiaoyu/HJI-4374-2023; Zhang, Xiaoyu/N-6847-2014; Zhang,
   xiaoyu/GXA-3206-2022; Wang, Zixuan/HZJ-2348-2023; xiaoyu,
   zhang/JXY-7226-2024; , chengjian/KGL-5551-2024
OI , chengjian/0000-0003-1289-2758
CR [Anonymous], P CVPR
   [Anonymous], P 8 ACM SIGMM INT WO, DOI DOI 10.1145/1178677.1178714
   Bai J., 2006, C EMPRICAL METHODS N, P551
   BAI J, 2007, P 30 ANN INT ACM SIG, P15
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chang SF, 2005, INT CONF ACOUST SPEE, P1005
   CHURCH KW, 1990, 27TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P76
   Cilibrasi R, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P2309, DOI 10.1109/ISIT.2006.261979
   DVORAK JC, BLOG PHENOMENA 2002
   GOLDSTONE RL, 1994, J EXPT PSYCH LEARN M, P3
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   HOEM J, 2004, BLOGTALKS 2 0, P237
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   JING Y, 1994, P RIAO, P146
   KING A, 2003, VLOGGING VIDEO WEBLO
   Liu JJ, 2007, PROCEEDINGS OF THE 2007 CHINESE CONTROL AND DECISION CONFERENCE, P605, DOI 10.1145/1291233.1291380
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Meng Y, 2003, PROC CVPR IEEE, P416
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Nardi B., 2004, P C COMPUTER SUPPORT, P222
   Nardi BA, 2004, COMMUN ACM, V47, P41, DOI 10.1145/1035134.1035163
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   PARKER C, 2005, P IEEE MULT LOS AL C, P4
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Rosenbloom A, 2004, COMMUN ACM, V47, P30, DOI 10.1145/1035134.1035161
   Rui X., 2007, Proc. 15th ACM intl. conf. on multimedia, P585, DOI DOI 10.1145/1291233.1291378
   Santorini Beatrice., 1995, Part-of-Speech Tagging Guidelines for the Penn Treebank Project
   Schutze H, 1997, INFORM PROCESS MANAG, V33, P307, DOI 10.1016/S0306-4573(96)00068-4
   STOLARZ D, 2005, CREATING YOUR OWN ME
   Turney PD, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P417
   Voorhees E. M., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P61
   WEI XY, 2007, P 15 INT C MULT AUGS, P981
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   ZHANG X, 2008, P 2008 IEEE INT C MU
NR 36
TC 25
Z9 32
U1 2
U2 76
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 272
EP 285
DI 10.1109/TMM.2008.2009689
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800008
DA 2024-07-18
ER

PT J
AU Chasanis, VT
   Likas, AC
   Galatsanos, NP
AF Chasanis, Vasileios T.
   Likas, Aristidis C.
   Galatsanos, Nikolaos P.
TI Scene Detection in Videos Using Shot Clustering and Sequence Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Global k-means; key-frames; scene detection; sequence alignment
ID SEGMENTATION
AB Video indexing requires the efficient segmentation of video into scenes. The video is first segmented into shots and a set of key-frames is extracted for each shot. Typical scene detection algorithms incorporate time distance in a shot similarity metric. In the method we propose, to overcome the difficulty of having prior knowledge of the scene duration, the shots are clustered into groups based only on their visual similarity and a label is assigned to each shot according to the group that it belongs to. Then, a sequence alignment algorithm is applied to detect when the pattern of shot labels changes, providing the final scene segmentation result. In this way shot similarity is computed based only on visual features, while ordering of shots is taken into account during sequence alignment. To cluster the shots into groups we propose an improved spectral clustering method that both estimates the number of clusters and employs the fast global k-means algorithm in the clustering stage after the eigenvector computation of the similarity matrix. The same spectral clustering method is applied to extract the key-frames of each shot and numerical experiments indicate that the content of each shot is efficiently summarized using the method we propose herein. Experiments on TV-series and movies also indicate that the proposed scene detection method accurately detects most of the scene boundaries while preserving a good tradeoff between recall and precision.
C1 [Chasanis, Vasileios T.; Likas, Aristidis C.] Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
   [Galatsanos, Nikolaos P.] Univ Patras, Dept Elect & Comp Engn, Rion 26500, Greece.
C3 University of Ioannina; University of Patras
RP Chasanis, VT (corresponding author), Univ Ioannina, Dept Comp Sci, GR-45110 Ioannina, Greece.
EM vchasani@cs.uoi.gr; arly@cs.uoi.gr; ngalatsanos@upa-tras.gr
FU E.U.; Greek Ministry of Development-GSRT
FX Manuscript received March 24, 2008; revised August 31, 2008. First
   published December 22, 2009; current version published January 08, 2009.
   This work, the research project (PENED), was supported by the
   E.U.-European Social Fund (75%) and the Greek Ministry of
   Development-GSRT (25%). The associate editor coordinating the review if
   this manuscript and approving it for publication was Dr. Marcel Worring.
CR [Anonymous], 2001, Proceedings of Neural Information Processing Systems (NIPS)
   [Anonymous], 1999, Visual Information Retrieval
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Gianluigi C, 2006, J REAL-TIME IMAGE PR, V1, P69, DOI 10.1007/s11554-006-0001-1
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Jones NC., 2004, An introduction to bioinformatics algorithms
   Likas A, 2003, PATTERN RECOGN, V36, P451, DOI 10.1016/S0031-3203(02)00060-2
   Liu TY, 2004, PATTERN RECOGN LETT, V25, P1451, DOI 10.1016/j.patrec.2004.05.020
   NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4
   Odobez JM, 2003, LECT NOTES COMPUT SC, V2728, P310
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   RASHEED Z, 2003, P INT C COMP VIS PAT
   Sethi I. K., 1995, Proceedings of the SPIE - The International Society for Optical Engineering, V2420, P329, DOI 10.1117/12.205299
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Sundaram H, 2002, IEEE T MULTIMEDIA, V4, P482, DOI 10.1109/TMM.2002.802017
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   XING EP, 2003, CSD031265 U CAL
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Zha H., 2001, NEURAL INFO PROCESSI
   Zhai Y, 2006, IEEE T MULTIMEDIA, V8, P686, DOI 10.1109/TMM.2006.876299
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 22
TC 99
Z9 108
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 89
EP 100
DI 10.1109/TMM.2008.2008924
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700008
DA 2024-07-18
ER

PT J
AU Yu, YK
   Wong, KH
   Or, SH
   Chen, JZ
AF Yu, Ying Kin
   Wong, Kin Hong
   Or, Siu Hang
   Chen Junzhou
TI Controlling Virtual Cameras Based on a Robust Model-Free Pose
   Acquisition Technique
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Augmented reality; interacting multiple model; multimedia processing;
   pose tracking; probabilistic data association; stereo vision; trifocal
   tensor; virtual reality
ID MOTION ESTIMATION
AB This paper presents a novel method that acquires camera position and orientation from a stereo image sequence without prior knowledge of the scene. To make the algorithm robust, the Interacting Multiple Model Probabilistic Data Association Filter (IMMPDAF) is introduced. The Interacting Multiple Model (IMM) technique allows the existence of more than one dynamic system in the filtering process and in return leads to improved accuracy and stability even under abrupt motion changes. The Probabilistic Data Association (PDA) framework makes the automatic selection of measurement sets possible, resulting in enhanced robustness to occlusions and moving objects. In addition to the IMMPDAF, the trifocal tensor is employed in the computation so that the step of reconstructing the 3-D models can be eliminated. This further guarantees the precision of estimation and computation efficiency. Real stereo image sequences have been used to test the proposed method in the experiment. The recovered 3-D motions are accurate in comparison with the ground truth data and have been applied to control cameras in a virtual environment.
C1 [Yu, Ying Kin; Wong, Kin Hong; Or, Siu Hang; Chen Junzhou] Chinese Univ Hong Kong, Comp Sci & Engn Dept, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Yu, YK (corresponding author), Chinese Univ Hong Kong, Comp Sci & Engn Dept, Shatin, Hong Kong, Peoples R China.
EM ykyu.hk@gmail.com; khwong@cse.cuhk.edu.hk; shor@cse.cuhk.edu.hk;
   jzchen@cse.cuhk.edu.hk
FU Chinese University of Hong Kong, Shatin, Hong Kong [2050350, 2050410]
FX Manuscript received July 20, 2007: revised September 09, 2008. First
   published December 22, 2008: current version published January 08, 2009.
   This work was supported by direct Grants under Projects 2050350 and
   2050410 from the Faculty of Engineering, The Chinese University of Hong
   Kong, Shatin, Hong Kong. The associate editor coordinating the review of
   this manusscript and approving it for publication was Dr. Zhengyou
   Zhang.
CR Abolmaesumi P, 2004, IEEE T MED IMAGING, V23, P772, DOI 10.1109/TMI.2004.826954
   Avidan S, 2001, IEEE T PATTERN ANAL, V23, P73, DOI 10.1109/34.899947
   AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503
   Bar-Shalom Y., 1988, TRACKING DATA ASS
   BROIDA TJ, 1990, IEEE T AERO ELEC SYS, V26, P639, DOI 10.1109/7.55557
   Chiu K C, 2000, BMC Genet, V1, P2, DOI 10.1186/1471-2156-1-2
   Chiuso A, 2002, IEEE T PATTERN ANAL, V24, P523, DOI 10.1109/34.993559
   Davison AJ, 2001, PROC CVPR IEEE, P384
   Davison AJ, 2002, IEEE T PATTERN ANAL, V24, P865, DOI 10.1109/TPAMI.2002.1017615
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hoff W, 2000, IEEE T VIS COMPUT GR, V6, P319, DOI 10.1109/2945.895877
   HOULES A, 1989, IEEE T AEROSP ELECT, V25
   Kanbara M., 2000, Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048), P255, DOI 10.1109/VR.2000.840506
   Lee S., 1990, Proceedings 1990 IEEE International Conference on Robotics and Automation (Cat. No.90CH2876-1), P414, DOI 10.1109/ROBOT.1990.126012
   Lippiello V, 2001, PROCEEDINGS OF THE 2001 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS (CCA'01), P702, DOI 10.1109/CCA.2001.973950
   LLOYD B, DEPT COMPUTER SCI U
   LOWE DG, 1991, IEEE T PATTERN ANAL, V13, P441, DOI 10.1109/34.134043
   Murray RM, 1994, MATH INTRO ROBOTIC M
   Oskiper T., 2007, P IEEE C COMP VIS PA
   SOATTO S, 1994, EUR C COMP VIS STOCK
   Tomasi C, 1991, DETECTION TRACKING P
   Vacchetti L, 2003, PROC CVPR IEEE, P241
   Ware C., 1990, Computer Graphics, V24, P175, DOI 10.1145/91394.91442
   WENG JY, 1993, IEEE T PATTERN ANAL, V15, P864, DOI 10.1109/34.232074
   Yu YK, 2008, IEEE T INSTRUM MEAS, V57, P622, DOI 10.1109/TIM.2007.911641
   Yu YK, 2006, IEEE T SYST MAN CY B, V36, P1081, DOI 10.1109/TSMCB.2006.874133
   Yu YK, 2006, IEEE T MULTIMEDIA, V8, P521, DOI 10.1109/TMM.2006.870734
   Yu YK, 2005, IEEE T SYST MAN CY B, V35, P1295, DOI 10.1109/TSMCB.2005.850164
   Yu YK, 2005, IEEE T SYST MAN CY B, V35, P587, DOI 10.1109/TSMCB.2005.846665
   YU YK, 2006, P IEEE C COMP VIS PA, V1, P1274
NR 31
TC 6
Z9 10
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 182
EP 189
DI 10.1109/TMM.2008.2008871
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700016
DA 2024-07-18
ER

PT J
AU Cho, Y
   Karande, S
   Misra, K
   Radha, H
   Yoo, J
   Hong, J
AF Cho, Yongju
   Karande, Shirish
   Misra, Kiran
   Radha, Hayder
   Yoo, Jeongju
   Hong, Jinwoo
TI On Channel Capacity Estimation and Prediction for Rate-Adaptive Wireless
   Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Channel quality prediction; coding rate; rate adaptation; wireless LAN;
   wireless video
AB Packet drops caused by residue errors (MAC-layer errors) can severely deteriorate the wireless video quality. Prior studies have shown that this loss of quality can be circumvented by using forward error correction (FEC) to recover information from the corrupted packets. The performance of FEC encoded video streaming is critically dependent upon the choice of source and channel coding rates. In practice, the wireless channel conditions can vary significantly, thus altering the optimal rate choices. Thus, it is essential to develop an architecture which can estimate the channel capacity, and utilize this estimate for rate allocation. In this paper we develop such a framework. Our contributions consist of two parts. In the first part we develop a prediction framework that leverages the received packets' signal to silence ratio (SSR) indications and MAC-layer checksum as side information to predict the operational channel capacity. In the second part, we use this prediction framework for rate allocation. The optimal rate allocation is dependent upon the channel capacity, the distribution of the (capacity) prediction error and the rate-distortion (RD) characteristics of the video source. Consequently, we propose a framework that utilizes the aforementioned statistics for RD optimal rate adaptation. We exhibit the efficacy of the proposed scheme by simulations using actual 802.11 b wireless traces, an RD model for the video source and an ideal FEC model. Simulations using source RD models derived from five different popular video codecs (including H.264), show that the proposed framework provides up-to 5-dB improvements in peak signal-to-noise ratio (PSNR) when compared with conventional rate-adaptive schemes.
C1 [Cho, Yongju; Yoo, Jeongju; Hong, Jinwoo] Elect & Telecommun Res Inst, Taejon 305350, South Korea.
   [Cho, Yongju; Karande, Shirish; Misra, Kiran; Radha, Hayder] Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI);
   Michigan State University
RP Cho, Y (corresponding author), Elect & Telecommun Res Inst, Taejon 305350, South Korea.
EM choyong8@egr.msu.edu; karandes@egr.msu.edu; misrakir@egr.msu.edu;
   radha@egr.msu.edu; jjyoo@etri.re.kr; jwhong@etri.re.kr
FU MIC/IITA [12005-S-103-03]
FX Manuscript received August 08, 2007: revised April 21, 2008. Current
   version published November 17. 2008. This work was supported by the IT
   R&D program of MIC/IITA 12005-S-103-03, Development of Ubiquitous
   Content Access Technology for Convergence of Broadcasting and
   Communications]. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. S.-H. Gary Chan.
CR AGUAYO D, 2004, P SIGCOMM 2004 AUG
   ALLAN DW, 1987, IEEE T ULTRASON FERR, V34, P647, DOI 10.1109/T-UFFC.1987.26997
   Boyd S., 2006, Convex Optimization
   CHO Y, 2007, P CISS 2007 MAR
   KARANDE S, 2006, P ICME JUL
   Karande SS, 2007, IEEE T MULTIMEDIA, V9, P307, DOI 10.1109/TMM.2006.886280
   Karande SS, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/64695
   Khayam SA, 2003, SIGNAL PROCESS-IMAGE, V18, P575, DOI 10.1016/S0923-5965(03)00053-5
   KHAYAM SA, 2006, THESIS MICHIGAN STAT
   Lambert P, 2006, IEEE T CIRC SYST VID, V16, P134, DOI 10.1109/TCSVT.2005.857783
   LARZON LA, 1999, P IEEE INT C COMM IC
   Leon-Garcia A., 1994, Probability and random processes for electrical engineering
   Lin S., 1983, Error Control Coding: Fundamentals and Applications
   PAUCHET F, 2005, JOINT VID TEAM JVT 1
   RIEMANN R, 2001, MITCSAILTR2005011
   SINGH A, 2001, P NOSSDAV
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 17
TC 6
Z9 7
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1419
EP 1426
DI 10.1109/TMM.2008.2004901
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700017
DA 2024-07-18
ER

PT J
AU Kuan, YH
   Kuo, CM
   Yang, NC
AF Kuan, Yu-Hsin
   Kuo, Chung-Ming
   Yang, Nai-Chung
TI Color-based image salient region segmentation using novel region merging
   strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dominant color; importance index; merging likelihood; nonparametric
   density estimation; salient region
ID EDGE; EXTRACTION; RETRIEVAL
AB In this paper, we propose a novel unsupervised algorithm for the segmentation of salient regions in color images. There are three phases in this algorithm. In the first phase, we use non-parametric density estimation to extract candidates of dominant colors in an image, which are then used for the quantization of the image. The label map of the quantized image forms initial regions of segmentation. In the second phase, we define salient region with two properties; i.e., conspicuous; compact and complete. According to the definition, two new parameters are proposed. One is called "Importance index", which is used to measure the importance of a region, and the other is called "Merging likelihood", which is utilized to measure the suitability of region merging. Initial regions are merged based on the two new parameters. In the third phase, a similarity check is performed to further merge the surviving regions. Experimental results show that the proposed method achieves excellent segmentation performance for most of our test images. In addition, the computation is very efficient.
C1 [Kuan, Yu-Hsin; Kuo, Chung-Ming; Yang, Nai-Chung] I Shou Univ, Dept Informat Engn, Kaohsiung, Taiwan.
C3 I Shou University
RP Kuan, YH (corresponding author), I Shou Univ, Dept Informat Engn, Kaohsiung, Taiwan.
EM hankkuan@gmail.com; kuocm@isu.edu.tw; ncyang@isu.edu.tw
CR Ahuja N, 1996, IEEE T PATTERN ANAL, V18, P1211, DOI 10.1109/34.546258
   CHANG YL, 1994, IEEE T IMAGE PROCESS, V3, P868, DOI 10.1109/83.336259
   CHENAOUA KS, 2003, P 10 IEEE INT C EL C, V1, P240
   Cheng HD, 2002, PATTERN RECOGN, V35, P373, DOI 10.1016/S0031-3203(01)00054-1
   Cheng SC, 2003, IEE P-VIS IMAGE SIGN, V150, P270, DOI 10.1049/ip-vis:20030520
   CHU CC, 1993, IEEE T PATTERN ANAL, V15, P1241, DOI 10.1109/34.250843
   Comaniciu D, 1997, PROC CVPR IEEE, P750, DOI 10.1109/CVPR.1997.609410
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Dimai A., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P686, DOI 10.1109/ICIAP.1999.797674
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Fan JP, 2001, IEEE T CIRC SYST VID, V11, P1073, DOI 10.1109/76.954494
   Fan JP, 2001, IEEE T IMAGE PROCESS, V10, P1454, DOI 10.1109/83.951532
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fowlkes CC, 2007, J VISION, V7, DOI 10.1167/7.8.2
   Gauch JM, 1999, IEEE T IMAGE PROCESS, V8, P69, DOI 10.1109/83.736688
   Gevers T, 2002, IEEE T PATTERN ANAL, V24, P848, DOI 10.1109/TPAMI.2002.1008391
   HADDON JF, 1990, IEEE T PATTERN ANAL, V12, P929, DOI 10.1109/34.58867
   HECKBERT PS, 1982, ACM COMPUTER GRAPHIC, V16, P297
   Hoang MA, 2005, SIGNAL PROCESS, V85, P265, DOI 10.1016/j.sigpro.2004.10.009
   Huang Q, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA246
   Iannizzotto G, 2000, IEEE T IMAGE PROCESS, V9, P1232, DOI 10.1109/83.847835
   JOHANSSON B, 2000, LITHISYR2215 LINK U
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kim JM, 2005, IEEE T IMAGE PROCESS, V14, P1486, DOI 10.1109/TIP.2005.854442
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Ma WY, 2000, IEEE T IMAGE PROCESS, V9, P1375, DOI 10.1109/83.855433
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Montoya MDG, 2003, J PARALLEL DISTR COM, V63, P387, DOI 10.1016/S0743-7315(03)00039-X
   Pauwels EJ, 1999, COMPUT VIS IMAGE UND, V75, P73, DOI 10.1006/cviu.1999.0763
   PAVLIDIS T, 1990, IEEE T PATTERN ANAL, V12, P225, DOI 10.1109/34.49050
   REN X, 2005, MIDLEVEL CUES IMPROV
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Trémeau A, 2000, IEEE T IMAGE PROCESS, V9, P735, DOI 10.1109/83.841950
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   WAN SJ, 1990, COLOR RES APPL, V15, P52, DOI 10.1002/col.5080150109
   Wan SY, 2003, IEEE T IMAGE PROCESS, V12, P1007, DOI 10.1109/TIP.2003.815258
   WANI MA, 1994, IEEE T PATTERN ANAL, V16, P314, DOI 10.1109/34.276131
NR 39
TC 22
Z9 29
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 832
EP 845
DI 10.1109/TMM.2008.922853
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800015
DA 2024-07-18
ER

PT J
AU Wang, F
   Ngo, CW
   Pong, TC
AF Wang, Feng
   Ngo, Chong-Wah
   Pong, Ting-Chuen
TI Simulating a smartboard by real-time gesture detection in lecture videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE gesture detection; lecture video; real-time simulated smartboard
AB Gesture plays an important role for recognizing lecture activities in video content analysis. In this paper, we propose a real-time gesture detection algorithm by integrating cues from visual, speech and electronic slides. In contrast to the conventional "complete gesture" recognition, we emphasize detection by the prediction from "incomplete gesture". Specifically, intentional gestures are predicted by the modified hidden Markov model (HMM) which can recognize incomplete gestures before the whole gesture paths are observed. The multimodal correspondence between speech and gesture is exploited to increase the accuracy and responsiveness of gesture detection. In lecture presentation, this algorithm enables the on-the-fly editing of lecture slides by simulating appropriate camera motion to highlight the intention and flow of lecturing. We develop a real-time application, namely simulated smartboard, and demonstrate the feasibility of our prediction algorithm using hand gesture and laser pen with simple setup without involving expensive hardware.
C1 [Wang, Feng; Pong, Ting-Chuen] Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; City University of Hong
   Kong
RP Wang, F (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Clear Water Bay, Kowloon, Hong Kong, Peoples R China.
EM fwang@cs.cityu.edu.hk; cwngo@cs.cityu.edu.hk; tcpong@ust.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
CR ABOWD G, 2000, ACM MULTIMEDIA, P187
   [Anonymous], 2007, Video content analysis moves to the edge
   CHEN M, 2003, ACM MULT C BERK CA
   CHEN M, 2002, ACM MULT C JUAN PINS, P476
   Erol B., 2003, ACM MULTIMEDIA, P498
   Gleicher M., 2000, Proceedings ACM Multimedia 2000, P375, DOI 10.1145/354384.354537
   Gleicher M.L., 2002, SMARTGRAPH 02, P9
   He LW, 2007, IEEE T MULTIMEDIA, V9, P198, DOI 10.1109/TMM.2006.886385
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   *IBM, IBM AUT AUD SYST
   Ju SX, 1998, IEEE T CIRC SYST VID, V8, P686, DOI 10.1109/76.718513
   Liu HC, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P77, DOI 10.1109/ICME.2002.1035722
   LIU Q, 2001, P SIGCHI C HUM FACT, P442
   LIU T, 2003, P INT C IM VID RETR, P362
   MACHNICKI E, 2002, P MULT COMP NETW SAN
   MAHMOOD TS, 2000, ACM MULTIMEDIA, P85
   Martin J., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P403, DOI 10.1109/AFGR.2000.840666
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   NGO CW, 2002, P INT C MULT EXP LAU, V2, P26
   ONISHI M, 2004, INT C PATT REC CAMBR, V1, P23
   PHUNG DQ, 2003, ACM MULT C BERK CA, P287
   ROWE LA, BMRC LECT BROWSER
   Rui Y., 2003, ACM CHI, P457
   Shih TK, 2007, IEEE T MULTIMEDIA, V9, P487, DOI 10.1109/TMM.2006.886265
   Starner T, 1998, IEEE T PATTERN ANAL, V20, P1371, DOI 10.1109/34.735811
   WALLICK MN, 2005, SIGGRAPH
   Wang F, 2004, INT C PATT RECOG, P934, DOI 10.1109/ICPR.2004.1334682
   WANG F, 2003, ACM MULTIMEDIA, P315
   WANG F, 2005, ACM MULT C HILT SING, P327
   WANG F, 2006, P INT C MULT EXP TOR, P653
   Wang F, 2007, IEEE T MULTIMEDIA, V9, P397, DOI 10.1109/TMM.2006.886292
   Young S.J., 1993, HTK HIDDEN MARKOV MO
NR 32
TC 10
Z9 12
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 926
EP 935
DI 10.1109/TMM.2008.922871
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800022
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Fu, Y
   Huang, TS
AF Fu, Yun
   Huang, Thomas S.
TI Human age estimation with regression on discriminative aging manifold
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE age estimation; conformal embedding analysis; manifold; multiple linear
   regression; subspace learning
ID FACE; RECOGNITION; CLASSIFICATION
AB Recently, extensive studies on human faces in the Human-Computer Interaction (HCI) field reveal significant potentials for designing automatic age estimation systems via face image analysis. The success of such research may bring in many innovative HCI tools used for the applications of human-centered multimedia communication. Due to the temporal property of age progression, face images with aging features may display some sequential patterns with low-dimensional distributions. In this paper, we demonstrate that such aging patterns can he effectively extracted from a discriminant subspace learning algorithm and visualized as distinct manifold structures. Through the manifold method of analysis on face images, the dimensionality redundancy of the original image space can be significantly reduced with subspace learning. A multiple linear regression procedure, especially with a quadratic model function, can be facilitated by the low dimensionality to represent the manifold space embodying the discriminative property. Such a processing has been evaluated by extensive simulations and compared with the state-of-the-art methods. Experimental results on a large size aging database demonstrate the effectiveness and robustness of our proposed framework.
C1 [Fu, Yun; Huang, Thomas S.] Univ Illinois, Beckman Inst Adv Sci & Technol, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Fu, Y (corresponding author), Univ Illinois, Beckman Inst Adv Sci & Technol, 405 N Mathews Ave, Urbana, IL 61801 USA.
EM yunfu2@ifp.uiuc.edu; huang@ifp.uiuc.edu
RI yan, shuicheng/A-8531-2014; yan, shuicheng/HCH-9860-2022
OI yan, shuicheng/0000-0001-8906-3777; yan, shuicheng/0000-0003-4527-1018
FU Beckman Graduate Fellowship; U.S. Government VACE program; National
   Science Foundation [CCF 04-26627]
FX This work was supported in part by the Beckman Graduate Fellowship, the
   U.S. Government VACE program, and National Science Foundation Grant CCF
   04-26627. The views and conclusions are those of the authors, not of the
   US Government or its Agencies.
CR [Anonymous], 1991, P 1991 IEEE COMP SOC, DOI DOI 10.1109/CVPR.1991.139758
   [Anonymous], 2006, P 14 ANN ACM INT C M
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Cai D, 2006, IEEE T IMAGE PROCESS, V15, P3608, DOI 10.1109/TIP.2006.881945
   Chen HT, 2005, PROC CVPR IEEE, P846
   Deffenbacher KA, 1998, PERCEPTION, V27, P1233, DOI 10.1068/p271233
   Duda R., 1973, Pattern Classification and Scene Analysis
   FU Y, 2007, IEEE C CVPR 07 WORKS
   Fu Y, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1383
   Fu Y, 2008, IEEE T IMAGE PROCESS, V17, P226, DOI 10.1109/TIP.2007.914203
   Fu Y, 2006, IEEE T CIRC SYST VID, V16, P830, DOI 10.1109/TCSVT.2006.877398
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   HE XF, 2003, P NIPS 03
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Kumar B.V. K. V., 2006, CORRELATION PATTERN
   Kwon YH, 1999, COMPUT VIS IMAGE UND, V74, P1, DOI 10.1006/cviu.1997.0549
   Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Ma Y., 2007, INT C MACHINE LEARNI, V227, P577, DOI DOI 10.1145/1273496.1273569
   Pang YW, 2005, LECT NOTES COMPUT SC, V3644, P117
   Ramanathan N., 2006, IEEE COMP SOC C COMP, P387, DOI [DOI 10.1109/CVPR.2006.187, 10.1109/CVPR.2006.187]
   Ramanathan N, 2006, IEEE T IMAGE PROCESS, V15, P3349, DOI 10.1109/TIP.2006.881993
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Seung HS, 2000, SCIENCE, V290, P2268, DOI 10.1126/science.290.5500.2268
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Weisberg S., 2004, APPL LINEAR REGRESSI, V3rd
   YAN S, 2007, IEEE C ICME 07
   Yan YD, 2007, COMP MATER SCI, V40, P1, DOI 10.1016/j.commatsci.2006.10.020
   [No title captured]
NR 30
TC 289
Z9 326
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 578
EP 584
DI 10.1109/TMM.2008.921847
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200003
DA 2024-07-18
ER

PT J
AU Schreer, O
   Englert, R
   Elsert, P
   Tanger, R
AF Schreer, Oliver
   Englert, Roman
   Elsert, Peter
   Tanger, Ralf
TI Real-time vision and speech driven avatars for multimedia applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE avatar; multimodality; real-time tracking; segmentation
AB Recent progress in advanced video communication services and multimedia applications is grounded on novel human machine interfaces, improved usability, and user friendliness driven by user centric research and development. In this paper, we describe a complete system concept and algorithmic details of an example application within this area. The key features of the system are vision and speech based interfaces, which are used to animate an avatar for an audio-visual representation of a communication partner. The system is applied in two application scenarios, namely video chat and customer care services. Both applications are mass-market oriented and therefore careful design and development of robust and supporting user interfaces are required. The presented approach is integrated into a complete real-time prototype system, which is permanently demonstrated in the showcase at the head quarter of Deutsche Telekom, Bonn, Germany.
C1 [Schreer, Oliver; Elsert, Peter; Tanger, Ralf] Heinrich Hertz Inst Nachrichtentech Berlin GmbH, Fraunhofer Inst Telecommun, D-10587 Berlin, Germany.
   [Englert, Roman] Ben Gurion Univ Negev, Deutsch Telekom Labs, IL-84105 Beer Sheva, Israel.
C3 Fraunhofer Gesellschaft; Ben Gurion University; Deutsche Telekom AG
RP Schreer, O (corresponding author), Heinrich Hertz Inst Nachrichtentech Berlin GmbH, Fraunhofer Inst Telecommun, D-10587 Berlin, Germany.
EM oliver.schreer@hhi.fraunhofer.de; roman.englert@telekom.de;
   Peter.Eisert@hhi.fraunhofer.de; Ralf.Tanger@hhi.fraunhofer.de
RI Eisert, Peter/AAX-7968-2020
OI Schreer, Oliver/0000-0001-6900-8287; Eisert, Peter/0000-0001-8378-4805
FU Deutsche Telekom Laboratories, Germany
FX This work was supported by Deutsche Telekom Laboratories, Germany. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Alan Hanjalic.
CR ASKAR S, 2004, P 1 EUR C VIS MED PR
   Chang CC, 2005, SMART STRUCT SYST, V1, P29, DOI 10.12989/sss.2005.1.1.029
   COOGAN T, 2006, P INT S COMP VIS ISV
   COSTANZO C, 2003, P INT PAR DISTR PROC, P112
   DORFMUELLERULHA.K, 2001, P ACM IEEE INT S AUG, P30
   Eisert P, 1998, IEEE COMPUT GRAPH, V18, P70, DOI 10.1109/38.708562
   Eisert P., 1997, PROC 3D IMAGE ANAL S, P51
   Eisert P, 2006, SIGNAL PROCESS-IMAGE, V21, P493, DOI 10.1016/j.image.2006.03.003
   Ekman P, 1978, FACIAL ACTION CODING
   ENGLERT R, 2006, P 20 S HUM FACT TEL
   Hasanuzzaman M, 2004, IEEE ROBIO 2004: PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, P413
   Hu YX, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P651
   Iannizzotto G, 2005, LECT NOTES COMPUT SC, V3617, P115, DOI 10.1007/11553595_14
   Imagawa K, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P462, DOI 10.1109/AFGR.1998.670991
   *ISO IEC FDIS, 1999, 144962 ISOIEC FDIS
   Jennings C., 1999, Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378), P152, DOI 10.1109/RATFG.1999.799238
   JUST A, 2004, BRIT MACH VIS C LOND
   LOVELL BC, 2002, P AS C COMP VIS, P336
   MacLean J, 2001, IEEE ICCV WORKSHOP ON RECOGNITION, ANALYSIS AND TRACKING OF FACES AND GESTURES IN REAL-TIME SYSTEMS, PROCEEDINGS, P133, DOI 10.1109/RATFG.2001.938922
   Malassiotis S, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P955, DOI 10.1109/ICIP.2001.958283
   MO Z, 2006, P IEEE INT C COMP VI, P7
   Moeslund TB, 2001, COMPUT VIS IMAGE UND, V81, P231, DOI 10.1006/cviu.2000.0897
   MORISHIMA S, 1989, P IEEE ICASSP GLASG, P1795
   Oka K, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P429, DOI 10.1109/AFGR.2002.1004191
   Pandzic I., 2002, MPEG-4 Facial Animation: The Standard, Implementation and Applications
   Pavlovic VI, 1997, IEEE T PATTERN ANAL, V19, P677, DOI 10.1109/34.598226
   RAO RR, 1996, P PICT COD S MAR
   SCHREER O, 2007, P INT C IM AN PROC M
   Sigal L, 2004, IEEE T PATTERN ANAL, V26, P862, DOI 10.1109/TPAMI.2004.35
   Starner T, 2003, MACH VISION APPL, V14, P59, DOI 10.1007/s00138-002-0096-8
   STORRING M., 1999, P 7 S INTELLIGENT RO, P187
   THEOBALD BJ, 2003, P INT C ACOUSTICS SP, V5, P800
   YPSILOS IA, 2004, P IEEE S 3D DAT PROC
   ZHU X, 2000, P 4 INT C AUT FAC GE, P446
NR 34
TC 11
Z9 13
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 352
EP 360
DI 10.1109/TMM.2008.917336
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100005
DA 2024-07-18
ER

PT J
AU Mastronarde, NH
   van der Schaar, M
AF Mastronarde, Nicholas H.
   van der Schaar, Mihaela
TI A queuing-theoretic approach to task scheduling and processor selection
   for video-decoding applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE middleware; multiprocessor decoding; multimedia systems; queuing models
   for systems; real-time system scheduling; video decoding complexity
ID TAIL PROBABILITIES; ADAPTATION; COMPLEXITY; SYSTEMS; MULTIPROCESSOR;
   H.264/AVC; QUALITY
AB We propose a cross-layer design for resource-constrained systems that simultaneously decode multiple video streams on multiple parallel processors, cores, or processing elements. Our proposed design explicitly considers the coder specific application characteristics such as the decoding dependencies, decoding deadlines, and distortion impacts of different video packets (e.g., frames, slices, groups of slices etc.). The key to the cross-layer design is the resource management control plane (RMCP) that coordinates the scheduling and processor selection across the active applications. The RMCP deploys a priority-queuing model that can evaluate the system congestion and predict the total expected video quality for the set of active decoding tasks. Using this model, we develop a robust distortion- and delay-aware scheduling algorithm for video packets. This algorithm aims to maximize the sum of achieved video qualities over all of the decoded video sequences. Additionally, we propose a processor selection scheme intended to minimize the delays experienced by the queued video packets. In this way, the number of missed decoding deadlines is reduced and the overall decoded video quality is increased. We compare queuing-theoretic based scheduling strategies to media agnostic scheduling strategies (i.e., earliest-deadline-first scheduling) that do not jointly consider the decoding deadlines and distortion impacts. Our results illustrate that by directly considering the video application's properties in the design of a video decoding system, significant system performance gains on the order of 4 dB peak-signal-to-noise ratio can be achieved.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Mastronarde, NH (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM nhmistro@ee.ucla.edu; mihaela@ee.uc1a.edu
RI Mastronarde, Nicholas/W-5332-2019; Mastronarde, Nicholas/IZD-7746-2023
OI Mastronarde, Nicholas/0000-0002-8474-7237; Mastronarde,
   Nicholas/0000-0002-8474-7237
CR ABATE J, 1995, OPER RES, V43, P885, DOI 10.1287/opre.43.5.885
   [Anonymous], SIGOPS OPER SYST REV
   [Anonymous], 1998, Fundamentals of Queuing Theory
   Bertsekas D. P., 1992, Data Networks, V2nd
   BURNS A, 1991, SOFTWARE ENG J, V6, P116, DOI 10.1049/sej.1991.0015
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Dutta S, 2001, IEEE DES TEST COMPUT, V18, P21, DOI 10.1109/54.953269
   HACHMANN M, 2005, AMD DETAILS EARLY QU
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   Jiang YM, 2001, IEEE COMMUN LETT, V5, P175, DOI 10.1109/4234.917105
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P629, DOI 10.1109/TMM.2006.888017
   Kahle JA, 2005, IBM J RES DEV, V49, P589, DOI 10.1147/rd.494.0589
   Kleinrock L., 1975, Queuing Systems, VI
   Lappalainen V, 2003, IEEE T CIRC SYST VID, V13, P717, DOI 10.1109/TCSVT.2003.814968
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   van der Schaar M, 2005, IEEE T MULTIMEDIA, V7, P471, DOI 10.1109/TMM.2005.846790
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   VANDERTOL E, 2003, P SPIE C IM VID COMM
   Wang Y, 2005, IEEE T CIRC SYST VID, V15, P1270, DOI 10.1109/TCSVT.2005.854224
   Yuan W., 2003, PROC ACM S OPERATING, P149
   Yuan WH, 2006, IEEE T MOBILE COMPUT, V5, P799, DOI 10.1109/TMC.2006.98
NR 21
TC 7
Z9 11
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1493
EP 1507
DI 10.1109/TMM.2007.906568
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400014
DA 2024-07-18
ER

PT J
AU Balam, J
   Gibson, JD
AF Balam, Jagadeesh
   Gibson, Jerry D.
TI Multiple descriptions and path diversity for voice communications over
   wireless mesh networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE AMR-WB; CELP; G.729; mesh networks; MOS; multiple descriptions; path
   diversity; PESQ; voice communications
AB A key feature of wireless mesh networks is that multiple independent paths through the network are available. Multiple descriptions coding is often suggested as a source coding scheme to take advantage of this path diversity. We compare multiple description (MD) coding with path diversity (PD) against a full-rate single description (SD) coder without PD, and two simple PD methods of 1) repeating a half-rate SD coder over both paths and 2) repeating the full-rate parent SD coder over the two paths. We first present a theoretical analysis comparing the average distortion per symbol in packetized communication using the above mentioned MD and PD methods to transmit a memoryless Gaussian source over additive white Gaussian noise channels. Next, using two new MD speech coders with balanced side descriptions derived from the AMR-WB and G.729 standards, we evaluate delivered voice quality using PESQ-MOS and compare MD coding against the PD methods for random and bursty packet losses. Both the theoretical analyses and the speech coding experiments show that with packet overheads, the simple PD methods may be preferable to MD coding. A new performance measure that incorporates both quality and bit rate is shown to account for the tradeoffs more explicitly.
C1 Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Balam, J (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
EM balam@ece.ucsb.edu; gibson@ece.ucsb.edu
CR Akyildiz IF, 2005, IEEE COMMUN MAG, V43, pS23, DOI 10.1109/MCOM.2005.1509968
   Anandakumar AK, 2000, INT CONF ACOUST SPEE, P3682, DOI 10.1109/ICASSP.2000.860201
   [Anonymous], P862 ITUT
   [Anonymous], P IEEE INFOCOM
   BALAM J, 2006, P IWCMC VANC BC CAN
   BALAM J, 2006, P INF THEOR APPL WOR
   BALAM J, 2005, P AS C SIGN SYST COM
   BARRIAC V, 2006, DISCUSSION UNIFIED O
   BATLLO JC, 1994, P IEEE INT S INF THE
   BESSETTE B, 2002, IEEE T SPEECH AUD PR
   DONG H, 2003, P AS C SIGN SYST COM
   DONG H, 2004, ICASSP 04
   DONG H, 2004, P IEEE WIR COMM NETW
   EFFROS M, 2004, INF THEOR WORKSH 200
   ELGAMAL AA, 1982, IEEE T INFORM THEORY, V28, P851, DOI 10.1109/TIT.1982.1056588
   FENG H, 2003, P IEEE INT S INF THE
   Gogate N, 2002, IEEE T CIRC SYST VID, V12, P777, DOI 10.1109/TCSVT.2002.803229
   GOYAL VK, 2001, IEEE SIGN PROC MAG S
   HOENE C, 2001, SPIE 2001 VOIC OV IP, P157
   INGLE A, 1995, IEEE T SPEECH AUDI P, V3, P48, DOI 10.1109/89.365381
   JAYANT NS, 1981, IEEE T COMMUN, V29, P101, DOI 10.1109/TCOM.1981.1094975
   JIANG W, 2000, P IEEE INT C MULT EX, V1, P444
   KUBIN G, 1999, IEEE WORKSH SPEECH C, P81
   Lee CC, 2000, 2000 IEEE WORKSHOP ON SPEECH CODING, PROCEEDINGS, P69, DOI 10.1109/SCFT.2000.878399
   LIN C, 2004, WMASH 04 NEW YORK, P11
   MIU A, 2003, IEEE INT C MULT EXPO
   OZAROW L, 1980, BELL SYST TECH J, V59, P1909, DOI 10.1002/j.1538-7305.1980.tb03344.x
   PETRACCA M, 2004, P 1 INT S CONTR COMM
   Rix AW, 2003, P ONL WORKSH MEAS SP, P17
   Servetti A, 2005, IEEE VTS VEH TECHNOL, P2330
   SERVETTI A, 2004, P SPEC WORKSH MAU SW
   SERVETTI A, 2003, P IEEE INT WORKSH DS
   SHETTY N, 2006, P IWCMC VANC BC CAN
   Voran SD, 2005, INT CONF ACOUST SPEE, P129
   WAH BW, 2002, P ICME 02, V2, P597
   WANG Y, 2002, P 2002 INT C IM PROC, V1
   ZHONG X, 2002, ICASSP
   2006, SPECTRALINK VOICE PR
   2001, SG12 ITUT
   26190 3GPP TS
   2000, G729 ITUT
   1993, G711 ITUT
   1996, G191 ITUT
NR 43
TC 15
Z9 16
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 1073
EP 1088
DI 10.1109/TMM.2007.898945
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Volkmer, T
   Thom, JA
   Tahaghoghi, SMM
AF Volkmer, Timo
   Thom, James A.
   Tahaghoghi, Seyed M. M.
TI Modeling human judgment of digital imagery for multimedia retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE annotation; latent class modeling
AB The application of machine learning techniques to image and video search has been shown to boost the performance of multimedia retrieval systems, and promises to lead to more generalized semantic search approaches. In particular, the availability of large training collections allows model-driven search using a substantial number of semantic concepts. The training collections are obtained in a manual annotation process where human raters review images and assign predefined semantic concept labels. Besides being prone to human error, manual image annotation is biased by the view of the individual annotator because visual information almost always leaves room for ambiguity. Ideally, several independent judgments are obtained per image, and the inter-rater agreement is assessed. While disagreement between ratings bears valuable information on the annotation quality, it complicates the task of clearly classifying rated images based on multiple judgments. In the absence of a gold standard, evaluating multiple judgments and resolving disagreement between raters is not trivial. In this paper, we present an approach using latent structure analysis to solve this problem. We apply latent class modeling to the annotation data collected during the TRECVID 2005 Annotation Forum, and demonstrate how to use this statistic to clearly classify each image on the basis of varying numbers of ratings. We use latent class modeling to quantify the annotation quality and discuss the results in comparison with the well-known Kappa inter-rater agreement measure.
C1 RMIT Univ, Sch Comp Sci & Informat Technol, Melbourne, Vic 3001, Australia.
C3 Royal Melbourne Institute of Technology (RMIT)
RP Volkmer, T (corresponding author), RMIT Univ, Sch Comp Sci & Informat Technol, Melbourne, Vic 3001, Australia.
EM tvolkmer@cs.rmit.edu.au; jat@cs.rmit.edu.au; saied@cs.rmit.edu.au
OI Thom, James/0000-0002-6666-9132
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], P ACM C HUM FACT COM
   COHEN J, 1960, EDUC PSYCHOL MEAS, V20, P37, DOI 10.1177/001316446002000104
   Di Eugenio B, 2004, COMPUT LINGUIST, V30, P95, DOI 10.1162/089120104773633402
   Fleiss J., 2003, STAT METHODS RATES P, P598, DOI DOI 10.1002/0471445428.CH18
   HARTLEY HO, 1958, BIOMETRICS, V14, P174, DOI 10.2307/2527783
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Rost J., 1997, Applications of latent trait and latent class models in the social sciences, P13
   Smeaton AF, 2005, LECT NOTES COMPUT SC, V3568, P11
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   UEBERSAX JS, 1990, STAT MED, V9, P559, DOI 10.1002/sim.4780090509
   UEBERSAX JS, 1988, PSYCHOL BULL, V104, P405, DOI 10.1037/0033-2909.104.3.405
   Volkmer T., 2005, PROC 13 ANN ACM INT, P892, DOI DOI 10.1145/1101149.1101341
   [No title captured]
   [No title captured]
   [No title captured]
NR 16
TC 8
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 967
EP 974
DI 10.1109/TMM.2007.900153
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800006
DA 2024-07-18
ER

PT J
AU Corsini, M
   Gelasca, ED
   Ebrahimi, T
   Barni, M
AF Corsini, Massimiliano
   Gelasca, Elisa Drelie
   Ebrahimi, Touradj
   Barni, Mauro
TI Watermarked 3-D mesh quality assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D objects quality assessment; 3-D watermarking; mesh watermarking;
   objective metrics; perceptual metrics; subjective evaluation
ID METRICS
AB This paper addresses the problem of assessing distortions produced by watermarking 3-D meshes. In particular, a new methodology for subjective evaluation of the quality of 3-D objects is proposed and implemented. Two objective metrics derived from measures of surface roughness are then proposed and their efficiency to predict the perceptual impact of 3-D watermarking is assessed and compared with the state of the art. Results obtained show good correlations between the proposed objective metrics and subjective assessments by human observers.
C1 CNR, Ist Sci & Tecnol Informaz, Comp Vis Lab, I-56124 Pisa, Italy.
   Univ Calif Santa Barbara, Dept Elect & Comp Engn, Vis Res Lab, Santa Barbara, CA 93106 USA.
   Ecole Polytech Fed Lausanne, Signal Proc Inst, CH-1015 Lausanne, Switzerland.
   Univ Siena, Dept Informat Engn, I-53100 Siena, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR); University
   of California System; University of California Santa Barbara; Swiss
   Federal Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne; University of Siena
RP Corsini, M (corresponding author), CNR, Ist Sci & Tecnol Informaz, Comp Vis Lab, I-56124 Pisa, Italy.
EM massimiliano.corsini@isti.cnr.it; elisa.drelie@a3.epfl.ch;
   touradj.ebrahimi@epfl.ch; barni@dii.unisi.it
RI Corsini, Massimiliano/B-6375-2015
OI Corsini, Massimiliano/0000-0003-0543-1638; Ebrahimi,
   Touradj/0000-0002-9900-3687
CR [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   Aspert N, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P705, DOI 10.1109/ICME.2002.1035879
   Bartolini F, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P450, DOI 10.1109/ICIP.1998.723523
   Benedens O, 1999, PROC SPIE, V3657, P329, DOI 10.1117/12.344683
   Benedens O., 1999, P MULT SEC WORKSH AC, V99, P95
   BOLIN MR, 1998, P 25 ANN C COMP GRAP, P299
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   CORSINI M, 2005, WORKSH IM AN MULT IN
   Cox IJ, 1997, P SOC PHOTO-OPT INS, V3016, P92, DOI 10.1117/12.274502
   Desbrun M, 1999, COMP GRAPH, P317, DOI 10.1145/311535.311576
   Ferwerda J. A., 1997, Proc. ACM SIGGRAPH, P143
   Gelasca E. D., 2005, IEEE INT C IM PROC, P241
   Goldstein E.Bruce., 1996, SENSATION PERCEPTION
   KANAI S, 1998, P 6 IFIP WG 5 2 GEO, P296
   KOBBELT L, 1997, P 7 IMA C MATH SURF, P101
   Lehmann E.L., 1998, NONPARAMETRIC STAT M
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   Lubin J., 1993, DIGITAL IMAGES HUMAN
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   Shacked R, 2001, COMPUT GRAPH FORUM, V20, pC215, DOI 10.1111/1467-8659.00514
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   UCCHEDDU F, 2004, P 2004 MULT SEC WORK, P143
   UCCHEDDU F, 2004, P 10 INT C VIRT SYST
   VANSCHYNDELL RG, 1994, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP.1994.413536
   Williams MC, 2003, ELEC SOC S, V2003, P3
   Winkler S, 2003, PROC SPIE, V5203, P371, DOI 10.1117/12.512550
   Wolfgang RB, 1999, P IEEE, V87, P1108, DOI 10.1109/5.771067
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
NR 30
TC 99
Z9 115
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 247
EP 256
DI 10.1109/TMM.2006.886261
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Om, H
   Chand, S
AF Om, Hari
   Chand, Satish
TI Geometrico-harmonic broadcasting scheme with continuous redundancy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cautious harmonic scheme; harmonic scheme; logical channel; polyharmonic
   scheme; quasi-harmonic scheme; video channel
ID RECEIVING SCHEME; VIDEO
AB The harmonic broadcasting scheme has the best performance for the user latency. It, however, does not always provide the video data in time to the users. To provide the video data reliably, its two main variants-cautious and quasi-harmonic schemes have been proposed. They require more bandwidth than the harmonic scheme. The cautious and quasi-harmonic schemes need 0.50b and 0.1771b more bandwidth, respectively, than the harmonic scheme in limiting case. In this paper, a new broadcasting scheme: geometrico-harmonic scheme with continuous redundancy is proposed. This scheme provides the video data in time unlike the harmonic scheme, and its bandwidth requirement can be had as close to that of the harmonic scheme as we please. Moreover, the extra bandwidth required in this scheme can be evaluated at any point of time, that is, it can be estimated for a fractional size of a segment and/or subsegment. The bandwidth is a scarce resource and its requirement for any fractional size of segments (or subsegments) may be helpful while dealing with the variable bandwidth rate-encoded videos. In the proposed scheme, the user latency can be made arbitrarily close to that of the harmonic scheme. In comparison to the cautious and the quasi-harmonic schemes, it has better performance for the user latency as well as the buffer storage. For disk transfer rate, it performs better than the quasi-harmonic scheme and in comparison to the cautious harmonic scheme its performance is same.
C1 Indian Sch Mines, Dept Comp Sci & Engn, Dhanbad 826004, Bihar, India.
   Netaji Subhas Inst Technol, Comp Engn Div, New Delhi 110075, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (Indian School of Mines) Dhanbad; Netaji Subhas University of
   Technology
RP Om, H (corresponding author), Indian Sch Mines, Dept Comp Sci & Engn, Dhanbad 826004, Bihar, India.
EM hariom63@rediffmail.com; schand86@hotmail.com
RI OM, HARI/AAY-6011-2021
OI OM, HARI/0000-0002-9750-2706
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   BARNETT SA, 1995, P C AUSTR TEL NETW
   Carter SW, 2001, APS COMM NETW MULTIM, P179
   CHERVENAK AL, 1994, THESIS BERKELEY U CA
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   DAN A, 1995, J PARALLEL DISTR COM, V30, P168, DOI 10.1006/jpdc.1995.1135
   EAGER DL, 1998, P 4 INT WORKSH MULT, P18
   GAO L, 1998, P NOSSDAV 98 CAMBR U, P183
   Gao LX, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P117, DOI 10.1109/MMCS.1999.778179
   Griwodz C, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P349, DOI 10.1145/266180.266386
   Hua K., 1997, PROC SIGCOMM, P89
   Jackson WG, 2002, INORG REACT MECH, V4, P1, DOI 10.1080/102866202100002518
   Juhn LS, 1998, IEEE T CONSUM ELECTR, V44, P343, DOI 10.1109/30.681948
   Juhn LS, 1997, IEEE T CONSUM ELECTR, V43, P1110, DOI 10.1109/30.642378
   Juhn LS, 1998, IEEE T BROADCAST, V44, P100, DOI 10.1109/11.713059
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Paris J.-F., 1999, Proceedings Eight International Conference on Computer Communications and Networks (Cat. No.99EX370), P118, DOI 10.1109/ICCCN.1999.805505
   Pâris JF, 1998, IEEE IC COMP COM NET, P690, DOI 10.1109/ICCCN.1998.998831
   Paris JF, 1998, SIXTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P127, DOI 10.1109/MASCOT.1998.693685
   Pâris JF, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P189, DOI 10.1145/319463.319600
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   WONG JW, 1988, P IEEE, V76, P1566, DOI 10.1109/5.16350
   Yang ZY, 1999, IEEE T BROADCAST, V45, P318, DOI 10.1109/11.796274
NR 25
TC 3
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 410
EP 419
DI 10.1109/TMM.2006.886295
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900018
DA 2024-07-18
ER

PT J
AU Doërr, G
   Dugelay, JL
   Kirovski, D
AF Doerr, Gwenael
   Dugelay, Jean-Luc
   Kirovski, Darko
TI On the need for signal-coherent watermarks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE block replacement attack; self-similarities in multimedia data;
   signal-coherent watermarks
AB Digital watermarking has been introduced in the 1990s as a complementary technology for copyright protection. In an effort to anticipate hostile behavior of adversaries, the research community is constantly introducing new attacks to benchmark watermarking systems. In this paper, we present a generic attack strategy based on block replacement. As multimedia content is often highly repetitive, the attack exploits signal's self-similarities to replace each signal block with another, perceptually similar one. Guided by the principles of the proposed attack framework, we implemented three attack algorithms for different types of multimedia content: video shots, audio tracks and still images. Finally, considering the effectiveness of the proposed algorithms, we identify the properties that a watermark should have to counter this attacking strategy.
C1 UCL, Ipswich IP5 3RE, Suffolk, England.
   Eurecom Inst, F-06904 Sophia Antipolis, France.
   Micorsoft Res, Redmond, WA 98052 USA.
C3 University of London; University College London; IMT - Institut
   Mines-Telecom; EURECOM
RP Doërr, G (corresponding author), UCL, Adastral Pk Postgrad Campus, Ipswich IP5 3RE, Suffolk, England.
EM g.doerr@adastral.ucl.ac.uk; dugelay@eurecom.fr; darkok@microsoft.com
RI DUGELAY, Jean-Luc/ABE-7096-2021
OI DUGELAY, jean-luc/0000-0003-3151-4330; Doerr,
   Gwenael/0009-0005-5124-3111
CR [Anonymous], LNCS
   BRANDENBURG K, 1998, APPL DIGITAL SIGNAL
   Cox I., 2001, Digital Watermarking
   Doërr G, 2004, PROC SPIE, V5306, P304, DOI 10.1117/12.526914
   Doérr G, 2004, IEEE T SIGNAL PROCES, V52, P2955, DOI 10.1109/TSP.2004.833867
   DOERR G, 2003, P IEEE INT C MULT EX, V2, P505
   DOERR G, 2004, P ACM MULT SEC WORKS, P133
   Fisher Y., 1994, Fractal Image Compression
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Holliman M, 2000, P SOC PHOTO-OPT INS, V3971, P186, DOI 10.1117/12.384972
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kalker T, 1999, PROC SPIE, V3657, P103, DOI 10.1117/12.344661
   Katzenbeisser SC, 2000, ART H COMP SCI LIBR, P17
   Kirovski D, 2002, LECT NOTES COMPUT SC, V2696, P177
   Kirovski Darko., 2001, LECT NOTES COMPUTER, V2137, P354
   Kutter M, 1999, PROC SPIE, V3657, P226, DOI 10.1117/12.344672
   LANGELAAR GC, 1998, P EUSIPCO 98, V4, P2281
   MALVAR H, 1999, P IEEE INT C AC SPEE, V3, P1421
   NICOLAS H, 1995, J VIS COMMUN IMAGE R, V6, P303, DOI 10.1006/jvci.1995.1026
   Oppenheim A. V., 1989, Discrete -Time Signal Processing
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Rey C, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P633, DOI 10.1109/ICIP.2002.1039050
   SMOLIC A, 1996, P PICT COD S MAR
   Su JK, 2002, IEEE T MULTIMEDIA, V4, P551, DOI 10.1109/TMM.2002.806535
   Voloshynovskiy S, 2000, PROC SPIE, V3971, P358, DOI 10.1117/12.384990
NR 26
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 896
EP 904
DI 10.1109/TMM.2006.879917
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400002
DA 2024-07-18
ER

PT J
AU Kankanhalli, MS
   Wang, J
   Jain, R
AF Kankanhalli, Mohan S.
   Wang, Jun
   Jain, Ramesh
TI Experiential sampling on multiple data streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dynamical systems; experiential computing; experiential sampling;
   sampling; visual attention
AB Multimedia systems must deal with multiple data streams. Each data stream usually contains significant volume of redundant noisy data. In many real-time applications, it is essential to focus the computing resources on a relevant subset of data streams at any given time instant and use it to build the model of the environment. We formulate this problem as an experiential sampling problem and propose an approach to utilize computing resources efficiently on the most informative subset of data streams. In this paper, we generalize our experiential sampling framework to multiple data streams and provide an evaluation measure for this technique. We have successfully applied this framework to the problems of traffic monitoring, face detection and monologue detection.
C1 Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
   Delft Univ Technol, Fac Elect Engn, Delft, Netherlands.
   Univ Calif Irvine, Bren Sch Informat & Comp Sci, Irvine, CA 92697 USA.
C3 National University of Singapore; Delft University of Technology;
   University of California System; University of California Irvine
RP Kankanhalli, MS (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117543, Singapore.
EM mohan@comp.nus.edu.sg; jun.wang@tudelft.nl; jain@ics.uci.edu
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR Abowd GD, 1999, LECT NOTES COMPUT SC, V1707, P304
   [Anonymous], 1988, Bayesian Statistics
   Chang EC, 2000, APPL COMPUT HARMON A, V9, P312, DOI 10.1006/acha.2000.0324
   Colombo C., 1996, IMAGE TECHNOLOGY, P109
   Debouk R, 2002, DISCRETE EVENT DYN S, V12, P417, DOI 10.1023/A:1019770124060
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Forsyth DA, 2001, INT J COMPUT VISION, V41, P109, DOI 10.1023/A:1011165200654
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jain R, 2003, COMMUN ACM, V46, P48, DOI 10.1145/792704.792729
   Kankanhalli MS, 2006, IEEE T MULTIMEDIA, V8, P937, DOI 10.1109/TMM.2006.879876
   Lieberman H, 2000, IBM SYST J, V39, P617, DOI 10.1147/sj.393.0617
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   Miller EK, 2000, NAT REV NEUROSCI, V1, P59, DOI 10.1038/35036228
   Most SB, 2001, PSYCHOL SCI, V12, P9, DOI 10.1111/1467-9280.00303
   NAVALPAKKAM V, 2002, P 2 WORKSH BIOL MOT, P453
   PIROLLI P, 1996, PSYCH REV, V106, P643
   Scholl BJ, 2001, COGNITION, V80, P1, DOI 10.1016/S0010-0277(00)00152-9
   Schwartz EL, 1995, NEURAL NETWORKS, V8, P1297, DOI 10.1016/0893-6080(95)00092-5
   VIOLA P, 2001, 200101 CAMBRIDGE RES
   Walther D, 2002, LECT NOTES COMPUT SC, V2525, P472
NR 22
TC 8
Z9 8
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 947
EP 955
DI 10.1109/TMM.2006.879875
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400007
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Fang, HC
   Chang, YW
   Wang, TC
   Huang, CT
   Chen, LG
AF Fang, Hung-Chi
   Chang, Yu-Wei
   Wang, Tu-Chih
   Huang, Chao-Tsung
   Chen, Liang-Gee
TI High-performance JPEG 2000 encoder with rate-distortion optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE discrete wavelet transform; embedded block coding with optimized
   truncation; HDTV; image compression; JPEG 2000; rate-distortion
   optimization
ID IMAGE COMPRESSION; ARCHITECTURE
AB An 81 MSamples/s JPEG 2000 single-chip encoder is implemented on 5.5 mm(2) area using 0.25-mu m CMOS technology. This IC can losslessly encode HDTV 720p resolution at 30 frames/s in real time. Three techniques are adopted: line-based discrete wavelet transform, parallel embedded block coding, and precompression rate-distortion optimization. The line-based discrete wavelet transform achieves the minimum external memory access, while the internal memory is reduced by a proper memory access scheme. The parallel embedded block coding increases the throughput and reduces the memory bandwidth with similar hardware cost comparing to conventional architectures. By accurately estimating bit rates, the precompression rate-distortion optimization reduces the required computational power and processing time of the embedded block coding since the code-blocks are truncated before compression. Experimental results show that this encoder has the highest throughput with the smallest area compared with other designs in the literature.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10617, Taiwan.
   Natl Taiwan Univ, Grad Inst Elect Engn, DSP IC Design Lab, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Fang, HC (corresponding author), MediaTek Inc, Hsinchu 300, Taiwan.
EM honchi@video.ee.ntu.edu.tw; wayne@video.ee.ntu.edu.tw;
   eric@video.ee.ntu.edu.tw; supertg@video.ee.ntu.edu.tw;
   lgchen@video.ee.ntu.edu.tw
OI CHEN, LIANG-GEE/0000-0001-9746-9355
CR *AMPHION, 2002, CS6510 ONL
   Andra K, 2002, IEEE T SIGNAL PROCES, V50, P966, DOI 10.1109/78.992147
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Chang YW, 2004, PROC SPIE, V5308, P1353, DOI 10.1117/12.527682
   Fang HC, 2005, IEEE T CIRC SYST VID, V15, P1086, DOI 10.1109/TCSVT.2005.852618
   FANG HC, 2003, P IEEE INT S CIRC SY, V2, P736
   GALL L, 1988, P IEEE INT C AC SPEE, V2, P761
   Lian CJ, 2003, IEEE T CIRC SYST VID, V13, P219, DOI 10.1109/TCSVT.2003.809833
   Mallat S.G. A., 1989, IEEE T PATTERN ANAL, V11, P7
   Schumacher PR, 2003, IEEE T CONSUM ELECTR, V49, P780, DOI 10.1109/TCE.2003.1261155
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   TAUBMAN D, 2000, P IEEE INT C IM PROC, V2, P33
   Tseng PC, 2002, APCCAS 2002: ASIA-PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, VOL 1, PROCEEDINGS, P363, DOI 10.1109/APCCAS.2002.1114971
   Yamauchi H, 2003, ISSCC DIG TECH PAP I, V46, P46
NR 15
TC 9
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 645
EP 653
DI 10.1109/TMM.2006.876305
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300001
DA 2024-07-18
ER

PT J
AU Cai, L
   Shen, XM
   Pan, JP
   Mark, JW
AF Cai, L
   Shen, XM
   Pan, JP
   Mark, JW
TI Performance analysis of TCP-friendly AIMD algorithms for multimedia
   applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE congestion control; internet; multimedia; AIMD; quality-of-service;
   TCP-friendly
AB In this paper, the performance of TCP-friendly generic AIMD (Additive Increase and Multiplicative Decrease) algorithms for Web-based playback and multirate multimedia applications is investigated. The necessary and sufficient TCP-friendly condition is derived, and the effectiveness and responsiveness of AIMD are studied. Due to practical implications, a Dynamic TCP-friendly AIMD (DTAIMD) algorithm is proposed. Extensive simulation results are given to verify the derived necessary and sufficient condition, and to demonstrate the performance of the proposed DTAIMD algorithm.
C1 Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 University of Waterloo
RP Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
EM cai@bbcr.uwaterloo.ca; xshen@bbcr.uwaterloo.ca; jpan@nttmcl.com;
   jwmark@bbcr.uwaterloo.ca
RI Cai, Lin/C-3286-2016; Shen, Xuemin/AAH-2564-2020
OI Cai, Lin/0000-0002-1093-4865; Shen, Xuemin/0000-0002-4140-287X; Pan,
   Jianping/0000-0003-4893-6847
CR [Anonymous], 2018 IETF RFC
   [Anonymous], P ACM SIGCOMM 98
   BANSAL D, 2000, MITLCSTR806
   CHIU DM, 1989, COMPUT NETWORKS ISDN, V17, P1, DOI 10.1016/0169-7552(89)90019-6
   FFOYD S, 2000, P ACM SIGCOMM 2000, P43
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   Floyd S., NETWORK SIMULATOR LB
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   Floyd Sally, 1999, The newreno modification to tcp's fast recovery algorithm
   Jacobson V., 1988, Computer Communication Review, V18, P314, DOI 10.1145/52325.52356
   KARANDIKAR S, 2000, ACM COMPUT COMMUN RE, V30
   KELLY FP, 1985, J ROY STAT SOC B MET, V47, P379
   MATHIS M, 1997, ACM COMPUT COMMUN RE, V27
   Radha HM, 2001, IEEE T MULTIMEDIA, V3, P53, DOI 10.1109/6046.966110
   Ramakrishnan K, 1999, 2481 IETF RFC
   RAMJEE R, 1994, IEEE INFOCOM SER, P680, DOI 10.1109/INFCOM.1994.337672
   Rejaie R, 1999, COMP COMM R, V29, P189, DOI 10.1145/316194.316222
   Rejaie R, 1999, IEEE INFOCOM SER, P1337, DOI 10.1109/INFCOM.1999.752152
   YANG YR, 2000, GEN AIMD CONGESTION
   ZHANG H, 1995, P IEEE, V83, P1374, DOI 10.1109/5.469298
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 21
TC 64
Z9 67
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 339
EP 355
DI 10.1109/TMM.2005.843360
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400015
OA Green Published
DA 2024-07-18
ER

PT J
AU Gan, T
   Ma, KK
   Zhang, LR
AF Gan, T
   Ma, KK
   Zhang, LR
TI Dual-plan bandwidth smoothing for layer-encoded video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo
CY AUG 26-29, 2002
CL SWISS FED INST TECHNOL, LAUSANNE, SWITZERLAND
SP Ecole Polytech Fed Lausanne, IBM Res, France Telecon R&D, Interact Multimodal Informat Management
HO SWISS FED INST TECHNOL
DE bandwidth smoothing; renogiated constant bitrate network service;
   scalable video streaming
AB Traditional bandwidth smoothing techniques can be naturally supported by the renegotiated constant bit rate (RCBR) service model, but renegotiation failure in RCBR may cause buffer underflow and interrupt the playback of video. To address this concern, a novel dual-plan bandwidth smoothing (DBS) scheme is proposed in this paper by taking advantage of the SNR scalability of layer-encoded video. Upon renegotiation failure, the proposed scheme can adaptively discard certain enhancement layers to guarantee continuous video playback at the original frame rate. Experiments are carried out to demonstrate the validity of the proposed scheme. The impacts of renegotiation interval, granularity of enhancement layers, and playback buffer size on resulted video quality are also studied. From the simulation results, it is shown that the performance of the RCBR-based DBS scheme can be improved by 1) reducing the minimum time gap of renegotiation interval; 2) employing multilayer video encoding with finer granularity; and/or 3) increasing flee playback buffer size.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Ctr Signal Proc, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Ctr Signal Proc, Singapore 639798, Singapore.
EM gantong@pmail.ntu.edu.sg; ekkma@ntu.edu.sg; elzhang@ntu.edu.sg
RI Ma, Kai-Kuang/KBA-9411-2024; Ma, Kai-Kuang/A-5148-2011
CR FEN W, 1997, P IEEE INFOCOM KOB J, P58
   Feng WC, 1997, MULTIMEDIA SYST, V5, P297, DOI 10.1007/s005300050062
   FENG WC, 1995, COMPUT COMMUN, V18, P709, DOI 10.1016/0140-3664(95)98484-M
   Feng WC, 1999, IEEE T MULTIMEDIA, V1, P302, DOI 10.1109/6046.784468
   GAN T, 2002, P IEEE ICME LAUS SWI, P529
   Grossglauser M, 1997, IEEE ACM T NETWORK, V5, P741, DOI 10.1109/90.650136
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   SCHAAR M, 2001, IEEE T CIRCUITS SYST, V11, P318
   *SPRINT ADV TECH L, 2001, 2001125 SPRINT ADV T
   ZHANG C, 1998, INT SER ON ADVANC FR, V2, P1
   Zhang L, 2000, COMPUT COMMUN, V23, P133, DOI 10.1016/S0140-3664(99)00161-9
   Zhang ZL, 1999, IEEE INFOCOM SER, P472, DOI 10.1109/INFCOM.1999.751380
   MPEG 4 TRACES
NR 14
TC 12
Z9 14
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 379
EP 392
DI 10.1109/TMM.2005.843359
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400018
DA 2024-07-18
ER

PT J
AU Gutierrez-Osuna, R
   Kakumanu, PK
   Esposito, A
   Garcia, ON
   Bojorquez, A
   Castillo, JL
   Rudomin, I
AF Gutierrez-Osuna, R
   Kakumanu, PK
   Esposito, A
   Garcia, ON
   Bojorquez, A
   Castillo, JL
   Rudomin, I
TI Speech-driven facial animation with realistic dynamics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE face image analysis and synthesis; lip synchronization; 3-D audio/video
   processing
ID EXPRESSIONS; MODELS
AB This paper presents an integral system capable of generating animations with realistic dynamics, including the individualized nuances, of three-dimensional (3-D) human faces driven by speech acoustics. The system is capable of capturing short phenomena in the orofacial dynamics of a given speaker by tracking the 3-D location of various MPEG-4 facial points through stereovision. A perceptual transformation of the speech spectral envelope and prosodic cues are combined into an acoustic feature vector to predict 3-D orofacial dynamics by means of a nearest-neighbor algorithm. The Karhunen-Loeve transformation is used to identify the principal components of orofacial motion, decoupling perceptually natural components from experimental noise. We also present a highly optimized MPEG-4 compliant player capable of generating audio-synchronized animations at 60 frames/s. The player is based on a pseudo-muscle model augmented with a nonpenetrable ellipsoidal structure to approximate the skull and the jaw. This structure adds a sense of volume that provides more realistic dynamics than existing simplified pseudo-muscle-based approaches, yet it is simple enough to work at the desired frame rate. Experimental results on an audiovisual database of compact TIMIT sentences are presented to illustrate the performance of the complete system.
C1 Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
   Wright State Univ, Dept Comp Sci & Engn, Dayton, OH 45435 USA.
   Univ Naples 2, Dept Psychol, Naples, Italy.
   Univ N Texas, Coll Engn, Denton, TX 76203 USA.
   ITESM, CEM, Dept Comp Sci, Zaragoza 52926, Mexico.
C3 Texas A&M University System; Texas A&M University College Station;
   University System of Ohio; Wright State University Dayton; Universita
   della Campania Vanvitelli; University of North Texas System; University
   of North Texas Denton; Tecnologico de Monterrey
RP Texas A&M Univ, Dept Comp Sci, College Stn, TX 77843 USA.
EM rgutier@cs.tamu.edu; kpraveen@cs.wright.edu; iiass.anna@tin.it;
   ogarcia@unt.edu; adbojorq@itesm.mx; rudomin@itesm.mx
RI Esposito, Anna/GWC-6719-2022; ESPOSITO, Anna/P-4018-2015
OI ESPOSITO, Anna/0000-0002-7268-1795; Rudomin, Isaac/0000-0002-1672-1756;
   Gutierrez-Osuna, Ricardo/0000-0003-2817-2085
CR [Anonymous], 1998, Perceiving talking faces: From speech perception to a behavioral principle
   [Anonymous], 1999, AUDITORYVISUAL SPEEC
   Arslan LM, 1999, SPEECH COMMUN, V27, P81, DOI 10.1016/S0167-6393(98)00068-5
   ARUN KS, 1987, IEEE T PATTERN ANAL, V9, P699, DOI 10.1109/TPAMI.1987.4767965
   AVERSANO G, 2001, P IEEE MIDW S CIRC S
   Benoît C, 1998, SPEECH COMMUN, V26, P117, DOI 10.1016/S0167-6393(98)00045-4
   BERNSTEIN L, 1996, P INT C SPOK LANG PR, V3, P1477
   BESKOW J, 1995, P EUR MADR SPAIN
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Cohen M. M., 1993, MODELS TECHNIQUES CO, P141
   Cosatto E, 1998, COMP ANIM CONF PROC, P103, DOI 10.1109/CA.1998.681914
   Duda R., 1973, Pattern Classification and Scene Analysis
   Esposito A, 2002, PERSP NEURAL COMP, P178
   FINN K, 1986, THESIS GEORGETOWN U
   Garofolo J., 1988, Getting started with the DARPA TIMIT CD-ROM: An acoustic phonetic continuous speech database
   Hermansky H, 1994, IEEE T SPEECH AUDI P, V2, P578, DOI 10.1109/89.326616
   Hong PY, 2002, IEEE T NEURAL NETWOR, V13, P916, DOI 10.1109/TNN.2002.1021892
   *ISO IEC, 1999, JTC1SC29WG1M2725 ISO
   Kahler K., 2001, P INT C GRAPH INT, P27
   KAKUMANU P, 2002, THESIS WRIGHT STATE
   KAKUMANU P, 2001, P WORKSH PERC US INT
   Klatt D. H., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1278
   Kshirsagar S, 2001, INT FED INFO PROC, V68, P24
   Lavagetto F., 1995, IEEE Transactions on Rehabilitation Engineering, V3, P90, DOI 10.1109/86.372898
   Lavagetto F, 1999, IEEE T CIRC SYST VID, V9, P277, DOI 10.1109/76.752095
   Lee Y., 1995, SIGGRAPH, P55, DOI [10.1145/218380.218407, DOI 10.1145/218380.218407]
   MONTGOMERY AA, 1983, J ACOUST SOC AM, V73, P2134, DOI 10.1121/1.389537
   MORISHIMA S, 1991, IEEE J SEL AREA COMM, V9, P594, DOI 10.1109/49.81953
   Morishima S, 2001, IEEE SIGNAL PROC MAG, V18, P26, DOI 10.1109/79.924886
   Nedel LP, 1998, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P156, DOI 10.1109/CGI.1998.694263
   OHMAN T, 1998, AUDIO VISUAL SPEECH
   Ostermann J, 1998, COMP ANIM CONF PROC, P49, DOI 10.1109/CA.1998.681907
   Pandzic IS, 1999, VISUAL COMPUT, V15, P330, DOI 10.1007/s003710050182
   Parke F., 1996, COMPUTER FACIAL ANIM
   PARKE FI, 1982, IEEE COMPUT GRAPH, V2, P61
   PARKE FI, 1972, UTECCSC72120 DEP COM
   Pelachaud C, 1996, COGNITIVE SCI, V20, P1, DOI 10.1207/s15516709cog2001_1
   PICONE JW, 1993, P IEEE, V81, P1215, DOI 10.1109/5.237532
   Platt S. M., 1981, Computer Graphics, V15, P245, DOI 10.1145/965161.806812
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Rogozan A, 1998, SPEECH COMMUN, V26, P149, DOI 10.1016/S0167-6393(98)00056-9
   Rudomín I, 2001, J VISUAL COMP ANIMAT, V12, P215, DOI 10.1002/vis.261
   RUDOMIN I, 2002, EUROGRAPHICS    0909
   TERHARDT E, 1979, HEARING MECH SPEECH, P26
   Waters K., 1990, Proceedings of the First Conference on Visualization in Biomedical Computing (Cat. No.90TH0311-1), P77, DOI 10.1109/VBC.1990.109305
   WATERS K, 1987, P 14 ANN C COMP GRAP, P17
   Waters K., 1993, DECface: An automatic lipsynchronization algorithm for synthetic faces
   Yamamoto E, 1998, SPEECH COMMUN, V26, P105, DOI 10.1016/S0167-6393(98)00054-5
   Yehia H, 1998, SPEECH COMMUN, V26, P23, DOI 10.1016/S0167-6393(98)00048-X
   ZWICKER E, 1961, J ACOUST SOC AM, V33, P248, DOI 10.1121/1.1908630
NR 51
TC 36
Z9 44
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 33
EP 42
DI 10.1109/TMM.2004.840611
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300004
DA 2024-07-18
ER

PT J
AU Muneesawang, P
   Guan, L
AF Muneesawang, P
   Guan, L
TI An interactive approach for CBIR using a network of radial basis
   functions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based image retrieval; digital library; relevance feedback;
   machine learning; radial basis function network; nonlinear human
   perception
ID IMAGE RETRIEVAL; RELEVANCE FEEDBACK; FEATURES
AB An important requirement for constructing effective content-based image retrieval (CBIR) systems is accurate characterization of visual information. Conventional nonadaptive models, which are usually adopted for this task in simple CBIR systems, do not adequately capture all aspects of the characteristics of the human visual system. An effective way of addressing this problem is to adopt a "human-computer" interactive approach, where the users directly teach the system about what they regard as being significant image features and their own notions of image similarity. We propose a machine learning approach for this task, which allows users to directly modify query characteristics by specifying their attributes in the form of training examples. Specifically, we apply a radial-basis function (RBF) network for implementing an adaptive metric which progressively models the notion of image similarity through continual relevance feedback from users. Experimental results show that the proposed methods not only outperform conventional CBIR systems in terms of both accuracy and robustness, but also previously proposed interactive systems.
C1 Naresuan Univ, Dept Elect & Comp Engn, Phisanuloke 65000, Thailand.
   Ryerson Polytech Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Naresuan University; Toronto Metropolitan University
RP Naresuan Univ, Dept Elect & Comp Engn, Phisanuloke 65000, Thailand.
EM pmuneesa@ee.ryerson.ca; lguan@ee.ryerson.ca
CR [Anonymous], 1988, Spatial Vision
   [Anonymous], COMP VIS PATT REC WO
   BACK JR, 2000, VIRAGE IMAGE SEARCH
   Celentano A, 1998, J ELECTRON IMAGING, V7, P308, DOI 10.1117/1.482646
   CIOCCA G, 1999, VISUAL INFORMATION I, P105
   CUNNINGHAM SJ, 1998, BLAIN LINK COMPUTING
   DISCIASCIO E, 1999, P SOC PHOTO-OPT INS, V3656, P561
   Flickner M., 1995, IEEE COMPUT, V28, P23, DOI DOI 10.1109/2.410146
   Gersho A., 2003, Vector Quantization and Signal Compression
   Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602
   Haykin S., 1994, NEURAL NETWORKS COMP
   *ISO IEC JTC1 SC29, N3914 ISOIEC JTC1SC2
   Karayiannis NB, 1998, IEEE T IMAGE PROCESS, V7, P1223, DOI 10.1109/83.704313
   Ma WY, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P568, DOI 10.1109/ICIP.1997.647976
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   MULLER H, 2000, INT C PATT REC BARC
   MUNEESAWANG P, 2000, 1 IEEE PAC RIM C MUL, P188
   MUNEESAWANG P, 2000, P IEEE INT C IM PROC, V2, P526
   Peng J, 1999, COMPUT VIS IMAGE UND, V75, P150, DOI 10.1006/cviu.1999.0770
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326
   RIEDMAN JH, 1994, FLEXIBLE METRIC NEAR
   ROCCHIO JJ, 1971, SMAT RETRIEVAL SYSTE
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   RUI Y, 2000, P IEEE INT C COMP VI
   RUI Y, 1997, P IEEE WORKSH CONT B, P82, DOI DOI 10.1109/IVL.1997.629724
   SALTON G, 1983, INTODUCTON MODERN IN
   SCLAROFF S, 1997, P IEEE WORKSH CONT B, P2
   SFAR M, 2000, IEEE INT C MULT EXP, P141
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   TIKHONOV AN, 1999, NEURAL NETWORKS COMP
   Wu P, 2000, SIGNAL PROCESS-IMAGE, V16, P33, DOI 10.1016/S0923-5965(00)00016-3
   1999, COREL GALERY MAGIC 6
NR 35
TC 61
Z9 67
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 703
EP 716
DI 10.1109/TMM.2004.834866
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800004
DA 2024-07-18
ER

PT J
AU Babaguchi, N
   Kawai, Y
   Ogura, T
   Kitahashi, T
AF Babaguchi, N
   Kawai, Y
   Ogura, T
   Kitahashi, T
TI Personalized abstraction of broadcasted American football video by
   highlight selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE broadcasted sports video; event detection; highlight; personalization;
   video abstraction
AB Video abstraction is defined as creating shorter video clips or video posters from an original video stream. In this paper, we propose a method of generating a personalized abstract of broadcasted American football video. We first detect significant events in the video stream by matching textual overlays appearing in an image frame with the descriptions of gamestats in which highlights of the game are described. Then, we select highlight shots which should be included in the video abstract from those detected events reflecting on their significance degree and personal preferences, and generate a video clip by connecting the shots augmented with related audio and text. An hour-length video can be compressed into a minute-length personalized abstract. We experimentally verified the effectiveness of this method by comparing man-made video abstracts.
C1 Osaka Univ, Dept Elect Commun, Osaka 5650871, Japan.
   NHK Japan Broadcasting Corp, Tokyo 1508001, Japan.
   Kwansei Gakuin Univ, Dept Informat, Hyogo 6691337, Japan.
C3 Osaka University; NHK Japan Broadcasting Corp; Kwansei Gakuin University
RP Osaka Univ, Dept Elect Commun, Osaka 5650871, Japan.
EM babaguchi@comm.eng.osaka-u.ac.jp
CR Aigrain P, 1996, MULTIMED TOOLS APPL, V3, P179, DOI 10.1007/BF00393937
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Babaguchi N, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1519, DOI 10.1109/ICME.2000.871056
   Babaguchi N, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P782, DOI 10.1109/MMCS.1999.779299
   BABAGUCHI N, 2000, P ACM MULT 2000 WORK, P205
   CHANG SF, 2000, P IEEE ICME
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   ECHIGO T, 2000, P 4 AS C COMP VIS JA, P364
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   Hirsh H, 2000, COMMUN ACM, V43, P102, DOI 10.1145/345124.345159
   Jasinschi RS, 2001, INT CONF ACOUST SPEE, P1405, DOI 10.1109/ICASSP.2001.941192
   Lienhart R, 1997, COMMUN ACM, V40, P54, DOI 10.1145/265563.265572
   Merialdo B, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P323, DOI 10.1145/319463.319637
   OH JH, 2000, P IEEE ICME
   Riecken D, 2000, COMMUN ACM, V43, P27
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Smith MA, 1997, PROC CVPR IEEE, P775, DOI 10.1109/CVPR.1997.609414
   Smyth B, 2000, COMMUN ACM, V43, P107, DOI 10.1145/345124.345161
   TOKLU C, 2000, P IEEE ICME
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   ZHONG D, 2001, P IEEE ICME 01, P920
NR 22
TC 80
Z9 85
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 575
EP 586
DI 10.1109/tmm.2004.830811
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800006
DA 2024-07-18
ER

PT J
AU Mukherjee, DP
   Maitra, S
   Acton, ST
AF Mukherjee, DP
   Maitra, S
   Acton, ST
TI Spatial domain digital watermarking of multimedia objects for buyer
   authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE buyer key; digital watermarking; error correcting code; image key
ID IMAGES
AB Most of the existing watermarking processes become vulnerable when the attacker knows the watermark insertion algorithm. This paper presents an invisible spatial domain watermark insertion algorithm for which we show that the watermark can be recovered even if the attacker tries to manipulate the watermark with the knowledge of the watermarking process. The process incorporates buyer specific watermarks within a single multimedia object, and the same multimedia object has different watermarks that differ from owner to owner. Therefore recovery of this watermark not only authenticates the particular owner of the multimedia object but also could be used to identify the buyer involved in the forging process. This is achieved after spatially dividing the multimedia signal randomly into a set of disjoint subsets (referred to as the image key) and then manipulating the intensity of these subsets differently depending on a buyer specific key. These buyer specific keys are generated using a secret permutation of error correcting codes so that exact keys are not known even with the knowledge of the error correcting scheme. During recovery process a manipulated buyer key (due to attack) is extracted from the knowledge of, the image key. The recovered buyer key is matched with the exact buyer key in the database utilizing the principles of error correction. The survival of the watermark is demonstrated for a wide range of transformations and forging attempts on multimedia objects both in spatial and frequency domains. We have shown that quantitatively our watermarking survives rewatermarking attack using the knowledge of the watermarking process more efficiently compared to a spread spectrum based technique. The efficacy of the process increases in scenarios in which there exist fewer numbers of buyer keys for a specific multimedia object. We have also shown that a minor variation of the watermark insertion process can survive a "Stirmark" attack. By making the image key and the intensity manipulation process specific for a buyer and with proper selection of error correcting codes, certain categories of collusion attacks can also be precluded.
C1 Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, W Bengal, India.
   Indian Stat Inst, Comp & Stat Serv Ctr, Kolkata 700108, W Bengal, India.
   Univ Virginia, Dept Elect & Comp Engn, Charlottesville, VA 22094 USA.
C3 Indian Statistical Institute; Indian Statistical Institute Kolkata;
   Indian Statistical Institute; Indian Statistical Institute Kolkata;
   University of Virginia
RP Indian Stat Inst, Elect & Commun Sci Unit, Kolkata 700108, W Bengal, India.
EM dipti@isical.ac.in; subho@isical.ac.in; acton@vir-ginia.edu
CR [Anonymous], 1977, THEORY ERROR CORRE 1
   CACHIN C, 1998, 2 WORKSH INF HID POR
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   COX IJ, 1997, EL IM 97 FEB
   Craver S, 1998, COMMUN ACM, V41, P44, DOI 10.1145/278476.278486
   Ergun F, 1999, LECT NOTES COMPUT SC, V1592, P140
   GONZALEZ RC, 1988, DIGITAL IMAGE PROCES, P16
   Hernández JR, 2000, IEEE T IMAGE PROCESS, V9, P55, DOI 10.1109/83.817598
   HONSINGER C, DATA EMBEDDING USING
   JOHNSON NF, 2000, INFORMATION HIDING S
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Lu CS, 2001, IMAGE PROC SER, P507
   Meerwald P., 2001, THESIS U SALZBURG AU
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Ruanaidh JO, 1999, THEOR COMPUT SCI, V226, P117, DOI 10.1016/S0304-3975(99)00069-9
   Voloshynovskiy S, 2001, SIGNAL PROCESS, V81, P1177, DOI 10.1016/S0165-1684(01)00039-1
   Yeung MM, 1998, COMMUN ACM, V41, P30
NR 17
TC 91
Z9 99
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 1
EP 15
DI 10.1109/TMM.2003.819759
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200001
DA 2024-07-18
ER

PT J
AU Luo, HT
   Eleftheriadis, A
AF Luo, HT
   Eleftheriadis, A
TI Model-based segmentation and tracking of head-and-shoulder video objects
   for real time multimedia services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE low bit rate coding; MPEG-4; statistical modeling; video object
   segmentation
ID OCCLUSION; SCENE
AB A statistical model-based video segmentation algorithm is presented for head-and-shoulder type video. This algorithm uses domain knowledge by abstracting the head-and-shoulder object with a blob-based statistical region model and a shape model. The object segmentation problem is then converted into a model detection and tracking problem. At the system level, a hierarchical structure is designed and spatial and temporal filters are used to improve segmentation quality. This algorithm runs in real time over a QCIF size video, and segments it into background, head and shoulder three video objects on average Pentium PC platforms. Due to its real time feature, this algorithm is appropriate for real time multimedia services such as videophone and web chat. Simulation results are offered to compare MPEG-4 performance with H.263 on segmented video objects with respects to compression efficiency, bit rate adaptation and functionality.
C1 Hewlett Packard Labs, Palo Alto, CA 94304 USA.
   Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
C3 Hewlett-Packard; Columbia University
RP Luo, HT (corresponding author), Hewlett Packard Labs, Palo Alto, CA 94304 USA.
CR AACH T, 1993, SIGNAL PROCESS, V31, P165, DOI 10.1016/0165-1684(93)90063-G
   AIZAWA K, 1993, MOTION ANAL IMAGE SE
   Altunbasak Y, 1998, GRAPH MODEL IM PROC, V60, P13, DOI 10.1006/gmip.1997.0453
   BICHSEL M, 1994, IEEE T PATTERN ANAL, V16, P1138, DOI 10.1109/34.334396
   CHALOM E, 1996, ICIP LANS SEPT
   Chang MM, 1997, IEEE T IMAGE PROCESS, V6, P1326, DOI 10.1109/83.623196
   Choi JG, 1997, IEEE T CIRC SYST VID, V7, P279, DOI 10.1109/76.564107
   Correia P, 1998, SIGNAL PROCESS, V66, P125, DOI 10.1016/S0165-1684(98)00002-4
   Fu Y, 2000, IEEE T IMAGE PROCESS, V9, P2051, DOI 10.1109/83.887973
   Gu C, 1998, IEEE T CIRC SYST VID, V8, P572, DOI 10.1109/76.718504
   KIM C, 1999, ICIP KOB JAP OCT
   LETTERA C, 1991, SIGNAL PROCESS-IMAGE, V1, P181
   LUO H, 1999, ACM INT MULT C ORL F
   Malassiotis S, 1998, IEEE T CIRC SYST VID, V8, P756, DOI 10.1109/76.728419
   MARCOTEGUI B, 1999, ICIP KOB JAP OCT
   MECH R, 1996, JTCISC29WG11 ISOIEC
   PURI A, 1997, ACM MOBILE NETWO AUG
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Steinbach E, 1998, SIGNAL PROCESS, V66, P233, DOI 10.1016/S0165-1684(98)00008-5
   Toklu C, 2000, IEEE T CIRC SYST VID, V10, P624, DOI 10.1109/76.845008
   Tuncel E, 2000, IEEE T CIRC SYST VID, V10, P776, DOI 10.1109/76.856454
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zhong D, 1999, IEEE T CIRC SYST VID, V9, P1259, DOI 10.1109/76.809160
NR 25
TC 18
Z9 28
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 379
EP 389
DI 10.1109/TMM.2003.813285
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Al-Qatf, M
   Wang, XF
   Hawbani, A
   Abdussalam, A
   Alsamhi, SH
AF Al-Qatf, Majjed
   Wang, Xingfu
   Hawbani, Ammar
   Abdussalam, Amr
   Alsamhi, Saeed Hammod
TI Image Captioning With Novel Topics Guidance and Retrieval-Based Topics
   Re-Weighting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE End-to-end manner; enhanced topic predictor; image captioning;
   retrieval-based topic re-weighting; subsequent topic predictor; topic
   embedding
AB Topic modelling (TM) has shown significant progress in boosting the effectiveness of image captioning in the last few years. Although important improvements have been shown in previous topic-guided image captioning models, some challenges remain unsolved, such as the independence of the topic predictors and the sentence generators, resulting in ineffective exploitation of semantic information. Also, all the predicted topics or the top-one topic are used throughout the whole captioning task without considering the current time step's linguistic context, which deviates the captioning network to focus on inaccurate image objects. To tackle these challenges, we propose a novel image captioning method consisting of four modules: enhanced topic predictor (ETP), retrieval-based topics re-weighting module (RTR), subsequent topic predictor (STP), and caption generation module. The prediction and generation modules are trained in an end-to-end manner to promote the efficient use of topics by predicting suitable topics at each time step. ETP predicts the topics using the image features, and is enhanced with topic embedding (TE). The RTR is only applied in the testing stage for re-weighting the topics predicted by ETP. In each time step, the STP automatically predicts concise topics subsets to alleviate the diversity of the image topics. Compared with the existing topic-based models, our model can automatically generate more accurate and diverse captions, boosting the explainability of how the topics influence the generated word in each time step. Extensive experiments on the MS-COCO and Flickr30K benchmark datasets show that our method enhances the overall image captioning's performance and the topic prediction task, and outperforms many recent image captioning approaches in terms of the evaluation metrics.
C1 [Al-Qatf, Majjed; Wang, Xingfu; Hawbani, Ammar] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
   [Abdussalam, Amr] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
   [Alsamhi, Saeed Hammod] Natl Univ Ireland, Insight Ctr Data Analyt, Galway, Ireland.
   [Alsamhi, Saeed Hammod] IBB Univ, Ibb, Yemen.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Ollscoil na Gaillimhe-University of Galway
RP Wang, XF; Hawbani, A (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
EM malqatf@mail.ustc.edu.cn; wangxfu@ustc.edu.cn; anmande@ustc.edu.cn;
   amr2010@mail.ustc.edu.cn; saeed.alsamhi@insight-centre.org
RI Alsamhi, Saeed Hamood/ABE-6903-2022; Hawbani, Ammar/S-3356-2019
OI Alsamhi, Saeed Hamood/0000-0003-2857-6979; Hawbani,
   Ammar/0000-0002-1069-3993; Wang, Xingfu/0000-0002-1301-3535; ALQATF,
   MAJJED/0000-0002-1796-344X
FU National Administration of Traditional Chinese Medicine through
   Innovation Team and Talents Cultivation Program [ZYYCXTD-D-202208]
FX This work is supported by the National Administration of Traditional
   Chinese Medicine through Innovation Team and Talents Cultivation Program
   under Grant ZYYCXTD-D-202208.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Bird S., 2009, NATURAL LANGUAGE PRO
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Dash SK, 2020, ARAB J SCI ENG, V45, P3025, DOI 10.1007/s13369-019-04262-2
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Faghri F., 2018, P BRIT MACH VIS C SE, P1
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Huang YQ, 2020, IEEE T IMAGE PROCESS, V29, P4013, DOI 10.1109/TIP.2020.2969330
   Ji JZ, 2020, IEEE T IMAGE PROCESS, V29, P7615, DOI 10.1109/TIP.2020.3004729
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lindh A, 2018, LECT NOTES COMPUT SC, V11139, P176, DOI 10.1007/978-3-030-01418-6_18
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Maas A. L., 2018, P INT C MACH LEARN, V30, P1
   Mao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4258
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pedregosa F, 2011, J MACH LEARN RES, V12, P2825
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582
   Sun JM, 2022, INFORM FUSION, V77, P233, DOI 10.1016/j.inffus.2021.07.008
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tariq A, 2017, IEEE T IMAGE PROCESS, V26, P619, DOI 10.1109/TIP.2016.2628585
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vendrov I., 2016, ICLR, P1
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wang CZ, 2022, APPL INTELL, V52, P6575, DOI 10.1007/s10489-021-02734-3
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang X, 2018, IEEE-ACM T AUDIO SPE, V26, P2255, DOI 10.1109/TASLP.2018.2860287
   Wei HY, 2020, COMPUT VIS IMAGE UND, V201, DOI 10.1016/j.cviu.2020.103068
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu J, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3336495
   Wu LX, 2021, IEEE T CIRC SYST VID, V31, P3118, DOI 10.1109/TCSVT.2020.3036860
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yang L, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3386725
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   Yu NG, 2019, IEEE T IMAGE PROCESS, V28, P2743, DOI 10.1109/TIP.2018.2889922
   Yuan J, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3394955
   Zeng SN, 2022, IEEE T CYBERNETICS, V52, P4935, DOI 10.1109/TCYB.2020.3025757
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhou L, 2020, IEEE T IMAGE PROCESS, V29, P694, DOI 10.1109/TIP.2019.2928144
   Zia U, 2020, NEURAL COMPUT APPL, V32, P10471, DOI 10.1007/s00521-019-04587-x
NR 56
TC 4
Z9 4
U1 5
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5984
EP 5999
DI 10.1109/TMM.2022.3202690
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500025
DA 2024-07-18
ER

PT J
AU Cui, B
   Shao, Z
   Tao, W
   Zhao, H
AF Cui, Bin
   Shao, Zhuang
   Tao, Wei
   Zhao, Hui
TI Hole Inpainting Algorithm for Half-Organized Point Cloud Obtained by
   Structured-Light Section System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Three-dimensional displays; Bayes methods;
   Surface treatment; Optimization; Data models; Surface reconstruction;
   Point cloud inpainting; hole repairing; regional growth; bayesian
   estimation; half-organized point cloud; light-section system
AB The advances in sensors and data processing technologies enrich the types of 3D point clouds acquirement, empowering numerous extensive and novel applications such as 3D reconstruction in various scenarios. However, the hole defects affect the accuracy and fidelity of the acquired point clouds, hindering further development and application of 3D point clouds. Aiming at the hole defects in point clouds, a Bayesian hole inpainting algorithm for the half-organized point cloud is proposed, where the point cloud is obtained by a structured-light section system. The algorithm establishes a Bayesian probability model in the hole region, which adopts specific distributions of the point cloud to estimate the maximum likelihood parameter. Simulation and experimental results show that the proposed approach outperforms other competing algorithms significantly in repairing various types of holes, both in objective and subjective qualities. In addition, the proposed algorithm has better scalabilities in the cases of wrong topology definition and self-intersection confusion. This is the first algorithm specially designed for hole inpainting in half-organized point clouds, which maximizes the comprehensive consideration of local features and global optimization, supplemented by targeted prior knowledge of density, Riemannian manifold, and discrete attributes.
C1 [Cui, Bin; Shao, Zhuang; Tao, Wei; Zhao, Hui] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhao, H (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM cuibin728@sjtu.edu.cn; shaozhuang@sjtu.edu.cn; taowei@sjtu.edu.cn;
   huizhao@sjtu.edu.cn
RI Shao, Zhuang/HQZ-6926-2023; TAO, WEI/KQV-2575-2024
OI Shao, Zhuang/0000-0002-8055-2395; , cui/0000-0003-1966-7246
FU National Key Research and Development Program of China
FX No Statement Available
CR Attene M, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P271
   Berger M, 2017, COMPUT GRAPH FORUM, V36, P301, DOI 10.1111/cgf.12802
   Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619
   Chen H, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108431
   Chen H, 2022, MULTIMED TOOLS APPL, V81, P14641, DOI 10.1007/s11042-021-11019-3
   Cignoni P., 2008, P EUR IT CHAPT C, P129, DOI [DOI 10.2312/LOCALCHAPTEREVENTS/ITALCHAP/ITALIANCHAPCONF2008/129-136, 10.2312/LocalChapterEvents/ItalChap/ItalianChapConf2008/129-136]
   Cui B, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13214457
   Cui B, 2020, APPL OPTICS, V59, P8618, DOI 10.1364/AO.401350
   Dinesh C, 2018, IEEE SIGNAL PROC LET, V25, P878, DOI 10.1109/LSP.2018.2831621
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Geoffrey M., 2021, Front part of a sarcophagus
   He J, 2019, IEEE IMAGE PROC, P4385, DOI [10.1109/icip.2019.8803497, 10.1109/ICIP.2019.8803497]
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Huang Y, 2023, VISUAL COMPUT, V39, P723, DOI 10.1007/s00371-021-02370-5
   Li SM, 2021, ISPRS J PHOTOGRAMM, V171, P385, DOI 10.1016/j.isprsjprs.2020.11.021
   Li YB, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107478
   Liang XL, 2018, ISPRS J PHOTOGRAMM, V144, P137, DOI 10.1016/j.isprsjprs.2018.06.021
   Lin FZ, 2022, NEUROCOMPUTING, V507, P221, DOI 10.1016/j.neucom.2022.08.007
   Min XQ, 2022, REMOTE SENS-BASEL, V14, DOI 10.3390/rs14081900
   Park SY, 2003, PATTERN RECOGN LETT, V24, P2967, DOI 10.1016/S0167-8655(03)00157-0
   Phang JTS, 2021, MULTIMED TOOLS APPL, V80, P17879, DOI 10.1007/s11042-021-10605-9
   Qiu YJ, 2011, INT J ADV MANUF TECH, V53, P255, DOI 10.1007/s00170-010-2829-6
   Quinsat Y, 2015, INT J ADV MANUF TECH, V81, P411, DOI 10.1007/s00170-015-7185-0
   Ring W, 2012, SIAM J OPTIMIZ, V22, P596, DOI 10.1137/11082885X
   S. U. C. G. Laboratory, The stanford 3D scanning repository
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Wang Q, 2019, ADV ENG INFORM, V39, P306, DOI 10.1016/j.aei.2019.02.007
   Wang RS, 2018, IEEE J-STARS, V11, P606, DOI 10.1109/JSTARS.2017.2781132
   Wen ZK, 2022, MEASUREMENT, V190, DOI 10.1016/j.measurement.2021.110668
   Zhang JM, 2022, ENG ANAL BOUND ELEM, V139, P152, DOI 10.1016/j.enganabound.2022.03.020
NR 30
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8170
EP 8182
DI 10.1109/TMM.2022.3233254
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000045
DA 2024-07-18
ER

PT J
AU Duan, WH
   Liu, ZH
   Jia, CM
   Wang, SS
   Ma, SW
   Gao, W
AF Duan, Wenhong
   Liu, Zhenhua
   Jia, Chuanmin
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI Differential Weight Quantization for Multi-Model Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Computational modeling; Task analysis;
   Redundancy; Training; Correlation; Image coding; Neural network;
   multi-model compression; weights increment; differential quantization
ID NEURAL-NETWORK; DEEP
AB Low bit-width quantization can effectively reduce the storage and computational costs of deep neural networks. Existing quantization methods are commonly designed for single model compression. For multi-model compression scenarios, multiple models for the same task or similar tasks need to be compressed simultaneously in multimedia tasks, such as compressing image super-resolution models for different scales and transferring of different models in multimedia. However, single model quantization methods do not consider the correlations among the weights of different models, which limits the further compression for the above multi-model compression scenarios. To sufficiently excavate the potential of compression on multi-model, we propose a novel quantization scheme for multi-model compression, namely differential weight quantization (DWQ), which focuses on the weights increment between the target model and the reference model. Specifically, DWQ is achieved by increment computation, increment quantization and fine-tuning, which utilizes the reference model to guide the subsequent quantization on the target model. Due to the correlations between the weights of different models, the distribution of weights increment is more centralized compared with original weights, which can achieve a higher compression ratio by lower bit representation on weights increment. Moreover, the progressive training method is proposed to accelerate the convergence and reduce quantization loss on the DWQ framework. Extensive experiments validate the effectiveness of DWQ based on weight-sharing and parameterized clipping activation (PACT) quantization technologies on multiple tasks. The proposed framework can achieve 2x compression improvement and reduce 30% computational complexity with comparable performance in the popular multimedia tasks.
C1 [Duan, Wenhong; Gao, Wen] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
   [Duan, Wenhong; Liu, Zhenhua; Jia, Chuanmin; Wang, Shanshe; Gao, Wen] Peking Univ, Natl Engn Lab VideoTechnol, Beijing 100871, Peoples R China.
   [Wang, Shanshe] Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing, Peoples R China.
C3 Shanghai Jiao Tong University; Peking University; Peking University
RP Wang, SS (corresponding author), Peking Univ, Natl Engn Lab VideoTechnol, Beijing 100871, Peoples R China.
EM whduan@sjtu.edu.cn; liu-zh@pku.edu.cn; cmjia@pku.edu.cn;
   sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
OI Jia, Chuanmin/0000-0002-7418-6245
FU National Natural Science Foundation of China [62025101, 61931014,
   62072008, 62101007]; High-performance Computing Platform of Peking
   University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62025101, 61931014, 62072008, and
   62101007, and in part by the High-performance Computing Platform of
   Peking University. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Susanto Rahardja.
CR Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Anh-Huy Phan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P522, DOI 10.1007/978-3-030-58526-6_31
   Bai ZW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107538
   Bailin Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P639, DOI 10.1007/978-3-030-58536-5_38
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Chen ZY, 2021, AAAI CONF ARTIF INTE, V35, P4010
   Choi J, 2018, Arxiv, DOI [arXiv:1805.06085, DOI 10.48550/ARXIV.1805.06085, 10.48550/arXiv.1805.06085]
   Courbariaux Y., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.5555/2969442.2969588
   Denton E, 2014, ADV NEUR IN, V27
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Gong YC, 2014, Arxiv, DOI arXiv:1412.6115
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Han  S., 2015, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Jaderberg M., 2014, CORR
   Kaiyu Yue, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P312, DOI 10.1007/978-3-030-58555-6_19
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Krishnamoorthi R, 2018, Arxiv, DOI arXiv:1806.08342
   Lebedev V., 2015, ICLR, P1
   Li FF, 2016, Arxiv, DOI arXiv:1605.04711
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu LY, 2021, PR MACH LEARN RES, V139
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Ma JY, 2022, IEEE T MULTIMEDIA, V24, P3157, DOI 10.1109/TMM.2021.3094058
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Nagel M., 2020, INT C MACHINE LEARNI, P7197
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Tai C., 2016, P 4 INT C LEARN REPR
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Wang YH, 2016, ADV NEUR IN, V29, P253
   Wang ZZ, 2020, IEEE T MULTIMEDIA, V22, P2126, DOI 10.1109/TMM.2019.2950523
   Wang Z, 2021, PROC CVPR IEEE, P14908, DOI 10.1109/CVPR46437.2021.01467
   Xu XZ, 2022, IEEE T MULTIMEDIA, V24, P2752, DOI 10.1109/TMM.2021.3087098
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Xu YH, 2018, AAAI CONF ARTIF INTE, P4335
   Yao XX, 2021, IEEE T MULTIMEDIA, V23, P1426, DOI 10.1109/TMM.2020.2997126
   Yao ZW, 2021, PR MACH LEARN RES, V139
   Yaohui Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13166, DOI 10.1109/CVPR42600.2020.01318
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Ying Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P259, DOI 10.1007/978-3-030-58526-6_16
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang LB, 2021, IEEE T MULTIMEDIA, V23, P4158, DOI 10.1109/TMM.2020.3037502
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhou A., 2017, ARXIV170203044
   Zhou Shuchang, 2016, arXiv
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
   Zichao Guo, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P544, DOI 10.1007/978-3-030-58517-4_32
NR 55
TC 0
Z9 0
U1 8
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6397
EP 6410
DI 10.1109/TMM.2022.3208530
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500053
DA 2024-07-18
ER

PT J
AU Fang, NY
   Qiu, LM
   Zhang, SY
   Wang, ZL
   Hu, KR
   Dong, LY
AF Fang, Naiyu
   Qiu, Lemiao
   Zhang, Shuyou
   Wang, Zili
   Hu, Kerui
   Dong, Liangyu
TI A Novel Human Image Sequence Synthesis Method by Pose-Shape-Content
   Inference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing; Shape; Interpolation; Standards; Manifolds; Image sequences;
   Optical imaging; Human image sequence synthesis; multiscale
   feature-level optical flow; pose-shape-content inference; pose manifold;
   style code infusion
AB In online clothing sales, static model images only describe specific clothing statuses towards consumers. Without increasing shooting costs, it is a subject to display clothing dynamically by synthesizing a continuous image sequence between static images. This paper proposes a novel human image sequence synthesis method by pose-shape-content inference. In the condition of two reference poses, the pose is interpolated in the pose manifold controlled by a linear parameter. The interpolated pose is transferred into the end shape by AdaIN and the attention mechanism to infer target shape. Then the content in the reference image is transferred into this target shape. In the content transfer, the visual features of the human body cluster and clothing cluster are extracted, respectively. And the Sobel gradient is adopted to extract clothing texture variation. In the feature inferring, the multiscale feature-level optical flow warps source features, and style code infusion infers new region content without source features. Extensive experiments demonstrate that our method is superior in inferring clear layouts and transferring reasonable content compared to the pose transfer baselines. Moreover, our method has been verified to apply in parsing-guided image inference and dynamic display based on the pose sequence.
C1 [Fang, Naiyu; Qiu, Lemiao; Zhang, Shuyou; Wang, Zili; Hu, Kerui; Dong, Liangyu] Zhejiang Univ, State Key Lab Fluid Power Transmiss Control, Hangzhou 310027, Peoples R China.
C3 Zhejiang University
RP Qiu, LM (corresponding author), Zhejiang Univ, State Key Lab Fluid Power Transmiss Control, Hangzhou 310027, Peoples R China.
EM fangnaiyu@zju.edu.cn; qiulm@zju.edu.cn; zsy@zju.edu.cn;
   ziliwang@zju.edu.cn; hkr457@zju.edu.cn; dong_liangyu@zju.edu.cn
RI Qiu, Lemiao/JWO-6135-2024; Fang, Naiyu/KMA-5576-2024; Wang,
   Zili/KBP-9936-2024
OI lemiao, Qiu/0000-0001-9358-0099; Fang, Naiyu/0000-0003-0145-1690
FU National Key R&D Program of China [2018YFB1700700]
FX This work was supported by the National Key R&D Program of China under
   Grant 2018YFB1700700. The Associate Editor coordinating the review of
   this manuscript and approving it for publication was Dr. Wen-Huang
   Cheng.
CR Albahar B, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480559
   AlBahar B, 2019, IEEE I CONF COMP VIS, P9015, DOI 10.1109/ICCV.2019.00911
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13789, DOI 10.1109/ICCV48922.2021.01355
   Chopra A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5413, DOI 10.1109/ICCV48922.2021.00538
   Diakogiannis FI, 2020, ISPRS J PHOTOGRAMM, V162, P94, DOI 10.1016/j.isprsjprs.2020.01.013
   Dong H, 2019, IEEE I CONF COMP VIS, P9025, DOI 10.1109/ICCV.2019.00912
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ge YY, 2019, PROC CVPR IEEE, P5332, DOI 10.1109/CVPR.2019.00548
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7847, DOI 10.1109/CVPR42600.2020.00787
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   He HY, 2020, AAAI CONF ARTIF INTE, V34, P10949
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jaderberg M, 2015, ADV NEUR IN, V28
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Li K, 2020, IEEE T IMAGE PROCESS, V29, P9584, DOI 10.1109/TIP.2020.3029455
   Li YN, 2019, PROC CVPR IEEE, P3688, DOI 10.1109/CVPR.2019.00381
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JX, 2022, IEEE T MULTIMEDIA, V24, P4314, DOI 10.1109/TMM.2021.3115628
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Ma L, 2017, NEURIPS, P405
   Ma LY, 2023, IEEE T MULTIMEDIA, V25, P930, DOI 10.1109/TMM.2021.3134157
   Men YF, 2020, PROC CVPR IEEE, P5083, DOI 10.1109/CVPR42600.2020.00513
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Parmar G, 2022, Arxiv, DOI arXiv:2104.11222
   Raffiee AH, 2021, INT C PATT RECOG, P3923, DOI 10.1109/ICPR48806.2021.9412908
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2019, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2019.00246
   Tang H., 2020, P BRIT MACH VIS C, P1
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu J, 2021, IEEE T MULTIMEDIA, V23, P2222, DOI 10.1109/TMM.2021.3070972
   Yang F, 2021, PROC CVPR IEEE, P9894, DOI 10.1109/CVPR46437.2021.00977
   Yu RY, 2019, IEEE I CONF COMP VIS, P10510, DOI 10.1109/ICCV.2019.01061
   Yurui Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7687, DOI 10.1109/CVPR42600.2020.00771
   Zablotskaia P., 2019, P BRIT MACH VIS C, P1
   Zhang JS, 2021, PROC CVPR IEEE, P7978, DOI 10.1109/CVPR46437.2021.00789
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, IEEE T CIRC SYST VID, V29, P3037, DOI 10.1109/TCSVT.2018.2873599
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 55
TC 3
Z9 3
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6512
EP 6524
DI 10.1109/TMM.2022.3209924
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500062
DA 2024-07-18
ER

PT J
AU Gao, JX
   Chen, JJ
   Fu, HZ
   Jiang, YG
AF Gao, Jixiang
   Chen, Jingjing
   Fu, Huazhu
   Jiang, Yu-Gang
TI Dynamic Mixup for Multi-Label Long-Tailed Food Ingredient Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ingredient recognition; imbalanced multi-label classification;
   long-tailed problem
AB Recognizing the ingredients composition for given food images facilitates the estimation of nutrition facts, which is crucial to various health relevant applications. Nevertheless, ingredient recognition is a multi-label long-tailed classification problem, where each image may contain multiple labels and the class distributions are highly imbalanced. Most existing approaches leverage off-the-shelf Convolutional Neural Networks (CNN) for multi-label ingredient recognition, overlooking the long-tailed issue, which results in low accuracy for tail ingredient categories. To address this problem, this paper proposes a dynamic Mixup (D-Mixup) approach, aiming to dynamically augment minority ingredients, in order to boost the recognition performance for tail ingredient categories. Specifically, our D-Mixup approach dynamically selects two training images based on the predictions of the previous training epoch, and generates a new synthetic image to train the recognition network. In this way, the training samples of tailed classes can be dynamically enlarged and better discriminative representations can be learnt for rare classes. Extensive experiments on both VIREO Food-172 dataset and UEC Food-100 dataset demonstrate the effectiveness of the proposed D-Mixup method.
C1 [Gao, Jixiang; Chen, Jingjing; Jiang, Yu-Gang] Fudan Univ, Shanghai 200437, Peoples R China.
   [Fu, Huazhu] ASTAR, Inst High Performance Comp IHPC, Singapore, Singapore.
C3 Fudan University; Agency for Science Technology & Research (A*STAR);
   A*STAR - Institute of High Performance Computing (IHPC)
RP Chen, JJ (corresponding author), Fudan Univ, Shanghai 200437, Peoples R China.
EM jxgao19@fudan.edu.cn; chenjingjing@fudan.edu.cn; hzfu@ieee.org;
   ygj@fudan.edu.cn
RI WANG, YILUN/KFB-0627-2024; Fu, Huazhu/A-1411-2014; chen,
   huan/KEC-2019-2024
OI Fu, Huazhu/0000-0002-9702-5524; 
FU National Natural Science Foundation of China
FX No Statement Available
CR Aizawa K, 2015, IEEE MULTIMEDIA, V22, P4, DOI 10.1109/MMUL.2015.39
   [Anonymous], 2009, 2009 WORKSH APPL COM, DOI [DOI 10.1109/WACV.2009.5403087, 10.1109/wacv.2009.5403087]
   Beijbom O, 2015, IEEE WINT CONF APPL, P844, DOI 10.1109/WACV.2015.117
   Bettadapura V, 2015, IEEE WINT CONF APPL, P580, DOI 10.1109/WACV.2015.83
   Bolaños M, 2017, LECT NOTES COMPUT SC, V10590, P394, DOI 10.1007/978-3-319-70742-6_37
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2018, MULTIMED TOOLS APPL, V77, P29457, DOI 10.1007/s11042-018-5964-y
   Chen JJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1771, DOI 10.1145/3123266.3123428
   Chen JJ, 2017, LECT NOTES COMPUT SC, V10132, P588, DOI 10.1007/978-3-319-51811-4_48
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen M. -Y., 2012, P SIGGRAPH 2012 TECH, P1
   Dong Q, 2017, IEEE I CONF COMP VIS, P1869, DOI 10.1109/ICCV.2017.205
   He HB, 2009, IEEE T KNOWL DATA EN, V21, P1263, DOI 10.1109/TKDE.2008.239
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hoashi H., 2010, Proceedings 2010 IEEE International Symposium on Multimedia (ISM 2010), P296, DOI 10.1109/ISM.2010.51
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang S., 2020, arXiv
   Inoue H, 2018, Arxiv, DOI arXiv:1801.02929
   Kawano Y, 2014, PROCEEDINGS OF THE 2014 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP'14 ADJUNCT), P589, DOI 10.1145/2638728.2641339
   Kawano Y, 2013, IEEE COMPUT SOC CONF, P1, DOI 10.1109/CVPRW.2013.5
   Kingma D. P., 2014, arXiv
   Li BL, 2022, PROC CVPR IEEE, P6960, DOI 10.1109/CVPR52688.2022.00684
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Mangla P, 2020, IEEE WINT CONF APPL, P2207, DOI [10.1109/wacv45572.2020.9093338, 10.1109/WACV45572.2020.9093338]
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Matsuda Y, 2012, INT C PATT RECOG, P2017
   Min WQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1331, DOI 10.1145/3343031.3350948
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Ming ZY, 2018, LECT NOTES COMPUT SC, V10705, P129, DOI 10.1007/978-3-319-73600-6_12
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan JR, 2020, Arxiv, DOI arXiv:2003.05176
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang YN, 2019, CEA'19: PROCEEDINGS OF THE 11TH WORKSHOP ON MULTIMEDIA FOR COOKING AND EATING ACTIVITIES, P1, DOI 10.1145/3326458.3326929
   Wang YX, 2017, 31 ANN C NEURAL INFO, V30
   Zhang HY, 2018, Arxiv, DOI [arXiv:1710.09412, DOI 10.48550/ARXIV.1710.09412]
   Zhang XJ, 2016, J COMPUT SCI TECH-CH, V31, P489, DOI 10.1007/s11390-016-1642-6
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhou BY, 2020, Arxiv, DOI arXiv:1912.02413
   Zhou F, 2016, PROC CVPR IEEE, P1124, DOI 10.1109/CVPR.2016.127
   Zhou ZH, 2010, COMPUT INTELL-US, V26, P232, DOI 10.1111/j.1467-8640.2010.00358.x
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
   Zhu XX, 2014, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2014.122
NR 50
TC 10
Z9 10
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4764
EP 4773
DI 10.1109/TMM.2022.3181789
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700004
DA 2024-07-18
ER

PT J
AU Li, WY
   Liu, XY
   Yuan, YX
AF Li, Wuyang
   Liu, Xinyu
   Yuan, Yixuan
TI SCAN plus plus : Enhanced Semantic Conditioned Adaptation for Domain
   Adaptive Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional kernel; domain adaptive object detection; optimal transport;
   unbiased semantics
AB Domain Adaptive Object Detection (DAOD) transfers an object detector from the labeled source domain to a novel unlabelled target domain. Recent advances bridge the domain gap by aligning category-agnostic feature distribution and minimizing the domain discrepancy for adapting semantic distribution. Though great success, these methods model domain discrepancy with prototypes within a batch, yielding a biased estimation of domain-level statistics. Moreover, the category-agnostic alignment leads to the disagreement of the cross-domain semantic distribution with inevitable classification errors. To address these two issues, we propose an enhanced Semantic Conditioned AdaptatioN (SCAN++) framework, which leverages unbiased semantics for DAOD. Specifically, in the source domain, we design the conditional kernel to sample Pixel of Interests (PoIs), and aggregate PoIs with a cross-image graph to estimate an unbiased semantic sequence. Conditioned on the semantic sequence, we further update the parameter of the conditional kernel in a semantic conditioned manifestation module, and establish a novel conditional graph in the target domain to model unlabeled semantics. After modeling the semantic distribution in both domains, we integrate the conditional kernel into adversarial alignment to achieve semantic-aware adaptation in a Conditional Kernel guided Alignment (CKA) module. Meanwhile, the Semantic Sequence guided Transport (SST) module is proposed to transfer reliable semantic knowledge to the target domain through solving the cross-domain Optimal Transport (OT) assignment, achieving unbiased adaptation at the semantic level. Comprehensive experiments on four adaptation scenarios demonstrate that SCAN++ achieves state-of-the-art results. The code is available at https://github.com/CityU-AIM-Group/SCAN/tree/SCAN++.
C1 [Li, Wuyang] City Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Liu, Xinyu; Yuan, Yixuan] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Chinese University of Hong Kong
RP Yuan, YX (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
EM wuyangli2-c@my.cityu.edu.hk; xliu423-c@my.cityu.edu.hk;
   yxyuan@ee.cuhk.edu.hk
RI Yuan, Yixuan/KSL-8440-2024
OI Yuan, Yixuan/0000-0002-0853-6948; Li, Wuyang/0000-0002-7338-9251; Liu,
   Xinyu/0000-0002-5180-6958
FU Hong Kong Research Grants Council (RGC) General Research Fund [11211221,
   CityU 9043152]; Hong Kong Innovation and Technology Commission (InnoHK
   Project CIMDA)
FX This work was supported in part by the Hong Kong Research Grants Council
   (RGC) General Research Fund 11211221 under Grant CityU 9043152, and in
   part by the Hong Kong Innovation and Technology Commission (InnoHK
   Project CIMDA).
CR Bernal J, 2015, COMPUT MED IMAG GRAP, V43, P99, DOI 10.1016/j.compmedimag.2015.02.007
   Chen YH, 2021, INT J COMPUT VISION, V129, P2223, DOI 10.1007/s11263-021-01447-x
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng BW, 2022, Arxiv, DOI arXiv:2104.06404
   Cheng-Chun Hsu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P733, DOI 10.1007/978-3-030-58545-7_42
   Congcong Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P481, DOI 10.1007/978-3-030-58601-0_29
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   De Brabandere B, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JH, 2021, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR46437.2021.00408
   Fu KX, 2023, IEEE T PATTERN ANAL, V45, P6183, DOI [10.1109/TPAMI.2022.3204713, 10.1109/CVPR46437.2021.00878]
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092
   Lei Ba J., 2016, arXiv
   Li WY, 2022, AAAI CONF ARTIF INTE, P1421
   Li WY, 2022, PROC CVPR IEEE, P5281, DOI 10.1109/CVPR52688.2022.00522
   Li WY, 2021, IEEE T IMAGE PROCESS, V30, P9456, DOI 10.1109/TIP.2021.3126423
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu XY, 2022, IEEE T MED IMAGING, V41, P1897, DOI 10.1109/TMI.2022.3150435
   Liu XY, 2021, MED IMAGE ANAL, V71, DOI 10.1016/j.media.2021.102052
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Munir M. A., 2021, ADV NEURAL INFORM PR, P22770
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezaeianaran F, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9184, DOI 10.1109/ICCV48922.2021.00907
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Schubert E, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/3068335
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tian K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9113, DOI 10.1109/ICCV48922.2021.00900
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Hoang TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2578, DOI 10.1145/3343031.3356073
   VS Vibashan, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P4514, DOI 10.1109/CVPR46437.2021.00449
   Wang Y, 2021, PROC CVPR IEEE, P9598, DOI 10.1109/CVPR46437.2021.00948
   Wu AM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9322, DOI 10.1109/ICCV48922.2021.00921
   Yang B, 2019, ADV NEUR IN, V32
   Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079
   Yao ZL, 2021, PROC CVPR IEEE, P12326, DOI 10.1109/CVPR46437.2021.01215
   Yuan J, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2019.107131
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhang YX, 2021, PROC CVPR IEEE, P12420, DOI 10.1109/CVPR46437.2021.01224
   Zhao L, 2022, PROC CVPR IEEE, P14197, DOI 10.1109/CVPR52688.2022.01382
   Zheng Y., 2020, P IEEE CVF C COMP VI, P13766
   Zhou XY, 2019, Arxiv, DOI [arXiv:1904.07850, 10.48550/arXiv.1904.07850]
NR 51
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7051
EP 7061
DI 10.1109/TMM.2022.3217388
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000025
DA 2024-07-18
ER

PT J
AU Lin, JX
   Yin, LY
   Wang, YJ
AF Lin, Jianxin
   Yin, Lianying
   Wang, Yijun
TI Steformer: Efficient Stereo Image Super-Resolution With Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superresolution; Transformers; Image reconstruction; Feature extraction;
   Task analysis; Convolution; Computer architecture; Stereo image
   processing; image super-resolution; transformer
ID PARALLAX ATTENTION; NETWORK
AB With the rapid development of stereoscopic vision applications, stereo image processing techniques have attracted increasing attention in both academic and industrial communities. In this paper, we study the fundamental stereo image super-resolution (SR) problem, which aims to recover high-resolution stereo images from low-resolution (LR) stereo images. Since disparities between stereo images vary significantly, convolutional network-based stereo image SR methods show a limitation in capturing long-range dependencies. To address this problem, this paper proposes to leverage the capability of self-attention in Transformers to efficiently capture reliable stereo correspondence and incorporate cross-view information for stereo image SR. Our model, named Steformer, consists of three parts: cross attentive feature extraction, cross-to-intra information integration and high-quality image reconstruction. In particular, the cross attentive feature extraction module employs residual cross Steformer blocks (RCSB) for long-range cross-view information extraction. Then, the cross-to-intra information integration module exploits cross-view and intra-view information using cross-to-intra attention mechanism (C2IAM). Finally, residual Steformer blocks (RSB) are designed for feature pre-processing in high-quality image reconstruction. Extensive experiments show that Steformer achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations, while the total number of parameters can be reduced by up to 40.71%.
C1 [Lin, Jianxin; Yin, Lianying; Wang, Yijun] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
C3 Hunan University
RP Wang, YJ (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM linjianxin@hnu.edu.cn; yin2110@hnu.edu.cn; wyjun@hnu.edu.cn
RI Wang, Yijun/GXW-1763-2022
OI wang, yijun/0000-0002-3372-8167; Lin, Jianxin/0000-0003-0359-8821
FU National Natural Science Foundation of China
FX No Statement Available
CR Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Brown T., 2020, P ADV NEUR INF PROC, P1877
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen CQ, 2022, IEEE T MULTIMEDIA, V24, P202, DOI 10.1109/TMM.2021.3050092
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chu XJ, 2022, IEEE COMPUT SOC CONF, P1238, DOI 10.1109/CVPRW56347.2022.00130
   Dai Q., P 29 ACM INT C MULT, P1985
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Guo XY, 2019, PROC CVPR IEEE, P3268, DOI 10.1109/CVPR.2019.00339
   Isaac JS, 2015, 2015 INT C TECHN SUS, P1, DOI [10.1109/ictsd.2015.7095900, DOI 10.1109/ICTSD.2015.7095900, 10.1109/ICTSD.2015.7095900]
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim JH, 2020, Arxiv, DOI arXiv:1811.12043
   Li H, 2023, IEEE T MULTIMEDIA, V25, P4752, DOI 10.1109/TMM.2022.3181457
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I, 2019, Arxiv, DOI arXiv:1711.05101
   Ma CX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2420, DOI 10.1145/3474085.3475408
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Paszke A, 2019, ADV NEUR IN, V32
   Radford A., 2019, LANGUAGE MODELS ARE
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Raffel C, 2020, J MACH LEARN RES, V21
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Shermeyer J, 2019, IEEE COMPUT SOC CONF, P1432, DOI 10.1109/CVPRW.2019.00184
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song W, 2020, AAAI CONF ARTIF INTE, V34, P12031
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Tzovaras D, 1998, SIGNAL PROCESS-IMAGE, V11, P205, DOI 10.1016/S0923-5965(97)00029-5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang LG, 2022, IEEE COMPUT SOC CONF, P905, DOI 10.1109/CVPRW56347.2022.00105
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Wang YQ, 2021, IEEE COMPUT SOC CONF, P766, DOI 10.1109/CVPRW53098.2021.00086
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Xie EZ, 2021, ADV NEUR IN, V34
   Xu QY, 2021, IEEE SIGNAL PROC LET, V28, P613, DOI 10.1109/LSP.2021.3066125
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu XY, 2022, IEEE T MULTIMEDIA, V24, P3074, DOI 10.1109/TMM.2021.3092571
   Zou WB, 2023, IEEE T MULTIMEDIA, V25, P4623, DOI 10.1109/TMM.2022.3179926
NR 53
TC 7
Z9 7
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8396
EP 8407
DI 10.1109/TMM.2023.3236845
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000032
DA 2024-07-18
ER

PT J
AU Liu, S
   Quan, WZ
   Wang, CQ
   Liu, Y
   Liu, B
   Yan, DM
AF Liu, Shuo
   Quan, Weize
   Wang, Chaoqun
   Liu, Yuan
   Liu, Bin
   Yan, Dong-Ming
TI Dense Modality Interaction Network for Audio-Visual Event Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention; audio-visual event localization; dense modality interaction;
   Multi-modality
ID SOUND
AB Human perception systems can integrate audio and visual information automatically to obtain a profound understanding of real-world events. Accordingly, fusing audio and visual contents is important to solve the audio-visual event (AVE) localization problem. Although most existing works have fused audio and visual modalities to explore their relationship with attention-based networks, we can delve into their relationship more deeply to improve the fusion capability of the two modalities. In this paper, we propose a dense modality interaction network (DMIN) to elegantly leverage audio and visual information by integrating two novel modules, namely, the audio-guided triplet attention (AGTA) module and the dense inter-modality attention (DIMA) module. The AGTA module enables audio information to guide the network to pay more attention to event-relevant visual regions. This guidance is conducted in the channel, temporal, and spatial dimensions, which emphasize informative features, temporal relationships and spatial regions, to boost the capacity of representations. Furthermore, the DIMA module establishes the dense-relationship between audio and visual modalities. Specifically, the DIMA module leverages the information of all channel pairs of audio and visual features to formulate the cross-modality attention weight, which is superior to the multi-head attention module that uses limited information. Moreover, a novel unimodal discrimination loss (UDL) is introduced to exploit the unimodal and fused features together for more exact AVE localization. The experimental results show that our method is remarkably superior to the state-of-the-art methods in fully- and weakly-supervised AVE settings. To further evaluate the model's ability to build audio-visual connections, we design a dense cross modality relation network (DCMR) to solve the cross-modality localization task. DCMR is a simple deformation of a DMIN, and the experimental results further illustrate that DIMA can explore denser relationships between the two modalities. Code is available at https://github.com/weizequan/DMIN.git.
C1 [Liu, Shuo; Quan, Weize; Wang, Chaoqun; Liu, Bin; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Liu, Shuo; Quan, Weize; Wang, Chaoqun; Liu, Yuan; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Liu, Yuan] Alibaba Grp, Speech Lab, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Alibaba Group
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
EM liushuo2019@ia.ac.cn; qweizework@gmail.com; wangchaoqun2018@ia.ac.cn;
   liuyuanthelma@gmail.com; liubin@nlpr.ia.ac.cn; yandongming@gmail.com
OI Yan, Dong-Ming/0000-0003-2209-2404; liu, bin/0000-0003-1529-1552
FU National Natural Science Foundation of China [62102418, 62172415];
   National Key Ramp;D Program of China [2019YFB2204104]; Alibaba Group
   through Alibaba Innovative Research Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102418 and 62172415, in part by the
   National Key R & D Program of China under Grant 2019YFB2204104, and in
   part by Alibaba Group through Alibaba Innovative Research Program.&
   nbsp;
CR Anastasopoulos A, 2019, Arxiv, DOI arXiv:1903.02930
   Andrew G., 2013, ICML, P1247
   [Anonymous], 2018, IEEE Transactions on Emerging Topics in Computational Intelligence
   Bulkin DA, 2006, CURR OPIN NEUROBIOL, V16, P415, DOI 10.1016/j.conb.2006.06.008
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chuang Gan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P758, DOI 10.1007/978-3-030-58621-8_44
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Fayek HM, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P558
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gabbay A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3051, DOI 10.1109/ICASSP.2018.8462527
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghose S, 2021, IEEE T MULTIMEDIA, V23, P1895, DOI 10.1109/TMM.2020.3005033
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hsieh TI, 2019, ADV NEUR IN, V32
   Hu D, 2016, PROC CVPR IEEE, P3574, DOI 10.1109/CVPR.2016.389
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Kim J.-H., 2017, PROC INT C LEARN REP, P247
   Kingma D. P., 2014, arXiv
   Kuo IY, 2021, AAAI CONF ARTIF INTE, V35, P8217
   Lee JS, 2008, IEEE T MULTIMEDIA, V10, P767, DOI 10.1109/TMM.2008.922789
   Li Bochen, 2019, INT SOC MUSIC INFORM, P604
   Lin Y., 2019, P 2019 3 INT C ENERG
   Lin Yan-Bo, 2020, P AS C COMP VIS
   Lu R, 2018, IEEE SIGNAL PROC LET, V25, P1315, DOI 10.1109/LSP.2018.2853566
   Maron O, 1998, ADV NEUR IN, V10, P570
   Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Pérez-Rúa JM, 2019, PROC CVPR IEEE, P6959, DOI 10.1109/CVPR.2019.00713
   Petridis S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6548, DOI 10.1109/ICASSP.2018.8461326
   Pramanik S, 2020, Arxiv, DOI arXiv:1907.07804
   Ramaswamy J, 2020, INT CONF ACOUST SPEE, P4372, DOI [10.1109/ICASSP40776.2020.9053895, 10.1109/icassp40776.2020.9053895]
   Ramaswamy J, 2020, IEEE WINT CONF APPL, P2959, DOI 10.1109/WACV45572.2020.9093616
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith L, 2005, ARTIF LIFE, V11, P13, DOI 10.1162/1064546053278973
   Surís D, 2019, LECT NOTES COMPUT SC, V11132, P711, DOI 10.1007/978-3-030-11018-5_62
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Vaswani A, 2017, ADV NEUR IN, V30
   Vielzeuf V, 2019, LECT NOTES COMPUT SC, V11134, P575, DOI 10.1007/978-3-030-11024-6_44
   Wang D., 2020, P IEEECVF C COMPUTER, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang HH, 2017, IEEE INT CON MULTI, P949, DOI 10.1109/ICME.2017.8019301
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Wu Y, 2019, IEEE I CONF COMP VIS, P6301, DOI 10.1109/ICCV.2019.00639
   Xu HM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3893, DOI 10.1145/3394171.3413581
   Xuan HY, 2020, AAAI CONF ARTIF INTE, V34, P279
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Yue KY, 2018, ADV NEUR IN, V31
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
   Zhao H, 2019, IEEE I CONF COMP VIS, P1735, DOI 10.1109/ICCV.2019.00182
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zhou BL, 2015, Arxiv, DOI arXiv:1512.02167
   Zhou JX, 2021, PROC CVPR IEEE, P8432, DOI 10.1109/CVPR46437.2021.00833
   Zhou P, 2019, INT CONF ACOUST SPEE, P6565, DOI 10.1109/ICASSP.2019.8683733
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
   Zhu H, 2021, INT J AUTOM COMPUT, V18, P351, DOI 10.1007/s11633-021-1293-0
NR 62
TC 2
Z9 2
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2734
EP 2748
DI 10.1109/TMM.2022.3150469
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600022
DA 2024-07-18
ER

PT J
AU Lo, FPW
   Guo, Y
   Sun, YN
   Qiu, JN
   Lo, B
AF Lo, Frank Po Wen
   Guo, Yao
   Sun, Yingnan
   Qiu, Jianing
   Lo, Benny
TI An Intelligent Vision-Based Nutritional Assessment Method for Handheld
   Food Items
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Volume estimation; 3D point cloud; weak supervision; domain adaptation;
   auxiliary classifier GAN
ID VOLUME ESTIMATION; RECONSTRUCTION
AB Dietary assessment has proven to be effective to evaluate the dietary intake of patients with diabetes and obesity. The traditional approach of accessing the dietary intake is to conduct a 24-hour dietary recall, a structured interview designed to obtain information on food categories and volume consumed by the participants. Due to unconscious biases in this kind of self-reporting approaches, many research studies have explored the use of vision-based approaches to provide accurate and objective assessments. Despite the promising results of food recognition by deep neural networks, there still exist several hurdles in deep learning-based food volume estimation ranging from domain shift between synthetic and raw 3D models, shape completion ambiguity and lack of large-scale paired training dataset. Therefore, this paper proposed an intelligent nutritional assessment approach via weakly-supervised point cloud completion, which aims to close the reality gap in 3D point cloud completion tasks and address the targeted challenges. Then the volume can be easily estimated from the completed representation of the food. Another major merit of our system is that it can be used to estimate the volume of handheld food items without requiring the constraints including placing the food items on a table or next to fiducial markers, which facilitates the implementation on both wearable and handheld cameras. Comprehensive experiments have been carried out on major benchmark datasets and self-constructed volume-annotated dataset respectively, in which the proposed method demonstrates comparable results with several strong fully-supervised baseline methods and shows superior completion ability in handling food volume estimation.
C1 [Lo, Frank Po Wen; Sun, Yingnan; Lo, Benny] Imperial Coll London, Dept Surg & Canc, London SW7 2BX, England.
   [Guo, Yao] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.
   [Qiu, Jianing] Imperial Coll London, Dept Comp, London SW7 2BX, England.
C3 Imperial College London; Shanghai Jiao Tong University; Imperial College
   London
RP Guo, Y (corresponding author), Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.
EM po.lo15@imperial.ac.uk; yao.guo@sjtu.edu.cn; y.sun16@imperial.ac.uk;
   jq916@ic.ac.uk; benny.lo@imperial.ac.uk
RI Sun, Yingnan/V-3195-2017; Qiu, Jianing/ISV-1514-2023; Lo,
   Benny/F-2155-2015
OI Lo, Frank Po Wen/0000-0002-0358-6567; Lo, Benny/0000-0002-5080-108X;
   Qiu, Jianing/0000-0003-4166-3428
FU Innovative Passive Dietary Monitoring Project - Bill and Melinda Gates
   Foundation [INV-006713]; Science and Technology Commission of Shanghai
   Municipality [20DZ2220400]; Shanghai Pilot Program for Basic
   Research-Shanghai Jiao Tong University [21TQ1400203]
FX This work was supported in part by Innovative Passive Dietary Monitoring
   Project funded by the Bill and Melinda Gates Foundation under
   Opportunity INV-006713, in part by the Science and Technology Commission
   of Shanghai Municipality under Grant 20DZ2220400, and in part by the
   Shanghai Pilot Program for Basic Research-Shanghai Jiao Tong University
   under Grant 21TQ1400203.
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   Agrawal S, 2019, IEEE WINT CONF APPL, P1099, DOI 10.1109/WACV.2019.00122
   Calli B, 2015, IEEE ROBOT AUTOM MAG, V22, P36, DOI 10.1109/MRA.2015.2448951
   Chen Xinyue, 2020, INT C LEARN REPR
   Dai A, 2018, PROC CVPR IEEE, P4578, DOI 10.1109/CVPR.2018.00481
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Edelsbrunner H., 1995, Foundations of Software Technology and Theoretical Computer Science. 15th Conference. Proceedings, P391
   Gadelha M, 2018, LECT NOTES COMPUT SC, V11211, P105, DOI 10.1007/978-3-030-01234-2_7
   Gao AQ, 2018, INT CONF WEARAB IMPL, P110, DOI 10.1109/BSN.2018.8329671
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li RH, 2021, PROC CVPR IEEE, P344, DOI 10.1109/CVPR46437.2021.00041
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Lo FPW, 2020, IEEE T IND INFORM, V16, P577, DOI 10.1109/TII.2019.2942831
   Lo FPW, 2019, INT CONF WEARAB IMPL, DOI 10.1109/bsn.2019.8771089
   Lo FPW, 2020, IEEE J BIOMED HEALTH, V24, P1926, DOI 10.1109/JBHI.2020.2987943
   Lu Y, 2021, INT C PATT RECOG, P8156, DOI 10.1109/ICPR48806.2021.9412339
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P1136, DOI 10.1109/TMM.2020.2993948
   Mandikal P., 2018, P BRIT MACH VIS C
   Do HM, 2021, IEEE T AUTOM SCI ENG, V18, P1229, DOI 10.1109/TASE.2020.2999203
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Odena A, 2017, PR MACH LEARN RES, V70
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Saleh K, 2019, IEEE INT CONF COMP V, P3235, DOI 10.1109/ICCVW.2019.00404
   Sarmad M, 2019, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR.2019.00605
   Song SR, 2017, PROC CVPR IEEE, P190, DOI 10.1109/CVPR.2017.28
   Stutz D, 2018, PROC CVPR IEEE, P1955, DOI 10.1109/CVPR.2018.00209
   Sui CY, 2020, IEEE T AUTOM SCI ENG, V17, P2130, DOI 10.1109/TASE.2020.2991803
   Sun MG, 2015, J HEALTHC ENG, V6, P1, DOI 10.1260/2040-2295.6.1.1
   Sun QX, 2021, IEEE T AUTOM SCI ENG, V18, P2061, DOI 10.1109/TASE.2020.3032831
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang F, 2019, IEEE INT CONF ROBOT, P3296, DOI [10.1109/icra.2019.8794467, 10.1109/ICRA.2019.8794467]
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Xu C, 2013, IEEE IMAGE PROC, P2534, DOI 10.1109/ICIP.2013.6738522
   Yang B, 2019, IEEE T PATTERN ANAL, V41, P2820, DOI 10.1109/TPAMI.2018.2868195
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu XM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12478, DOI 10.1109/ICCV48922.2021.01227
   Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088
   Zhang XM, 2018, ADV NEUR IN, V31
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zitian Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7659, DOI 10.1109/CVPR42600.2020.00768
NR 47
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5840
EP 5851
DI 10.1109/TMM.2022.3199911
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500014
DA 2024-07-18
ER

PT J
AU Pang, WF
   Xie, W
   He, QH
   Li, YX
   Yang, JC
AF Pang, Wenfeng
   Xie, Wei
   He, Qianhua
   Li, Yanxiong
   Yang, Jichen
TI Audiovisual Dependency Attention for Violence Detection in Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audiovisual dependency attention; dependency map; violence detection
ID SCENES; MOVIES; FUSION
AB Violence detection in videos can help maintain public order, detect crimes, or provide timely assistance. In this paper, we aim to leverage multimodal information to determine whether successive frames contain violence. Specifically, we propose an audiovisual dependency attention (AVD-attention) module modified from the co-attention architecture to fuse visual and audio information, unlike commonly used methods such as the feature concatenation, addition, and score fusion. Because the AVD-attention module's dependency map contains sufficient fusion information, we argue that it should be applied more sufficiently. A combination pooling method is utilized to convert the dependency map to an attention vector, which can be considered a new feature that includes fusion information or a mask of the attention feature map. Since some information in the input feature might be lost after processing by attention modules, we employ a multimodal low-rank bilinear method that considers all pairwise interactions among two features in each time step to complement the original information for output features of the module. AVD-attention outperformed co-attention in experiments on the XD-Violence dataset. Our system outperforms state-of-the-art systems.
C1 [Pang, Wenfeng; Xie, Wei; He, Qianhua; Li, Yanxiong] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
   [Yang, Jichen] Guangdong Polytech Normal Univ, Sch Cyberspace Secur, Speech Informat Secur Lab, Guangzhou 510640, Peoples R China.
C3 South China University of Technology; Guangdong Polytechnic Normal
   University
RP He, QH (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.; Yang, JC (corresponding author), Guangdong Polytech Normal Univ, Sch Cyberspace Secur, Speech Informat Secur Lab, Guangzhou 510640, Peoples R China.
EM wenfengpang@gmail.com; chester.w.xie@gmail.com; eeqhhe@scut.edu.cn;
   eeyxli@scut.edu.cn; nisonyoung@163.com
RI zhao, sheng/JWO-6127-2024; Liu, Jingyi/JWP-6326-2024; Han,
   Liang/KFR-6745-2024; liu, sha/JXL-6600-2024
OI Xie, Wei/0000-0003-1770-8710
FU National Natural Science Foundation of China [62111530145, 61771200,
   61571192, 61301300]; Natural Science Foundation of Guangdong Province
   [2022A1515011687]; Guangdong Basic and Applied Basic Research
   Foundation, China [2021A1515011454]; Project of the International
   Science and Technology Collaboration of Guangdong Province, China
   [2021A0505030003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62111530145, 61771200, 61571192, and
   61301300, in part by the Natural Science Foundation of Guangdong
   Province under Grant 2022A1515011687, in part by Guangdong Basic and
   Applied Basic Research Foundation, China, under Grant 2021A1515011454,
   and in part by the Project of the International Science and Technology
   Collaboration of Guangdong Province, China, under Grant 2021A0505030003.
CR Aytar Y, 2016, ADV NEUR IN, V29
   Carbonneau MA, 2018, PATTERN RECOGN, V77, P329, DOI 10.1016/j.patcog.2017.10.009
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Constantin MG, 2022, IEEE T AFFECT COMPUT, V13, P347, DOI 10.1109/TAFFC.2020.2986969
   Datta A, 2002, INT C PATT RECOG, P433, DOI 10.1109/ICPR.2002.1044748
   Demarty CH, 2015, MULTIMED TOOLS APPL, V74, P7379, DOI 10.1007/s11042-014-1984-4
   Dong ZH, 2016, COMM COM INF SC, V662, P517, DOI 10.1007/978-981-10-3002-4_43
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan B, 2021, IEEE WINT CONF APPL, P4012, DOI 10.1109/WACV48630.2021.00406
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Gan C, 2019, IEEE I CONF COMP VIS, P7052, DOI 10.1109/ICCV.2019.00715
   Gan Chuang, 2020, P IEEECVF C COMPUTER, P10478, DOI DOI 10.1109/CVPR42600.2020.01049
   Gao Y, 2016, IMAGE VISION COMPUT, V48-49, P37, DOI 10.1016/j.imavis.2016.01.006
   Giannakopoulos T, 2010, LECT NOTES ARTIF INT, V6040, P91, DOI 10.1007/978-3-642-12842-4_13
   Hanson A, 2019, LECT NOTES COMPUT SC, V11130, P280, DOI 10.1007/978-3-030-11012-3_24
   Hasan M, 2016, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2016.86
   Hassner T., 2012, P IEEE C COMP VIS PA, P1
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Kazakos E, 2019, IEEE I CONF COMP VIS, P5491, DOI 10.1109/ICCV.2019.00559
   Kim J.-H., 2016, arXiv
   Kingma P. D., 2015, arXiv
   Kipf N., 2016, arXiv
   Kooij JFP, 2016, COMPUT VIS IMAGE UND, V144, P106, DOI 10.1016/j.cviu.2015.06.009
   Lin JA, 2009, LECT NOTES COMPUT SC, V5879, P930
   Lin Y.-B., 2020, PROC ASIAN C COMPUT, P274
   Long X, 2018, AAAI CONF ARTIF INTE, P7202
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Lu JS, 2016, ADV NEUR IN, V29
   Molchanov P., 2017, INT C LEARN REPR ICL, P1, DOI DOI 10.1002/9781118786352.WBIEG1156
   Oualla M, 2014, INT CONF MULTIMED, P1101, DOI 10.1109/ICMCS.2014.6911186
   Owens A, 2018, LECT NOTES COMPUT SC, V11210, P639, DOI 10.1007/978-3-030-01231-1_39
   Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Perez M, 2019, INT CONF ACOUST SPEE, P2662, DOI [10.1109/icassp.2019.8683676, 10.1109/ICASSP.2019.8683676]
   Schölkopf B, 2000, ADV NEUR IN, V12, P582
   Sudhakaran S., 2017, P ADV VIDEO SIGNAL B, P1
   Sultani W, 2018, PROC CVPR IEEE, P6479, DOI 10.1109/CVPR.2018.00678
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   Tian YP, 2018, LECT NOTES COMPUT SC, V11206, P252, DOI 10.1007/978-3-030-01216-8_16
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu P., 2020, COMPUTER VISION ECCV, P322
   Xu L., 2010, P IEEE INT C AC SPEE, P3538
   Yan JJ, 2016, IEEE T MULTIMEDIA, V18, P1319, DOI 10.1109/TMM.2016.2557721
   Yapeng Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P436, DOI 10.1007/978-3-030-58580-8_26
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zajdel W, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P200, DOI 10.1109/AVSS.2007.4425310
   Zha J., 2019, ACM Trans. MultimediaComput., Commun., Appl., V15, P1
   Zhang DX, 2019, INFORM FUSION, V52, P268, DOI 10.1016/j.inffus.2019.03.005
   Zhao Hang, 2018, P EUR C COMP VIS ECC, P570, DOI DOI 10.1109/CVPR.2018.00374
   Zhou Zhi-Hua, 2004, Tech. Rep, V1
   Zhu ZY, 2013, INT CONF MACH LEARN, P973, DOI 10.1109/ICMLC.2013.6890423
   Zhu ZY, 2015, 2015 IEEE 16TH INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT), P768, DOI 10.1109/ICCT.2015.7399944
NR 53
TC 2
Z9 2
U1 4
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4922
EP 4932
DI 10.1109/TMM.2022.3184533
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300023
DA 2024-07-18
ER

PT J
AU Peng, DZ
   Jin, LW
   Ma, WH
   Xie, CY
   Zhang, HS
   Zhu, SG
   Li, J
AF Peng, Dezhi
   Jin, Lianwen
   Ma, Weihong
   Xie, Canyu
   Zhang, Hesuo
   Zhu, Shenggao
   Li, Jing
TI Recognition of Handwritten Chinese Text by Segmentation: A
   Segment-Annotation-Free Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Handwritten Chinese text recognition; online and offline text
   recognition; segmentation-based text recognition; weakly supervised
   learning
ID NEURAL-NETWORK; ONLINE; MODEL
AB Online and offline handwritten Chinese text recognition (HTCR) has been studied for decades. Early methods adopted oversegmentation-based strategies but suffered from low speed, insufficient accuracy, and high cost of character segmentation annotations. Recently, segmentation-free methods based on connectionist temporal classification (CTC) and attention mechanism, have dominated the field of HCTR. However, people actually read text character by character, especially for ideograms such as Chinese. This raises the question: are segmentation-free strategies really the best solution to HCTR? To explore this issue, we propose a new segmentation-based method for recognizing handwritten Chinese text that is implemented using a simple yet efficient fully convolutional network. A novel weakly supervised learning method is proposed to enable the network to be trained using only transcript annotations; thus, the expensive character segmentation annotations required by previous segmentation-based methods can be avoided. Owing to the lack of context modeling in fully convolutional networks, we propose a contextual regularization method to integrate contextual information into the network during the training stage, which can further improve the recognition performance. Extensive experiments conducted on four widely used benchmarks, namely CASIA-HWDB, CASIA-OLHWDB, ICDAR2013, and SCUT-HCCDoc, show that our method significantly surpasses existing methods on both online and offline HCTR, and exhibits a considerably higher inference speed than CTC/attention-based approaches.
C1 [Peng, Dezhi; Jin, Lianwen; Ma, Weihong; Xie, Canyu; Zhang, Hesuo] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Jin, Lianwen] Peng Cheng Lab, Guangzhou 518066, Peoples R China.
   [Zhu, Shenggao; Li, Jing] Huawei Cloud Comp Technol Co Ltd, Shenzhen 518129, Guangdong, Peoples R China.
C3 South China University of Technology
RP Jin, LW (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.; Jin, LW (corresponding author), Peng Cheng Lab, Guangzhou 518066, Peoples R China.
EM pengdzscut@foxmail.com; lianwen.jin@gmail.com;
   eeweihong_ma@mail.scut.edu.cn; eecanyuxie@mail.scut.edu.cn;
   eehesuo.zhang@mail.scut.edu.cn; zhushenggao@huawei.com;
   lijing260@huawei.com
RI Jin, Lianwen/AAJ-6536-2020; Peng, Dezhi/JRW-1806-2023
OI Peng, Dezhi/0000-0002-3263-3449; MA, WEIHONG/0009-0007-2252-3856; Jin,
   Lianwen/0000-0002-5456-0957
FU NSFC [61936003, 61771199]; GD-NSF [2017A030312006]
FX This work was supported in part by NSFC under Grants 61936003 and
   61771199, and in part by GD-NSF under Grant 2017A030312006.
CR [Anonymous], 1958, Trans. Amer. Math. Soc., DOI DOI 10.2307/1993193
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Chen K, 2017, PROC INT CONF DOC, P1068, DOI [10.1109/ICDAR.2017.177, 10.1109/ICCTEC.2017.00234]
   Cheng ZZ, 2017, IEEE I CONF COMP VIS, P5086, DOI 10.1109/ICCV.2017.543
   Chinese linguistic data consortium, CONT CORP DEV STAT L
   Chorowski J, 2015, ADV NEUR IN, V28
   Dezhi Peng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P25, DOI 10.1109/ICDAR.2019.00014
   Du J, 2016, INT C PATT RECOG, P3428, DOI 10.1109/ICPR.2016.7900164
   Fogel S, 2020, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR42600.2020.00438
   Graham B, 2013, Arxiv, DOI arXiv:1308.0371
   Graves A., 2006, P 23 INT C MACHINE L, P369
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   Hambly B, 2010, ANN MATH, V171, P109, DOI 10.4007/annals.2010.171.109
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jaderberg M., 2014, PROC ADV NEURAL INFO
   Jin G., PH CORPUS
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Liu B, 2021, LECT NOTES COMPUT SC, V12823, P274, DOI 10.1007/978-3-030-86334-0_18
   Liu CL, 2011, PROC INT CONF DOC, P37, DOI 10.1109/ICDAR.2011.17
   Liu MF, 2018, INT CONF FRONT HAND, P56, DOI 10.1109/ICFHR-2018.2018.00019
   Luo CJ, 2019, PATTERN RECOGN, V90, P109, DOI 10.1016/j.patcog.2019.01.020
   Lyons T., 2002, System control and rough paths, DOI DOI 10.1093/ACPROF:OSO/9780198506485.001.0001
   Lyons T, 2014, PROCEEDINGS OF THE INTERNATIONAL CONGRESS OF MATHEMATICIANS (ICM 2014), VOL IV, P163
   Marti U.-V., 2002, International Journal on Document Analysis and Recognition, V5, P39, DOI 10.1007/s100320200071
   Messina R, 2015, PROC INT CONF DOC, P171
   Neubeck A, 2006, INT C PATT RECOG, P850, DOI 10.1109/icpr.2006.479
   Ott M, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P48
   Peng DZ, 2021, LECT NOTES COMPUT SC, V12823, P157, DOI 10.1007/978-3-030-86334-0_11
   Rui Zhang, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1577, DOI 10.1109/ICDAR.2019.00253
   Shi BG, 2019, IEEE T PATTERN ANAL, V41, P2035, DOI 10.1109/TPAMI.2018.2848939
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi BG, 2016, PROC CVPR IEEE, P4168, DOI 10.1109/CVPR.2016.452
   Su TH, 2009, PATTERN RECOGN, V42, P167, DOI 10.1016/j.patcog.2008.05.012
   Sun L, 2016, INT CONF FRONT HAND, P271, DOI [10.1109/ICFHR.2016.0059, 10.1109/ICFHR.2016.54]
   The People's daily corpus, PEOPL DAIL NEWS INF
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DH, 2012, PATTERN RECOGN, V45, P3661, DOI 10.1016/j.patcog.2012.04.020
   Wang QF, 2012, IEEE T PATTERN ANAL, V34, P1469, DOI 10.1109/TPAMI.2011.264
   Wang S, 2016, INT CONF FRONT HAND, P84, DOI 10.1109/ICFHR.2016.25
   Wang TW, 2020, AAAI CONF ARTIF INTE, V34, P12216
   Wang ZX, 2020, INT CONF FRONT HAND, P157, DOI 10.1109/ICFHR2020.2020.00038
   Wang ZR, 2020, PATTERN RECOGN, V100, DOI 10.1016/j.patcog.2019.107102
   Wang ZR, 2018, INT J DOC ANAL RECOG, V21, P241, DOI 10.1007/s10032-018-0307-0
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Wu YC, 2017, PATTERN RECOGN, V65, P251, DOI 10.1016/j.patcog.2016.12.026
   Wu YC, 2017, PROC INT CONF DOC, P79, DOI 10.1109/ICDAR.2017.22
   Xie CY, 2020, LECT NOTES COMPUT SC, V12116, P45, DOI 10.1007/978-3-030-57058-3_4
   Xie ZC, 2016, INT C PATT RECOG, P4011
   Xie ZC, 2019, PROC CVPR IEEE, P6531, DOI 10.1109/CVPR.2019.00670
   Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978
   Xing LJ, 2019, IEEE I CONF COMP VIS, P9125, DOI 10.1109/ICCV.2019.00922
   Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007
   Yang WX, 2015, PROC INT CONF DOC, P551, DOI 10.1109/ICDAR.2015.7333822
   Yin F, 2013, PROC INT CONF DOC, P1464, DOI 10.1109/ICDAR.2013.218
   Yuhuan Xiu, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1464, DOI 10.1109/ICDAR.2019.00235
   Zhan FN, 2018, LECT NOTES COMPUT SC, V11212, P257, DOI 10.1007/978-3-030-01237-3_16
   Zhang HS, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107559
   Zhang JS, 2021, IEEE T MULTIMEDIA, V23, P2471, DOI 10.1109/TMM.2020.3011316
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zhou XD, 2014, PATTERN RECOGN, V47, P1904, DOI 10.1016/j.patcog.2013.12.002
   Zhou XD, 2013, IEEE T PATTERN ANAL, V35, P2413, DOI 10.1109/TPAMI.2013.49
   Zhu ZY, 2020, INT CONF FRONT HAND, P288, DOI 10.1109/ICFHR2020.2020.00060
NR 67
TC 8
Z9 8
U1 8
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2368
EP 2381
DI 10.1109/TMM.2022.3146771
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100059
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qin, W
   Zhang, HW
   Hong, RC
   Lim, EP
   Sun, QR
AF Qin, Wei
   Zhang, Hanwang
   Hong, Richang
   Lim, Ee-Peng
   Sun, Qianru
TI Causal Interventional Training for Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Automobiles; Task analysis; Visualization; Road
   transportation; Image recognition; Data models; Causality; causal
   intervention; deep learning; imagenet; image recognition
AB Deep learning models often fit undesired dataset bias in training. In this paper, we formulate the bias using causal inference, which helps us uncover the ever-elusive causalities among the key factors in training, and thus pursue the desired causal effect without the bias. We start from revisiting the process of building a visual recognition system, and then propose a structural causal model (SCM) for the key variables involved in dataset collection and recognition model: object, common sense, bias, context, and label prediction. Based on the SCM, one can observe that there are "good " and "bad " biases. Intuitively, in the image where a car is driving on a high way in a desert, the "good " bias denoting the common-sense context is the highway, and the "bad " bias accounting for the noisy context factor is the desert. We tackle this problem with a novel causal interventional training (CIT) approach, where we control the observed context in each object class. We offer theoretical justifications for CIT and validate it with extensive classification experiments on CIFAR-10, CIFAR-100 and ImageNet, e.g., surpassing the standard deep neural networks ResNet-34 and ResNet-50, respectively, by 0.95% and 0.70% accuracies on the ImageNet. Our code is open-sourced on the GitHub https://github.com/qinwei-hfut/CIT.
C1 [Qin, Wei; Hong, Richang] Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230006, Peoples R China.
   [Qin, Wei; Hong, Richang] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Zhang, Hanwang] Nanyang Technol Univ, Sch Comp Sci, Singapore 639798, Singapore.
   [Lim, Ee-Peng; Sun, Qianru] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
C3 Hefei University of Technology; Nanyang Technological University;
   Singapore Management University
RP Hong, RC (corresponding author), Hefei Univ Technol, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230006, Peoples R China.; Hong, RC (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
EM qinwei.hfut@gmail.com; hanwangzhang@ntu.edu.sg; hongrc.hfut@gmail.com;
   eplim@smu.edu.sg; qianrusun@smu.edu.sg
RI Sun, Qianru/HHN-0249-2022; Lim, Ee-Peng/E-8562-2012
OI Lim, Ee-Peng/0000-0003-0065-8665
FU A*STAR under its AME YIRG [61932009]; National Research Foundation
   Singapore under its International Research Centres in Singapore Funding
   Initiative; National Key Research and Development Program of China
   [A20E6c0101]; National Natural Science Foundation of China
   [2019YFA0706200, 61732007]; Singapore MOE McRF Tier 2
FX This work was supported by A*STAR under its AME YIRG under Grant Project
   A20E6c0101, in part by the National Research Foundation Singapore under
   its International Research Centres in Singapore Funding Initiative, in
   part by the National Key Research and Development Program of China under
   Grant 2019YFA0706200, in part by the National Natural Science Foundation
   of China under Grants 61732007, 61932009, and in part by the Singapore
   MOE McRF Tier 2. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lei Zhang.
   (Corresponding author: Richang Hong.)
CR Agarwal Vedika, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9687, DOI 10.1109/CVPR42600.2020.00971
   Alvi M, 2019, LECT NOTES COMPUT SC, V11129, P556, DOI 10.1007/978-3-030-11009-3_34
   Arjovsky M, 2020, Arxiv, DOI arXiv:1907.02893
   Atzmon Yuval, 2020, Advances in Neural Information Processing Systems, V33, P1462
   Barnea E, 2019, PROC CVPR IEEE, P7404, DOI 10.1109/CVPR.2019.00759
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Cao B, 2019, IEEE T NEUR NET LEAR, V30, P1731, DOI 10.1109/TNNLS.2018.2872675
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Fahimi F, 2021, IEEE T NEUR NET LEAR, V32, P4039, DOI 10.1109/TNNLS.2020.3016666
   Flores CF, 2019, PATTERN RECOGN, V94, P62, DOI 10.1016/j.patcog.2019.05.002
   Geiger D., 1989, P 5 C UNCERTAINTY AR, P139
   Goodfellow I., 2014, Neural Information Processing Systems, V27
   Greenland S, 1999, EPIDEMIOLOGY, V10, P37, DOI 10.1097/00001648-199901000-00008
   Guan MY, 2018, AAAI CONF ARTIF INTE, P3109
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ilse J. M., 2021, INT C MACHINE LEARNI, P4555
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Janzing D., 2019, ADV NEUR IN, V32
   Kaushik D., 2020, PROC INT C LEARN REP
   Kim B, 2019, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR.2019.00922
   Koh PW, 2017, PR MACH LEARN RES, V70
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A., 2012, Advances in Neural Information Processing Systems, V25, P1106
   Li YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4091, DOI 10.1145/3474085.3475540
   Liu J, 2016, AAAI CONF ARTIF INTE, P4232
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu XL, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3375787
   Liu ZG, 2019, PROC CVPR IEEE, P9996, DOI 10.1109/CVPR.2019.01024
   Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14
   Lu J, 2018, PR MACH LEARN RES, V80
   Luo YW, 2022, IEEE T PATTERN ANAL, V44, P3940, DOI 10.1109/TPAMI.2021.3064379
   MacKinnon DP, 2007, ANNU REV PSYCHOL, V58, P593, DOI 10.1146/annurev.psych.58.110405.085542
   Magliacane S, 2018, ADV NEUR IN, V31
   Mahajan D., 2019, PROC WORKSHOP NEURAL
   McNamee R, 2003, OCCUP ENVIRON MED, V60, P227, DOI 10.1136/oem.60.3.227
   Mitrovic J., 2021, PROC INT C LEARN REP
   Neal L, 2018, LECT NOTES COMPUT SC, V11210, P620, DOI 10.1007/978-3-030-01231-1_38
   Paszke A, 2019, ADV NEUR IN, V32
   Pearl J., 1993, Statistical Science, V8, P266, DOI DOI 10.1214/SS/1177010894
   Pearl J., 2009, CAUSALITY MODELS REA
   Pearl J, 2014, PSYCHOL METHODS, V19, P459, DOI 10.1037/a0036434
   Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057
   Pearl Judea, 2012, P 28 C UNCERTAINTY A, P3
   Prabhushankar M., 2021, P IEEE INT C IM PROC, P3697
   Ren MY, 2018, PR MACH LEARN RES, V80
   Richiardi L, 2013, INT J EPIDEMIOL, V42, P1511, DOI 10.1093/ije/dyt127
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schwab P., 2019, Advances in neural information processing systems, P10220
   Shao FF, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3321, DOI 10.1145/3474085.3475485
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh K.K., 2020, P IEEE CVF C COMP VI, P11070
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suter Raphael, 2019, P MACHINE LEARNING R, P6056
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Tan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10757, DOI 10.1109/CVPR42600.2020.01077
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Vanderweele TJ, 2013, ANN STAT, V41, P196, DOI 10.1214/12-AOS1058
   Yamashita Y, 2012, IEEE T MULTIMEDIA, V14, P619, DOI 10.1109/TMM.2012.2191396
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Yang X, 2019, IEEE T NEUR NET LEAR, V30, P2987, DOI [10.1109/TNNLS.2018.2861991, 10.1109/TNNLS.2018.2790479]
   Yoon J., 2018, 6 INT C LEARN REPR C
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   ZHANG D, 2020, ADV NEURAL INFORM PR, V33, P655, DOI DOI 10.5555/3495724.3495780
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhu ZT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3609
   Zmigrod R, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1651
NR 71
TC 10
Z9 11
U1 8
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1033
EP 1044
DI 10.1109/TMM.2021.3136717
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900027
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Rao, AY
   Xu, LN
   Li, ZZ
   Huang, QQ
   Kuang, ZH
   Zhang, WY
   Lin, DH
AF Rao, Anyi
   Xu, Linning
   Li, Zhizhong
   Huang, Qingqiu
   Kuang, Zhanghui
   Zhang, Wayne
   Lin, Dahua
TI A Coarse-to-Fine Framework for Automatic Video Unscreen
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Amateur green screen matting; automatic video unscreen; background
   estimation
ID SEGMENTATION; IMAGE
AB Video unscreen, a technique to extract foreground from given videos, has been playing an important role in today's video production pipeline. Existing systems developed for this purpose which mainly rely on video segmentation or video matting, either suffer from quality deficiencies or require tedious manual annotations. In this work, we aim to develop a fully automatic video unscreen framework that is able to obtain high-quality foreground extraction without the need of human intervention in a controlled environment. Our framework adopts a coarse-to-fine strategy, where the obtained background estimate given an initial mask prediction in turn helps the refinement of the mask by the alpha composition equation. We conducted experiments on two datasets, 1) the Adobe's Synthetic-Composite dataset, and 2) DramaStudio, our newly collected large-scale green screen video matting dataset, exhibiting the controlled environments. The results show that the proposed framework outperforms existing algorithms and commercial software, both quantitatively and qualitatively. We also demonstrate its utility in person replacement in videos, which can further support a variety of video editing applications.
C1 [Rao, Anyi; Xu, Linning; Huang, Qingqiu; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
   [Li, Zhizhong; Kuang, Zhanghui; Zhang, Wayne] Sense Time Res, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Rao, AY (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.
EM anyirao@ie.cuhk.edu.hk; linningxu@ie.cuhk.edu.hk; lizz@sensetime.com;
   hq016@ie.cuhk.edu.hk; kuangzhanghui@sensetime.com;
   wayne.zhang@sensetime.com; dhlin@ie.cuhk.edu.hk
RI Zhang, Wayne/AAY-7082-2021; Rao, Anyi/IXE-1082-2023; Zhang,
   Wayne/GXM-6869-2022; Lin, Dahua/W-6576-2019; Li, Zhizhong/ADV-8886-2022
OI Zhang, Wayne/0000-0002-8415-1062; Rao, Anyi/0000-0003-1004-7753; Zhang,
   Wayne/0000-0002-8415-1062; Lin, Dahua/0000-0002-8865-7896; Li,
   Zhizhong/0000-0003-0574-2487
CR Agarwal S, 2021, IEEE COMPUT SOC CONF, P981, DOI 10.1109/CVPRW53098.2021.00109
   Aksoy Y, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2907940
   [Anonymous], 2019, P IEEECVF INT C COMP
   Anyi Rao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P17, DOI 10.1007/978-3-030-58621-8_2
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P713, DOI 10.1007/978-3-030-58610-2_42
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen Y., 2019, US Patent App, Patent No. [15/945021, 15945021]
   Chuang YY, 2001, PROC CVPR IEEE, P264
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Feng DL, 2011, COMM COM INF SC, V202, P380
   Forte M, 2020, Arxiv, DOI arXiv:2003.07711
   Granados M, 2012, LECT NOTES COMPUT SC, V7572, P682, DOI 10.1007/978-3-642-33718-5_49
   Hou QQ, 2019, IEEE I CONF COMP VIS, P4129, DOI 10.1109/ICCV.2019.00423
   Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398
   Huang XM, 2022, IEEE T MULTIMEDIA, V24, P4458, DOI 10.1109/TMM.2021.3094356
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jain S, 2019, PROC CVPR IEEE, P8858, DOI 10.1109/CVPR.2019.00907
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang XK, 2022, IEEE T MULTIMEDIA, V24, P3049, DOI 10.1109/TMM.2021.3092143
   Jiangyue Xia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P174, DOI 10.1007/978-3-030-58610-2_11
   Jinlin Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8560, DOI 10.1109/CVPR42600.2020.00859
   Jung CR, 2009, IEEE T MULTIMEDIA, V11, P571, DOI 10.1109/TMM.2009.2012924
   Ke L, 2021, P IEEECVF INT C COMP, P14468
   Lee SY, 2010, GRAPH MODELS, V72, P25, DOI 10.1016/j.gmod.2010.03.001
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li XX, 2018, LECT NOTES COMPUT SC, V11207, P93, DOI 10.1007/978-3-030-01219-9_6
   Li YY, 2020, AAAI CONF ARTIF INTE, V34, P11450
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu YH, 2022, IEEE T MULTIMEDIA, V24, P2727, DOI 10.1109/TMM.2021.3087007
   Lu E, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417760
   Lu H, 2019, IEEE I CONF COMP VIS, P3265, DOI 10.1109/ICCV.2019.00336
   Lu X., 2020, ECCV, V12348, P661, DOI DOI 10.1007/978-3-030-58580-8_39
   Lu Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P490, DOI 10.1007/978-3-030-58568-6_29
   Luiten J, 2019, LECT NOTES COMPUT SC, V11364, P565, DOI 10.1007/978-3-030-20870-7_35
   Marra F, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P384, DOI 10.1109/MIPR.2018.00084
   Mingmin Zhen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P445, DOI 10.1007/978-3-030-58583-9_27
   Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713
   Ouyang H., 2021, P IEEE CVF INT C COM, P14579
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Porter T., 1984, Computers & Graphics, V18, P253
   Qingqiu Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P709, DOI 10.1007/978-3-030-58548-8_41
   Rao A., 2020, P IEEE CVF C COMP VI, P10146
   remove, REM BACKGR IM
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sabel J, 2021, IEEE COMPUT SOC CONF, P962, DOI 10.1109/CVPRW53098.2021.00107
   Sengupta S, 2020, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR42600.2020.00236
   Seoung Wug Oh, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P9225, DOI 10.1109/ICCV.2019.00932
   Shahrian E, 2014, COMPUT GRAPH FORUM, V33, P381, DOI 10.1111/cgf.12297
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Sidiropoulos P, 2011, IEEE T CIRC SYST VID, V21, P1163, DOI 10.1109/TCSVT.2011.2138830
   Stauffer C., 1999, ADAPTIVE BACKGROUND
   Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721
   topten, TOP 20 BACKGR REM TO
   Unscreen, REM VID BACKGR
   Vatolin Dmitriy S, 2015, BMVC, P99
   Voigtlaender P, 2019, PROC CVPR IEEE, P9473, DOI 10.1109/CVPR.2019.00971
   Wang C, 2019, AAAI CONF ARTIF INTE, P5232
   Wang J, 2007, FOUND TRENDS COMPUT, V3, P97, DOI 10.1561/0600000019
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Wexler Y, 2004, PROC CVPR IEEE, P120
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384
   Xu YS, 2018, PROC CVPR IEEE, P6556, DOI 10.1109/CVPR.2018.00686
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yang Z, 2019, IEEE I CONF COMP VIS, P931, DOI 10.1109/ICCV.2019.00102
   Yang ZQ, 2020, PROC CVPR IEEE, P5305, DOI 10.1109/CVPR42600.2020.00535
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yifan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P352, DOI 10.1007/978-3-030-58607-2_21
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Zhang H, 2021, IEEE WINT CONF APPL, P365, DOI 10.1109/WACV48630.2021.00041
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang YK, 2019, PROC CVPR IEEE, P7461, DOI 10.1109/CVPR.2019.00765
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhou TF, 2021, PROC CVPR IEEE, P6981, DOI 10.1109/CVPR46437.2021.00691
   Zhou TF, 2020, AAAI CONF ARTIF INTE, V34, P13066
   Zhu BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P297, DOI 10.1145/3123266.3123286
   Zhu GM, 2019, IEEE T MULTIMEDIA, V21, P1011, DOI 10.1109/TMM.2018.2869278
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
NR 88
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2723
EP 2733
DI 10.1109/TMM.2022.3150177
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600021
DA 2024-07-18
ER

PT J
AU Shih, YJ
   Wu, SL
   Zalkow, F
   Müller, M
   Yang, YH
AF Shih, Yi-Jen
   Wu, Shih-Lun
   Zalkow, Frank
   Mueller, Meinard
   Yang, Yi-Hsuan
TI Theme Transformer: Symbolic Music Generation With Theme-Conditioned
   Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Music; Decoding; Bars; Training; Testing; Deep learning;
   Automatic symbolic music generation; contrastive learning; parallel
   attention; positional encoding; theme-conditioned generation; theme
   retrieval; transformers
AB Attention-based Transformer models have been increasingly employed for automatic music generation. To condition the generation process of such a model with a user-specified sequence, a popular approach is to take that conditioning sequence as a priming sequence and ask a Transformer decoder to generate a continuation. However, this prompt-based conditioning cannot guarantee that the conditioning sequence would develop or even simply repeat itself in the generated continuation. In this paper, we propose an alternative conditioning approach, called theme-based conditioning, that explicitly trains the Transformer to treat the conditioning sequence as a thematic material that has to manifest itself multiple times in its generation result. This is achieved with two main technical contributions. First, we propose a deep learning-based approach that uses contrastive representation learning and clustering to automatically retrieve thematic materials from music pieces in the training data. Second, we propose a novel gated parallel attention module to be used in a sequence-to-sequence (seq2seq) encoder/decoder architecture to more effectively account for a given conditioning thematic material in the generation process of the Transformer decoder. We report on objective and subjective evaluations of variants of the proposed Theme Transformer and the conventional promptbased baseline, showing that our best model can generate, to some extent, polyphonic pop piano music with repetition and plausible variations of a given condition.
C1 [Shih, Yi-Jen] Natl Taiwan Univ, Elect Engn, Taipei 10617, Taiwan.
   [Wu, Shih-Lun] Natl Taiwan Univ, Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Zalkow, Frank; Mueller, Meinard] Univ Erlangen Nurnberg, Int Audio Labs Erlangen, D-91058 Erlangen, Germany.
   [Zalkow, Frank; Mueller, Meinard] Fraunhofer IIS, D-91058 Erlangen, Germany.
   [Yang, Yi-Hsuan] Acad Sinica, Res Ctr IT Innovat, Taipei 11564, Taiwan.
C3 National Taiwan University; National Taiwan University; University of
   Erlangen Nuremberg; University of Erlangen Nuremberg; Academia Sinica -
   Taiwan
RP Yang, YH (corresponding author), Acad Sinica, Res Ctr IT Innovat, Taipei 11564, Taiwan.
EM yjshih23@gmail.com; b06902080@csie.ntu.edu.tw;
   frank.zalkow@audiolabs-erlangen.de;
   meinard.mueller@audiolabs-erlangen.de; yang@citi.sinica.edu.tw
OI Zalkow, Frank/0000-0003-1383-4541; Yang, Yi-Hsuan/0000-0002-2724-6161;
   Mueller, Meinard/0000-0001-6062-7524
CR BENAMAR R, 2017, P INT S INTELL DATA, P14
   BOZHANOV B, 2014, COMPUTOSER RULEBASED
   Brown T., 2020, P ADV NEUR INF PROC, V33, P1877
   CHEN T, 2020, INT C MACH LEARN, P1597, DOI DOI 10.48550/ARXIV.2002.05709
   CHO K, 2014, P WORKSHOP SYNTAX SE
   Choi K, 2021, IEEE ACCESS, V9, P42071, DOI 10.1109/ACCESS.2021.3065831
   CHOROMANSKI K, 2021, P INT C LEARN REPRES
   Cífka O, 2020, IEEE-ACM T AUDIO SPE, V28, P2638, DOI 10.1109/TASLP.2020.3019642
   DAI S, 2021, P INT SOC MUSIC INF
   DEVLIN J, 2018, BERT PRETRAINING OF
   Donahue C., 2019, P 20 INT SOC MUS INF, P685
   DRABKIN W, 2001, GROVE MUSIC ONLINE, DOI DOI 10.1093/GM0/9781561592630.001.0001/0M0-9781561592630-E-0000027789
   ELOWSSON A, 2012, P INT C MUS PERC COG, P276
   ENS J, 2020, MMM EXPLORING CONDIT
   Fan A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P889
   FANG L, 2021, TRANSFORMERBASED CON
   Grekow J, 2021, IEEE ACCESS, V9, P129088, DOI 10.1109/ACCESS.2021.3113829
   GUAN Y, 2018, MELODIC PHRASE SEGME
   Hamanaka M, 2006, J NEW MUSIC RES, V35, P249, DOI 10.1080/09298210701563238
   HERNANDEZOLIVAN C, 2021, MUSIC COMPOSITION DE
   Herremans D, 2019, IEEE T AFFECT COMPUT, V10, P510, DOI 10.1109/TAFFC.2017.2737984
   Hsiao WY, 2021, AAAI CONF ARTIF INTE, V35, P178
   Hsu JL, 2001, IEEE T MULTIMEDIA, V3, P311, DOI 10.1109/6046.944475
   HUANG CZA, 2019, P INT C LEARN REPRES
   HUANG S, 2020, P ACM MULTIMEDIA C, P1180
   Jacob Bruce L., 1996, Organised Sound, V1, P157, DOI [10.1017/S1355771896000222, DOI 10.1017/S1355771896000222]
   Janssen B., 2013, International Symposium on Computer Music Modeling and Retrieval. CMMR 2013: Sound, Music, P277
   JHAMTANI H, 2019, P MACH LEARN MEDIA D
   JI S, 2020, A COMPREHENSIVE SURV
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   KE G, 2021, P INT C LEARN REPRES
   KESKAR NS, 2019, CTRL A CONDITIONAL T
   KINGMA DP, 2014, ADAM A METHOD STOCHA
   Kriegel H.-P., 1996, KNOWLEDGE DISCOVERY, P226, DOI DOI 10.5555/3001460
   Lartillot O., 2014, PROC 15 INT SOC MUSI, P361
   LARTILLOT O, 2009, P KNOWL REPRESENTATI
   Lewis M., 2020, P ACL
   Lim YQ, 2021, IEEE MULTIMEDIA, V28, P83, DOI 10.1109/MMUL.2020.3046491
   LIU Y, 2019, FINE TUNE BERT FOR E
   Medeot Gabriele, 2018, INT SOC MUSIC INFORM, P725
   Meredith D, 2002, J NEW MUSIC RES, V31, P321, DOI 10.1076/jnmr.31.4.321.14162
   MEREDITH D, 2019, P MACH LEARN KNOWL D, P485
   MIURA Y, 2009, P INT COMP MUS C, P125
   Muhamed A, 2021, AAAI CONF ARTIF INTE, V35, P408
   Payne C., 2019, MuseNet
   PINTO A, 2010, P WORKSHOP MINING LE, P102
   Radford A., 2019, LANGUAGE MODELS ARE
   Rashkin H, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4274
   REN Y, 2016, P INT WORKSHOP FOLK
   Ren Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1198, DOI 10.1145/3394171.3413721
   Roberts A., 2018, INT C MACHINE LEARNI, P4364, DOI DOI 10.48550/ARXIV.1803.05428.90I
   Shan MK, 2010, MULTIMED TOOLS APPL, V46, P1, DOI 10.1007/s11042-009-0303-y
   Sheng ZH, 2021, AAAI CONF ARTIF INTE, V35, P13798
   Uitdenbogerd A, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P57, DOI 10.1145/319463.319470
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   VELEZDEVILLA S, 2021, P SOUND MUSIC COMPUT, P131
   Volk A., 2011, Proceedings of the International Conference on Informatics and Semiotics in Organisations, P137
   Wang TM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5233
   Wang XH, 2022, INT J MACH LEARN CYB, V13, P293, DOI 10.1007/s13042-021-01402-9
   WANG Z, 2020, P INT SOC MUSIC INF
   Wu J, 2020, IEEE T CYBERNETICS, V50, P2749, DOI 10.1109/TCYB.2019.2953194
   WU SL, 2020, P INT SOC MUSIC INF
   WU X, 2020, TRANSFORMER XL BASED
   Yang LC, 2020, NEURAL COMPUT APPL, V32, P4773, DOI 10.1007/s00521-018-3849-7
   ZALKOW F, 2016, P ICMC, P206
   Zalkow F., 2020, Trans. Int. Soc. Music Inf. Retriev., V3, P180, DOI DOI 10.5334/TISMIR.68
   Zhang DJ, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P5419
   ZHANG N, IEEE T NEURAL NETW L
   ZOU Y, 2021, MELONS GENERATING ME
NR 69
TC 18
Z9 18
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3495
EP 3508
DI 10.1109/TMM.2022.3161851
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shu, YC
   Li, HB
   Xiao, B
   Bi, XL
   Li, WS
AF Shu, Yucheng
   Li, Hengbo
   Xiao, Bin
   Bi, Xiuli
   Li, Weisheng
TI Cross-Mix Monitoring for Medical Image Segmentation With Limited
   Supervision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Biomedical imaging; Training; Data models;
   Perturbation methods; Task analysis; Monitoring; Cross-mix teaching;
   medical image segmentation; Index Terms; Semi-supervised learning;
   transductive monitor
AB Image segmentation is a fundamental building block of automatic medical applications. It has been greatly improved since the emergence of deep neural networks. However, deep-learning based models often require a large number of manual annotations, which has seriously hindered its practical usage. To alleviate this problem, numerous works were proposed by utilizing unlabeled data based on semi-supervised frameworks. Recently, the Mean-Teacher (MT) model has been successfully applied in many scenarios due to its effective learning strategy. Nevertheless, the existing MT model still have certain limitations. Firstly, various sorts of perturbations are often added to the training data to gain extra generalization ability through consistency training. However, if the variation is too weak, it may cause the Lazy Student Phenomenon, and bring large fluctuations to the learning model. On the contrary, large image perturbations may enlarge the performance gap between the teacher and student. In this case, the student may lose its learning momentum, and more seriously, drag down the overall performance of the whole system. In order to address these issues, we introduce a novel semi-supervised medical image segmentation framework, in which a Cross-Mix Teaching paradigm is proposed to provide extra data flexibility, thus effectively avoid Lazy Student Phenomenon. Moreover, a lightweight Transductive Monitor is applied to server as the bridge that connect the teacher and student for active knowledge distillation. In the light of this cross-network information mixing and transfer mechanism, our method is able to continuously explore the discriminative information contained in unlabeled data. Extensive experiments on challenging medical image data sets demonstrate that our method is able to outperform current state-of-the-art semi-supervised segmentation methods under severe lack of supervision.
C1 [Shu, Yucheng; Li, Hengbo; Xiao, Bin; Bi, Xiuli; Li, Weisheng] Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Xiao, B (corresponding author), Chongqing Univ Posts & Telecommun, Sch Comp Sci & Technol, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
EM shuyc@cqupt.edu.cn; bogowangwang@gmail.com; xiaobin@cqupt.edu.cn;
   bixl@cqupt.edu.cn; liws@cqupt.edu.cn
RI Xiao, Bin/E-2722-2012; HB, LI/GRR-7437-2022
OI Shu, Yucheng/0000-0002-5737-9571; Bi, Xiuli/0000-0003-3134-217X
FU National Natural Science Foundation of China [61906024, 61976031,
   61801068]; National Major Scientific Research Instrument Development
   Project of China [62027827]; National Key R&D Program of China
   [2019YFE0110800, 2016YFC1000307-3]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61906024, 61976031, and 61801068, in
   part by the National Major Scientific Research Instrument Development
   Project of China under Grant 62027827, and in part by the National Key
   R&D Program of China under Grants 2019YFE0110800 and 2016YFC1000307-3.
CR Baur Christoph, 2017, Medical Image Computing and Computer Assisted Intervention  MICCAI 2017. 20th International Conference. Proceedings: LNCS 10435, P311, DOI 10.1007/978-3-319-66179-7_36
   Blendowski M, 2021, MED IMAGE ANAL, V67, DOI 10.1016/j.media.2020.101822
   Bortsova G, 2019, LECT NOTES COMPUT SC, V11769, P810, DOI 10.1007/978-3-030-32226-7_90
   Chaitanya K, 2019, LECT NOTES COMPUT SC, V11492, P29, DOI 10.1007/978-3-030-20351-1_3
   Chen ZZ, 2019, IEEE T IMAGE PROCESS, V28, P5785, DOI 10.1109/TIP.2019.2922072
   Choi Y, 2021, IEEE COMPUT SOC CONF, P3538, DOI 10.1109/CVPRW53098.2021.00393
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fu HZ, 2018, IEEE T MED IMAGING, V37, P1597, DOI 10.1109/TMI.2018.2791488
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Huang ZL, 2018, PROC CVPR IEEE, P7014, DOI 10.1109/CVPR.2018.00733
   Hung WC, 2018, Arxiv, DOI arXiv:1802.07934
   Huo XY, 2021, PROC CVPR IEEE, P1235, DOI 10.1109/CVPR46437.2021.00129
   Jose J, 2021, BIOMED SIGNAL PROCES, V66, DOI 10.1016/j.bspc.2021.102480
   Kang Li, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P418, DOI 10.1007/978-3-030-59710-8_41
   Nguyen KL, 2019, IEEE T IMAGE PROCESS, V28, P3678, DOI 10.1109/TIP.2019.2900587
   Laine S, 2017, Arxiv, DOI [arXiv:1610.02242, DOI 10.48550/ARXIV.1610.02242]
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li K, 2021, IEEE T MED IMAGING, V40, P2771, DOI 10.1109/TMI.2020.3038828
   Li X, 2020, IEEE T IMAGE PROCESS, V29, P128, DOI 10.1109/TIP.2019.2930874
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo XD, 2021, Arxiv, DOI arXiv:2009.04448
   Luo XD, 2021, LECT NOTES COMPUT SC, V12902, P318, DOI 10.1007/978-3-030-87196-3_30
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Mittal S, 2021, IEEE T PATTERN ANAL, V43, P1369, DOI 10.1109/TPAMI.2019.2960224
   Oliver A, 2018, ADV NEUR IN, V31
   Pang TY, 2020, Arxiv, DOI arXiv:1909.11515
   Peng J., 2021, Mach. Learn. Biomed. Imaging, P1
   Perone CS, 2018, LECT NOTES COMPUT SC, V11045, P12, DOI 10.1007/978-3-030-00889-5_2
   Qiao SY, 2018, LECT NOTES COMPUT SC, V11219, P142, DOI 10.1007/978-3-030-01267-0_9
   Quintanilla E, 2021, IEEE T MULTIMEDIA, V23, P1083, DOI 10.1109/TMM.2020.2992941
   Reiss S, 2021, PROC CVPR IEEE, P9527, DOI 10.1109/CVPR46437.2021.00941
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rui Huang, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12264), P146, DOI 10.1007/978-3-030-59719-1_15
   Shen YQ, 2021, MED IMAGE ANAL, V68, DOI 10.1016/j.media.2020.101908
   Shu YC, 2021, KNOWL-BASED SYST, V220, DOI 10.1016/j.knosys.2021.106950
   Shu YC, 2020, PROC INT C TOOLS ART, P753, DOI 10.1109/ICTAI50040.2020.00120
   Song YF, 2020, IEEE SIGNAL PROC LET, V27, P31, DOI 10.1109/LSP.2019.2953999
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tarvainen A, 2017, ADV NEUR IN, V30
   Verma V, 2020, Arxiv, DOI arXiv:1903.03825
   Wang L, 2022, IEEE T PATTERN ANAL, V44, P3048, DOI 10.1109/TPAMI.2021.3055564
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yu L., MED IMAGE COMPUTING
   Yu Q., 2021, arXiv
   Yu-Ting Chang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8988, DOI 10.1109/CVPR42600.2020.00901
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Alom MZ, 2018, Arxiv, DOI arXiv:1811.04241
   Zhang HY, 2018, Arxiv, DOI [arXiv:1710.09412, DOI 10.48550/ARXIV.1710.09412]
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
   Zhou ZW, 2018, LECT NOTES COMPUT SC, V11045, P3, DOI 10.1007/978-3-030-00889-5_1
NR 53
TC 8
Z9 8
U1 10
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1700
EP 1712
DI 10.1109/TMM.2022.3154159
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100008
DA 2024-07-18
ER

PT J
AU Su, YC
   Shao, ZW
   Zhou, Y
   Meng, FR
   Zhu, HC
   Liu, B
   Yao, R
AF Su, Yuchen
   Shao, Zhiwen
   Zhou, Yong
   Meng, Fanrong
   Zhu, Hancheng
   Liu, Bing
   Yao, Rui
TI TextDCT: Arbitrary-Shaped Text Detection via Discrete Cosine Transform
   Mask
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Arbitrary-shaped scene text detection; discrete cosine transform;
   single-level head; segmented non-maximum suppression
AB Arbitrary-shaped scene text detection is a challenging task due to the variety of text changes in font, size, color, and orientation. Most existing regression based methods resort to regress the masks or contour points of text regions to model the text instances. However, regressing the complete masks requires high training complexity, and contour points are not sufficient to capture the details of highly curved texts. To tackle the above limitations, we propose a novel light-weight anchor-free text detection framework called TextDCT, which adopts the discrete cosine transform (DCT) to encode the text masks as compact vectors. Further, considering the imbalanced number of training samples among pyramid layers, we only employ a single-level head for top-down prediction. To model the multi-scale texts in a single-level head, we introduce a novel positive sampling strategy by treating the shrunk text region as positive samples, and design a feature awareness module (FAM) for spatial-awareness and scale-awareness by fusing rich contextual information and focusing on more significant features. Moreover, we propose a segmented non-maximum suppression (S-NMS) method that can filter low-quality mask regressions. Extensive experiments are conducted on four challenging datasets, which demonstrate our TextDCT obtains competitive performance on both accuracy and efficiency. Specifically, TextDCT achieves F-measure of 85.1 at 17.2 frames per second (FPS) and F-measure of 84.9 at 15.1 FPS for CTW1500 and Total-Text datasets, respectively.
C1 [Su, Yuchen; Shao, Zhiwen; Zhou, Yong; Meng, Fanrong; Zhu, Hancheng; Liu, Bing; Yao, Rui] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
   [Su, Yuchen; Shao, Zhiwen; Zhou, Yong; Meng, Fanrong; Zhu, Hancheng; Liu, Bing; Yao, Rui] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Peoples R China.
C3 China University of Mining & Technology
RP Shao, ZW (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Peoples R China.
EM yuchen_su@cumt.edu.cn; zhiwen_shao@cumt.edu.cn; yzhou@cumt.edu.cn;
   mengfr@cumt.edu.cn; zhuhancheng@cumt.edu.cn; liubing@cumt.edu.cn;
   ruiyao@cumt.edu.cn
RI TIAN, YI/KHU-9704-2024; Li, Kunpeng/KFS-6306-2024; chen,
   huan/KEC-2019-2024; Wang, Zejun/KBB-8454-2024; su, yuchen/GSD-2025-2022;
   Yin, Jing/KDO-6274-2024; Wang, zhenhua/KFA-8731-2024; Shao,
   Zhiwen/N-8985-2018
OI su, yuchen/0000-0001-7743-8260; Yao, Rui/0000-0003-2734-915X; Liu,
   Bing/0000-0002-2365-6606; Shao, Zhiwen/0000-0002-9383-8384
FU National Natural Science Foundation of China [62106268]; High-Level
   Talent Program for Innovation and Entrepreneurship (Shuang Chuang
   Doctor) of Jiangsu Province [JSSCBS20211220]; National Natural Science
   Foundation of China [62101555, 62172417]; Natural Science Foundation of
   Jiangsu Province [BK20201346, BK20210488]; Six Talent Peaks Project in
   Jiangsu Province [2015-DZXX-010]; Fundamental Research Funds for the
   Central Universities [2021QN1072]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62106268, and in part by the High-Level
   Talent Program for Innovation and Entrepreneurship (Shuang Chuang
   Doctor) of Jiangsu Province under Grant JSSCBS20211220, in part by the
   National Natural Science Foundation of China under Grants 62101555 and
   62172417, in part by the Natural Science Foundation of Jiangsu Province
   under Grants BK20201346 and BK20210488, in part by Six Talent Peaks
   Project in Jiangsu Province under Grant 2015-DZXX-010, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   2021QN1072.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252
   Chen ZH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4939, DOI 10.1145/3474085.3475351
   Dai PW, 2021, Arxiv, DOI arXiv:2107.11800
   Dai PW, 2021, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR46437.2021.00731
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   HAQUE MA, 1985, IEEE T ACOUST SPEECH, V33, P1532, DOI 10.1109/TASSP.1985.1164737
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He MH, 2021, PROC CVPR IEEE, P8809, DOI 10.1109/CVPR46437.2021.00870
   Hong ZY, 2019, IEEE I CONF COMP VIS, P2861, DOI 10.1109/ICCV.2019.00295
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Lyu PY, 2018, LECT NOTES COMPUT SC, V11218, P71, DOI 10.1007/978-3-030-01264-9_5
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Nayef Nibal, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1582, DOI 10.1109/ICDAR.2019.00254
   Nayef N, 2017, PROC INT CONF DOC, P1454, DOI 10.1109/ICDAR.2017.237
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen X, 2021, PROC CVPR IEEE, P8716, DOI 10.1109/CVPR46437.2021.00861
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Veit A, 2016, Arxiv, DOI arXiv:1601.07140
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wu B., 2021, arXiv
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xing MT, 2022, IEEE T MULTIMEDIA, V24, P3129, DOI 10.1109/TMM.2021.3093727
   Xiong B, 2016, IEEE WINT CONF APPL
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Yan CX, 2020, IEEE T IMAGE PROCESS, V29, P8163, DOI 10.1109/TIP.2020.3011807
   Ye J., 2021, PROC INT JOINT C ART, P7
   Yi CC, 2014, IEEE T IMAGE PROCESS, V23, P2972, DOI 10.1109/TIP.2014.2317980
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1285, DOI 10.1109/ICCV48922.2021.00134
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
   Zhu YX, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107336
NR 65
TC 5
Z9 5
U1 15
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5030
EP 5042
DI 10.1109/TMM.2022.3186431
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300030
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, PF
   Ding, CX
   Shao, ZY
   Hong, ZB
   Zhang, SL
   Tao, DC
AF Wang, Pengfei
   Ding, Changxing
   Shao, Zhiyin
   Hong, Zhibin
   Zhang, Shengli
   Tao, Dacheng
TI Quality-Aware Part Models for Occluded Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person re-identification; occlusion; attention models
ID NETWORK
AB Occlusion poses a major challenge for person re-identification (ReID). Existing approaches typically rely on outside tools to infer visible body parts, which may be suboptimal in terms of both computational efficiency and ReID accuracy. In particular, they may fail when facing complex occlusions, such as those between pedestrians. Accordingly, in this paper, we propose a novel method named Quality-aware Part Models (QPM) for occlusion-robust ReID. First, we propose to jointly learn part features and predict part quality scores. As no quality annotation is available, we introduce a strategy that automatically assigns low scores to occluded body parts, thereby weakening the impact of occluded body parts on ReID results. Second, based on the predicted part quality scores, we propose a novel identity-aware spatial attention (ISA) module. In this module, a coarse identity-aware feature is utilized to highlight pixels of the target pedestrian, so as to handle the occlusion between pedestrians. Third, we design an adaptive and efficient approach for generating global features from common non-occluded regions with respect to each image pair. This design is crucial, but is often ignored by existing methods. QPM has three key advantages: 1) it does not rely on any outside tools in either the training or inference stages; 2) it handles occlusions caused by both objects and other pedestrians; 3) it is highly computationally efficient. Experimental results on four popular databases for occluded ReID demonstrate that QPM consistently outperforms state-of-the-art methods by significant margins. The code of QPM is available at https://github.com/Wang-pengfei/QPM.
C1 [Wang, Pengfei; Ding, Changxing; Shao, Zhiyin] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.
   [Ding, Changxing] Pazhou Lab, Guangzhou 510330, Peoples R China.
   [Hong, Zhibin] Baidu Inc, Dept Comp Vis Technol VIS, Shenzhen 518000, Peoples R China.
   [Zhang, Shengli] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518052, Peoples R China.
   [Tao, Dacheng] JD com, JD Explore Acad, Beijing 100176, Peoples R China.
C3 South China University of Technology; Pazhou Lab; Baidu; Shenzhen
   University
RP Ding, CX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510000, Peoples R China.; Ding, CX (corresponding author), Pazhou Lab, Guangzhou 510330, Peoples R China.
EM eepengfei.wang@mail.scut.edu.cn; chxding@scut.edu.cn;
   eezyshao@mail.scut.edu.cn; hongzhibin@baidu.com; zsl@szu.edu.cn;
   taodacheng@jd.com
RI Tao, Dacheng/A-5449-2012; zhang, shengli/HKM-5705-2023
OI Tao, Dacheng/0000-0001-7225-5449; Wang, Pengfei/0000-0002-3675-9508
FU National Natural Science Foundation of China [62076101, 61702193,
   62171291]; Program for Guangdong Introducing Innovative and
   Entrepreneurial Teams [2017ZT07X183]; CCF-Baidu Open Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076101, 61702193, and 62171291, in
   part by the Program for Guangdong Introducing Innovative and
   Entrepreneurial Teams under Grant 2017ZT07X183, and in part by CCF-Baidu
   Open Fund.
CR Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang B, 2021, IEEE T MULTIMEDIA, V24, P3218, DOI 10.1109/TMM.2021.3095789
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liao SC, 2013, IEEE T PATTERN ANAL, V35, P1193, DOI 10.1109/TPAMI.2012.191
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Miao JX, 2022, IEEE T NEUR NET LEAR, V33, P4624, DOI 10.1109/TNNLS.2021.3059515
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Qi L, 2019, Arxiv, DOI arXiv:1804.03864
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Scheirer WJ, 2016, INT CONF BIOMETR THE, DOI 10.1109/BTAS.2016.7791198
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Shizhen Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P647, DOI 10.1007/978-3-030-58539-6_39
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tian MQ, 2018, PROC CVPR IEEE, P5794, DOI 10.1109/CVPR.2018.00607
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang K, 2021, IEEE T IMAGE PROCESS, V30, P3405, DOI 10.1109/TIP.2021.3060909
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wang ZM, 2017, IEEE T GEOSCI REMOTE, V55, P3071, DOI 10.1109/TGRS.2017.2650938
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu AC, 2017, IEEE T IMAGE PROCESS, V26, P2588, DOI 10.1109/TIP.2017.2675201
   Xie P, 2022, IEEE T MULTIMEDIA, V24, P3908, DOI 10.1109/TMM.2021.3109665
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Zhang MM, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3093334
   Zhang X, 2022, IEEE T IND INFORM, V18, P3306, DOI 10.1109/TII.2020.3036164
   Zhang ZZ, 2020, INT CONF CONDIT MON, P404, DOI 10.1109/CVPR42600.2020.01042
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2017, PROC CVPR IEEE, P3346, DOI 10.1109/CVPR.2017.357
   Zheng L, 2019, IEEE T IMAGE PROCESS, V28, P4500, DOI 10.1109/TIP.2019.2910414
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou KY, 2019, Arxiv, DOI arXiv:1910.10093
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou Z, 2017, PROC CVPR IEEE, P6776, DOI 10.1109/CVPR.2017.717
   Zhuo JX, 2019, Arxiv, DOI arXiv:1907.03253
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 75
TC 23
Z9 23
U1 7
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3154
EP 3165
DI 10.1109/TMM.2022.3156282
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200016
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wu, KL
   Chen, J
   Ma, JY
AF Wu, Kangle
   Chen, Jun
   Ma, Jiayi
TI DMEF: Multi-Exposure Image Fusion Based on a Novel Deep Decomposition
   Method
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep decomposition; illumination; reflection; multi-exposure image
   fusion; retinex theory
ID QUALITY ASSESSMENT; RETINEX; PERFORMANCE; NETWORK
AB In this paper, we propose a novel deep decomposition approach based on Retinex theory for multi-exposure image fusion, termed as DMEF. According to the assumption of Retinex theory, we firstly decompose the source images into illumination and reflection maps by the data-driven decomposition network, among which we introduce the pathwise interaction block that reactivates the deep features lost in one path and embeds them into another path. Therefore, loss of illumination and reflection features during decomposition can be effectively suppressed. And then the high dynamic range illumination map could be obtained by fusing the separated illumination maps in the fusion network. Thus, the reconstructed details in under-exposed and over-exposed regions will be clearer with the help of the fused reflection map which contains complete high-frequency scene information. Finally, the fused illumination and reflection maps are multiplied pixel-by-pixel to obtain the final fused image. Moreover, to retain the discontinuity in the illumination map where gradient of reflection map changes steeply, we introduce the structure-preservation smoothness loss function to retain the structure information and eliminate visual artifacts in these regions. The superiority of our proposed network is demonstrated by applying extensive experiments compared with other state-of-the-art fusion methods subjectively and objectively.
C1 [Wu, Kangle; Chen, Jun] China Univ Geosci, Sch Automat, Hubei Key Lab Adv Control & Intelligent Automat C, Engn Res Ctr Intelligent Technol Geoexplorat,Mini, Wuhan 430074, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Sch Elect Informat, Wuhan 430072, Peoples R China.
C3 China University of Geosciences; Wuhan University
RP Chen, J (corresponding author), China Univ Geosci, Sch Automat, Hubei Key Lab Adv Control & Intelligent Automat C, Engn Res Ctr Intelligent Technol Geoexplorat,Mini, Wuhan 430074, Peoples R China.
EM wukangle@cug.edu.cn; chenjun71983@163.com; jyma2010@gmail.com
RI Ma, Jiayi/Y-2470-2019
OI Ma, Jiayi/0000-0003-3264-3265; Wu, Kangle/0000-0002-0147-756X; Chen,
   Jun/0000-0001-9005-6849; Wu, Kangle/0009-0009-3469-047X
FU National Natural Science Foundation of China [62073304, 41977242,
   61973283]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62073304, 41977242, and 61973283. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. P. Shivakumara. (Corresponding author: Jun Chen.)
CR Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   Fang YM, 2020, IEEE T IMAGE PROCESS, V29, P1127, DOI 10.1109/TIP.2019.2940678
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Huang F, 2018, IEEE ACCESS, V6, P42877, DOI 10.1109/ACCESS.2018.2859355
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jian LH, 2021, IEEE T MULTIMEDIA, V24, P3314, DOI 10.1109/TMM.2021.3096088
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Kotwal K., 2011, P INT C INF FUS, P1
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P5805, DOI 10.1109/TIP.2020.2987133
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Liu Y, 2015, J VIS COMMUN IMAGE R, V31, P208, DOI 10.1016/j.jvcir.2015.06.021
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P2519, DOI 10.1109/TIP.2017.2671921
   Ma KD, 2015, IEEE IMAGE PROC, P1717, DOI 10.1109/ICIP.2015.7351094
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Moon J, 2014, INT SOC DESIGN CONF, P256, DOI 10.1109/ISOCC.2014.7087635
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen R, 2013, IEEE T IMAGE PROCESS, V22, P2469, DOI 10.1109/TIP.2012.2236346
   Shi YM, 2019, Arxiv, DOI arXiv:1906.06027
   Son CH, 2016, IEEE T IMAGE PROCESS, V25, P2866, DOI 10.1109/TIP.2016.2556618
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, P BRIT MACH VIS C, P155
   Xiang H. Y., Adv. Mater. Res., V403, P2200
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Yang WH, 2021, IEEE T IMAGE PROCESS, V30, P2072, DOI 10.1109/TIP.2021.3050850
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang W, 2017, INFORM SCIENCES, V415, P19, DOI 10.1016/j.ins.2017.05.019
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu Ya-hui, 2012, Application Research of Computers, V29, P2784, DOI 10.3969/j.issn.1001-3695.2012.07.105
NR 40
TC 3
Z9 3
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5690
EP 5703
DI 10.1109/TMM.2022.3198327
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500004
DA 2024-07-18
ER

PT J
AU Xue, T
   El Ali, A
   Zhang, TY
   Ding, GY
   Cesar, P
AF Xue, Tong
   El Ali, Abdallah
   Zhang, Tianyi
   Ding, Gangyi
   Cesar, Pablo
TI CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation
   Dataset for 360° VR Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Physiology; Annotations; Resists; Emotion recognition;
   Electroencephalography; Visualization; 360 degrees video; virtual
   reality; emotion; dataset; HMD; physiological signals; head and eye
   movement; continuous annotation
ID OF-THE-ART; SALIENCY; PREDICTION
AB Watching 360 degrees videos using Virtual Reality (VR) head-mounted displays (HMDs) provides interactive and immersive experiences, where videos can evoke different emotions. Existing emotion self-report techniques within VR however are either retrospective or interrupt the immersive experience. To address this, we introduce the Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 degrees Videos (CEAP-360VR). We conducted a controlled study (N=32) where participants used a Vive Pro Eye HMD to watch eight validated affective 360 degrees video clips, and annotated their valence and arousal (V-A) continuously. We collected (a) behavioral (head and eye movements; pupillometry) signals (b) physiological (heart rate, skin temperature, electrodermal activity) responses (c) momentary emotion self-reports (d) within-VR discrete emotion ratings (e) motion sickness, presence, and workload. We show the consistency of continuous annotation trajectories and verify their mean V-A annotations. We find high consistency between viewed 360 degrees video regions across subjects, with higher consistency for eye than head movements. We furthermore run baseline classification experiments, where Random Forest classifiers with 2s segments show good accuracies for subject-independent models: 66.80% (V) and 64.26% (A) for binary classification; 49.92% (V) and 52.20% (A) for 3-class classification. Our open dataset allows further experiments with continuous emotion self-reports collected in 360 degrees VR environments, which can enable automatic assessment of immersive Quality of Experience (QoE) and momentary affective states.
C1 [Xue, Tong; Ding, Gangyi] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
   [Xue, Tong; El Ali, Abdallah; Zhang, Tianyi] Ctr Wiskunde & Informat CWI, Distributed & Interact Syst, NL-1089 XG Amsterdam, Netherlands.
   [Zhang, Tianyi] Delft Univ Technol, Multimedia Comp Grp, NL-2600 AA Delft, Netherlands.
C3 Beijing Institute of Technology; Delft University of Technology
RP Xue, T (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
EM xuetong@bit.edu.cn; abdallah.elali@gmail.com; tianyi.zhang@cwi.nl;
   dgy@bit.edu.cn; p.s.cesar@cwi.nl
RI Zhang, Tianyi/AAG-6220-2021
OI Zhang, Tianyi/0000-0001-6293-881X; El Ali, Abdallah/0000-0002-9954-4088
FU National Key Research and Development Program of China [2020YFF0305200];
   National Natural Science Foundation of China [62177005]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFF0305200 and in part by
   the National Natural Science Foundation of China under Grant 62177005.
CR Abadi MK, 2015, IEEE T AFFECT COMPUT, V6, P209, DOI 10.1109/TAFFC.2015.2392932
   Alcaniz M., 2003, PSYCHNOLOGY J, V1, P141
   Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   Assilmia Fathima., 2017, Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, Denver, CO, USA, May 06-11, 2017, Extended Abstracts, P2359, DOI [10.1145/3027063.3053211, DOI 10.1145/3027063.3053211]
   Bach DR, 2010, INT J PSYCHOPHYSIOL, V75, P349, DOI 10.1016/j.ijpsycho.2010.01.005
   Barrett LF, 2019, PSYCHOL SCI PUBL INT, V20, P1, DOI 10.1177/1529100619832930
   Beck J, 2019, TOUR REV, V74, P586, DOI 10.1108/TR-03-2017-0049
   Birjandtalab J, 2016, 2016 IEEE INTERNATIONAL WORKSHOP ON SIGNAL PROCESSING SYSTEMS (SIPS), P110, DOI 10.1109/SiPS.2016.27
   Boucsein W, 2012, ELECTRODERMAL ACTIVITY, SECOND EDITION, P1, DOI 10.1007/978-1-4614-1126-0
   Bradley MM, 2008, PSYCHOPHYSIOLOGY, V45, P602, DOI 10.1111/j.1469-8986.2008.00654.x
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Bray E. T., 2014, 7159 RFC
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chen LF, 2021, IEEE TETCI, V5, P205, DOI 10.1109/TETCI.2019.2909930
   Cicchetti D. V., 1994, PSYCHOL ASSESSMENTS, V6, P284, DOI [DOI 10.1037/1040-3590.6.4.284, 10.1037/1040-3590.6.4.284, https://doi.org/10.1037/1040-3590.6.4.284]
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   Egan D, 2016, 2016 EIGHTH INTERNATIONAL CONFERENCE ON QUALITY OF MULTIMEDIA EXPERIENCE (QOMEX)
   Fatourechi M, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P777, DOI 10.1109/ICMLA.2008.34
   Fleureau J, 2013, INT CONF AFFECT, P73, DOI 10.1109/ACII.2013.19
   FREDRICKSON BL, 1993, J PERS SOC PSYCHOL, V65, P45, DOI 10.1037/0022-3514.65.1.45
   Girard JM, 2018, BEHAV RES METHODS, V50, P902, DOI 10.3758/s13428-017-0915-5
   Gunst RF, 2009, WIRES COMPUT STAT, V1, P234, DOI 10.1002/wics.27
   Handayani D., 2015, TELKOMNIKA, V13, P1343, DOI [10.12928/telkomnika.v13i4.2735, DOI 10.12928/TELK0MNIKA.V13I4.2735]
   Hart S. G., 2006, P HUM FACT ERG SOC A, V50, P904, DOI DOI 10.1177/154193120605000909
   He C, 2017, LECT NOTES ELECTR EN, V399, P15, DOI 10.1007/978-981-10-2404-7_2
   He LJ, 2018, 24TH ACM SYMPOSIUM ON VIRTUAL REALITY SOFTWARE AND TECHNOLOGY (VRST 2018), DOI 10.1145/3281505.3281508
   ITU-T Recommendation,, 1999, SUBJ VID QUAL ASS ME, P4
   Kardan O, 2015, J EXP PSYCHOL HUMAN, V41, P1502, DOI 10.1037/a0039673
   Kennedy RS, 1993, The international journal of aviation psychology, V3, P203, DOI [DOI 10.1207/S15327108IJAP0303, 10.1207/s15327108ijap0303_3]
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Li BJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02116
   Li C, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P932, DOI 10.1145/3240508.3240581
   Li J, 2015, IEEE I CONF COMP VIS, P190, DOI 10.1109/ICCV.2015.30
   Liao D, 2020, IEEE J ELECTROMAG RF, V4, P216, DOI 10.1109/JERM.2019.2948767
   Lopes P, 2017, INT CONF AFFECT, P158, DOI 10.1109/ACII.2017.8273594
   Lutz A, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001897
   Ma JX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P176, DOI 10.1145/3343031.3350871
   MacQuarrie A, 2017, P IEEE VIRT REAL ANN, P45, DOI 10.1109/VR.2017.7892230
   Marín-Morales J, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185163
   Marín-Morales J, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-32063-4
   Maskey M, 2013, J AUTISM DEV DISORD, V43, P851, DOI 10.1007/s10803-012-1622-9
   Milstein N, 2020, FRONT BEHAV NEUROSCI, V14, DOI 10.3389/fnbeh.2020.00148
   Miranda-Correa JA, 2021, IEEE T AFFECT COMPUT, V12, P479, DOI 10.1109/TAFFC.2018.2884461
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Moon SE, 2017, IEEE T MULTIMEDIA, V19, P340, DOI 10.1109/TMM.2016.2614880
   Nagel F, 2007, BEHAV RES METHODS, V39, P283, DOI 10.3758/BF03193159
   Nasoz F., 2004, Cognition, Technology & Work, V6, P4, DOI 10.1007/s10111-003-0143-x
   Ozcinar C, 2018, INT WORK QUAL MULTIM, P1
   Pfleging B, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P5776, DOI 10.1145/2858036.2858117
   Putze S, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376144
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Rigas G, 2007, LECT NOTES ARTIF INT, V4511, P314
   Rodríguez DZ, 2014, IEEE T CONSUM ELECTR, V60, P436, DOI 10.1109/TCE.2014.6937328
   Romeo L, 2022, IEEE T AFFECT COMPUT, V13, P389, DOI 10.1109/TAFFC.2019.2954118
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schubert T, 2001, PRESENCE-VIRTUAL AUG, V10, P266, DOI 10.1162/105474601300343603
   Sharma K, 2019, SCI DATA, V6, DOI 10.1038/s41597-019-0209-0
   Shu L, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18072074
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Soleymani M, 2016, IEEE T AFFECT COMPUT, V7, P17, DOI 10.1109/TAFFC.2015.2436926
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Subramanian R, 2018, IEEE T AFFECT COMPUT, V9, P147, DOI 10.1109/TAFFC.2016.2625250
   Tang W, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123126
   Tarnowski P, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/2909267
   Toet A, 2019, LECT NOTES COMPUT SC, V11883, P330, DOI 10.1007/978-3-030-31908-3_24
   Van den Broeck M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P762, DOI 10.1145/3123266.3123347
   van den Broek E. L., 2011, THESIS U TWENTE, DOI [10.3990/1.9789036532433, DOI 10.3990/1.9789036532433]
   van den Broek EL, 2010, COMM COM INF SC, V52, P21
   Wang JY, 2021, IEEE T MULTIMEDIA, V23, P3227, DOI 10.1109/TMM.2020.3021984
   Wickramasuriya DS, 2017, 2017 IEEE-NIH HEALTHCARE INNOVATIONS AND POINT OF CARE TECHNOLOGIES (HI-POCT), P52, DOI 10.1109/HIC.2017.8227582
   Xu M, 2020, IEEE J-STSP, V14, P5, DOI 10.1109/JSTSP.2020.2966864
   Xu M, 2019, IEEE T CIRC SYST VID, V29, P3516, DOI [10.1109/TCSVT.2018.2886277, 10.1080/17445302.2018.1558727]
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Xue T., 2021, C HUM FACT COMP SYST, DOI DOI 10.1145/3411764.3445487
   Xue T, 2021, EXTENDED ABSTRACTS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'21), DOI 10.1145/3411763.3451627
   Xue T, 2020, CHI'20: EXTENDED ABSTRACTS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3334480.3382895
   Zhang TY, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376808
   Zhang TY, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21010052
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhao BB, 2018, 2018 IEEE SMARTWORLD, UBIQUITOUS INTELLIGENCE & COMPUTING, ADVANCED & TRUSTED COMPUTING, SCALABLE COMPUTING & COMMUNICATIONS, CLOUD & BIG DATA COMPUTING, INTERNET OF PEOPLE AND SMART CITY INNOVATION (SMARTWORLD/SCALCOM/UIC/ATC/CBDCOM/IOP/SCI), P346, DOI 10.1109/SmartWorld.2018.00091
   Zhao SC, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3363560
   Zhiwei Zhu, 2002, Proceedings ETRA 2002. Eye Tracking Research and Applications Symposium, P139
   Zou FY, 2019, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2019.01138
NR 87
TC 16
Z9 16
U1 10
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 243
EP 255
DI 10.1109/TMM.2021.3124080
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 8C6OL
UT WOS:000917725300003
OA Green Published
DA 2024-07-18
ER

PT J
AU Yang, KW
   Tian, XM
AF Yang, Kaiwen
   Tian, Xinmei
TI Domain-Class Correlation Decomposition for Generalizable Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain generalization; information entropy; person re-identification
ID ENHANCEMENT; ATTENTION; NETWORK
AB Domain generalization in person re-identification is a highly important meaningful and practical task in which a model trained with data from several source domains is expected to generalize well to unseen target domains. Domain adversarial learning is a promising domain generalization method that aims to remove domain information in the latent representation through adversarial training. However, in person re-identification, the domain and class are correlated, and we theoretically show that domain adversarial learning will lose certain information about class due to this domain-class correlation. Inspired by causal inference, we propose to perform interventions to the domain factor d, aiming to decompose the domain-class correlation. To achieve this goal, we proposed estimating the resulting representation z(*) caused by the intervention through first-and second-order statistical characteristic matching. Specifically, we build a memory bank to restore the statistical characteristics of each domain. Then, we use the newly generated samples {z(*), y, d(*)} to compute the loss function. These samples are domain-class correlation decomposed; thus, we can learn a domain-invariant representation that can capture more class-related features. Extensive experiments show that our model outperforms the state-of-the-art methods on the large-scale domain generalization Re-ID benchmark.
C1 [Yang, Kaiwen; Tian, Xinmei] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Tian, XM (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM kwyang@mail.ustc.edu.cn; xinmei@ustc.edu.cn
FU NSFC [61872329]; Fundamental Research Funds for the Central Universities
   [WK3490000005]; MCC Lab of Information Science and Technology
   Institution, USTC
FX This work was supported in part by NSFC under Grant 61872329, in part by
   the Fundamental Research Funds for the Central Universities under Grant
   WK3490000005, and in part by the GPU cluster built by MCC Lab of
   Information Science and Technology Institution, USTC.
CR Akuzawa K., 2019, JOINT EUR C MACH LEA, P315
   Bak S, 2018, LECT NOTES COMPUT SC, V11217, P193, DOI 10.1007/978-3-030-01261-8_12
   Balaji Y., 2018, Advances in Neural Information Processing Systems, P998
   Carlucci FM, 2019, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2019.00233
   Chen PX, 2021, AAAI CONF ARTIF INTE, V35, P1054
   Chen P, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P620
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ding YH, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3369393
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fan X, 2019, LECT NOTES COMPUT SC, V11362, P19, DOI 10.1007/978-3-030-20890-5_2
   Fengchun Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12553, DOI 10.1109/CVPR42600.2020.01257
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Ge Yixiao, 2020, ARXIV200101526
   Ghifary M, 2017, IEEE T PATTERN ANAL, V39, P1414, DOI 10.1109/TPAMI.2016.2599532
   Gong MM, 2016, PR MACH LEARN RES, V48
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heinze-Deml C, 2021, MACH LEARN, V110, P303, DOI 10.1007/s10994-020-05924-1
   Hinton G. E., 2002, Advances in Neural InformationProcessing Systems, P857
   Hirzer M, 2011, LECT NOTES COMPUT SC, V6688, P91, DOI 10.1007/978-3-642-21227-7_9
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jia J., 2019, BMVC, V117
   Kaiyang Zhou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P561, DOI 10.1007/978-3-030-58517-4_33
   Li D, 2019, IEEE I CONF COMP VIS, P1446, DOI 10.1109/ICCV.2019.00153
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li HL, 2018, PROC CVPR IEEE, P5400, DOI 10.1109/CVPR.2018.00566
   Li S, 2018, PROC CVPR IEEE, P369, DOI 10.1109/CVPR.2018.00046
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li Y, 2018, AAAI CONF ARTIF INTE, P3579
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YY, 2022, IEEE T MULTIMEDIA, V24, P415, DOI 10.1109/TMM.2021.3052354
   Li YJ, 2017, ADV NEUR IN, V30
   Lin S., 2018, BMVC, V9
   Loesch A, 2019, IEEE IMAGE PROC, P4574, DOI [10.1109/icip.2019.8803643, 10.1109/ICIP.2019.8803643]
   Loy CC, 2010, INT J COMPUT VISION, V90, P106, DOI 10.1007/s11263-010-0347-5
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Motiian S, 2017, IEEE I CONF COMP VIS, P5716, DOI 10.1109/ICCV.2017.609
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Peng PX, 2016, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2016.146
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shankar S, 2018, Arxiv, DOI arXiv:1804.10745
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Song JF, 2019, PROC CVPR IEEE, P719, DOI 10.1109/CVPR.2019.00081
   Song LC, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107173
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Sun YF, 2021, IEEE T PATTERN ANAL, V43, P902, DOI 10.1109/TPAMI.2019.2938523
   Tang H., 2019, PROC IEEECVF C COMPU
   Tolstikhin I., 2018, 6 INT C LEARN REPR I
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wei LH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P420, DOI 10.1145/3123266.3123279
   Xie QK, 2021, IEEE T MULTIMEDIA, V23, P597, DOI 10.1109/TMM.2020.2985525
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang JW, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1074, DOI 10.1145/3240508.3240645
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yao LY, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3444944
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zhang H, 2021, IEEE T IMAGE PROCESS, V30, P5287, DOI 10.1109/TIP.2021.3082298
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zhao HY, 2017, PROC CVPR IEEE, P907, DOI 10.1109/CVPR.2017.103
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhou F., 2020, arXiv
   Zhou K., 2021, PROC INT C LEARN REP
   Zhu YH, 2020, NEUROCOMPUTING, V403, P88, DOI 10.1016/j.neucom.2020.04.088
NR 78
TC 2
Z9 2
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3386
EP 3396
DI 10.1109/TMM.2022.3160057
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, ML
   Xu, CH
   Wu, A
   Deng, C
AF Yang, Muli
   Xu, Chenghao
   Wu, Aming
   Deng, Cheng
TI A Decomposable Causal View of Compositional Zero-Shot Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compositional zero-shot learning; vision and language; image
   recognition; causality
AB Composing and recognizing novel concepts that are combinations of known concepts, i.e., compositional generalization, is one of the greatest power of human intelligence. With the development of artificial intelligence, it becomes increasingly appealing to build a vision system that can generalize to unknown compositions based on restricted known knowledge, which has so far remained a great challenge to our community. In fact, machines can be easily misled by superficial correlations in the data, disregarding the causal patterns that are crucial to generalization. In this paper, we rethink compositional generalization with a causal perspective, upon the context of Compositional Zero-Shot Learning (CZSL). We develop a simple yet strong approach based on our novel Decomposable Causal view (dubbed "DECA"), by approximating the causal effect with the combination of three easy-to-learn components. Our proposed DECA(1) is evaluated on two challenging CZSL benchmarks by recognizing unknown compositions of known concepts. Despite being simple in the design, our approach achieves consistent improvements over state-of-the-art baselines, demonstrating its superiority towards the goal of compositional generalization.
C1 [Yang, Muli; Xu, Chenghao; Wu, Aming; Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
C3 Xidian University
RP Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM muliyang.xd@gmail.com; chx@stu.xidian.edu.cn; amwu@xidian.edu.cn;
   chdeng.xd@gmail.com
FU National Natural Science Foundation of China [62132016, 62171343,
   62071361]; Key Research and Development Program of Shaanxi
   [2021ZDLGY01-03]; Fundamental Research Funds for the Central
   Universities [ZDRC2102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132016, 62171343, and 62071361, in
   part by the Key Research and Development Program of Shaanxi under Grant
   2021ZDLGY01-03, and in part by the Fundamental Research Funds for the
   Central Universities under Grant ZDRC2102.
CR Alayrac JB, 2017, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2017.234
   [Anonymous], 2009, Advances in neural information processing systems
   Atzmon Y, 2016, Arxiv, DOI arXiv:1608.07639
   Atzmon Y, 2020, ADV NEUR IN, V33
   Ba J. L., P NIPS 2016 DEEP LEA
   Bin Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P438, DOI 10.1007/978-3-030-58548-8_26
   Bojanowski P., 2017, Transactions of the Association for Computational Linguistics, V5, P135, DOI DOI 10.1162/TACLA00051
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4
   Chen BH, 2019, PROC CVPR IEEE, P2745, DOI 10.1109/CVPR.2019.00286
   Chen CY, 2014, PROC CVPR IEEE, P200, DOI 10.1109/CVPR.2014.33
   Chen H, 2020, Arxiv, DOI arXiv:2010.14343
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen XL, 2021, PROC CVPR IEEE, P15745, DOI 10.1109/CVPR46437.2021.01549
   Cruz RS, 2018, IEEE WINT CONF APPL, P729, DOI 10.1109/WACV.2018.00085
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Fathi A, 2013, PROC CVPR IEEE, P2579, DOI 10.1109/CVPR.2013.333
   Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Huang H, 2019, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2019.00089
   Hudson D.A., 2018, INT C LEARN REPR ICL
   Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Judea P., 2000, Causality: Models, Reasoning, and Inference, V521
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kingma D., 2014, ICLR, DOI DOI 10.1063/1.4902458
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Koushik J., 2017, Compositional reasoning for visual question answering
   Lake B. M., 2014, Ph. D. Dissertation
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Mancini M, 2021, PROC CVPR IEEE, P5218, DOI 10.1109/CVPR46437.2021.00518
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P3919, DOI 10.1109/TMM.2020.3033124
   Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129
   Mitrovic J., 2021, P INT C LEARN REPR, P1
   Morgan SL, 2015, ANAL METHOD SOC RES, P1
   Muli Yang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10245, DOI 10.1109/CVPR42600.2020.01026
   Naeem MF, 2021, PROC CVPR IEEE, P953, DOI 10.1109/CVPR46437.2021.00101
   Nagarajan T, 2018, LECT NOTES COMPUT SC, V11205, P172, DOI 10.1007/978-3-030-01246-5_11
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Nan ZX, 2019, AAAI CONF ARTIF INTE, P8811
   Ni J, 2019, ADV NEUR IN, V32
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Pearl J., 2001, P 17 C UNCERTAINTY A, P411, DOI DOI 10.5555/2074022.2074073
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Purushwalkam S, 2019, IEEE I CONF COMP VIS, P3592, DOI 10.1109/ICCV.2019.00369
   Qin W, 2023, IEEE T MULTIMEDIA, V25, P1033, DOI 10.1109/TMM.2021.3136717
   Quinn T, 2018, TLS-TIMES LIT SUPPL, P31
   Robins J. M., 2003, HIGHLY STRUCTURED ST, P70, DOI DOI 10.1093/OSO/9780198510550.003.0007
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880
   Ruis Frank, 2021, Advances in Neural Information Processing Systems, V34, P10641
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Scholkopf Bernhard, 2019, arXiv
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tan W., 2021, P IEEECVF INT C COMP, P3091
   Tang K., 2020, ADV NEURAL INFORM PR, P1513
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Wang WL, 2018, AAAI CONF ARTIF INTE, P4211
   Wang X, 2020, Arxiv, DOI arXiv:1906.04854
   Wang X, 2019, PROC CVPR IEEE, P1831, DOI 10.1109/CVPR.2019.00193
   Wang Y.-X., 2017, Advances in Neural Information Processing Systems, P7032
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xu GY, 2021, 1ST WORKSHOP ON META LEARNING AND ITS APPLICATIONS TO NATURAL LANGUAGE PROCESSING (METANLP 2021), P19
   Xu ZW, 2021, IEEE T MULTIMEDIA, V24, P3652, DOI 10.1109/TMM.2021.3104411
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Yong-Lu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11313, DOI 10.1109/CVPR42600.2020.01133
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yue Z., 2020, P 34 C NEUR INF PROC, P2734
   Yue ZQ, 2021, PROC CVPR IEEE, P15399, DOI 10.1109/CVPR46437.2021.01515
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhu PK, 2019, PROC CVPR IEEE, P2990, DOI 10.1109/CVPR.2019.00311
NR 82
TC 0
Z9 0
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5892
EP 5902
DI 10.1109/TMM.2022.3200578
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500018
DA 2024-07-18
ER

PT J
AU Zhang, SH
   Zuo, DX
   Yang, YL
   Zhang, XW
AF Zhang, Shihui
   Zuo, Dongxu
   Yang, Yongliang
   Zhang, Xiaowei
TI A Transferable Adversarial Belief Attack With Salient Region
   Perturbation Restriction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Belief attack; dynamic update; perturbation restriction; high
   transferability
AB Deep neural networks are vulnerable to adversarial examples which are crafted by adding small perturbations on benign examples. However, most existing attack methods often perform a poor transferability to attack black-box models, especially to attack defense methods. In addition, perturbations added to semantically irrelevant regions of benign examples are usually inefficient for attacks. To address these issues, we propose a transferable adversarial belief attack with salient region perturbation restriction method, which improves transferability of adversarial examples and decreases the amount of perturbations significantly. Specifically, we first design a salient-region-based perturbation restriction strategy to restrict the range of perturbations into a salient region. After that, we present a transferable belief attack method to generate the adversarial examples iteratively. Besides, our method can be easily integrated with other gradient-based transfer attack methods to further enhance the transferability of adversarial examples. Extensive experiments on the ImageNet dataset show that our method achieves higher transferability with lower perturbations than the state-of-the-art attack methods.
C1 [Zhang, Shihui; Zuo, Dongxu; Yang, Yongliang; Zhang, Xiaowei] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Peoples R China.
C3 Yanshan University
RP Zuo, DX (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Peoples R China.
EM sshhzz@ysu.edu.cn; zzddxx@stumail.ysu.edu.cn; yll0731@stumail.edu.cn;
   xwzhang0724@163.com
RI zhang, shihui/ABD-6039-2021; Zhang, Shihui/KFB-3255-2024; Yang,
   Yongliang/AAA-6243-2019; Zhang, Xiaowei/HTS-3267-2023; zuo,
   dongxu/GRX-7611-2022
OI zhang, shihui/0000-0001-7601-3631; Yang, Yongliang/0000-0002-3144-8604;
   Zhang, Xiaowei/0000-0003-2059-6262; zuo, dongxu/0000-0003-2578-9071
FU Central Government Guided Local Funds for Science and Technology
   Development [216Z0301G]; National Natural Science Foundation of China
   [61379065]; Natural Science Foundation of Hebei province in China
   [F2019203285]
FX This work was supported in part by the Central Government Guided Local
   Funds for Science and Technology Development under Grant 216Z0301G, in
   part by the National Natural Science Foundation of China under Grant
   61379065, and in part by the Natural Science Foundation of Hebei
   province in China under Grant F2019203285.
CR [Anonymous], 2014, P INT C LEARN REPR
   Brendel W., 2018, ICLR
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen JB, 2020, P IEEE S SECUR PRIV, P1277, DOI 10.1109/SP40000.2020.00045
   Chen PY, 2017, P 10 ACM WORKSH ART, P15, DOI [10.1145/3128572.3140448, DOI 10.1145/3128572.3140448]
   Cheng M., 2018, P INT C LEARN REPR
   Cheng YP, 2022, IEEE T MULTIMEDIA, V24, P3807, DOI 10.1109/TMM.2021.3108009
   Cohen Jeremy, 2019, INT C MACH LEARN PML, P1310
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo C., 2018, 6 INT C LEARN REPR I
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Ilyas A., 2018, P INT C LEARN REPR, P1
   Ilyas Andrew, 2018, P INT C MACH LEARN, V80
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kurakin Alexey, 2017, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin J., 2020, P 8 INT C LEARN REPR
   Liu Y., 2016, P INT C LEARN REPR
   Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095
   Madry A., 2018, ARXIV
   Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543
   Papernot N, 2016, Arxiv, DOI arXiv:1605.07277
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   Wierstra D, 2014, J MACH LEARN RES, V15, P949
   Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Zhou MY, 2020, PROC CVPR IEEE, P231, DOI 10.1109/CVPR42600.2020.00031
   Zhuang JT, 2020, ADV NEUR IN, V33
NR 37
TC 4
Z9 4
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4296
EP 4306
DI 10.1109/TMM.2022.3173533
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200016
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Gong, WK
   Sun, YJ
   Li, LD
   Wu, JJ
   Gao, XB
AF Zhou, Yu
   Gong, Weikang
   Sun, Yanjing
   Li, Leida
   Wu, Jinjian
   Gao, Xinbo
TI Pyramid Feature Aggregation for Hierarchical Quality Prediction of
   Stitched Panoramic Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stitched panoramic image quality assessment (SPIQA); imaginary reference
   generation; hierarchical quality prediction; pyramid feature
   aggregation; Stitched panoramic image quality assessment (SPIQA);
   imaginary reference generation; hierarchical quality prediction; pyramid
   feature aggregation
ID INDEX
AB Panoramic image quality assessment (PIQA) is crucial to the successful application of technologies that can provide immersive visual experience. Stitching distortions are one of the main types of distortions that result in panoramic image degradation. However, most existing PIQA methods are general-purpose ones, which ignore the special characteristics of the stitching distortions caused by imperfect stitching algorithms. This results in unsatisfactory performance. To this end, we propose an effective stitched PIQA method, which consists of an imaginary reference generation (IRG) module and a hierarchical quality prediction (HQP) module. Among them, the IRG module is proposed to mimic the capability of the human visual system in imagining the raw version in the face of a degraded image. For the IRG module learning, we construct a large-scale database. The HQP module is presented to adapt to the particularity and complexity of stitching distortions, which is achieved by the pyramid feature aggregation. Extensive experiments and comparisons have been performed on the stitched PIQA database and the experimental results demonstrate the superiority of the proposed method in evaluating the quality of stitched panoramic images.
C1 [Zhou, Yu; Gong, Weikang; Sun, Yanjing] China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
   [Zhou, Yu] Xuzhou First Peoples Hosp, Xuzhou 221116, Peoples R China.
   [Li, Leida; Wu, Jinjian] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Gao, Xinbo] Chongqing Univ Posts & Telecommun, Chongqing Key Lab Image Cognit, Chongqing 400065, Peoples R China.
C3 China University of Mining & Technology; Xidian University; Chongqing
   University of Posts & Telecommunications
RP Sun, YJ (corresponding author), China Univ Min & Technol, Sch Informat & Control Engn, Xuzhou 221116, Peoples R China.
EM zhouy@cumt.edu.cn; gongweikang1998@163.com; yjsun@cumt.edu.cn;
   ldli@xidian.edu.cn; jinjian.wu@mail.xidian.edu.cn; gaoxb@cqupt.edu.cn
RI Sun, Yanjing/Q-2576-2019
OI Sun, Yanjing/0000-0002-1389-3958
FU National Natural Science Foundation of China [62001475, 62036007,
   BK20200649]; Natural Science Foundation of Jiangsu Province [62171340];
   Program for "Industrial IoT and Emergency Collaboration" Innovative
   Research Team in China University of Mining and Technology [2020ZY002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62001475, in part by the Natural Science
   Foundation of Jiangsu Province under Grant BK20200649, in part by the
   National Natural Science Foundation of China under Grants 62171340 and
   62036007, and in part by the Program for "Industrial IoT and Emergency
   Collaboration" Innovative Research Team in China University of Mining
   and Technology under Grant 2020ZY002.
CR Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Chen S., 2018, P IEEE INT C MULT EX, P1
   Duan HY, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351786
   Hou JW, 2020, IEEE IMAGE PROC, P3463, DOI 10.1109/ICIP40778.2020.9191241
   Huang W, 2019, IEEE T MED IMAGING, V38, P2338, DOI 10.1109/TMI.2019.2906677
   Jiang H, 2021, IEEE T IMAGE PROCESS, V30, P2364, DOI 10.1109/TIP.2021.3052073
   Keighrey C, 2021, IEEE T MULTIMEDIA, V23, P333, DOI 10.1109/TMM.2020.2982046
   Kim HG, 2020, IEEE T CIRC SYST VID, V30, P917, DOI 10.1109/TCSVT.2019.2898732
   Kingma D. P., 2014, arXiv
   Kupyn O, 2019, IEEE I CONF COMP VIS, P8877, DOI 10.1109/ICCV.2019.00897
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Lim HT, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6737, DOI 10.1109/ICASSP.2018.8461317
   Lin HH, 2019, INT WORK QUAL MULTIM
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ling SY, 2018, IEEE INT CON MULTI
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Shangdong Zhu, 2018, 2018 IEEE International Conference on Information and Automation (ICIA). Proceedings, P1605, DOI 10.1109/ICInfA.2018.8812565
   Sun W, 2020, IEEE J-STSP, V14, P64, DOI 10.1109/JSTSP.2019.2955024
   Sun YL, 2017, IEEE SIGNAL PROC LET, V24, P1408, DOI 10.1109/LSP.2017.2720693
   Tian CZ, 2021, J VIS COMMUN IMAGE R, V81, DOI 10.1016/j.jvcir.2021.103324
   Wang X, 2020, IEEE J BIOMED HEALTH, V24, P3431, DOI 10.1109/JBHI.2020.2983730
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1705, DOI 10.1109/TMM.2013.2268053
   Xu JH, 2021, IEEE T CIRC SYST VID, V31, P1724, DOI 10.1109/TCSVT.2020.3015186
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang LY, 2017, IEEE INT CONF COMP V, P2487, DOI 10.1109/ICCVW.2017.293
   Yang YY, 2020, SIGNAL PROCESS, V173, DOI 10.1016/j.sigpro.2020.107586
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yue GH, 2020, IEEE T IND INFORM, V16, P1764, DOI 10.1109/TII.2019.2927527
   Yue GH, 2019, IEEE T MULTIMEDIA, V21, P2184, DOI 10.1109/TMM.2019.2913315
   Yue GH, 2019, IEEE T IMAGE PROCESS, V28, P2075, DOI 10.1109/TIP.2018.2875913
   Zakharchenko V, 2016, PROC SPIE, V9970, DOI 10.1117/12.2235885
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Y, 2013, J ELECTRON IMAGING, V22, DOI 10.1117/1.JEI.22.4.043025
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng XL, 2020, IEEE ACCESS, V8, P31647, DOI 10.1109/ACCESS.2020.2972158
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou XS, 2017, 2017 4TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P46, DOI 10.1109/ICISCE.2017.20
   Zhou Y, 2022, IEEE T CIRC SYST VID, V32, P1767, DOI 10.1109/TCSVT.2021.3081162
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhou Y, 2018, IEEE T MULTIMEDIA, V20, P3019, DOI 10.1109/TMM.2018.2829607
NR 52
TC 6
Z9 6
U1 8
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4177
EP 4186
DI 10.1109/TMM.2022.3171684
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200008
DA 2024-07-18
ER

PT J
AU Akhtar, A
   Gao, W
   Li, L
   Li, Z
   Jia, W
   Liu, S
AF Akhtar, Anique
   Gao, Wen
   Li, Li
   Li, Zhu
   Jia, Wei
   Liu, Shan
TI Video-Based Point Cloud Compression Artifact Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Geometry; Quantization (signal); Bit rate;
   Encoding; Vehicle dynamics; Deep learning; Point cloud; artifact
   removal; compression; 3D deep learning; quantization; V-PCC
ID MPEG
AB Photo-realistic point cloud capture and transmission are the fundamental enablers for immersive visual communication. The coding process of dynamic point clouds, especially video-based point cloud compression (V-PCC) developed by the MPEG standardization group, is now delivering state-of-the-art performance in compression efficiency. V-PCC is based on the projection of the point cloud patches to 2D planes and encoding the sequence as 2D texture and geometry patch sequences. However, the resulting quantization errors from coding can introduce compression artifacts, which can be very unpleasant for the quality of experience (QoE). In this work, we developed a novel out-of-the-loop point cloud geometry artifact removal solution that can significantly improve reconstruction quality without additional bandwidth cost. Our novel framework consists of a point cloud sampling scheme, an artifact removal network, and an aggregation scheme. The point cloud sampling scheme employs a cube-based neighborhood patch extraction to divide the point cloud into patches. The geometry artifact removal network then processes these patches to obtain artifact-removed patches. The artifact-removed patches are then merged together using an aggregation scheme to obtain the final artifact-removed point cloud. We employ 3D deep convolutional feature learning for geometry artifact removal that jointly recovers both the quantization direction and the quantization noise level by exploiting projection and quantization prior. The simulation results demonstrate that the proposed method is highly effective and can considerably improve the quality of the reconstructed point cloud.
C1 [Akhtar, Anique; Li, Li; Li, Zhu; Jia, Wei] Univ Missouri Kansas City, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
   [Gao, Wen; Liu, Shan] Tencent Amer, Palo Alto, CA 94301 USA.
   [Gao, Wen; Li, Li; Liu, Shan] Univ Sci & Technol China, CAS, Key Lab Technol Geospatial Informat Proc & Applic, Hefei 230027, Peoples R China.
C3 University of Missouri System; University of Missouri Kansas City;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Li, Z (corresponding author), Univ Missouri Kansas City, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
EM aniqueakhtar@mail.umkc.edu; wengao@tencent.com; lil1@ustc.edu.cn;
   zhu.li@ieee.org; wj3wr@umsystem.edu; shanl@tencent.com
RI huang, shan/JVN-1240-2024
OI , Shan/0000-0002-1442-1207; Akhtar, Anique/0000-0003-2701-6611; Jia,
   Wei/0000-0003-0053-6959
FU NSF I/UCRC [CNS-1747751]
FX This work was supported by the NSF I/UCRC under Grant CNS-1747751.
CR Akhtar A., 2019, P ICC IEEE INT C COM, P1
   Akhtar A, 2019, IEEE IMAGE PROC, P2369, DOI [10.1109/icip.2019.8803373, 10.1109/ICIP.2019.8803373]
   [Anonymous], ISO/IEC JTC1/SC29/WG11, call for proposals on 3d video coding technology, Doc
   [Anonymous], 2017, document ISO/IEC JTC1/SC29/WG11 N17229
   [Anonymous], 2014, 6DoF vs 3DoF
   [Anonymous], 2019, MOBILE MAPPING SYSTE
   [Anonymous], DOCUMENT ISOIEC JTC1
   Bjontegaard G, 2001, VCEG-M33 ITU-T SG 16/Q 6
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dinesh C., 2018, P IEEE INT WORKSHOP, P1
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Fu ZQ, 2018, IEEE IMAGE PROC, P2137, DOI 10.1109/ICIP.2018.8451550
   Fuchs H, 2014, COMPUTER, V47, P46, DOI 10.1109/MC.2014.185
   Galteri L, 2017, IEEE I CONF COMP VIS, P4836, DOI 10.1109/ICCV.2017.517
   Gao X, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   Guennebaud G, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239474
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Hu W, 2019, IEEE T IMAGE PROCESS, V28, P4087, DOI 10.1109/TIP.2019.2906554
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Krivokuca Maja, 2018, ISO/ IEC JTC1/ SC29 WG11 (MPEG) input document m42914
   Li L, 2021, IEEE T MULTIMEDIA, V23, P2806, DOI 10.1109/TMM.2020.3016894
   Li RH, 2019, IEEE I CONF COMP VIS, P7202, DOI 10.1109/ICCV.2019.00730
   Li YZ, 2018, ADV NEUR IN, V31
   Lipman Y, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276405
   Lu G, 2018, LECT NOTES COMPUT SC, V11218, P591, DOI 10.1007/978-3-030-01264-9_35
   Matsuzaki Kohei, 2019, 2019 IEEE 8th Global Conference on Consumer Electronics (GCCE), P489, DOI 10.1109/GCCE46687.2019.9015550
   Mattei E, 2017, COMPUT GRAPH FORUM, V36, P123, DOI 10.1111/cgf.13068
   Öztireli AC, 2009, COMPUT GRAPH FORUM, V28, P493, DOI 10.1111/j.1467-8659.2009.01388.x
   Qi CR, 2017, ADV NEUR IN, V30
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rusu RB, 2008, ROBOT AUTON SYST, V56, P927, DOI 10.1016/j.robot.2008.08.005
   Schwarz S., 2018, ISO/IEC JTC1/SC29/WG11
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tu CX, 2019, IEEE INT CONF ROBOT, P3274, DOI [10.1109/icra.2019.8794264, 10.1109/ICRA.2019.8794264]
   Tulvan C., 2016, ISOIEC JTC1SC29WG11
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   Wang PS, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073608
   Yu LQ, 2018, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2018.00295
   Yu LQ, 2018, LECT NOTES COMPUT SC, V11211, P398, DOI 10.1007/978-3-030-01234-2_24
   Yu YK, 2020, NEUROCOMPUTING, V384, P192, DOI 10.1016/j.neucom.2019.12.032
   Yue Qian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P752, DOI 10.1007/978-3-030-58529-7_44
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
NR 51
TC 22
Z9 24
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2866
EP 2876
DI 10.1109/TMM.2021.3090148
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000014
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Cao, L
   Zhang, HJ
   Feng, L
AF Cao, Lei
   Zhang, Huijun
   Feng, Ling
TI Building and Using Personal Knowledge Graph to Improve Suicidal Ideation
   Detection on Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blogs; Social networking (online); Psychology; Stress; Statistics;
   Sociology; Knowledge engineering; Personal knowledge graph; social
   interaction; social media; suicidal ideation detection
ID MENTAL-HEALTH; PARENTING STYLE; NORMATIVE DATA; RISK-FACTORS; VALIDITY;
   BEHAVIOR; ANXIETY; STRESS; ASSOCIATION; TWITTER
AB A large number of individuals are suffering from suicidal ideation in the world. There are a number of causes behind why an individual might suffer from suicidal ideation. As the most popular platform for self-expression, emotion release, and personal interaction, individuals may exhibit a number of symptoms of suicidal ideation on social media. Nevertheless, challenges from both data and knowledge aspects remain as obstacles, constraining the social media-based detection performance. Data implicitness and sparsity make it difficult to discover the inner true intentions of individuals based on their posts. Inspired by psychological studies, we build and unify a high-level suicide-oriented knowledge graph with deep neural networks for suicidal ideation detection on social media. We further design a two-layered attention mechanism to explicitly reason and establish key risk factors to individual's suicidal ideation. The performance study on microblog and Reddit shows that: 1) with the constructed personal knowledge graph, the social media-based suicidal ideation detection can achieve over 93% accuracy; and 2) among the six categories of personal factors, post, personality, and experience are the top-3 key indicators. Under these categories, posted text, stress level, stress duration, posted image, and ruminant thinking contribute to one's suicidal ideation detection.
C1 [Cao, Lei; Zhang, Huijun; Feng, Ling] Tsinghua Univ, Ctr Computat Mental Healthcare, Dept Comp Sci & Technol, Res Inst Data Sci, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Feng, L (corresponding author), Tsinghua Univ, Ctr Computat Mental Healthcare, Dept Comp Sci & Technol, Res Inst Data Sci, Beijing 100084, Peoples R China.
EM cao-l17@mails.tsinghua.edu.cn; zhang-hj17@mails.tsinghua.edu.cn;
   fengling@mail.tsinghua.edu.cn
RI Zhang, Huijun/AAJ-7296-2020
OI Cao, Lei/0000-0002-2778-6870
FU National Natural Science Foundation of China [61872214, 61532015,
   61521002]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61872214, 61532015, and 61521002. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. ChuanWu.
CR Alambo A, 2019, IEEE INT C SEMANT CO, P468, DOI [10.1109/ICSC.2019.00090, 10.1109/ICOSC.2019.8665525]
   Bagge C, 1998, PSYCHOL REP, V83, P637
   Blüml V, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0076646
   Bouarara Hadj Ahmed, 2020, International Journal of Knowledge-Based Organizations, V10, P49, DOI 10.4018/IJKBO.2020010103
   Braithwaite SR, 2016, JMIR MENT HEALTH, V3, DOI 10.2196/mental.4822
   Brisset C, 2014, J IMMIGR MINOR HEALT, V16, P1238, DOI 10.1007/s10903-013-9971-9
   Cao L, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1718
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen Yen-Chun, 2020, P 58 ANN M ASS COMP, P7893, DOI DOI 10.18653/V1/2020.ACL-MAIN.705
   Cheng QJ, 2017, J MED INTERNET RES, V19, DOI 10.2196/jmir.7276
   Christensen H, 2014, INT J ENV RES PUB HE, V11, P8193, DOI 10.3390/ijerph110808193
   Cooperman NA, 2005, J BEHAV MED, V28, P149, DOI 10.1007/s10865-005-3664-3
   Coppersmith G., 2015, P JOINT STAT M P STA, P1
   Coppersmith G., 2014, P WORKSH COMP LING C, P51, DOI [10.3115/v1/w14-3207, DOI 10.3115/V1/W14-3207]
   Coppersmith G, 2018, BIOMED INFORM INSIGH, V10, DOI 10.1177/1178222618792860
   Crawford JR, 2003, BRIT J CLIN PSYCHOL, V42, P111, DOI 10.1348/014466503321903544
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   DARLING N, 1993, PSYCHOL BULL, V113, P487, DOI 10.1037/0033-2909.113.3.487
   De Choudhury M, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P2098, DOI 10.1145/2858036.2858207
   Du CN, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P4019
   Du JC, 2018, BMC MED INFORM DECIS, V18, DOI 10.1186/s12911-018-0632-8
   Essau CA, 2005, DEPRESS ANXIETY, V22, P130, DOI 10.1002/da.20115
   Fodeh Samah J., 2019, 2019 International Conference on Data Mining Workshops (ICDMW). Proceedings, P941, DOI 10.1109/ICDMW.2019.00137
   Fu KW, 2007, PSYCHOL ASSESSMENT, V19, P422, DOI 10.1037/1040-3590.19.4.422
   Gaur M, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P514, DOI 10.1145/3308558.3313698
   Gould M, 2003, AM BEHAV SCI, V46, P1269, DOI 10.1177/0002764202250670
   Greenspon TS, 2014, PSYCHOL SCHOOLS, V51, P986, DOI 10.1002/pits.21797
   Guan L, 2015, JMIR MENT HEALTH, V2, DOI 10.2196/mental.4227
   Guan Li, 2014, ARXIV14070466
   Hamilton WL, 2017, ADV NEUR IN, V30
   Harris KM, 2017, INT J MENT HEALTH NU, V26, P181, DOI 10.1111/inm.12223
   Harris KM, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0127442
   Harris KM, 2014, DEATH STUD, V38, P387, DOI 10.1080/07481187.2013.768313
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henry JD, 2005, BRIT J CLIN PSYCHOL, V44, P227, DOI 10.1348/014466505X29657
   Huang C. L., 2012, CHINESE J PSYCHOL, V54, P185, DOI DOI 10.6129/CJP.2012.5402.04
   Huang X., 2015, P 29 PAC AS C LANG I, P553
   Huang YP, 2007, IEEE INT SYM MULTIM, P517, DOI 10.1109/ISM.Workshops.2007.92
   Jacob N., 2014, J CRISIS, P1
   Jashinsky J, 2014, CRISIS, V35, P51, DOI 10.1027/0227-5910/a000234
   Ji S., 2020, IEEE T COMPUTAT SOCI, P1
   Ji Shaoxiong, 2020, ARXIV200407601
   Jones N., 2019, PROC 8 INT C AFFECT, P1
   Just MA, 2017, NAT HUM BEHAV, V1, P911, DOI 10.1038/s41562-017-0234-y
   Kipf TN, 2017, INT C LEARN REPR
   Lai KW, 2001, INT J PSYCHOL, V36, P81, DOI 10.1080/00207590042000065
   Li Q, 2017, IEEE J BIOMED HEALTH, V21, P1434, DOI 10.1109/JBHI.2016.2586519
   Li T. M., 2013, P PAC AS WORKSH INT, P29, DOI DOI 10.1007/978-3-642-39693-9_4
   Limsopatham N, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1014
   Lv MZ, 2015, PEERJ, V3, DOI 10.7717/peerj.1455
   Malhotra A., 2018, EAI Endorsed Trans. Pervasive Health Technol, V6, P164259, DOI DOI 10.4108/EAI.13-7-2018.164259
   Mandelli L, 2015, PSYCHIAT RES, V226, P38, DOI 10.1016/j.psychres.2014.11.001
   Masuda N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062262
   Mbarek A, 2019, WEBIST: PROCEEDINGS OF THE 15TH INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND TECHNOLOGIES, P289, DOI 10.5220/0008167602890296
   Mishra R, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE STUDENT RESEARCH WORKSHOP, P147
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P396, DOI 10.1109/TKDE.2014.2330813
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Nock MK, 2008, BRIT J PSYCHIAT, V192, P98, DOI 10.1192/bjp.bp.107.040113
   O'Dea Bridianne, 2015, Internet Interventions, V2, P183, DOI 10.1016/j.invent.2015.03.005
   Parraga-Alava J, 2019, P INT C CHIL COMPUT, DOI 10.1109/sccc49216.2019.8966443
   Pestian J, 2010, BIOMED INFORM INSIGH, V3, P19, DOI 10.4137/BII.S4706
   Pestian JP, 2017, SUICIDE LIFE-THREAT, V47, P112, DOI 10.1111/sltb.12312
   Ren FJ, 2016, IEEE J BIOMED HEALTH, V20, P1384, DOI 10.1109/JBHI.2015.2459683
   RICH AR, 1987, SUICIDE LIFE-THREAT, V17, P265
   Rickwood DJ, 2007, MED J AUSTRALIA, V187, pS35, DOI 10.5694/j.1326-5377.2007.tb01334.x
   Robinson Jo, 2015, Shanghai Arch Psychiatry, V27, P27, DOI 10.11919/j.issn.1002-0829.214133
   SABBATH JC, 1969, J AMER ACAD CHILD PS, V8, P272, DOI 10.1016/S0002-7138(09)61906-3
   Sawhney Ramit., 2018, P ACL 2018 STUDENT R, P91, DOI [DOI 10.18653/V1/P18-3013, 10.18653/v1/p18-3013]
   Sawhney Ramit., 2018, P 9 WORKSHOP COMPUTA, P167, DOI [DOI 10.18653/V1/W18-6223, 10.18653/v1/w18-6223]
   Shing H-C, 2020, P 58 ANN M ASS COMP, P8124
   Sinha PP, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P941, DOI 10.1145/3357384.3358060
   Steele C.M., 1988, ADV EXPT SOCIAL PSYC, V21, P261, DOI [10.1016/S0065-2601(08)60229-4, DOI 10.1016/S0065-2601(08)60229-4, 10.1016/S0065-2601, DOI 10.1016/S0065-2601]
   Sueki H, 2015, J AFFECT DISORDERS, V170, P155, DOI 10.1016/j.jad.2014.08.047
   Tadesse MM, 2020, ALGORITHMS, V13, DOI 10.3390/a13010007
   Velickovic Petar, 2018, INT C LEARN REPR
   Vioulès MJ, 2018, IBM J RES DEV, V62, DOI 10.1147/JRD.2017.2768678
   Xiao H., 2018, bert-as-service
   Xue F, 2020, IEEE T MULTIMEDIA, V22, P2098, DOI 10.1109/TMM.2019.2951194
   Zachrisson HD, 2006, BMC PUBLIC HEALTH, V6, DOI 10.1186/1471-2458-6-34
   Zha ZJ, 2020, IEEE T MULTIMEDIA, V22, P1836, DOI 10.1109/TMM.2020.2972168
   Zhang L, 2015, LECT NOTES COMPUT SC, V8944, P549, DOI 10.1007/978-3-319-15554-8_45
NR 81
TC 30
Z9 30
U1 14
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 87
EP 102
DI 10.1109/TMM.2020.3046867
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Du, X
   Pun, CM
AF Du, Xia
   Pun, Chi-Man
TI Robust Audio Patch Attacks Using Physical Sample Simulation and
   Adversarial Patch Noise Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Perturbation methods; Speech recognition; Robustness; Signal to noise
   ratio; Training; Detectors; Voice activity detection; Adversarial robust
   attacks; audio attack; computer vision; ensemble method
AB Deep neural network (DNNs) based Automatic Speech Recognition (ASR) systems are known vulnerable to adversarial attacks that are maliciously implemented by adding small but powerful distortions to the original audio input. However, most existing methods that generate audio adversarial examples targeting ASR models cannot achieve successful robust attacks against defense methods. This paper proposes a novel framework for robust audio patch attacks using Physical Sample Simulation (PSS) and Adversarial Patch Noise Generation (APNG). First, the proposed PSS simulated real-audio with selected room impulse response for training the adversarial patches. Second, the proposed APNG generates the imperceptible audio adversarial patch examples using the voice activity detector to hide the adversarial patch noise into the non-silent locations of the input audio. Furthermore, the design Sounds Pressure Level-based adaptive noise minimization algorithm helps us further reduce the perturbation during the attack. The experimental results show that our proposed method can achieve the highest attack success rates and SNRs in various cases, comparing with other state-of-the-art attacks.
C1 [Du, Xia; Pun, Chi-Man] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau
RP Pun, CM (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM yb77479@connect.um.edu.mo; cmpun@umac.mo
RI Pun, Chi Man/GRJ-3703-2022
OI Du, Xia/0000-0002-6298-846X; Pun, Chi-Man/0000-0003-1788-3746
FU University of Macau [MYRG2018-00035-FST, MYRG201900086-FST]; Science and
   Technology Development Fund, Macau SAR [0034/2019/AMJ, 0087/2020/A2]
FX This work was partly supported by the University of Macau (File no.
   MYRG2018-00035-FST, and MYRG201900086-FST); and in part by the Science
   and Technology Development Fund, Macau SAR (File no. 0034/2019/AMJ, and
   0087/2020/A2). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Liangliang Cao.
CR Abdullah H, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23362
   Alzantot Moustafa, 2018, PROC INT C NEURAL IN, P1
   Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Amodei D, 2016, PR MACH LEARN RES, V48
   Ariav I, 2019, IEEE J-STSP, V13, P265, DOI 10.1109/JSTSP.2019.2901195
   Athalye A, 2018, Arxiv, DOI arXiv:1707.07397
   Brown, 2017, ADVERSARIAL PATCH
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009
   Cisse M. M., 2017, ARXIV170705373, P6977
   Croce F, 2019, IEEE I CONF COMP VIS, P4723, DOI 10.1109/ICCV.2019.00482
   Du X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1634, DOI 10.1145/3394171.3413808
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Gilg V., 2020, PROC INT WORKSHOP AC, P131
   Gong Y., 2018, PROC DYN NOVEL ADV M, P1
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Graves  A., 2006, P 23 INT C MACH LEAR, P369, DOI DOI 10.1145/1143844.1143891
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   HAIGH JA, 1993, TENCON'93: 1993 IEEE REGION 10 CONFERENCE ON COMPUTER, COMMUNICATION, CONTROL AND POWER ENGINEERING, VOL 3, P321, DOI 10.1109/TENCON.1993.327987
   Hannun A, 2014, Arxiv, DOI arXiv:1412.5567
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang T, 2020, INFORM SCIENCES, V531, P159, DOI 10.1016/j.ins.2020.03.066
   Jeub M., 2010, PROC INT C ACOUST, P1
   Jeub M, 2009, 2009 16TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, VOLS 1 AND 2, P550
   Jiang LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P864, DOI 10.1145/3343031.3351088
   Junqua J.-C., 1991, EUROSPEECH 91. 2nd European Conference on Speech Communication and Technology Proceedings, P1371
   Karmon D, 2018, PR MACH LEARN RES, V80
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kinoshita T, 2013, 2013 9TH INTERNATIONAL WORKSHOP ON ELECTROMAGNETIC COMPATIBILITY OF INTEGRATED CIRCUITS (EMC COMPO 2013), P1, DOI 10.1109/EMCCompo.2013.6735162
   Komkov S, 2019, Arxiv, DOI arXiv:1908.08705
   Kurakin Alexey, 2017, INT C LEARN REPR
   Kwon H, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2521, DOI 10.1145/3319535.3363246
   Kwon H, 2020, IEEE T INF FOREN SEC, V15, P526, DOI 10.1109/TIFS.2019.2925452
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee B., 2007, MILCOM 2007-IEEE Military Communications Conference, P1
   Li JCB, 2019, PR MACH LEARN RES, V97
   Li TL, 2021, INFORM SCIENCES, V547, P568, DOI 10.1016/j.ins.2020.08.043
   Moattar M. H., 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P2549
   Nakamura Satoshi, 2000, LREC
   Peddinti V, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2440
   Qin Y, 2019, PR MACH LEARN RES, V97
   Rajaratnam K, 2018, IEEE INT SYMP SIGNAL, P197, DOI 10.1109/ISSPIT.2018.8642623
   Sakhnov K., 2009, IAENG INT J COMPUT S, V36, P1
   Scheibler R, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P351, DOI 10.1109/ICASSP.2018.8461310
   Sch”nherr L, 2020, Arxiv, DOI arXiv:1908.01551
   Shete D., 2014, IOSR-JVSP, V4, P1, DOI DOI 10.9790/4200-04110105
   Taori Rohan, 2019, 2019 IEEE Security and Privacy Workshops (SPW). Proceedings, P15, DOI 10.1109/SPW.2019.00016
   Wang J., 2021, IEEE T MULTIMEDIA, P1
   Xie Y, 2019, IEEE-ACM T AUDIO SPE, V27, P1675, DOI 10.1109/TASLP.2019.2925934
   Yakura H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5334
   Yang Zhuolin, 2019, 7 INT C LEARN REPR I
   Yusuf B, 2019, IEEE-ACM T AUDIO SPE, V27, P1126, DOI 10.1109/TASLP.2019.2911164
NR 53
TC 4
Z9 4
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4381
EP 4393
DI 10.1109/TMM.2021.3116426
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000002
DA 2024-07-18
ER

PT J
AU Huang, QB
   Liang, Y
   Wei, JL
   Yi, C
   Liang, HY
   Leung, HF
   Li, Q
AF Huang, Qingbao
   Liang, Yu
   Wei, Jielong
   Yi, Cai
   Liang, Hanyu
   Leung, Ho-fung
   Li, Qing
TI Image Difference Captioning With Instance-Level Fine-Grained Feature
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Semantics; Visualization; Task analysis; Image color
   analysis; Proposals; Interference; Image difference captioning; change
   captioning; change description; instance-level; fine-grained feature
   extraction; similarity-based difference finding
AB The task of image difference captioning aims at locating changed objects in similar image pairs and describing the difference with natural language. The key challenges of this task are to comprehend the context of image pairs sufficiently and locate the changed objects accurately in the presence of viewpoint change. Previous studies focus on pixel-level image features, neglecting rich explicit features of objects in an image pair which are beneficial to generate a fine-grained difference caption. Additionally, existing generative models suffer from accurately locate the differences in the interference of viewpoint change. To address these issues, we propose an Instance-Level Fine-Grained Difference Captioning (IFDC) model, which consists of a fine-grained feature extraction module, a multi-round feature fusion module, a similarity-based difference finding module, and a difference captioning module. To describe the changed objects comprehensively, we extract the fine-grained features, i.e., visual features, semantic features, and positional features at instance-level, as the objects' representation. To enhance the model's immunity to viewpoint change, we design a similarity-based difference finding module to locate the changed objects accurately. Extensive experiments show that our IFDC model achieves comparable performance with the state-of-the-art models on the datasets of CLEVR-Change and Spot-the-Diff, thus verifying the effectiveness of our proposed model. Our source code is available at https://github.com/VISLANG-Lab/IFDC.
C1 [Huang, Qingbao; Liang, Yu; Wei, Jielong; Liang, Hanyu] Guangxi Univ, Sch Elect Engn, Nanning 530004, Peoples R China.
   [Huang, Qingbao; Yi, Cai] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
   [Huang, Qingbao] Guangxi Univ, Inst Artificial Intelligence, Nanning 530004, Peoples R China.
   [Huang, Qingbao; Yi, Cai] MOE China, Key Lab Big Data & Intelligent Robot SCUT, Guangzhou 510006, Peoples R China.
   [Leung, Ho-fung] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
C3 Guangxi University; South China University of Technology; Guangxi
   University; Chinese University of Hong Kong; Hong Kong Polytechnic
   University
RP Yi, C (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
EM qbhuang@gxu.edu.cn; 1363738990@qq.com; 1712306010@st.gxu.edu.cn;
   ycai@scut.edu.cn; hanyu_liang@163.com; lhf@cuhk.edu.hk;
   csqli@comp.polyu.edu.hk
RI Leung, Ho-fung/F-2878-2011; Li, Qing/JMH-1365-2023
OI Leung, Ho-fung/0000-0003-4914-2934; Li, Qing/0000-0003-3370-471X; Huang,
   Qingbao/0000-0001-7691-347X
FU National Natural Science Foundation of China [62076100, 51767005];
   Fundamental Research Funds for the Central Universities, SCUT [D2210010,
   D2200150, D2201300]; Science and Technology Planning Project of
   Guangdong Province [2017B050506004]; Science and Technology Programs of
   Guangzhou [201704030076, 201707010223, 201802010027, 201902010046]; Hong
   Kong Research grants Council, China [PolyU1121417, C1031-18G]; Hong Kong
   Polytechnic University, China [1.9B0V]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076100 and 51767005, and the
   Collaborative Research grants from the Fundamental Research Funds for
   the Central Universities, SCUT (D2210010, D2200150, and D2201300), in
   part by the Science and Technology Planning Project of Guangdong
   Province under Grant 2017B050506004, in part by the Science and
   Technology Programs of Guangzhou under Grants 201704030076,
   201707010223, 201802010027, and 201902010046, and in part by the Hong
   Kong Research grants Council, China (PolyU1121417, C1031-18G), and in
   part by InternalResearch grant from the Hong Kong Polytechnic
   University, China (1.9B0V).
CR Aafaq N, 2019, PROC CVPR IEEE, P12479, DOI 10.1109/CVPR.2019.01277
   Acheampong Kingsley Nketia, 2019, P INT C BIG DAT INT, P51
   Alcantarilla PF, 2018, AUTON ROBOT, V42, P1301, DOI 10.1007/s10514-018-9734-5
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen WZ, 2017, IEEE IJCNN, P1403, DOI 10.1109/IJCNN.2017.7966017
   Dan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10052, DOI 10.1109/CVPR42600.2020.01007
   Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gilton Davis, 2020, ARXIV200312633
   Gueguen L, 2015, PROC CVPR IEEE, P1321, DOI 10.1109/CVPR.2015.7298737
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4989
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Guo LT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P765, DOI 10.1145/3343031.3350943
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Huang T.-H., 2016, NAACL HLT, P1233
   Jhamtani H, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4024
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D. P., 2014, arXiv
   Li JN, 2020, IEEE T MULTIMEDIA, V22, P554, DOI 10.1109/TMM.2019.2930041
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Liao B, 2020, IEEE ACCESS, V8, P79754, DOI 10.1109/ACCESS.2020.2990539
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu RT, 2019, PROC CVPR IEEE, P4180, DOI 10.1109/CVPR.2019.00431
   Liu Y, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3730
   Liu YF, 2020, AAAI CONF ARTIF INTE, V34, P11645
   Liu Z, 2018, IEEE T IMAGE PROCESS, V27, P1822, DOI 10.1109/TIP.2017.2784560
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Oluwasanmi A, 2019, IEEE ACCESS, V7, P175929, DOI 10.1109/ACCESS.2019.2957513
   Oluwasanmi A, 2019, IEEE ACCESS, V7, P106772, DOI 10.1109/ACCESS.2019.2931223
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park DH, 2019, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2019.00472
   Patriarche J, 2004, J DIGIT IMAGING, V17, P158, DOI 10.1007/s10278-004-1010-x
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi MS, 2020, IEEE T CIRC SYST VID, V30, P2617, DOI 10.1109/TCSVT.2019.2921655
   Qiu Y, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20174761
   Qiu Y, 2020, IEEE ROBOT AUTOM LET, V5, P4743, DOI 10.1109/LRA.2020.3003290
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sakurada K., 2015, P BMCV SWANS UK 7 10, V61, P1, DOI [DOI 10.5244/C.29.61, 10.5244/C.29.61]
   Su Weijie, 2020, INT C LEARN REPR
   Subudhi BN, 2020, IEEE T MULTIMEDIA, V22, P912, DOI 10.1109/TMM.2019.2938342
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xiangxi Shi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P574, DOI 10.1007/978-3-030-58568-6_34
   Xu Jiacheng, 2020, P 58 ANN M ASS COMP, P5021, DOI 10.18653/v1/2020.acl-main.451
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu Shuangjie, 2019, ARXIV191006426
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang PC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5356
   Yang X, 2019, IEEE I CONF COMP VIS, P4249, DOI 10.1109/ICCV.2019.00435
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Zhengcong Fei, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P100, DOI 10.1145/3372278.3390679
NR 60
TC 16
Z9 16
U1 7
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2004
EP 2017
DI 10.1109/TMM.2021.3074803
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200018
DA 2024-07-18
ER

PT J
AU Huang, Y
   Wu, Q
   Xu, JS
   Zhong, Y
   Zhang, P
   Zhang, ZX
AF Huang, Yan
   Wu, Qiang
   Xu, Jingsong
   Zhong, Yi
   Zhang, Peng
   Zhang, Zhaoxiang
TI Alleviating Modality Bias Training for Infrared-Visible Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Gallium nitride; Cameras; Task analysis; Network architecture;
   Lighting; Data models; Cross modality; modality bias training; person
   re-identification
AB The task of infrared-visible person re-identification (IV-reID) is to recognize people across two modalities (i.e., RGB and IR). Existing cutting-edge approaches normally use a pair of images that have the same IDs (i.e., ID-tied cross-modality image pairs) and input them into an ImageNet-trained ResNet50. The ResNet50 backbone model can learn shared features across modalities to tolerate modality discrepancies between RGB and IR. This work will unveil a Modality Bias Training (MBT) problem that is less discussed in IV-reID, which will demonstrate that MBT significantly compromises the performance of IV-reID. Due to MBT, IR information can be overwhelmed by RGB information during training when the ResNet50 model is pretrained based on a large amount of RGB images from ImageNet. Thus, the trained models are more inclined to RGB information. Accordingly, the cross-modality generalization ability of the model is also compromised. To tackle this issue, we present a Dual-level Learning Strategy (DLS) that 1) enforces the focus of the network on ID-exclusive (rather than ID-tied) labels of cross-modality image pairs to mitigate the problem of MBT and 2) introduces third modality data that contain both RGB and IR information to further prevent the information from the IR modality from being overwhelmed during training. Our third modality images are generated by a generative adversarial network. A dynamic ID-exclusive Smooth (dIDeS) label is proposed for the generated third modality data. In experiments, comprehensive experiments are carried out to demonstrate the success of DLS in tackling the MBT issue exposed in IV-reID.
C1 [Huang, Yan; Wu, Qiang; Xu, Jingsong; Zhang, Peng] Univ Technol Sydney, Global Big Data Technol Ctr GBDTC, Sch Elect & Data Engn, Ultimo, NSW 2007, Australia.
   [Zhong, Yi] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Zhang, Zhaoxiang] Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, CAS Ctr Excellence Brain Sci & IntelligenceTechno, Inst Automat, Beijing 100190, Peoples R China.
C3 University of Technology Sydney; Beijing Institute of Technology;
   Chinese Academy of Sciences; Institute of Automation, CAS
RP Zhong, Y (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
EM yan.huang-3@student.uts.edu.au; Qiang.Wu@uts.edu.au;
   jingsong.xu@uts.edu.au; yi.zhong@bit.edu.cn;
   Peng.Zhang-2@student.uts.edu.au; zhaoxiang.zhang@ia.ac.cn
RI zhong, yi/IQS-2997-2023; Huang, Yan/N-3447-2018
OI zhong, yi/0000-0002-9309-3407; Wu, Qiang/0000-0001-5641-2483; Huang,
   Yan/0000-0002-1363-5318; Zhang, Peng/0000-0001-6794-7352
FU Australian Government Research Training Program Scholarship; Beijing
   Institute of Technology Research Fund Program for Young Scholars
FX This work was supported in part by the Australian Government Research
   Training Program Scholarship and in part by the Beijing Institute of
   Technology Research Fund Program for Young Scholars. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Fatih Porikli.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Hao Y, 2019, AAAI CONF ARTIF INTE, P8385
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang GL, 2017, IEEE ICC
   Huang Y., 2019, PROC INT JOINT C NEU, P1
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Huang Y, 2017, NEUROCOMPUTING, V241, P191, DOI 10.1016/j.neucom.2017.02.055
   Gulrajani I, 2017, ADV NEUR IN, V30
   Kansal K, 2020, IEEE T CIRC SYST VID, V30, P3422, DOI 10.1109/TCSVT.2019.2963721
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Nguyen DT, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17030605
   Radford A., 2015, ARXIV
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Sun XX, 2019, PROC CVPR IEEE, P608, DOI 10.1109/CVPR.2019.00070
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang GA, 2020, AAAI CONF ARTIF INTE, V34, P12144
   Wang GA, 2019, IEEE I CONF COMP VIS, P3622, DOI 10.1109/ICCV.2019.00372
   Wang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4973
   Wang Z, 2020, IEEE T IMAGE PROCESS, V29, P2013, DOI 10.1109/TIP.2019.2946975
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang ZX, 2019, PROC CVPR IEEE, P618, DOI 10.1109/CVPR.2019.00071
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu AC, 2017, IEEE I CONF COMP VIS, P5390, DOI 10.1109/ICCV.2017.575
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yao Y., 2019, P IEEE EUR C COMP VI, P775
   Ye M, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1092
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Ye M, 2020, IEEE T INF FOREN SEC, V15, P407, DOI 10.1109/TIFS.2019.2921454
   Ye M, 2018, AAAI CONF ARTIF INTE, P7501
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang P, 2021, IEEE T MULTIMEDIA, V23, P3562, DOI 10.1109/TMM.2020.3028461
   Zhang P, 2020, IEEE T CIRC SYST VID, V30, P4554, DOI 10.1109/TCSVT.2019.2939564
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Liang, 2016, arXiv preprint arXiv
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 43
TC 30
Z9 32
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1570
EP 1582
DI 10.1109/TMM.2021.3067760
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200025
DA 2024-07-18
ER

PT J
AU Ma, D
   Zhang, F
   Bull, DR
AF Ma, Di
   Zhang, Fan
   Bull, David R.
TI BVI-DVC: A Training Database for Deep Video Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Databases; Training; Spatial resolution; Encoding; Spatial databases;
   Standards; Video compression; Video database; BVI-DVC; CNN training;
   deep learning; video compression
ID QUALITY
AB Deep learning methods are increasingly being applied in the optimisation of video compression algorithms and can achieve significantly enhanced coding gains, compared to conventional approaches. Such approaches often employ Convolutional Neural Networks (CNNs) which are trained on databases with relatively limited content coverage. In this paper, a new extensive and representative video database, BVI-DVC,is presented for training CNN-based video compression systems, with specific emphasis on machine learning tools that enhance conventional coding architectures, including spatial resolution and bit depth up-sampling, post-processing and in-loop filtering. BVI-DVC contains 800 sequences at various spatial resolutions from 270p to 2160p and has been evaluated on ten existing network architectures for four different coding tools. Experimental results show that this database produces significant improvements in terms of coding gains over five existing (commonly used) image/video training databases under the same training and evaluation configurations. The overall additional coding improvements by using the proposed database for all tested coding modules and CNN architectures are up to 10.3% based on the assessment of PSNR and 8.1% based on VMAF.
C1 [Ma, Di; Zhang, Fan; Bull, David R.] Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
C3 University of Bristol
RP Ma, D (corresponding author), Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
EM di.ma@bristol.ac.uk; fan.zhang@bristol.ac.uk; dave.bull@bristol.ac.uk
OI Bull, David/0000-0001-7634-190X
FU U.K. EPSRC [EP/L016656/1, EP/M000885/1]; NVIDIA GPU Seeding grants;
   EPSRC [EP/M000885/1] Funding Source: UKRI
FX This work was supported in part by U.K. EPSRC under Grants EP/L016656/1
   and EP/M000885/1 and in part by NVIDIA GPU Seeding grants.
CR Afonso M, 2019, IEEE T CIRC SYST VID, V29, P275, DOI 10.1109/TCSVT.2018.2878952
   Afonso M, 2017, IEEE IMAGE PROC, P3011, DOI 10.1109/ICIP.2017.8296835
   Agustsson E., 2020, P IEEE CVF C COMP VI, P8503
   Agustsson E, 2017, IEEE COMPUT SOC CONF, P1122, DOI 10.1109/CVPRW.2017.150
   Alam MM, 2015, PROC SPIE, V9599, DOI 10.1117/12.2188913
   [Anonymous], AOMEDIA VIDEO 1 AV1
   [Anonymous], DAREFUL COMPLETELY F
   [Anonymous], 2015, H265 ITUT
   [Anonymous], HARMONIC INC 4 K DEM
   [Anonymous], VIDEVO FREE STOCK VI
   [Anonymous], M MARTINEZ FREE 4K S
   [Anonymous], IRIS32 FREE 4 K UHD
   Balle J, 2017, 5 INT C LEARN REPR I
   Bampis CG, 2017, IEEE T IMAGE PROCESS, V26, P5217, DOI 10.1109/TIP.2017.2729891
   Bampis Z. Li, 2018, arXiv
   Bjontegaard G., 2001, Document VCEG-M33
   Bossen F., 2019, Document JVET-N1010
   Bross B., 2020, JVET-S2001
   Bull D, 2018, JVET M NO JVETJ0031
   Bull D.R, 2021, Intelligent image and video compression: communicating pictures
   Carreira J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1808.01340
   Carreira J, 2019, Arxiv, DOI arXiv:1907.06987
   Cheon M, 2018, IEEE T CIRC SYST VID, V28, P1467, DOI 10.1109/TCSVT.2017.2683504
   Ding DD, 2021, P IEEE, V109, P1494, DOI 10.1109/JPROC.2021.3059994
   Djelouah A, 2019, IEEE I CONF COMP VIS, P6430, DOI 10.1109/ICCV.2019.00652
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Guo Lu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P456, DOI 10.1007/978-3-030-58536-5_27
   Habibian A, 2019, IEEE I CONF COMP VIS, P7032, DOI 10.1109/ICCV.2019.00713
   Hashimoto T., 2018, JTC1SC29WG11 ISO IEC
   He DL, 2021, PROC CVPR IEEE, P14766, DOI 10.1109/CVPR46437.2021.01453
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu ZH, 2021, PROC CVPR IEEE, P1502, DOI 10.1109/CVPR46437.2021.00155
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Jimbo S, 2018, IEEE GLOB CONF CONSU, P726, DOI 10.1109/GCCE.2018.8574691
   Katsavounidis I., 2015, CHIMERA VIDEO SEQUEN
   Katsenou AV, 2021, IEEE T MULTIMEDIA, V23, P26, DOI 10.1109/TMM.2020.2976591
   Katsenou AV, 2019, IEEE IMAGE PROC, P4145, DOI [10.1109/ICIP.2019.8803523, 10.1109/icip.2019.8803523]
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Keimel C., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P390, DOI 10.1109/MMSP.2010.5662052
   Keimel C, 2012, INT WORK QUAL MULTIM, P97, DOI 10.1109/QoMEX.2012.6263865
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li C, 2017, IEEE IMAGE PROC, P4577, DOI 10.1109/ICIP.2017.8297149
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921877
   Li Z., 2016, NETFLIX TECH BLOG, V6
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Lin JP, 2020, PROC CVPR IEEE, P3543, DOI 10.1109/CVPR42600.2020.00360
   Lin JP, 2019, IEEE T CIRC SYST VID, V29, P3701, DOI 10.1109/TCSVT.2018.2884203
   Lin JY, 2015, J VIS COMMUN IMAGE R, V30, P1, DOI 10.1016/j.jvcir.2015.02.012
   Liu D, 2017, IEEE I CONF COMP VIS, P2526, DOI 10.1109/ICCV.2017.274
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu D, 2020, IEEE T CIRC SYST VID, V30, P1267, DOI 10.1109/TCSVT.2019.2945057
   Liu Jerry, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P453, DOI 10.1007/978-3-030-58520-4_27
   Liu S., 2020, JVET M NO JVET T0041
   Liu S., 2020, JVET M NO JVET T0011
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Ma CY, 2018, IEEE IMAGE PROC, P1772, DOI 10.1109/ICIP.2018.8451166
   Ma D, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102865
   Ma D, 2020, Arxiv, DOI arXiv:2011.09190
   Ma D, 2021, IEEE J-STSP, V15, P378, DOI 10.1109/JSTSP.2020.3043064
   Ma D, 2020, PROC SPIE, V11510, DOI 10.1117/12.2567633
   Ma D, 2019, PROC SPIE, V11137, DOI 10.1117/12.2530688
   Ma D, 2019, IEEE IMAGE PROC, P1094, DOI [10.1109/icip.2019.8803798, 10.1109/ICIP.2019.8803798]
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mackin A, 2019, IEEE T MULTIMEDIA, V21, P1499, DOI 10.1109/TMM.2018.2880603
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Monfort M, 2020, IEEE T PATTERN ANAL, V42, P502, DOI 10.1109/TPAMI.2019.2901464
   Moss FM, 2016, IEEE T CIRC SYST VID, V26, P1977, DOI 10.1109/TCSVT.2015.2461971
   Nah S, 2019, IEEE COMPUT SOC CONF, P1996, DOI 10.1109/CVPRW.2019.00251
   Nah S, 2019, IEEE COMPUT SOC CONF, P1974, DOI 10.1109/CVPRW.2019.00249
   Papadopoulos MA, 2015, IEEE IMAGE PROC, P2781, DOI 10.1109/ICIP.2015.7351309
   Puri S, 2017, EUR SIGNAL PR CONF, P798, DOI 10.23919/EUSIPCO.2017.8081317
   Rippel O, 2019, IEEE I CONF COMP VIS, P3453, DOI 10.1109/ICCV.2019.00355
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Smaira L., 2020, arXiv, DOI DOI 10.48550/ARXIV.2010.10864
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Song R, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Szeto R, 2020, IEEE T PATTERN ANAL, V42, P1053, DOI 10.1109/TPAMI.2019.2951667
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Ultra Video Group, About us
   Wan S., 2019, JVET M NO JVET O0079
   Wang DZ, 2019, IEEE IMAGE PROC, P2671, DOI [10.1109/ICIP.2019.8803253, 10.1109/icip.2019.8803253]
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang HQ, 2017, J VIS COMMUN IMAGE R, V46, P292, DOI 10.1016/j.jvcir.2017.04.009
   Wang JY, 2020, Arxiv, DOI arXiv:2008.00499
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang YL, 2019, IEEE INT WORKSH MULT, DOI 10.1109/mmsp.2019.8901772
   Wang YB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yeh CH, 2018, IEEE ACCESS, V6, P50087, DOI 10.1109/ACCESS.2018.2867342
   Zhang F., 2021, SIGNAL PROCESS-IMAGE
   Zhang F, 2021, IEEE MULTIMEDIA, V28, P74, DOI 10.1109/MMUL.2021.3052437
   Zhang F, 2019, IEEE IMAGE PROC, P1720, DOI [10.1109/ICIP.2019.8803185, 10.1109/icip.2019.8803185]
   Zhang F, 2018, IEEE T MULTIMEDIA, V20, P2620, DOI 10.1109/TMM.2018.2817070
   Zhang F, 2011, IEEE J-STSP, V5, P1378, DOI 10.1109/JSTSP.2011.2165201
   Zhang Xi, 2021, P IEEE CVF C COMP VI, P13354, DOI DOI 10.1109/CVPR46437.2021.01315
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhang Y, 2020, INFORM SCIENCES, V506, P395, DOI 10.1016/j.ins.2019.07.096
   Zhao H., 2019, IEEE Access, V8, P920
   Zhao L, 2019, IEEE T IMAGE PROCESS, V28, P4832, DOI 10.1109/TIP.2019.2913545
   Zhao ZH, 2019, IEEE T CIRC SYST VID, V29, P3291, DOI 10.1109/TCSVT.2018.2876399
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
   Zhu XX, 2016, INT J COMPUT VISION, V119, P76, DOI 10.1007/s11263-015-0812-2
NR 115
TC 29
Z9 29
U1 3
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3847
EP 3858
DI 10.1109/TMM.2021.3108943
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Maniotis, P
   Thomos, N
AF Maniotis, Pantelis
   Thomos, Nikolaos
TI Viewport-Aware Deep Reinforcement Learning Approach for 360° Video
   Caching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360 degrees video; deep reinforcement learning; tile-encoding;
   viewport-aware caching
ID MOBILE EDGE; NETWORKS; DELIVERY; PERFORMANCE
AB 360 degrees video is an essential component of VR/AR/MR systems that provides immersive experience to the users. However, 360 degrees video is associated with high bandwidth requirements. The required bandwidth can be reduced by exploiting the fact that users are interested in viewing only a part of the video scene and that users request viewports that overlap with each other. Motivated by the findings of our recent works where the benefits of caching video tiles at edge servers instead of caching entire 360 degrees videos were shown, in this paper, we introduce the concept of virtual viewports that have the same number of tiles with the original viewports. The tiles forming these viewports are the most popular ones for each video and are determined by the users' requests. Then, we propose a reactive caching scheme that assumes unknown videos' and viewports' popularity. Our scheme determines which videos to cache as well as which is the optimal virtual viewport per video. Virtual viewports permit to lower the dimensionality of the cache optimization problem. To solve the problem, we first formulate the content placement of 360 degrees videos in edge cache networks as a Markov Decision Process (MDP), and then we determine the optimal caching placement using the Deep Q-Network (DQN) algorithm. The proposed solution aims at maximizing the overall quality of the 360 degrees videos delivered to the end-users by caching the most popular 360 degrees videos at base quality along with a virtual viewport in high quality. We extensively evaluate the performance of the proposed system and compare it with that of known systems such as Least Frequently Used (LFU), Least Recently Used (LRU), First In First Out (FIFO), over both synthetic and real 360 degrees video traces. The results reveal the large benefits coming from reactive caching of virtual viewports instead of the original ones in terms of the overall quality of the rendered viewports, the cache hit ratio, and the servicing cost.
C1 [Maniotis, Pantelis; Thomos, Nikolaos] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 University of Essex
RP Thomos, N (corresponding author), Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
EM p.maniotis@essex.ac.uk; nthomos@essex.ac.uk
RI Thomos, Nikolaos/AAU-2328-2020
OI Thomos, Nikolaos/0000-0001-7266-2642
CR Aladagli AD, 2017, 2017 INTERNATIONAL CONFERENCE ON 3D IMMERSION (IC3D)
   [Anonymous], 1998, TEACHING MATH APPL
   Assens M., 2017, P IEEE INT C COMP VI
   Bao Yanan., 2016, P INT C NETWORK PROT, P1
   BELLMAN R, 1966, SCIENCE, V153, P34, DOI 10.1126/science.153.3731.34
   Bertsekas D. P., 2011, DYNAMIC PROGRAMMING, Vii
   Blasco P, 2014, IEEE INT SYMP INFO, P51, DOI 10.1109/ISIT.2014.6874793
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Duanmu F., 2018, 2018 IEEE International Conference on Multimedia and Expo (ICME), P1
   Dulac-Arnold G., 2015, ARXIV151207679
   Ge C, 2018, IEEE J SEL AREA COMM, V36, P1816, DOI 10.1109/JSAC.2018.2845000
   Hosseini M, 2017, P IEEE VIRT REAL ANN, P423, DOI 10.1109/VR.2017.7892357
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li HJ, 2017, ICCAD-IEEE ACM INT, P847, DOI 10.1109/ICCAD.2017.8203866
   Liu D, 2016, IEEE COMMUN MAG, V54, P22, DOI 10.1109/MCOM.2016.7565183
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Lo WC, 2017, ASIA-PAC NETW OPER M, P205, DOI 10.1109/APNOMS.2017.8094203
   Lu Q., 2019, ABS191100105, P1
   Luong NC, 2019, IEEE COMMUN SURV TUT, V21, P3133, DOI 10.1109/COMST.2019.2916583
   Mahzari A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P173, DOI 10.1145/3240508.3240680
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Maniotis P, 2019, IEEE INT WORKSH MULT
   Müller S, 2017, IEEE T WIREL COMMUN, V16, P1024, DOI 10.1109/TWC.2016.2636139
   Nisioti E, 2019, IEEE J SEL AREA COMM, V37, P2211, DOI 10.1109/JSAC.2019.2933887
   Papaioannou G, 2019, PROCEEDINGS OF THE 2019 THE TWENTIETH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING (MOBIHOC '19), P171, DOI 10.1145/3323679.3326515
   Poderys J, 2018, IEEE ACCESS, V6, P8630, DOI 10.1109/ACCESS.2018.2809490
   Poularakis K, 2014, IEEE INFOCOM SER, P1087, DOI 10.1109/INFOCOM.2014.6848038
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Rossi S., 2017, 2017 IEEE 19 INT WOR, P1, DOI DOI 10.1109/MMSP.2017.8122230
   Sermpezis P, 2018, IEEE J SEL AREA COMM, V36, P1300, DOI 10.1109/JSAC.2018.2844983
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Skupin R, 2016, IEEE INT SYM MULTIM, P399, DOI [10.1109/ISM.2016.0089, 10.1109/ISM.2016.137]
   Su Z, 2017, IEEE T MULTIMEDIA, V19, P2210, DOI 10.1109/TMM.2017.2733338
   Sun LY, 2019, IEEE J EM SEL TOP C, V9, P43, DOI 10.1109/JETCAS.2019.2898877
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Vu TX, 2018, IEEE T WIREL COMMUN, V17, P2827, DOI 10.1109/TWC.2018.2803816
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wei Y., 2018, 2018 IEEE INT C COMM, P1, DOI [DOI 10.1109/RADAR.2018.8557307, 10.1109/ICCW.2018.8403711, DOI 10.1109/ICCW.2018.8403711]
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Zhang S, 2017, IEEE T VEH TECHNOL, V66, P11264, DOI 10.1109/TVT.2017.2724547
   Zhong C, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/ciss.2018.8362276
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 43
TC 20
Z9 21
U1 4
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 386
EP 399
DI 10.1109/TMM.2021.3052339
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300030
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Mou, C
   Zhang, J
   Fan, XP
   Liu, HF
   Wang, RG
AF Mou, Chong
   Zhang, Jian
   Fan, Xiaopeng
   Liu, Hangfan
   Wang, Ronggang
TI COLA-Net: Collaborative Attention Network for Image Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Task analysis; Collaboration; Image denoising; Finite
   element analysis; Computer architecture; Training; Deep neural network;
   feature fusion; image denoising; non-local attention; image restoration
ID SPARSE REPRESENTATION; QUALITY ASSESSMENT
AB Local and non-local attention-based methods have been well studied in various image restoration tasks while leading to promising performance. However, most of the existing methods solely focus on one type of attention mechanism (local or non-local). Furthermore, by exploiting the self-similarity of natural images, existing pixel-wise non-local attention operations tend to give rise to deviations in the process of characterizing long-range dependence due to image degeneration. To overcome these problems, in this paper we propose a novel collaborative attention network (COLA-Net) for image restoration, as the first attempt to combine local and non-local attention mechanisms to restore image content in the areas with complex textures and with highly repetitive details respectively. In addition, an effective and robust patch-wise non-local attention model is developed to capture long-range feature correspondences through 3D patches. Extensive experiments on synthetic image denoising, real image denoising and compression artifact reduction tasks demonstrate that our proposed COLA-Net is able to achieve state-of-the-art performance in both peak signal-to-noise ratio and visual perception, while maintaining an attractive computational complexity. The source code is available on https://github.com/MC-E/COLA-Net.
C1 [Mou, Chong; Zhang, Jian; Wang, Ronggang] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhang, Jian] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Fan, Xiaopeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Peoples R China.
   [Liu, Hangfan] Univ Penn, Ctr Biomed Image Comp & Analyt, Philadelphia, PA 19104 USA.
C3 Peking University; Peng Cheng Laboratory; Harbin Institute of
   Technology; University of Pennsylvania
RP Zhang, J (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM eechongm@gmail.com; zhangjian.sz@pku.edu.cn; fxp@hit.edu.cn;
   hfliu@upenn.edu; rgwang@pkusz.edu.cn
RI Liu, Hangfan/Y-7653-2019
OI Liu, Hangfan/0000-0002-1207-7713; Wang, Ronggang/0000-0003-0873-0465;
   Zhang, Jian/0000-0001-5486-3125
FU Key-Area Research, and Development Program of Guangdong Province
   [2019B121204008]; National Natural Science Foundation of China
   [61902009]; Shenzhen Research Project [201806080921419290]
FX This work was supported in part by the Key-Area Research, and
   Development Program of Guangdong Province under Grant 2019B121204008, in
   part by the National Natural Science Foundation of China under Grant
   61902009, and in part by Shenzhen Research Project under Grant
   201806080921419290. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Joao Ascenso.
CR Anaya J, 2018, J VIS COMMUN IMAGE R, V51, P144, DOI 10.1016/j.jvcir.2018.01.012
   Anwar S, 2019, IEEE I CONF COMP VIS, P3155, DOI 10.1109/ICCV.2019.00325
   Brooks T, 2019, PROC CVPR IEEE, P11028, DOI 10.1109/CVPR.2019.01129
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Cai J, 2020, IEEE COMPUT SOC CONF, P1852, DOI 10.1109/CVPRW50498.2020.00235
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Foi A, 2007, IEEE T IMAGE PROCESS, V16, P1395, DOI 10.1109/TIP.2007.891788
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Kim Y, 2020, PROC CVPR IEEE, P3479, DOI 10.1109/CVPR42600.2020.00354
   Lecouat Bruno, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P238, DOI 10.1007/978-3-030-58542-6_15
   Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338
   Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Liu D, 2018, ADV NEUR IN, V31
   Liu HF, 2015, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2015.7298646
   Mao XJ, 2016, ADV NEUR IN, V29
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mou C., 2021, P IEEE INT C AC SPEE, P1
   Plötz T, 2018, ADV NEUR IN, V31
   Plötz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294
   Qiao P, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1847, DOI 10.1145/3123266.3123370
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang TY, 2017, PROC INT C TOOLS ART, P1272, DOI 10.1109/ICTAI.2017.00192
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia Z., 2020, P IEEE WINT C APPL C, P2426
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Yue ZS, 2019, ADV NEUR IN, V32
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2020, PROC CVPR IEEE, P2693, DOI 10.1109/CVPR42600.2020.00277
   Zhang J, 2016, IEEE T IMAGE PROCESS, V25, P1246, DOI 10.1109/TIP.2016.2515985
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2014, SIGNAL PROCESS, V103, P114, DOI 10.1016/j.sigpro.2013.09.025
   Zhang J, 2014, IEEE T CIRC SYST VID, V24, P915, DOI 10.1109/TCSVT.2014.2302380
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao C, 2018, IEEE ACCESS, V6, P76838, DOI 10.1109/ACCESS.2018.2882990
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P2057, DOI 10.1109/TCSVT.2016.2580399
NR 51
TC 49
Z9 51
U1 3
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1366
EP 1377
DI 10.1109/TMM.2021.3063916
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nie, RC
   Ma, CZ
   Cao, JD
   Ding, HW
   Zhou, DM
AF Nie, Rencan
   Ma, Chaozhen
   Cao, Jinde
   Ding, Hongwei
   Zhou, Dongming
TI A Total Variation With Joint Norms For Infrared and Visible Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Infrared and visible image fusion; total variation; joint norms; weight
   estimation
ID MULTI-FOCUS IMAGE; RGB-D SLAM; MOTION REMOVAL; MULTISOURCE; FILTER
AB A single infrared image or visible image for the same scene is usually insufficient to simultaneously reveal the infrared objects and the scene details. Thus, image fusion techniques play an important role in producing a single image from the images captured by infrared and visible sensors. In this paper, we propose a novel total variation (TV)-based fusion for infrared and visible images. In our model, a weighted fidelity term is employed to fuse both the infrared objects in the infrared image and the salient scenes in the visible image. To this end, a weight estimation method is developed based on the global luminance contrast-based saliency. Also, to overcome the over-fitting, two constraints are further introduced to merge more details from the visible image and prevent the luminance degradation for the fused result, respectively. Moreover, joint norms are exploited to produce a better result. l(2,1,rc) provides the structural group sparseness for the fidelity term, whereas l(1/2) presents the better gradient sparse for the detail preserving term and l(2) is utilized for the luminance degradation preventing term. Experimental results indicate that the proposed method can give state-of-the-art performances both in visual perception and quantitative scores than other methods.
C1 [Nie, Rencan; Ma, Chaozhen; Ding, Hongwei; Zhou, Dongming] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650500, Yunnan, Peoples R China.
   [Nie, Rencan] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
   [Cao, Jinde] Southeast Univ, Sch Math, Nanjing 210096, Peoples R China.
   [Cao, Jinde] Yonsei Univ, Yonsei Frontier Lab, Seoul 03722, South Korea.
C3 Yunnan University; Southeast University - China; Southeast University -
   China; Yonsei University
RP Cao, JD (corresponding author), Southeast Univ, Sch Math, Nanjing 210096, Peoples R China.
EM rcnie@ynu.edu.cn; chaozhenma@mail.ynu.edu.cn; jdcao@seu.edu.cn;
   dhw1964@163.com; zhoudm@ynu.edu.cn
RI 马, 朝振/JQX-3003-2023
OI 马, 朝振/0000-0001-7116-270X; Zhou, Dongming/0000-0003-0139-9415; Nie,
   Rencan/0000-0003-0568-1231
FU National Natural Science Foundation of China [61966037, 61833005,
   62066047, 61463052]; National Key Research and Development Project of
   China [2020YFA0714301]; China Postdoctoral Science Foundation
   [2017M621586]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61966037, 61833005, 62066047, and
   61463052, in part by the National Key Research and Development Project
   of China under Grant 2020YFA0714301, and in part by China Postdoctoral
   Science Foundation under Grant 2017M621586. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Concetto Spampinato.
CR Ancuti CO, 2017, IEEE T IMAGE PROCESS, V26, P65, DOI 10.1109/TIP.2016.2621674
   Cai PD, 2020, IEEE ROBOT AUTOM LET, V5, P4218, DOI 10.1109/LRA.2020.2994027
   Cui XH, 2016, 2016 THIRD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN RECOGNITION (AIPR)
   Dong LM, 2015, NEUROCOMPUTING, V159, P268, DOI 10.1016/j.neucom.2015.01.050
   Ghamisi P, 2019, IEEE GEOSC REM SEN M, V7, P6, DOI 10.1109/MGRS.2018.2890023
   Guan WJ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2596, DOI 10.1109/ICASSP.2018.8461523
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hait E, 2019, IEEE T IMAGE PROCESS, V28, P880, DOI 10.1109/TIP.2018.2872630
   Kong WW, 2013, INFRARED PHYS TECHN, V61, P27, DOI 10.1016/j.infrared.2013.06.009
   Kumar A, 2019, IEEE T IMAGE PROCESS, V28, P2921, DOI 10.1109/TIP.2019.2892663
   Kumar M, 2009, IEEE T IMAGE PROCESS, V18, P2137, DOI 10.1109/TIP.2009.2025006
   Li DZ, 2018, ISA T, V82, P210, DOI 10.1016/j.isatra.2017.08.014
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2013, IEEE T GEOSCI REMOTE, V51, P4779, DOI 10.1109/TGRS.2012.2230332
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu W, 2020, IEEE T CIRC SYST VID, V30, P23, DOI 10.1109/TCSVT.2018.2890202
   Liu Y, 2016, AAAI CONF ARTIF INTE, P201
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma Y, 2016, NEUROCOMPUTING, V202, P12, DOI 10.1016/j.neucom.2016.03.009
   Muller AC, 2009, INFORM FUSION, V10, P137, DOI 10.1016/j.inffus.2008.08.008
   Naidu VPS, 2011, DEFENCE SCI J, V61, P479, DOI 10.14429/dsj.61.705
   Nie F., 2010, NIPS
   Nie RC, 2021, IEEE T CIRC SYST VID, V31, P986, DOI 10.1109/TCSVT.2020.2998696
   Qin JH, 2020, NEUROCOMPUTING, V379, P334, DOI 10.1016/j.neucom.2019.10.076
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Shi MZ, 2016, SIGNAL PROCESS, V126, P65, DOI 10.1016/j.sigpro.2015.11.022
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Sun YX, 2021, IEEE T AUTOM SCI ENG, V18, P1000, DOI 10.1109/TASE.2020.2993143
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Tu XG, 2016, MED BIOL ENG COMPUT, V54, P1807, DOI 10.1007/s11517-016-1540-7
   Wencheng Wang, 2011, Journal of Computers, V6, P2559, DOI 10.4304/jcp.6.12.2559-2566
   Xu ZB, 2010, SCI CHINA INFORM SCI, V53, P1159, DOI 10.1007/s11432-010-0090-0
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang C, 2008, INFORM FUSION, V9, P156, DOI 10.1016/j.inffus.2006.09.001
   Yang Y, 2017, IEEE T INSTRUM MEAS, V66, P691, DOI 10.1109/TIM.2017.2658098
   Yang Y, 2015, IEEE SENS J, V15, P2824, DOI 10.1109/JSEN.2014.2380153
   Yang Y, 2011, PROCEDIA ENGINEER, V24, P177, DOI 10.1016/j.proeng.2011.11.2622
   Yu BT, 2016, NEUROCOMPUTING, V182, P1, DOI 10.1016/j.neucom.2015.10.084
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao WD, 2017, IEEE T INSTRUM MEAS, V66, P2283, DOI 10.1109/TIM.2017.2700198
NR 50
TC 20
Z9 22
U1 5
U2 50
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1460
EP 1472
DI 10.1109/TMM.2021.3065496
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200018
DA 2024-07-18
ER

PT J
AU Song, JK
   Zhang, JQ
   Gao, LL
   Zhao, Z
   Shen, HT
AF Song, Jingkuan
   Zhang, Jingqiu
   Gao, Lianli
   Zhao, Zhou
   Shen, Heng Tao
TI AgeGAN plus plus : Face Aging and Rejuvenation With Dual Conditional
   GANs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Faces; Aging; Gallium nitride; Generative adversarial networks;
   Training; Image reconstruction; Prototypes; Adversarial learning;
   conditional GANs; domain manipulation; dual GANs; representation
   disentanglement; face aging and rejuvenation
AB Face aging and rejuvenation is applied to predict what a person looks like at different ages. While prior work brought about a significant progress in this topic, there are two central problems remaining to be solved : 1) most prior works require sequential data during training, while it is very rare in existing datasets; and 2) how to render an aging face and preserve personality at the same time. To deal with these problems, we develop a novel dual conditional GANs mechanism, thus aging faces can be trained with multiple sets of unlabeled facial images of different ages. Our basic architecture is AgeGAN, in which the primal conditional GAN converts input faces to other ages based on relevant age conditions, and the dual conditional GAN learns to invert the task. We further improve our networks, termed AgeGAN++, in which we share the weights between the primal part and the dual part to to streamline the model. Moreover, in order to get more sensible results, a representation disentanglement component is integrated with the latent facial representation, and an enhanced discriminator is applied on the generated process. In addition, we firstly perform an interpolation experiment to demonstrate that our generators are powerful and effective for face aging and rejuvenation. Experimental results on four public datasets demonstrate the appealing performance of the proposed methods by comparing with the state-of-the-art methods. Our code and a demo are released at https://github.com/Sherry-JQ/AgeGAN.
C1 [Song, Jingkuan] Qiqihar Univ, Sch Informat Technol, Qiqihar 161006, Heilongjiang, Peoples R China.
   [Zhang, Jingqiu; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Sichuan, Peoples R China.
   [Zhang, Jingqiu; Gao, Lianli; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
   [Zhao, Zhou] Zhejiang Univ, Sch Comp Sci, Hangzhou 310058, Peoples R China.
C3 Qiqihar University; University of Electronic Science & Technology of
   China; University of Electronic Science & Technology of China; Zhejiang
   University
RP Song, JK (corresponding author), Qiqihar Univ, Sch Informat Technol, Qiqihar 161006, Heilongjiang, Peoples R China.
EM jingkuan.song@gmail.com; is.jingqiuzhang@gmail.com;
   lianli.gao@uestc.edu.cn; zhaozhou@zju.edu.cn; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021; zhao, zhao/JAC-1686-2023; Zhao,
   zhuo/JYO-7894-2024
OI song, jingkuan/0000-0002-2549-8322
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [61 772 116, 61 872 064,
   62020106008]; Sichuan Science and Technology Program [2019JDTD0005]
FX This work was supported in part by National Key Research and Development
   Program of China (No. 2018AAA0102200), in part by the National Natural
   Science Foundation of China under Grants 61 772 116, 61 872 064, and
   62020106008, and in part by Sichuan Science and Technology Program under
   Grant 2019JDTD0005.
CR [Anonymous], 2012, PROC ACM SIGGRAPH PO, DOI 10.1145/2342896.2343002
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chen X, 2016, ADV NEUR IN, V29
   Duong CN, 2017, IEEE I CONF COMP VIS, P3755, DOI 10.1109/ICCV.2017.403
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Duong C. N, 2019, P IEEE CVF C COMP VI, p10 013
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He D, 2016, ADV NEUR IN, V29
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Kemelmacher-Shlizerman I, 2014, PROC CVPR IEEE, P3334, DOI 10.1109/CVPR.2014.426
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma DP, 2014, ADV NEUR IN, V27
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li PP, 2018, INT C PATT RECOG, P1073, DOI 10.1109/ICPR.2018.8545119
   Liu AH, 2018, ADV NEUR IN, V31
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lopez R., 2018, P 32 INT C NEURAL IN, V31, P6117
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Odena A, 2017, PR MACH LEARN RES, V70
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Perarnau G., 2016, NIPS WORKSH ADV TRAI
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Ramanathan N., 2006, 2006 IEEE COMP SOC C, V1, P387
   Ramanathan N, 2008, IEEE INT CONF AUTOMA, P1006
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Suo JL, 2012, IEEE T PATTERN ANAL, V34, P2083, DOI 10.1109/TPAMI.2012.22
   Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39
   Tiddeman B, 2001, IEEE COMPUT GRAPH, V21, P42, DOI 10.1109/38.946630
   Wang W, 2019, IEEE T PATTERN ANAL, V41, P654, DOI 10.1109/TPAMI.2018.2803166
   Wang W, 2016, PROC CVPR IEEE, P2378, DOI 10.1109/CVPR.2016.261
   Wang ZW, 2018, PROC CVPR IEEE, P7939, DOI 10.1109/CVPR.2018.00828
   Yang HY, 2018, PROC CVPR IEEE, P31, DOI 10.1109/CVPR.2018.00011
   Yang HY, 2016, IEEE T IMAGE PROCESS, V25, P2493, DOI 10.1109/TIP.2016.2547587
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zeng JF, 2018, IEEE ACCESS, V6, P66715, DOI 10.1109/ACCESS.2018.2877706
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 42
TC 13
Z9 16
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 791
EP 804
DI 10.1109/TMM.2021.3059336
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100022
DA 2024-07-18
ER

PT J
AU Wu, ZY
   Li, S
   Chen, CL
   Hao, AM
   Qin, H
AF Wu, Zhenyu
   Li, Shuai
   Chen, Chenglizhao
   Hao, Aimin
   Qin, Hong
TI Deeper Look at Image Salient Object Detection: Bi-Stream Network With a
   Small Training Dataset
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training data; Training; Feature extraction; Logic gates; Object
   detection; Visualization; Task analysis; Bi-stream fusion; image salient
   object detection; small-scale training set
ID FUSION; MODEL
AB Compared with the conventional hand-crafted approaches, the deep learning based ISOD (image salient object detection) models have achieved tremendous performance improvements by training exquisitely crafted fancy networks over large-scale training sets. However, do we really need large-scale training set for ISOD? In this article, we provide a deeper insight into the interrelationship between the ISOD performance and the training data. To alleviate the conventional demands for large-scale training data, we provide a feasible way to construct a novel small-scale training set, which only contains 4 K images. To take full advantage of this new set, we propose a novel bi-stream network consisting of two different feature backbones. Benefit from the proposed gate control unit, this bi-stream network is able to achieve complementary fusion status for its subbranches. To our best knowledge, this is the first attempt to use a small-scale training set to compete with other large-scale ones; nevertheless, our method can still achieve the leading SOTA performance on all tested benchmark datasets. Both the code and dataset are publicly available at https://github.com/wuzhenyubuaa/TSNet.
C1 [Wu, Zhenyu; Li, Shuai; Chen, Chenglizhao; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Shuai; Hao, Aimin] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Chen, Chenglizhao] Qingdao Univ, Qingdao 266071, Shandong, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Beihang University; Peng Cheng Laboratory; Qingdao University; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Chen, CL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM wuzhenyu_961@126.com; lishuaiouc@126.com; cclz123@163.com;
   ham_buaa@163.com; qin@cs.stonybrook.edu
OI QIN, HONG/0000-0001-7699-1355
FU National Natural Science Foundation of China [61802215, 61806106,
   61672077, 61532002]; Natural Science Foundation of Shandong Province
   [ZR2019BF011, ZR2019QF009]; National Science Foundation of the USA
   [IIS-1715985, IIS-1812606]; National Key R&D Program of China
   [2017YF-F0106407]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61802215, 61806106, 61672077, and
   61532002, in part by the Natural Science Foundation of Shandong Province
   under Grants ZR2019BF011 and ZR2019QF009, in part by the National
   Science Foundation of the USA under Grants IIS-1715985 and IIS-1812606,
   and in part by the National Key R&D Program of China under Grant
   2017YF-F0106407. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Engin Erzin.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2019, 33 AAAI C ART INT
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Chen CLZ, 2016, PATTERN RECOGN, V52, P410, DOI 10.1016/j.patcog.2015.09.033
   Chen CLZ, 2015, PATTERN RECOGN, V48, P2885, DOI 10.1016/j.patcog.2015.01.025
   Chen CLZ, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2403232
   Chen SH, 2020, IEEE T IMAGE PROCESS, V29, P3763, DOI 10.1109/TIP.2020.2965989
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Chu X, 2017, PROC CVPR IEEE, P5669, DOI 10.1109/CVPR.2017.601
   Dauphin Y. N., 2015, P 4 INT C LEARN REPR
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Deng-Ping Fan, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12266), P263, DOI 10.1007/978-3-030-59725-2_26
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2020, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR42600.2020.00285
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Feng MY, 2020, IEEE T IMAGE PROCESS, V29, P4696, DOI 10.1109/TIP.2020.2975919
   Fu KR, 2020, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR42600.2020.00312
   Fu KR, 2019, NEUROCOMPUTING, V356, P69, DOI 10.1016/j.neucom.2019.04.062
   Gehring J, 2017, PR MACH LEARN RES, V70
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hou SH, 2017, IEEE I CONF COMP VIS, P502, DOI 10.1109/ICCV.2017.62
   HSU KJ, 2017, BMVC
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Lake B., 2011, P ANN M COGNITIVE SC, P1
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Li YX, 2021, IEEE T CIRC SYST VID, V31, P2315, DOI 10.1109/TCSVT.2020.3023080
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Ma GX, 2020, IEEE T MULTIMEDIA, V22, P324, DOI 10.1109/TMM.2019.2929943
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918
   Mnih V, 2014, ADV NEUR IN, V27
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Sindhwani V., 2005, P ICML WORKSH LEARN
   Snell J, 2017, ADV NEUR IN, V30
   Su JM, 2019, IEEE I CONF COMP VIS, P3798, DOI 10.1109/ICCV.2019.00390
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang W, 2021, ACTA CLIN BELG, V76, P70, DOI 10.1080/17843286.2019.1649081
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang X., NEUROCOMPUTING
   Wang XH, 2021, IEEE T IMAGE PROCESS, V30, P458, DOI 10.1109/TIP.2020.3037470
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Wu Z., 2020, ARXIV200804158
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Zeng Y, 2019, PROC CVPR IEEE, P6067, DOI 10.1109/CVPR.2019.00623
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P3534, DOI 10.1109/TIP.2019.2962688
   Zhang L, 2019, PROC CVPR IEEE, P6017, DOI 10.1109/CVPR.2019.00618
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhang Z., 2020, ARXIV200414582
   Zhao JX, 2019, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2019.00405
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao S., 2020, ARXIV200600269
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou H., 2020, P IEEE C COMP VIS PA, P9141, DOI 10.1109/CVPR42600.2020.00916
NR 92
TC 20
Z9 21
U1 2
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 73
EP 86
DI 10.1109/TMM.2020.3046871
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300006
DA 2024-07-18
ER

PT J
AU Zeng, Z
   Wang, T
   Ma, FL
   Zhang, L
   Shen, PY
   Shah, SAA
   Bennamoun, M
AF Zeng, Zhi
   Wang, Ting
   Ma, Fulei
   Zhang, Liang
   Shen, Peiyi
   Shah, Syed Afaq Ali
   Bennamoun, Mohammed
TI Probability-Based Framework to Fuse Temporal Consistency and Semantic
   Information for Background Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image segmentation; Deep learning; Training; Visualization;
   Recurrent neural networks; Merging; Background segmentation; deep
   learning framework; information fusion; semantic segmentation; temporal
   consistency; the law of total probability
ID OBJECT DETECTION; VIDEO; SURVEILLANCE; SUBTRACTION; ALGORITHMS;
   TRACKING; NETWORK
AB The fusion of temporal consistency and semantic information with limited foreground information for background segmentation using deep learning is an underinvestigated problem. In this paper, we explore the relation between temporal consistency and semantic information based on the law of total probability. A highly concise framework is proposed to fuse these two types of information. A theoretical proof is given to show that the proposed framework is more accurate than either the temporal consistency-based model or the semantic information-based model and that each model is a special case of the proposed framework. The proposed framework is a white-box framework that can easily be embedded into a deep neural network as a merging layer. In the proposed model, only a few parameters must be learned, which substantially reduces the need for a large dataset. In addition, these interpretable parameters reflect our understanding of the background and can be applied to a wide range of environments. Extensive evaluations indicate the promising performance of the proposed method. Our code and trained weights for the experiments are available at GitHub.(1) (1) https://github.com/zengzhi2015/SS_TC_BS (We encourage the reader to run the program for a better understanding of the proposed method).
C1 [Zeng, Zhi; Wang, Ting; Ma, Fulei; Zhang, Liang; Shen, Peiyi] Xidian Univ, Xian 710071, Peoples R China.
   [Shah, Syed Afaq Ali] Murdoch Univ, Coll Sci Hlth Engn & Educ, Murdoch, WA 6150, Australia.
   [Bennamoun, Mohammed] Univ Western Australia, Sch Comp Sci & Software Engn, Perth, WA 6000, Australia.
C3 Xidian University; Murdoch University; University of Western Australia
RP Zeng, Z (corresponding author), Xidian Univ, Xian 710071, Peoples R China.
EM zhizeng@mail.xidian.edu.cn; tingw2016hg@126.com; fuleima@xidian.edu.cn;
   liangzhang@xidian.edu.cn; pyshen@xidian.edu.cn;
   afaq.shah@murdoch.edu.au; mohammed.bennamoun@uwa.edu.au
RI Bennamoun, Mohammed/C-2789-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; Zeng, Zhi/0000-0001-5896-0192;
   Shah, Syed Afaq Ali/0000-0003-2181-8445; Zhang,
   Liang/0000-0003-4331-5830
FU National Key R&D Program of China [2019YFB1311600, 2020YFF0304900];
   National Natural Science Foundation of China [61805185, 51805397,
   62072358]; Australian Research Council [DP150100294]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFB1311600, in part by the National Natural Science
   Foundation of China under Grants 61805185 and 51805397, in part by the
   Australian Research Council Discovery Projects under Grant ID:
   DP150100294, in part by the NationalNatural Science Foundation ofChina
   underGrant 62072358, and in part by the National Key R&D Program of
   China under Grant 2020YFF0304900.
CR Aich S, 2018, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW.2018.00032
   Allebosch G, 2015, LECT NOTES COMPUT SC, V9386, P130, DOI 10.1007/978-3-319-25903-1_12
   [Anonymous], 2000, P EUR C COMP VIS, DOI DOI 10.1007/3-540-45053-X_48
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   BASU S, 1987, PATTERN RECOGN, V20, P497, DOI 10.1016/0031-3203(87)90077-X
   Bianco S, 2017, IEEE T EVOLUT COMPUT, V21, P914, DOI 10.1109/TEVC.2017.2694160
   Bloisi DD, 2014, MACH VISION APPL, V25, P1257, DOI 10.1007/s00138-013-0554-5
   Borji A, 2021, IEEE T PATTERN ANAL, V43, P679, DOI 10.1109/TPAMI.2019.2935715
   Botev ZI, 2010, ANN STAT, V38, P2916, DOI 10.1214/10-AOS799
   Bouwmans T, 2014, COMPUT SCI REV, V11-12, P31, DOI 10.1016/j.cosrev.2014.04.001
   Braham M, 2017, IEEE IMAGE PROC, P4552, DOI 10.1109/ICIP.2017.8297144
   Braham M, 2016, INT CONF SYST SIGNAL, P113
   Briese C, 2018, INT CONF UNMAN AIRCR, P606, DOI 10.1109/ICUAS.2018.8453372
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YY, 2019, IEEE T CIRC SYST VID, V29, P2567, DOI 10.1109/TCSVT.2017.2770319
   Choo S, 2019, LECT NOTES COMPUT SC, V11366, P357, DOI 10.1007/978-3-030-20876-9_23
   Christiansen P, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16111904
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   De Gregorio M., 2017, P ESANN, P453
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu C, 1998, IEEE T CIRC SYST VID, V8, P572, DOI 10.1109/76.718504
   Guerra-Filho G.B., 2005, J THEORETICAL APPL I, VRITA 12, P61
   He K., 2018, IEEE T PATTERN ANAL, V42, P386, DOI DOI 10.1109/TPAMI.2018.2844175
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu H., 2017, P 34 INT C MACH LEAR, V4, P2482
   Hu ZH, 2018, IEEE ACCESS, V6, P43450, DOI 10.1109/ACCESS.2018.2861223
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jégou S, 2017, IEEE COMPUT SOC CONF, P1175, DOI 10.1109/CVPRW.2017.156
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   Jiang T, 2020, IEEE T GEOSCI REMOTE, V58, P4666, DOI 10.1109/TGRS.2020.2965961
   Khan SU, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-27515-w
   Kingma D. P., 2014, arXiv
   Kiran BR, 2018, J IMAGING, V4, DOI 10.3390/jimaging4020036
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DS, 2005, IEEE T PATTERN ANAL, V27, P827, DOI 10.1109/TPAMI.2005.102
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Lee S. H., 2019, SYMMETRY-BASEL, V11
   Liang D, 2015, PATTERN RECOGN, V48, P1374, DOI 10.1016/j.patcog.2014.10.020
   Lim LA, 2020, PATTERN ANAL APPL, V23, P1369, DOI 10.1007/s10044-019-00845-9
   Lim LA, 2018, PATTERN RECOGN LETT, V112, P256, DOI 10.1016/j.patrec.2018.08.002
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Y, 2019, IEEE I CONF COMP VIS, P1232, DOI 10.1109/ICCV.2019.00132
   López-Rubio E, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500563
   Maddalena L., 2012, 2012 IEEE COMP SOC C, P21, DOI [10.1109/CVPRW.2012.6238922, DOI 10.1109/CVPRW.2012.6238922]
   Maddalena L, 2010, NEURAL COMPUT APPL, V19, P179, DOI 10.1007/s00521-009-0285-8
   Maninis KK, 2019, IEEE T PATTERN ANAL, V41, P1515, DOI 10.1109/TPAMI.2018.2838670
   Martins I, 2017, LECT NOTES COMPUT SC, V10255, P50, DOI 10.1007/978-3-319-58838-4_6
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Ni ZL, 2019, IEEE ENG MED BIO, P5735, DOI [10.1109/EMBC.2019.8856495, 10.1109/embc.2019.8856495]
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Oda H, 2018, LECT NOTES COMPUT SC, V11071, P228, DOI 10.1007/978-3-030-00934-2_26
   Ozkan K., 2018, J ELECT IMAG, V27, P1
   Pan JS, 2021, IEEE T PATTERN ANAL, V43, P2449, DOI 10.1109/TPAMI.2020.2969348
   Peng Suo, 2008, 2008 9th International Conference on Signal Processing (ICSP 2008), P1436, DOI 10.1109/ICOSP.2008.4697402
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ramírez-Alonso G, 2016, NEUROCOMPUTING, V175, P990, DOI 10.1016/j.neucom.2015.04.118
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajid H, 2017, IEEE T IMAGE PROCESS, V26, P3249, DOI 10.1109/TIP.2017.2695882
   Sakkos D, 2018, MULTIMED TOOLS APPL, V77, P23023, DOI 10.1007/s11042-017-5460-9
   Sedky M, 2014, IEEE COMPUT SOC CONF, P405, DOI 10.1109/CVPRW.2014.65
   Senior AW, 2011, LECT NOTES COMPUT SC, V6468, P164
   Shakeri M, 2012, PROCEEDINGS OF THE 10TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA 2012), P4507, DOI 10.1109/WCICA.2012.6359241
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Sobral A, 2014, COMPUT VIS IMAGE UND, V122, P4, DOI 10.1016/j.cviu.2013.12.005
   St-Charles PL, 2015, IEEE WINT CONF APPL, P990, DOI 10.1109/WACV.2015.137
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Tang J, 2007, ELECTRON LETT, V43, P448, DOI 10.1049/el:20073674
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Unzueta L, 2012, IEEE T INTELL TRANSP, V13, P527, DOI 10.1109/TITS.2011.2174358
   Varghese A., 2017, IPSJ T COMPUT VIS AP, V9
   Wang G, 2018, IEEE T MULTIMEDIA, V20, P2921, DOI 10.1109/TMM.2018.2829163
   Wang KF, 2018, IEEE ACCESS, V6, P15505, DOI 10.1109/ACCESS.2018.2812880
   Wang R, 2014, IEEE COMPUT SOC CONF, P420, DOI 10.1109/CVPRW.2014.68
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wang Z., 2018, ARXIV181105118
   Wei MS, 2018, LIGHT-SCI APPL, V7, DOI 10.1038/lsa.2018.6
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Xu Y, 2015, IEEE INT CONF RFID, P1, DOI 10.1109/RFID.2015.7113066
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Ye J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P99, DOI 10.1145/2671188.2749340
   Zaharescu A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1753, DOI 10.1109/ICCVW.2011.6130461
   Zeng Z, 2017, IEEE T FUZZY SYST, V25, P584, DOI 10.1109/TFUZZ.2016.2566811
   Zeng Z, 2014, OPT EXPRESS, V22, P21577, DOI 10.1364/OE.22.021577
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang X, 2017, IEEE T MULTIMEDIA, V19, P2425, DOI 10.1109/TMM.2017.2701645
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng WB, 2020, NEUROCOMPUTING, V394, P178, DOI 10.1016/j.neucom.2019.04.088
   [郑文博 Zheng Wenbo], 2018, [自动化学报, Acta Automatica Sinica], V44, P878
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 103
TC 2
Z9 2
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 740
EP 754
DI 10.1109/TMM.2021.3058770
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100018
DA 2024-07-18
ER

PT J
AU Zhu, LC
   Fan, HH
   Luo, YW
   Xu, ML
   Yang, Y
AF Zhu, Linchao
   Fan, Hehe
   Luo, Yawei
   Xu, Mingliang
   Yang, Yi
TI Temporal Cross-Layer Correlation Mining for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Three-dimensional displays; Logic gates; Correlation;
   Trajectory; Aggregates; Training; Deep learning; video feature learning;
   video classification; action recognition; frame correlation mining
AB Neighboring frames are more correlated compared to frames from further temporal distances. In this paper, we aim to explore the temporal correlations among neighboring frames and exploit cross-layer multi-scale features for action recognition. First, we present a Temporal Cross-Layer Correlation (TCLC) framework for temporal correlation learning. The unified framework uncovers both local and global structures from video data, enabling a better exploration of temporal context and assisting cross-layer spatio-temporal feature learning. Second, we propose a novel cross-layer attention and a center-guided attention mechanism to integrate features with contextual knowledge from multiple scales. Our method is a two-stage process for effective cross-layer feature learning. The first stage incorporates the cross-layer attention module to decide the importance weight of the convolutional layers. The second stage leverages the center-guided attention mechanism to aggregate local features from each layer for the generation of a final video representation. We leverage global centers to extract shared semantic knowledge among videos. We evaluate TCLC on three action recognition datasets, i.e., UCF-101, HMDB-51 and Kinetics. Our experimental results demonstrate the superiority of our proposed temporal correlation mining method.
C1 [Zhu, Linchao; Fan, Hehe; Yang, Yi] Univ Technol Sydney, Australian Artificial Intelligence Inst, ReLER Lab, Sydney, NSW 2007, Australia.
   [Luo, Yawei] Zhejiang Univ, Hangzhou 310007, Zhejiang, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Zhengzhou 450000, Henan, Peoples R China.
C3 University of Technology Sydney; Zhejiang University; Zhengzhou
   University
RP Fan, HH (corresponding author), Univ Technol Sydney, Australian Artificial Intelligence Inst, ReLER Lab, Sydney, NSW 2007, Australia.
EM zhulinchao7@gmail.com; crane.h.fan@gmail.com; royalvane@hust.edu.cn;
   iexumingliang@zzu.edu.cn; Yi.Yang@uts.edu.au
RI yang, yang/HGT-7999-2022; Yang, Yi/B-9273-2017; Zhu,
   Linchao/AAE-6700-2020; Luo, Yawei/AFK-9247-2022
OI Yang, Yi/0000-0002-0512-880X; Zhu, Linchao/0000-0002-4093-7557; Luo,
   Yawei/0000-0002-7037-1806; Fan, Hehe/0000-0001-9572-2345
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 1997, NEURAL COMPUT
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YP, 2018, ADV NEUR IN, V31
   Cho K., 2015, EMNLP
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Diba Ali, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P593, DOI 10.1007/978-3-030-58558-7_35
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hong RC, 2012, IEEE MULTIMEDIA, V19, P72, DOI 10.1109/MMUL.2011.53
   Kay W., 2017, ARXIV170506950
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Liu Z., 2020, ARXIV200506803 CORR
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K., 2012, ARXIV12120402CS
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang KZ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P97, DOI 10.1145/2647868.2654912
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang Shiwen, 2020, INT C LEARN REPR
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zhao SC, 2018, IEEE T CIRC SYST VID, V28, P1839, DOI 10.1109/TCSVT.2017.2682196
   Zhu LC, 2017, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2017.147
NR 47
TC 58
Z9 59
U1 8
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 668
EP 676
DI 10.1109/TMM.2021.3057503
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100012
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Bai, YQ
   Zhu, ZJ
   Jiang, GY
   Sun, HF
AF Bai, Yongqiang
   Zhu, Zhongjie
   Jiang, Gangyi
   Sun, Huifang
TI Blind Quality Assessment of Screen Content Images Via Macro-Micro
   Modeling of Tensor Domain Dictionary
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Tensors; Dictionaries; Image color analysis; Image
   quality; Image coding; Mathematical model; Screen content image; image
   quality assessment; no-reference; macro-micro modeling; dictionary
   learning
ID FEATURES
AB Screen content images (SCIs) have been rapidly and widely applied in interactive multimedia applications. The problem of quality assessment for SCIs is an interesting research topic. Most of the existing methods use subjective and independent features in gray domain to predict the image quality, which cannot comprehensively characterize the image properties or lack unified mathematical explanation for SCIs. To address these problems, we propose a novel blind quality assessment method based on macro-micro modeling of tensor domain dictionary for SCIs in this article. In the proposed method, the tensor decomposition is explored first to avoid the loss of color information, and then a target dictionary is learned more effectively with the principal components. Furthermore, a macro-micro model is established to characterize the micro and macro features in the target dictionary space, which can provide a systematic mathematical interpretation for feature extraction. For the micro features, a log-normal pooling scheme is designed to enhance the effectiveness of feature aggregation by analyzing the particularity of the statistical distribution of sparse codes. Additionally, the statistical properties are mainly discussed and studied based on the Bernoulli law of large numbers, and then a reliable macro feature is generated to describe the relationship between the statistical distribution and quality degradation of SCIs. Experimental results determined by using three public SCI databases show that the proposed method can perform better than relevant existing methods in the prediction of the visual quality of SCIs, especially in terms of the generalization for distortion type and interpretability for feature generation.
C1 [Bai, Yongqiang; Zhu, Zhongjie] Zhejiang Wanli Univ, Coll Informat & Intelligence Engn, Ningbo 315100, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Sun, Huifang] Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA.
C3 Zhejiang Wanli University; Ningbo University
RP Zhu, ZJ (corresponding author), Zhejiang Wanli Univ, Coll Informat & Intelligence Engn, Ningbo 315100, Peoples R China.; Jiang, GY (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM byq-163@163.com; zhongjiezhu@hotmail.com; jianggangyi@nbu.edu.cn;
   hsun@merl.com
RI jiang, gang/KII-8233-2024
FU Natural Science Foundation of China [61671412, 61871247, 61931022];
   Natural Science Foundation of Zhejiang Province [LY19F010002,
   LY21F010014]; Commonweal Projects of Zhejiang Province [LGN20F010001];
   Natural Science Foundation of Ningbo, China [2018A610053, 202003N4323];
   General Scientific Research Project of Zhejiang Education Department
   [Y201941122]; Ningbo Municipal Projects for Leading and Top Talents
   [NBLJ201801006]; Fundamental Research Funds for Zhejiang Provincial
   Colleges and Universities; School-level Research and Innovation Team of
   Zhejiang Wanli University
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61671412, Grant 61871247, and Grant 61931022, in part
   by the Natural Science Foundation of Zhejiang Province under Grant
   LY19F010002 and Grant LY21F010014, in part by the Commonweal Projects of
   Zhejiang Province under Grant LGN20F010001, in part by the Natural
   Science Foundation of Ningbo, China under Grant 2018A610053 and Grant
   202003N4323, in part by the General Scientific Research Project of
   Zhejiang Education Department under Grant Y201941122, in part by the
   Ningbo Municipal Projects for Leading and Top Talents under Grant
   NBLJ201801006, in part by the Fundamental Research Funds for Zhejiang
   Provincial Colleges and Universities, and in part by the School-level
   Research and Innovation Team of Zhejiang Wanli University.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Aldahdooh A, 2019, IEEE T MULTIMEDIA, V21, P2026, DOI 10.1109/TMM.2018.2882091
   [Anonymous], 2014, C LEARNING THEORY
   [Anonymous], 2016, P IEEE INT C QUAL MU
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bai YQ, 2019, SIGNAL PROCESS, V161, P248, DOI 10.1016/j.sigpro.2019.03.013
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Chen JN, 2018, IEEE SIGNAL PROC LET, V25, P1685, DOI 10.1109/LSP.2018.2871250
   Chen ZJ, 2011, ADV INTEL SOFT COMPU, V100, P19
   Chung KL, 2019, IEEE T IMAGE PROCESS, V28, P1108, DOI 10.1109/TIP.2018.2875340
   Chung KL, 2017, IEEE T IMAGE PROCESS, V26, P6034, DOI 10.1109/TIP.2017.2749148
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fu Y, 2018, IEEE T CIRC SYST VID, V28, P2428, DOI 10.1109/TCSVT.2018.2854176
   Gottschalk PG, 2005, ANAL BIOCHEM, V343, P54, DOI 10.1016/j.ab.2005.04.035
   Gu K, 2018, IEEE T VIS COMPUT GR, V24, P2689, DOI 10.1109/TVCG.2017.2771284
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Jiang X., 2019, ARXIV190300705
   Kim I, 2018, IET IMAGE PROCESS, V12, P479, DOI 10.1049/iet-ipr.2017.0853
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kuang W, 2020, IEEE T IMAGE PROCESS, V29, P170, DOI 10.1109/TIP.2019.2924810
   Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826
   Lu N, 2018, SIGNAL PROCESS, V145, P225, DOI 10.1016/j.sigpro.2017.12.004
   Ma SW, 2016, IEEE MULTIMEDIA, V23, P16, DOI 10.1109/MMUL.2016.16
   Mandal D, 2016, IEEE T IMAGE PROCESS, V25, P3826, DOI 10.1109/TIP.2016.2577885
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ni ZK, 2018, IEEE T IMAGE PROCESS, V27, P4516, DOI 10.1109/TIP.2018.2839890
   Ni ZK, 2017, I S INTELL SIG PROC, P774, DOI 10.1109/ISPACS.2017.8266580
   Nowak P, 2019, IEEE T FUZZY SYST, V27, P2293, DOI 10.1109/TFUZZ.2019.2896849
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Rubinstein R., 2013, Efficient implementation of the KSVD algorithm using batch orthogonal matching pursuit
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Tang T, 2019, IET IMAGE PROCESS, V13, P1382, DOI 10.1049/iet-ipr.2018.6221
   Wang RF, 2019, IEEE ACCESS, V7, P5285, DOI 10.1109/ACCESS.2018.2889992
   Wang SQ, 2016, IEEE J EM SEL TOP C, V6, P532, DOI 10.1109/JETCAS.2016.2598756
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu J, 2019, DIGIT SIGNAL PROCESS, V91, P31, DOI 10.1016/j.dsp.2018.12.004
   Yang AS, 2017, MULTIDIM SYST SIGN P, V28, P1249, DOI 10.1007/s11045-016-0395-2
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yang JC, 2018, SIGNAL PROCESS, V153, P336, DOI 10.1016/j.sigpro.2018.07.006
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540
   Yue GH, 2019, DIGIT SIGNAL PROCESS, V91, P21, DOI 10.1016/j.dsp.2018.12.007
   Zhang L, 2018, IET IMAGE PROCESS, V12, P738, DOI 10.1049/iet-ipr.2017.0897
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang Y, 2018, IEEE T IMAGE PROCESS, V27, P5113, DOI 10.1109/TIP.2018.2851390
   Zheng LR, 2019, IEEE T MULTIMEDIA, V21, P2057, DOI 10.1109/TMM.2019.2894939
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
NR 52
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4259
EP 4271
DI 10.1109/TMM.2020.3039382
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900027
DA 2024-07-18
ER

PT J
AU Bakir, N
   Hamidouche, W
   Fezza, SA
   Samrouth, K
   Déforges, O
AF Bakir, Nader
   Hamidouche, Wassim
   Fezza, Sid Ahmed
   Samrouth, Khouloud
   Deforges, Olivier
TI Light Field Image Coding Using VVC Standard and View Synthesis Based on
   Dual Discriminator GAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Encoding; Cameras; Decoding; Transforms; Transform coding;
   Standards; Coding structure; deep learning; light field; quality
   enhancement; RDO; VVC; view synthesis
ID COMPRESSION
AB Light field (LF) technology is considered as a promising way for providing a high-quality virtual reality (VR) content. However, such an imaging technology produces a large amount of data requiring efficient LF image compression solutions. In this paper, we propose a LF image coding method based on a view synthesis and view quality enhancement techniques. Instead of transmitting all the LF views, only a sparse set of reference views are encoded and transmitted, while the remaining views are synthesized at the decoder side. The transmitted views are encoded using the versatile video coding (VVC) standard and are used as reference views to synthesize the dropped views. The selection of non-reference dropped views is performed using a rate-distortion optimization based on the VVC temporal scalability. The dropped views are reconstructed using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure that the quality of the views is consistent, at the decoder, a quality enhancement procedure is performed on the reconstructed views allowing smooth navigation across views. Experimental results show that the proposed method provides high coding performance and overcomes the state-of-the-art LF image compression methods by -36.22% in terms of BD-BR and 1.35 dB in BD-PSNR. The web page of this work is available at https://naderbakir79.github.io/LFD2GAN.html.
C1 [Bakir, Nader; Hamidouche, Wassim; Deforges, Olivier] Univ Rennes, INSA Rennes, CNRS, IETR UMR, F-6164 Rennes, France.
   [Fezza, Sid Ahmed] Natl Inst Telecommun, Oran 31000, Algeria.
   [Fezza, Sid Ahmed] ICT, Oran 31000, Algeria.
   [Samrouth, Khouloud] Lebanese Univ, Fac Engn, Elect & Telecommun Dept, Tripoli 1300, Lebanon.
C3 Universite de Rennes; Institut National des Sciences Appliquees de
   Rennes; Centre National de la Recherche Scientifique (CNRS); Lebanese
   University
RP Hamidouche, W (corresponding author), Univ Rennes, INSA Rennes, CNRS, IETR UMR, F-6164 Rennes, France.
EM Nader.Bakir@insa-rennes.fr; Wassim.Hatnidouche@insa-rennes.fr;
   sfezza@inttic.dz; khouloud.samrout@gmail.com;
   Olivier.Deforges@insa-rennes.fr
RI Fezza, Sid Ahmed/JJF-6642-2023
OI Fezza, Sid Ahmed/0000-0001-6453-8588; DEFORGES,
   olivier/0000-0003-0750-0959; Bakir, Nader/0009-0003-5786-4248
CR Ahmad W, 2019, IEEE ACCESS, V7, P143002, DOI 10.1109/ACCESS.2019.2944765
   [Anonymous], 2018, P 7 EUR WORKSH VIS I
   [Anonymous], 2015, P 3 INT C LEARN REPR
   [Anonymous], 2017, IEEE IMAGE PROC
   Bai YC, 2018, LECT NOTES COMPUT SC, V11217, P210, DOI 10.1007/978-3-030-01261-8_13
   Bakir N., 2020, PROC IEEE INT C MULT, P1
   Bakir N, 2019, IEEE DATA COMPR CONF, P554, DOI 10.1109/DCC.2019.00066
   Brites C, 2021, IEEE T CIRC SYST VID, V31, P339, DOI 10.1109/TCSVT.2020.2976784
   Chaurasia G, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2487228.2487238
   Chaurasia G, 2011, COMPUT GRAPH FORUM, V30, P1223, DOI 10.1111/j.1467-8659.2011.01981.x
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P4889, DOI 10.1109/TIP.2018.2839524
   Chen YL, 2020, IEEE SIGNAL PROC LET, V27, P1135, DOI 10.1109/LSP.2020.3003533
   Conti C, 2020, IEEE ACCESS, V8, P49244, DOI 10.1109/ACCESS.2020.2977767
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Conti C, 2018, SIGNAL PROCESS-IMAGE, V60, P144, DOI 10.1016/j.image.2017.10.006
   de Carvalho MB, 2018, IEEE IMAGE PROC, P435, DOI 10.1109/ICIP.2018.8451684
   Dib E, 2019, IEEE DATA COMPR CONF, P369, DOI 10.1109/DCC.2019.00045
   Farrugia R. A, 2019, IEEE INT C IMAGE PRO, P121
   Fiss J, 2014, IEEE INT CONF COMPUT
   Galea C, 2019, INT CONF ACOUST SPEE, P1882, DOI 10.1109/ICASSP.2019.8683548
   Gershun Andrei, 1939, J MATH PHYS, V18, P51, DOI [10.1002/sapm193918151, DOI 10.1002/SAPM193918151]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hannuksela Miska M., 2015, 2015 IEEE International Conference on Image Processing (ICIP). Proceedings, P2154, DOI 10.1109/ICIP.2015.7351182
   Hou JH, 2019, IEEE T CIRC SYST VID, V29, P517, DOI 10.1109/TCSVT.2018.2802943
   Huang FC, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766922
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Irene V., 2018, SPIE
   Jeon HG, 2015, PROC CVPR IEEE, P1547, DOI 10.1109/CVPR.2015.7298762
   Jia CM, 2019, IEEE J EM SEL TOP C, V9, P177, DOI 10.1109/JETCAS.2018.2886642
   Jiang XR, 2017, IEEE J-STSP, V11, P1132, DOI 10.1109/JSTSP.2017.2747078
   Kalantari NK, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980251
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Komatsu K, 2018, IEEE IMAGE PROC, P903, DOI 10.1109/ICIP.2018.8451812
   Kondermann D, 2016, IEEE COMPUT SOC CONF, P19, DOI 10.1109/CVPRW.2016.10
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li L, 2017, IEEE J-STSP, V11, P1107, DOI 10.1109/JSTSP.2017.2725198
   Liu D., 2016, ARXIV PREPRINT ARXIV, P1, DOI DOI 10.1109/ICMEW.2016.7574674
   Liu DY, 2020, IEEE T MULTIMEDIA, V22, P846, DOI 10.1109/TMM.2019.2934426
   Nguyen T., 2017, NIPS, P2670
   Raj S., 2016, Stanford lytro light field archive[EB/OL]
   Rerabek M, 2016, NEWLIGHT FIELD IMAGE
   Sidaty N, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954562
   Taubman D., 2013, JPEG2000 Image Compression Fundamentals, Standards and Practice
   Wang J, 2020, IEEE DATA COMPR CONF, P397, DOI 10.1109/DCC47342.2020.00047
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wien M., 2017, PRELIMINARY JOINT CA
   Ye J. C. Y, 2019, P DOC JVET N1002 14
   Ye P., 2012, K MEANS SPARSE CODIN
   Zhao JB, 2019, IEEE ACCESS, V7, P135982, DOI 10.1109/ACCESS.2019.2930644
   Zhao S, 2016, 2016 VISUAL COMMUNIC, P1, DOI [DOI 10.1109/ICME.2016.7552952., DOI 10.1109/INEC.2016.7589349]
   Zhao SY, 2017, IEEE IMAGE PROC, P4562, DOI 10.1109/ICIP.2017.8297146
   Zhao Xiaotong, 2016, IEEE VEHICULAR TECHN
   Zhao X, 2018, IEEE T IMAGE PROCESS, V27, P2514, DOI 10.1109/TIP.2018.2802202
NR 54
TC 12
Z9 13
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2972
EP 2985
DI 10.1109/TMM.2021.3068563
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, G
   Zhang, C
   Zou, YX
AF Chen, Guang
   Zhang, Can
   Zou, Yuexian
TI AFNet: Temporal Locality-Aware Network With Dual Structure for Accurate
   and Fast Action Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Feature extraction; Three-dimensional displays; Task
   analysis; Training; Object detection; Convolution; Action detection;
   joint representation; temporal locality-aware network; contextual
   structured spatial temporal pooling; AFNet
AB Inspired by Faster R-CNN, current state-of-the-art region-based action detection approaches like R-C3D and TAL-Net creatively proposed Temporal Region Proposal Network (TRPN) to generate proposals, which greatly improved action detection accuracy. However, since smooth L1 loss adopted in TRPN focuses on relative offset to pre-set anchor segments and is not sensitive enough to action boundaries and temporal regions, there is still room for improvement in temporal proposal generation. In this work, we elaborately design a Temporal Locality-Aware Network (TLAN) to learn a binary classifier using frame-level annotations. This allows our framework to effectively distinguish action instance (positive temporal regions) from background (negative temporal regions) by jointly optimizing temporal regions classification and temporal reference boxes regression, thus enabling precise localization. We further introduce a novel pooling method named Contextual Structured Spatial Temporal Pooling (CSSTP) to better exploit context and spatial-temporal information in an end-to-end fashion. Finally, TLAN and CSSTP are incorporated into a unified framework named AFNet. Extensive experiments have been conducted to evaluate the performance of our method. We achieve state-of-the-art performance on THUMOS'14 (20.6% higher than R-C3D, 6.7% higher than TAL-Net mAP @0.5) and competitive performance on Charades and ActivityNet. Besides, our inference speed reaches 1024 FPS, which is 250x faster than TAL-Net (3.5 FPS) and comparable to R-C3D (1030 FPS).
C1 [Chen, Guang; Zhang, Can; Zou, Yuexian] Peking Univ, Sch Elect & Comp Engn, ADSPLAB, Shenzhen 518055, Peoples R China.
   [Zou, Yuexian] Pengcheng Lab, Shenzhen 518066, Peoples R China.
C3 Peking University
RP Zou, YX (corresponding author), Peking Univ, Sch Elect & Comp Engn, ADSPLAB, Shenzhen 518055, Peoples R China.
EM guangchen@pku.edu.cn; zhangcan@pku.edu.cn; zouyx@pku.edu.cn
OI ZOU, Yue-Xian/0000-0001-9999-6140; Zhang, Can/0000-0001-9530-5218
FU IER foundation [HT-JD-CXY-201904]; Shenzhen Municipal Development and
   Reform Commission (Disciplinary Development Program for Data Science and
   Intelligent Computing)
FX This work was supported in part by IER foundation under Grant
   HT-JD-CXY-201904 and in part by Shenzhen Municipal Development and
   Reform Commission (Disciplinary Development Program for Data Science and
   Intelligent Computing).
CR [Anonymous], 2018, ICMR 18 P 2018 ACM, DOI DOI 10.1145/3206025.3206029
   [Anonymous], 2017, CVPR
   Buch S., 2017, P BRIT MACH VIS C BM
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J., 2018, arXiv
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao J., 2017, ARXIV170704818
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao Jiyang, 2018, PROC EUROPEAN C COMP
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heilbron FC, 2017, PROC CVPR IEEE, P3175, DOI 10.1109/CVPR.2017.338
   Huang GL, 2017, IEEE ICC
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   LeCun Y., 1995, The handbook of brain theory and neural networks, V3361, DOI [10.5555/303568.303704, DOI 10.5555/303568.303704]
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Ma SG, 2016, PROC CVPR IEEE, P1942, DOI 10.1109/CVPR.2016.214
   McInnes L, 2018, J OPEN SOURCE SOFTWA, V3, P861, DOI [DOI 10.21105/JOSS.00861, 10.21105/joss.00861]
   Mettes P, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P427, DOI 10.1145/2671188.2749404
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Piergiovanni AJ, 2019, PR MACH LEARN RES, V97
   Piergiovanni AJ, 2017, AAAI CONF ARTIF INTE, P4247
   Piergiovanni AJ, 2018, PROC CVPR IEEE, P5304, DOI 10.1109/CVPR.2018.00556
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shou Z, 2018, LECT NOTES COMPUT SC, V11207, P551, DOI 10.1007/978-3-030-01219-9_33
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang KV, 2013, IEEE I CONF COMP VIS, P2696, DOI 10.1109/ICCV.2013.335
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Xiong Y., 2017, CoRR
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu MZ, 2019, IEEE I CONF COMP VIS, P5531, DOI 10.1109/ICCV.2019.00563
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou K, 2016, DESTECH TRANS COMP
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 62
TC 8
Z9 9
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2672
EP 2682
DI 10.1109/TMM.2020.3014555
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600010
DA 2024-07-18
ER

PT J
AU Emami, H
   Aliabadi, MM
   Dong, M
   Chinnam, RB
AF Emami, Hajar
   Aliabadi, Majid Moradi
   Dong, Ming
   Chinnam, Ratna Babu
TI SPA-GAN: Spatial Attention GAN for Image-to-Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Generators; Generative adversarial networks; Task
   analysis; Computational modeling; Training; Computer architecture;
   Image-to-image translation; attention mechanism; generative adversarial
   networks
AB Image-to-image translation is to learn a mapping between images from a source domain and images from a target domain. In this paper, we introduce the attention mechanism directly to the generative adversarial network (GAN) architecture and propose a novel spatial attention GAN model (SPA-GAN) for image-to-image translation tasks. SPA-GAN computes the attention in its discriminator and use it to help the generator focus more on the most discriminative regions between the source and target domains, leading to more realistic output images. We also find it helpful to introduce an additional feature map loss in SPA-GAN training to preserve domain specific features during translation. Compared with existing attention-guided GAN models, SPA-GAN is a lightweight model that does not need additional attention networks or supervision. Qualitative and quantitative comparison against state-of-the-art methods on benchmark datasets demonstrates the superior performance of SPA-GAN.
C1 [Emami, Hajar; Aliabadi, Majid Moradi; Dong, Ming] Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
   [Chinnam, Ratna Babu] Wayne State Univ, Dept Ind & Syst Engn, Detroit, MI 48202 USA.
C3 Wayne State University; Wayne State University
RP Dong, M (corresponding author), Wayne State Univ, Dept Comp Sci, Detroit, MI 48202 USA.
EM hajar.emami.gohari@wayne.edu; majid.moradi.aliabadi@wayne.edu;
   mdong@cs.wayne.edu; Ratna.Chinnam@wayne.edu
RI Chinnam, Ratna Babu/F-5508-2011
OI Chinnam, Ratna Babu/0000-0003-0980-1544
FU US National Science Foundation (NSF) [CNS-1637312]
FX This work was supported in part by US National Science Foundation (NSF)
   under Grant CNS-1637312.
CR [Anonymous], 2017, P ASM 36 INT C OC
   [Anonymous], 2018, P INT C LEARN REPR
   [Anonymous], 2018, NIPS
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Cui XT, 2018, INT CONF COMPUT NETW, P166, DOI 10.1109/ICCNC.2018.8390300
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li WC, 2017, INT CONF ACOUST SPEE, P3156, DOI 10.1109/ICASSP.2017.7952738
   Liu Ming Yu, 2016, ADV NEURAL INF PROCE, P469
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Mo S, 2019, P INT C LEARN REPR
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Rensink RA, 2000, VIS COGN, V7, P17, DOI 10.1080/135062800394667
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang J., 2017, P INT C LEARN REPR
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zagoruyko S., 2017, P INT C LEARN REPR
   Zhang H, 2019, PROCEEDINGS OF 2019 IEEE 3RD INTERNATIONAL ELECTRICAL AND ENERGY CONFERENCE (CIEEC), P735, DOI 10.1109/CIEEC47146.2019.CIEEC-2019293
   Zhang R., 2019, P INT C LEARN REPR
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 41
TC 99
Z9 108
U1 18
U2 117
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 391
EP 401
DI 10.1109/TMM.2020.2975961
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600031
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Kamel, A
   Sheng, B
   Li, P
   Kim, J
   Feng, DD
AF Kamel, Aouaidjia
   Sheng, Bin
   Li, Ping
   Kim, Jinman
   Feng, David Dagan
TI Hybrid Refinement-Correction Heatmaps for Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose estimation; Heating systems; Feature extraction; Predictive models;
   Convolutional neural networks; Detectors; Computer vision; Human pose
   estimation; pose refinement; pose correction; heatmaps fusion
ID DEEP; REPRESENTATION
AB In this paper, we present a method (Hybrid-Pose) to improve human pose estimation in images. We adopt Stacked Hourglass Networks to design two convolutional neural network models, RNet for pose refinement and CNet for pose correction. The CNet (Correction Network) guides the pose refinement RNet (Refinement Network) to correct the joint location before generating the final pose. Each of the two models is composed of four hourglasses, and each hourglass generates a group of detection heatmaps for the joints. The RNet model hourglasses have the same structure. However, the CNet model is designed with hourglasses of different structures for pose guidance. Since the pose estimation in RGB images is very sensitive to the image scene, our proposed approach generates multiple outputs of detection heatmaps to broaden the searching scope for the correct joints locations. We use the RNet model to refine the joints locations in each hourglass stage horizontally, then the heatmaps of each stage are fused with the heatmaps of all the CNet model hourglasses vertically in a hybrid manner. Our method shows competitive results with the existing state-of-the-art approaches on MPII and FLIC benchmark datasets. Although our proposed method focuses on improving single-person pose estimation, we also show the influence of this improvement on multi-person pose estimation by detecting multiple people using SSD detector, then estimating the pose of each person individually.
C1 [Kamel, Aouaidjia; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Kamel, Aouaidjia] Algerian Space Agcy, Space Tech Ctr, Arzew 31200, Algeria.
   [Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Kim, Jinman; Feng, David Dagan] Univ Sydney, Biomed & Multimedia Informat Technol Res Grp, Sch Informat Technol, Sydney, NSW 2006, Australia.
C3 Shanghai Jiao Tong University; Algerian Space Agency (ASAL); Hong Kong
   Polytechnic University; University of Sydney
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM kameldz40@gmail.com; shengbin@sjtu.edu.cn; p.li@polyu.edu.hk;
   jinman.kim@sydney.edu.au; dagan.feng@sydney.edu.au
RI Kim, Jin Man/HJO-8987-2023; Li, Ping/AAO-2019-2020; Kim,
   Jin/AAS-5810-2021
OI Li, Ping/0000-0002-1503-0240; Kim, Jin/0000-0002-7667-9588; KAMEL,
   AOUAIDJIA/0000-0001-6286-9527; Sheng, Bin/0000-0001-8678-2784; kim,
   jinman/0000-0001-5960-1060
FU National Key Research and Development Program of China [2018YFF0300903];
   Science and Technology Commission of ShanghaiMunicipality [18410750700,
   17411952600, 16DZ0501100, 61872241, 61572316]; Hong Kong Polytechnic
   University [P0030419, P0030929]
FX This work was supported in part by the National Key Research and
   Development Program of China (2018YFF0300903), and in part by the
   Science and Technology Commission of ShanghaiMunicipality under Grants
   18410750700, 17411952600, and 16DZ0501100, in part by theNationalNatural
   Science Foundation of China under Grants 61872241 and 61572316, and in
   part by The Hong Kong Polytechnic University under Grants P0030419 and
   P0030929.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen SX, 2018, IEEE T MULTIMEDIA, V20, P2209, DOI 10.1109/TMM.2017.2786869
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Dong L, 2018, IEEE T CIRC SYST VID, V28, P2803, DOI 10.1109/TCSVT.2017.2707477
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ke LP, 2018, LECT NOTES COMPUT SC, V11206, P731, DOI 10.1007/978-3-030-01216-8_44
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan XY, 2005, IEEE I CONF COMP VIS, P470
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ouyang WL, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.299
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun M, 2011, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2011.6126309
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang W, 2018, LECT NOTES COMPUT SC, V11207, P197, DOI 10.1007/978-3-030-01219-9_12
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Tian Yuandong, 2012, Proceedings of the European Conference on Computer Vision, P256
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang Y, 2008, LECT NOTES COMPUT SC, V5304, P710, DOI 10.1007/978-3-540-88690-7_53
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
NR 35
TC 27
Z9 29
U1 4
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1330
EP 1342
DI 10.1109/TMM.2020.2999181
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200012
DA 2024-07-18
ER

PT J
AU Kim, H
   Remaggi, L
   Fowler, S
   Jackson, PJB
   Hilton, A
AF Kim, Hansung
   Remaggi, Luca
   Fowler, Sam
   Jackson, Philip J. B.
   Hilton, Adrian
TI Acoustic Room Modelling Using 360 Stereo Cameras
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Indoor geometry reconstruction; Audio-visual processing; Room acoustic
   modelling; Geometrical acoustics
ID AUDIO; TIME; RECONSTRUCTION; LOCALIZATION; SIMULATION; VOLUME; REAL
AB In this paper we propose a pipeline for estimating acoustic 3D room structure with geometry and attribute prediction using spherical 360. cameras. Instead of setting microphone arrays with loudspeakers to measure acoustic parameters for specific rooms, a simple and practical single-shot capture of the scene using a stereo pair of 360 cameras can be used to simulate those acoustic parameters. We assume that the room and objects can be represented as cuboids aligned to the main axes of the room coordinate (Manhattan world). The scene is captured as a stereo pair using off-the-shelf consumer spherical 360 cameras. A cuboidbased 3D room geometry model is estimated by correspondence matching between captured images and semantic labelling using a convolutional neural network (SegNet). The estimated geometry is used to produce frequency-dependent acoustic predictions of the scene. This is, to our knowledge, the first attempt in the literature to use visual geometry estimation and object classification algorithms to predict acoustic properties. Results are compared to measurements through calculated reverberant spatial audio object parameters used for reverberation reproduction customized to the given loudspeaker set up.
C1 [Kim, Hansung] Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
   [Remaggi, Luca] Creat Labs, London W1F 8WQ, England.
   [Fowler, Sam; Jackson, Philip J. B.; Hilton, Adrian] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Southampton; University of Surrey
RP Kim, H (corresponding author), Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
EM h.kim@soton.ac.uk; luca_remaggi@cle.creative.com;
   sam.fowler@surrey.ac.uk; p.jackson@surrey.ac.uk; a.hilton@surrey.ac.uk
RI Jackson, Philip J B/E-8422-2013; Hilton, Adrian/N-3736-2014
OI Jackson, Philip J B/0000-0001-7933-5935; Hilton,
   Adrian/0000-0003-4223-238X; Kim, Hansung/0000-0003-4907-0491
FU EPSRC Programme Grant S3A: Future Spatial Audio for an Immersive
   Listener Experience at Home [EP/L000539/1]; BBC, BBC Audio Research
   Partnership; Audio-Visual Media Research Platform [EP/P022529/1]; EPSRC
   [EP/V038087/1, EP/M028321/1, EP/P022529/1, EP/L000539/1] Funding Source:
   UKRI
FX This work was supported in part by EPSRC Programme Grant S3A: Future
   Spatial Audio for an Immersive Listener Experience at Home
   (EP/L000539/1), in part by the BBC as part of the BBC Audio Research
   Partnership, and in part by Audio-Visual Media Research Platform
   (EP/P022529/1) The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Jian Zhang.
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2012, ITURBS7753
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bailey W, 2018, P 144 AES CONV MIL I
   Beranek LL, 2006, J ACOUST SOC AM, V120, P1399, DOI 10.1121/1.2221392
   Bilbao S, 2013, IEEE T AUDIO SPEECH, V21, P1524, DOI 10.1109/TASL.2013.2256897
   BORISH J, 1984, J ACOUST SOC AM, V75, P1827, DOI 10.1121/1.390983
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Cheng I, 2009, IEEE MULTIMEDIA, V16, P16, DOI 10.1109/MMUL.2009.11
   Choi S, 2015, PROC CVPR IEEE, P5556, DOI 10.1109/CVPR.2015.7299195
   Coleman P., 2017, P 142 AES CONV BERL
   Coleman P, 2018, IEEE T MULTIMEDIA, V20, P1919, DOI 10.1109/TMM.2018.2794780
   Coleman P, 2017, J AUDIO ENG SOC, V65, P66, DOI 10.17743/jaes.2016.0059
   Cucharero J, 2019, ACOUSTICS-BASEL, V1, P644, DOI 10.3390/acoustics1030038
   D'Antonio P., 2016, ACOUSTIC ABSORBERS D, V3rd
   Dai A, 2020, PROC CVPR IEEE, P846, DOI 10.1109/CVPR42600.2020.00093
   De Sena E, 2015, IEEE-ACM T AUDIO SPE, V23, P1478, DOI 10.1109/TASLP.2015.2438547
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Farina Angelo, 2000, 108 AES CONVENTION
   Furukawa Y, 2009, IEEE I CONF COMP VIS, P80, DOI 10.1109/ICCV.2009.5459145
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gonzalez R., 2017, DIGITAL IMAGE PROCES
   Gupta A, 2010, LECT NOTES COMPUT SC, V6314, P482, DOI 10.1007/978-3-642-15561-1_35
   Gupta S, 2015, INT J COMPUT VISION, V112, P133, DOI 10.1007/s11263-014-0777-6
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Hao Q, 2013, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2013.121
   Huang XS, 2015, LECT NOTES COMPUT SC, V9315, P14, DOI 10.1007/978-3-319-24078-7_2
   Im S, 2016, LECT NOTES COMPUT SC, V9907, P156, DOI 10.1007/978-3-319-46487-9_10
   Judd DB, 1932, J OPT SOC AM, V22, P72, DOI 10.1364/JOSA.22.000072
   Kim H, 2005, SIGNAL PROCESS-IMAGE, V20, P61, DOI 10.1016/j.image.2004.10.004
   Kim H., 2017, P 142 AES
   Kim HS, 2016, INT CONF 3D VISION, P519, DOI 10.1109/3DV.2016.83
   Kim H, 2015, COMPUT VIS IMAGE UND, V139, P104, DOI 10.1016/j.cviu.2015.04.001
   Kim H, 2013, INT J COMPUT VISION, V104, P94, DOI 10.1007/s11263-013-0616-1
   Kowalczyk K, 2011, IEEE T AUDIO SPEECH, V19, P34, DOI 10.1109/TASL.2010.2045179
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   KROKSTAD A, 1968, J SOUND VIB, V8, P118, DOI 10.1016/0022-460X(68)90198-3
   Kuster M, 2008, J ACOUST SOC AM, V124, P982, DOI 10.1121/1.2940585
   Kwon SW, 2004, AUTOMAT CONSTR, V13, P67, DOI 10.1016/j.autcon.2003.08.007
   Li ML, 2016, INT J DIGIT EARTH, V9, P806, DOI 10.1080/17538947.2016.1143982
   Li SG, 2006, INT C PATT RECOG, P1046
   Lindau A, 2012, J AUDIO ENG SOC, V60, P887
   Lindau A, 2012, ACTA ACUST UNITED AC, V98, P804, DOI 10.3813/AAA.918562
   Liu JJ, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107378
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Long J., 2015, PROC C COMPUT VIS PA
   Matzen K, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073645
   Meng ZH, 2006, 2006 INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES,VOLS 1-3, P468
   Menzies D., 2016, P AES CONV
   MIDDLEBROOKS JC, 1991, ANNU REV PSYCHOL, V42, P135, DOI 10.1146/annurev.ps.42.020191.001031
   Murphy D, 2007, IEEE SIGNAL PROC MAG, V24, P55, DOI 10.1109/MSP.2007.323264
   Naylor PA, 2007, IEEE T AUDIO SPEECH, V15, P34, DOI 10.1109/TASL.2006.876878
   Neidhardt Annika, 2018, 144 INT AES CONV MIL, P1
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Nguatem W, 2012, INT ARCH PHOTOGRAMM, V39-B3, P149
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ohta Y., 2014, MIXED REALITY MERGIN
   Petridis S, 2016, INT CONF ACOUST SPEE, P2304, DOI 10.1109/ICASSP.2016.7472088
   Pike C., 2014, P AES C
   Pike C., 2017, P AUDIO ENG SOC CONV, V142
   Poletti MA, 2015, J AUDIO ENG SOC, V63, P31, DOI 10.17743/jaes.2015.0003
   Porschmann C., 2017, P 20 INT C DIG AUD E
   Remaggi L, 2015, P 138 AES CONV
   Remaggi L, 2017, IEEE-ACM T AUDIO SPE, V25, P296, DOI 10.1109/TASLP.2016.2633802
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salas-Moreno RF, 2013, PROC CVPR IEEE, P1352, DOI 10.1109/CVPR.2013.178
   Savioja L, 2003, IEEE T SPEECH AUDI P, V11, P783, DOI 10.1109/TSA.2003.818028
   Savioja L, 2015, J ACOUST SOC AM, V138, P708, DOI 10.1121/1.4926438
   Schönbein M, 2014, IEEE INT C INT ROBOT, P716, DOI 10.1109/IROS.2014.6942637
   Silberman N, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Siltanen S, 2008, ACTA ACUST UNITED AC, V94, P410, DOI 10.3813/AAA.918049
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Sinha SN, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409112
   Song S., 2019, P C COMP VIS PATT RE
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Song X, 2020, INT J COMPUT VISION, V128, P910, DOI 10.1007/s11263-019-01287-w
   Stenzel H., 2017, P AES CONV
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Välimäki V, 2012, IEEE T AUDIO SPEECH, V20, P1421, DOI 10.1109/TASL.2012.2189567
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   VORLANDER M, 1989, J ACOUST SOC AM, V86, P172, DOI 10.1121/1.398336
   Wang AR, 2015, IEEE T IMAGE PROCESS, V24, P4459, DOI 10.1109/TIP.2015.2465133
   Wang R, 2017, IEEE ICC
   Wang SX, 2013, IEEE T MULTIMEDIA, V15, P870, DOI 10.1109/TMM.2013.2240674
   Won C, 2019, IEEE I CONF COMP VIS, P8986, DOI 10.1109/ICCV.2019.00908
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012
NR 92
TC 3
Z9 3
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4117
EP 4130
DI 10.1109/TMM.2020.3037537
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900016
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ren, QH
   Lu, SJ
   Zhang, JX
   Hu, RJ
AF Ren, Qinghua
   Lu, Shijian
   Zhang, Jinxia
   Hu, Renjie
TI Salient Object Detection by Fusing Local and Global Contexts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Computational modeling; Visualization; Task
   analysis; Predictive models; Object detection; Context modeling; Deep
   learning; contextual information; visual attention; salient object
   detection
ID SEGMENTATION; MODEL
AB Benefiting from the powerful discriminative feature learning capability of convolutional neural networks (CNNs), deep learning techniques have achieved remarkable performance improvement for the task of salient object detection (SOD) in recent years. However, most existing deep SOD models do not fully exploit informative contextual features, which often leads to suboptimal detection performance in the presence of a cluttered background. This paper presents a context-aware attention module that detects salient objects by simultaneously constructing connections between each image pixel and its local and global contextual pixels. Specifically, each pixel and its neighbors bidirectionally exchange semantic information by computing their correlation coefficients, and this process aggregates contextual attention features both locally and globally. In addition, an attention-guided hierarchical network architecture is designed to capture fine-grained spatial details by transmitting contextual information from deeper to shallower network layers in a top-down manner. Extensive experiments on six public SOD datasets show that our proposed model demonstrates superior SOD performance against most of the current state-of-the-art models under different evaluation metrics.
C1 [Ren, Qinghua; Hu, Renjie] Southeast Univ, Sch Elect Engn, Nanjing 210096, Peoples R China.
   [Lu, Shijian] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Zhang, Jinxia] Southeast Univ, Sch Automat, Nanjing 210096, Peoples R China.
C3 Southeast University - China; Nanyang Technological University;
   Southeast University - China
RP Lu, SJ (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM renqinghua@seu.edu.cn; shijian.lu@ntu.edu.sg; jinxiazhang99@163.com;
   hurenjie@seu.edu.cn
RI Lu, Shijian/AAU-4831-2021
OI Lu, Shijian/0000-0002-6766-2506
FU China Scholarship Council [201906090194]; NTU Start-up [M4082034];
   National Natural Science Fund of China [61703100]; Natural Science
   Foundation of Jiangsu [BK20170692]; Fundamental Research Funds for the
   Central Universities; Big Data Computing Center of Southeast University
FX This work was supported in part by the Scholarship from China
   Scholarship Council under Grant 201906090194, in part by the NTU
   Start-up under Grant M4082034, in part by the National Natural Science
   Fund of China under Grant 61703100, in part by the Natural Science
   Foundation of Jiangsu under Grant BK20170692, in part by the Fundamental
   Research Funds for the Central Universities, and in part by the Big Data
   Computing Center of Southeast University.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Z., 2017, P IEEE C COMP VIS PA, P4021, DOI DOI 10.1109/CVPR.2017.510
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kingma D. P., 2014, arXiv
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu SJ, 2012, LECT NOTES COMPUT SC, V7578, P321, DOI 10.1007/978-3-642-33786-4_24
   Lu SJ, 2014, IEEE T PATTERN ANAL, V36, P195, DOI 10.1109/TPAMI.2013.158
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi JP, 2016, IEEE T PATTERN ANAL, V38, P717, DOI 10.1109/TPAMI.2015.2465960
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Thiry L, 2018, IEEE INT CONGR BIG, P272, DOI 10.1109/BigDataCongress.2018.00049
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu CY, 2016, FIFTH INTERNATIONAL CONFERENCE ON EDUCATIONAL INNOVATION THROUGH TECHNOLOGY (EITT 2016), P96, DOI 10.1109/EITT.2016.26
   Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang W, 2016, IEEE T NEUR NET LEAR, V27, P1266, DOI 10.1109/TNNLS.2015.2461603
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
NR 57
TC 36
Z9 36
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1442
EP 1453
DI 10.1109/TMM.2020.2997178
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200020
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Virtusio, JJ
   Ople, JJM
   Tan, DS
   Tanveer, M
   Kumar, N
   Hua, KL
AF Virtusio, John Jethro
   Ople, Jose Jaena Mari
   Tan, Daniel Stanley
   Tanveer, M.
   Kumar, Neeraj
   Hua, Kai-Lung
TI Neural Style Palette: A Multimodal and Interactive Style Transfer From a
   Single Style Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Feature extraction; Visualization; Space
   exploration; Task analysis; User interfaces; Training; Interactive Style
   Transfer; Hybrid Human-Artificial Intelligence; Anchor Styles
AB Despite the myriad of attributes found in a single style image, existing neural style transfer methods produce outputs with limited variety-typically only a single realization of the style image. They also do not provide an easy way to control the stylization process, limiting the creative freedom of users. In this paper, we propose Neural Style Palette (NSP), a method for interactively generating a variety of stylized images from only a single style input. Our approach allows human influence in the stylization process, a design inspired by Hybrid Human-Artificial Intelligence. Like a color palette, NSP enables a meaningful interaction by presenting a collection of sub-textures, which we also refer to as anchor styles, that act as a visual guide for the users. These anchor styles capture different attributes in the single style image that the users can creatively blend to create their desired realizations. To offer a diversified selection in the NSP, we constrain the anchor styles to be distant from one another while maintaining faithfulness to the original style image. This is possible through our two proposed novel losses: a style-separation loss that encourages the sub-textures to be distinct and a unification loss to ensure that the sub-textures center around the original style while encouraging additional diversity. We perform several experiments to prove the effectiveness of our method and generalize to improve existing methods.
C1 [Virtusio, John Jethro; Ople, Jose Jaena Mari; Tan, Daniel Stanley; Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, CSIE, Taipei 106, Taiwan.
   [Tanveer, M.] Indian Inst Technol Indore, Dept Math, Indore 453552, Madhya Pradesh, India.
   [Kumar, Neeraj] Thapar Inst Engn & Technol Deemed Univ, Dept Comp Sci & Engn, Patiala 147004, Punjab, India.
C3 National Taiwan University of Science & Technology; Indian Institute of
   Technology System (IIT System); Indian Institute of Technology (IIT) -
   Indore; Thapar Institute of Engineering & Technology
RP Hua, KL (corresponding author), Natl Taiwan Univ Sci & Technol, CSIE, Taipei 106, Taiwan.
EM D10715811@mail.ntust.edu.tw; D10815808@mail.ntust.edu.tw;
   D10515805@mail.ntust.edu.tw; mtanveer@iiti.ac.in;
   neeraj.kumar@thapar.edu; hua@mail.ntust.edu.tw
RI Kumar, Neeraj/L-3500-2016; Tanveer, Mohammad/I-4585-2013
OI Kumar, Neeraj/0000-0002-3020-3947; Tanveer,
   Mohammad/0000-0002-5727-3697; Tan, Daniel Stanley/0000-0002-8071-9060;
   Hua, Kai-Lung/0000-0002-7735-243X
FU Center of Intelligent Robots from The Featured Areas Research Center
   Program within the framework of the Higher Education Sprout Project by
   the Ministry of Education (MOE) in Taiwan; Ministry of Science and
   Technology of Taiwan [MOST109-2218-E-011-010,
   MOST109-2221-E-011-125-MY3, MOST109-2923-E-011-010]
FX This work was supported in part by the Center for Cyber-physical System
   Innovation and Center of Intelligent Robots from The Featured Areas
   Research Center Program within the framework of the Higher Education
   Sprout Project by the Ministry of Education (MOE) in Taiwan and in part
   by Ministry of Science and Technology of Taiwan under Grants
   MOST109-2218-E-011-010, MOST109-2221-E-011-125-MY3,
   MOST109-2923-E-011-010.
CR [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], 2017, P INTERNA TIONAL C L
   [Anonymous], 2015, CHANGINGCROPPINGPATT
   [Anonymous], 2018, ECCV 2018
   Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Gabbay A., 2019, P INT C LEARN REPR
   Gatys L., 2015, NIPS
   Gatys L.A, 2017, CVPR, P3985
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hidayati SC, 2021, IEEE T MULTIMEDIA, V23, P365, DOI 10.1109/TMM.2020.2980195
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Jing YC, 2018, LECT NOTES COMPUT SC, V11217, P244, DOI 10.1007/978-3-030-01261-8_15
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kamar E., 2016, IJCAI, P4070
   Kingma D. P., 2014, arXiv
   Kolkin N, 2019, PROC CVPR IEEE, P10043, DOI 10.1109/CVPR.2019.01029
   Kotovenko D, 2019, IEEE I CONF COMP VIS, P4421, DOI 10.1109/ICCV.2019.00452
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee HD, 2020, INT J OFFENDER THER, V64, P1299, DOI 10.1177/0306624X20909238
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li K, 2019, IEEE I CONF COMP VIS, P3582, DOI 10.1109/ICCV.2019.00368
   Li Y., 2017, Universal style transfer via feature transforms, P386
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36
   Liu X., 2020, P IEEE CVF C COMP VI, P8057
   Lu M, 2019, IEEE I CONF COMP VIS, P5951, DOI 10.1109/ICCV.2019.00605
   Mechrez R., 2018, P EUR C COMP VIS ECC, P768
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Nigam I, 2019, IEEE I CONF COMP VIS, P402, DOI 10.1109/ICCV.2019.00049
   Park Taesung, 2020, Advances in Neural Information Processing Systems, V33, P7198
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860
   Ulyanov D, 2017, PROC CVPR IEEE, P4105, DOI 10.1109/CVPR.2017.437
   Veit A., 2017, CVPR, P830
   Virtusio J. J., 2018, P IEEE VIS COMM IM P, P1
   Virtusio J. J., IEEE T MULTIMEDIA, DOI [10.1109/TMM.2020.3009484, DOI 10.1109/TMM.2020.3009484]
   Wang X, 2017, P IEEE C COMP VIS PA, P5239
   Yao Y, 2019, PROC CVPR IEEE, P1467, DOI 10.1109/CVPR.2019.00156
   Zhang YL, 2019, IEEE I CONF COMP VIS, P5942, DOI 10.1109/ICCV.2019.00604
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 21
Z9 21
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2245
EP 2258
DI 10.1109/TMM.2021.3087026
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800007
DA 2024-07-18
ER

PT J
AU Wu, F
   Yang, W
   Ren, J
   Lyu, F
   Yang, P
   Zhang, YX
   Shen, XM
AF Wu, Fan
   Yang, Wang
   Ren, Ju
   Lyu, Feng
   Yang, Peng
   Zhang, Yaoxue
   Shen, Xuemin
TI <i>NDN-MMRA</i>: Multi-Stage Multicast Rate Adaptation in Named Data
   Networking WLAN
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Wireless LAN; IEEE 802; 11 Standard; Reliability; Wireless
   communication; IP networks; Multicast protocols; Streaming media; Media
   content delivery; named data networking; WLAN; multicast rate
   adaptation; dynamic multicast
ID VIDEO; TRANSPORT; ENERGY
AB Named Data Networking (NDN) is considered as a prominent architecture towards future Wireless Local Area Networks (WLAN), and multicast plays an important role in data delivery such as media streaming, multipoint videoconferencing, etc. However, to achieve high-efficiency multicast in NDN WLAN is challenging for two significant reasons. First, without feedback mechanism in IEEE 802.11 standards, to guarantee reliability, the current multicast scheme transmits the multicast data with the basic rate (e.g., 1 Mbps for IEEE 802.11b), which inevitably increases the transmission delay for high-speed consumers. Second, as a NDN multicast group is constituted by consumers who are requesting the same content, multicast groups are easy to form and evolve rapidly, where a data rate adaptation scheme is requisite to accommodate differential multicast groups. In this paper, we propose a multi-stage multicast rate adaptation scheme for NDN WLAN, named NDN-MMRA, to minimize the total transmission time with reliability guarantee for multicast group members. In NDN-MMRA, by checking the Pending Interest Table (PIT) status information, the number of consumers in each multicast group as well as their receiving capabilities are known ahead; with the available data rates in a specific 802.11 standard, NDN-MMRA determines: 1) how many transmission stages are required; and 2) in each stage, which data rate should be adopted. The merit is that with multi-stage transmissions, the data rate can be adapted in descending order to accommodate high-speed consumers with delay minimized, and low-speed consumers with reliability guaranteed. We implement NDN-MMRA in NS-3 by adopting the ndnSIM module, and conduct extensive experiments to demonstrate its efficacy under different IEEE 802.11 standards and various underlying WLAN topologies.
C1 [Wu, Fan; Yang, Wang; Ren, Ju; Lyu, Feng; Zhang, Yaoxue] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
   [Yang, Peng] Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Hubei, Peoples R China.
   [Zhang, Yaoxue] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Shen, Xuemin] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 Central South University; Huazhong University of Science & Technology;
   Tsinghua University; University of Waterloo
RP Yang, W (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
EM wfwufan@csu.edu.cn; yangwang@csu.edu.cn; renju@csu.edu.cn;
   fenglyu@csu.edu.cn; yangpeng@hust.edu.cn; zhangyx@tsinghua.edu.cn;
   sshen@uwaterloo.ca
RI Ren, Ju/ABD-5213-2021; yang, wang/KRP-3067-2024; Wu, Fan/J-9583-2019;
   Shen, Xuemin/AAH-2564-2020; Ren, Ju/ABD-5251-2021
OI Wu, Fan/0000-0003-3615-1217; Shen, Xuemin/0000-0002-4140-287X; Ren,
   Ju/0000-0003-2782-183X; Yang, Wang/0000-0003-0774-9762
FU National Natural Science Foundation of China [62072474, 62072472,
   62002389, 61702562, U19A2067]; National Key R&D Program of China
   [2019YFA0706403]; 111 Project [B18059]; Young Elite Scientists
   Sponsorship Program by CAST [2018QNRC001]; Young Talents Plan of Hunan
   Province of China [2019RS2001]; Natural Sciences and Engineering
   Research Council (NSERC) of Canada
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072474, 62072472, 62002389, 61702562,
   and U19A2067, in part by the National Key R&D Program of China under
   Grant 2019YFA0706403, in part by 111 Project under Grant B18059, in part
   by the Young Elite Scientists Sponsorship Program by CAST under Grant
   2018QNRC001, in part by the Young Talents Plan of Hunan Province of
   China under Grant 2019RS2001, and in part by Natural Sciences and
   Engineering Research Council (NSERC) of Canada. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Shaoen Wu.
CR [Anonymous], 2016, PROC IEEE INFOCOM 35
   Ascigil O, 2018, PROCEEDINGS OF THE 5TH ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ICN'18), P67, DOI 10.1145/3267955.3267968
   Ben Makhlouf A, 2013, IEEE T WIREL COMMUN, V12, P908, DOI 10.1109/TWC.2013.13.120626
   Bourtsoulatze E, 2018, IEEE T MULTIMEDIA, V20, P1561, DOI 10.1109/TMM.2017.2767778
   Bukhari J, 2018, IEEE T BROADCAST, V64, P915, DOI 10.1109/TBC.2018.2834720
   C.V.N. Index, 2019, CISC VIS NETW IND GL
   Chandra R, 2009, I C NETWORK PROTOCOL, P161, DOI 10.1109/ICNP.2009.5339686
   Cheng N, 2018, IEEE NETWORK, V32, P160, DOI 10.1109/MNET.2018.1700460
   Daldoul Y, 2015, COMPUT NETW, V79, P236, DOI 10.1016/j.comnet.2015.01.008
   Fu LY, 2018, IEEE ACM T NETWORK, V26, P633, DOI 10.1109/TNET.2018.2790639
   Geithner T, 2018, IEEE WCNC
   Ghasemi C., 2018, P ICNP CAMBR UK, P1
   Go YM, 2018, IEEE T MOBILE COMPUT, V17, P433, DOI 10.1109/TMC.2017.2721947
   Gong W, 2018, IEEE INFOCOM SER, P1259, DOI 10.1109/INFOCOM.2018.8485845
   Gupta V, 2018, IEEE T WIREL COMMUN, V17, P2319, DOI 10.1109/TWC.2018.2791605
   Joseph V, 2012, IEEE INFOCOM SER, P567, DOI 10.1109/INFCOM.2012.6195799
   Lee G, 2017, IEEE INFOCOM SER
   Lim H, 2018, IEEE NETWORK, V32, P124, DOI 10.1109/MNET.2018.1800088
   Lim WS, 2012, IEEE T MOBILE COMPUT, V11, P780, DOI 10.1109/TMC.2011.95
   Lyu F, 2020, IEEE INFOCOM SER, P1181, DOI [10.1109/INFOCOM41043.2020.9155413, 10.1109/infocom41043.2020.9155413]
   Lyu F, 2021, IEEE T MOBILE COMPUT, V20, P2607, DOI 10.1109/TMC.2020.2984261
   Lyu F, 2021, IEEE T INTELL TRANSP, V22, P1248, DOI 10.1109/TITS.2020.2966586
   Ma G, 2017, IEEE J SEL AREA COMM, V35, P1076, DOI 10.1109/JSAC.2017.2680958
   Mastorakis S, 2017, ACM SIGCOMM COMP COM, V47
   Ren J, 2019, IEEE T VEH TECHNOL, V68, P1578, DOI 10.1109/TVT.2018.2888635
   Ren J, 2019, IEEE T EMERG TOP COM, V7, P149, DOI 10.1109/TETC.2016.2555806
   Ren J, 2016, IEEE T WIREL COMMUN, V15, P3143, DOI 10.1109/TWC.2016.2517618
   Saltarin J, 2017, IEEE T MULTIMEDIA, V19, P2182, DOI 10.1109/TMM.2017.2737950
   Samain J, 2017, IEEE T MULTIMEDIA, V19, P2166, DOI 10.1109/TMM.2017.2733340
   Shin Y, 2017, P IEEE SECON, P1
   Stais C, 2015, J NETW COMPUT APPL, V50, P92, DOI 10.1016/j.jnca.2014.06.006
   Ucar I, 2018, COMPUT COMMUN, V117, P164, DOI 10.1016/j.comcom.2017.07.002
   Vella JM, 2013, IEEE COMMUN SURV TUT, V15, P718, DOI 10.1109/SURV.2012.050412.00095
   Westphal C., 2018, P ACM ICN BOST US, P1
   Wu F, 2020, IEEE T VEH TECHNOL, V69, P901, DOI 10.1109/TVT.2019.2952665
   Wu F, 2018, 2018 27TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN)
   Wu F, 2016, MOBIHOC '16: PROCEEDINGS OF THE 17TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P191, DOI 10.1145/2942358.2942362
   Yang P, 2019, IEEE T MULTIMEDIA, V21, P915, DOI 10.1109/TMM.2018.2870521
   Zhang LX, 2014, ACM SIGCOMM COMP COM, V44, P66, DOI 10.1145/2656877.2656887
   Zhang Y, 2018, PROCEEDINGS OF THE 5TH ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ICN'18), P125, DOI 10.1145/3267955.3267959
   Zhang Z, 2020, IEEE T MULTIMEDIA, V22, P1069, DOI 10.1109/TMM.2019.2935683
   Zhou B, 2017, IEEE T COMMUN, V65, P2956, DOI 10.1109/TCOMM.2017.2699958
   Zuo S, 2017, IEEE T WIREL COMMUN, V16, P4562, DOI 10.1109/TWC.2017.2700302
NR 43
TC 16
Z9 16
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3250
EP 3263
DI 10.1109/TMM.2020.3023282
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000024
DA 2024-07-18
ER

PT J
AU Zhang, WD
   Zhang, Q
   Zhang, W
   Gu, JJ
   Li, YB
AF Zhang, Weidong
   Zhang, Qian
   Zhang, Wei
   Gu, Jianjun
   Li, Yibin
TI From Edge to Keypoint: An End-to-End Framework For Indoor Layout
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Image edge detection; Training; Estimation; Semantics; Task
   analysis; Gallium nitride; Generative adversarial network; indoor layout
   estimation; scene understanding
ID FUSION
AB The task of spatial layout estimation of monocular image is to segment an RGB image of indoor scenes with semantic surface labels (i.e., ceiling, floor, front wall, left wall, and right wall). Most recent methods have to produce layout hypotheses based on the estimated edge map or semantic labels, and then rank the layout hypotheses. In this paper, we present an end-to-end framework that can directly output the layout type and keypoint coordinates (defined in the LSUN challenge). The proposed method takes advantage of transfer learning via learning on the fake samples, i.e., plenty of artificial {type, keypoints, edge map} triplets are generated to learn the mapping from edge maps to keypoint coordinates. Generative adversarial network (GAN) is implemented in this work for domain adaptation of the edge maps. Experimental results show that the proposed method can achieve state-of-the-art layout estimation performance on benchmark datasets.
C1 [Zhang, Weidong] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Zhang, Weidong] Xian Univ Posts & Telecommun, Sch Commun & Informat Engn, Xian 710100, Peoples R China.
   [Zhang, Qian; Zhang, Wei; Gu, Jianjun; Li, Yibin] Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Shandong, Peoples R China.
C3 Shandong University; Xi'an University of Posts & Telecommunications;
   Shandong University
RP Zhang, W (corresponding author), Shandong Univ, Sch Control Sci & Engn, Jinan 250061, Shandong, Peoples R China.
EM chluzhre@gmail.com; zhq9669@gmail.com; davidzhang@sdu.edu.cn;
   gujianjunsdu@gmail.com; liyb@sdu.edu.cn
OI Li, Yibin/0000-0002-5906-5074
FU National Natural Science Foundation of China [61991411, U1913204];
   National Key Research and Development Plan of China [2018AAA0102504];
   Shandong Major Scientific and Technological Innovation Project (MSTIP)
   [2018CXGC1503]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61991411, and U1913204, in part by
   theNationalKey Research andDevelopment Plan of China under Grant
   2018AAA0102504, and in part by the Shandong Major Scientific and
   Technological Innovation Project (MSTIP) under Grant 2018CXGC1503. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2016, Asian Conference on Computer Vision
   [Anonymous], 2010, NIPS
   Camplani M, 2013, IEEE T CYBERNETICS, V43, P1560, DOI 10.1109/TCYB.2013.2271112
   Coughlan JM, 2001, ADV NEUR IN, V13, P845
   Dasgupta S, 2016, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2016.73
   Del Pero L, 2013, PROC CVPR IEEE, P153, DOI 10.1109/CVPR.2013.27
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hedau V, 2010, LECT NOTES COMPUT SC, V6316, P224, DOI 10.1007/978-3-642-15567-3_17
   Hedau V, 2009, IEEE I CONF COMP VIS, P1849, DOI 10.1109/ICCV.2009.5459411
   Hirzer M, 2020, IEEE WINT CONF APPL, P2901, DOI [10.1109/WACV45572.2020.9093451, 10.1109/wacv45572.2020.9093451]
   Hoiem D, 2007, INT J COMPUT VISION, V75, P151, DOI 10.1007/s11263-006-0031-y
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Izadinia H, 2017, PROC CVPR IEEE, P2422, DOI 10.1109/CVPR.2017.260
   Joseph SL, 2013, IEEE SYS MAN CYBERN, P3585, DOI 10.1109/SMC.2013.611
   Kruzhilov I., 2019, ASIAN C PATTERN RECO, P557
   Lee CY, 2017, IEEE I CONF COMP VIS, P4875, DOI 10.1109/ICCV.2017.521
   Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872
   Liu CX, 2015, PROC CVPR IEEE, P3413, DOI 10.1109/CVPR.2015.7298963
   Liu ZG, 2017, IEEE T MULTIMEDIA, V19, P874, DOI 10.1109/TMM.2016.2636750
   Mallya A, 2015, IEEE I CONF COMP VIS, P936, DOI 10.1109/ICCV.2015.113
   Martin-Brualla R, 2014, LECT NOTES COMPUT SC, V8691, P1, DOI [10.7749/citiescommunitiesterritories.dec2014.029.art01, 10.1007/978-3-319-10578-9_1]
   Radford A., 2015, ARXIV151106434
   Ramalingam S, 2013, PROC CVPR IEEE, P3065, DOI 10.1109/CVPR.2013.394
   Schwing AG, 2012, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2012.6248006
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Weidong Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P632, DOI 10.1007/978-3-030-58517-4_37
   Xiao JX, 2012, LECT NOTES COMPUT SC, V7572, P668, DOI [10.1007/s11263-014-0711-y, 10.1007/978-3-642-33718-5_48]
   Yan C., 2020, IEEE T PATTERNANAL M
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Zhang WD, 2020, IEEE T CYBERNETICS, V50, P2730, DOI 10.1109/TCYB.2019.2895837
   Zhang WD, 2017, IEEE T MULTIMEDIA, V19, P935, DOI 10.1109/TMM.2016.2642780
   Zhang Y., LARGESCALE SCENE UND
   Zhao H, 2017, PROC CVPR IEEE, P870, DOI 10.1109/CVPR.2017.99
   Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401
NR 38
TC 4
Z9 4
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4483
EP 4490
DI 10.1109/TMM.2020.3042669
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800007
DA 2024-07-18
ER

PT J
AU Sadat, MN
   Dai, R
   Kong, LC
   Zhu, JY
AF Sadat, Mohammad Nazmus
   Dai, Rui
   Kong, Lingchao
   Zhu, Jingyi
TI QoE-Aware Multi-Source Video Streaming in Content Centric Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Quality of experience; Switches; Video coding; Video
   recording; Quality assessment; Static VAr compensators; Adaptive video
   streaming; quality of experience (QoE); content centric networking;
   in-network caching; scalable video coding
AB Content Centric Networking (CCN), a future Internet architecture, brings new challenges in maintaining the Quality of Experience (QoE) for video streaming. Because of the universal caching capability of CCN routers, streaming from multiple sources will be common, and switching between content sources might affect QoE by inducing delays and consequently stalls in video playback. This paper proposes a new QoE-aware multi-source video streaming scheme for CCN. First, the content distributions of video files among CCN nodes for different caching methods are studied. Second, an adaptive video streaming with distributed caching (ASDC) algorithm is designed to guarantee QoE during the switching between content sources. The ASDC algorithm considers the delivery of scalable video streams. It automatically adapts the layers in a video stream when there is source switching, based on a QoE model that characterizes the effect of stalling. Experimental results show that the ASDC algorithm outperforms dynamic adaptive streaming over HTTP (DASH) in the CCN platform in terms of the QoE obtained from human subjective tests.
C1 [Sadat, Mohammad Nazmus; Dai, Rui; Kong, Lingchao; Zhu, Jingyi] Univ Cincinnati, Dept Elect Engn & Comp Sci, Cincinnati, OH 45221 USA.
C3 University System of Ohio; University of Cincinnati
RP Sadat, MN (corresponding author), Univ Cincinnati, Dept Elect Engn & Comp Sci, Cincinnati, OH 45221 USA.
EM sadatms@mail.uc.edu; rui.dai@uc.edu; konglo@mail.uc.edu;
   zhujy@mail.uc.edu
RI Sadat, Mohammad Nazmus/AAP-5015-2021
OI Sadat, Mohammad Nazmus/0000-0002-4482-8462; Dai, Rui/0000-0001-6620-7862
FU National Science Foundation [CNS-1644946]; National Institute of
   Standards and Technology [60NANB17D193]
FX This work was supported in part by the National Science Foundation under
   Grant CNS-1644946 and in part by the National Institute of Standards and
   Technology under Grant 60NANB17D193.
CR [Anonymous], 2012, 264SVC ENCOD
   [Anonymous], 2016, POLAR BIOL 2, DOI DOI 10.1007/S00300-015-1654-7
   [Anonymous], 2015, 2015 IEEE C STAND
   [Anonymous], 2011, FOR MEN
   [Anonymous], 2012, ITURBT50013 INT TEL
   [Anonymous], 2014, IEEE I C NETW INFRAS
   [Anonymous], 2012, JICA KMC
   [Anonymous], 2011, DECREE CHAIRPERSON A
   [Anonymous], 2014, 2014 IF NETW C
   [Anonymous], 2017, 2017 31 IEEE INT C, DOI DOI 10.1109/WAINA.2017.95
   Araldo A., 2014, P 1 ACM C INF CENTR, P147
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Chai WK, 2012, LECT NOTES COMPUT SC, V7289, P27, DOI 10.1007/978-3-642-30045-5_3
   Chiocchetti R, 2013, IEEE ICC
   de la Fuente Y. Sanchez, 2011, P 2 ANN ACM C MULT S, P257
   Van DD, 2016, 2016 IEEE INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC), P843, DOI 10.1109/ITNEC.2016.7560480
   Duanmu ZF, 2017, IEEE J-STSP, V11, P154, DOI 10.1109/JSTSP.2016.2608329
   Finamore A., 2011, P ACM SIGCOMM C INT, P345
   Garetto M., 2016, ACM T MODEL PERFORM, V1, P3
   Huysegems R, 2012, BELL LABS TECH J, V16, P25, DOI 10.1002/bltj.20532
   Jacobson V, 2012, COMMUN ACM, V55, P117, DOI 10.1145/2063176.2063204
   Janowski L, 2009, INT WORK QUAL MULTIM, P35, DOI 10.1109/QOMEX.2009.5246979
   Kong LC, 2017, IEEE INT SYM MULTIM, P399, DOI 10.1109/ISM.2017.80
   Koster F., 2017, P 9 INT C QUAL MULT, P1
   Lederer S., 2012, P 3 MULT SYST C, P89
   Liu Z., 2016, P IEEE INT C COMM SY, P1
   Mau DO, 2015, IEEE GLOB COMM CONF, DOI 10.1109/GLOCOM.2015.7417045
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Psaras I., 2012, P 2 ICN WORKSH INF C, P55
   Rossini G, 2013, COMPUT COMMUN, V36, P771, DOI 10.1016/j.comcom.2013.01.008
   Saltarin J, 2017, IEEE T MULTIMEDIA, V19, P2182, DOI 10.1109/TMM.2017.2737950
   Samain J, 2017, IEEE T MULTIMEDIA, V19, P2166, DOI 10.1109/TMM.2017.2733340
   Sanchez Y, 2012, SIGNAL PROCESS-IMAGE, V27, P329, DOI 10.1016/j.image.2011.10.002
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Singh KD, 2012, CONSUM COMM NETWORK, P127, DOI 10.1109/CCNC.2012.6181070
   Traverso S, 2013, ACM SIGCOMM COMP COM, V43, P6
   V. Cisco, 2017, CISC VIS NETW IND GL, P2016
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiph.Org, 2014, VID TEST MED COLL
   Xylomenos G, 2014, IEEE COMMUN SURV TUT, V16, P1024, DOI 10.1109/SURV.2013.070813.00063
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
NR 42
TC 9
Z9 10
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2321
EP 2330
DI 10.1109/TMM.2019.2957995
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NG7MB
UT WOS:000564163800002
OA hybrid
DA 2024-07-18
ER

PT J
AU Ghassab, VK
   Bouguila, N
AF Ghassab, Vahid Khorasani
   Bouguila, Nizar
TI Light Field Super-Resolution Using Edge-Preserved Graph-Based
   Regularization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Image edge detection; Image reconstruction; Three-dimensional
   displays; Spatial resolution; Light field; super-resolution; edge
   preserving; ADMM; Plug-and-play; graph
ID SINGLE-IMAGE SUPERRESOLUTION; RESOLUTION
AB The light field information would be captured through light field cameras and in different directions regarding 3D image view recordings. In this paper, in order to increase the spatio-angular super-resolution quality and to decrease the reconstruction error regarding the light field information, we use a graph-based light field super-resolution strategy. Accordingly, in order to apply the complementary data in the light field views, we use a graph regularizer for the total recovery of the information and an edge-preserving technique that represents an isometry between curves in the 2D manifold and 5D space of the RGB image views. Moreover, the reconstruction of the light field information is based on applying the alternating direction method of multipliers (ADMM) algorithm. Accordingly, a recent enhanced ADMM model has been used in this paper which is denominated as "Plug-and-Play" and permits the user to plug an image reconstruction technique and a denoising methodology as the first and second sub-problems respectively. On that account, we would be able to resolve the light field super-resolution problem considering the graph-based light field structure as the first sub-problem and the edge-preserving technique as the denoising methodology. Consequently, by applying the proposed super-resolution strategy, the super-resolved light field outcome would be more favorable in terms of visual quality and reconstruction errors in comparison with other state-of-the-art methodologies.
C1 [Ghassab, Vahid Khorasani; Bouguila, Nizar] Concordia Univ, Concordia Inst Informat & Engn, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Ghassab, VK (corresponding author), Concordia Univ, Concordia Inst Informat & Engn, Montreal, PQ, Canada.
EM vahid.khorasani@concordia.ca; nizar.bouguila@concordia.ca
OI Khorasani Ghassab, Vahid/0000-0001-7697-9364
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada (NSERC). The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Xiaochun
   Cao.
CR ADELSON EH, 1992, IEEE T PATTERN ANAL, V14, P99, DOI 10.1109/34.121783
   Arvo J., 1994, P ACM SIGGR, P335
   Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210
   Bevilacqua M, 2014, IEEE T IMAGE PROCESS, V23, P5334, DOI 10.1109/TIP.2014.2364116
   BOLLES RC, 1987, INT J COMPUT VISION, V1, P7, DOI 10.1007/BF00128525
   Brifman Alon, 2019, IEEE Trans Image Process, V28, P6063, DOI 10.1109/TIP.2019.2924173
   Candès EJ, 2014, COMMUN PUR APPL MATH, V67, P906, DOI 10.1002/cpa.21455
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chan SH, 2017, IEEE T COMPUT IMAG, V3, P84, DOI 10.1109/TCI.2016.2629286
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Farsiu S, 2004, IEEE T IMAGE PROCESS, V13, P1327, DOI 10.1109/TIP.2004.834669
   Gao XB, 2012, IEEE T IMAGE PROCESS, V21, P469, DOI 10.1109/TIP.2011.2161482
   Gastal ESL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964964
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gonzalez R.C., 2002, Digital Image Processing, V2nd
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   HARALICK RM, 1984, IEEE T PATTERN ANAL, V6, P58, DOI 10.1109/TPAMI.1984.4767475
   HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837
   Heber S, 2014, LECT NOTES COMPUT SC, V8694, P751
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Kwan C, 2017, INT CONF ACOUST SPEE, P6180, DOI 10.1109/ICASSP.2017.7953344
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P338, DOI 10.1109/TMM.2018.2859026
   Mitra Kaushik, 2012, 2012 IEEE COMPUTER S, P22
   Mitzel D, 2009, LECT NOTES COMPUT SC, V5748, P432, DOI 10.1007/978-3-642-03798-6_44
   Ng R, 2005, RES REPORT, DOI DOI 10.1145/3097571
   ONeill B., 1966, Elementary Differential Geometry
   Quan HT, 2012, TELECOMMUN SYST, V49, P35, DOI 10.1007/s11235-010-9351-x
   Ramanathan P, 2007, IEEE T MULTIMEDIA, V9, P813, DOI 10.1109/TMM.2007.893350
   Rossi M, 2018, IEEE T IMAGE PROCESS, V27, P4207, DOI 10.1109/TIP.2018.2828983
   Shidanshidi H, 2015, IEEE T MULTIMEDIA, V17, P1677, DOI 10.1109/TMM.2015.2447274
   Thomos N, 2006, IEEE T IMAGE PROCESS, V15, P54, DOI 10.1109/TIP.2005.860338
   Unger M, 2010, LECT NOTES COMPUT SC, V6376, P313
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Wang T, 2015, ADV INTEL SYS RES, V117, P1
   Wang YL, 2018, IEEE T IMAGE PROCESS, V27, P4274, DOI 10.1109/TIP.2018.2834819
   Wanner S, 2012, LECT NOTES COMPUT SC, V7576, P608, DOI 10.1007/978-3-642-33715-4_44
   Wong TT, 2002, IEEE T MULTIMEDIA, V4, P361, DOI 10.1109/TMM.2002.802835
   Xu J, 2019, IEEE T MULTIMEDIA, V21, P1108, DOI 10.1109/TMM.2018.2871948
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yoon Y, 2017, IEEE SIGNAL PROC LET, V24, P848, DOI 10.1109/LSP.2017.2669333
   Zhang K., 2019, COMPUT VISION PATTER
NR 46
TC 24
Z9 26
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1447
EP 1457
DI 10.1109/TMM.2019.2946094
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100006
DA 2024-07-18
ER

PT J
AU Jing, PG
   Ye, S
   Nie, LQ
   Liu, J
   Su, YT
AF Jing, Peiguang
   Ye, Shu
   Nie, Liqiang
   Liu, Jing
   Su, Yuting
TI Low-Rank Regularized Multi-Representation Learning for Fashion
   Compatibility Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sparse matrices; Matrix decomposition; Clothing; Feature extraction;
   Task analysis; Manifolds; Visualization; Image understanding; fashion
   compatibility; low-rank constraint; sparse representation; subspace
   learning
ID THRESHOLDING ALGORITHM; REGRESSION; SHRINKAGE
AB The currently flourishing fashion-oriented community websites and the continuous pursuit of fashion have attracted the increased research interest of the fashion analysis community. Many studies show that predicting the compatibility of fashion outfits is a nontrivial task due to the difficulty in capturing the implicit patterns affecting fashion compatibility prediction and the complex relationships presented by raw data. To address these problems, in this paper, we propose a transductive low-rank hypergraph regularizer multiple-representation learning framework (LHMRL), whereby we formulate the processes of feature representation and fashion compatibility prediction in a joint framework. Specifically, we first introduce a low-rank regularized multiple-representation learning framework, in which the lowest-rank multiple representations of samples can be learned to characterize samples from different perspectives. In this framework, we maximize the total difference among multiple representations based on Grassmann manifold theory and incorporate a common hypergraph regularizer to naturally encode the complex relationships between fashion items and an outfit. To enhance the representation ability of our model, we then develop a supervised learning term by exploiting two types of supervision information from labeled data. Experiments on a publicly available large-scale dataset demonstrate the effectiveness of our proposed model over the state-of-the-art methods.
C1 [Jing, Peiguang; Ye, Shu; Liu, Jing; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
C3 Tianjin University; Shandong University
RP Su, YT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM pgjing@tju.edu.cn; yeshu330@outlook.com; nieliqiang@gmail.com;
   jliu_tju@tju.edu.cn; ytsu@tju.edu.cn
RI Liu-Zeng, Jing/F-8582-2011
OI Jing, Peiguang/0000-0003-2648-7358
FU National Natural Science Foundation of China [61802277]; China
   Postdoctoral Science Foundation [2019M651038]; Tianjin Research Program
   of Application Foundation and Advanced Technology [17ZXRGGX00180]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61802277, in part by the China
   Postdoctoral Science Foundation Funded Project under Grant 2019M651038,
   and in part by the Tianjin Research Program of Application Foundation
   and Advanced Technology under Grant 17ZXRGGX00180.
CR Ak KE, 2018, PROC CVPR IEEE, P7708, DOI 10.1109/CVPR.2018.00804
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   Altman D G., 1990, Practical Statistics for Medical Research, DOI DOI 10.1201/9780429258589
   [Anonymous], 2018, ARXIV180309196
   [Anonymous], ICCV
   [Anonymous], 2000, Em: Proceedings of the 2000 ACM workshops on Multimedia, DOI DOI 10.1145/357744.357758
   [Anonymous], 2010, UILUENG092215
   [Anonymous], 1995, STORAGE RETRIEVAL IM, DOI DOI 10.1117/12.205308
   BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cai X, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1124
   Chatterjee S, 1986, STAT SCI, V1, P379, DOI DOI 10.1214/SS/1177013622
   Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981
   Chen YD, 2018, IEEE T MULTIMEDIA, V20, P3212, DOI 10.1109/TMM.2018.2834867
   Ding Z, 2014, AAAI CONF ARTIF INTE, P1192
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P5228, DOI 10.1109/TNNLS.2018.2796133
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P1006, DOI 10.1109/TNNLS.2017.2648880
   Ha Yu-I, 2017, ARXIV170404137
   Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Harandi M., 2014, ARXIV14018126
   Harandi M. T., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2705, DOI 10.1109/CVPR.2011.5995564
   Hou CP, 2013, IEEE T IMAGE PROCESS, V22, P340, DOI 10.1109/TIP.2012.2214044
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Jolliffe I. T., 2002, PRINCIPAL COMPONENT
   Li X, 2016, IEEE T MULTIMEDIA, V18, P474, DOI 10.1109/TMM.2016.2518478
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liu AN, 2018, SIGNAL PROCESS, V152, P206, DOI 10.1016/j.sigpro.2018.06.001
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu Q, 2010, 2010 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY AND SECURITY INFORMATICS (IITSI 2010), P663, DOI 10.1109/IITSI.2010.40
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Shih Y.-S., 2017, P AAAI, P2403
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Simonyan K., 2014, 14091556 ARXIV
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Stockman G, 2003, COMPUTER VISION
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang HY, 2017, IEEE T MULTIMEDIA, V19, P969, DOI 10.1109/TMM.2016.2638624
   Wei-Lin Hsiao, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P7161, DOI 10.1109/CVPR.2018.00748
   Wen J, 2018, NEURAL NETWORKS, V102, P36, DOI 10.1016/j.neunet.2018.02.002
   Yamaguchi K, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P773, DOI 10.1145/2647868.2654958
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Zhang YMZ, 2013, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2013.93
   Zhang Z, 2016, IEEE T IMAGE PROCESS, V25, P2429, DOI 10.1109/TIP.2016.2547180
   Zhou D., 2006, ADV NEURAL INFORM PR, P1601, DOI DOI 10.7551/MITPRESS/7503.003.0205
   Zhou P, 2016, IEEE T NEUR NET LEAR, V27, P1080, DOI 10.1109/TNNLS.2015.2436951
NR 52
TC 13
Z9 13
U1 4
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1555
EP 1566
DI 10.1109/TMM.2019.2944749
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100015
DA 2024-07-18
ER

PT J
AU Xiao, XD
   Wang, W
   Chen, TB
   Cao, Y
   Jiang, T
   Zhang, Q
AF Xiao, Xuedou
   Wang, Wei
   Chen, Taobin
   Cao, Yang
   Jiang, Tao
   Zhang, Qian
TI Sensor-Augmented Neural Adaptive Bitrate Video Streaming on UAVs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Throughput; Streaming media; Acceleration; Heuristic algorithms; Bit
   rate; Unmanned aerial vehicles; Adaptation models; Unmanned aerial
   vehicle; adaptive bitrate algorithm; video streaming; sensor-augmented
   system; deep reinforcement learning
AB Recent advances in unmanned aerial vehicle (UAV) technology have revolutionized a broad class of civil and military applications. However, the designs of wireless technologies that enable real-time streaming of high-definition video between UAVs and ground clients present a conundrum. Most existing adaptive bitrate (ABR) algorithms are not optimized for the air-to-ground links, which usually fluctuate dramatically due to the dynamic flight states of the UAV. In this paper, we present SA-ABR, a new sensor-augmented system that generates ABR video streaming algorithms with the assistance of various kinds of inherent sensor data that are used to pilot UAVs. By incorporating the inherent sensor data with network observations, SA-ABR trains a deep reinforcement learning (DRL) model to extract salient features from the flight state information and automatically learn an ABR algorithm to adapt to the varying UAV channel capacity through the training process. SA-ABR does not rely on any assumptions or models about UAV's flight states or the environment, but instead, it makes decisions by exploiting temporal properties of past throughput through the long short-term memory (LSTM) to adapt itself to a wide range of highly dynamic environments. We have implemented SA-ABR in a commercial UAV and evaluated it in the wild. We compare SA-ABR with a variety of existing state-of-the-art ABR algorithms, and the results show that our system outperforms the best known existing ABR algorithm by 21.4% in terms of the average quality of experience (QoE) reward.
C1 [Xiao, Xuedou; Wang, Wei; Chen, Taobin; Cao, Yang; Jiang, Tao] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Zhang, Qian] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Huazhong University of Science & Technology; Hong Kong University of
   Science & Technology
RP Wang, W (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
EM xuedouxiao@hust.edu.cn; weiwangw@hust.edu.cn; chentaobin@hust.edu.cn;
   ycao@hust.edu.cn; taojiang@hust.edu.cn; qianzh@cse.ust.hk
RI Wang, Wei/AAB-7341-2019; Cao, Yang/HGD-6463-2022; Zhang,
   Qian/B-9058-2009; jiang, tao/GWC-7108-2022; Jiang, Tao/IWM-7503-2023
OI Wang, Wei/0000-0002-2772-4856; Zhang, Qian/0000-0001-9205-1881; 
FU National Key R&D Program of China [2017YFE0121500]; National Natural
   Science Foundation of China (NSFC) [91738202, 61871441, 61729101,
   61601193, 61720106001]; Young Elite Scientists Sponsorship Program by
   CAST [2018QNRC001]; Major Program of National Natural Science Foundation
   of Hubei in China [2016CFA009]; Key Laboratory of Dynamic Cognitive
   System of Electromagnetic Spectrum Space (Nanjing University Aeronautics
   Astronautics); Ministry of Industry and Information Technology, Nanjing,
   China [KF20181911]; RGC [CERG 16203719, 16204418]; Guangdong Natural
   Science Foundation [2017A030312008]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFE0121500, in part by the National Natural Science
   Foundation of China (NSFC) under Grants 91738202, 61871441, 61729101,
   61601193, and 61720106001, in part by Young Elite Scientists Sponsorship
   Program by CAST under Grant 2018QNRC001, in part by the Major Program of
   National Natural Science Foundation of Hubei in China under Grant
   2016CFA009, in part by the Key Laboratory of Dynamic Cognitive System of
   Electromagnetic Spectrum Space (Nanjing University Aeronautics
   Astronautics), in part by the Ministry of Industry and Information
   Technology, Nanjing, 211106, China under Grant KF20181911, in part by
   the RGC under Contract CERG 16203719 and 16204418, and in part by the
   Guangdong Natural Science Foundation 2017A030312008.
CR Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], AGR DRONES ARE CHANG
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], H520 OV COMM UAV
   [Anonymous], 2010, P 10 ENC NAC ED MAT
   Asadpour M, 2013, PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT '13), P127
   Chen YJ, 2019, IEEE T MOBILE COMPUT, V18, P2270, DOI 10.1109/TMC.2018.2875910
   Chiariotti F, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P77, DOI 10.1145/2910017.2910603
   Claeys M, 2014, CONNECT SCI, V26, DOI 10.1080/09540091.2014.885273
   Fleureau J, 2016, DRONET'16: PROCEEDINGS OF THE 2ND WORKSHOP ON MICRO AERIAL VEHICLE NETWORKS, SYSTEMS, AND APPLICATIONS FOR CIVILIAN USE, P35, DOI 10.1145/2935620.2935622
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jiang JC, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P253, DOI 10.1145/3230543.3230574
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kan NW, 2019, INT CONF ACOUST SPEE, P4030, DOI 10.1109/ICASSP.2019.8683779
   Kim S, 2019, IEEE T MULTIMEDIA, V21, P442, DOI 10.1109/TMM.2018.2856626
   Krumm J, 2004, PROCEEDINGS OF MOBIQUITOUS 2004, P4
   Li Y, 2014, IEEE T MOBILE COMPUT, V13, P1911, DOI 10.1109/TMC.2013.159
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V, 2016, PR MACH LEARN RES, V48
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun L, 2015, I C NETWORK PROTOCOL, P467, DOI 10.1109/ICNP.2015.59
   van der Hooft J, 2015, PROCEEDINGS OF THE 2015 IFIP/IEEE INTERNATIONAL SYMPOSIUM ON INTEGRATED NETWORK MANAGEMENT (IM), P131, DOI 10.1109/INM.2015.7140285
   Wang W, 2019, IEEE T WIREL COMMUN, V18, P796, DOI 10.1109/TWC.2018.2883443
   Wang W, 2016, IEEE COMMUN MAG, V54, P33, DOI 10.1109/MCOM.2016.7378423
   Wang XL, 2016, PROCEEDINGS OF THE 3RD WORKSHOP ON HOT TOPICS IN WIRELESS (HOTWIRELESS '16), P2, DOI 10.1145/2980115.2980119
   Wang ZZ, 2017, ADV MECH ENG, V9, DOI 10.1177/1687814017715420
   Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   WIRED
NR 33
TC 40
Z9 42
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1567
EP 1576
DI 10.1109/TMM.2019.2945167
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chaudhary, C
   Goyal, P
   Prasad, DN
   Chen, YPP
AF Chaudhary, Chandramani
   Goyal, Poonam
   Prasad, Dhanashree Nellayi
   Chen, Yi-Ping Phoebe
TI Enhancing the Quality of Image Tagging Using a Visio-Textual Knowledge
   Base
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Knowledge based systems; Visualization; Image annotation; Encyclopedias;
   Electronic publishing; Internet; Tagging; image representation;
   knowledge based systems
ID VISUAL KNOWLEDGE; TAG REFINEMENT; WEB; ANNOTATION; RELEVANCE
AB Auto-tagging of images is important for image understanding and for tag-based applications viz. image retrieval, visual question-answering, image captioning, etc. Although existing tagging methods incorporate both visual and textual information to assign/refine tags, they lag in tag-image relevance, completeness, and preciseness, thereby resulting in the unsatisfactory performance of tag-based applications. In order to bridge this gap, we propose a novel framework for tag assignment using knowledge embedding (TAKE) from a proposed external knowledge base, considering properties such as Rarity, Newness, Generality, and Naturalness (RNGN properties). These properties help in providing a rich semantic representation to images. Existing knowledge bases provide multiple types of relations extracted through only one modality, either text or visual, which is not effective in image related applications. We construct a simple yet effective Visio-Textual Knowledge Base (VTKB) with only four relations using reliable resources such as Wikipedia, thesauruses, dictionaries, etc. Our large scale experiments demonstrate that the proposed combination of TAKE and VTKB assigns a large number of high quality tags in comparison to the ConceptNet and ImageNet knowledge bases when used in conjunction with TAKE. Also, the effectiveness of knowledge embedding through VTKB is evaluated for image tagging and tag-based image retrieval (TBIR).
C1 [Chaudhary, Chandramani; Goyal, Poonam; Prasad, Dhanashree Nellayi] Birla Inst Technol & Sci, Dept Comp Sci & Informat Syst, Pilani 333031, Rajasthan, India.
   [Chen, Yi-Ping Phoebe] La Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
C3 Birla Institute of Technology & Science Pilani (BITS Pilani); La Trobe
   University
RP Chen, YPP (corresponding author), La Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
EM chandramani.chaudhary@pilani.bits-pilani.ac.in;
   poonam@pilani.bits-pilani.ac.in; dhanashree.np@pilani.bits-pilani.ac.in;
   phoebe.chen@latrobe.edu.au
RI Chaudhary, Chandramani/AAZ-2568-2021; Chen, Yi-Ping Phoebe/B-8844-2008
OI Chen, Yi-Ping Phoebe/0000-0002-4122-3767
CR [Anonymous], 1992, COLING 1992, DOI DOI 10.3115/992133.992154
   [Anonymous], 2005, Advances in neural information processing systems
   [Anonymous], 2011, P ACL
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], 2010, P ACM MULTIMEDIA
   [Anonymous], 2008, COLING 2008 P WORKSH
   Belongie S, 2016, PATTERN RECOGN LETT, V72, P15, DOI 10.1016/j.patrec.2015.11.023
   Bizer C, 2009, J WEB SEMANT, V7, P154, DOI 10.1016/j.websem.2009.07.002
   Chaudhary C, 2019, MULTIMED TOOLS APPL, V78, P17623, DOI 10.1007/s11042-018-7131-x
   Chaudhary C, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P257, DOI 10.1145/3206025.3206050
   Chaudhary C, 2017, IEEE INT CONF BIG DA, P566, DOI 10.1109/BigData.2017.8257972
   Chen P, 2013, PROCEEDINGS OF THE 2013 INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY (FPT), P480, DOI 10.1109/FPT.2013.6718421
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Chowdhury SN, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P117, DOI 10.1145/3159652.3159693
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Cui CR, 2017, MULTIMED TOOLS APPL, V76, P8831, DOI 10.1007/s11042-016-3512-1
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Etzioni O, 2004, P 13 INT C WORLD WID, P100, DOI DOI 10.1145/988672.988687
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Hoffart J, 2013, ARTIF INTELL, V194, P28, DOI 10.1016/j.artint.2012.06.001
   Hu MQ, 2017, IEEE T IMAGE PROCESS, V26, P4871, DOI 10.1109/TIP.2017.2717185
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Lee S, 2014, MULTIMED TOOLS APPL, V72, P1363, DOI 10.1007/s11042-013-1439-3
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li SW, 2017, IEEE T PATTERN ANAL, V39, P2423, DOI 10.1109/TPAMI.2017.2651818
   Li XR, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1325
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Li ZC, 2017, AAAI CONF ARTIF INTE, P4154
   Liao S, 2015, IEEE T MULTIMEDIA, V17, P1058, DOI 10.1109/TMM.2015.2436057
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Liu J, 2010, INT CONF COMPUT AUTO, P491, DOI 10.1109/ICCAE.2010.5451908
   Liu Y, 2011, 2011 AASRI CONFERENCE ON ARTIFICIAL INTELLIGENCE AND INDUSTRY APPLICATION (AASRI-AIIA 2011), VOL 1, P411
   Makadia A, 2010, INT J COMPUT VISION, V90, P88, DOI 10.1007/s11263-010-0338-6
   Mathews A, 2015, IEEE WINT CONF APPL, P595, DOI 10.1109/WACV.2015.85
   Mikolov T., 2013, INT C LEARNING REPRE, P1
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Min QX, 2010, INT CONF COMPUT AUTO, P815, DOI 10.1109/ICCAE.2010.5451821
   Mitchell T, 2015, AAAI CONF ARTIF INTE, P2302
   Ordonez V, 2015, INT J COMPUT VISION, V115, P29, DOI 10.1007/s11263-015-0815-z
   Qian XM, 2017, IEEE T IMAGE PROCESS, V26, P3734, DOI 10.1109/TIP.2017.2699623
   Qian XM, 2014, IEEE T CYBERNETICS, V44, P2493, DOI 10.1109/TCYB.2014.2309593
   Richang Hong, 2015, IEEE Transactions on Big Data, V1, P152, DOI 10.1109/TBDATA.2016.2515640
   Salton G., 1983, Introduction toModern Information Retrieval
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Simonyan K., 2014, 14091556 ARXIV
   Tandon N, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P523, DOI 10.1145/2556195.2556245
   Tandon N, 2016, AAAI CONF ARTIF INTE, P243
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Uddin MN, 2013, EXPERT SYST APPL, V40, P1645, DOI 10.1016/j.eswa.2012.09.006
   Uricchio T, 2017, PATTERN RECOGN, V71, P144, DOI 10.1016/j.patcog.2017.05.019
   Wang JD, 2014, COMPUT VIS IMAGE UND, V124, P61, DOI 10.1016/j.cviu.2014.02.011
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Wu W T, 2012, P 2012 ACM SIGMOD IN, P481, DOI DOI 10.1145/2213836.2213891
   Yao JC, 2018, IEEE T MULTIMEDIA, V20, P224, DOI 10.1109/TMM.2017.2716829
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhang Y, 2016, PROC CVPR IEEE, P5985, DOI 10.1109/CVPR.2016.644
NR 59
TC 16
Z9 17
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 897
EP 911
DI 10.1109/TMM.2019.2937181
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400006
DA 2024-07-18
ER

PT J
AU Zha, YF
   Ku, T
   Li, YQ
   Zhang, P
AF Zha, Yufei
   Ku, Tao
   Li, Yunqiang
   Zhang, Peng
TI Deep Position-Sensitive Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Semantics; Task analysis; Feature extraction; Object tracking;
   Additives; Visual tracking; softmax loss; position-sensitive loss;
   ranking
ID OBJECT TRACKING; VISUAL TRACKING
AB Classification-based tracking strategies often face more challenges from intra-class discrimination than from inter-class separability. Even for deep convolutional neural networks that have been widely proven to be effective in various vision tasks, their intra-class discriminative capability is still limited by the weakness of softmax loss, especially for targets not seen in the training dataset. By taking intrinsic attributes of training samples into account, in this paper, we propose a position-sensitive loss coupled with softmax loss to achieve intra-class compactness and inter-class explicitness. Particularly, two additive margins are introduced to encode the position attribute for decision boundary maximization, which is also utilized with the proposed loss to supervise the fine-tuned features on the pre-trained model. With the nearest neighbor ranking measurement in the feature embedding domain, the whole scheme is able to reach an optimized balance between the feature-level inter-class semantic separability and instance-level intra-class relative distance ranking. We evaluate the proposed work on different popular benchmarks, and experimental results demonstrate that our tracking strategy performs favorably against most of the state-of-the-art trackers in the comparison of accuracy and robustness.
C1 [Zha, Yufei; Zhang, Peng] Northwestern Polytech Univ, Sch Comp Sci, Xian 710038, Peoples R China.
   [Ku, Tao] Univ Utrecht, Dept Informat & Comp Sci, NL-3584 CC Utrecht, Netherlands.
   [Li, Yunqiang] Delft Univ Technol, Vis Lab, NL-2628 CD Delft, Netherlands.
C3 Northwestern Polytechnical University; Utrecht University; Delft
   University of Technology
RP Zhang, P (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710038, Peoples R China.
EM zhayufei@126.com; t.ku@uu.nl; y.li-19@tudelft.nl; zh0036ng@nwpu.edu.cn
RI wang, zhenhui/JMQ-0550-2023; Liu, Gui/JHU-8707-2023; Li,
   Chun/KBC-9591-2024
OI Zha, yufei/0000-0001-5013-2501
FU National Natural Science Foundation of China [61773397, 61571362,
   61703423]; Natural Science Basic Research Plan in Shaanxi Province of
   China [2018JM6015]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773397, 61571362, and 61703423 and in
   part by the Natural Science Basic Research Plan in Shaanxi Province of
   China under Grant 2018JM6015. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Marco
   Carli.
CR [Anonymous], 2015, INT C LEARN REPR MAY
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], P EUR C COMP VIS
   [Anonymous], EUR C COMP VIS ECCV
   [Anonymous], P IEEE INT C COMP VI
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cehovin L, 2016, IEEE T IMAGE PROCESS, V25, P1261, DOI 10.1109/TIP.2016.2520370
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Chen K, 2018, IEEE T CIRC SYST VID, V28, P3377, DOI 10.1109/TCSVT.2017.2757061
   Cui JS, 2013, IEEE T SYST MAN CY-S, V43, P996, DOI 10.1109/TSMCA.2012.2223670
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2016.159
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jung I, 2018, LECT NOTES COMPUT SC, V11208, P89, DOI 10.1007/978-3-030-01225-0_6
   Kristan M, 2016, IEEE T PATTERN ANAL, V38, P2137, DOI 10.1109/TPAMI.2016.2516982
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li HX, 2016, IEEE T IMAGE PROCESS, V25, P1834, DOI 10.1109/TIP.2015.2510583
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Liang JQ, 2017, IEEE T MULTIMEDIA, V19, P1077, DOI 10.1109/TMM.2016.2644862
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liu FH, 2018, IEEE T IMAGE PROCESS, V27, P2777, DOI 10.1109/TIP.2018.2813161
   Liu WY, 2016, PR MACH LEARN RES, V48
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ondrúska P, 2016, AAAI CONF ARTIF INTE, P3361
   Pflugfelder R, 2017, ARXIV170700569
   Possegger H, 2015, PROC CVPR IEEE, P2113, DOI 10.1109/CVPR.2015.7298823
   Pu S., 2018, ADV NEURAL INFORM PR, P1
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wang X, 2019, IEEE T CYBERNETICS, V49, P146, DOI 10.1109/TCYB.2017.2768570
   Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhu G, 2016, PROC CVPR IEEE, P943, DOI 10.1109/CVPR.2016.108
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 67
TC 8
Z9 9
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 96
EP 107
DI 10.1109/TMM.2019.2922125
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000010
DA 2024-07-18
ER

PT J
AU Liang, LY
   Zhang, XL
AF Liang, Lingyu
   Zhang, Xinglin
TI Adaptive Label Propagation for Facial Appearance Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image editing; face appearance transfer; label propagation; face
   replacement; face blending
ID IMAGE; RECOGNITION
AB Facial appearance transfer (FAT) is a critical component of various facial editing tasks. It aims to transfer the facial appearance of a reference into a target with good visual consistency. When there are considerable visual differences between a reference and a target, however, it may introduce visual artifacts into the results. To tackle this problem, we propose a facial appearance map with illumination-aware and region-aware properties that allows seamless FAT. We formulate the appearance-map generation as label propagation (LP) on a similarity graph, and propose a new regularization structure to facilitate the adaptive appearance-map diffusion. Solving the original LP model of appearance map in general requires on the order O(kn(2)) time for an n-nodes graph where each node has k neighbors. It may be computationally prohibitive for an image with a large spatial resolution. To tackle this problem, we mathematically analyze the graph-based LP model and propose a fast algorithm with smart subset sampling. It selects a subset with m nodes of the graph with n nodes (m << n) to approximate the solution to the original system, which significantly reduces its computational requirements from O(kn(2)) to O(m(2)n). Based on the adaptive LP-based appearance map, we construct a framework to achieve various editing effects with FAT, including face replacement, face dubbing, face swapping, and transfiguring. Comparisons with related methods show the effectiveness of the adaptive LP model for FAT. Qualitative and quantitative evaluations verify the computational improvements of the approximation algorithm.
C1 [Liang, Lingyu] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Zhang, Xinglin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Zhang, XL (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM lianglysky@gmail.com; zhxlinsc@gmail.com
FU National Natural Science Foundation of China [61502176, 61872149,
   61602184, 61872151]; Natural Science Foundation of Guangdong Province
   [2018B030306010, 2017A030313376]; Guangdong Special Support Program
   [2017TQ04X482]; Pearl River S&T Nova Program of Guangzhou
   [201806010088]; Science and Technology Program of Guangzhou
   [201707010147]; Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61502176, 61872149, 61602184, and
   61872151, in part by Natural Science Foundation of Guangdong Province
   under Grants 2018B030306010 (Distinguished Young Scholar) and
   2017A030313376, in part by the Guangdong Special Support Program under
   Grant 2017TQ04X482, in part by the Pearl River S&T Nova Program of
   Guangzhou underGrant 201806010088, in part by Science and Technology
   Program of Guangzhou under Grant 201707010147, and in part by
   Fundamental Research Funds for the Central Universities. The associate
   editor coordinating the reviewof thismanuscript and approving it for
   publicationwas Dr. Lei Zhang.
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Aksoy Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201275
   An XB, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360639
   [Anonymous], 2001, P NEUR INF PROC SYST
   [Anonymous], 2016, in Face Detec-tion and Facial Image Analysis, DOI DOI 10.1007/978-3-319-25958-1_8
   Atkinson K, 2005, Texts in Applied Mathematics, V39
   Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712
   Bhat P, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1731047.1731048
   Billinghurst Mark, 2015, Foundations and Trends in Human-Computer Interaction, V8, P73, DOI 10.1561/1100000049
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Brahnam S., 2014, Local Binary Patterns - New Variants and Applications
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Chapelle O., 2006, SEMISUPERVISED LEARN
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen XW, 2013, IEEE T IMAGE PROCESS, V22, P4249, DOI 10.1109/TIP.2013.2271548
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Dale K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024164
   Dou H, 2017, IEEE T MULTIMEDIA, V19, P1718, DOI 10.1109/TMM.2017.2689327
   Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185
   Garrido P, 2014, PROC CVPR IEEE, P4217, DOI 10.1109/CVPR.2014.537
   Guo D, 2009, PROC CVPR IEEE, P73, DOI 10.1109/CVPRW.2009.5206833
   Guo GD, 2011, IEEE I CONF COMP VIS, P2510, DOI 10.1109/ICCV.2011.6126537
   Henriquez P, 2017, IEEE T MULTIMEDIA, V19, P1467, DOI 10.1109/TMM.2017.2666545
   Huang GQ, 2009, INT J COMPUT INTEG M, V22, P579, DOI 10.1080/09511920701724934
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Joachims T., 2003, P 20 INT C MACH LEAR, V20, P290, DOI DOI 10.1145/2612669.2612699
   Kemelmacher-Shlizerman I, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925871
   KEMELMACHERSHLIZER, 2014, PROC CVPR IEEE, P3334, DOI DOI 10.1109/CVPR.2014.426
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   KRISHNAN D., 2013, ACM T GRAPHIC, V32, P142
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   Liang JQ, 2017, IEEE T MULTIMEDIA, V19, P1077, DOI 10.1109/TMM.2016.2644862
   Liang LY, 2017, IEEE T CIRC SYST VID, V27, P125, DOI 10.1109/TCSVT.2016.2602812
   Liang LY, 2013, IEICE T INF SYST, VE96D, P2904, DOI 10.1587/transinf.E96.D.2904
   Liang X, 2018, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: IOT AND SMART CITY (ICIT 2018), P173, DOI 10.1145/3301551.3301555
   Ling HB, 2017, IEEE MULTIMEDIA, V24, P10, DOI 10.1109/MMUL.2017.3051517
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P1724, DOI 10.1109/TMM.2017.2780761
   Liu SF, 2015, PROC CVPR IEEE, P3451, DOI 10.1109/CVPR.2015.7298967
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lukac R., 2010, COMPUTATIONAL PHOTOG
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Nguyen MH, 2008, COMPUT GRAPH FORUM, V27, P627, DOI 10.1111/j.1467-8659.2008.01160.x
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ortega A, 2018, P IEEE, V106, P808, DOI 10.1109/JPROC.2018.2820126
   Paszke A., 2016, ARXIV160602147
   Peers P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239503
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Reinhard E, 2013, P IEEE, V101, P1998, DOI 10.1109/JPROC.2013.2260711
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Saad Yousef., 2003, Iterative Methods for Sparse Linear Systems
   Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Sun WT, 2017, IEEE T MULTIMEDIA, V19, P1870, DOI 10.1109/TMM.2017.2688929
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Theobalt Chris- tian, 2016, P IEEE C COMPUTER VI, DOI DOI 10.1109/CVPR.2016.262
   Thies J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818056
   Thomaz CE, 2010, IMAGE VISION COMPUT, V28, P902, DOI 10.1016/j.imavis.2009.11.005
   Tong WS, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P211, DOI 10.1109/PG.2007.31
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Williams CKI, 2001, ADV NEUR IN, V13, P682
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang S, 2017, INT CONF ACOUST SPEE, P1353, DOI 10.1109/ICASSP.2017.7952377
   Yang SM, 2015, AER ADV ENG RES, V8, P1, DOI 10.1109/PESGM.2015.7285904
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang Zechen., 2018, ACM Trans. Graph, P11
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou P, 2017, IEEE COMPUT SOC CONF, P1831, DOI 10.1109/CVPRW.2017.229
   Zhu X., 2003, P 20 INT C MACH LEAR, V3, P58, DOI DOI 10.1109/18.850663
   Zhu X., 2002, CMUCALD02107, V3175, P237, DOI DOI 10.1007/978-3-540-28649-3_29
NR 77
TC 3
Z9 3
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3068
EP 3082
DI 10.1109/TMM.2019.2918717
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200008
DA 2024-07-18
ER

PT J
AU Yang, S
   Wang, YW
   Shi, YM
   Fei, ZS
AF Yang, Shu
   Wang, Yaowei
   Shi, Yemin
   Fei, Zesong
TI Can Categories and Attributes Be Learned in a Multi-Task Way?
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-task learning; category-constrained attribute prediction;
   regularization
ID CLOTHING RETRIEVAL; CLASSIFICATION; INFORMATION
AB Intuitively, we can think of object recognition and attribute prediction as correlated tasks. However, they appeared to conflict in a simple two-branch multi-task framework (a category branch and an attribute branch) with a shared backbone part (convolutional layers and pooling layers). The performance dropped along with the iterative training steps. This result might have been caused by the noncoherent feature distribution between the object recognition features and the attribute prediction features. Recognition features are discriminative for different categories and are not sensitive to intracategory variations, while attribute prediction features are discriminative for different attributes, although these attributes can exist in objects from the same category. Thus, a conflict occurs when we force the network to learn the two kinds of distinct features simultaneously. To address this problem, we propose the category and attribute prediction network (CAP-net), in which a category-constrained attribute prediction structure is introduced to transfer the object recognition knowledge and avoid the conflict between two features. The CAP-net parameters can be learned easily with a regularization method. Extensive experimental results show that the CAP-net outperforms the state-of-the-art methods on object recognition and attribute prediction tasks.
C1 [Yang, Shu; Fei, Zesong] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
   [Wang, Yaowei] Peking Univ, Shenzhen Grad Sch, Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Wang, Yaowei] Peking Univ, Shenzhen Grad Sch, Natl Engn Lab Video Technol, Shenzhen 518055, Peoples R China.
   [Shi, Yemin] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100091, Peoples R China.
C3 Beijing Institute of Technology; Peng Cheng Laboratory; Peking
   University; Peking University; Peking University
RP Fei, ZS (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.; Wang, YW (corresponding author), Peking Univ, Shenzhen Grad Sch, Peng Cheng Lab, Shenzhen 518055, Peoples R China.; Wang, YW (corresponding author), Peking Univ, Shenzhen Grad Sch, Natl Engn Lab Video Technol, Shenzhen 518055, Peoples R China.
EM yangshu91@bit.edu.cn; yaoweiwang@pku.edu.cn; shiyemin@pku.edu.cn;
   feizesong@bit.edu.cn
OI Yang, Shu/0000-0003-1587-0474
FU National Natural Science Foundation of China [61471042]; Ministry of
   Education of China-China Mobile Communication Corporation Research Fund
   [MCM20170101]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61471042, and in part by the Ministry of
   Education of China-China Mobile Communication Corporation Research Fund
   under Grant MCM20170101. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Raouf
   Hamzaoui.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   [Anonymous], 2005, P C NEUR INF PROC SY
   [Anonymous], PROC CVPR IEEE
   [Anonymous], CNSTR2011001 CAL I
   [Anonymous], ARXIV180406505
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2018, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2017.2723882
   [Anonymous], 2012, PROC 29 INT C MACH L
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2016, Tensorpack
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   Chen CY, 2014, PROC CVPR IEEE, P200, DOI 10.1109/CVPR.2014.33
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Choi SW, 2014, LECT NOTES COMPUT SC, V8695, P361, DOI 10.1007/978-3-319-10584-0_24
   Dai JF, 2016, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR.2016.343
   Demirel B, 2017, IEEE I CONF COMP VIS, P1241, DOI 10.1109/ICCV.2017.139
   Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089
   Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109, DOI DOI 10.1145/1014052.1014067
   Fan BJ, 2018, IEEE T MULTIMEDIA, V20, P2303, DOI 10.1109/TMM.2018.2804762
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Gan C, 2016, AAAI CONF ARTIF INTE, P3487
   Gan C, 2015, AAAI CONF ARTIF INTE, P3769
   Gan C, 2016, PROC CVPR IEEE, P87, DOI 10.1109/CVPR.2016.17
   Gan C, 2016, INT J COMPUT VISION, V120, P61, DOI 10.1007/s11263-016-0893-6
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Jayaraman D, 2014, ADV NEUR IN, V27
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Kumar A., 2012, P 29 INT C MACH LEAR, P1383
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Layne R, 2014, ADV COMPUT VIS PATT, P93, DOI 10.1007/978-1-4471-6296-4_5
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Li Y, 2015, IEEE I CONF COMP VIS, P3819, DOI 10.1109/ICCV.2015.435
   Liang KM, 2015, IEEE I CONF COMP VIS, P2506, DOI 10.1109/ICCV.2015.288
   Liang XD, 2017, PROC CVPR IEEE, P4408, DOI 10.1109/CVPR.2017.469
   Liu HM, 2017, PROC CVPR IEEE, P6259, DOI 10.1109/CVPR.2017.663
   Lu YJ, 2015, IEEE T MULTIMEDIA, V17, P1213, DOI 10.1109/TMM.2015.2438712
   Ma ZG, 2017, IEEE T MULTIMEDIA, V19, P1558, DOI 10.1109/TMM.2017.2659221
   Mahajan D, 2011, IEEE I CONF COMP VIS, P1227, DOI 10.1109/ICCV.2011.6126373
   Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Patterson G, 2016, LECT NOTES COMPUT SC, V9910, P85, DOI 10.1007/978-3-319-46466-4_6
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Romera-Paredes Bernardino, 2013, JMLR Workshop and Conference Proceedings, P1444
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi ZY, 2015, PROC CVPR IEEE, P4184, DOI 10.1109/CVPR.2015.7299046
   Singh KK, 2016, LECT NOTES COMPUT SC, V9910, P753, DOI 10.1007/978-3-319-46466-4_45
   Su C, 2016, LECT NOTES COMPUT SC, V9906, P475, DOI 10.1007/978-3-319-46475-6_30
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   Wang JY, 2017, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2017.65
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang YH, 2019, EUR FINANC MANAG, V25, P380, DOI 10.1111/eufm.12166
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Wimalawarne Kishan, 2014, NIPS, V27, P2825
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yang Yongxin, 2014, ARXIV14127489
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XX, 2014, IEEE GLOB COMM CONF, P4168, DOI 10.1109/GLOCOM.2014.7037461
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
   Zheng L., 2016, ARXIV160400133
NR 74
TC 5
Z9 5
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3194
EP 3204
DI 10.1109/TMM.2019.2919469
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200017
DA 2024-07-18
ER

PT J
AU Yang, M
   Zhu, C
   Lan, XG
   Zheng, NN
AF Yang, Meng
   Zhu, Ce
   Lan, Xuguang
   Zheng, Nanning
TI Efficient Estimation of View Synthesis Distortion for Depth Coding
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video; depth coding; view synthesis distortion; H.264/AVC; 3D-HEVC;
   rate-distortion optimization
ID JOINT BIT ALLOCATION; LAGRANGIAN MULTIPLIER; MULTIVIEW VIDEO; MODEL;
   DIBR; MAPS
AB Depth coding in depth-based three-dimensional (3-D) video is unique in that its quality is measured by view synthesis distortion (VSD) rather than the depth distortion itself, which further complicates the coding optimization as the VSD is related to quality of both the associated depth and texture videos. In this paper, an efficient VSD estimation scheme is developed to measure the effect of depth errors on the VSD for a block given its depth distortion in mean-squared error. Unlike other relevant VSD models which involve computationally intensive parameter training or Fourier transform, the proposed scheme is free of parameter training, while taking the advantage of integer 4 x 4 discrete Cosine transform to replace Fourier transform, thus well-saving computational cost and diminishing sensitivity to training dataset of video. The proposed scheme is then incorporated on the coding unit basis into the rate-distortion optimization for depth coding optimization, coupled with adapting quantization parameter accordingly to accommodate local effect of the depth errors on the VSD. Experimental results show that our solution obtains better results in depth coding than three testing solutions, on the platform of H.264/AVC reference software JM16.0. Benefiting from the efficiency of the VSD estimation, low coding complexity is obtained as well. The proposed solution is further evaluated on the reference software HTM13.0 of the latest 3-D high-efficiency video coding standard, exhibiting better and comparable results compared against the HTM codec with the view synthesis optimization disabled and enabled, respectively.
C1 [Yang, Meng; Lan, Xuguang; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Yang, Meng; Lan, Xuguang; Zheng, Nanning] Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.
   [Zhu, Ce] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; University of
   Electronic Science & Technology of China
RP Yang, M (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.; Yang, M (corresponding author), Xi An Jiao Tong Univ, Natl Engn Lab Visual Informat Proc & Applicat, Xian 710049, Shaanxi, Peoples R China.
EM mengyang@xjtu.edu.cn; eczhu@uestc.edu.cn; xglan@xjtu.edu.cn;
   nnzheng@xjtu.edu.cn
RI Zhu, Ce/AEN-1875-2022; Lan, Xuguang/N-8814-2019
OI Lan, Xuguang/0000-0002-3422-944X
FU NSFC [61627811, 91748208, 61503294]; National Key R&D Program of China
   [2017YFB1302200, 2016YFB1000903]
FX This work was supported in part by the NSFC under Grants 61627811,
   91748208, and 61503294, and in part by the National Key R&D Program of
   China under Grants 2017YFB1302200 and 2016YFB1000903. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. J. Ostermann. (Corresponding author: Meng Yang.)
CR [Anonymous], JCT3VG1100 ITUT SG 1
   [Anonymous], 2006, Elements of Information Theory
   [Anonymous], 2015, 3D HEVC VIDEO CODING
   [Anonymous], 2011, N12035 ISOIEC JTC1SC
   [Anonymous], 2013, MPEG 3 DV VIEW SYNTH
   [Anonymous], 2001, ITU T VCEG M AUST TE
   Dai QH, 2005, IEEE T SIGNAL PROCES, V53, P3219, DOI 10.1109/TSP.2005.851115
   Fang L, 2016, IEEE T IMAGE PROCESS, V25, P1961, DOI 10.1109/TIP.2016.2535345
   Fang L, 2014, IEEE T IMAGE PROCESS, V23, P185, DOI 10.1109/TIP.2013.2287608
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fu JJ, 2013, IEEE T MULTIMEDIA, V15, P1340, DOI 10.1109/TMM.2013.2247584
   Gao YB, 2019, IEEE T CIRC SYST VID, V29, P546, DOI 10.1109/TCSVT.2017.2787190
   Gao YB, 2017, IEEE T IMAGE PROCESS, V26, P4457, DOI 10.1109/TIP.2017.2713598
   Hu SD, 2013, IEEE T IMAGE PROCESS, V22, P585, DOI 10.1109/TIP.2012.2219549
   Kim WS, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2447737
   Kim WS, 2009, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2009.5414304
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Li S, 2016, IEEE T CIRC SYST VID, V26, P117, DOI 10.1109/TCSVT.2015.2450131
   Liu YW, 2009, SIGNAL PROCESS-IMAGE, V24, P666, DOI 10.1016/j.image.2009.06.002
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Milani S, 2010, IEEE SIGNAL PROC LET, V17, P51, DOI 10.1109/LSP.2010.2051619
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Oh BT, 2011, IEEE J-STSP, V5, P1344, DOI 10.1109/JSTSP.2011.2164893
   Oh KJ, 2009, IEEE SIGNAL PROC LET, V16, P747, DOI 10.1109/LSP.2009.2024112
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tech G, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P25, DOI 10.1109/PCS.2012.6213277
   Tianwu Yang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P85, DOI 10.1109/ICME.2012.171
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang M, 2017, IEEE T IMAGE PROCESS, V26, P5122, DOI 10.1109/TIP.2017.2723731
   Yang M, 2015, IEEE INT SYMP CIRC S, P2812, DOI 10.1109/ISCAS.2015.7169271
   Yao C, 2016, IEEE T MULTIMEDIA, V18, P2015, DOI 10.1109/TMM.2016.2594145
   Yuan H, 2014, IEEE T CIRC SYST VID, V24, P443, DOI 10.1109/TCSVT.2013.2280071
   Yuan H, 2012, IEEE T BROADCAST, V58, P558, DOI 10.1109/TBC.2012.2187612
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang D, 2015, IEEE T CIRC SYST VID, V25, P827, DOI 10.1109/TCSVT.2014.2363746
   Zhang Y, 2013, IEEE T IMAGE PROCESS, V22, P3497, DOI 10.1109/TIP.2013.2265883
   Zhu C., 2013, 3D-TV System with Depth-Image-Based Rendering - Architectures, Techniques and Challenges
NR 41
TC 3
Z9 3
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 863
EP 874
DI 10.1109/TMM.2018.2870540
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700005
DA 2024-07-18
ER

PT J
AU Yan, JJ
   Wu, DP
   Wang, RY
AF Yan, Junjie
   Wu, Dapeng
   Wang, Ruyan
TI Socially Aware Trust Framework for Multimedia Delivery in D2D
   Cooperative Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE D2D cooperative communications; multimedia delivery; trust evaluation;
   trust decision
ID TO-DEVICE COMMUNICATION; NETWORK SLICE DESIGN; RESOURCE-ALLOCATION;
   CELLULAR NETWORKS; RELAY SELECTION; POWER
AB The continuous advances in the storage and transmission capabilities of smart devices have made them possible to share multimedia services with each other through device-to-device (D2D) communications. However, when D2D users transmit multimedia services in manner of cooperative communications, the relay users with non-cooperative behavior may lead to a sharp decline in the quality of services for the receivers. In this paper, a socially aware trust framework for multimedia delivery is proposed to choose the trustworthy D2D cooperative users from D2D relay users. By considering the different D2D cooperative scenarios, the trust relationships of entities in the network are divided into two cases: capability trust mainly from base station (BS) to relay users and social trust from sender to relay users. In particular, capability trust is quantified based on the service capability of a user, such as caching capability, processing capability, and transmission capability, and social trust is obtained based on the historical interactive behaviors among users, such as cooperative behavior, altruistic behavior, and reciprocal behavior. By analyzing such two types of trust, the trustworthy D2D cooperative users are selected from relay users to support different cooperative scenarios. The numerical results verify that the proposed trust framework can effectively identify the selfish users and notably enhance the delivery efficiency of multimedia services, such as delay and delivery success ratio.
C1 [Yan, Junjie; Wu, Dapeng; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Yan, Junjie; Wu, Dapeng; Wang, Ruyan] Key Lab Opt Commun & Networks, Chongqing 400065, Peoples R China.
   [Yan, Junjie; Wu, Dapeng; Wang, Ruyan] Key Lab Ubiquitous Sensing & Networking, Chongqing 400065, Peoples R China.
C3 Chongqing University of Posts & Telecommunications
RP Wu, DP (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.; Wu, DP (corresponding author), Key Lab Opt Commun & Networks, Chongqing 400065, Peoples R China.
EM cqupt2013yjj@sina.com; wudp@cqupt.edu.cn; wangry@cqupt.edu.cn
RI Wu, Dapeng/IWE-0674-2023; Yan, Jun/IXD-7801-2023
OI Wu, Dapeng/0000-0003-2105-9418; 
FU National Natural Science Foundation of China [61871062, 61771082];
   Program for Innovation Team Building at Institutions of Higher Education
   in Chongqing [CXTDX201601020]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871062 and Grant 61771082 and in part
   by the Program for Innovation Team Building at Institutions of Higher
   Education in Chongqing under Grant CXTDX201601020. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Su Zhou. (Corresponding author: Dapeng Wu.)
CR [Anonymous], 2016, CISC VIS NETW IND GL
   [Anonymous], 2011 IEEE VEH TECHN
   Cao Y, 2018, IEEE J SEL AREA COMM, V36, P905, DOI 10.1109/JSAC.2018.2824360
   Chaintreau A, 2007, IEEE T MOBILE COMPUT, V6, P606, DOI 10.1109/TMC.2007.1060
   Chang Z, 2018, IEEE T VEH TECHNOL, V67, P1570, DOI 10.1109/TVT.2017.2762745
   Chen X, 2015, IEEE ACM T NETWORK, V23, P1471, DOI 10.1109/TNET.2014.2329956
   Chen Y, 2009, IEEE T MULTIMEDIA, V11, P1170, DOI 10.1109/TMM.2009.2026101
   Datsika E, 2016, IEEE ACCESS, V4, P3697, DOI 10.1109/ACCESS.2016.2586305
   Lee D, 2012, 2012 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA 2012), P455
   Li Y, 2014, IEEE WIREL COMMUN, V21, P52, DOI 10.1109/MWC.2014.6940433
   Li Y, 2014, IEEE COMMUN MAG, V52, P150, DOI 10.1109/MCOM.2014.6829957
   Li Y, 2015, IEEE T WIREL COMMUN, V14, P4093, DOI 10.1109/TWC.2015.2416715
   Li ZD, 2019, IEEE J SEL AREA COMM, V37, P283, DOI 10.1109/JSAC.2018.2872374
   Liu GC, 2014, IEEE INFOCOM SER, P1698, DOI 10.1109/INFOCOM.2014.6848107
   Luo CQ, 2014, IEEE T PARALL DISTR, V25, P3211, DOI 10.1109/TPDS.2013.2297922
   Ma XR, 2012, 2012 IEEE 23RD INTERNATIONAL SYMPOSIUM ON PERSONAL INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P1020, DOI 10.1109/PIMRC.2012.6362495
   Ometov A, 2016, IEEE WIREL COMMUN, V23, P103, DOI 10.1109/MWC.2016.7553033
   Peng MG, 2016, IEEE NETWORK, V30, P46, DOI 10.1109/MNET.2016.7513863
   Scott James., 2009, CRAWDAD trace cambridge/haggle/imote/infocom2006 (v. 2009-05-29)
   Tan B, 2018, IEEE WIREL COMMUN, V25, P88, DOI 10.1109/MWC.2018.1800021
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   Wang L, 2015, IET COMMUN, V9, P342, DOI 10.1049/iet-com.2014.0436
   Wang TY, 2015, IEEE T WIREL COMMUN, V14, P7004, DOI 10.1109/TWC.2015.2463281
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wu D, 2017, IEEE T MULTIMEDIA, V19, P2571, DOI 10.1109/TMM.2017.2700621
   Wu DP, 2019, IEEE INTERNET THINGS, V6, P1928, DOI 10.1109/jiot.2018.2884485
   Wu DP, 2019, IEEE INTERNET THINGS, V6, P9266, DOI 10.1109/JIOT.2018.2888543
   Wu DP, 2019, IEEE T MULTIMEDIA, V21, P1788, DOI 10.1109/TMM.2018.2885931
   Wu DP, 2018, IEEE INTERNET THINGS, V5, P2958, DOI 10.1109/JIOT.2017.2768073
   Xiong JB, 2019, IEEE INTERNET THINGS, V6, P1530, DOI 10.1109/JIOT.2018.2842773
   Xu C, 2013, IEEE J SEL AREA COMM, V31, P348, DOI 10.1109/JSAC.2013.SUP.0513031
   Yao YY, 2011, INFORM SCIENCES, V181, P1080, DOI 10.1016/j.ins.2010.11.019
   Zhang HJ, 2017, IEEE J SEL AREA COMM, V35, P1936, DOI 10.1109/JSAC.2017.2720898
   Zhang HJ, 2016, IEEE WIREL COMMUN, V23, P48, DOI 10.1109/MWC.2016.1600066WC
   Zhang HM, 2015, IEEE INT CONF COMM, P675, DOI 10.1109/ICCW.2015.7247259
   Zhang MY, 2014, IEEE ICC, P2257, DOI 10.1109/ICC.2014.6883659
   Zhang Y, 2012, IEEE NETWORK, V26, P6, DOI 10.1109/MNET.2012.6201210
   Zhang Y, 2011, IEEE COMMUN MAG, V49, P44, DOI 10.1109/MCOM.2011.5741145
   Zhang YR, 2015, IEEE T WIREL COMMUN, V14, P177, DOI 10.1109/TWC.2014.2334661
   Zhang ZF, 2018, IEEE INTERNET THINGS, V5, P2323, DOI 10.1109/JIOT.2017.2749443
   Zhou L, 2018, IEEE INTERNET THINGS, V5, P1657, DOI 10.1109/JIOT.2017.2785624
   Zhou L, 2017, IEEE COMMUN MAG, V55, P91, DOI 10.1109/MCOM.2017.1700481
NR 42
TC 27
Z9 29
U1 0
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 625
EP 635
DI 10.1109/TMM.2018.2890196
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800009
DA 2024-07-18
ER

PT J
AU Yin, JL
   Chen, BH
   Li, Y
AF Yin, Jia-Li
   Chen, Bo-Hao
   Li, Ying
TI Highly Accurate Image Reconstruction for Multimodal Noise Suppression
   Using Semisupervised Learning on Big Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise removal; big image data; semisupervised learning
ID SWITCHING MEDIAN FILTER; IMPULSE NOISE; SPARSE REPRESENTATION; REMOVAL
AB Impulse noise corruption in digital images frequently occurs because of errors generated by noisy sensors or communication channels, such as faulty memory locations in devices, malfunctioning pixels within a camera, or bit errors in transmission. Although recently developed big data streaming enhances the viability of video communication, visual distortions in images caused by impulse noise corruption can negatively affect video communication applications. In addition, sparsity, density, and multimodality in large volumes of noisy images have often been ignored in recent studies, whereas these issues have become important because of the increasing viability of video communication services. To effectively eliminate the visual effects generated by the impulse noise from the corrupted images, this study proposes a novel model that uses a devised cost function involving semisupervised learning based on a large amount of corrupted image data with a few labeled training samples. The proposed model qualitatively and quantitatively outperforms the existing state-of-the-art image reconstruction models in terms of the denoising effect.
C1 [Yin, Jia-Li; Li, Ying] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350116, Fujian, Peoples R China.
   [Yin, Jia-Li; Chen, Bo-Hao] Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 320, Taiwan.
   [Chen, Bo-Hao] Yuan Ze Univ, Innovat Ctr Big Data & Digital Convergence, Taoyuan 320, Taiwan.
   [Li, Ying] Fuzhou Univ, Fujian Prov Key Lab Informat Secur & Network Syst, Fuzhou 350116, Fujian, Peoples R China.
C3 Fuzhou University; Yuan Ze University; Yuan Ze University; Fuzhou
   University
RP Chen, BH (corresponding author), Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 320, Taiwan.; Chen, BH (corresponding author), Yuan Ze Univ, Innovat Ctr Big Data & Digital Convergence, Taoyuan 320, Taiwan.
EM xiajiaran@sina.com; bhchen@saturn.yzu.edu.tw; fj_liying@fzu.edu.cn
RI ARSLAN, Okan/AAA-3232-2020
FU Ministry of Science and Technology, Taiwan [MOST 106-2221-E-155-066,
   MOST 106-2218-E-155-007, MOST 105-2218-E-155-003, MOST
   105-2218-E-155-010]; National Natural Science Foundation of China
   [61075022]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grants MOST 106-2221-E-155-066, MOST
   106-2218-E-155-007, MOST 105-2218-E-155-003, and MOST 105-2218-E-155-010
   and in part by the National Natural Science Foundation of China under
   Grant 61075022.
CR [Anonymous], IEEE T MULTIMEDIA
   BROWNRIGG DRK, 1984, COMMUN ACM, V27, P807, DOI 10.1145/358198.358222
   Chen BH, 2017, 2017 IEEE THIRD INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2017), P338, DOI 10.1109/BigMM.2017.42
   Chen CLP, 2015, IEEE T IMAGE PROCESS, V24, P4014, DOI 10.1109/TIP.2015.2456432
   Chen T, 2001, IEEE SIGNAL PROC LET, V8, P1, DOI 10.1109/97.889633
   Chen T, 2001, IEEE T CIRCUITS-II, V48, P784, DOI 10.1109/82.959870
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong YQ, 2007, IEEE T IMAGE PROCESS, V16, P1112, DOI 10.1109/TIP.2006.891348
   Esakkirajan S, 2011, IEEE SIGNAL PROC LET, V18, P287, DOI 10.1109/LSP.2011.2122333
   Huang DA, 2014, IEEE T MULTIMEDIA, V16, P83, DOI 10.1109/TMM.2013.2284759
   HWANG H, 1995, IEEE T IMAGE PROCESS, V4, P499, DOI 10.1109/83.370679
   Jakhetiya V, 2017, IEEE T MULTIMEDIA, V19, P93, DOI 10.1109/TMM.2016.2609419
   Jiang JL, 2014, IEEE T IMAGE PROCESS, V23, P2651, DOI 10.1109/TIP.2014.2317985
   KO SJ, 1991, IEEE T CIRCUITS SYST, V38, P984, DOI 10.1109/31.83870
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin PH, 2016, J DISP TECHNOL, V12, P344, DOI 10.1109/JDT.2015.2487559
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Liu LC, 2017, IEEE T CYBERNETICS, V47, P600, DOI 10.1109/TCYB.2016.2521428
   Rodriguez P., 2012, Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2012), P1077, DOI 10.1109/ICASSP.2012.6288073
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Toh KKV, 2010, IEEE SIGNAL PROC LET, V17, P281, DOI 10.1109/LSP.2009.2038769
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 1999, IEEE T CIRCUITS-II, V46, P78, DOI 10.1109/82.749102
   Wu J, 2016, IEEE T MULTIMEDIA, V18, P893, DOI 10.1109/TMM.2016.2535727
   Xiao Y, 2011, PATTERN RECOGN, V44, P1708, DOI 10.1016/j.patcog.2011.02.002
   Xie J, 2015, IEEE T MULTIMEDIA, V17, P1525, DOI 10.1109/TMM.2015.2457678
   Yuan GZ, 2015, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2015.7299175
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang PX, 2014, IEEE SIGNAL PROC LET, V21, P1280, DOI 10.1109/LSP.2014.2333012
NR 30
TC 43
Z9 45
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3045
EP 3056
DI 10.1109/TMM.2018.2820910
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800014
DA 2024-07-18
ER

PT J
AU Xiong, BA
   Liu, QG
   Xiong, JJ
   Li, SQ
   Wang, SS
   Liang, D
AF Xiong, Biao
   Liu, Qiegen
   Xiong, Jiaojiao
   Li, Sanqian
   Wang, Shanshan
   Liang, Dong
TI Field-of-Experts Filters Guided Tensor Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-rank tensor; tensor completion; field-of-experts; multi-view
   features; alternative minimization
ID IMAGE-RECONSTRUCTION; DECOMPOSITION; FACTORIZATION
AB Most low-rank tensor approximations are NP-hard problems. In this paper, we introduce a novel concept: field-of-experts (FoE) filters guided tensor completion, which aims to integrate the strengths of the emerging tensor completion method and the conventional FoE filters. Specifically, the target image is convolved by FoE filters to produce multiview features as a high-order tensor, which captures complementary information from multiple views. In order to impose the concept, we employ two strategies to model the new tensor, one is called FoE filters guided low-rank tensor completion, and another is called FoE filters guided simultaneous tensor decomposition and completion (FoE-STDC). The resulting objectives are solved efficiently by alternating minimization. Extensive experimental results validate the superior performance and robustness of the proposed methods over their corresponding counterparts in all cases. Particularly, the proposed FoE-STDC is superior to the state-of-the-art tensor completion methods.
C1 [Xiong, Biao] Dept Res & Dev DIPPER3D, Wuhan 430000, Hubei, Peoples R China.
   [Liu, Qiegen; Xiong, Jiaojiao; Li, Sanqian] Nanchang Univ, Dept Elect Informat Engn, Nanchang 330031, Jiangxi, Peoples R China.
   [Wang, Shanshan; Liang, Dong] Chinese Acad Sci, Paul C Lauterbur Res Ctr Biomed Imaging, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
C3 Nanchang University; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS
RP Liang, D (corresponding author), Chinese Acad Sci, Paul C Lauterbur Res Ctr Biomed Imaging, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM b.xiong@dipper3d.com; liuqiegen@ncu.edu.cn; xiongjiaojiao0126@163.com;
   lisanqian@email.ncu.edu.cn; sophiasswang@hotmail.com;
   dong.liang@siat.ac.cn
RI sanqian, li/KIK-7023-2024; Wang, Shanshan/T-6972-2017; Liang,
   Dong/A-3335-2011; Li, Yuanyuan/J-3539-2014; Liang, Dong/A-3335-2011
OI Wang, Shanshan/0000-0002-0575-6523; Liang, Dong/0000-0003-1358-9777; Li,
   Yuanyuan/0000-0001-6151-9306; Liang, Dong/0000-0001-6257-0875
FU National Natural Science Foundation of China [61362001, 61362009,
   61661031, 61471350]; Jiangxi advanced project for postdoctoral research
   fund [2014KY02]; Young and Key Scientist Training Plan of Jiangxi
   Province [20162BCB23019, 20171BBH80023, GJJ170566]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61362001, Grant 61362009, Grant
   61661031, and Grant 61471350; in part by the Jiangxi advanced project
   for postdoctoral research fund under Grant 2014KY02; and in part by the
   Young and Key Scientist Training Plan of Jiangxi Province under Grant
   20162BCB23019, Grant 20171BBH80023, and Grant GJJ170566.
CR Acar E, 2011, CHEMOMETR INTELL LAB, V106, P41, DOI 10.1016/j.chemolab.2010.08.004
   [Anonymous], 2002, THESIS STANFORD U
   [Anonymous], 2007, P KDD CUP WORKSH
   [Anonymous], MATH PROGRAM
   [Anonymous], ARXIV14073254
   [Anonymous], ARXIV161007126
   Bach FR, 2008, J MACH LEARN RES, V9, P1019
   Bengua JA, 2016, 2016 10TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND COMMUNICATION SYSTEMS (ICSPCS), DOI 10.1109/ICSPCS.2016.7843326
   Bengua JA, 2017, IEEE T IMAGE PROCESS, V26, P2466, DOI 10.1109/TIP.2017.2672439
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5
   Chen YL, 2014, IEEE T PATTERN ANAL, V36, P577, DOI 10.1109/TPAMI.2013.164
   Ding T., 2007, PROC IEEE 11 INT C C, P1
   Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010
   Geng X, 2011, IEEE T SYST MAN CY B, V41, P881, DOI 10.1109/TSMCB.2010.2097588
   Guo XJ, 2015, PROC CVPR IEEE, P3603, DOI 10.1109/CVPR.2015.7298983
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Ji TY, 2016, INFORM SCIENCES, V326, P243, DOI 10.1016/j.ins.2015.07.049
   Kiers HAL, 2000, J CHEMOMETR, V14, P105, DOI 10.1002/1099-128X(200005/06)14:3<105::AID-CEM582>3.0.CO;2-I
   Kressner D, 2014, BIT, V54, P447, DOI 10.1007/s10543-013-0455-z
   Li K, 2012, IEEE T SYST MAN CY B, V42, P539, DOI 10.1109/TSMCB.2011.2168953
   Li N, 2010, IEEE IMAGE PROC, P517, DOI 10.1109/ICIP.2010.5651225
   Liansheng Zhuang, 2011, Proceedings of the Sixth International Conference on Image and Graphics (ICIG 2011), P511, DOI 10.1109/ICIG.2011.86
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39
   Liu J, 2009, IEEE I CONF COMP VIS, P2114
   Liu QG, 2013, SIAM J IMAGING SCI, V6, P1689, DOI 10.1137/110857349
   Liu QG, 2013, IEEE T IMAGE PROCESS, V22, P4652, DOI 10.1109/TIP.2013.2277798
   Liu QG, 2013, IEEE T MED IMAGING, V32, P1290, DOI 10.1109/TMI.2013.2256464
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meyer Y., 2001, OSCILLATING PATTERNS, V22
   Narita A, 2012, DATA MIN KNOWL DISC, V25, P298, DOI 10.1007/s10618-012-0280-z
   Oreifej O, 2013, IEEE T PATTERN ANAL, V35, P450, DOI 10.1109/TPAMI.2012.97
   Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286
   Osher S, 2005, MULTISCALE MODEL SIM, V4, P460, DOI 10.1137/040605412
   Osher S, 2003, MULTISCALE MODEL SIM, V1, P349, DOI 10.1137/S1540345902416247
   Peng X, 2015, IEEE SIGNAL PROC LET, V22, P1184, DOI 10.1109/LSP.2014.2376699
   Qiu Q, 2015, J MACH LEARN RES, V16, P187
   Rai P, 2014, PR MACH LEARN RES, V32, P1800
   Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6
   Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Sorber L, 2013, SIAM J OPTIMIZ, V23, P695, DOI 10.1137/120868323
   Tan HC, 2014, NEUROCOMPUTING, V133, P161, DOI 10.1016/j.neucom.2013.11.020
   Tucker L., 1963, PROBLEMS MEASURING C, V15, P122
   Vese LA, 2003, J SCI COMPUT, V19, P553, DOI 10.1023/A:1025384832106
   Wang H, 2014, AAAI CONF ARTIF INTE, P2846
   Wang YX, 2011, IEEE IMAGE PROC, P3409, DOI 10.1109/ICIP.2011.6116443
   Yang S, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P641
   Yang SY, 2014, IEEE T IMAGE PROCESS, V23, P2793, DOI 10.1109/TIP.2014.2319742
   Yilmaz Y.K., 2011, Advances in Neural Information Processing Systems 24, P2151
   Yokota T, 2017, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2017.409
   Yokota T, 2016, IEEE T SIGNAL PROCES, V64, P5423, DOI 10.1109/TSP.2016.2586759
   Yokota T, 2016, INT CONF ACOUST SPEE, P2514, DOI 10.1109/ICASSP.2016.7472130
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhao QB, 2015, IEEE T PATTERN ANAL, V37, P1751, DOI 10.1109/TPAMI.2015.2392756
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
NR 57
TC 15
Z9 15
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2316
EP 2329
DI 10.1109/TMM.2018.2806225
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200007
DA 2024-07-18
ER

PT J
AU Zhang, SY
   Yang, Y
   Xiao, J
   Liu, XM
   Yang, Y
   Xie, D
   Zhuang, YT
AF Zhang, Songyang
   Yang, Yang
   Xiao, Jun
   Liu, Xiaoming
   Yang, Yi
   Xie, Di
   Zhuang, Yueting
TI Fusing Geometric Features for Skeleton-Based Action Recognition Using
   Multilayer LSTM Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; skeleton; geometric feature; LSTM; score fusion
ID JOINTS
AB Recent skeleton-based action recognition approaches achieve great improvement by using recurrent neural network (RNN) models. Currently, these approaches build an end-to-end network from coordinates of joints to class categories and improve accuracy by extending RNN to spatial domains. First, while such well-designed models and optimization strategies explore relations between different parts directly from joint coordinates, we provide a simple universal spatial modeling method perpendicular to the RNN model enhancement. Specifically, according to the evolution of previous work, we select a set of simple geometric features, and then seperately feed each type of features to a three-layer LSTM framework. Second, we propose a multistream LSTM architecture with a new smoothed score fusion technique to learn classification from different geometric feature streams. Furthermore, we observe that the geometric relational features based on distances between joints and selected lines outperform other features and the fusion results achieve the state-of-the-art performance on four datasets. We also show the sparsity of input gate weights in the first LSTM layer trained by geometric features and demonstrate that utilizing joint-line distances as input require less data for training.
C1 [Zhang, Songyang; Yang, Yang; Xiao, Jun; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Liu, Xiaoming] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
   [Yang, Yi] Univ Technol Sydney, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW 2007, Australia.
   [Xie, Di] Hikvis Res Inst, Hangzhou 310051, Zhejiang, Peoples R China.
C3 Zhejiang University; Michigan State University; University of Technology
   Sydney
RP Yang, Y (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM zhangsongyang@zju.edu.cn; yangya@zju.edu.cn; junx@cs.zju.edu.cn;
   liuxm@cse.msu.edu; yi.yang@uts.edu.au; xiedi@hikvision.com;
   yzhuang@cs.zju.edu.cn
RI yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022; yang,
   yang/GVT-5210-2022; Yang, Yi/B-9273-2017; Lang, Ming/HIK-0758-2022
OI Yang, Yi/0000-0002-0512-880X; , Di/0000-0001-8065-5901; Zhang,
   Songyang/0000-0003-4316-3320
FU National Key Research and Development Program of China [2017YFB0203001];
   National Natural Science Foundation of China [61572431]; Zhejiang
   Natural Science Foundation [LZ17F020001]; Key R&D Program of Zhejiang
   Province [2018C01006]; ZJU; Hikvision Research Institute
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB0203001, in part by the
   National Natural Science Foundation of China under Grant 61572431, in
   part by the Zhejiang Natural Science Foundation under Grant LZ17F020001,
   in part by the Key R&D Program of Zhejiang Province under Grant
   2018C01006, and in part by the Joint Research Program of ZJU & Hikvision
   Research Institute.
CR Anirudh R, 2015, PROC CVPR IEEE, P3147, DOI 10.1109/CVPR.2015.7298934
   [Anonymous], 2016, P INT C LEARN REPR W
   [Anonymous], 2015, PROC CVPR IEEE
   Aydin R, 2014, IN C IND ENG ENG MAN, P1, DOI 10.1109/IEEM.2014.7058588
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Breuel T. M., ARXIV150802774
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Chen C, 2011, IEEE T VIS COMPUT GR, V17, P1676, DOI 10.1109/TVCG.2010.272
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Evangelidis G, 2014, INT C PATT RECOG, P4513, DOI 10.1109/ICPR.2014.772
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Hinton G. E., 2012, ARXIV PREPRINT ARXIV
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kapsouras I, 2014, J VIS COMMUN IMAGE R, V25, P1432, DOI 10.1016/j.jvcir.2014.04.007
   Li CK, 2017, IEEE SIGNAL PROC LET, V24, P624, DOI 10.1109/LSP.2017.2678539
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Li WB, 2015, IEEE I CONF COMP VIS, P4444, DOI 10.1109/ICCV.2015.505
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Mahasseni B, 2016, PROC CVPR IEEE, P3054, DOI 10.1109/CVPR.2016.333
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Ofli F, 2013, IEEE WORK APP COMP, P53, DOI 10.1109/WACV.2013.6474999
   Ohn-Bar E, 2013, IEEE COMPUT SOC CONF, P465, DOI 10.1109/CVPRW.2013.76
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Shahri Alimohammad, 2016, 2016 IEEE Tenth International Conference on Research Challenges in Information Science (RCIS), P1, DOI 10.1109/RCIS.2016.7549312
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Vinagre M, 2015, LECT NOTES ELECTR EN, V325, P263, DOI 10.1007/978-3-319-10891-9_15
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xia L., 2012, IEEE COMP SOC C COMP, P20, DOI DOI 10.1109/CVPRW.2012.6239233
   Xu DJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1645, DOI 10.1145/3123266.3123427
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yao A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.67
   Yimeng Zhang, 2012, 2012 IEEE Workshop on Applications of Computer Vision (WACV), P249, DOI 10.1109/WACV.2012.6163009
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang HW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P781, DOI 10.1145/2964284.2964308
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
   Zhang YM, 2012, LECT NOTES COMPUT SC, V7574, P707, DOI 10.1007/978-3-642-33712-3_51
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 53
TC 130
Z9 143
U1 5
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2330
EP 2343
DI 10.1109/TMM.2018.2802648
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200008
DA 2024-07-18
ER

PT J
AU Fu, Q
   Luo, Y
   Wen, YG
   Tao, DC
   Li, Y
   Duan, LY
AF Fu, Qiang
   Luo, Yong
   Wen, Yonggang
   Tao, Dacheng
   Li, Ying
   Duan, Ling-Yu
TI Toward Intelligent Product Retrieval for TV-to-Online (T2O) Application:
   A Transfer Metric Learning Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE TV-to-Online; distance metric learning; transfer learning; heterogeneous
   domains; manifold regularization; ranking-based loss
ID MANIFOLD REGULARIZATION; IMAGE; SCENE
AB It is desired (especially for young people) to shop for the same or similar products shown in the multimedia contents (such as online TV programs). This indicates an urgent demand for improving the experience of TV-to-Online (T2O). In this paper, a transfer learning approach as well as a prototype system for effortless T2O experience is developed. In the system, a key component is high-precision product search, which is to fulfill exact matching between a query item and the database ones. The matching performance primarily relies on distance estimation, but the data characteristics cannot be well modeled and exploited by a simple Euclidean distance. This motivates us to introduce distance metric learning (DML) for improving the distance estimation. However, in traditional DML methods. the side information (such as the similar/dissimilar constraints or relevance/irrelevance judgements) in the target domain is leveraged. These methods may fail due to limited side information. Fortunately, this issue can be alleviated by utilizing transfer metric learning (TML) to exploit information from other related domains. In this paper,a novel manifold regularized heterogeneous multitask metric learning framework is proposed, in which each domain is treated equally. The proposed approach allows us to simultaneously exploit the information from other domains and the unlabeled information. Furthermore, the ranking-based loss is adopted to make our model more appropriate for search. Experiments on two challenging real-world datasets demonstrate the effectiveness of the proposed method. This TML approach is expected to impact the transformation of the emerging T2O trend in both TV and online video domains.
C1 [Fu, Qiang; Li, Ying] Peking Univ, Sch Software & Microelect, Beijing 100080, Peoples R China.
   [Luo, Yong; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, UBTECH Sydney Artificial Intelligence Ctr, Darlington, NSW 2008, Australia.
   [Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, Sch Informat Technol, Darlington, NSW 2008, Australia.
   [Duan, Ling-Yu] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100080, Peoples R China.
C3 Peking University; Nanyang Technological University; University of
   Sydney; University of Sydney; Peking University
RP Duan, LY (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100080, Peoples R China.
EM qiang.fu@pku.edu.cn; yluo180@gmail.com; ygwen@ntu.edu.sg;
   dacheng.tao@sydney.edu.au; li.ying@pku.edu.cn; lingyu@pku.edu.cn
RI Wen, Yonggang/B-8848-2011; Wen, Yonggang/P-9406-2017; Tao,
   Dacheng/A-5449-2012; Fu, Qiang/GZM-6691-2022
OI Wen, Yonggang/0000-0002-2751-5114; Tao, Dacheng/0000-0001-7225-5449;
   Luo, Yong/0000-0002-2296-6370; Fu, Qiang/0000-0003-1774-6805
FU Tier 1 projects [RG17/14, RG26/16]; Data Science and Artificial
   Intelligence Research Centre, Nanyang Technological University;
   Australian Research Council [FL-170100117, DP-180103424, DP-140102164,
   LP-150100671]; Key Research and Development Program of Beijing Municipal
   Science and Technology Commission [D171100003517002]; National Natural
   Science Foundation of China [U1611461, 61661146005]; National Key
   Research and Development Program of China [2016YFB1001501]; 
   [NRF2015ENC-GDCR01001-003];  [NRF2015ENC-GBICRD001-012]
FX This work was supported in part by Singapore NRF2015ENC-GDCR01001-003
   (administrated via IMDA), NRF2015ENC-GBICRD001-012 (administrated via
   BCA), by Tier 1 projects (RG17/14, RG26/16), by the Data Science and
   Artificial Intelligence Research Centre, Nanyang Technological
   University, and by Australian Research Council Projects FL-170100117,
   DP-180103424, DP-140102164, and LP-150100671. This work was also
   supported by the Key Research and Development Program of Beijing
   Municipal Science and Technology Commission (No. D171100003517002), in
   part by the National Natural Science Foundation of China under Grant
   U1611461 and Grant 61661146005, and by the National Key Research and
   Development Program of China under Grant 2016YFB1001501. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Honggang Wang.
CR [Anonymous], ACM T INTELLIGENT SY
   [Anonymous], 2012, ARXIV12064660
   [Anonymous], 2015, MICROMOMENTS YOUR GU
   [Anonymous], 2014, ARXIV13066709V4
   [Anonymous], TECH REP
   [Anonymous], 2011, P 25 AAAI
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Chua Tat-Seng, 2009, CIVR
   Davis J. V., 2007, ICML, P209
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995
   Everingham Mark, 2007, PASCAL VISUAL OBJECT
   Geng B, 2011, IEEE T IMAGE PROCESS, V20, P2980, DOI 10.1109/TIP.2011.2134107
   Goldberger J., 2004, P 17 INT C NEUR INF, P513
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Jin YC, 2013, IEEE GLOB COMM CONF, P1747, DOI 10.1109/GLOCOM.2013.6831326
   Joachims Thorsten, 2005, P 22 INT C MACH LEAR, P377, DOI DOI 10.1145/1102351.1102399
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Kulis B, 2011, PROC CVPR IEEE, P1785, DOI 10.1109/CVPR.2011.5995702
   Li Ge, 2012, Journal of Knowledge-Based Innovation in China, V4, P55, DOI 10.1108/17561411211208767
   Lim DKH, 2014, PR MACH LEARN RES, V32, P1980
   Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756
   Liu TI, 2008, C IND ELECT APPL, P1, DOI 10.1109/ICIEA.2008.4582469
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo Y, 2018, IEEE T NEUR NET LEAR, V29, P4051, DOI 10.1109/TNNLS.2017.2750321
   Luo Y, 2016, IEEE T IMAGE PROCESS, V25, P414, DOI 10.1109/TIP.2015.2495116
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Luo Y, 2014, IEEE T IMAGE PROCESS, V23, P3789, DOI 10.1109/TIP.2014.2332398
   Luo Y, 2013, IEEE T NEUR NET LEAR, V24, P709, DOI 10.1109/TNNLS.2013.2238682
   McFee B., 2010, ICML, P775
   Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5
   Ng A. Y., 2002, Advances in Neural Information Processing Systems, P1473
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parameswaran S., 2010, ADV NEURAL INFORM PR, P1867
   Richang Hong, 2015, IEEE Transactions on Big Data, V1, P152, DOI 10.1109/TBDATA.2016.2515640
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Suzuki R, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P57, DOI 10.1145/2702123.2702358
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Wang C., 2011, 22 INT JOINT C ART I, P1541
   Wang HB, 2016, IEEE T MULTIMEDIA, V18, P1579, DOI 10.1109/TMM.2016.2569412
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Xiaoxiao Shi, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1049, DOI 10.1109/ICDM.2010.65
   Xie Pengtao., 2013, Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, P1806
   Yang PP, 2013, MACH LEARN, V92, P133, DOI 10.1007/s10994-013-5379-y
   Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271
   Zhou JTY, 2014, JMLR WORKSH CONF PRO, V33, P1095
NR 49
TC 6
Z9 6
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2114
EP 2125
DI 10.1109/TMM.2018.2791803
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600016
DA 2024-07-18
ER

PT J
AU Kroher, N
   Pikrakis, A
   Díaz-Báñez, JM
AF Kroher, Nadine
   Pikrakis, Aggelos
   Diaz-Banez, Jose-Miguel
TI Discovery of Repeated Melodic Phrases in Folk Singing Recordings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music information retrieval; melodic pattern discovery; melody
   segmentation
ID SIMILARITY; TRANSCRIPTION; REPETITION; PERCEPTION
AB In music, repetition is a fundamental concept to establish structure and create temporal relationships. Previous approaches to detecting repetition in music recordings have mainly focused on discovering repeated patterns of variable length and instrumentation at arbitrary locations. In this paper, we present a novel method for the discovery of repeated sung phrases in folk music recordings and, in particular, in oral music traditions, where written scores are usually unavailable. At a first stage, a segmentation algorithm partitions automatically generated note-level transcriptions of the singing melody into sections that correspond to the structural unit of a phrase. A clustering algorithm is then used to form clusters of phrases, where each cluster contains instances of the same melodic content. The clustering algorithm operates on the basis of a distance measure between melodic sequences and, to this end, various melodic distance measures are investigated. A detailed evaluation procedure is used to assess the performance of the algorithm on three different European music traditions and the influence of transcription and segmentation errors is investigated. The proposed system is shown to outperform the state-of-the-art in audio-based approaches to repeated phrase discovery for this task.
C1 [Kroher, Nadine; Diaz-Banez, Jose-Miguel] Univ Seville, Dept Appl Math 2, Seville 41004, Spain.
   [Pikrakis, Aggelos] Univ Piraeus, Dept Informat, Piraeus 18534, Greece.
C3 University of Sevilla; University of Piraeus
RP Kroher, N (corresponding author), Univ Seville, Dept Appl Math 2, Seville 41004, Spain.
EM nkroher@us.es; pikrakis@unipi.gr; dbanez@us.es
RI Pikrakis, Aggelos/AAR-1334-2021; Kroher, Nadine/C-9414-2017; Diaz-Banez,
   Jose Miguel/L-1996-2014
OI Pikrakis, Aggelos/0000-0001-7355-327X; Diaz-Banez, Jose
   Miguel/0000-0002-4031-4309
FU Junta de Andalucia; FEDER funds of the European Union, Project COFLAII
   [P12-TIC-1362]
FX This work was supported in part by the Junta de Andalucia and the FEDER
   funds of the European Union, Project COFLAII under the Grant
   P12-TIC-1362. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yi-Hsuan Yang.
CR [Anonymous], P INT C MUS INF RETR
   Benetos E, 2013, J INTELL INF SYST, V41, P407, DOI 10.1007/s10844-013-0258-3
   Bohak C, 2016, MATH PROBL ENG, V2016, DOI 10.1155/2016/8297987
   Boot P, 2016, J NEW MUSIC RES, V45, P223, DOI 10.1080/09298215.2016.1208666
   BRADLEY IL, 1971, J RES MUSIC EDUC, V19, P295, DOI 10.2307/3343764
   Collins T., 2014, Audio Engineering Society Conference: 53rd International Conference: Semantic Audio, P1
   Dannenberg RB, 2003, J NEW MUSIC RES, V32, P153, DOI 10.1076/jnmr.32.2.153.16738
   Dowling W. J., ATTENTION PERCEPTION, V14, P37
   Ellis DPW, 2007, J NEW MUSIC RES, V36, P51, DOI 10.1080/09298210701653344
   Ellis DPW, 2007, INT CONF ACOUST SPEE, P1429
   Frieler K., 2014, PROC INT WORKSHOP FO, P108
   Gulati S, 2014, 10TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS SITIS 2014, P264, DOI 10.1109/SITIS.2014.73
   Janssen B., 2013, International Symposium on Computer Music Modeling and Retrieval. CMMR 2013: Sound, Music, P277
   Kaufman L., 1987, Statistical Data Analysis Based on the L1-Norm and Related Methods. First International Conference, P405
   Kotsifakos A., 2012, P 5 INT C PERV TECHN
   Kroher N., 2016, P EUR SIGN PROC C NI, P41
   Kroher N, 2016, ACM J COMPUT CULT HE, V9, DOI 10.1145/2875428
   Kroher N, 2016, IEEE-ACM T AUDIO SPE, V24, P901, DOI 10.1109/TASLP.2016.2531284
   Kruskall J.B., 1983, Time Warps, String Edits and Macromolecules
   Lamont A, 2001, MUSIC PERCEPT, V18, P245, DOI 10.1525/mp.2001.18.3.245
   Lerdahl F., 1985, A Generative Theory of Tonal Music
   Levenshtein V. I., 1966, SOV PHYS DOKL, V10, P707
   Li ML, 2015, 2015 12TH INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (FSKD), P1966, DOI 10.1109/FSKD.2015.7382250
   Lomax A., 1968, FOLK SONG STYLE CULT, V88
   Margulis EH, 2012, MUSIC PERCEPT, V29, P377, DOI 10.1525/MP.2012.29.4.377
   Mauch Matthias, 2015, P INT C TECHN MUS NO, P23
   MCCULLOUGH LE, 1977, ETHNOMUSICOLOGY, V21, P85, DOI 10.2307/850853
   McNab R. J., 1996, Proceedings of the 1st ACM International Conference on Digital Libraries, P11, DOI 10.1145/226931.226934
   MONGEAU M, 1990, COMPUT HUMANITIES, V24, P161, DOI 10.1007/BF00117340
   Müllensiefen D, 2006, STUD CLASS DATA ANAL, P299, DOI 10.1007/3-540-34416-0_32
   Müller M, 2013, IEEE T AUDIO SPEECH, V21, P531, DOI 10.1109/TASL.2012.2227732
   Muller M., 2012, P ITG C SPEECH COMM, P1
   Muller M., 2009, P 10 INT SOC MUS INF, P735
   Muller M., 2015, FUNDAMENTALS MUSIC P
   Nieto O., 2014, Proc. of the 15th International Society for Music Information Retrieval Conference, P411
   Novello A., 2006, P 7 INT C MUS INF RE, P246
   Pikrakis A, 2016, EUR SIGNAL PR CONF, P1212, DOI 10.1109/EUSIPCO.2016.7760441
   Rodriguez-Lopez M., 2013, INT S COMPUTER MUSIC, P548
   Rodriguez-Lopez Marcelo E., 2015, Mathematics and Computation in Music. 5th International Conference, MCM 2015. Proceedings: LNCS 9110, P73, DOI 10.1007/978-3-319-20603-5_7
   ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Song LM, 2013, 2013 9TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P349, DOI 10.1109/CIS.2013.80
   Temperley D., 2004, The cognition of basic musical structures
   Turnbull D., 2007, Proc. of the 5th International Society of Music Information Retrieval, P42
   Typke R, 2007, MUSIC SCI, P153
   Typke Rainer., 2003, ISMIR 2003: Proceedings of the Fourth International Conference on Music Information Retrieval, P107
   Urbano J, 2011, LECT NOTES COMPUT SC, V6684, P338, DOI 10.1007/978-3-642-23126-1_21
   van Kranenburg P., 2010, THESIS
   van Kranenburg P., 2014, TECH REP
   van Kranenburg P., 2010, P INT C MUS PERC COG, P794
   Velardo V, 2016, COMPUT MUSIC J, V40, P70, DOI 10.1162/COMJ_a_00359
   Walton C. W., 2005, BASIC FORMS IN MUSIC
   Wang CI, 2015, INT CONF ACOUST SPEE, P683, DOI 10.1109/ICASSP.2015.7178056
NR 54
TC 5
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1291
EP 1304
DI 10.1109/TMM.2017.2771450
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400002
DA 2024-07-18
ER

PT J
AU Zhang, SQ
   Zhang, SL
   Huang, TJ
   Gao, W
AF Zhang, Shiqing
   Zhang, Shiliang
   Huang, Tiejun
   Gao, Wen
TI Speech Emotion Recognition Using Deep Convolutional Neural Network and
   Discriminant Temporal Pyramid Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Speech emotion recognition; feature learning; deep convolutional neural
   network; discriminant temporal pyramid matching; Lp-norm pooling
ID FEATURES; DIMENSIONALITY
AB Speech emotion recognition is challenging because of the affective gap between the subjective emotions and low-level features. Integrating multilevel feature learning and model training, deep convolutional neural networks (DCNN) has exhibited remarkable success in bridging the semantic gap in visual tasks like image classification, object detection. This paper explores how to utilize a DCNN to bridge the affective gap in speech signals. To this end, we first extract three channels of log Mel-spectrograms (static, delta, and delta delta) similar to the red, green, blue (RGB) image representation as the DCNN input. Then, the AlexNet DCNN model pretrained on the large ImageNet dataset is employed to learn high-level feature representations on each segment divided from an utterance. The learned segment-level features are aggregated by a discriminant temporal pyramid matching (DTPM) strategy. DTPM combines temporal pyramid matching and optimal Lp-norm pooling to form a global utterance-level feature representation, followed by the linear support vector machines for emotion classification. Experimental results on four public datasets, that is, EMO-DB, RML, eNTERFACE05, and BAUM-1s, show the promising performance of our DCNN model and the DTPM strategy. Another interesting finding is that the DCNN model pretrained for image applications performs reasonably good in affective speech feature extraction. Further fine tuning on the target emotional speech datasets substantially promotes recognition performance.
C1 [Zhang, Shiqing; Zhang, Shiliang; Huang, Tiejun; Gao, Wen] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
   [Zhang, Shiqing] Taizhou Univ, Inst Intelligent Informat Proc, Taizhou 317000, Peoples R China.
C3 Peking University; Taizhou University
RP Zhang, SQ (corresponding author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
EM tzczsq@pku.edu.cn; slzhang.jdl@pku.edu.cn; tjhuang@pku.edu.cn;
   wgao@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
OI Zhang, Shiqing/0000-0001-8184-5088
FU National Science Foundation of China; Zhejiang Provincial National
   Science Foundation of China [61572050, LY16F020011, 91538111,
   61620106009]; National 1000 Youth Talents Plan
FX This work was supported in part by the National Science Foundation of
   China; in part by the Zhejiang Provincial National Science Foundation of
   China under Grants 61572050, LY16F020011, 91538111, and 61620106009; and
   in part by the National 1000 Youth Talents Plan.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Albornoz EM, 2011, COMPUT SPEECH LANG, V25, P556, DOI 10.1016/j.csl.2010.10.001
   Anagnostopoulos CN, 2015, ARTIF INTELL REV, V43, P155, DOI 10.1007/s10462-012-9368-5
   [Anonymous], AMSTER658
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2011, P CVPR 2011 COL SPRI
   [Anonymous], REAL TIME SPEECH MUS
   [Anonymous], ICSLP
   [Anonymous], 2013, 2013 10 IEEE INT C W, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2011, 22 INT JT C ART INT, DOI 10.5555/2283516.2283603
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Bozkurt E, 2015, SIG PROCESS COMMUN, P1374, DOI 10.1109/SIU.2015.7130097
   Burkhardt F, 2005, INTERSPEECH, V5, P1517, DOI DOI 10.21437/INTERSPEECH.2005-446
   Cao HW, 2015, COMPUT SPEECH LANG, V29, P203, DOI 10.1016/j.csl.2014.04.002
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Dellaert F, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1970, DOI 10.1109/ICSLP.1996.608022
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Elmadany NE, 2016, IEEE INT SYMP CIRC S, P590, DOI 10.1109/ISCAS.2016.7527309
   Eyben F, 2016, IEEE T AFFECT COMPUT, V7, P190, DOI 10.1109/TAFFC.2015.2457417
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gao L, 2016, INT CONF ACOUST SPEE, P2817, DOI 10.1109/ICASSP.2016.7472191
   Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34
   Han K, 2014, INTERSPEECH, P223
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang ZW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P801, DOI 10.1145/2647868.2654984
   Iliev AI, 2010, COMPUT SPEECH LANG, V24, P445, DOI 10.1016/j.csl.2009.02.005
   Jin Q, 2015, INT CONF ACOUST SPEE, P4749, DOI 10.1109/ICASSP.2015.7178872
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Luengo I, 2010, IEEE T MULTIMEDIA, V12, P490, DOI 10.1109/TMM.2010.2051872
   Lugger M, 2007, INT CONF ACOUST SPEE, P17
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20
   Morrison D, 2007, SPEECH COMMUN, V49, P98, DOI 10.1016/j.specom.2006.11.004
   Nicholson J, 2000, NEURAL COMPUT APPL, V9, P290, DOI 10.1007/s005210070006
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Petrushin ValeryA., 2000, PROC ICSLP 2000, P222
   Provost EM, 2013, INT CONF ACOUST SPEE, P3682, DOI 10.1109/ICASSP.2013.6638345
   Quiros-Ramirez MA, 2015, INT J MACH LEARN CYB, V6, P119, DOI 10.1007/s13042-013-0192-2
   Ramakrishnan S, 2013, TELECOMMUN SYST, V52, P1467, DOI 10.1007/s11235-011-9624-z
   Schuller B, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P577
   Schuller B, 2013, INTERSPEECH, P148
   Schuller B, 2011, IEEE T AFFECT COMPUT, V2, P192, DOI 10.1109/T-AFFC.2011.17
   Schuller B, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P552, DOI 10.1109/ASRU.2009.5372886
   Schuller B, 2006, INTERSPEECH 2006 AND 9TH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, VOLS 1-5, P1818
   Shami M. T., 2005, 2005 IEEE International Conference on Multimedia and Expo
   Stuhlsatz A, 2011, INT CONF ACOUST SPEE, P5688
   Sun YX, 2015, BIOMED SIGNAL PROCES, V18, P80, DOI 10.1016/j.bspc.2014.10.008
   Sundberg J, 2011, IEEE T AFFECT COMPUT, V2, P162, DOI 10.1109/T-AFFC.2011.14
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tawari A, 2010, IEEE T MULTIMEDIA, V12, P502, DOI 10.1109/TMM.2010.2058095
   Trigeorgis G, 2016, INT CONF ACOUST SPEE, P5200, DOI 10.1109/ICASSP.2016.7472669
   Valstar M., 2013, P 3 ACM INT WORKSHOP, DOI [10.1145/2512530.2512533, DOI 10.1145/2512530.2512533]
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang KX, 2015, IEEE T AFFECT COMPUT, V6, P69, DOI 10.1109/TAFFC.2015.2392101
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Wu SQ, 2011, SPEECH COMMUN, V53, P768, DOI 10.1016/j.specom.2010.08.013
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhalehpour S, 2017, IEEE T AFFECT COMPUT, V8, P300, DOI 10.1109/TAFFC.2016.2553038
   Zhang SL, 2010, IEEE T MULTIMEDIA, V12, P510, DOI 10.1109/TMM.2010.2059634
   Zhang SQ, 2008, LECT NOTES COMPUT SC, V5264, P457
   Zhao XM, 2015, NEURAL COMPUT APPL, V26, P735, DOI 10.1007/s00521-014-1755-1
   Zhao XM, 2014, NEURAL COMPUT APPL, V24, P1539, DOI 10.1007/s00521-013-1377-z
NR 70
TC 239
Z9 260
U1 11
U2 167
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1576
EP 1590
DI 10.1109/TMM.2017.2766843
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400023
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Quan, R
   Han, JW
   Zhang, DW
   Nie, FP
   Qian, XM
   Li, XL
AF Quan, Rong
   Han, Junwei
   Zhang, Dingwen
   Nie, Feiping
   Qian, Xueming
   Li, Xuelong
TI Unsupervised Salient Object Detection via Inferring From Imperfect
   Saliency Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; weak prediction; fusion strategy; local
   spatial consistency constraint
ID ATTENTION
AB Visual saliency detection has become an active research direction in recent years. A large number of saliency models, which can automatically locate objects of interest in images, have been developed. As these models take advantage of different kinds of prior assumptions, image features, and computational methodologies, they have their own strengths and weaknesses and may cope with only one or a few types of images well. Inspired by these facts, this paper proposes a novel salient object detection approach with the idea of inferring a superior model from a variety of previous imperfect saliency models via optimally leveraging the complementary information among them. The proposed approach mainly consists of three steps. First, a number of existing unsupervised saliency models are adopted to provide weak/imperfect saliency predictions for each region in the image. Then, a fusion strategy is used to fuse each image region's weak saliency predictions into a strong one by simultaneously considering the performance differences among various weak predictions and various characteristics of different image regions. Finally, a local spatial consistency constraint that ensures high similarity of the saliency labels for neighboring image regions with similar features is proposed to refine the results. Comprehensive experiments on five public benchmark datasets and comparisons with a number of state-of-the-art approaches can demonstrate the effectiveness of the proposed work.
C1 [Quan, Rong; Han, Junwei; Zhang, Dingwen] Northwestern Polytech Univ, Sch Automat, Xian 710072, Shaanxi, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Nie, Feiping] Northwestern Polytech Univ, Ctr Opt IMagery Anal & Learning, Xian 710072, Shaanxi, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Shaanxi, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Northwestern Polytechnical University; Xi'an Jiaotong
   University; Chinese Academy of Sciences; Xi'an Institute of Optics &
   Precision Mechanics, CAS
RP Han, JW (corresponding author), Northwestern Polytech Univ, Sch Automat, Xian 710072, Shaanxi, Peoples R China.
EM rongquan0806@gmail.com; junweihan2010@gmail.com;
   zhangdingwen2006yyy@gmail.com; feipingnie@gmail.com;
   qianxm@mail.xjtu.edu.cn; xuelong_li@ieee.org
RI zhang, dingwen/R-3463-2019; Li, Xuelong/Z-3785-2019; li,
   xiang/GWM-6319-2022; Li, Xuelong/ABF-3381-2020; Quan,
   Rong/AAA-9401-2021; Nie, Feiping/B-3039-2012
OI zhang, dingwen/0000-0001-8369-8886; Li, Xuelong/0000-0002-0019-4197;
   Nie, Feiping/0000-0002-0871-6519
FU National Science Foundation of China [61473231]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61473231.
CR Achanta R, 2008, LECT NOTES COMPUT SC, V5008, P66
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen TC, 2009, PROC EUR SOLID-STATE, P1
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Hou X., 2007, P INT C IEEE COMP VI
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jiang P, 2015, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2015.33
   Ko BC, 2006, J OPT SOC AM A, V23, P2462, DOI 10.1364/JOSAA.23.002462
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li X, 2013, IEEE I CONF COMP VIS, P3328, DOI 10.1109/ICCV.2013.413
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y., 2013, P 21 ACM INT C MULT, P749
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   MAI L, 2013, PROC CVPR IEEE, P1131, DOI DOI 10.1109/CVPR.2013.150
   Marchesotti L, 2009, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2009.5459467
   Margolin R, 2013, PROC CVPR IEEE, P1139, DOI 10.1109/CVPR.2013.151
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Rutishauser U., 2004, PROCEEDINGS OF THE 2, P1137
   Shi KY, 2013, PROC CVPR IEEE, P2115, DOI 10.1109/CVPR.2013.275
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Whitehill J., 2009, Advances in Neural Information Processing Systems, P2035
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2017, IEEE T IMAGE PROCESS, V26, P1746, DOI 10.1109/TIP.2017.2658957
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 42
TC 28
Z9 30
U1 3
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1101
EP 1112
DI 10.1109/TMM.2017.2763780
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400007
DA 2024-07-18
ER

PT J
AU Khan, I
AF Khan, Imran
TI Robust Sparse and Dense Nonrigid Structure From Motion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sparse and dense NRSfM; density-based spatial clustering; supervised
   Gauss-Newton
ID SHAPE; FACTORIZATION
AB Modeling deformable three-dimensional (3-D) shape from a video sequence is a fundamental task in computer vision. Nonrigid structure from motion (NRSfM) refers to the problem of recovering the 3-D shape and pose of an object, deforming over time from a monocular video sequence. Presently, dense NRSfM is a research problem of great interest in academia and the industry due to the large demand for 3-D data in various contexts. We provide a robust system for the sparse and dense NRSfM. The strength of our approach comes with the ability to deal with the trajectories corrupted with outliers that serve as an input to NRSfM. To tackle this problem, the input trajectories are processed with a density-based spatial clustering approach that is combined with a RANSAC technique for the outlier's detection. Processing the trajectories with this process enhances the trajectories by removing the unwanted outliers. Also, extending the work from sparse to dense NRSfM substantially increases the difficulty of the optimization problem. Thus, the proposed system also provides asymptotic improvements to the current optimization approaches by providing an efficient and a novel supervised Gauss Newton method. Extensive experiments have demonstrated that the proposed method outperforms most of the existing NRSfM methods. The results show that the proposed method reconstructs largely deforming objects accurately and efficiently.
C1 [Khan, Imran] Ctr Adv Studies Engn, Dept Comp Engn, Islamabad 44000, Pakistan.
RP Khan, I (corresponding author), Ctr Adv Studies Engn, Dept Comp Engn, Islamabad 44000, Pakistan.
EM imi_case@yahoo.com
CR Abou-Moustafa KT, 2010, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2010.5539925
   Akhter I., 2008, ADV NEURAL INFORM PR, P41
   Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201
   [Anonymous], 1952, J RES NBS
   Barnett V., 1994, Outliers in statistical data, V3
   Barth AdamT., 2008, BodyNets'08: Proceedings of the ICST 3rd international conference on Body area networks, P1
   Bjorck A., 1996, NUMERICAL METHODS LE, P66
   Brand M, 2005, PROC CVPR IEEE, P122
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   Buchanan AM, 2005, PROC CVPR IEEE, P316
   Chang MMY, 2005, IEEE T MULTIMEDIA, V7, P253, DOI 10.1109/TMM.2005.843344
   Condat L, 2014, EUR SIGNAL PR CONF, P1806
   Dai Y., 2012, P IEEE C COMP VIS PA, P101
   Del Bue A., 2006, PROC IEEE C COMPUTER, V1, P1191
   Ester M., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P226
   Fayad J, 2010, LECT NOTES COMPUT SC, V6314, P297, DOI 10.1007/978-3-642-15561-1_22
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7
   Giannarou S, 2013, IEEE T PATTERN ANAL, V35, P130, DOI 10.1109/TPAMI.2012.81
   Gotardo P. F. U., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3065, DOI 10.1109/CVPR.2011.5995560
   Hast A, 2013, J WSCG, V21
   Kang ZZ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0117341
   Khan I, 2014, IEEE T MULTIMEDIA, V16, P1350, DOI 10.1109/TMM.2014.2308415
   Latorre F., 2008, IEEE INT S PARALLEL, P1, DOI [10.1109/CVPR.2008.4587523, DOI 10.1109/CVPR.2008.4587523]
   Lee M, 2013, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR.2013.169
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Paladini Marco, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2898, DOI 10.1109/CVPRW.2009.5206602
   Paladini M, 2010, LECT NOTES COMPUT SC, V6312, P15, DOI 10.1007/978-3-642-15552-9_2
   Pizarro D, 2012, INT J COMPUT VISION, V97, P54, DOI 10.1007/s11263-011-0452-0
   Ricco S, 2012, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2012.6247877
   Russell C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3009, DOI 10.1109/CVPR.2011.5995383
   Russell C, 2014, LECT NOTES COMPUT SC, V8695, P583, DOI 10.1007/978-3-319-10584-0_38
   Russell C, 2012, SECOND JOINT 3DIM/3DPVT CONFERENCE: 3D IMAGING, MODELING, PROCESSING, VISUALIZATION & TRANSMISSION (3DIMPVT 2012), P509, DOI 10.1109/3DIMPVT.2012.70
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Torr PHS, 2000, COMPUT VIS IMAGE UND, V78, P138, DOI 10.1006/cviu.1999.0832
   Torresani L, 2004, ADV NEUR IN, V16, P1555
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Vicente S, 2012, LECT NOTES COMPUT SC, V7574, P426, DOI 10.1007/978-3-642-33712-3_31
   Xiao J, 2004, LECT NOTES COMPUT SC, V2034, P573
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zhou HY, 2012, IEEE T MULTIMEDIA, V14, P168, DOI 10.1109/TMM.2011.2170406
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P171, DOI 10.1109/TMM.2014.2384396
NR 45
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 841
EP 850
DI 10.1109/TMM.2017.2758740
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000006
DA 2024-07-18
ER

PT J
AU Takahashi, N
   Gygli, M
   Van Gool, L
AF Takahashi, Naoya
   Gygli, Michael
   Van Gool, Luc
TI AENet: Learning Deep Audio Features for Video Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; audio feature; large audio event dataset;
   large input field; highlight detection
ID RECOGNITION
AB We propose a new deep network for audio event recognition, called AENet. In contrast to speech, sounds coming from audio events may be produced by a wide variety of sources. Furthermore, distinguishing them often requires analyzing an extended time period due to the lack of clear subword units that are present in speech. In order to incorporate this long-time frequency structure of audio events, we introduce a convolutional neural network (CNN) operating on a large temporal input. In contrast to previous works, this allows us to train an audio event detection system end to end. The combination of our network architecture and a novel data augmentation outperforms previous methods for audio event detection by 16%. Furthermore, we perform transfer learning and show that our model learned generic audio features, similar to the way CNNs learn generic features on vision tasks. In video analysis, combining visual features and traditional audio features, such as mel frequency cepstral coefficients, typically only leads to marginal improvements. Instead, combining visual features with our AENet features, which can be computed efficiently on a GPU, leads to significant performance improvements on action recognition and video highlight detection. In video highlight detection, our audio features improve the performance by more than 8% over visual features alone.
C1 [Takahashi, Naoya] ETH, CH-8092 Zurich, Switzerland.
   [Takahashi, Naoya] Sony Corp, Syst R&D Grp, Tokyo 1418610, Japan.
   [Gygli, Michael; Van Gool, Luc] ETH, Comp Vis Lab, CH-8092 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Sony
   Corporation; Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Takahashi, N (corresponding author), ETH, CH-8092 Zurich, Switzerland.; Takahashi, N (corresponding author), Sony Corp, Syst R&D Grp, Tokyo 1418610, Japan.
EM Naoya.Takahashi@sony.com; gygli@vision.ee.ethz.ch;
   vangool@vision.ee.ethz.ch
RI , Naoya/AAC-9434-2019
OI , Naoya/0000-0001-8553-4797
CR Abdel-Hamid O, 2013, INTERSPEECH, P3365
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   [Anonymous], 2016, ARXIV160407160
   [Anonymous], SIGNAL PROCESS MAG
   [Anonymous], INT CONF ACOUST SPEE
   [Anonymous], 2005, Proc._Neural_Information_Processing_System
   Ashraf K, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P611, DOI 10.1145/2671188.2749396
   Beltrán J, 2015, PATTERN RECOGN LETT, V68, P153, DOI 10.1016/j.patrec.2015.08.027
   Choi W, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2002
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Cimpoi M., 2015, P COMP VIS PATT REC
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J., 2013, Decaf: A deep convolutional activation feature for generic visual recognition
   Donahue J, 2014, PR MACH LEARN RES, V32
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Espi M, 2015, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-015-0069-2
   Feichtenhofer C., 2016, ADV NEURAL INFORM PR, P3468
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fisher W.M., 1986, P DARPAR SPEECH REC, P93
   Font F., 2013, P 2013 ACM MULTIMEDI, P411, DOI 10.1145/2502081.2502245
   Gencoglu O, 2014, EUR SIGNAL PR CONF, P506
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gygli M., 2016, P COMP VIS PATT REC
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Heckerman D., 1989, P 5 ANN C UNCERTAINT, P163
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hinton G. E., 2012, 12070580 ARXIV
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Huang Z, 2013, INTERSPEECH, P2281
   Jaitly N., 2013, ICML, V28
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee K, 2010, IEEE T AUDIO SPEECH, V18, P1406, DOI 10.1109/TASL.2009.2034776
   Liang JW, 2015, INT CONF ACOUST SPEE, P2279, DOI 10.1109/ICASSP.2015.7178377
   Lim H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3325
   Lu XG, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1176
   Metze F, 2014, IEEE INT CON MULTI
   Mostefa D, 2007, LANG RESOUR EVAL, V41, P389, DOI 10.1007/s10579-007-9054-4
   Nakamura S., 2000, INTRO NITRIDES SEMIC, P2
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   National Institute of Standards and Technology Gaithersburg MD USA, 2011, 2011 TRECVID MULT EV
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Over P., TRECVID 2013 INTRO G
   Pancoast Stephanie, 2012, P INTERSPEECH
   Phan H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3441
   Razavian A. S., 2014, Workshop on IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2014), P806, DOI [10.1109/cvprw.2014.131, DOI 10.1109/CVPRW.2014.131]
   Sercu T, 2016, INT CONF ACOUST SPEE, P4955, DOI 10.1109/ICASSP.2016.7472620
   Shiwen Deng, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P8232, DOI 10.1109/ICASSP.2014.6855206
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro K., 2012, P COMP VIS PATT REC
   Sun M., 2014, P EUR C COMPUT VIS
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang Y, 2016, INT CONF ACOUST SPEE, P2742, DOI 10.1109/ICASSP.2016.7472176
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Wu QX, 2013, IEEE T SYST MAN CY-S, V43, P875, DOI 10.1109/TSMCA.2012.2226575
   Xu M, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1352012.1352015
   Yipei Wang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1360, DOI 10.1109/ICASSP.2014.6853819
   Zhang T, 2001, IEEE T SPEECH AUDI P, V9, P441, DOI 10.1109/89.917689
   Zhuang XD, 2010, PATTERN RECOGN LETT, V31, P1543, DOI 10.1016/j.patrec.2010.02.005
NR 65
TC 102
Z9 108
U1 0
U2 42
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 513
EP 524
DI 10.1109/TMM.2017.2751969
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hadizadeh, H
   Bajic, IV
AF Hadizadeh, Hadi
   Bajic, Ivan V.
TI Full-Reference Objective Quality Assessment of Tone-Mapped Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image quality assessment (IQA); image naturalness; natural scene
   statistics; structural fidelity; tone mapping
ID STATISTICS; SALIENCY; NORMALIZATION
AB In this paper, we present a novel method for full-reference image quality assessment (IQA) of tone-mapped images displayed on standard low dynamic range (LDR) displays. Due to the dynamic range compression caused by the tone-mapping process, a mixture of several artifacts and distortions may be produced in the tone-mapped images. This makes the quality assessment of the tone-mapped images very challenging. Due to the diversity of such artifacts and distortions, we propose a "bag of features" (BOF) approach to tackle this problem. Specifically, in the proposed method, a number of different perceptually relevant quality-related features are first extracted from a given tone-mapped image and its reference HDR image. These features are designed such that they capture different aspects and attributes of the tone-mapped image such as its structural fidelity, naturalness, and overall brightness. A support vector regressor is then trained based on the extracted features, and it is used for measuring the visual quality of a tone-mapped image. Our experimental results indicate that the proposed method achieves high accuracy as compared to several existing methods.
C1 [Hadizadeh, Hadi] Quchan Univ Adv Technol, Quchan 9477167335, Iran.
   [Bajic, Ivan V.] Simon Fraser Univ, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hadizadeh, H (corresponding author), Quchan Univ Adv Technol, Quchan 9477167335, Iran.
EM h.hadizadeh@qiet.ac.ir; ibajic@ensc.sfu.ca
RI ; Bajic, Ivan/I-1241-2013
OI Hadizadeh, Hadi/0000-0003-2018-0523; Bajic, Ivan/0000-0003-3154-5743
CR [Anonymous], 2016, 2016 8 INT C QUAL MU
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   [Anonymous], IEEE T NEUR IN PRESS
   [Anonymous], 2004, METHOD SPECIFYING AC
   [Anonymous], 2004, UCID UNCOMPRESSED CO
   [Anonymous], 2000, HDB PARAMETRIC NONPA
   [Anonymous], 2016, P IEEE INT C QUAL MU
   [Anonymous], 2016, THE LIBSVM PACKAGE
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   Carandini M, 1997, J NEUROSCI, V17, P8621
   Chen Y, 2016, IEEE T MULTIMEDIA, V18, P576, DOI 10.1109/TMM.2016.2525010
   DELONG ER, 1988, BIOMETRICS, V44, P837, DOI 10.2307/2531595
   Fairchild M.D., 2005, Color Appearance Models, V2nd
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Griffin LD, 2007, IEEE T PATTERN ANAL, V29, P1355, DOI 10.1109/TPAMI.2007.1066
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Hadizadeh H., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3177, DOI 10.1109/ICIP.2011.6116342
   Hadizadeh H, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2617743
   Hadizadeh H, 2016, PATTERN RECOGN LETT, V80, P144, DOI 10.1016/j.patrec.2016.06.010
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   Heidrich Wolfgang, ERIK REINHARD
   Hubel D. H., 1995, Eye, Brain, and Vision
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   KOENDERINK JJ, 1987, BIOL CYBERN, V55, P367, DOI 10.1007/BF00318371
   Krasula L., 2014, P QUAL MULT EXP MAY, P1
   Kundu D, 2016, IEEE IMAGE PROC, P96, DOI 10.1109/ICIP.2016.7532326
   Li J., 2013, P SPIE IS T EL IM ST, P1
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Van LP, 2016, IEEE T MULTIMEDIA, V18, P364, DOI 10.1109/TMM.2015.2512231
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Mantiuk R., 2009, COMP GRAPH FOR P EUR, V28, P2
   Mantiuk RK, 2012, COMPUT GRAPH FORUM, V31, P2478, DOI 10.1111/j.1467-8659.2012.03188.x
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Montesinos P, 1998, INT C PATT RECOG, P838, DOI 10.1109/ICPR.1998.711280
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Papoulis A., 1989, PROBABILITY STAT
   Petit J., 2009, P ACM S APPL PERC GR, P134
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schlick Christophe., 1994, Quantization techniques for visualization of high dynamic range pictures, P7
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Shirley P., 2011, J GRAPH GPU GAME TOO, V15, P199
   Srivastava A, 2003, J MATH IMAGING VIS, V18, P17, DOI 10.1023/A:1021889010444
   van de Weijer J, 2006, IEEE T PATTERN ANAL, V28, P150, DOI 10.1109/TPAMI.2006.3
   Wainwright MJ, 2002, NEURAL INF PROCESS S, P203
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson A. B., 1993, DIGITAL IMAGES HUMAN
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 60
TC 34
Z9 36
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 392
EP 404
DI 10.1109/TMM.2017.2740023
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200011
DA 2024-07-18
ER

PT J
AU Cheng, F
   Tillo, T
   Xiao, JM
   Jeon, B
AF Cheng, Fei
   Tillo, Tammam
   Xiao, Jimin
   Jeon, Byeungwoo
TI Texture Plus Depth Video Coding Using Camera Global Motion Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional (3D) video coding; global motion; H.264; HEVC; HD;
   texture plus depth; temporal projection; virtual reference frame
ID MULTIVIEW-VIDEO; BIT ALLOCATION; PREDICTION; DESIGN
AB In video coding, traditional motion estimation methods work well for videos with camera translational motion, but their efficiency drops for other motions, such as rotational and dolly motions. In this paper, a motion-information-based three-dimensional (3D) video coding method is proposed for texture plus depth 3D video. The synchronized global motion information of the camera is obtained to assist the encoder improve its rate-distortion performance by projecting the temporal neighboring texture and depth frames into the position of the current frame, using the depth and camera motion information. Then, the projected frames are added into the reference buffer list as virtual reference frames. As these virtual reference frames could be more similar to the current to-be-encoded frame than the conventional reference frames, the required bits to represent the residual will be reduced. The experimental results demonstrate that the proposed scheme enhances the coding performance for all camera motion types and for various scene settings and resolutions using H.264 and HEVC standards, respectively. With the computer graphic sequences, for H.264, the average gain of texture and depth coding are up to 2 dB and 1 dB, respectively. For HEVC and HD resolution sequences, the gain of texture coding reaches 0.4 dB. For realistic sequences, up to 0.5 dB gain (H.264) is achieved for the texture video, while up to 0.7 dB gain is achieved for the depth sequences.
C1 [Cheng, Fei; Tillo, Tammam; Xiao, Jimin] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
   [Jeon, Byeungwoo] Sungkyunkwan Univ, Sch Elect & Elect Engn, Seoul 03063, South Korea.
C3 Xi'an Jiaotong-Liverpool University; Sungkyunkwan University (SKKU)
RP Cheng, F (corresponding author), Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
EM fei.cheng@xjtlu.edu.cn; tammam.tillo@xjtlu.edu.cn;
   jimin.xiao@xjtlu.edu.cn; bjeon@skku.edu
RI Jeon, Byeungwoo/AAS-1096-2021
OI Jeon, Byeungwoo/0000-0002-5650-2881
FU National Natural Science Foundation of China [61210006, 60972085,
   61501379]; Jiangsu Science and Technology Programme [BK20150375]; MSIP,
   Korea under the GITRC [IITP-2017-2015-0-00742]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61210006, Grant 60972085, and Grant
   61501379, in part by the Jiangsu Science and Technology Programme under
   BK20150375, and in part by the MSIP, Korea, under the GITRC support
   program (IITP-2017-2015-0-00742) supervised by the IITP. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.(Corresponding author:
   Fei Cheng.)
CR Abidi M.A., 1992, Data fusion in robotics and machine intelligence
   Abou-Elailah A, 2013, IEEE T CIRC SYST VID, V23, P158, DOI 10.1109/TCSVT.2012.2203211
   Advanced Video Coding for Generic Audiovisual Services, 2005, RECH264ISOIEC1449610
   Benzie P, 2007, IEEE T CIRC SYST VID, V17, P1647, DOI 10.1109/TCSVT.2007.905377
   BOOR CD, 1962, J MATH PHYS CAMB, V41, P212
   Chen F, 2015, 2015 IEEE 6TH INTERNATIONAL SYMPOSIUM ON MICROWAVE, ANTENNA, PROPAGATION, AND EMC TECHNOLOGIES (MAPE), P1, DOI 10.1109/MAPE.2015.7510253
   Chen XM, 2011, IEEE T CIRC SYST VID, V21, P335, DOI 10.1109/TCSVT.2011.2114210
   Chen Y, 2016, IEEE T MULTIMEDIA, V18, P576, DOI 10.1109/TMM.2016.2525010
   Chen Y, 2014, J VIS COMMUN IMAGE R, V25, P679, DOI 10.1016/j.jvcir.2013.03.013
   Cheng F, 2015, LECT NOTES COMPUT SC, V9314, P721, DOI 10.1007/978-3-319-24075-6_69
   Draelos M, 2015, IEEE IMAGE PROC, P2520, DOI 10.1109/ICIP.2015.7351256
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Hsu KY, 2011, IEEE WRK SIG PRO SYS, P90, DOI 10.1109/SiPS.2011.6088955
   Ma SW, 2014, IEEE T MULTIMEDIA, V16, P266, DOI 10.1109/TMM.2013.2284751
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Nearing J.C., 2003, Mathematical Tools for Physics
   Pei-Jun Lee, 2011, Proceedings of the 2011 International Conference on System Science and Engineering (ICSSE), P338, DOI 10.1109/ICSSE.2011.5961924
   Peris M, 2012, INT C PATT RECOG, P1038
   Seo J., 2010, 3DTV C TRUE VISION C, P1
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Yea S, 2009, SIGNAL PROCESS-IMAGE, V24, P89, DOI 10.1016/j.image.2008.10.007
   Yuan H, 2015, IEEE T MULTIMEDIA, V17, P2134, DOI 10.1109/TMM.2015.2477682
   Yun XP, 2006, IEEE T ROBOT, V22, P1216, DOI 10.1109/TRO.2006.886270
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zou F, 2014, IEEE T CIRC SYST VID, V24, P1696, DOI 10.1109/TCSVT.2014.2313891
NR 30
TC 2
Z9 2
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2361
EP 2374
DI 10.1109/TMM.2017.2700622
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200001
DA 2024-07-18
ER

PT J
AU Bagci, KT
   Sahin, KE
   Tekalp, AM
AF Bagci, Kadir Tolga
   Sahin, Kemal Emrecan
   Tekalp, A. Murat
TI Compete or Collaborate: Architectures for Collaborative DASH Video Over
   Future Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic adaptive streaming over HTTP (DASH); QoE fairness; QoE
   fluctuations; software-defined networks (SDN); transmission control
   protocol (TCP)
ID QUALITY; FAIRNESS
AB Dynamic adaptive streaming over HTTP (DASH) clients compete with each other over one or more bottleneck links in a network, which results in fluctuations in TCP throughput and QoE, QoE unfairness among clients, and underutilization of the network capacity. We propose centralized and distributed architectures for collaboration between network service provider (NSP), video service provider (VSP), and users (DASH clients) to provide NSP-managed or VSP-managed DASH services over software-defined networks (SDN) with quality-of-service (QoS) reserved network slices. We show that QoS reservation alone is not sufficient to overcome QoE fluctuations per client and unfairness between heterogeneous video clients, and clients also need to employ TCP receive-window adaptation knowing their fair-share bitrate. To this effect, we propose two collaborative streaming service models to inform clients about their fair-share bitrates. We first present an NSP-managed service model with centralized collaboration between the NSP, VSP, and the users, where a traffic engineering manager at the NSP assigns a fair-share bitrate to each DASH client. We then present a VSP-managed service model with centralized or distributed collaboration architectures, where in the former the VSP determines the fair-share bitrate for each client over a reserved network slice and in the latter a group of DASH clients sharing a reserved network slice collaborate among themselves. In the novel distributed collaboration framework, collaboration groups are identified by the VSP, and clients within a group share critical parameters with each other so that each client can estimate its fair-share bitrate. Experimental results demonstrate that collaboration rather than competition between clients not only helps them achieve a smooth goodput near their fair-share bitrate, but also improves the total goodput over the reserved slice.
C1 [Bagci, Kadir Tolga; Sahin, Kemal Emrecan; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Sahin, KE (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM kbagci@ku.edu.tr; ksahin@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020
FU TUBITAK [115E299]
FX This work was supported by the TUBITAK Project 115E299. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Xiaoqing Zhu. (Corresponding author: Kemal Emrecan
   Sahin.)
CR [Anonymous], 2012, Introduction to Cisco IOS NetFlow-A Technical Overview
   [Anonymous], INF TECHN MPEG SYS 6
   [Anonymous], OF CONFIG 1 2
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], 2015, P IFIP IEEE NETW
   [Anonymous], 2013, 7047 IETF RFC
   [Anonymous], IEEE COMSOC MTC E LE
   [Anonymous], 2015, QUALITY MULTIMEDIA E
   [Anonymous], P 40 ALL C COMM CONT
   Bagci KT, 2016, IEEE IMAGE PROC, P1519, DOI 10.1109/ICIP.2016.7532612
   Barbera M, 2007, IEEE ICC, P6141, DOI 10.1109/ICC.2007.1017
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Chen YJ, 2015, IEEE COMMUN SURV TUT, V17, P1126, DOI 10.1109/COMST.2014.2363139
   Georgopoulos P., 2013, Proceedings of the 2013 ACM SIGCOMM Workshop on Future Human-centric Multimedia Networking, FhMN '13, (New York, NY, USA), P15, DOI DOI 10.1145/2491172.2491181
   Houdaille R., 2012, P 3 MULTIMEDIA SYSTE, P1
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Karakus M, 2017, J NETW COMPUT APPL, V80, P200, DOI 10.1016/j.jnca.2016.12.019
   Kleinrouweler JW, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P36, DOI 10.1145/2910017.2910599
   Kuschnig R., 2010, MMSYS, P157
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Mu M, 2016, IEEE J SEL AREA COMM, V34, P2168, DOI 10.1109/JSAC.2016.2577318
   Nam H, 2014, IEEE GLOB COMM CONF, P1317, DOI 10.1109/GLOCOM.2014.7036990
   Oyman O, 2012, IEEE COMMUN MAG, V50, P20, DOI 10.1109/MCOM.2012.6178830
   Palma David, 2014, 2014 Third European Workshop on Software Defined Networks (EWSDN), P125, DOI 10.1109/EWSDN.2014.34
   Petrangeli S, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818361
   Rainer B, 2017, IEEE T MULTIMEDIA, V19, P849, DOI 10.1109/TMM.2016.2629761
   Ramakrishnan S, 2015, IEEE INT SYM MULTIM, P120, DOI 10.1109/ISM.2015.53
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Sideris A, 2014, INT CONF TELECOMM, P29, DOI 10.1109/TEMU.2014.6917731
   Vassilaras S, 2017, IEEE COMMUN MAG, V55, P112, DOI 10.1109/MCOM.2017.1600939
   Wang Z, 2004, SIGNAL PROCESS-IMAGE, V19, P121, DOI 10.1016/S0923-5965(03)00076-6
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 32
TC 27
Z9 28
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2152
EP 2165
DI 10.1109/TMM.2017.2736638
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600003
DA 2024-07-18
ER

PT J
AU Huang, FL
   Li, XL
   Zhang, SC
   Zhang, JL
   Chen, JH
   Zhai, ZN
AF Huang, Faliang
   Li, Xuelong
   Zhang, Shichao
   Zhang, Jilian
   Chen, Jinhui
   Zhai, Zhinian
TI Overlapping Community Detection for Multimedia Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Ensemble learning; line graph; overlapping-communities detection;
   particle swarm optimization (PSO); social network
ID SWARM OPTIMIZATION ALGORITHM; COMPLEX NETWORKS
AB Finding overlapping communities from multimedia social networks is an interesting and important problem in data mining and recommender systems. However, extant overlapping community discovery with swarm intelligence often generates overlapping community structures with superfluous small communities. To deal with the problem, in this paper, an efficient algorithm (LEPSO) is proposed for overlapping communities discovery, which is based on line graph theory, ensemble learning, and particle swarm optimization (PSO). Specifically, a discrete PSO, consisting of an encoding scheme with ordered neighbors and a particle updating strategy with ensemble clustering, is devised for improving the optimization ability to search communities hidden in social networks. Then, a postprocessing strategy is presented for merging the finer-grained and suboptimal overlapping communities. Experiments on some real-world and synthetic datasets show that our approach is superior in terms of robustness, effectiveness, and automatically determination of the number of clusters, which can discover overlapping communities that have better quality than those computed by state-of-the-art algorithms for overlapping communities detection.
C1 [Huang, Faliang] Fujian Normal Univ, Fujian Engn Ctr Publ Serv Big Data Min & Applicat, Fac Software, Fuzhou 350007, Fujian, Peoples R China.
   [Li, Xuelong] Chinese Acad Sci, Xian Inst Opt & Precis Mech, Xian 710119, Shaanxi, Peoples R China.
   [Zhang, Shichao] Guangxi Normal Univ, Coll Comp Sci & IT, Guilin 541000, Peoples R China.
   [Zhang, Jilian] Jinan Univ, Dept Comp Sci, Guangzhou 510630, Shi, Peoples R China.
   [Chen, Jinhui] Kobe Univ, Grad Sch Syst Informat, Kobe, Hyogo 6570013, Japan.
   [Zhai, Zhinian] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 311122, Zhejiang, Peoples R China.
C3 Fujian Normal University; Chinese Academy of Sciences; Xi'an Institute
   of Optics & Precision Mechanics, CAS; Guangxi Normal University; Jinan
   University; Kobe University; Zhejiang University of Science & Technology
RP Huang, FL (corresponding author), Fujian Normal Univ, Fujian Engn Ctr Publ Serv Big Data Min & Applicat, Fac Software, Fuzhou 350007, Fujian, Peoples R China.; Zhang, JL (corresponding author), Jinan Univ, Dept Comp Sci, Guangzhou 510630, Shi, Peoples R China.
EM faliang.huang@gmail.com; xuelong_li@opt.ac.cn;
   zhangsc@mailbox.gxnu.edu.cn; zhangjilian@yeah.net;
   ianchen@me.cs.scitec.kobe-u.ac.jp; zhaizhinian@gmail.com
RI Li, Xuelong/ABF-3381-2020; Li, Xuelong/Z-3785-2019; Zhang,
   Shichao/JXW-9650-2024; Chen, Jinhui/AFN-2369-2022; Zhang,
   Shichao/AAA-7608-2020; li, xiang/GWM-6319-2022
OI Chen, Jinhui/0000-0002-3701-9026; Li, Xuelong/0000-0002-0019-4197
FU China 1000-Plan National Distinguished Professorship; China 973 Program
   [2013CB329404]; China Key Research Program [2016YFB1000905]; Natural
   Science Foundation of China [61672177, 61363009]; Natural Science
   Foundation of Fujian Province [2017J01497]; Guangxi Bagui Teams for
   Innovation and Research; Guangxi Collaborative Innovation Center of
   MultisSource Information Integration and Intelligent Processing
FX This work was supported in part by the China 1000-Plan National
   Distinguished Professorship, in part by the China 973 Program under
   Grant 2013CB329404, in part by the China Key Research Program under
   Grant 2016YFB1000905, in part by the Natural Science Foundation of China
   under Grant 61672177 and Grant 61363009, in part by the Natural Science
   Foundation of Fujian Province under Grant 2017J01497, in part by the
   Guangxi Bagui Teams for Innovation and Research, and in part by the
   Guangxi Collaborative Innovation Center of MultisSource Information
   Integration and Intelligent Processing. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Shu-Ching Chen. (Corresponding authors: Faliang
   Huang; Jilian Zhang.)
CR Afshinmanesh F, 2005, EUROCON 2005: THE INTERNATIONAL CONFERENCE ON COMPUTER AS A TOOL, VOL 1 AND 2 , PROCEEDINGS, P217
   Ahn YY, 2010, NATURE, V466, P761, DOI 10.1038/nature09182
   Amelio A, 2014, LECT NOTES SOC NETW, P105, DOI 10.1007/978-3-7091-1797-2__6
   [Anonymous], 2013, SOCIAL MED RETRIEVAL
   [Anonymous], P 27 ANN ACM S APPL, DOI DOI 10.1145/2245276.2245321
   Ayad HG, 2010, PATTERN RECOGN, V43, P1943, DOI 10.1016/j.patcog.2009.11.012
   Bello G, 2011, LECT NOTES COMPUT SC, V6936, P160, DOI 10.1007/978-3-642-23878-9_20
   Bingol H., 2007, ARXIV PREPRINT ARXIV
   Broilo M, 2010, IEEE T MULTIMEDIA, V12, P267, DOI 10.1109/TMM.2010.2046269
   Cai Q, 2016, INT J BIO-INSPIR COM, V8, P84, DOI 10.1504/IJBIC.2016.076329
   Chen WN, 2010, IEEE T EVOLUT COMPUT, V14, P278, DOI 10.1109/TEVC.2009.2030331
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Chou HH, 2013, IEEE T CYBERNETICS, V43, P296, DOI 10.1109/TSMCB.2012.2205678
   Chuang LY, 2011, EXPERT SYST APPL, V38, P12699, DOI 10.1016/j.eswa.2011.04.057
   Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111
   Duan XD, 2008, IEEE C EVOL COMPUTAT, P1074, DOI 10.1109/CEC.2008.4630930
   Esmin AAA, 2015, ARTIF INTELL REV, V44, P23, DOI 10.1007/s10462-013-9400-4
   Evans TS, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016105
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Gargi U., 2011, PROC 5 INT C WEBLOGS, P486
   Gong MG, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1627
   Hruschka ER, 2009, IEEE T SYST MAN CY C, V39, P133, DOI 10.1109/TSMCC.2008.2007252
   Huang Fa-Liang, 2013, Journal of Software, V24, P2062, DOI 10.3724/SP.J.1001.2013.04400
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Kennedy J, 1997, IEEE SYS MAN CYBERN, P4104, DOI 10.1109/ICSMC.1997.637339
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   Lee S, 2008, PROG NAT SCI-MATER, V18, P1161, DOI 10.1016/j.pnsc.2008.03.018
   Li JW, 2013, SOFT COMPUT, V17, P925, DOI 10.1007/s00500-012-0942-1
   Lipczak Marek., 2009, P 11 ANN C GENETIC E, P1243
   Mei T, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071402
   Mirjalili S, 2013, SWARM EVOL COMPUT, V9, P1, DOI 10.1016/j.swevo.2012.09.002
   Nicosia V, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/03/P03024
   Palla G, 2005, NATURE, V435, P814, DOI 10.1038/nature03607
   Pereira-Leal JB, 2004, PROTEINS, V54, P49, DOI 10.1002/prot.10505
   Pizzuti C, 2008, LECT NOTES COMPUT SC, V5199, P1081, DOI 10.1007/978-3-540-87700-4_107
   Pizzuti Clara., 2009, GECCO 09, P859, DOI DOI 10.1145/1569901.1570019
   Santos R.L. T., 2007, CHARACTERIZING YOUTU
   Shahriari M, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16 COMPANION), P855, DOI 10.1145/2872518.2889292
   Shang RH, 2013, PHYSICA A, V392, P1215, DOI 10.1016/j.physa.2012.11.003
   Shi C, 2013, DATA KNOWL ENG, V87, P394, DOI 10.1016/j.datak.2013.05.004
   Sureka A, 2010, LECT NOTES COMPUT SC, V6458, P13, DOI 10.1007/978-3-642-17187-1_2
   Trelea IC, 2003, INFORM PROCESS LETT, V85, P317, DOI 10.1016/S0020-0190(02)00447-7
   Tsatsou D., 2010, ONLINE MULTIMEDIA AD, P233
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Wang Z, 2014, IEEE T SYST MAN CY-S, V44, P499, DOI 10.1109/TSMC.2013.2256890
   Wen X., IEEE T EVOL IN PRESS
   Whang JJ, 2016, IEEE T KNOWL DATA EN, V28, P1272, DOI 10.1109/TKDE.2016.2518687
   Wu ZH, 2012, J COMPUT SCI TECH-CH, V27, P468, DOI 10.1007/s11390-012-1236-x
   Xie JR, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501657
   Yeung CMA, 2009, 20TH ACM CONFERENCE ON HYPERTEXT AND HYPERMEDIA (HYPERTEXT 2009), P251
   Zhang Y, 2012, P 18 ACM SIGKDD INT, P606, DOI DOI 10.1145/2339530.2339629
   Zhao YL, 2013, ACM T MULTIM COMPUT, V10, DOI 10.1145/2502415
   Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135
NR 53
TC 24
Z9 26
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1881
EP 1893
DI 10.1109/TMM.2017.2692650
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400016
DA 2024-07-18
ER

PT J
AU Liu, HW
   Liu, L
   Le, TD
   Lee, I
   Sun, SL
   Li, JY
AF Liu, Huawen
   Liu, Lin
   Le, Thuc Duy
   Lee, Ivan
   Sun, Shiliang
   Li, Jiuyong
TI Nonparametric Sparse Matrix Decomposition for Cross-View Dimensionality
   Reduction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-view data; dimension reduction; matrix decomposition; sparse
   learning; sparsity-inducing function
ID PARTIAL LEAST-SQUARES; CANONICAL CORRELATION-ANALYSIS; OBJECT TRACKING;
   SELECTION
AB Cross-view data are collected from two different views or sources about the same subjects. As the information from these views often consolidate and/or complement each other, cross-view data analysis can gain more insights for decision making. A main challenge of cross-view data analysis is how to effectively explore the inherently correlated and high-dimensional data. Dimension reduction offers an effective solution for this problem. However, how to choose right models and parameters involved for dimension reduction is still an open problem. In this paper, we propose an effective sparse learning algorithm for cross-view dimensionality reduction. A distinguished character of our model selection is that it is nonparametric and automatic. Specifically, we represent the correlation of cross-view data using a covariance matrix. Then, we decompose the matrix into a sequence of low-rank ones by solving an optimization problem in an alternating least squares manner. More importantly, a new and nonparametric sparsity-inducing function is developed to derive a parsimonious model. Extensive experiments are conducted on real-world data sets to evaluate the effectiveness of the proposed algorithm. The results show that our method is competitive with the state-of-the-art sparse learning algorithms.
C1 [Liu, Huawen] Zhejiang Normal Univ, Dept Comp Sci, Jinhua 321004, Peoples R China.
   [Liu, Lin; Le, Thuc Duy; Lee, Ivan; Li, Jiuyong] Univ South Australia, Sch Informat Technol & Math Sci, Adelaide, SA 5095, Australia.
   [Sun, Shiliang] East China Normal Univ, Dept Comp Sci & Technol, Shanghai 200241, Peoples R China.
C3 Zhejiang Normal University; University of South Australia; East China
   Normal University
RP Liu, HW (corresponding author), Zhejiang Normal Univ, Dept Comp Sci, Jinhua 321004, Peoples R China.
EM hwliu@zjnu.edu.cn; lin.liu@unisa.edu.au; thuc.le@unisa.edu.au;
   Ivan.lee@unisa.edu.au; slsun@cs.ecnu.edu.cn; jiuyong.li@unisa.edu.au
RI Li, Jiuyong/AAY-2706-2020; Liu, Lin/ABD-1224-2020; Le, Thuc
   Duy/G-8444-2019; Lee, Ivan/F-4131-2013
OI Li, Jiuyong/0000-0002-9023-1878; Liu, Lin/0000-0003-2843-5738; Le, Thuc
   Duy/0000-0002-9732-4313; Lee, Ivan/0000-0002-2826-6367
FU National Science Foundation of China [61572443, 61673179]; ARC Discovery
   Grant [DP130104090]; Shanghai Key Laboratory of Intelligent Information
   Processing Grant [IIPL-2016-001]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61572443 and Grant 61673179, in part by the ARC
   Discovery Grant DP130104090, and in part by the Shanghai Key Laboratory
   of Intelligent Information Processing Grant IIPL-2016-001. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Zhen Wen.
CR Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Bakry A, 2013, PROC CVPR IEEE, P684, DOI 10.1109/CVPR.2013.94
   Boulesteix AL, 2007, BRIEF BIOINFORM, V8, P32, DOI 10.1093/bib/bb1016
   Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231
   Croux C, 2013, TECHNOMETRICS, V55, P202, DOI 10.1080/00401706.2012.727746
   Eweiwi A, 2015, PATTERN RECOGN LETT, V51, P8, DOI 10.1016/j.patrec.2014.07.017
   Hardoon DR, 2011, MACH LEARN, V83, P331, DOI 10.1007/s10994-010-5222-7
   Hong ZP, 2013, J MULTIVARIATE ANAL, V117, P163, DOI 10.1016/j.jmva.2013.02.011
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Katsaggelos AK, 2015, P IEEE, V103, P1635, DOI 10.1109/JPROC.2015.2459017
   Katsurai M, 2014, IEEE T MULTIMEDIA, V16, P1059, DOI 10.1109/TMM.2014.2306655
   Lê Cao KA, 2008, STAT APPL GENET MOL, V7, DOI 10.2202/1544-6115.1390
   Lee M, 2010, BIOMETRICS, V66, P1087, DOI 10.1111/j.1541-0420.2010.01392.x
   Li QM, 2016, IEEE T INTELL TRANSP, V17, P1062, DOI 10.1109/TITS.2015.2495342
   Liu HF, 2012, IEEE T PATTERN ANAL, V34, P1299, DOI 10.1109/TPAMI.2011.217
   Liu HW, 2015, PATTERN RECOGN, V48, P1724, DOI 10.1016/j.patcog.2014.11.007
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Mazumder R, 2011, J AM STAT ASSOC, V106, P1125, DOI 10.1198/jasa.2011.tm09738
   Parkhomenko E, 2009, STAT APPL GENET MOL, V8, DOI 10.2202/1544-6115.1406
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Shi CJA, 2015, IEEE T MULTIMEDIA, V17, P16, DOI 10.1109/TMM.2014.2375792
   Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6
   Sun SL, 2010, NEUROCOMPUTING, V73, P2980, DOI 10.1016/j.neucom.2010.07.007
   Tan X, 2015, IEEE T MULTIMEDIA, V17, P660, DOI 10.1109/TMM.2015.2410135
   Tang S, 2012, IEEE T MULTIMEDIA, V14, P43, DOI 10.1109/TMM.2011.2168198
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang FS, 2013, NEURAL PROCESS LETT, V37, P135, DOI 10.1007/s11063-012-9238-9
   Wang Q, 2012, IEEE T IMAGE PROCESS, V21, P4454, DOI 10.1109/TIP.2012.2205700
   Wang YX, 2013, IEEE T KNOWL DATA EN, V25, P1336, DOI 10.1109/TKDE.2012.51
   Wang ZY, 2015, MULTIMED TOOLS APPL, V74, P10277, DOI 10.1007/s11042-014-2166-0
   Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu XD, 2014, IEEE T KNOWL DATA EN, V26, P97, DOI 10.1109/TKDE.2013.109
   Yang D, 2014, J COMPUT GRAPH STAT, V23, P923, DOI 10.1080/10618600.2013.858632
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yuan YH, 2017, MULTIMED TOOLS APPL, V76, P731, DOI 10.1007/s11042-015-3070-y
   Yuan YH, 2014, PATTERN RECOGN, V47, P1411, DOI 10.1016/j.patcog.2013.09.009
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhong BN, 2014, NEUROCOMPUTING, V133, P317, DOI 10.1016/j.neucom.2013.11.004
   Zou H., 2004, Journal of Computational and Graphical Statistics, V15, p262 o
NR 41
TC 16
Z9 16
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1848
EP 1859
DI 10.1109/TMM.2017.2683258
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400013
DA 2024-07-18
ER

PT J
AU Song, JR
   Yang, FZ
   Zhou, YC
   Gao, S
AF Song, Jiarun
   Yang, Fuzheng
   Zhou, Yicong
   Gao, Shan
TI Parametric Planning Model for Video Quality Evaluation of IPTV Services
   Combining Channel and Video Characteristics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network planning; planning model; quality of experience (QoE) planning;
   video quality assessment; video streaming applications
ID NETWORKED VIDEO; ITU
AB Parametric planning models are designed for estimating the video quality, which can be applied to effective planning, implementation, and management of network video applications and communication networks. However, different from the bitstream-based evaluation models, the planning models are not allowed to exploit the video streams, with only limited information available for use, i.e., a few general parameters predetermined by the service providers and network operators. In this paper, a parametric planning model combining channel and video characteristics is proposed to estimate the video distortion caused by packet loss for Internet protocol television (IPTV) services. More specifically, the probability distribution of the channel states is determined by detailed analysis of the channel characteristics. Then, considering the influence of burst packet loss and the temporal dependence between frames, several sequence-level and frame-level parameters for video quality evaluation are derived from the perspective of the probability distribution of the channel states. Utilizing these parameters, the proposed model approximates the video quality considering the effects of direct packet loss and error propagation. Experimental results show that the proposed model has a superior performance for video quality estimation than the three commonly used parametric planning models.
C1 [Song, Jiarun] Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Peoples R China.
   [Yang, Fuzheng] Xidian Univ, State Key Lab ISN, Xian 710071, Peoples R China.
   [Yang, Fuzheng] RMIT Univ, Sch Elect & Comp Engn, Melbourne, Vic 3001, Australia.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau, Peoples R China.
   [Gao, Shan] Huawei Technol Co Ltd, Media Technol Lab, Shenzhen 518129, Peoples R China.
C3 Xidian University; Xidian University; Royal Melbourne Institute of
   Technology (RMIT); University of Macau; Huawei Technologies
RP Song, JR (corresponding author), Xidian Univ, Collaborat Innovat Ctr Informat Sensing & Underst, Xian 710071, Peoples R China.
EM jrsong@xidian.edu.cn; fzhyang@mail.xidian.edu.cn; yicongzhou@umac.mo;
   simon.gaoshan@huawei.com
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384; Song, Jiarun/0000-0001-6718-4201
FU National Science Foundation of China [61601349, 61371089, 61571337,
   61601348]; 111 Project [B08038]
FX This work was supported by the National Science Foundation of China
   under Grant 61601349, Grant 61371089, Grant 61571337, and Grant
   61601348, and by the 111 Project B08038. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ivan Bajic.
CR [Anonymous], 2012, PARAMETRIC NONINTRUS
   [Anonymous], 2011, 472REV1 ITU TD
   [Anonymous], 2003, COM12D97E ITU
   [Anonymous], SPSS 17 0 STAT ANAL
   [Anonymous], 2006, METHODOLOGY SUBJECTI
   [Anonymous], 2008, QUALITY EXPERIENCE R
   [Anonymous], 2014, SUBJECTIVE VIDEO QUA
   [Anonymous], 2008, OBJECTIVE PERCEPTUAL
   [Anonymous], 2005, 3984 RFC IETF
   [Anonymous], 2007, OPINION MODEL VIDEO
   Bellard F., 2012, FFMPEG
   Coverdale P, 2011, IEEE SIGNAL PROC MAG, V28, P91, DOI 10.1109/MSP.2011.942467
   Deng R, 2015, IEEE T MULTIMEDIA, V17, P1495, DOI 10.1109/TMM.2015.2456506
   G. 1071 ITU-T RECOMMENDATION, 2015, OPINION MODEL NETWOR
   Garcia M. N., 2010, 2010 10th International Conference on Information Sciences, Signal Processing and their Applications (ISSPA 2010), P349, DOI 10.1109/ISSPA.2010.5605528
   Gnedenko B.V., 1962, THEORY PROBABILITY
   Goldsmith AJ, 1996, IEEE T INFORM THEORY, V42, P868, DOI 10.1109/18.490551
   Holden J., 2016, ELEVATE FAST FORWARD
   Li ZC, 2009, IEEE T CIRC SYST VID, V19, P917, DOI 10.1109/TCSVT.2009.2022806
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Raake A, 2011, IEEE SIGNAL PROC MAG, V28, P68, DOI 10.1109/MSP.2011.942472
   Study Group 12, 2009, 32COM12JKK ITUT TSB
   Takahashi A, 2008, IEEE COMMUN MAG, V46, P78, DOI 10.1109/MCOM.2008.4473087
   Taylor A. E., 1952, AM MATH MONTHLY, V59, P20, DOI DOI 10.1080/00029890.1952.11988058
   WANG HS, 1995, IEEE T VEH TECHNOL, V44, P163, DOI 10.1109/25.350282
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   Yamagishi K, 2008, IEEE ICC, P110, DOI 10.1109/ICC.2008.29
   Yang FZ, 2012, IEEE COMMUN MAG, V50, P203, DOI 10.1109/MCOM.2012.6353702
   Yang FZ, 2012, IEEE J-STSP, V6, P672, DOI 10.1109/JSTSP.2012.2207705
   Yang FZ, 2010, IEEE T CIRC SYST VID, V20, P1544, DOI 10.1109/TCSVT.2010.2087433
   Yu Y, 2007, IEEE WCNC, P2055
   Yuk SW, 2001, IEEE T MULTIMEDIA, V3, P366, DOI 10.1109/6046.944479
   Zhang Y, 2007, IEEE T MULTIMEDIA, V9, P445, DOI 10.1109/TMM.2006.887989
NR 33
TC 11
Z9 11
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1015
EP 1029
DI 10.1109/TMM.2016.2638621
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000010
OA hybrid
DA 2024-07-18
ER

PT J
AU Wang, L
   Zhao, X
   Si, YF
   Cao, LL
   Liu, YC
AF Wang, Lei
   Zhao, Xu
   Si, Yunfei
   Cao, Liangliang
   Liu, Yuncai
TI Context-Associative Hierarchical Memory Model for Human Activity
   Recognition and Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human activity; human memory; instance-based learning; one-shot
   learning; prediction; recognition
AB Human activity recognition is a challenging high-level vision task, for which multiple factors, such as subject, object, and their diverse interactions, have to be considered and modeled. Current learning-based methods are limited in the capability to integrate human-level concepts into an easily extensible computational framework. Inspired by the existing human memory model, we present a context-associative approach to recognize activity with human-object interaction. The proposed system can recognize incoming visual content based on the previous experienced activities. The high-level activity is parsed into consecutive subactivities, and we build a context cluster to model the temporal relations. The semantic attributes of the subactivity are organized by a concept hierarchy. Based on the hierarchy, a series of similarity functions are defined to turn the recognition computing into retrievals over the contextual memory, similar to the auto-associative characteristics of human memory. Partially matching in retrieval and stored memory make the activity prediction possible. The dynamical evolution of the brain memory is mimicked to allow decay and reinforcement of the input information, providing a natural way to maintain data and save computational time. We evaluate our approach on three data sets: CAD-120, MHOI, and OPPORTUNITY. The proposed method demonstrates promising results comparedwith other state of-the-art techniques.
C1 [Wang, Lei; Zhao, Xu; Si, Yunfei; Liu, Yuncai] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
   [Cao, Liangliang] CustomerserviceAI Com, Amherst, MA 01001 USA.
C3 Shanghai Jiao Tong University
RP Zhao, X (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
EM wltongxing@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; yunfei.si@duke.edu;
   liangliang.cao@gmail.com; whomliu@sjtu.edu.cn
OI Wang, Lei/0000-0002-0296-8993
FU National Natural Science Foundation of China [NSFC 61273285, 61375019,
   61673269]
FX This work was supported by the National Natural Science Foundation of
   China under Grant NSFC 61273285, Grant 61375019, and Grant 61673269.
CR AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
   [Anonymous], 1974, The psychology of learning and motivation, DOI DOI 10.1016/S0079-7421(08)60452-1
   [Anonymous], IEEE TCSVT
   [Anonymous], 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239179
   [Anonymous], 2012, PREDICTION CANDIDATE
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2005, MEMORY BASED LANGUAG
   Atkinson R. C, 1968, Human Memory Internet, V2, P89
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Chang XB, 2015, IEEE T IMAGE PROCESS, V24, P1905, DOI 10.1109/TIP.2015.2409564
   Chavarriaga R, 2013, PATTERN RECOGN LETT, V34, P2033, DOI 10.1016/j.patrec.2012.12.014
   Chen DM, 2015, IEEE T MULTIMEDIA, V17, P1019, DOI 10.1109/TMM.2015.2427744
   Cheng HT, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P355, DOI 10.1145/2493432.2493511
   Deng TJ, 2013, IEEE T KNOWL DATA EN, V25, P2119, DOI 10.1109/TKDE.2012.157
   Dong G., 2007, SEQUENCE DATA MINING, V33
   Gupta A., 2007, PROC IEEE C COMPUT V, P1
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Hawkins J., 2004, On intelligence
   Hu JF, 2016, IEEE T CIRC SYST VID, V26, P647, DOI 10.1109/TCSVT.2015.2397200
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang Y., 2012, Int. Conf. Mach. Learning (ICML), P1543
   Koppula Hema, 2013, P ICML, P792
   Koppula HS, 2013, INT J ROBOT RES, V32, P951, DOI 10.1177/0278364913478446
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li K, 2012, INT C PATT RECOG, P1779
   Li Kang, 2014, IEEE Trans Pattern Anal Mach Intell, V36, P1644, DOI 10.1109/TPAMI.2013.2297321
   Li L.J., 2007, PROC IEEE INT C COMP
   Lin TC, 2015, IEEE T IMAGE PROCESS, V24, P1330, DOI 10.1109/TIP.2015.2403236
   Liu T, 2016, NEUROSCIENCE, V313, P83, DOI 10.1016/j.neuroscience.2015.11.047
   MANDLER G, 1980, PSYCHOL REV, V87, P252, DOI 10.1037/0033-295X.87.3.252
   Mandler G, 2011, CURR DIR PSYCHOL SCI, V20, P232, DOI 10.1177/0963721411414656
   Mylonas P, 2009, IEEE T MULTIMEDIA, V11, P229, DOI 10.1109/TMM.2008.2009681
   Nguyen TV, 2015, IEEE T CIRC SYST VID, V25, P77, DOI 10.1109/TCSVT.2014.2333151
   Oh SM, 2011, PROC CVPR IEEE
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Petitjean F, 2011, PATTERN RECOGN, V44, P678, DOI 10.1016/j.patcog.2010.09.013
   Qi GJ, 2010, IEEE T MULTIMEDIA, V12, P278, DOI 10.1109/TMM.2010.2046270
   Rybok L, 2014, IEEE WINT CONF APPL, P646, DOI 10.1109/WACV.2014.6836041
   Ryoo MS, 2011, IEEE I CONF COMP VIS, P1036, DOI 10.1109/ICCV.2011.6126349
   SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5
   SPERLING G, 1963, Hum Factors, V5, P19
   Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721
   Sung JY, 2012, IEEE INT CONF ROBOT, P842, DOI 10.1109/ICRA.2012.6224591
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   TULVING E, 1973, PSYCHOL REV, V80, P352, DOI 10.1037/h0020071
   TULVING E, 1984, BEHAV BRAIN SCI, V7, P223, DOI 10.1017/S0140525X0004440X
   Tulving E., 1993, CURR DIR PSYCHOL SCI, V2, P67, DOI DOI 10.1111/1467-8721.EP10770899
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang J, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995493
   Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wang ZS, 2010, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2010.5540125
   Wilson DR, 2000, MACH LEARN, V38, P257, DOI 10.1023/A:1007626913721
   Wixted JT, 2007, PSYCHOL REV, V114, P152, DOI 10.1037/0033-295X.114.1.152
   Wu XX, 2013, IEEE T CIRC SYST VID, V23, P1422, DOI 10.1109/TCSVT.2013.2244794
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Zhang H, 2015, IEEE T HUM-MACH SYST, V45, P598, DOI 10.1109/THMS.2015.2443037
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhen XT, 2013, IEEE T CIRC SYST VID, V23, P1182, DOI 10.1109/TCSVT.2013.2240916
   Zhu YY, 2013, PROC CVPR IEEE, P2491, DOI 10.1109/CVPR.2013.322
   Zhu YK, 2014, IEEE T MULTIMEDIA, V16, P1585, DOI 10.1109/TMM.2014.2321534
NR 67
TC 23
Z9 26
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 646
EP 659
DI 10.1109/TMM.2016.2617079
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400018
DA 2024-07-18
ER

PT J
AU Zhou, Z
   Jiang, N
   Chen, K
   Zhang, JC
AF Zhou, Zhong
   Jiang, Na
   Chen, Ke
   Zhang, Jingchang
TI Automatic Mesh Animation Preview With User Voting-Based Refinement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic preview; mesh animation; motion saliency; user voting
ID CAMERA CONTROL; SEGMENTATION; SELECTION
AB With the rapid growth in the number and quality of mesh animations, browsing mesh animations wastes considerable bandwidth and requires tremendous rendering resources. Accordingly, the preview technique, which provides users a rapid understanding of a mesh animation before downloading, has received increasing attention. In this paper, we propose an automatic mesh animation preview method that incorporates the interframe motion saliency, intraframe surface saliency, user preference, and camera smoothness constraint to formulate the viewpoint selection as a minimization problem. Then, the minimization is solved by finding the shortest path, and the viewpoints for mesh animation preview are generated accordingly. A voting mechanism is introduced into this process to collect user feedbacks and periodically use user voting feedbacks to refine the preview camera path. The experiment results show that our mesh preview method helps users acquire a good understanding of animation contents. A user study demonstrates that our preview results are superior to those generated by typical preview methods in terms of the subjective visual quality.
C1 [Zhou, Zhong; Jiang, Na; Chen, Ke; Zhang, Jingchang] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Beihang University
RP Zhou, Z (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM zz@buaa.edu.cn; jiangna@buaa.edu.cn; chenke19850113@gmail.com;
   jczhang@buaa.edu.cn
FU National 863 Program of China [2015AA016403]; Natural Science Foundation
   of China [61572061, 61472020]
FX This work was supported by the National 863 Program of China under Grant
   2015AA016403, and by the Natural Science Foundation of China under Grant
   61572061 and Grant 61472020.
CR Ahn JK, 2013, IEEE T MULTIMEDIA, V15, P485, DOI 10.1109/TMM.2012.2235417
   Alregib G, 2005, IEEE T MULTIMEDIA, V7, P1149, DOI 10.1109/TMM.2005.858404
   Anna gele., 2012, Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, SCA'12, P53
   [Anonymous], IET
   [Anonymous], 1963, The method of paired comparisons
   [Anonymous], 1985, ACM SIGGRAPH COMPUTE
   Arcila R, 2013, GRAPH MODELS, V75, P10, DOI 10.1016/j.gmod.2012.10.004
   Arcila R, 2010, WSCG 2010: FULL PAPERS PROCEEDINGS, P33
   Assa J, 2010, COMPUT GRAPH FORUM, V29, P595, DOI 10.1111/j.1467-8659.2009.01629.x
   Assa J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409068
   Bernard J, 2013, IEEE T VIS COMPUT GR, V19, P2257, DOI 10.1109/TVCG.2013.178
   Cagniart C., 2010, P IEEE CVPR, P1
   Christie M, 2005, LECT NOTES COMPUT SC, V3638, P40
   Christie M, 2008, COMPUT GRAPH FORUM, V27, P2197, DOI 10.1111/j.1467-8659.2008.01181.x
   Corrigan T., 2004, THE FILM EXPERIENCE, P140
   Dong L, 2015, IEEE T MULTIMEDIA, V17, P2174, DOI 10.1109/TMM.2015.2484221
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Fleishman S, 2000, COMPUT GRAPH FORUM, V19, P101, DOI 10.1111/1467-8659.00447
   Fu JJ, 2013, IEEE T MULTIMEDIA, V15, P1340, DOI 10.1109/TMM.2013.2247584
   Halit C, 2011, COMPUT ANIMAT VIRT W, V22, P3, DOI 10.1002/cav.380
   Han SR, 2010, IEEE IMAGE PROC, P2945, DOI 10.1109/ICIP.2010.5652185
   Hisada M, 2002, COMPUT GRAPH FORUM, V21, P689, DOI 10.1111/1467-8659.00627
   Hoffman DD, 1997, COGNITION, V63, P29, DOI 10.1016/S0010-0277(96)00791-3
   HORN BKP, 1987, J OPT SOC AM A, V4, P629, DOI 10.1364/JOSAA.4.000629
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   KAMADA T, 1988, COMPUT VISION GRAPH, V41, P43, DOI 10.1016/0734-189X(88)90116-8
   Kendall MG, 1939, ANN MATH STAT, V10, P275, DOI 10.1214/aoms/1177732186
   Kwon JY, 2008, VISUAL COMPUT, V24, P475, DOI 10.1007/s00371-008-0228-x
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Meyer N, 2003, VISUALIZATION AND MATHEMATICS III, P35
   Park SB, 2006, IEEE T MULTIMEDIA, V8, P885, DOI 10.1109/TMM.2006.879914
   Rhodin H, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818082
   Saleem W., 2007, P 23 SPRING C COMP G, P115
   Secord A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2019627.2019628
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Shilane P, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P108
   Stoev SL, 2002, VIS 2002: IEEE VISUALIZATION 2002, PROCEEDINGS, P545, DOI 10.1109/VISUAL.2002.1183826
   Taniguchi Y., 1995, MULTIMEDIA 95 P 3 AC, P25
   Tao J, 2013, IEEE T VIS COMPUT GR, V19, P393, DOI 10.1109/TVCG.2012.143
   Vázquez PP, 2003, COMPUT GRAPH FORUM, V22, P689, DOI 10.1111/j.1467-8659.2003.00717.x
   Vieira T, 2009, COMPUT GRAPH FORUM, V28, P717, DOI 10.1111/j.1467-8659.2009.01412.x
   Yeh IC, 2012, IEEE T VIS COMPUT GR, V18, P1496, DOI 10.1109/TVCG.2011.273
   Zhao S., 2013, P 4 ACM MULT SYST C, P178
NR 43
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 327
EP 339
DI 10.1109/TMM.2016.2612124
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800009
DA 2024-07-18
ER

PT J
AU Wang, T
   Ji, ZX
   Sun, QS
   Chen, Q
   Jing, XY
AF Wang, Tao
   Ji, Zexuan
   Sun, Quansen
   Chen, Qiang
   Jing, Xiao-Yuan
TI Interactive Multilabel Image Segmentation via Robust Multilayer Graph
   Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive image segmentation; multilabel Markov random field (MRF);
   multilayer graph; graph cuts; parallel partial optimality
ID ENERGY MINIMIZATION; ALGORITHMS
AB The combination of pixel and superpixel has been widely utilized in the interactive segmentation methods to overcome the sensitivity to the seeds' quantity and quality. However, because of the introduction of more variables and variables' interactions, the pixel-superpixel combination methods are still limited to the segmentation accuracy and computational complexity. To solve these problems, in this paper, we propose an interactive multilabel image segmentation method. In the proposed segmentation model, the multilayer relationships among the pixel layer, superpixel layer, and label layer are fused by the Markov random field framework to further improve the segmentation accuracy. During the optimization stage, the parallel partial optimality strategy is utilized to effectively solve the multilabel submodular energy function. Experimental results on challenging data sets demonstrate the competitiveness of the proposed method comparing with several state-of-the-art interactive algorithms.
C1 [Wang, Tao; Ji, Zexuan; Sun, Quansen; Chen, Qiang] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
   [Jing, Xiao-Yuan] Nanjing Univ Posts & Telecommun, Sch Automat, Nanjing 210094, Jiangsu, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of Posts
   & Telecommunications
RP Ji, ZX (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
EM wangtaoatnjust@163.com; jizexuan@njust.edu.cn; sunquansen@njust.edu.cn;
   chen2qiang@163.com; jingxy_2000@126.com
RI He, Chen/JLM-5059-2023; chen, qiang/HGU-5418-2022; chen,
   qiang/GWZ-7308-2022
FU National Science Foundation of China [61401209, 61273251]; Natural
   Science Foundation of Jiangsu Province, China [BK20140790]; Fundamental
   Research Funds for the Central Universities [30916011324,
   30920140111004]; China Postdoctoral Science Foundation [2014T70525,
   2013M531364]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61401209 and Grant 61273251, in part by the Natural
   Science Foundation of Jiangsu Province, China, under Grant BK20140790,
   in part by the Fundamental Research Funds for the Central Universities
   under Grant 30916011324 and Grant 30920140111004, and in part by China
   Postdoctoral Science Foundation under Grant 2014T70525 and Grant
   2013M531364. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Enrico Magli.
   (Corresponding author: Zexuan Ji.)
CR Alahari K, 2010, IEEE T PATTERN ANAL, V32, P1846, DOI 10.1109/TPAMI.2009.194
   [Anonymous], 2004, MSRC GROUND TRUE DAT
   [Anonymous], 1994, MARKOV RANDOM FIELD
   [Anonymous], 2007, CVPR, DOI DOI 10.1109/CVPR.2007.383203
   Arbelaez P., 2008, Proc. IEEE Conf. Computer Vision and Pattern Recognition CVPR 2008, P1
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Blake A, 2004, LECT NOTES COMPUT SC, V3021, P428
   Bonato T, 2011, CONTRACTION BASED SE
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Dong XP, 2015, IEEE T IMAGE PROCESS, V24, P3966, DOI 10.1109/TIP.2015.2456636
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Han SD, 2009, IEEE T IMAGE PROCESS, V18, P2289, DOI 10.1109/TIP.2009.2025560
   He XM, 2006, LECT NOTES COMPUT SC, V3951, P338
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Kappes JH, 2013, PROC CVPR IEEE, P1752, DOI 10.1109/CVPR.2013.229
   Kappes JH, 2012, PROC CVPR IEEE, P1688, DOI 10.1109/CVPR.2012.6247863
   Kim TH, 2008, LECT NOTES COMPUT SC, V5304, P264
   Kim TH, 2010, PROC CVPR IEEE, P3201, DOI 10.1109/CVPR.2010.5540078
   Kohli P, 2005, IEEE I CONF COMP VIS, P922
   Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0
   Kohli Pushmeet., 2007, IEEE C COMPUTER VISI, P1
   Kohli Pushmeet., 2008, P INT C MACHINE LEAR, P480
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031
   Komodakis N, 2007, IEEE I CONF COMP VIS, P488
   Kovtun I, 2003, LECT NOTES COMPUT SC, V2781, P402
   Kovtun Ivan., 2011, Control Systems and Computers, V3, P35
   Lempitsky V, 2010, IEEE T PATTERN ANAL, V32, P1392, DOI 10.1109/TPAMI.2009.143
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Meila M., 2005, P 22 INT C MACHINE L, P577
   Mille J, 2015, INT J COMPUT VISION, V112, P1, DOI 10.1007/s11263-014-0751-3
   Rother C., 2004, ACM Transactions on Graphics (SIGGRAPH), V23, P309
   Schraudolph N. N., 2009, Advances in Neural Information Processing Systems, P1417
   Sener O, 2014, IEEE T MULTIMEDIA, V16, P1292, DOI 10.1109/TMM.2014.2314069
   Shekhovtsov A, 2013, INT J COMPUT VISION, V104, P315, DOI 10.1007/s11263-012-0571-2
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Swoboda P, 2014, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2014.153
   The Berkeley Segmentation Database, 2010, BERKELEY SEGMENTATIO
   Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046
   Wang T, 2016, INFORM SCIENCES, V358, P92, DOI 10.1016/j.ins.2016.04.017
   Wang T, 2016, PATTERN RECOGN, V55, P28, DOI 10.1016/j.patcog.2016.01.018
   Wang T, 2015, J VIS COMMUN IMAGE R, V33, P10, DOI 10.1016/j.jvcir.2015.08.013
   Wang T, 2015, NEUROCOMPUTING, V158, P13, DOI 10.1016/j.neucom.2015.02.010
   Wang TH, 2012, IEEE T MULTIMEDIA, V14, P389, DOI 10.1109/TMM.2011.2177078
   Wang WG, 2016, IEEE T MULTIMEDIA, V18, P1011, DOI 10.1109/TMM.2016.2545409
   Xiang SM, 2011, IEEE T MULTIMEDIA, V13, P342, DOI 10.1109/TMM.2010.2103930
   Yang WX, 2010, IEEE T IMAGE PROCESS, V19, P2470, DOI 10.1109/TIP.2010.2048611
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
NR 54
TC 23
Z9 23
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2358
EP 2371
DI 10.1109/TMM.2016.2600441
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200004
OA Bronze
DA 2024-07-18
ER

PT J
AU Ye, M
   Liang, C
   Yu, Y
   Wang, Z
   Leng, QM
   Xiao, CX
   Chen, J
   Hu, RM
AF Ye, Mang
   Liang, Chao
   Yu, Yi
   Wang, Zheng
   Leng, Qingming
   Xiao, Chunxia
   Chen, Jun
   Hu, Ruimin
TI Person Reidentification via Ranking Aggregation of Similarity Pulling
   and Dissimilarity Pushing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person reidentification; ranking aggregation; similarity and
   dissimilarity
ID IDENTIFICATION; TRACKING; CAMERA; FUSION
AB Person reidentification is a key technique to match different persons observed in nonoverlapping camera views. Many researchers treat it as a special object-retrieval problem, where ranking optimization plays an important role. Existing ranking optimization methods mainly utilize the similarity relationship between the probe and gallery images to optimize the original ranking list, but seldom consider the important dissimilarity relationship. In this paper, we propose to use both similarity and dissimilarity cues in a ranking optimization framework for person reidentification. Its core idea is that the true match should not only be similar to those strongly similar galleries of the probe, but also be dissimilar to those strongly dissimilar galleries of the probe. Furthermore, motivated by the philosophy of multiview verification, a ranking aggregation algorithm is proposed to enhance the detection of similarity and dissimilarity based on the following assumption: the true match should be similar to the probe in different baseline methods. In other words, if a gallery blue image is strongly similar to the probe in one method, while simultaneously strongly dissimilar to the probe in another method, it will probably be a wrong match of the probe. Extensive experiments conducted on public benchmark datasets and comparisons with different baseline methods have shown the great superiority of the proposed ranking optimization method.
C1 [Ye, Mang; Liang, Chao; Wang, Zheng; Chen, Jun; Hu, Ruimin] Wuhan Univ, State Key Lab Software Engn, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430072, Peoples R China.
   [Ye, Mang; Liang, Chao; Wang, Zheng; Chen, Jun; Hu, Ruimin] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
   [Ye, Mang] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
   [Leng, Qingming] Jiujiang Univ, Sch Informat Sci & Technol, Jiujiang 332005, Peoples R China.
   [Xiao, Chunxia] Wuhan Univ, State Key Lab Software Engn, Wuhan 430072, Peoples R China.
   [Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
C3 Wuhan University; Wuhan University; Hong Kong Baptist University;
   Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Jiujiang University; Wuhan
   University; Wuhan University
RP Liang, C (corresponding author), Wuhan Univ, State Key Lab Software Engn, Collaborat Innovat Ctr Geospatial Technol, Wuhan 430072, Peoples R China.; Liang, C (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Peoples R China.
EM yemang@whu.edu.cn; cliang@whu.edu.cn; yiyu@nii.ac.jp;
   wangzwhu@whu.edu.cn; lengqingming@126.com; cxxiao@whu.edu.cn;
   chenj@whu.edu.cn; hurm1964@gmail.com
RI Wang, Zheng/ABC-6029-2020; Chen, Jun/AAD-8167-2022; Ye,
   Mang/AAT-6142-2020; Wang, Zheng/AAQ-8628-2020; Ye, Mang/HCJ-0591-2022
OI Wang, Zheng/0000-0003-3846-9157; Ye, Mang/0000-0003-3989-7655; Wang,
   Zheng/0000-0003-3846-9157; Ye, Mang/0000-0003-3989-7655
FU National Nature Science Foundation of China [61303114, 61501413,
   61562048, 61472288]; National High Technology Research and Development
   Program of China [2015AA016306]; Internet of Things Development Funding
   Project of Ministry of Industr; Technology Research Project of Ministry
   of Public Security [2014JSYJA016]; Nature Science Foundation of Jiangsu
   Province [BK20160386]; Nature Science Foundation of Hubei Province
   [2014CFB712]; Nature Science Foundation of Jiangxi Province
   [20151BAB217013]; Fundamental Research Funds for the Central
   Universities [2042014kf0250]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61303114, Grant 61501413, Grant
   61562048, and Grant 61472288, in part by the National High Technology
   Research and Development Program of China under Grant 2015AA016306, in
   part by the Internet of Things Development Funding Project of Ministry
   of Industry in 2013 (25), in part by the Technology Research Project of
   Ministry of Public Security under Grant 2014JSYJA016, in part by the
   Nature Science Foundation of Jiangsu Province under Grant BK20160386, in
   part by the Nature Science Foundation of Hubei Province under Grant
   2014CFB712, in part by the Nature Science Foundation of Jiangxi Province
   under Grant 20151BAB217013, and in part by the Fundamental Research
   Funds for the Central Universities under Grant 2042014kf0250. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Winston Hsu. (Corresponding
   author: Chao Liang.)
CR Ali S., 2010, ACM Multimedia Conference (ACM MM), P895
   An L., 2013, PROC 7 INT C DISTRIB, P1
   [Anonymous], 2013, P IEEE INT MULT EXP
   [Anonymous], IEEE T CIRC IN PRESS
   [Anonymous], IEEE T NEUR IN PRESS
   [Anonymous], 2007, P IEEE INT WORKSH PE
   [Anonymous], 2014, PERSON REIDENTIFICAT
   Chen C, 2016, IEEE SENS J, V16, P773, DOI 10.1109/JSEN.2015.2487358
   Chen C, 2015, IEEE T HUM-MACH SYST, V45, P51, DOI 10.1109/THMS.2014.2362520
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fox NA, 2007, IEEE T MULTIMEDIA, V9, P701, DOI 10.1109/TMM.2007.893339
   Gheissari N., 2006, 2006 IEEE computer society conference on computer vision and pattern recognition (CVPR'06), V2, P1528
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hariharakrishnan K, 2005, IEEE T MULTIMEDIA, V7, P853, DOI 10.1109/TMM.2005.854437
   Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56
   Hoi SCH, 2008, IEEE T MULTIMEDIA, V10, P607, DOI 10.1109/TMM.2008.921735
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Hsu WinstonH., 2007, ACM MM
   Jiang JJ, 2016, IEEE T CIRC SYST VID, V26, P1674, DOI 10.1109/TCSVT.2015.2433538
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Lan RS, 2016, IEEE T IMAGE PROCESS, V25, P566, DOI 10.1109/TIP.2015.2507404
   Leng QM, 2015, MULTIMED TOOLS APPL, V74, P6989, DOI 10.1007/s11042-014-1949-7
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li W, 2012, IEEE IMAGE PROC, P1621, DOI 10.1109/ICIP.2012.6467186
   Li Z, 2013, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2013.463
   Liang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P771, DOI 10.1145/2733373.2807399
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu CX, 2013, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2013.62
   Liu CX, 2012, LECT NOTES COMPUT SC, V7583, P391, DOI 10.1007/978-3-642-33863-2_39
   Lo Presti L, 2012, IEEE T MULTIMEDIA, V14, P346, DOI 10.1109/TMM.2011.2173323
   Ma AJ, 2015, LECT NOTES COMPUT SC, V9007, P397, DOI 10.1007/978-3-319-16814-2_26
   Mang Ye, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P105, DOI 10.1007/978-3-319-14445-0_10
   Nabiei R., 2016, Sensing, Processing and Learning for Intelligent Machines (SPLINE), 2016 First International Workshop on, P1
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   Qin J, 2016, IEEE T IMAGE PROCESS, V25, P756, DOI 10.1109/TIP.2015.2508600
   Qin T., 2010, ADV NEURAL INFORM PR, V23, P1948
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Tao DP, 2013, IEEE T CIRC SYST VID, V23, P1675, DOI 10.1109/TCSVT.2013.2255413
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Wang YM, 2014, IEEE T CIRC SYST VID, V24, P1350, DOI 10.1109/TCSVT.2014.2305519
   Wang Z., 2014, PROC 15 ANN PACIFICR, P1
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Ye M, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1239, DOI 10.1145/2733373.2806326
   Ye M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P547, DOI 10.1145/2671188.2749347
   Yu J, 2014, IEEE T MULTIMEDIA, V16, P159, DOI 10.1109/TMM.2013.2284755
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zhang ST, 2012, LECT NOTES COMPUT SC, V7573, P660, DOI 10.1007/978-3-642-33709-3_47
   Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314
   Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng L, 2015, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2015.7298783
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
NR 58
TC 151
Z9 158
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2553
EP 2566
DI 10.1109/TMM.2016.2605058
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200020
DA 2024-07-18
ER

PT J
AU Zheng, HT
   Fang, L
   Ji, MQ
   Strese, M
   Özer, Y
   Steinbach, E
AF Zheng, Haitian
   Fang, Lu
   Ji, Mengqi
   Strese, Matti
   Ozer, Yigitcan
   Steinbach, Eckehard
TI Deep Learning for Surface Material Classification Using Haptic and
   Visual Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; haptic signal; hybrid inputs; surface
   material classification
ID CONVOLUTIONAL NEURAL-NETWORKS; TEXTURE CLASSIFICATION
AB When a user scratches a hand-held rigid tool across an object surface, an acceleration signal can be captured, which carries relevant information about the surface material properties. More importantly, such haptic acceleration signals can be used together with surface images to jointly recognize the surface material. In this paper, we present a novel deep learning method dealing with the surface material classification problem based on a fully convolutional network, which takes the aforementioned acceleration signal and a corresponding image of the surface texture as inputs. Compared to the existing surface material classification solutions which rely on a careful design of hand-crafted features, our method automatically extracts discriminative features utilizing advanced deep learning methodologies. Experiments performed on the TUM surface material database demonstrate that our method achieves state-of-the-art classification accuracy robustly and efficiently.
C1 [Zheng, Haitian; Ji, Mengqi] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Fang, Lu] Hong Kong Univ Sci & Technol, Inst Robot, Hong Kong, Hong Kong, Peoples R China.
   [Strese, Matti; Ozer, Yigitcan; Steinbach, Eckehard] Tech Univ Munich, D-80333 Munich, Germany.
C3 Hong Kong University of Science & Technology; Hong Kong University of
   Science & Technology; Technical University of Munich
RP Fang, L (corresponding author), Hong Kong Univ Sci & Technol, Inst Robot, Hong Kong, Hong Kong, Peoples R China.
EM zheng.ht.ustc@gmail.com; fanglu922@gmail.com; mji@ust.hk;
   matti.strese@tum.de; yiit.oezer@tum.de; eckehard.steinbach@tum.de
OI Ozer, Yigitcan/0000-0003-2235-8655; Steinbach,
   Eckehard/0000-0001-8853-2703
FU Natural Science Foundation of China [61303151]; GRF [16211615];
   Alexander von Humboldt Foundation
FX This work was supported in part by the Natural Science Foundation of
   China under Contract 61303151, and in part by the GRF 16211615. The work
   of L. Fang was supported in part by the Alexander von Humboldt
   Foundation under a Research Fellowship. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Wolfgang Hurst.
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Abdel-Hamid Ossama., 2013, P 14 ANN C INT SPEEC, P3366
   [Anonymous], CORR
   [Anonymous], THESIS
   [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2014, P INT SOC MUSIC INFO
   [Anonymous], 2015, 3 INT C LEARNING REP
   [Anonymous], 2014, Improved Musical Onset Detection with Convolutional Neural Networks, DOI DOI 10.1109/ICASSP.2014.6854953
   [Anonymous], IEEE INT WORKSH MACH
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], P 4 JOINT WORKSH HAN
   [Anonymous], SWEDISH U AGR SCI UP
   Arvis V., 2004, Image Analysis & Stereology, V23, P63, DOI 10.5566/ias.v23.p63-72
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Culbertson H, 2013, 2013 WORLD HAPTICS CONFERENCE (WHC), P295, DOI 10.1109/WHC.2013.6548424
   Fishel Jeremy A, 2012, Front Neurorobot, V6, P4, DOI 10.3389/fnbot.2012.00004
   Gao Y., 2015, CoRR
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kereliuk C, 2015, IEEE T MULTIMEDIA, V17, P2059, DOI 10.1109/TMM.2015.2478068
   Landin N, 2010, LECT NOTES COMPUT SC, V6192, P79, DOI 10.1007/978-3-642-14075-4_12
   Liu L, 2012, IEEE T PATTERN ANAL, V34, P574, DOI 10.1109/TPAMI.2011.145
   Liu N., 2014, INT C IM VIS COMP NZ, P96
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Qi XB, 2016, NEUROCOMPUTING, V171, P1230, DOI 10.1016/j.neucom.2015.07.071
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romano JM, 2014, IEEE HAPTICS SYM, P49, DOI 10.1109/HAPTICS.2014.6775432
   Romano JM, 2012, IEEE T HAPTICS, V5, P109, DOI [10.1109/TOH.2011.38, 10.1109/ToH.2011.38]
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Shen CY, 2015, IEEE T MULTIMEDIA, V17, P2084, DOI 10.1109/TMM.2015.2483370
   Simonyan K., 2014, CORR
   Strese Matti, 2014, 2014 IEEE International Symposium on Haptic, Audio and Visual Environments and Games (HAVE). Proceedings, P118, DOI 10.1109/HAVE.2014.6954342
   Strese M, 2015, 2015 IEEE WORLD HAPTICS CONFERENCE (WHC), P214, DOI 10.1109/WHC.2015.7177716
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Tivive F. H. C., 2006, PROC IEEE REGION 10, P1
   Varma M, 2003, PROC CVPR IEEE, P691
   Wager Stefan, 2013, Advances in Neural Information Processing Systems, P351, DOI DOI 10.48550/ARXIV.1307.1493
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 46
TC 69
Z9 70
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2407
EP 2416
DI 10.1109/TMM.2016.2598140
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Renoust, B
   Le, DD
   Satoh, S
AF Renoust, Benjamin
   Le, Duy-Dinh
   Satoh, Shin'Ichi
TI Visual Analytics of Political Networks From Face-Tracking of News Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data visualization; face recognition; graph; mltimedia database
ID VISUALIZATION; PRESIDENTIALIZATION; DESIGN; MEDIA
AB The rich nature of news makes it a classic subject of visual analytics research. Such analysis is often based on rich textual data. However, we want to test how much we can understand the news from video information through face detection and tracking. Towards this goal, we propose a visual analytics system and discuss its design and implementation to support media experts in understanding political interactions in an archive of 12 years of the Japanese public broadcaster NHK's News 7 program. After identifying the tasks and abstraction required for our analysis, we construct links from face detection and tracking to derive multiple political networks. Our proposed design embeds this rich data into a visual analytics framework that presents four levels of abstraction: time period, network, timeline, and face-tracks within video. We present how the exploration of the archive with our system results in good understanding of the Japanese politico-media scene during these 12 years while finding evidence of "presidentialization" of the media.
C1 [Renoust, Benjamin] Natl Inst Informat, Tokyo 1018430, Japan.
   [Renoust, Benjamin] CNRS, Japanese French Lab Informat, UMI 3527, Tokyo 1900100, Japan.
   [Le, Duy-Dinh; Satoh, Shin'Ichi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Research Organization of
   Information & Systems (ROIS); National Institute of Informatics (NII) -
   Japan
RP Renoust, B (corresponding author), Natl Inst Informat, Tokyo 1018430, Japan.; Renoust, B (corresponding author), CNRS, Japanese French Lab Informat, UMI 3527, Tokyo 1900100, Japan.
EM renoust@nii.ac.jp; ledduy@nii.ac.jp; satoh@nii.ac.jp
FU Japanese Society for the Promotion of Science [848]; Grants-in-Aid for
   Scientific Research [26240016, 15F14773] Funding Source: KAKEN
FX This work was supported by the Japanese Society for the Promotion of
   Science under FY2015 Grant-848 in-Aid for Scientific Research. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Yingcai Wu.
CR [Anonymous], 2013, P ACM MULTIMEDIA
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2016, Applied Network Science
   [Anonymous], 2013, ITE T MEDIA TECHNOLO
   [Anonymous], 2002, TOPIC DETECTION TRAC
   [Anonymous], ENCY SOCIAL NETWORK
   [Anonymous], 2015, 2015 INT C BIOM SPEC
   Barry A., 1997, VISUAL INTELLIGENCE
   Benikova D., 2014, P 12 WORKSH KONVENS, P1
   Bostock M, 2011, IEEE T VIS COMPUT GR, V17, P2301, DOI 10.1109/TVCG.2011.185
   Brehmer M, 2013, IEEE T VIS COMPUT GR, V19, P2376, DOI 10.1109/TVCG.2013.124
   Diakopoulos N., 2010, 2010 Proceedings of IEEE Symposium on Visual Analytics Science and Technology (VAST 2010), P115, DOI 10.1109/VAST.2010.5652922
   Everingham M., 2006, P BRIT MACH VIS C, V2
   Farfade SS, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P643, DOI 10.1145/2671188.2749408
   Frick A., 1994, INT S GRAPH DRAW, P388, DOI DOI 10.1007/3-540-58950-3_393
   Ghoniem M, 2004, IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2004, PROCEEDINGS, P17, DOI 10.1109/INFVIS.2004.1
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   Gupta S, 2016, IEEE PAC VIS SYMP, P168, DOI 10.1109/PACIFICVIS.2016.7465265
   Harrower M, 2003, CARTOGR J, V40, P27, DOI 10.1179/000870403235002042
   Henry N, 2007, LECT NOTES COMPUT SC, V4663, P288
   Henry N, 2007, IEEE T VIS COMPUT GR, V13, P1302, DOI 10.1109/TVCG.2007.70582
   Ide I, 2004, LECT NOTES COMPUT SC, V3115, P123
   Itoh M, 2014, IEEE PAC VIS SYMP, P129, DOI 10.1109/PacificVis.2014.49
   Jou W, 2015, JPN J POLIT SCI, V16, P357, DOI 10.1017/S1468109915000237
   Katayama Norio, 2005, ADV MULTIMEDIA INFOR, V1, P489
   Kienreich W, 2010, IEEE INT CONF INF VI, P375, DOI 10.1109/IV.2010.58
   Kochtchi A, 2014, COMPUT GRAPH FORUM, V33, P211, DOI 10.1111/cgf.12377
   Krauss ES, 2005, BRIT J POLIT SCI, V35, P357, DOI 10.1017/S0007123405000190
   Krstajic M., 2012, P IS T SPIE EL IM
   Lazer D, 2011, PS-POLIT SCI POLIT, V44, P61, DOI 10.1017/S1049096510001873
   Li Hongzhi., 2013, Proceedings of the 21st ACM international conference on Multimedia, P449
   Lu YF, 2016, IEEE T VIS COMPUT GR, V22, P220, DOI 10.1109/TVCG.2015.2467991
   Luo HZ, 2006, IEEE CONF VIS ANAL, P75
   Luo HZ, 2007, IEEE CONF VIS ANAL, P107
   Munzner T, 2009, IEEE T VIS COMPUT GR, V15, P921, DOI 10.1109/TVCG.2009.111
   Nan CJ, 2015, PROCEEDINGS OF THE 2015 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2015), P831, DOI 10.1145/2808797.2809306
   Renoust B, 2015, COMPUT GRAPH FORUM, V34, P321, DOI 10.1111/cgf.12644
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sedlmair M, 2012, IEEE T VIS COMPUT GR, V18, P2431, DOI 10.1109/TVCG.2012.213
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shneiderman B, 1996, IEEE SYMPOSIUM ON VISUAL LANGUAGES, PROCEEDINGS, P336, DOI 10.1109/VL.1996.545307
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Ngo TD, 2013, IEICE T INF SYST, VE96D, P1811, DOI 10.1587/transinf.E96.D.1811
   Viard J, 2016, THEOR COMPUT SCI, V609, P245, DOI 10.1016/j.tcs.2015.09.030
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Waumans MC, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0126470
NR 46
TC 21
Z9 23
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2184
EP 2195
DI 10.1109/TMM.2016.2614224
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
   Yan, SC
   Hossain, MS
   Ghoneim, A
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
   Yan, Shuicheng
   Hossain, M. Shamim
   Ghoneim, Ahmed
TI Deep Relative Attributes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; relative attributes (RA)
ID OBJECT CLASSES; RETRIEVAL
AB Relative attribute (RA) learning aims to learn the ranking function describing the relative strength of the attribute. Most of current learning approaches learn a linear ranking function for each attribute by use of the hand-crafted visual features. Different from the existing study, in this paper, we propose a novel deep relative attributes (DRA) algorithm to learn visual features and the effective nonlinear ranking function to describe the RA of image pairs in a unified framework. Here, visual features and the ranking function are learned jointly, and they can benefit each other. The proposed DRA model is comprised of five convolutional neural layers, five fully connected layers, and a relative loss function which contains the contrastive constraint and the similar constraint corresponding to the ordered image pairs and the unordered image pairs, respectively. To train the DRA model effectively, we make use of the transferred knowledge from the large scale visual recognition on ImageNet [1] to the RA learning task. We evaluate the proposed DRA model on three widely used datasets. Extensive experimental results demonstrate that the proposed DRA model consistently and significantly outperforms the state-of-the-art RA learning methods. On the public OSR, PubFig, and Shoes datasets, compared with the previous RA learning results [2], the average ranking accuracies have been significantly improved by about 8%, 9%, and 14%, respectively.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Hossain, M. Shamim; Ghoneim, Ahmed] King Saud Univ, Coll Comp & Informat Sci, Dept Software Engn, Riyadh 11543, Saudi Arabia.
   [Ghoneim, Ahmed] Menoufia Univ, Coll Sci, Dept Comp Sci, Menoufia 32721, Egypt.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Singapore; King Saud University; Egyptian Knowledge Bank
   (EKB); Menofia University
RP Yang, XS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
EM xiaoshan.yang@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   eleyans@nus.edu.sg; mshossain@ksu.edu.sa; ghoneim@ksu.edu.sa
RI Hossain, M. Shamim/K-1362-2014; ghoneim, ahmed/L-3019-2013; Guizani,
   Mohsen/AAX-4534-2021; xu, cj/HJZ-3488-2023; Yan,
   Shuicheng/HCI-1431-2022; Zhang, Tianzhu/AGY-9389-2022
OI Hossain, M. Shamim/0000-0001-5906-9422; Guizani,
   Mohsen/0000-0002-8972-8094; Zhang, Tianzhu/0000-0003-0764-6106; Ghoneim,
   Ahmed/0000-0003-2076-8925
FU Deanship of Scientific Research at King Saud University, Riyadh, Saudi
   Arabia [RGP-229]
FX The authors extend their appreciation to the Deanship of Scientific
   Research at King Saud University, Riyadh, Saudi Arabia, for funding this
   work through the research group project no. RGP-229.
CR [Anonymous], P ACM MULT C MM 12 N
   [Anonymous], PROC BRIT MACH VIS C
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], P ACM MULT C MM 12 N
   [Anonymous], 2013, P INT ACM C MULTIMED, DOI DOI 10.1145/2502081.2502093
   [Anonymous], 2012, 20 ACM INT C MULT
   [Anonymous], 2012, MM
   [Anonymous], P ACM MULT C MM 12 N
   Bengio Y., ADV NEURAL INFORM PR, P153
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Biswas A, 2013, PROC CVPR IEEE, P644, DOI 10.1109/CVPR.2013.89
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Cao XC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1093, DOI 10.1145/2647868.2654972
   Chen BC, 2013, IEEE T MULTIMEDIA, V15, P1163, DOI 10.1109/TMM.2013.2242460
   Chen L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1045, DOI 10.1145/2647868.2655050
   Chen L, 2014, PROC CVPR IEEE, P1027, DOI 10.1109/CVPR.2014.135
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1388, DOI 10.1109/TMM.2013.2250492
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P 20 INT C NEUR INF, P433
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Han YH, 2014, IEEE T MULTIMEDIA, V16, P1115, DOI 10.1109/TMM.2014.2306092
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu JL, 2014, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2014.242
   Huang JS, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P731, DOI 10.1145/2647868.2654885
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HH, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1013, DOI 10.1145/2647868.2655023
   Lin CH, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1157, DOI 10.1145/2647868.2655016
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451
   Parkash A, 2012, LECT NOTES COMPUT SC, V7574, P354, DOI 10.1007/978-3-642-33712-3_26
   Qian SS, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P99, DOI 10.1145/2733373.2806234
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Qian SS, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2659521
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Russakovsky O., 2010, LNCS, P1, DOI DOI 10.1007/978-3-642-35749-7_1
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salakhutdinov R., 2009, AISTATS
   Simonyan K., 2014, CORR
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang G, 2009, IEEE I CONF COMP VIS, P537, DOI 10.1109/ICCV.2009.5459194
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Wang YJ, 2010, PRODUCTION GRIDS IN ASIA: APPLICATIONS, DEVELOPMENTS AND GLOBAL TIES, P155, DOI 10.1007/978-1-4419-0046-3_13
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700286
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HW, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P187, DOI 10.1145/2647868.2654915
   Zhang TZ, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2602633
NR 60
TC 66
Z9 67
U1 4
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1832
EP 1842
DI 10.1109/TMM.2016.2582379
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800013
DA 2024-07-18
ER

PT J
AU Bai, S
   Bai, X
   Liu, WY
AF Bai, Song
   Bai, Xiang
   Liu, Wenyu
TI Multiple Stage Residual Model for Image Classification and Vector
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate nearest neighbor (ANN) search; image classification;
   residual vector; shape recognition; vector compression
ID QUANTIZATION; SHAPES
AB Feature coding is a fundamental issue with many vision tasks, such as image classification, image retrieval and image segmentation, etc. There is no doubt that the encoding procedure leads to information loss, due to the existence of quantization error. The residual vector, defined as the difference between the feature and its corresponding visual word, is the chief culprit to be responsible for the quantization error. Many previous algorithms consider it as a coding issue, and focus on reducing the quantization error by reconstructing the feature with more than one visual word, or by the so-called soft-assignment strategy. In this paper, we consider the problem from a different point of view, and propose an effective and efficient model called multiple stage residual model (MSRM). It makes full use of the residual vector to generate a multiple stage code. MSRM is a hierarchical structure, with the bottom stage producing the coarsest quantization, and the top stage producing the finest quantization. Moreover, our proposed model is a generic framework, which can be built upon many coding algorithms. The interplay of such a coarse-to-fine quantization procedure with a discriminative classifier (e.g., SVM) can improve the classification accuracy of the baseline algorithms significantly. As a special case of MSRM, multiple stage vector quantization (MSVQ) can be directly used for vector compression and approximate nearest neighbor search, and achieves competitive performances with high efficiency.
C1 [Bai, Song; Bai, Xiang; Liu, Wenyu] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology
RP Bai, S (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM songbai@hust.edu.cn; xbai@hust.edu.cn; liuwy@hust.edu.cn
RI Liu, Wenyu/AAG-1426-2019
OI Liu, Wenyu/0000-0002-4582-7488; Bai, Xiang/0000-0002-3449-5940
FU National Natural Science Foundation of China (NSFC) [6122308, 61573160]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 6122308 and Grant 61573160. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Wolfgang Hurst.
CR [Anonymous], 2013, International Conference on Machine Learning
   [Anonymous], P 12 AS C COMP VIS
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 31 INT C INT C MAC
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], CNSTR2007001 CAL TEC
   [Anonymous], INT C AC SPEECH SIGN
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Bai S, 2016, IEEE T IMAGE PROCESS, V25, P1056, DOI 10.1109/TIP.2016.2514498
   Bai X, 2015, IEEE T PATTERN ANAL, V37, P2361, DOI 10.1109/TPAMI.2015.2424863
   Bai X, 2009, IEEE I CONF COMP VIS, P575, DOI 10.1109/ICCV.2009.5459188
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Biing-Hwang Juang, 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P597
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   de Rooij O, 2013, IEEE T MULTIMEDIA, V15, P898, DOI 10.1109/TMM.2013.2237894
   Hu F, 2015, IEEE J-STARS, V8, P2015, DOI 10.1109/JSTARS.2015.2444405
   Huang YZ, 2014, IEEE T PATTERN ANAL, V36, P493, DOI 10.1109/TPAMI.2013.113
   Huang YZ, 2011, PROC CVPR IEEE, P1753, DOI 10.1109/CVPR.2011.5995682
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kosala Raymond., 2000, SIGKDD EXPLOR NEWSL, V2, P1
   Latecki LJ, 1999, COMPUT VIS IMAGE UND, V73, P441, DOI 10.1006/cviu.1998.0738
   Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li L.J., 2007, PROC IEEE INT C COMP
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo L, 2013, IEEE T MULTIMEDIA, V15, P1174, DOI 10.1109/TMM.2013.2242450
   Mairal J., 2009, ADV NEURAL INFORM PR, P1033
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Ramesh B, 2015, PATTERN RECOGN, V48, P894, DOI 10.1016/j.patcog.2014.09.019
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shaban A, 2013, PROC CVPR IEEE, P2794, DOI 10.1109/CVPR.2013.360
   Shabou A, 2012, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2012.6248107
   Shen W, 2014, IEEE T CYBERNETICS, V44, P1053, DOI 10.1109/TCYB.2013.2279071
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sun KB, 2005, PROC CVPR IEEE, P727
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008
   Yang JC, 2010, PROC CVPR IEEE, P3517, DOI 10.1109/CVPR.2010.5539958
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang W, 2015, IEEE T GEOSCI REMOTE, V53, P4472, DOI 10.1109/TGRS.2015.2400449
   Yi J, 2013, IEEE T MULTIMEDIA, V15, P1400, DOI 10.1109/TMM.2013.2250266
   Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
   Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250
   Zhou XB, 2010, PRACT RESOUR MENT, P141, DOI 10.1016/B978-0-12-375035-8.10005-9
NR 56
TC 15
Z9 16
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1351
EP 1362
DI 10.1109/TMM.2016.2557071
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600011
DA 2024-07-18
ER

PT J
AU Hua, Y
   Wang, SH
   Liu, SY
   Cai, AN
   Huang, QM
AF Hua, Yan
   Wang, Shuhui
   Liu, Siyuan
   Cai, Anni
   Huang, Qingming
TI Cross-Modal Correlation Learning by Adaptive Hierarchical Semantic
   Aggregation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; localized correlation learning; semantic
   hierarchy
ID SIMILARITY
AB With the explosive growth of web data, effective and efficient technologies are in urgent need for retrieving semantically relevant contents of heterogeneous modalities. Previous studies devote efforts to modeling simple cross-modal statistical dependencies, and globally projecting the heterogeneous modalities into a measurable subspace. However, global projections cannot appropriately adapt to diverse contents, and the naturally existing multilevel semantic relation in web data is ignored. We study the problem of semantic coherent retrieval, where documents from different modalities should be ranked by the semantic relevance to the query. Accordingly, we propose TINA, a correlation learning method by adaptive hierarchical semantic aggregation. First, by joint modeling of content and ontology similarities, we build a semantic hierarchy to measure multilevel semantic relevance. Second, with a set of local linear projections and probabilistic membership functions, we propose two paradigms for local expert aggregation, i.e., local projection aggregation and local distance aggregation. To learn the cross-modal projections, we optimize the structure risk objective function that involves semantic coherence measurement, local projection consistency, and the complexity penalty of local projections. Compared to existing approaches, a better bias-variance tradeoff is achieved by TINA in real-world cross-modal correlation learning tasks. Extensive experiments on widely used NUS-WIDE and ICML-Challenge for image-text retrieval demonstrate that TINA better adapts to the multilevel semantic relation and content divergence, and, thus, outperforms state of the art with better semantic coherence.
C1 [Hua, Yan; Wang, Shuhui; Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intellectual Informat Proc, Beijing 100190, Peoples R China.
   [Hua, Yan; Cai, Anni] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
   [Hua, Yan; Wang, Shuhui] Commun Univ China, Sch Informat Engn, Beijing 100024, Peoples R China.
   [Liu, Siyuan] Penn State Univ, Smeal Coll Business, University Pk, PA 16801 USA.
   [Liu, Siyuan] Shenzhen Inst Adv Technol, Inst Adv Comp & Digital Engn, Ctr Cloud Comp, Shenzhen 518055, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Beijing University of Posts & Telecommunications; Communication
   University of China; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Pennsylvania State University; Pennsylvania State
   University - University Park; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Wang, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intellectual Informat Proc, Beijing 100190, Peoples R China.; Wang, SH (corresponding author), Commun Univ China, Sch Informat Engn, Beijing 100024, Peoples R China.
EM huayan@cuc.edu.cn; wangshuhui@ict.ac.cn; siyuan@psu.edu;
   annicai@bupt.edu.cn; qmhuang@ucas.ac.cn
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2012, INT J COMPUT VIS
   [Anonymous], 2003, P ACM INT C MULT ACM
   [Anonymous], P 2008 IEEE C COMP V
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2011, P ICML
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2009, P ACM INT C IM VID R
   Archambeau Cedric., 2009, P 21 INT C NEUR INF, P73
   Banerjee S., 2003, P 18 INT JOINT C ART, P805
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Budanitsky A, 2006, COMPUT LINGUIST, V32, P13, DOI 10.1162/coli.2006.32.1.13
   Chen X., 2012, Artificial intelligence and statistics, P199, DOI DOI 10.1184/R1/6473711
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2011, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR.2011.5995474
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Gavves E, 2012, PROC CVPR IEEE, P3610, DOI 10.1109/CVPR.2012.6248106
   Grauman Kristen., 2011, Advances in neural information processing systems, P621
   Hardoon DR, 2011, MACH LEARN, V83, P331, DOI 10.1007/s10994-010-5222-7
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Karpathy Andrej, 2014, Advances in neural information processing systems, P1889
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li LJ, 2010, PROC CVPR IEEE, P3336, DOI 10.1109/CVPR.2010.5540027
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Lu XY, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P433
   Marszalek M, 2008, LECT NOTES COMPUT SC, V5305, P479, DOI 10.1007/978-3-540-88693-8_35
   Masci J., 2013, CORR
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Peng Y, 2010, NEURAL PROCESS LETT, V31, P1, DOI 10.1007/s11063-009-9123-3
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Sun TK, 2007, IMAGE VISION COMPUT, V25, P531, DOI 10.1016/j.imavis.2006.04.014
   Sun TK, 2008, IEEE DATA MINING, P1043, DOI 10.1109/ICDM.2008.28
   Verma N, 2012, PROC CVPR IEEE, P2280, DOI 10.1109/CVPR.2012.6247938
   Virtanen S., 2011, P 28 INT C INT C MAC, P457, DOI DOI 10.5555/3104482.3104540
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Y, 2010, AAAI CONF ARTIF INTE, P649
   Yu Z, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P395, DOI 10.1145/2600428.2609563
   Zhang H., 2007, P 15 INT C MULTIMEDI, P273
NR 51
TC 43
Z9 45
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1201
EP 1216
DI 10.1109/TMM.2016.2535864
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100021
DA 2024-07-18
ER

PT J
AU Sung, J
   Kim, M
   Lim, K
   Rhee, JKK
AF Sung, Jihoon
   Kim, Minseok
   Lim, Kyongchun
   Rhee, June-Koo Kevin
TI Efficient Cache Placement Strategy in Two-Tier Wireless Content Delivery
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cache placement; contention; cross-layer design; two-tier architecture;
   wireless content delivery network (WCDN)
ID COST; SERVICES
AB Internet content caching for multimedia services has received much attention mainly in the field of large-scale wired networking as a primary solution to save network resources and improve quality of service (QoS). Rapidly increasing consumption of multimedia content in mobile networks brings a challenge of how to efficiently deliver content in local wireless access networks. Cache embedment in wireless mesh environment is an intriguing attempt to enhance the QoS and service capacity, leading to the question of how to design an efficient content delivery network considering the inherent characteristics of the wireless environment. We propose and investigate an efficient cache placement strategy in novel two-tier wireless content delivery networks, which utilize separate channels for content dissemination and content service. Such a two-tier network system model helps to achieve much better content delivery performance with a greatly reduced system design complexity compared to single-tier network system models. Further, we incorporate a delay cost due to contention, which is mainly responsible for performance in shared-medium wireless networks, as a key metric for cache placement under the system model. After formally formulating the cache placement problem, we propose a cross-layer heuristic algorithm and demonstrate its performance compared with an optimal solution by integer linear programming. Simulation results show significant performance improvements by our strategy compared to the performance of existing representative strategies in terms of service delay, packet delivery ratio, and the amount of delivered packets within a given delay deadline.
C1 [Sung, Jihoon; Kim, Minseok; Lim, Kyongchun; Rhee, June-Koo Kevin] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Rhee, JKK (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
EM sung.jh@kaist.ac.kr; kms4105@kaist.ac.kr; lim.kc@kaist.ac.kr;
   rhee.jk@kaist.edu
RI kim, minseuk/ABA-3561-2021
OI Sung, Jihoon/0000-0003-0041-3238
FU ICT R&D program of MSIP/IITP, South Korea [B0101-15-1368]
FX This work was supported by the ICT R&D program of MSIP/IITP, South Korea
   (B0101-15-1368, "Development of an NFV-inspired networked switch and an
   operating system for multi-middlebox services"). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Liang Zhou. (Corresponding author: June-Koo Kevin
   Rhee.)
CR Afanasyev M, 2010, IEEE ACM T NETWORK, V18, P1359, DOI 10.1109/TNET.2010.2040087
   Ahuja S, 2008, IEEE T MULTIMEDIA, V10, P1382, DOI 10.1109/TMM.2008.2004930
   Almeida JM, 2004, IEEE T MULTIMEDIA, V6, P356, DOI 10.1109/TMM.2003.822796
   [Anonymous], 2012, WILL SMALL PACK DEGR
   [Anonymous], ISME J
   [Anonymous], 2013, FARM HARD DRIV 2 YEA
   [Anonymous], 2013, P IEEE 25 INT TEL C
   [Anonymous], 2014, Multimedia and Expo (ICME), 2014 IEEE International Conference on
   Bangerter B, 2014, IEEE COMMUN MAG, V52, P90, DOI 10.1109/MCOM.2014.6736748
   Chen P, 2006, 2006 1ST IEEE CONFERENCE ON INDUSTRIAL ELECTRONICS AND APPLICATIONS, VOLS 1-3, P1
   Cheng CM, 2015, IEEE T VEH TECHNOL, V64, P1051, DOI 10.1109/TVT.2014.2329015
   Chin WH, 2014, IEEE WIREL COMMUN, V21, P106, DOI 10.1109/MWC.2014.6812298
   Chiu GM, 2011, IEEE T COMPUT, V60, P1431, DOI 10.1109/TC.2010.231
   Cisco Systems, 2015, UNL POW INT THINGS
   Fan XP, 2011, J PARALLEL DISTR COM, V71, P603, DOI 10.1016/j.jpdc.2010.12.008
   Forouzan B. A., 2009, TCP IP PROTOCOL SUIT, P297
   Fu B, 2014, IEEE COMMUN SURV TUT, V16, P110, DOI 10.1109/SURV.2013.081313.00231
   Gkatzikis L, 2015, IEEE ICC, P5872, DOI 10.1109/ICC.2015.7249258
   Godaliyadda GMRI, 2010, WIREL TELECOMM SYMP
   Golrezaei N, 2013, IEEE COMMUN MAG, V51, P142, DOI 10.1109/MCOM.2013.6495773
   Hwang I, 2013, IEEE COMMUN MAG, V51, P20, DOI 10.1109/MCOM.2013.6525591
   Korte B, 2012, ALGORITHMS COMB, V21, P413, DOI 10.1007/978-3-642-24488-9_16
   Krishnan P, 2000, IEEE ACM T NETWORK, V8, P568, DOI 10.1109/90.879344
   Kumbhkar R., 2015, 2015 7th International Conference on Communication Systems and Networks (COMSNETS), P1
   Li B, 1999, IEEE INFOCOM SER, P1282, DOI 10.1109/INFCOM.1999.752146
   Nuggehalli P., 2003, Proceedings of the 4th ACM international symposium on Mobile ad hoc networking computing (MobiHoc), New York, NY, USA, P25
   Nuggehalli P, 2006, IEEE ACM T NETWORK, V14, P1045, DOI 10.1109/TNET.2006.882863
   Pathak P. H., 2014, DESIGNING NETWORK SE, P1
   Pathan M, 2014, WILEY SER PARA DIST, P3
   Poularakis K, 2014, IEEE INFOCOM SER, P1087, DOI 10.1109/INFOCOM.2014.6848038
   Qiu LL, 2001, IEEE INFOCOM SER, P1587, DOI 10.1109/INFCOM.2001.916655
   Simo-Reigadas J, 2015, COMPUT NETW, V93, P245, DOI 10.1016/j.comnet.2015.09.006
   Smith-Prei C, 2014, WOMEN GER YEARB, V30, P209
   Song J, 2015, IEEE ICC, P1825, DOI 10.1109/ICC.2015.7248590
   Srivastava V, 2005, IEEE COMMUN MAG, V43, P112, DOI 10.1109/MCOM.2005.1561928
   Sung J, 2013, 2013 INTERNATIONAL CONFERENCE ON ICT CONVERGENCE (ICTC 2013): FUTURE CREATIVE CONVERGENCE TECHNOLOGIES FOR NEW ICT ECOSYSTEMS, P238, DOI 10.1109/ICTC.2013.6675348
   Traverso S, 2015, IEEE T MULTIMEDIA, V17, P1839, DOI 10.1109/TMM.2015.2458043
   Wang HL, 2006, 2006 IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS, VOLS 1 AND 2, P7
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P867, DOI 10.1109/TMM.2015.2425216
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P92, DOI 10.1109/TMM.2014.2365364
   Xu CQ, 2013, IEEE T VEH TECHNOL, V62, P2273, DOI 10.1109/TVT.2012.2228682
   Yang R., 2013, SUMMARY SK TELECOMS
   Zhang Q, 2004, IEEE T MULTIMEDIA, V6, P587, DOI [10.1109/TMM.2004.830816, 10.1109/tmm.2004.830816]
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou L, 2014, IEEE T CIRC SYST VID, V24, P889, DOI 10.1109/TCSVT.2013.2291311
NR 46
TC 39
Z9 42
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1163
EP 1174
DI 10.1109/TMM.2016.2543658
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100018
DA 2024-07-18
ER

PT J
AU Toni, L
   Cheung, G
   Frossard, P
AF Toni, Laura
   Cheung, Gene
   Frossard, Pascal
TI In-Network View Synthesis for Interactive Multiview Video Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud-assisted applications; depth-image-based rendering (DIBR);
   interactive systems; network processing
ID PREDICTION STRUCTURES; REPRESENTATION; ALLOCATION
AB In multiview applications, camera views can be used as reference views to synthesize additional virtual viewpoints, allowing users to freely navigate within a 3D scene. However, bandwidth constraints may restrict the number of reference views sent to clients, limiting the quality of the synthesized viewpoints. In this work, we study the problem of in-network reference view synthesis aimed at improving the navigation quality at the clients. We consider a distributed cloud network architecture, where data stored in a main cloud is delivered to end users with the help of cloudlets, i.e., resource-rich proxies close to the users. We argue that, in case of limited bandwidth from the cloudlet to the users, re-sampling at the couldlet the viewpoints of the 3D scene (i.e., synthesizing novel virtual views in the cloudlets to be used as new references to the decoder) is beneficial compared to mere subsampling of the original set of camera views. We therefore cast a new reference view selection problem that seeks the subset of views minimizing the distortion over a view navigation window defined by the user under bandwidth constraints. We prove that the problem is NP-hard, and we propose an effective polynomial time algorithm using dynamic programming to solve the optimization problem under general assumptions that cover most of the multiview scenarios in practice. Simulation results confirm the performance gain offered by virtual view synthesis in the network.
C1 [Toni, Laura; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Cheung, Gene] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan
RP Toni, L; Frossard, P (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.; Cheung, G (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
EM laura.toni@epfl.ch; cheung@nii.ac.jp; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019; Cheung, Gene/AAB-9284-2020
FU Swiss National Science Foundation under CHIST-ERA project CONCERT ("A
   Context-Adaptive Content Ecosystem Under Uncertainty") [FNS 20CH21
   151569]
FX This work was supported in part by the Swiss National Science Foundation
   under the CHIST-ERA project CONCERT ("A Context-Adaptive Content
   Ecosystem Under Uncertainty"), FNS 20CH21 151569. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sanjeev Mehrotra.
CR [Anonymous], IEEE T CLOU IN PRESS
   Cai W, 2015, IEEE T CIRC SYST VID, V25, P2092, DOI 10.1109/TCSVT.2015.2450153
   Chakareski J, 2013, IEEE T IMAGE PROCESS, V22, P3473, DOI 10.1109/TIP.2013.2269801
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung NM, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P269
   Cormen Thomas H., 2001, INTRO ALGORITHMS
   De Abreu A, 2015, J VIS COMMUN IMAGE R, V33, P255, DOI 10.1016/j.jvcir.2015.09.010
   De Abreu A, 2013, IEEE SIGNAL PROC LET, V20, P603, DOI 10.1109/LSP.2013.2259815
   De Vieeschouwer C, 2007, IEEE T MULTIMEDIA, V9, P1241, DOI 10.1109/TMM.2007.902869
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Fujihashi T, 2014, IEEE T MULTIMEDIA, V16, P228, DOI 10.1109/TMM.2013.2281588
   Guan ZY, 2014, COMPUTER, V47, P60, DOI 10.1109/MC.2014.114
   Huang ZZ, 2012, IEEE GLOB COMM CONF, P220, DOI 10.1109/GLOCOM.2012.6503116
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Ilkoo Ahn, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P109, DOI 10.1109/ICME.2012.95
   Ji MY, 2016, IEEE J SEL AREA COMM, V34, P176, DOI 10.1109/JSAC.2015.2452672
   Kim C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461926
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Li WF, 2008, IEEE IMAGE PROC, P1508, DOI 10.1109/ICIP.2008.4712053
   Liu S, 2005, IEEE T CIRC SYST VID, V15, P15, DOI 10.1109/TCSVT.2004.839996
   Liu YW, 2010, J VIS COMMUN IMAGE R, V21, P523, DOI 10.1016/j.jvcir.2010.02.004
   Maugey T, 2015, IEEE T IMAGE PROCESS, V24, P1573, DOI 10.1109/TIP.2015.2400817
   Maugey T, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P486, DOI 10.1109/VCIP.2014.7051612
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Miao D., 2013, P SOC PHOTO-OPT INS, V8856
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Pan  Z., 2011, P IEEE ICC, P1
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Rue Havard, 2005, Gaussian Markov Random Fields: Theory and Applications, DOI DOI 10.1201/9780203492024
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tekin C, 2014, ANN ALLERTON CONF, P643, DOI 10.1109/ALLERTON.2014.7028516
   Toni L., 2015, CORR
   Toni L, 2015, IEEE IMAGE PROC, P4486, DOI 10.1109/ICIP.2015.7351655
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   Toni L, 2014, IEEE T MULTIMEDIA, V16, P496, DOI 10.1109/TMM.2013.2291531
   Toni L, 2013, IEEE INT WORKSH MULT, P446, DOI 10.1109/MMSP.2013.6659330
   Verbelen T., 2012, P 3 ACM WORKSH MOB C, P29, DOI [10.1145/2307849.2307858, DOI 10.1145/2307849.2307858]
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu JC, 2011, IEEE T CIRC SYST VID, V21, P220, DOI 10.1109/TCSVT.2011.2105571
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   Xu T, 2015, MOBILE NETW APPL, V20, P320, DOI 10.1007/s11036-015-0596-1
   Yin Z, 2014, 3DTV C TRUE VIS CAPT, P1
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang C, 2013, IEEE SIGNAL PROC LET, V20, P106, DOI 10.1109/LSP.2012.2230165
NR 48
TC 25
Z9 29
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 852
EP 864
DI 10.1109/TMM.2016.2537207
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jing, GM
   Hu, YT
   Guo, YW
   Yu, YZ
   Wang, WP
AF Jing, Guangmei
   Hu, Yongtao
   Guo, Yanwen
   Yu, Yizhou
   Wang, Wenping
TI Content-Aware Video2Comics With Manga-Style Layout
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Comics; layout optimization; video presentation
ID VIDEO; VISUALIZATION; SCENE
AB We introduce in this paper a new approach that conveniently converts conversational videos into comics with manga-style layout. With our approach, the manga-style layout of a comic page is achieved in a content-driven manner, and the main components, including panels and word balloons, that constitute a visually pleasing comic page are intelligently organized. Our approach extracts key frames on speakers by using a speaker detection technique such that word balloons can be placed near the corresponding speakers. We qualitatively measure the information contained in a comic page. With the initial layout automatically determined, the final comic page is obtained by maximizing such a measure and optimizing the parameters relating to the optimal display of comics. An efficient Markov chain Monte Carlo sampling algorithm is designed for the optimization. Our user study demonstrates that users much prefer our manga-style comics to purely Western style comics. Extensive experiments and comparisons against previous work also verify the effectiveness of our approach.
C1 [Jing, Guangmei; Hu, Yongtao; Guo, Yanwen; Yu, Yizhou; Wang, Wenping] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Guo, Yanwen] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
   [Guo, Yanwen] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Chinese University of Hong Kong; Nanjing University; University of
   Illinois System; University of Illinois Urbana-Champaign; Beihang
   University
RP Guo, YW (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM gmjing@cs.hku.hk; ythu@cs.hku.hk; ywguo@nju.edu; yzyu@cs.hku.hk;
   wenping@cs.hku.hk
FU National Natural Science Foundation of China [61373059, 61321491]
FX The work of Y. Guo was supported by the National Natural Science
   Foundation of China under Grant 61373059 and Grant 61321491. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Shahram Shirani. (Corresponding
   author: Yanwen Guo.)
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P584, DOI 10.1145/1015706.1015764
   [Anonymous], 2006, P CVPR
   Barnes C, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778826
   Boreczky J., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P185, DOI 10.1145/332040.332428
   Bousseau A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276507
   Calic J, 2007, IEEE T CIRC SYST VID, V17, P931, DOI 10.1109/TCSVT.2007.897466
   Cao Y, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601183
   Cao Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366160
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen T, 2012, COMPUT GRAPH-UK, V36, P241, DOI 10.1016/j.cag.2012.02.010
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Chun BK, 2006, LECT NOTES COMPUT SC, V4292, P576
   Dollner J., 2009, SHADERX7 ADV RENDERI
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Goldman DB, 2006, ACM T GRAPHIC, V25, P862, DOI 10.1145/1141911.1141967
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Herranz L, 2012, IEEE T MULTIMEDIA, V14, P1290, DOI 10.1109/TMM.2012.2192917
   Hu Y., 2014, ACM T MULTIMEDIA COM, V11, P1
   Hu Y., 2015, P 23 ANN ACM I UNPUB
   Hwang WI, 2006, GRAPP 2006: PROCEEDINGS OF THE FIRST INTERNATIONAL CONFERENCE ON COMPUTER GRAPHICS THEORY AND APPLICATIONS, P299
   Kurlander D., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P225, DOI 10.1145/237170.237260
   Liu Jun S, 2008, MONTE CARLO STRATEGI
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Preu Jacqueline, 2007, P ACM ASS COMP MACH, P99
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Ryu DS, 2008, 8TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION TECHNOLOGY WORKSHOPS: CIT WORKSHOPS 2008, PROCEEDINGS, P336, DOI 10.1109/CIT.2008.Workshops.42
   Shamir A, 2006, IEEE COMPUT GRAPH, V26, P53, DOI 10.1109/MCG.2006.58
   Toyoura M., 2012, P S EYE TRACK RES AP, P373
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Wang J, 2004, ACM T GRAPHIC, V23, P574, DOI 10.1145/1015706.1015763
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P858, DOI 10.1109/TMM.2012.2187181
   Wang T, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1479
   Winnemöller H, 2006, ACM T GRAPHIC, V25, P1221, DOI 10.1145/1141911.1142018
   Winnemoeller H, 2012, COMPUT GRAPH-UK, V36, P740, DOI 10.1016/j.cag.2012.03.004
NR 38
TC 14
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2122
EP 2133
DI 10.1109/TMM.2015.2474263
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500002
DA 2024-07-18
ER

PT J
AU Yu, W
   Yang, KY
   Bai, YL
   Yao, HX
   Rui, Y
AF Yu, Wei
   Yang, Kuiyuan
   Bai, Yalong
   Yao, Hongxun
   Rui, Yong
TI Learning Cross Space Mapping via DNN Using Large Scale Click-Through
   Logs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross space mapping; deep neural network; image retrieval
AB The gap between low-level visual signals and high-level semantics has been progressively bridged by continuous development of deep neural network (DNN). With recent progress of DNN, almost all image classification tasks have achieved new records of accuracy. To extend the ability of DNN to image retrieval tasks, we proposed a unified DNN model for image-query similarity calculation by simultaneously modeling image and query in one network. The unified DNN is named the cross space mapping (CSM) model, which contains two parts, a convolutional part and a query-embedding part. The image and query are mapped to a common vector space via these two parts respectively, and image-query similarity is naturally defined as an inner product of their mappings in the space. To ensure good generalization ability of the DNN, we learn weights of the DNN from a large number of click-through logs which consists of 23 million clicked image-query pairs between 1 million images and 11.7 million queries. Both the qualitative results and quantitative results on an image retrieval evaluation task with 1000 queries demonstrate the superiority of the proposed method.
C1 [Yu, Wei; Yang, Kuiyuan; Rui, Yong] Microsoft Res, Beijing 100080, Peoples R China.
   [Yu, Wei; Bai, Yalong; Yao, Hongxun] Harbin Inst Technol, Harbin 150090, Peoples R China.
C3 Microsoft; Harbin Institute of Technology
RP Yu, W (corresponding author), Harbin Inst Technol, Harbin 150090, Peoples R China.
EM w.yu@hit.edu.cn; kuyang@microsoft.com; ylbai@mtlab.hit.edu.cn;
   h.yao@hit.edu.cn; yongrui@microsoft.com
FU National Natural Science Foundation of China [61472103]; Key Program
   [61133003]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61472103 and by the Key Program under
   Grant 61133003. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Jiebo Luo.
CR [Anonymous], 2004, P 12 ANN ACM INT C M, DOI [10.1145/1027527.1027608, DOI 10.1145/1027527.1027608]
   [Anonymous], P ACM MULT
   [Anonymous], P MSR BING IRC 2013
   [Anonymous], 2015, CORR
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2006, COMPUTER VISION ECCV
   [Anonymous], 2008, Proceedings of the 9th International Workshop on Multimedia Data Mining: held in conjunction with the ACM SIGKDD 2008
   [Anonymous], P ACM MULT IN PRESS
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Carneiro G, 2005, PROC CVPR IEEE, P163
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   Hua X.-S., 2013, Proceedings of the 21st ACM International Conference on Multimedia, Oct. 21-25, ACM, P243
   Ji RR, 2014, IEEE T IMAGE PROCESS, V23, P3099, DOI 10.1109/TIP.2014.2324291
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lu YJ, 2015, IEEE T MULTIMEDIA, V17, P1213, DOI 10.1109/TMM.2015.2438712
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tieu K, 2004, INT J COMPUT VISION, V56, P17, DOI 10.1023/B:VISI.0000004830.93820.78
   Vogel J, 2004, LECT NOTES COMPUT SC, V3115, P207
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Weston J, 2011, IJCAI
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang L, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2490823
   Zhou X, 2010, LECT NOTES COMPUT SC, V6315, P141, DOI 10.1007/978-3-642-15555-0_11
NR 30
TC 5
Z9 5
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 2000
EP 2007
DI 10.1109/TMM.2015.2480340
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, P
   Xiang, W
AF Gao, Pan
   Xiang, Wei
TI Disparity Vector Correction for View Synthesis Prediction-Based 3-D
   Video Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video transmission; disparity vector correction; prediction position
   error; view synthesis prediction
ID MULTIVIEW-VIDEO; PACKET LOSS; ERROR CONCEALMENT; DEPTH; TEXTURE; MODEL
AB View synthesis prediction (VSP) is an important tool for enhancing the coding efficiency in the next-generation three-dimensional (3-D) video systems. However, VSP will lead to prediction position errors when the depth maps are corrupted by packet losses during transmission. In order to mitigate the prediction position errors, a novel disparity vector correction algorithm is proposed in this paper. Firstly, we investigate the relationship between the rendering position errors and the depth errors according to the VSP procedure. The depth map errors due to packet losses are then recursively estimated at the decoder without the use of the error-free reconstructed frames. Finally, based on the estimation of the reconstructed depth errors, the received disparity vectors can be corrected to find the matching synthesized pixels as those used at the encoder, and thereby the view synthesis-based inter-view error propagation can be effectively stopped. Experimental results show that the proposed methods with the estimated and actual depth errors can provide significant improvements in terms of both objective and subjective evaluations.
C1 [Gao, Pan; Xiang, Wei] Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
C3 University of Southern Queensland
RP Gao, P (corresponding author), Univ So Queensland, Sch Mech & Elect Engn, Toowoomba, Qld 4350, Australia.
EM gaopan.1005@gmail.com; Wei.Xiang@usq.edu.au
RI Xiang, Wei/C-6765-2009; Xiang, Weidong/AAA-2883-2020
OI Xiang, Wei/0000-0002-0608-065X
FU Queensland Government's Smart Futures Fellowship; Postgraduate
   Scholarship Program of China Scholarship Council
FX This work was supported in part by the Queensland Government's Smart
   Futures Fellowship, and by the Postgraduate Scholarship Program of China
   Scholarship Council (2013-2014). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Cha
   Zhang.
CR [Anonymous], 1999, Q15I16R1 ITUT VCEG
   [Anonymous], 2013, 16WP3 ITUT SG
   [Anonymous], 2011, JTC1SC29WG11 ISOIEC
   [Anonymous], 2010, JTC1SC29WG11 ISOIEC
   [Anonymous], 2012, 3D ATM REFERENCE SOF
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], P SPIE
   Barot S., 2010, SPRABA9 TEX INSTR
   Domanski M, 2013, IEEE T IMAGE PROCESS, V22, P3517, DOI 10.1109/TIP.2013.2266580
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Gao P, 2014, IEEE T MULTIMEDIA, V16, P1797, DOI 10.1109/TMM.2014.2331013
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   Lee C, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P9, DOI 10.1109/PCS.2012.6213273
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Liu YQ, 2010, IEEE T CIRC SYST VID, V20, P600, DOI 10.1109/TCSVT.2009.2035838
   Macchiavello B., 2012, P C VIS INF PROC COM, V8305, P1
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Martinian E., 2006, PICT COD S, V37, P38
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shimizu S, 2007, IEEE T CIRC SYST VID, V17, P1485, DOI 10.1109/TCSVT.2007.903773
   Doan VH, 2013, IEEE INT SYMP CIRC S, P2900, DOI 10.1109/ISCAS.2013.6572485
   Wan S, 2007, IEEE T IMAGE PROCESS, V16, P1327, DOI 10.1109/TIP.2007.894230
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P728, DOI 10.1109/TMM.2007.893336
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Yea S, 2009, SIGNAL PROCESS-IMAGE, V24, P89, DOI 10.1016/j.image.2008.10.007
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang X, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P302, DOI 10.1109/GlobalSIP.2014.7032127
NR 33
TC 3
Z9 4
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1153
EP 1165
DI 10.1109/TMM.2015.2438711
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000003
DA 2024-07-18
ER

PT J
AU Guo, ZL
   Wang, Y
   Erkip, E
   Panwar, SS
AF Guo, Zhili
   Wang, Yao
   Erkip, Elza
   Panwar, Shivendra S.
TI Wireless Video Multicast With Cooperative and Incremental Transmission
   of Parity Packets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Incremental parity transmission; randomized distributed space time
   coding; user cooperation; video multicast
ID NETWORKS; DIVERSITY; SCHEME; LANS; MAC
AB This paper introduces a novel and efficient approach for user cooperation in wireless video multicast using randomized distributed space time codes (R-DSTC), in which the sender first transmits the source packets, and the sender and receivers that have received all source packets then generate and send the parity packets simultaneously using R-DSTC. As more parity packets are delivered, more receivers can recover all source packets and join the parity packet transmission. Four variations of the proposed systems are considered. The first one requires complete channel information between the sender and all receivers and between all receivers to derive the optimal transmission rates for sending source and parity packets, and employs receiver feedback to determine when to terminate parity transmission. The other three suboptimal systems do not require full channel information and/or receiver feedback, and hence are more feasible in practice. All four versions can support significantly higher video rates and correspondingly higher quality of decoded video, than prior approaches in the literature, which require full channel information but not feedback.
C1 [Guo, Zhili; Wang, Yao; Erkip, Elza; Panwar, Shivendra S.] NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Guo, ZL (corresponding author), NYU, Polytech Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM zg290@nyu.edu; yw523@nyu.edu; ee531@nyu.edu; panwar@catt.poly.edu
RI Panwar, Shivendra S/A-6884-2016; Panwar, Shivendra/K-6473-2019
OI Panwar, Shivendra S/0000-0002-9822-6838; Wang, Yao/0000-0003-3199-3802;
   Erkip, Elza/0000-0001-8718-8648
CR Alay O, 2011, IEEE T MULTIMEDIA, V13, P1127, DOI 10.1109/TMM.2011.2158088
   Alay Ö, 2010, 2010 IEEE 21ST INTERNATIONAL SYMPOSIUM ON PERSONAL INDOOR AND MOBILE RADIO COMMUNICATIONS (PIMRC), P2210, DOI 10.1109/PIMRC.2010.5671677
   Dianati M, 2006, IEEE T VEH TECHNOL, V55, P1032, DOI 10.1109/TVT.2005.863426
   Guan ZY, 2013, IEEE ACM T NETWORK, V21, P1173, DOI 10.1109/TNET.2013.2248020
   Guo Z., 2013, P 5 WORKSH MOB VID N, P19
   IEEE, 2003, 80211G2003 IEEE
   Kim J, 2003, IEEE T IMAGE PROCESS, V12, P121, DOI 10.1109/TIP.2003.809006
   Kuo CH, 2011, IEEE T CIRC SYST VID, V21, P816, DOI 10.1109/TCSVT.2011.2133410
   Laneman JN, 2004, IEEE T INFORM THEORY, V50, P3062, DOI 10.1109/TIT.2004.838089
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Lin TL, 2010, IEEE T IMAGE PROCESS, V19, P722, DOI 10.1109/TIP.2009.2038834
   Liu P, 2012, IEEE T WIREL COMMUN, V11, P1358, DOI 10.1109/TWC.2012.020712.101900
   Liu X, 2009, IEEE T MULTIMEDIA, V11, P730, DOI 10.1109/TMM.2009.2017636
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   Mastronarde N, 2012, IEEE J SEL AREA COMM, V30, P1597, DOI 10.1109/JSAC.2012.121002
   Nafaa A, 2008, IEEE COMMUN MAG, V46, P72, DOI 10.1109/MCOM.2008.4427233
   Nie C, 2013, IEEE T VEH TECHNOL, V62, P1399, DOI 10.1109/TVT.2012.2230281
   Sendonaris A, 2003, IEEE T COMMUN, V51, P1927, DOI 10.1109/TCOMM.2003.818096
   Sirkeci-Mergen B, 2007, IEEE T SIGNAL PROCES, V55, P5003, DOI 10.1109/TSP.2007.896061
   Verde F, 2010, IEEE T COMMUN, V58, P2667, DOI 10.1109/TCOMM.2010.09.080279
NR 21
TC 9
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1335
EP 1346
DI 10.1109/TMM.2015.2438718
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000018
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Dai, Q
   Mei, T
   Rui, Y
   Chang, SF
AF Jiang, Yu-Gang
   Dai, Qi
   Mei, Tao
   Rui, Yong
   Chang, Shih-Fu
TI Super Fast Event Recognition in Internet Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Consumer videos; efficiency; event recognition; Internet videos; real
   time
ID SCALE; CLASSIFICATION; FEATURES; SCENE
AB Techniques for recognizing high-level events in consumer videos on the Internet have many applications. Systems that produced state-of-the-art recognition performance usually contain modules requiring extensive computation, such as the extraction of the temporal motion trajectories, which cannot be deployed on large-scale datasets. In this paper, we provide a comprehensive study on efficient methods in this area and identify technical options for super fast event recognition in Internet videos. We start from analyzing a multimodal baseline that has produced good performance on popular benchmarks, by systematically evaluating each component in terms of both computational cost and contribution to recognition accuracy. After that, we identify alternative features, classifiers, and fusion strategies that can all be efficiently computed. In addition, we also provide a study on the following interesting question: for event recognition in Internet videos, what is the minimum number of visual and audio frames needed to obtain a comparable accuracy to that of using all the frames? Results on two rigorously designed datasets indicate that similar results can be maintained by using only a small portion of the visual frames. We also find that, different from the visual frames, the soundtracks contain little redundant information and thus sampling is always harmful. Integrating all the findings, our suggested recognition system is 2,350-fold faster than a baseline approach with even higher recognition accuracies. It recognizes 20 classes on a 120-second video sequence in just 1.78 seconds, using a regular desktop computer.
C1 [Jiang, Yu-Gang; Dai, Qi] Fudan Univ, Sch Comp Sci, Shanghai 201203, Peoples R China.
   [Mei, Tao; Rui, Yong] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Chang, Shih-Fu] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Chang, Shih-Fu] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
C3 Fudan University; Microsoft Research Asia; Microsoft; Columbia
   University; Columbia University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 201203, Peoples R China.
EM ygj@fudan.edu.cn; daiqi@fudan.edu.cn; tmei@microsoft.com;
   yongrui@microsoft.com; sfchang@ee.columbia.edu
RI Mei, Tao/GQZ-0596-2022; Dai, Qi/JXM-6895-2024
OI Mei, Tao/0000-0002-5990-7307; Dai, Qi/0000-0002-4693-2968
CR [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], E LETT IEEE MULT COM
   [Anonymous], P ACM INT C MULT RET
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], PROC BRIT MACH VIS C, DOI DOI 10.5244/C.24.52.1
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2014, INT C MULTIMEDIA RET, DOI DOI 10.1145/2578726.2578744
   [Anonymous], 2007, CIVR '07
   [Anonymous], TRECVID MULTIMEDIA E
   [Anonymous], 2004, P INT C MACH LEARN
   [Anonymous], ACM INT C MULT RETR
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2014, CORR
   [Anonymous], P ACM INT C MULT RET
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2014, ACMMM
   [Anonymous], 2008, 2008 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2008.4587730
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Bhattacharya S., 2014, P ACM INT C MULT RET, P105
   Brefeld U., 2006, Proceedings of the 23rd international conference on Machine learning, P137, DOI [10.1145/1143844.1143862, DOI 10.1145/1143844.1143862]
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Inoue N, 2013, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2013.156
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee K, 2010, IEEE T AUDIO SPEECH, V18, P1406, DOI 10.1109/TASL.2009.2034776
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S., 2008, CVPR
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Moosmann F, 2008, IEEE T PATTERN ANAL, V30, P1632, DOI 10.1109/TPAMI.2007.70822
   Nister David, 2006, CVPR
   Nowak E, 2006, LECT NOTES COMPUT SC, V3954, P490
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Shotton J, 2008, PROC CVPR IEEE, P1245
   Stein BE, 2008, NAT REV NEUROSCI, V9, P255, DOI 10.1038/nrn2331
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Zhen-Zhong Lan, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8325, P388, DOI 10.1007/978-3-319-04114-8_33
NR 50
TC 39
Z9 39
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1174
EP 1186
DI 10.1109/TMM.2015.2436813
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000005
DA 2024-07-18
ER

PT J
AU Pourian, N
   Manjunath, BS
AF Pourian, Niloufar
   Manjunath, B. S.
TI PixNet: A Localized Feature Representation for Classification and Visual
   Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Community detection; feature extraction; image classification;
   segmentation
ID SPATIAL LAYOUT; SEGMENTATION; RETRIEVAL; IMAGES
AB This paper presents a novel localized visual image feature motivated by image segmentation. The proposed feature embeds relative spatial information by learning different image parts while having a compact representation. First, an attributed graph representation of an image is created based on segmentation and localized image features. Subsequently, communities of image regions are discovered based on their spatial and visual characteristics over all images. The community detection problem is modeled as a spectral graph partitioning problem. This results in finding meaningful image part groupings. A histogram of communities forms a robust and spatially localized representation for each image in the database. Such a region-based representation enables one to search for queries that might not have been possible with global image representations. We apply this representation to image classification and search and retrieval tasks. Extensive experiments on three challenging datasets, including the large-scale ImageNet dataset, demonstrate that the proposed representation achieves promising results compared to the current state-of-the-art methods.
C1 [Pourian, Niloufar; Manjunath, B. S.] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara
RP Pourian, N (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
EM npourian@ece.ucsb.edu; manj@ece.ucsb.edu
RI Manjunath, B S/AAM-8190-2020
OI Manjunath, B S/0000-0003-2804-3611
FU ONR [N00014-12-1-0503]; UCSB
FX Manuscript received June 02, 2014; revised October 20, 2014 and February
   16, 2015; accepted February 21, 2015. Date of publication March 05,
   2015; date of current version April 15, 2015. This work was supported by
   the ONR under Grant N00014-12-1-0503 and by the UCSB Doctoral Scholar
   Fellowship. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Xiao-Ping Zhang.
CR [Anonymous], 2008, 2008 2 INT S SYSTEMS
   [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], 1991, An Introductory Tutorial on kD-Trees
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   BENTLEY JL, 1979, COMPUT SURV, V11, P397, DOI 10.1145/356789.356797
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Berg A., 2010, Large scale visual recognition challenge
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Bradski G, 2000, DR DOBBS J, V25, P120
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Friedman J. H., 1977, ACM T MATH SOFTWARE, V24, P1000
   FRIEDMAN JH, 1975, IEEE T COMPUT, V24, P1000, DOI 10.1109/T-C.1975.224110
   Gaede V, 1998, ACM COMPUT SURV, V30, P170, DOI 10.1145/280277.280279
   Harchaoui Z., 2007, IEEE C COMP VIS PATT, P1, DOI DOI 10.1109/CVPR.2007.383049
   Hastie T., 2009, ELEMENTS STAT LEARNI, P1
   Hegerath Andre, 2006, BMVC, P519
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jagadish HV, 2005, ACM T DATABASE SYST, V30, P364, DOI 10.1145/1071610.1071612
   Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135
   Krapac J, 2011, IEEE I CONF COMP VIS, P1487, DOI 10.1109/ICCV.2011.6126406
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li GD, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2379790.2379797
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Liu Y, 2013, PROC CVPR IEEE, P2075, DOI 10.1109/CVPR.2013.270
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Morioka N, 2010, LECT NOTES COMPUT SC, V6311, P692, DOI 10.1007/978-3-642-15549-9_50
   NIKULIN M.S., 2001, Hellinger Distance
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perronnin F., 2007, P IEEE CVPR, P1
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Salton G, 1986, Introduction to Modern Information Retrieval
   Sánchez J, 2012, PATTERN RECOGN LETT, V33, P2216, DOI 10.1016/j.patrec.2012.07.019
   Sánchez J, 2011, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2011.5995504
   Savarese S., 2006, P 2006 IEEE COMPUTER, V2, P2033, DOI DOI 10.1109/CVPR.2006.102
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Silpa-Anan C., 2008, [39] C. Silpa-Anan and R. Hartley, "Optimized KD-trees for fast image descriptor matching", IEEE Conf. on Computer Vision and Pattern Recognition, 2008, pp. 1-8., P1, DOI [DOI 10.1109/CVPR.2008.4587638, 10.1109/CVPR.2008.4587638]
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smith JR, 1999, COMPUT VIS IMAGE UND, V75, P165, DOI 10.1006/cviu.1999.0771
   Tolias G, 2011, IEEE I CONF COMP VIS, P1653, DOI 10.1109/ICCV.2011.6126427
   Verbeek J., 2007, PROC IEEE C COMPUTER, P1, DOI DOI 10.1109/CVPR.2007.383098
   Vezhnevets A, 2012, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2012.6247757
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vezhnevets A, 2011, IEEE I CONF COMP VIS, P643, DOI 10.1109/ICCV.2011.6126299
   Vezhnevets A, 2010, PROC CVPR IEEE, P3249, DOI 10.1109/CVPR.2010.5540060
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weyand T., 2009, Proc. of British Machine Vision Conference (BMVC), P1
   Xu H, 2011, IEEE I CONF COMP VIS, P1631, DOI 10.1109/ICCV.2011.6126424
   Yang K., 2012, Computer Vision and Pattern Recognition Workshops, P17
   Zhang S., 2011, IEEE International Workshop on Multimedia Signal Processing, P1
   Zhang SL, 2014, COMPUT VIS IMAGE UND, V118, P16, DOI 10.1016/j.cviu.2013.03.008
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhu L, 2010, IEEE T PATTERN ANAL, V32, P1029, DOI 10.1109/TPAMI.2009.65
NR 59
TC 8
Z9 8
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 616
EP 625
DI 10.1109/TMM.2015.2410734
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300004
OA Green Published
DA 2024-07-18
ER

PT J
AU Altintakan, UL
   Yazici, A
AF Altintakan, Umit Lutfu
   Yazici, Adnan
TI Towards Effective Image Classification Using Class-Specific Codebooks
   and Distinctive Local Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag-of-words; class-specific codebooks; distinctive local features;
   image classification; self-organizing maps
ID UNIVERSAL
AB Local image features, which are robust to scale, view, and orientation changes in images, play a key factor in developing effective visual classification systems. However, there are two main limitations to exploit these features in image classification problems: 1) a large number of key-points are located during the feature detection process, and 2) most of the key-points arise in background regions, which do not contribute to the classification process. In order to decrease the inverse effects of these limitations, we propose a new codebook generation approach through employing a new clustering method that generates class-specific codebooks along with a novel feature selection method in the bag-of-words model. We evaluate the performance of different classification techniques including Naive Bayesian, k-NN, and SVM on distinctive features. Experiments conducted on PASCAL Visual Object Classification collections have shown that the class-specific codebooks along with distinctive image features can significantly improve the classification performances.
C1 [Altintakan, Umit Lutfu; Yazici, Adnan] Middle E Tech Univ, Dept Comp Engn, TR-06351 Ankara, Turkey.
C3 Middle East Technical University
RP Altintakan, UL (corresponding author), Middle E Tech Univ, Dept Comp Engn, TR-06351 Ankara, Turkey.
EM umit.altintakan@ceng.metu.edu.tr; yazicig@ceng.metu.edu.tr
OI YAZICI, Adnan/0000-0001-9404-9494
CR [Anonymous], P 6 ACM INT C IM VID
   Avila S, 2013, COMPUT VIS IMAGE UND, V117, P453, DOI 10.1016/j.cviu.2012.09.007
   Bichot C., 2013, COMPUT VIS IMAGE UND, V117, P451
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen T, 2011, IEEE IMAGE PROC, P825, DOI 10.1109/ICIP.2011.6116684
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Garg V., 2011, Proceedings of the 2011 Third National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2011), P37, DOI 10.1109/NCVPRIPG.2011.15
   Ji RR, 2012, IEEE T IMAGE PROCESS, V21, P2282, DOI 10.1109/TIP.2011.2176950
   Kesorn K, 2012, IEEE T MULTIMEDIA, V14, P211, DOI 10.1109/TMM.2011.2170665
   Kinnunen T, 2012, PATTERN RECOGN LETT, V33, P2102, DOI 10.1016/j.patrec.2012.07.013
   Krapac J, 2011, IEEE I CONF COMP VIS, P1487, DOI 10.1109/ICCV.2011.6126406
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mukherjee J, 2014, IEEE STUDENT TECHNOL, P99, DOI 10.1109/TechSym.2014.6807922
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Perronnin F, 2008, IEEE T PATTERN ANAL, V30, P1243, DOI 10.1109/TPAMI.2007.70755
   Platt JC, 2000, ADV NEUR IN, P61
   Rifkin R, 2004, J MACH LEARN RES, V5, P101
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Viitaniemi V, 2008, LECT NOTES COMPUT SC, V5188, P231, DOI 10.1007/978-3-540-85891-1_26
   Villmann T, 1997, IEEE T NEURAL NETWOR, V8, P256, DOI 10.1109/72.557663
   Wei SK, 2013, IEEE T CYBERNETICS, V43, P2216, DOI 10.1109/TCYB.2013.2245890
   Xie Y, 2013, NEUROCOMPUTING, V119, P478, DOI 10.1016/j.neucom.2013.03.004
   Yang Jun, 2009, Proceedings of the 2009 2nd International Congress on Image and Signal Processing (CISP), DOI 10.1109/CISP.2009.5304123
   Zhang SL, 2011, IEEE T IMAGE PROCESS, V20, P2664, DOI 10.1109/TIP.2011.2128333
   Zhang Ya-Ping, 2010, Zoological Research, V31, P1
NR 28
TC 19
Z9 22
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 323
EP 332
DI 10.1109/TMM.2014.2388312
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700005
DA 2024-07-18
ER

PT J
AU Arashloo, SR
   Kittler, J
AF Arashloo, Shervin Rahimzadeh
   Kittler, Josef
TI Dynamic Texture Recognition Using Multiscale Binarized Statistical Image
   Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binarized statistical image features; multiresolution analysis;
   spatio-temporal descriptors; time-varying texture
ID CLASSIFICATION; REPRESENTATION; VIDEO
AB A spatio-temporal descriptor for representation and recognition of time-varying textures is proposed [binarized statistical image features on three orthogonal planes (BSIF-TOP)] in this paper. The descriptor, similar in spirit to the well known local binary patterns on three orthogonal planes approach, estimates histograms of binary coded image sequences on three orthogonal planes corresponding to spatial/spatio-temporal dimensions. However, unlike some other methods which generate the code in a heuristic fashion, binary code generation in the BSIF-TOP approach is realized by filtering operations on different regions of spatial/spatio-temporal support and by binarizing the filter responses. The filters are learnt via independent component analysis on each of three planes after preprocessing using a whitening transformation. By extending the BSIF-TOP descriptor to a multiresolution scheme, the descriptor is able to capture the spatio-temporal content of an image sequence at multiple scales, improving its representation capacity. In the evaluations on the UCLA, Dyntex, and Dyntex ++ dynamic texture databases, the proposed method achieves very good performance compared to existing approaches.
C1 [Arashloo, Shervin Rahimzadeh] Urmia Univ, Dept Elect Engn, Orumiyeh 57135, Iran.
   [Arashloo, Shervin Rahimzadeh; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Dept Elect Engn, Guildford GU2 7XH, Surrey, England.
C3 Urmia University; University of Surrey
RP Arashloo, SR (corresponding author), Urmia Univ, Dept Elect Engn, Orumiyeh 57135, Iran.
EM Sh.Rahimzadeh@hotmail.co.uk; J.Kittler@surrey.ac.uk
RI Arashloo, Shervin Rahimzadeh/A-6381-2019
OI Rahimzadeh Arashloo, Shervin/0000-0003-0189-4774
FU EPSRC/DSTL [EP/K014307/1]; EPSRC [EP/K014307/1, EP/F069421/1] Funding
   Source: UKRI
FX This work was supported in part by EPSRC/DSTL project Signal Processing
   in a Networked Battlespace under Contract EP/K014307/1 and the European
   Union project Beat. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Chia-Wen Lin.
CR Ali W, 2008, IEEE INT VEH SYM, P1144
   [Anonymous], COMP IMAG VIS
   [Anonymous], IEEE T INF IN PRESS
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], SIGNAL IMAGE VIDEO P
   [Anonymous], BIOM THEOR APPL SYST
   [Anonymous], COMPUT IMAG VIS
   [Anonymous], P 6 ACM SIGMM INT WO
   [Anonymous], 2005, WACV MOTION
   Arashloo SR, 2009, LECT NOTES COMPUT SC, V5681, P56, DOI 10.1007/978-3-642-03641-5_5
   Arashloo ShervinRahimzadeh., 2013, BIOMETRICS THEORY AP, P1, DOI DOI 10.1109/BTAS.2013.6712721
   Bar-Joseph Z, 2001, IEEE T VIS COMPUT GR, V7, P120, DOI 10.1109/2945.928165
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bo L., 2011, Neural Information Processing Systems, P2115
   Bruce V., 1996, VISUAL PERCEPTION
   Cannons KJ, 2010, LECT NOTES COMPUT SC, V6314, P511, DOI 10.1007/978-3-642-15561-1_37
   Chan A., 2007, PROC IEEE COMPUT VIS, P1
   Chan AB, 2005, PROC CVPR IEEE, P846
   Chetverikov D, 2005, ADV SOFT COMP, P17
   Derpanis KG, 2012, IEEE T PATTERN ANAL, V34, P1193, DOI 10.1109/TPAMI.2011.221
   Derpanis KG, 2010, LECT NOTES COMPUT SC, V5995, P301
   Derpanis KG, 2009, PROC CVPR IEEE, P232, DOI 10.1109/CVPRW.2009.5206817
   Dollar P., 2005, Proceedings. 2nd Joint IEEE International Workshop on Visual Surveillance and Performance Evaluation of Tracking and Surveillance (VS-PETS) (IEEE Cat. No. 05EX1178), P65
   Doretto G, 2003, INT J COMPUT VISION, V51, P91, DOI 10.1023/A:1021669406132
   Fitzgibbon AW, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P662, DOI 10.1109/ICCV.2001.937584
   Ghanem B., 2007, PROC IEEE 11 INT C C, P1
   Ghanem B., 2010, LECT NOTES COMPUT SC, P223
   Jing Huang, 2010, 2010 Proceedings of 3rd International Congress on Image and Signal Processing (CISP 2010), P1887, DOI 10.1109/CISP.2010.5647609
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Kannala J, 2012, INT C PATT RECOG, P1363
   Kung T., 1988, NATURAL COMPUTATION, P224
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Liao SC, 2010, PROC CVPR IEEE, P1301, DOI 10.1109/CVPR.2010.5539817
   O'Toole AJ, 2002, TRENDS COGN SCI, V6, P261, DOI 10.1016/S1364-6613(02)01908-3
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Péteri R, 2005, LECT NOTES COMPUT SC, V3523, P223
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Rahtu E, 2012, IMAGE VISION COMPUT, V30, P501, DOI 10.1016/j.imavis.2012.04.001
   Ravichandran A, 2009, PROC CVPR IEEE, P1651, DOI 10.1109/CVPRW.2009.5206847
   Saisan P, 2001, PROC CVPR IEEE, P58
   Sizintsev M, 2009, PROC CVPR IEEE, P493, DOI 10.1109/CVPRW.2009.5206728
   Smith JR, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P437
   Szummer M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P823, DOI 10.1109/ICIP.1996.560871
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Taylor Graham W, 2006, ADV NEURAL INFORM PR, V19, P2
   Wang XG, 2009, PROC CVPR IEEE, P142, DOI 10.1109/CVPRW.2009.5206736
   Wang YZ, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P213, DOI 10.1109/ICCV.2003.1238343
   Wildes R., 2000, P EUROPEAN C COMPUTE, P768
   Woolfe F, 2006, LECT NOTES COMPUT SC, V3952, P549
   Xu Y, 2011, IEEE I CONF COMP VIS, P1219, DOI 10.1109/ICCV.2011.6126372
   Yan X, 2014, LECT NOTES COMPUT SC, V8692, P215, DOI 10.1007/978-3-319-10593-2_15
   Yu K, 2011, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2011.5995732
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
NR 55
TC 65
Z9 68
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2099
EP 2109
DI 10.1109/TMM.2014.2362855
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300002
DA 2024-07-18
ER

PT J
AU Cricri, F
   Roininen, MJ
   Leppänen, J
   Mate, S
   Curcio, IDD
   Uhlmann, S
   Gabbouj, M
AF Cricri, Francesco
   Roininen, Mikko J.
   Leppanen, Jussi
   Mate, Sujeet
   Curcio, Igor D. D.
   Uhlmann, Stefan
   Gabbouj, Moncef
TI Sport Type Classification of Mobile Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fusion; mobile; sport; video
AB The recent proliferation of mobile video content has emphasized the need for applications such as automatic organization and automatic editing of videos. These applications could greatly benefit from domain knowledge about the content. However, extracting semantic information from mobile videos is a challenging task, due to their unconstrained nature. We extract domain knowledge about sport events recorded by multiple users, by classifying the sport type into soccer, American football, basketball, tennis, ice-hockey, or volleyball. We adopt a multi-user and multimodal approach, where each user simultaneously captures audio-visual content and auxiliary sensor data (from magnetometers and accelerometers). Firstly, each modality is separately analyzed; then, analysis results are fused for obtaining the sport type. The auxiliary sensor data is used for extracting more discriminative spatio-temporal visual features and efficient camera motion features. The contribution of each modality to the fusion process is adapted according to the quality of the input data. We performed extensive experiments on data collected at public sport events, showing the merits of using different combinations of modalities and fusion methods. The results indicate that analyzing multimodal and multi-user data, coupled with adaptive fusion, improves classification accuracies in most tested cases, up to 95.45%.
C1 [Cricri, Francesco; Roininen, Mikko J.; Uhlmann, Stefan; Gabbouj, Moncef] Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.
   [Leppanen, Jussi; Mate, Sujeet; Curcio, Igor D. D.] Nokia Res Ctr, Tampere 33721, Finland.
C3 Tampere University; Nokia Corporation; Siemens AG; Nokia Siemens
   Networks; Nokia Finland
RP Cricri, F (corresponding author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.
EM francesco.cricri@tut.fi; mikko.roininen@tut.fi;
   jussi.ar.leppanen@nokia.com; sujeet.mate@nokia.com;
   igor.curcio@nokia.com; stefan.uhlmann@tut.fi; moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014
OI Gabbouj, Moncef/0000-0002-9788-2323
CR [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], P BMVA BRIT MACH VIS
   [Anonymous], 2013, PROC IEEE INT C MULT
   [Anonymous], 2004, P 2004WORKSHOP STAT
   [Anonymous], 2009, IEEE INT WORKSHOP MU, DOI DOI 10.1109/MMSP.2009.5293292
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Brezeale D, 2008, IEEE T SYST MAN CY C, V38, P416, DOI 10.1109/TSMCC.2008.919173
   Cricri F, 2014, MULTIMED TOOLS APPL, V70, P119, DOI 10.1007/s11042-012-1085-1
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dong Y, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Guo J, 2012, P IEEE MTTS INT MICR, P1
   Kiranyaz S., 2011, Proceedings 2011 IEEE Workshop on Evolving and Adaptive Intelligent Systems (EAIS 2011), P147, DOI 10.1109/EAIS.2011.5945925
   Kiranyaz S, 2012, NEURAL NETWORKS, V34, P80, DOI 10.1016/j.neunet.2012.07.003
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Min Xu, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P526, DOI 10.1109/MMSP.2008.4665134
   Qian-Cheng Wang, 2010, 2010 International Conference on Machine Learning and Cybernetics (ICMLC 2010), P220, DOI 10.1109/ICMLC.2010.5581062
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Stokes Michael, A Standard Default Color Space for the Internet: sRGB
   Suresh V, 2005, 2005 INTERNATIONAL CONFERENCE ON INTELLIGENT SENSING AND INFORMATION PROCESSING, PROCEEDINGS, P187
   Wang JJ, 2006, INT C PATT RECOG, P778
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   You JY, 2010, SIGNAL PROCESS-IMAGE, V25, P287, DOI 10.1016/j.image.2010.02.001
   Young Steve, 2002, The HTK book
   Yuan X, 2006, IEEE IMAGE PROC, P2905, DOI 10.1109/ICIP.2006.313037
NR 26
TC 21
Z9 21
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 917
EP 932
DI 10.1109/TMM.2014.2307552
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800003
DA 2024-07-18
ER

PT J
AU Wang, LC
   Wang, R
   Kong, DH
   Yin, BC
AF Wang, Li-Chun
   Wang, Ru
   Kong, De-Hui
   Yin, Bao-Cai
TI Similarity Assessment Model for Chinese Sign Language Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Chinese sign language video; human visual system (HVS); sign language
   semantic; video similarity assessment
ID RECOGNITION; EXTRACTION; RETRIEVAL; CLASSIFICATION; ALGORITHM; GESTURES
AB This paper proposes a model for measuring similarity between videos which content is Chinese Sign Language (CSL), vision and sign language semantic are considered for the model. Vision component of the model is distance based on Volume Local Binary Patterns (VLBP), which is robust for motion and illumination. Semantic component of the model computes semantic distance based on definition of sign language semantic, which is defined as hand shape, location, orientation and movements. While quantizing the sign language semantic, contour is used to measure shape and orientation; trajectory is used for measuring location andmovement. Experiment results show that proposed assessment model is effective and assessing result given by the model is close to subjective scoring.
C1 [Wang, Li-Chun; Wang, Ru; Kong, De-Hui; Yin, Bao-Cai] Beijing Univ Technol, Coll Comp Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing, Peoples R China.
C3 Beijing University of Technology
RP Wang, LC (corresponding author), Beijing Univ Technol, Coll Comp Sci, Beijing Key Lab Multimedia & Intelligent Software, Beijing, Peoples R China.
EM wanglc@bjut.edu.cn
FU Natural Science Foundation of China [61227004, 61170104, 61370119,
   61033004, U0935004, 61133003]; Beijing Natural Science Foundation
   [4112008]
FX This paper is supported by Natural Science Foundation of China (No.
   61227004, 61170104, 61370119, 61033004, U0935004, 61133003), Beijing
   Natural Science Foundation (4112008). The authors thank Beijing 3rd
   School for the Deaf, who gives us a great help for Chinese sign's
   videotape. The authors also thank the anonymous reviewers for their
   constructive comments.
CR Alexander RG, 2011, J VISION, V11, DOI 10.1167/11.8.9
   [Anonymous], INT J MACHINE LEARNI
   [Anonymous], P MIR NOV
   [Anonymous], 2008, IEEE INT C AUTOMATIC, DOI [DOI 10.1109/AFGR.2008.4813472, 10.1109/AFGR.2008.4813472]
   [Anonymous], 2010, P 4 WORKSH REPR PROC
   Aran O, 2010, PATTERN RECOGN, V43, P1776, DOI 10.1016/j.patcog.2009.12.002
   Aran O, 2009, PATTERN RECOGN, V42, P812, DOI 10.1016/j.patcog.2008.09.010
   Ardizzone E., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P135, DOI 10.1109/ICPR.1996.546809
   Brashear H., 2006, Proceedings of the 8th international ACM SIGACCESS conference on Computers and Accessibility, Assets '06, P79
   Bungeroth J., 2006, 2nd Workshop on Represent. Processing of Sign Languages, P2000
   Chen FS, 2003, IMAGE VISION COMPUT, V21, P745, DOI 10.1016/S0262-8856(03)00070-2
   Chen Liang-Hua., 2008, P C AUSTRALASIAN DAT, P49
   Chen W. S., 2004, THESIS NATL CHENG KU
   Chrobak M, 2005, ACM T ALGORITHMS, V1, P350, DOI 10.1145/1103963.1103971
   CRUCIANI G, 1992, J CHEMOMETR, V6, P335, DOI 10.1002/cem.1180060604
   Cui Y, 2000, COMPUT VIS IMAGE UND, V78, P157, DOI 10.1006/cviu.2000.0837
   DAVIS J, 1994, IEE P-VIS IMAGE SIGN, V141, P101, DOI 10.1049/ip-vis:19941058
   Efthimiou E, 2007, LECT NOTES COMPUT SC, V4554, P657
   Erol B, 2005, IEEE T MULTIMEDIA, V7, P179, DOI 10.1109/TMM.2004.840607
   Gao Y., 2006, THESIS SHANGHAI JIAO
   Gupta L, 2001, IEEE T SYST MAN CY C, V31, P114, DOI 10.1109/5326.923274
   GUPTA L, 1995, PATTERN RECOGN, V28, P1587, DOI 10.1016/0031-3203(94)00023-F
   Hienz H, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P323, DOI 10.1109/AFGR.1996.557285
   Huang CL, 2001, MACH VISION APPL, V12, P243, DOI 10.1007/s001380050144
   Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022
   Kadous M W., 1996, P WORKSHOP INTEGRATI, P165
   Kelm P, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P25, DOI 10.1109/WIAMIS.2009.5031423
   Kim C, 2005, IEEE T CIRC SYST VID, V15, P127
   Koizumi Atsuko, 2002, P 3 INT C LANG RES E, P927
   Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Li J. L., 2008, J COMPUTAT INF SYST, V4, P1585
   Li Qingyong, 2008, Journal of Computer Aided Design & Computer Graphics, V20, P499
   Li Y, 2012, IEEE T BIO-MED ENG, V59, P2695, DOI 10.1109/TBME.2012.2190734
   Li Z., 2008, Proceedings of the 16th ACM international conference on Multimedia, P671
   Liang RH, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P558, DOI 10.1109/AFGR.1998.671007
   Liu Che-Bin., 2004, ACM Multimedia, P288
   Liu TM, 2003, IEEE T CIRC SYST VID, V13, P1006, DOI 10.1109/TCSVT.2003.816521
   Liu Xiaoming., 1999, P ACM INT C MULTIMED, P41, DOI [10.1145/319878.319889, DOI 10.1145/319878.319889]
   Lu S., 1998, Gesture and Sign Language in Human-Computer Interaction. International Gesture Workshop Proceedings, P259
   Luo W. W., 2006, THESIS BEIJING NORMA
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   Peng Yuxin., 2004, ACM SIGMM international workshop on Multimedia information retrieval, P53
   Psarrou A, 2002, IMAGE VISION COMPUT, V20, P349, DOI 10.1016/S0262-8856(02)00007-0
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Ren W, 2004, INT C PATT RECOG, P834, DOI 10.1109/ICPR.2004.1334657
   Richarz J, 2011, J AMB INTEL SMART EN, V3, P193, DOI 10.3233/AIS-2011-0109
   Rüegg J, 2013, COMPUT GRAPH FORUM, V32, P51, DOI 10.1111/cgf.12025
   Sarkar S., 2010, HDB PATTERN RECOGNIT, P1
   Schembri A., 2008, DEAFNESS COGNITION L
   Seshadrinathan K, 2007, INT CONF ACOUST SPEE, P869
   Shamaie A, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P602, DOI 10.1109/AFGR.2004.1301599
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Solina F, 2001, DESIGN AND MANAGEMENT OF MULTIMEDIA INFORMATION SYSTEMS: OPPORTUNITIES AND CHALLENGES, P268
   Starner T. E., 1997, MOTION BASED RECOGNI, P2279
   STOKOE WC, 1980, ANNU REV ANTHROPOL, V9, P365, DOI 10.1146/annurev.an.09.100180.002053
   Sun ZH, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P196, DOI 10.1109/IIH-MSP.2008.245
   Sze KW, 2005, IEEE T CIRC SYST VID, V15, P1148, DOI 10.1109/TCSVT.2005.852623
   Tan Y., P INT C IM PROC, V1999, P106
   Toguro M, 2005, INT CONF ACOUST SPEE, P445
   Von Agris U., 2007, Gesture in Human-Computer Interaction and Simulation
   Waldron M. B., 1995, IEEE Transactions on Rehabilitation Engineering, V3, P261, DOI 10.1109/86.413199
   Wang Q, 2007, COMPUT VIS IMAGE UND, V108, P87, DOI 10.1016/j.cviu.2006.11.009
   Wang R, 2011, CHINA COMMUN, V8, P139
   Wang Y, 2012, IEEE T CIRC SYST VID, V22, P989, DOI 10.1109/TCSVT.2012.2186745
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2001, THESIS U TEXAS AUSTI
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   Wei W, 2007, CAR C SECUR, P320
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P1061, DOI 10.1109/TPAMI.2002.1023803
   Yi Wu, 2000, Proceedings ACM Multimedia 2000, P465, DOI 10.1145/354384.376380
   Zaki MM, 2011, PATTERN RECOGN LETT, V32, P572, DOI 10.1016/j.patrec.2010.11.013
   Zhang J., 2008, THESIS NANKAI U TIAN
   Zhang Zhi-Yong, 2008, Journal of Software, V19, P2461, DOI 10.3724/SP.J.1001.2008.02461
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhu J. P., 2012, APPL MULTIVARIATE ST
NR 78
TC 24
Z9 25
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 751
EP 761
DI 10.1109/TMM.2014.2298382
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500015
DA 2024-07-18
ER

PT J
AU Park, H
   Lee, H
   Sull, S
AF Park, Hanje
   Lee, Hoonjae
   Sull, Sanghoon
TI Efficient Viewer-Centric Depth Adjustment Based on Virtual
   Fronto-Parallel Planar Projection in Stereo 3D Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D stereo; depth adjustment/control; disocclusion; disparity; parallax;
   shape distortion; viewer-centric; virtual fronto-parallel planar
   projection
ID VISUAL COMFORT
AB This paper presents an efficient method for adjusting the 3D depth of an object including as much as a whole scene in stereo 3D images by utilizing a virtual fronto-parallel planar projection in the 3D space perceived by a viewer. The proposed method just needs to establish object correspondence instead of the accurate estimation of the disparity field or point correspondence. We simulate the depth adjustment of a 3D point perceived by a viewer through a corresponding pair of points on the stereo 3D images by moving the virtual fronto-parallel plane on which the left and right points are projected. We show that the resulting transformation of image coordinates of the points can be simply expressed by three values of a scale factor and two translations that depend on one parameter for the depth adjustment. The experimental results demonstrate the feasibility of the proposed approach that yields less visual fatigue and smaller 3D shape distortion than the conventional parallax adjustment method. The overall procedure can be efficiently applied to each frame of a stereo video without causing any artifact.
C1 [Park, Hanje; Lee, Hoonjae; Sull, Sanghoon] Korea Univ, Sch Elect Engn, Seoul 136701, South Korea.
C3 Korea University
RP Sull, S (corresponding author), Korea Univ, Sch Elect Engn, Seoul 136701, South Korea.
EM hjpark@mpeg.korea.ac.kr; hoonjae@mpeg.korea.ac.kr; sull@korea.ac.kr
FU Seoul RAMP;BD Program project [ST100076]; Korea University Grant
FX This work was supported in part by Seoul R&BD Program project
   (ST100076), and in part by a Korea University Grant. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Klara Nahrstedt.
CR [Anonymous], 2002, METH SUBJ ASS QUAL T
   [Anonymous], P INT C COMP VIS OCT
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], P IEEE INT C CONS EL
   [Anonymous], P SPIE APPL DIGITAL
   [Anonymous], 2012, Computer vision: models, learning, and inference
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brown MZ, 2003, IEEE T PATTERN ANAL, V25, P993, DOI 10.1109/TPAMI.2003.1217603
   Chang NL, 1997, IEEE T IMAGE PROCESS, V6, P584, DOI 10.1109/83.563323
   Chin-Hong Sin, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P801, DOI 10.1109/ICCVW.2009.5457620
   Hanje Park, 2011, Proceedings of the 1st IEEE First International Conference on Consumer Electronics - Berlin (IEEE ICCE-Berlin 2011), P10, DOI 10.1109/ICCE-Berlin.2011.6031870
   Kim D, 2011, IEEE T CIRC SYST VID, V21, P231, DOI 10.1109/TCSVT.2011.2106275
   Kim D, 2010, IEEE INT CON MULTI, P956, DOI 10.1109/ICME.2010.5583082
   Konrad J, 1999, P SOC PHOTO-OPT INS, V3639, P179, DOI 10.1117/12.349379
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   McVeigh JS, 1996, P SOC PHOTO-OPT INS, V2657, P307, DOI 10.1117/12.238727
   Park H, 2012, IEEE IMAGE PROC, P1589, DOI 10.1109/ICIP.2012.6467178
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Speranza F., 2006, P SOC PHOTO-OPT INS, V6055, P94
   Vasudevan R, 2011, IEEE T MULTIMEDIA, V13, P573, DOI 10.1109/TMM.2011.2123871
   Wang Z. F., 2008, PROC IEEE INT C COMP, P1
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Yano S, 2002, DISPLAYS, V23, P191, DOI 10.1016/S0141-9382(02)00038-0
NR 24
TC 18
Z9 21
U1 2
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 326
EP 336
DI 10.1109/TMM.2013.2286567
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800004
DA 2024-07-18
ER

PT J
AU Siddique, MAR
   Kamruzzaman, J
   Hossain, MJ
AF Siddique, Md. Atiur Rahman
   Kamruzzaman, Joarder
   Hossain, Md. Jahangir
TI An Analytical Approach for Voice Capacity Estimation Over WiFi Network
   Using ITU-T E-Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Call capacity; conversational speech quality; E-model; voice over IP
ID WIRELESS LAN; IEEE-802.11; PERFORMANCE; CHANNEL
AB To ensure customer satisfaction and greater market acceptance, voice over Wi-Fi networks must ensure voice quality under various network parameters, configurations and traffic conditions, and other practical effects, e. g., channel noise, and capturing effects. An accurate voice capacity estimation model considering these factors can greatly assist network designers. In the current work, we propose an analytical model to estimate voice over Internet Protocol (VoIP) capacity over Wi-Fi networks addressing these issues. We employ widely used ITU-T E-model to assess voice quality and VoIP call capacity is presented in the form of an optimization problem with voice quality requirement as a constraint. In particular, we analyze delay and loss in channel access and queue, and their impacts on voice quality. The proposed capacity model is first developed for a single hop wireless local area network (WLAN) and then extended for multihop scenarios. To model real network scenario closely, we also consider channel noise and capture effect, and analyze the impacts of transmission range, interference range, and WLAN radius. In absence of any existing call capacity model that considers all the above factors concomitantly, our proposed model will be extremely useful to network designers and voice capacity planners.
C1 [Siddique, Md. Atiur Rahman] Australian Govt, Bur Meteorol, Melbourne, Vic 3008, Australia.
   [Kamruzzaman, Joarder] Monash Univ, Fac IT, Churchill 3842, Australia.
   [Hossain, Md. Jahangir] Univ British Columbia, Sch Engn, Kelowna, BC V1V 1V7, Canada.
C3 Bureau of Meteorology - Australia; Monash University; University of
   British Columbia
RP Siddique, MAR (corresponding author), Australian Govt, Bur Meteorol, Melbourne, Vic 3008, Australia.
EM atiurrs@gmail.com; joarder.kamruzzaman@monash.edu;
   jahangir.hossain@ubc.ca
OI Kamruzzaman, Joarder/0000-0002-3748-0277
CR [Anonymous], 1996, Methods for Subjective Determination of Transmission Quality
   [Anonymous], 1998, E MODEL COMPUTATIONA
   [Anonymous], 2011, VOIP MARKET GROWTH T
   [Anonymous], 2010, 2010 IEEE GLOBAL TEL
   [Anonymous], [No title captured]
   [Anonymous], 2005, E MODEL COMPUTATIONA
   BBC, 2005, EBAY BUY SKYP 2 6BN
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Chatzimisios P, 2003, ELECTRON LETT, V39, P1358, DOI 10.1049/el:20030868
   Chatzimisios R, 2003, ELECTRON LETT, V39, P1687, DOI 10.1049/el:20031081
   Cole RG, 2001, ACM SIGCOMM COMP COM, V31, P9, DOI 10.1145/505666.505669
   Cucurull J, 2010, LECT NOTES COMPUT SC, V6307, P339, DOI 10.1007/978-3-642-15512-3_18
   Daneshgaran F, 2008, IEEE T WIREL COMMUN, V7, P1276, DOI 10.1109/TWC.2008.060859
   Garg S, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P779
   Gross Donald, 2008, Simple Markovian Queueing Models, DOI [10.1002/9781118625651.ch2, DOI 10.1002/9781118625651.CH2]
   Guha S., 2006, P IPTPS SANT BARB CA
   Gupta P, 2000, IEEE T INFORM THEORY, V46, P388, DOI 10.1109/18.825799
   Hariri N, 2011, IEEE T INSTRUM MEAS, V60, P1594, DOI 10.1109/TIM.2010.2092871
   Hole DP, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P196, DOI 10.1109/ICC.2004.1312479
   I. ITU-T, 2003, ONE WAY TRANSMISSION, V114, P84
   Kashyap A, 2007, IEEE INFOCOM SER, P2036, DOI 10.1109/INFCOM.2007.236
   Lee S, 2004, IEEE INFOCOM SER, P894
   Maeda Y, 1999, IEICE T COMMUN, VE82B, P1677
   Martinez FJ, 2010, IEEE INTEL TRANSP SY, V2, P6, DOI 10.1109/MITS.2010.938166
   Perceptual Evaluation of Speech Quality (PESQ), 2001, OBJ METH END TO END, V862
   Sarikaya Y, 2012, AD HOC NETW, V10, P1058, DOI 10.1016/j.adhoc.2012.02.001
   Shin SH, 2009, IEEE T MOBILE COMPUT, V8, P1265, DOI 10.1109/TMC.2009.49
   Shoa-Yei Yeong, 2010, Proceedings of the 2010 Second International Conference on Network Applications Protocols and Services (NETAPPS 2010), P165, DOI 10.1109/NETAPPS.2010.36
   Siddique M. A. R., 2008, P IEEE GLOBECOM, P1
   Smith M., 2004, ELECTROMAGNETIC ENV
   Stoeckigt KO, 2010, IEEE T VEH TECHNOL, V59, P4553, DOI 10.1109/TVT.2010.2068318
   Ullah I., 2011, P INT C IMAGE PROCES, P1
   Wu CC, 2009, NOSSDAV 09: 18TH INTERNATIONAL WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO, P97
   Wu HT, 2002, IEEE INFOCOM SER, P599, DOI 10.1109/INFCOM.2002.1019305
NR 34
TC 4
Z9 4
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 360
EP 372
DI 10.1109/TMM.2013.2291212
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800007
DA 2024-07-18
ER

PT J
AU Chen, PC
   Lin, KS
   Chen, HH
AF Chen, Pei-Chun
   Lin, Keng-Sheng
   Chen, Homer H.
TI Emotional Accompaniment Generation System Based on Harmonic Progression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Accompaniment; harmonic progression; melody; music emotion; onset rate
ID MUSIC
AB A music piece consists of melody and accompaniment in many genres. In this paper, we present a system to automatically generate accompaniment that evokes specific emotions for a given melody. In particular, we propose harmonic progression and onset rate as two key features for emotion-based accompaniment generation. The former refers to the progression of chords, and the latter refers to the number of music events (such as notes and drums) in a unit time. The harmonic progression and the onset rate are altered according to the specified emotion represented by the valence and arousal parameters, respectively. The performance of the system is evaluated subjectively, and the result shows a perfect positive Spearman correlation between the specified emotion and the perceived emotion.
C1 [Chen, Pei-Chun] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Lin, Keng-Sheng] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 Stanford University; National Taiwan University; National Taiwan
   University; National Taiwan University
RP Chen, PC (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM heatherchen1003@gmail.com; pridek0912@gmail.com; homer@cc.ee.ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
FU National Science Council of Taiwan [NSC100-2221-E-002-198-MY3,
   NSC101-2622-E-002-006-CC2]; National Taiwan University [101R7609-2,
   10R80919-5]
FX This work was supported by grants from the National Science Council of
   Taiwan under Contracts NSC100-2221-E-002-198-MY3 and
   NSC101-2622-E-002-006-CC2 and from the National Taiwan University under
   Contracts 101R7609-2 and 10R80919-5. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Eckehard G. Steinbach.
CR Aldwell E., 1989, HARMONY VOICE LEADIN, P102
   [Anonymous], 2008, DAFX 08 ESPOO FINLAN
   [Anonymous], 2011, Music Emotion Recognition
   Becchetti C., 1999, SPEECH RECOGNITION T
   Benward B., 1989, MUSIC THEORY PRACTIC, V1
   Besson M, 2001, ANN NY ACAD SCI, V930, P232, DOI 10.1111/j.1749-6632.2001.tb05736.x
   Blume F., 1970, CLASSIC ROMANTIC MUS, P53
   Burdette Glenn., 1989, MUSIC RES FORUM, V4, P1
   Buxton W., 1986, ACM SIGCHI Bulletin, V17, P41
   Chuang Chia Cheng, 2007, TENCON 2007 2007 IEE, P1
   Cont A., 2007, ISMIR VIENN OCT, P1
   Dahlhaus C., 2001, The new grove dictionary of music and musicians, V2nd, P858
   DANNENBERG R, 1985, P 1984 INT COMP MUS, P193
   Doucleff M., 2012, WALL STREET J
   Eerola T., 2004, MIDI TOOLBOX MATLAB, P8
   FERRAND M, 2003, P 14 CIM 2003 FIR, P95
   Gabrielsson A., 2001, Music And Emotion, P223, DOI DOI 10.1525/MP.2004.21.4.561
   Gomez P, 2007, EMOTION, V7, P377, DOI 10.1037/1528-3542.7.2.377
   Hui Cheng, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563177
   Jelinek F., 1999, STAT METHODS SPEECH
   Jordanous A, 2009, J NEW MUSIC RES, V38, P197, DOI 10.1080/09298210903180245
   Juslin P., 2001, MUSIC EMOTION THEORY, P45
   Juslin PN, 2004, J NEW MUSIC RES, V33, P217, DOI 10.1080/0929821042000317813
   Krumhansl CL, 2000, PSYCHOL BULL, V126, P159, DOI 10.1037/0033-2909.126.1.159
   Laurier C., 2009, 7 ESCOM, P1
   Madgazin V. R., 2009, THESIS CORNELL U ITH, P1
   Maritz J. S., 1981, DISTRIBUTION FREE ST, P216
   Melucci M., 2002, JCDL 2002. Proceedings of the Second ACM/IEEE-CS Joint Conference on Digital Libraries, P310, DOI 10.1145/544220.544294
   Meyer L., 1956, EMOTION MEANING MUSI, P25
   Morris D., 2008, P AAAI CHIC JUL, P1
   Myers J. L., 2003, RES DESIGN STAT ANAL, P506
   Oliveira AP, 2010, KNOWL-BASED SYST, V23, P901, DOI 10.1016/j.knosys.2010.06.006
   Parncutt R., 1989, HARMONY PSYCHOACOUST, V19
   Puckette M., 1992, P ICMC SAN JOS, P199
   Raphael C., 1999, J COMPUT GRAPH STAT, V10, P487
   Schubert E, 2004, MUSIC PERCEPT, V21, P561, DOI 10.1525/mp.2004.21.4.561
   Simon I., 2008, P CHI, P1
   Smit C., 2010, KARAOKE MIDI TOOLBOX
   STAMBAUGH J, 1964, J PHILOS, V61, P265, DOI 10.2307/2022918
   Tagg P., 2009, ENCY POPULAR MUSIC W, P1
   Thayer R. E., 1989, BIOPSYCHOLOGY MOOD A
   Wöllmer M, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P597
   Yang YH, 2008, IEEE T AUDIO SPEECH, V16, P448, DOI 10.1109/TASL.2007.911513
   Yang YH, 2011, IEEE T AUDIO SPEECH, V19, P762, DOI 10.1109/TASL.2010.2064164
NR 44
TC 4
Z9 5
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1469
EP 1479
DI 10.1109/TMM.2013.2267206
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800001
DA 2024-07-18
ER

PT J
AU Chen, YY
   Cheng, AJ
   Hsu, WH
AF Chen, Yan-Ying
   Cheng, An-Jung
   Hsu, Winston H.
TI Travel Recommendation by Mining People Attributes and Travel Group Types
   From Community-Contributed Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo-tagged photos; people attributes; travel groups; travel
   recommendation
AB Leveraging community-contributed data (e. g., blogs, GPS logs, and geo-tagged photos) for personalized recommendation is one of the active research problems since there are rich contexts and human activities in such explosively growing data. In this work, we focus on personalized travel recommendation and show promising applications by leveraging the freely available community-contributed photos. We propose to conduct personalized travel recommendation by further considering specific user profiles or attributes (e. g., gender, age, race) as well as travel group types (e. g., family, friends, couple). Instead of mining photo logs only, we exploit the automatically detected people attributes and travel group types in the photo contents. By information-theoretic measures, we demonstrate that such detected user profiles are informative and effective for travel recommendation-especially providing a promising aspect for personalization. We effectively mine the demographics of individual and group travelers for different locations (or landmarks) and their travel paths. A probabilistic Bayesian learning framework which further entails mobile recommendation on the spot is introduced as well. We experiment on more than 10 million photos collected from 19 major cities worldwide and conduct the extensive investigation of profiling activities in communities according to temporal and spatial information. Note that the photos in the paper attribute to various Flickr users under the Creative Commons License. The experiments confirm that people attributes of individuals and groups are promising and orthogonal to prior works using travel logs only and can further improve prior travel recommendation methods especially for difficult predictions by further leveraging user contexts via mobile devices.
C1 [Chen, Yan-Ying; Cheng, An-Jung; Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University
RP Chen, YY (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
EM yanying@cmlab.csie.ntu.edu.tw; anon@cmlab.csie.ntu.edu.tw;
   winston@csie.ntu.edu.tw
FU National Science Council of Taiwan [NSC 101-2622-E-002-006-CC2];
   Excellent Research Projects of National Taiwan University [AE00-00-05];
   Microsoft Research Asia [FY12-RES-THEME-122]
FX Manuscript received September 07, 2012; revised January 10, 2013;
   accepted April 04, 2013. Date of publication May 29, 2013; date of
   current version September 13, 2013. This work was supported in part by
   grants from the National Science Council of Taiwan, under Contracts NSC
   101-2622-E-002-006-CC2, Excellent Research Projects of National Taiwan
   University (AE00-00-05), and Microsoft Research Asia
   (FY12-RES-THEME-122). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Daniel
   Gatica-Perez.
CR AMES M, 2007, P ACM SIGCHI C HUM F
   [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2010, P EUR C COMP VIS
   Arase Y., 2010, P ACM INT C MULT
   Berger J. O., 1985, STAT DECISION THEORY, DOI DOI 10.1007/978-1-4757-4286-2
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Cheng A.-J., 2010, P ACM INT C MULT
   Clements M., 2010, P ACM SIGIR C RES DE
   Gallagher A., 2009, P IEEE C COMP VIS PA
   Gao Y., 2010, P ACM INT C MULT
   Hao Q., 2010, P INT C WORLD WID WE
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Ji R., 2009, P ACM INT C MULT
   Kumar N., 2008, P EUR C COMP VIS
   Li B., 2011, P 20 INT C WORLD WID
   Li Y, 2009, IEEE INTERNATIONAL CONFERENCE ON MICROWAVES, COMMUNICATIONS, ANTENNAS AND ELECTRONICS SYSTEMS (COMCAS 2009)
   Liu D., 2009, P ACM INT C MULT
   Lu X., 2010, P ACM INT C MULT
   Mei T, 2010, IEEE MULTIMEDIA, V17, P16, DOI 10.1109/MMUL.2010.82
   Singh V. K., 2010, P ACM INT C MULT
   Thomas T. M.CoverandJ. A., 1991, ELEMENTS INFORM THEO, DOI 10.1002/0471200611
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Zheng Y., 2009, WWW, P791, DOI [10.1145/1526709.1526816, DOI 10.1145/1526709.1526816]
   Zheng Y, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1889681.1889683
   Zheng Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921596
NR 26
TC 85
Z9 96
U1 5
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1283
EP 1295
DI 10.1109/TMM.2013.2265077
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400006
DA 2024-07-18
ER

PT J
AU Guo, ZY
   Wang, ZJ
AF Guo, Zhenyu
   Wang, Z. Jane
TI An Unsupervised Hierarchical Feature Learning Framework for One-Shot
   Image Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep structure; Dirichlet process; feature combination; hierarchical
   feature learning; object recognition; pyramid matching
ID OBJECT CATEGORIZATION
AB One-shot recognition has attracted increasing attention recently, inspired by the fact that human cognitive systems could perform recognition tasks well provided only one or a few labeled training samples, in contrast to the conventional object recognition systems that require a large number of labeled training images. One-shot recognition is a visual classification task, where only one training sample is available for each object category in the target test domain, with the help of prior-knowledge data from the source domain. In this paper, we tackle this challenging one-shot recognition problem under a more exciting setting by using only unlabeled images as prior knowledge, which requires less labeling effort than previous works which adopt fully labeled data and/or a sophisticated attribute table designed by human experts. We propose a novel unsupervised hierarchical feature learning framework to learn a feature pyramid from the prior-knowledge domain. The proposed feature learning method also could be applied across multiple feature spaces. Furthermore, we propose using pyramid-matching kernels to combine multilevel features. Examining the "Animals with Attributes" and Caltech-4 data sets in our one-shot recognition setting, we show that the proposed unsupervised feature learning approach with very limited information could achieve comparable performance to that of supervised ones.
C1 [Guo, Zhenyu; Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V5Z 1M9, Canada.
C3 University of British Columbia
RP Guo, ZY (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V5Z 1M9, Canada.
EM zhenyug@ece.ubc.ca; zjanew@ece.ubc.ca
FU Canadian Natural Sciences and Engineering Research Council (NSERC)
   [STPGP 365164-08, 11R82396]
FX This work was supported by the Canadian Natural Sciences and Engineering
   Research Council (NSERC) under Grant STPGP 365164-08 and Grant 11R82396.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Francesco G. B. De Natale.
CR [Anonymous], P INT ACM SIGIR C RE
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], ADV NEURAL INFORM PR
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bennett KP, 1999, ADV NEUR IN, V11, P368
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Davis J. V., 2007, ICML, P209
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Gammerman A., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P148
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Ji Y, 2009, IEEE IMAGE PROC, P317, DOI 10.1109/ICIP.2009.5414507
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Maji S., 2008, PROC IEEE C COMPUT V, P1
   Nowak E, 2007, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2007.382969
   Raina R., 2007, P 24 INT C MACH LEAR, P759
   Shechtman E., 2007, PROC IEEE C COMPUT V, P1
   Tang KD, 2010, PROC CVPR IEEE, P3027, DOI 10.1109/CVPR.2010.5540053
   Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302
   van der Maaten L. J. P., 2008, Journal of Machine Learning Research, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   Yu XD, 2010, LECT NOTES COMPUT SC, V6315, P127
   Zhenyu Guo, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P1, DOI 10.1109/PSIVT.2010.8
NR 28
TC 22
Z9 24
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 621
EP 632
DI 10.1109/TMM.2012.2234729
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900013
DA 2024-07-18
ER

PT J
AU Yang, Y
   Song, JK
   Huang, Z
   Ma, ZG
   Sebe, N
   Hauptmann, AG
AF Yang, Yi
   Song, Jingkuan
   Huang, Zi
   Ma, Zhigang
   Sebe, Nicu
   Hauptmann, Alexander G.
TI Multi-Feature Fusion via Hierarchical Regression for Multimedia Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; multiple feature fusion; semi-supervised learning;
   video concept annotation
ID SEMANTIC GAP; RETRIEVAL; FRAMEWORK
AB Multimedia data are usually represented by multiple features. In this paper, we propose a new algorithm, namely Multi-feature Learning via Hierarchical Regression for multimedia semantics understanding, where two issues are considered. First, labeling large amount of training data is labor-intensive. It is meaningful to effectively leverage unlabeled data to facilitate multimedia semantics understanding. Second, given that multimedia data can be represented by multiple features, it is advantageous to develop an algorithm which combines evidence obtained from different features to infer reliable multimedia semantic concept classifiers. We design a hierarchical regression model to exploit the information derived from each type of feature, which is then collaboratively fused to obtain a multimedia semantic concept classifier. Both label information and data distribution of different features representing multimedia data are considered. The algorithm can be applied to a wide range of multimedia applications and experiments are conducted on video data for video concept annotation and action recognition. Using Trecvid and Care Media video datasets, the experimental results show that it is beneficial to combine multiple features. The performance of the proposed algorithm is remarkable when only a small amount of labeled training data are available.
C1 [Yang, Yi; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Song, Jingkuan; Huang, Zi] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Ma, Zhigang; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
C3 Carnegie Mellon University; University of Queensland; University of
   Trento
RP Yang, Y (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
RI yang, yang/GVT-5210-2022; yang, yang/GWB-9426-2022; Ma,
   Zhigang/H-3543-2015; Lang, Ming/HIK-0758-2022; Yang, Yi/B-9273-2017;
   Sebe, Niculae/KEC-2000-2024; yang, yang/HGT-7999-2022
OI Yang, Yi/0000-0002-0512-880X; Sebe, Niculae/0000-0002-6597-7248; HUANG,
   ZI/0000-0002-9738-4949
FU National Science Foundation [IIS-0812465, IIS-0917072]; National
   Institutes of Health (NIH) [1RC1MH090021-01]; European Commission
   [FP7-248984 GLOCAL]; Div Of Information & Intelligent Systems; Direct
   For Computer & Info Scie & Enginr [0917072] Funding Source: National
   Science Foundation
FX This work is supported in part by the National Science Foundation under
   Grants IIS-0812465 and IIS-0917072, the National Institutes of Health
   (NIH) under Grant 1RC1MH090021-01, and the European Commission under
   Contract FP7-248984 GLOCAL. The associate editor coordinating the review
   of this manuscript and approving it for publication was Daniel
   Gatica-Perez.
CR Amir A., 2005, P IBM RES TRECVID 20, P1
   [Anonymous], P ICCV
   [Anonymous], 2009, MOSIFT RECOGNIZING H
   [Anonymous], 2010, P INT C MULT
   [Anonymous], 2007, COLUMBIA U BASELINE
   [Anonymous], 2010, SDM
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cohen I, 2004, IEEE T PATTERN ANAL, V26, P1553, DOI 10.1109/TPAMI.2004.127
   Farquhar JDR, 2005, ADV NEURAL INFORM PR, V18, P355, DOI DOI 10.5555/2976248.2976293
   Goh K., 2006, IEEE T KNOWL DATA EN, V12, P2399
   Hauptmann AG, 2005, LECT NOTES COMPUT SC, V3568, P1
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Ji SW, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754431
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Lan ZZ, 2012, LECT NOTES COMPUT SC, V7131, P173
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lin C., 2003, P TRECVID WORKSH GAI
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma H, 2010, IEEE T MULTIMEDIA, V12, P462, DOI 10.1109/TMM.2010.2051360
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Mylonas P, 2009, IEEE T MULTIMEDIA, V11, P229, DOI 10.1109/TMM.2008.2009681
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   Rong Yan, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P324
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Vinokourov A., 2003, P NIPS, P339
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Yan R., 2003, P 11 ACM INT C MULT, V3, P339, DOI DOI 10.1145/957013.957086
   Yang Y, 2009, PR ELECTROMAGN RES S, P311, DOI 10.1145/1631272.1631316
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang Yi., 2009, Proceedings of the 17th ACM international conference on Multimedia, P175
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu X., 2006, SEMISUPERVISED LIT S
NR 38
TC 145
Z9 153
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 572
EP 581
DI 10.1109/TMM.2012.2234731
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900009
DA 2024-07-18
ER

PT J
AU Boland, D
   Constantinides, GA
AF Boland, David
   Constantinides, George A.
TI A Scalable Precision Analysis Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer-aided Engineering; Digital Arithmetic; Numerical analysis;
   Reconfigurable Architectures
ID WORD-LENGTH OPTIMIZATION; FLOATING-POINT; FPGA; ALGORITHM
AB In embedded computing, typically some form of silicon area or power budget restricts the potential performance achievable. For algorithms with limited dynamic range, custom hardware accelerators manage to extract significant additional performance for such a budget via mapping operations in the algorithm to fixed-point. However, for complex applications requiring floating-point computation, the potential performance improvement over software is reduced. Nonetheless, custom hardware can still customize the precision of floating-point operators, unlike software which is restricted to IEEE standard single or double precision, to increase the overall performance at the cost of increasing the error observed in the final computational result. Unfortunately, because it is difficult to determine if this error increase is tolerable, this task is rarely performed.
   We present a new analytical technique to calculate bounds on the range or relative error of output variables, enabling custom hardware accelerators to be tolerant of floating point errors by design. In contrast to existing tools that perform this task, our approach scales to larger examples and obtains tighter bounds, within a smaller execution time. Furthermore, it allows a user to trade the quality of bounds with execution time of the procedure, making it suitable for both small and large-scale algorithms.
C1 [Boland, David; Constantinides, George A.] Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, Circuits & Syst Res Grp, London, England.
C3 Imperial College London
RP Boland, D (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Elect & Elect Engn, Circuits & Syst Res Grp, London, England.
EM david.boland03@impe-rial.ac.uk; g.constantinides@imperial.ac.uk
OI Boland, David/0000-0001-5370-4464
FU EPSRC [EP/I020357/1] Funding Source: UKRI
CR [Anonymous], 2007, MIMO Wireless Communications
   Barrett R, 1994, TEMPLATES SOLUTION L, DOI DOI 10.1137/1.9781611971538
   Boland D, 2012, FPGA 12: PROCEEDINGS OF THE 2012 ACM-SIGDA INTERNATIONAL SYMPOSIUM ON FIELD PROGRAMMABLE GATE ARRAYS, P185
   Boland D, 2011, IEEE T COMPUT AID D, V30, P1691, DOI 10.1109/TCAD.2011.2161307
   Boland D, 2008, I C FIELD PROG LOGIC, P378
   BROWN WS, 1981, ACM T MATH SOFTWARE, V7, P445, DOI 10.1145/355972.355975
   Caffarena G, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/171027
   Cantin M.-A., 2001, ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196), P53, DOI 10.1109/ISCAS.2001.921982
   Cantin MA, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, PROCEEDINGS, P612
   Chang ML, 2004, ANN IEEE SYM FIELD P, P59, DOI 10.1109/FCCM.2004.18
   Chang ML, 2002, ANN IEEE SYM FIELD P, P229, DOI 10.1109/FPGA.2002.1106677
   Chow GCT, 2011, ANN IEEE SYM FIELD P, P17, DOI 10.1109/FCCM.2011.57
   Cmar R, 1999, DESIGN, AUTOMATION AND TEST IN EUROPE CONFERENCE AND EXHIBITION 1999, PROCEEDINGS, P271, DOI 10.1109/DATE.1999.761133
   Constantinides G. A., 2001, 9 ANN IEEE S FIELD P, P51
   Constantinides GA, 2002, ANN IEEE SYM FIELD P, P219, DOI 10.1109/FPGA.2002.1106676
   Constantinides GA, 2011, IEEE DES TEST COMPUT, V28, P8, DOI 10.1109/MDT.2011.48
   COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354
   Courtois N, 2000, LECT NOTES COMPUT SC, V1807, P392
   de Dinechin F., 2006, Applied Computing 2006. 21st Annual ACM Symposium on Applied Computing, P1318, DOI 10.1145/1141277.1141584
   de Figueiredo L.H., 1997, Self-Validated Numerical Methods and Applications
   deLorimier M., 2005, Proceedings of the 2005 ACM/SIGDA 13th international symposium on Field-programmable gate arrays, FPGA'05, P75
   Einarsson B., 2005, HDB ACCURACY RELIABI, P195
   Fisher B., 1996, POLYNOMIAL BASED ITE
   Gaffar AA, 2002, LECT NOTES COMPUT SC, V2438, P523
   Heath MichaelT., 2001, SCI COMPUTING
   Higham N.J., 2002, ACCURACY STABILITY N, V2nd ed.
   Keding H, 1998, DESIGN, AUTOMATION AND TEST IN EUROPE, PROCEEDINGS, P429, DOI 10.1109/DATE.1998.655893
   Kestur S, 2010, IEEE COMP SOC ANN, P288, DOI 10.1109/ISVLSI.2010.84
   Kinsman AB, 2010, IEEE T COMPUT AID D, V29, P405, DOI 10.1109/TCAD.2010.2041839
   Kum KI, 2001, IEEE T COMPUT AID D, V20, P921, DOI 10.1109/43.936374
   Kum KI, 1998, 1998 IEEE WORKSHOP ON SIGNAL PROCESSING SYSTEMS-SIPS 98, P569, DOI 10.1109/SIPS.1998.715819
   Langou Julie., 2006, P 2006 ACMIEEE C SUP, DOI [DOI 10.1145/1188455.1188573, 10.1145/1188455.1188573.]
   Lee DU, 2006, IEEE T COMPUT AID D, V25, P1990, DOI 10.1109/TCAD.2006.873887
   Lee DU, 2005, DES AUT CON, P837, DOI 10.1109/DAC.2005.193931
   Lopes AR, 2010, LECT NOTES COMPUT SC, V5992, P157, DOI 10.1007/978-3-642-12133-3_16
   Maciejowski J. M., 2002, Predictive Control with Constraints
   Makino K., 2003, International Journal of Pure and Applied Mathematics, V6, P239
   Menard D, 2008, IEEE T CIRCUITS-I, V55, P3197, DOI 10.1109/TCSI.2008.923279
   Moore R. E., 1966, INTERVAL ANAL
   Muller Jean-Michel, 2005, Elementary Functions: Algorithms and Implementation
   Neumaier A., 2003, Reliable Computing, V9, P43, DOI 10.1023/A:1023061927787
   Osborne WG, 2007, I C FIELD PROG LOGIC, P617, DOI 10.1109/FPL.2007.4380730
   Pang Y, 2010, IEEE T COMPUT AID D, V29, P1177, DOI 10.1109/TCAD.2010.2049154
   RATSCHEK H, 1980, SIAM J NUMER ANAL, V17, P656, DOI 10.1137/0717055
   Roy S, 2005, IEEE T COMPUT, V54, P886, DOI 10.1109/TC.2005.106
   Sung WY, 1995, IEEE T SIGNAL PROCES, V43, P3087, DOI 10.1109/78.476465
   Tesla c1060 Computing Processor Board, 2007, TESLA C1060 COMPUTIN
   Underwood K., 2004, FPGA 04, P171
   Willems M, 1997, DES AUT CON, P293, DOI 10.1145/266021.266105
   Zhao Z., 2003, HIGH PERFORMANCE EMB, P141
   Zhuo L, 2007, IEEE T PARALL DISTR, V18, P1377, DOI 10.1109/TPDS.2007.1068
NR 51
TC 6
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 242
EP 256
DI 10.1109/TMM.2012.2231666
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, XP
AF Zhang, Xinpeng
TI Reversible Data Hiding With Optimal Value Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distortion; payload; reversible data hiding
ID DIFFERENCE EXPANSION
AB In reversible data hiding techniques, the values of host data are modified according to some particular rules and the original host content can be perfectly restored after extraction of the hidden data on receiver side. In this paper, the optimal rule of value modification under a payload-distortion criterion is found by using an iterative procedure, and a practical reversible data hiding scheme is proposed. The secret data, as well as the auxiliary information used for content recovery, are carried by the differences between the original pixel-values and the corresponding values estimated from the neighbors. Here, the estimation errors are modified according to the optimal value transfer rule. Also, the host image is divided into a number of pixel subsets and the auxiliary information of a subset is always embedded into the estimation errors in the next subset. A receiver can successfully extract the embedded secret data and recover the original content in the subsets with an inverse order. This way, a good reversible data hiding performance is achieved.
C1 Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
C3 Shanghai University
RP Zhang, XP (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
EM xzhang@shu.edu.cn
FU National Natural Science Foundation of China [61073190, 61103181,
   60832010]; Research Fund for the Doctoral Program of Higher Education of
   China [20113108110010]; Program for Professor of Special Appointment
   (Eastern Scholar) at Shanghai Institutions of Higher Learning; Alexander
   von Humboldt Foundation
FX Manuscript received May 04, 2011; revised August 05, 2011; accepted July
   04, 2012. Date of publication November 21, 2012; date of current version
   January 15, 2013. This work was supported by the National Natural
   Science Foundation of China under Grants 61073190, 61103181 and
   60832010, by the Research Fund for the Doctoral Program of Higher
   Education of China under Grant 20113108110010, by the Program for
   Professor of Special Appointment (Eastern Scholar) at Shanghai
   Institutions of Higher Learning, and by the Alexander von Humboldt
   Foundation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Christophe De
   Vleeschouwer.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chang CC, 2008, IET INFORM SECUR, V2, P35, DOI 10.1049/iet-ifs:20070004
   De Vleeschouwer C, 2003, IEEE T MULTIMEDIA, V5, P97, DOI 10.1109/TMM.2003.809729
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   Goljan M., 2001, 4th Information Hiding Workshop, LNCS, V2137, P27, DOI DOI 10.1007/3-540-45496-9
   Hong W, 2010, SIGNAL PROCESS, V90, P2911, DOI 10.1016/j.sigpro.2010.04.012
   Hu YJ, 2009, IEEE T CIRC SYST VID, V19, P250, DOI 10.1109/TCSVT.2008.2009252
   Kamstra L, 2005, IEEE T IMAGE PROCESS, V14, P2082, DOI 10.1109/TIP.2005.859373
   Kim HJ, 2008, IEEE T INF FOREN SEC, V3, P456, DOI 10.1109/TIFS.2008.924600
   Kim KS, 2009, PATTERN RECOGN, V42, P3083, DOI 10.1016/j.patcog.2009.04.004
   Luo LX, 2010, IEEE T INF FOREN SEC, V5, P187, DOI 10.1109/TIFS.2009.2035975
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ni ZC, 2008, IEEE T CIRC SYST VID, V18, P497, DOI 10.1109/TCSVT.2008.918761
   Tai WL, 2009, IEEE T CIRC SYST VID, V19, P904, DOI 10.1109/TCSVT.2009.2017409
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tsai P, 2009, SIGNAL PROCESS, V89, P1129, DOI 10.1016/j.sigpro.2008.12.017
   Wang X, 2010, IEEE SIGNAL PROC LET, V17, P567, DOI 10.1109/LSP.2010.2046930
   Weng SW, 2008, IEEE SIGNAL PROC LET, V15, P721, DOI 10.1109/LSP.2008.2001984
   Wu HC, 2009, J SYST SOFTWARE, V82, P1966, DOI 10.1016/j.jss.2009.06.056
NR 21
TC 151
Z9 163
U1 1
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 316
EP 325
DI 10.1109/TMM.2012.2229262
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500008
DA 2024-07-18
ER

PT J
AU Lu, C
   Mandal, M
AF Lu, Cheng
   Mandal, Mrinal
TI A Robust Technique for Motion-Based Video Sequences Temporal Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video alignment; video synchronization; temporal registration; dynamic
   time warping; point sets alignment
ID OPTIMIZATION
AB In this paper, we propose a robust technique for temporal alignment of video sequences with similar planar motions acquired using uncalibrated cameras. In this technique, we model the motion-based video temporal alignment problem as a spatio-temporal discrete trajectory point sets alignment problem. First, the trajectory of the object of interest is tracked throughout the videos. A probabilistic method is then developed to calculate the 'soft' spatial correspondence between the trajectory point sets. Next, a dynamic time warping technique (DTW) is applied to the spatial correspondence information to compute the temporal alignment of the videos. The experimental results show that the proposed technique provides a superior performance over existing techniques for videos with similar trajectory patterns.
C1 [Lu, Cheng; Mandal, Mrinal] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
C3 University of Alberta
RP Lu, C (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2V4, Canada.
EM lcheng4@ual-berta.ca; mmandal@ualberta.ca
RI Lu, Cheng/AAH-1606-2021
OI Lu, Cheng/0000-0002-7651-3924
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2005, NEURAL NETWORKS PATT
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Caspi Y, 2002, IEEE T PATTERN ANAL, V24, P1409, DOI 10.1109/TPAMI.2002.1046148
   Caspi Y, 2006, INT J COMPUT VISION, V68, P53, DOI 10.1007/s11263-005-4842-z
   CHUDOVA D, 2003, P 19 C UNC ART INT
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Dai CX, 2006, IEEE IMAGE PROC, P501, DOI 10.1109/ICIP.2006.312436
   Giese MA, 2000, INT J COMPUT VISION, V38, P59, DOI 10.1023/A:1008118801668
   Gold Steven., 1994, Advances in Neural Information Processing Systems, V7, P957
   Hess R, 2007, PROC CVPR IEEE, P154
   Lee L, 2000, IEEE T PATTERN ANAL, V22, P758, DOI 10.1109/34.868678
   Li RN, 2010, LECT NOTES COMPUT SC, V6315, P547, DOI 10.1007/978-3-642-15555-0_40
   Lin CD, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, VOLS 1 AND 2, P1
   Lu C., EXPT RESULTS ROBUST
   Lu C, 2011, J VIS COMMUN IMAGE R, V22, P606, DOI 10.1016/j.jvcir.2011.06.003
   Lu C, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3488415
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Pádua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301
   Perperidis D., 2004, MED IMAGE COMPUTING, V3216, P441
   Rao C, 2002, INT J COMPUT VISION, V50, P203, DOI 10.1023/A:1020350100748
   Rao C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P939
   Rao C., VIEW INVARIANT REPRE
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Shrestha P, 2010, IEEE T MULTIMEDIA, V12, P79, DOI 10.1109/TMM.2009.2036285
   Singh M, 2008, LECT NOTES COMPUT SC, V5303, P554, DOI 10.1007/978-3-540-88688-4_41
   Singh M, 2007, IEEE T MULTIMEDIA, V9, P1004, DOI 10.1109/TMM.2007.898937
   Tresadern P., 2003, Proceedings of the 14th British Machine Vision Conference, P629
   Tuytelaars T, 2004, PROC CVPR IEEE, P762
NR 30
TC 18
Z9 19
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 70
EP 82
DI 10.1109/TMM.2012.2225036
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600006
DA 2024-07-18
ER

PT J
AU Ren, RD
   Collomosse, J
AF Ren, Reede
   Collomosse, John
TI Visual Sentences for Pose Retrieval Over Low-Resolution Cross-Media
   Dance Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content based image retrieval; dance archives; low-resolution pose
   similarity
ID HUMAN-BODY CONFIGURATIONS; SEGMENTATION; MODELS
AB We describe a system for matching human posture (pose) across a large cross-media archive of dance footage spanning nearly 100 years, comprising digitized photographs and videos of rehearsals and performances. This footage presents unique challenges due to its age, quality and diversity. We propose a forest-like pose representation combining visual structure (self-similarity) descriptors over multiple scales, without explicitly detecting limb positions which would be infeasible for our data. We explore two complementary multi-scale representations, applying passage retrieval and latent Dirichlet allocation (LDA) techniques inspired by the text retrieval domain, to the problem of pose matching. The result is a robust system capable of quickly searching large cross-media collections for similarity to a visually specified query pose. We evaluate over a cross-section of the UK National Research Centre for Dance's (UK-NRCD), and the Siobhan Davies Replay's (SDR) digital dance archives, using visual queries supplied by dance professionals. We demonstrate significant performance improvements over two base-lines: classical single and multi-scale bag of visual words (BoVW) and spatial pyramid kernel (SPK) matching [5].
C1 [Ren, Reede] Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
   [Collomosse, John] Univ Surrey, Ctr Vis Speech & Signal Proc, Surrey GU2 7XH, England.
C3 University of Glasgow; University of Surrey
RP Ren, RD (corresponding author), Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.
EM Reede.Ren@glasgow.ac.uk; J.Collomosse@surrey.ac.uk
FU AHRC [AH/H037926/1]; AHRC [AH/H037926/1] Funding Source: UKRI
FX This work was supported under the AHRC "Digital Dance Archives" project
   AH/H037926/1. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR AGARWAL A, 2006, P EUR C COMP VIS ECC
   Aggarwal A., 2006, P AS C COMP VIS ACCV, V3852
   Amati G, 2002, ACM T INFORM SYST, V20, P357, DOI 10.1145/582415.582416
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], 2009, P BRIT MACH VIS C BM
   [Anonymous], 2009, P INT C COMP VIS
   [Anonymous], 2009, P COMP VIS PATT REC
   Bosch A., 2007, P 6 ACM INT C IMAGE, P401, DOI DOI 10.1145/1282280.1282340
   Chatfield Ken, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P264, DOI 10.1109/ICCVW.2009.5457691
   Clarke C., 2008, P ACM SIGIR
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Deerwester S., 1990, J AM SOC INFORM SCI, V91, P993
   Eichner M., 2010, 272 ETH DITET BIWI T
   Felzenszwalb P., 2003, INT J COMPUT VIS, V61
   Ferrari V., 2009, P COMP VIS PATT REC
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   Guan P., 2009, P INT C COMP VIS
   HARTER SP, 1975, J AM SOC INFORM SCI, V26, P197, DOI 10.1002/asi.4630260402
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   Johnson S., 2010, P BRIT MACH VIS C
   Kleinberg J., 2005, ALGORITHM DESIGN, P158
   Laban R., 1966, CHOREUTICS
   Lan XY, 2005, IEEE I CONF COMP VIS, P470
   Lan XY, 2004, PROC CVPR IEEE, P722
   Lv YH, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P579
   Maji S., 2008, P COMP VIS PATT REC
   Mori G, 2004, PROC CVPR IEEE, P326
   Ning Y. G. H., 2008, P CVPR
   Nister D., 2006, P EUR C COMP VIS ECC
   Poppe R, 2007, COMPUT VIS IMAGE UND, V108, P4, DOI 10.1016/j.cviu.2006.10.016
   Ren XF, 2005, IEEE I CONF COMP VIS, P824
   Samangooei S, 2010, MULTIMED TOOLS APPL, V49, P195, DOI 10.1007/s11042-009-0391-8
   Shakhnarovich G., 2003, P IEEE INT C COMP VI, V2
   Srinivasan P., 2007, IEEE INT C COMPUTER, P1
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Yang Y., 2011, P COMP VIS PATT REC
   Zhao T, 2003, PROC CVPR IEEE, P459, DOI 10.1109/NSSMIC.2003.1352083
NR 38
TC 10
Z9 12
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1652
EP 1661
DI 10.1109/TMM.2012.2199971
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400014
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Kambhatla, KKR
   Kumar, S
   Paluri, S
   Cosman, PC
AF Kambhatla, Kashyap K. R.
   Kumar, Sunil
   Paluri, Seethal
   Cosman, Pamela C.
TI Wireless H.264 Video Quality Enhancement Through Optimal Prioritized
   Packet Fragmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H264; MAC layer; packet fragmentation; priority-aware; PSNR; real-time;
   video compression; video quality; video slice; video streaming; VQM;
   weighted goodput
ID TRANSMISSION; OPTIMIZATION; THROUGHPUT; SIZE
AB We introduce a cross-layer priority-aware packet fragmentation scheme at the MAC layer to enhance the quality of pre-encoded H.264/AVC compressed bitstreams over bit-rate limited error-prone links in wireless networks. The H.264 slices are classified in four priorities at the encoder based on their cumulative mean square error (CMSE) contribution towards the received video quality. The slices of a priority class in each frame are aggregated into video packets of corresponding priority. We derive the optimal fragment size for each priority class which achieves the maximum expected weighted goodput at different encoded video bit rates, slice sizes and bit error rates. Priority-aware packet fragmentation invokes slice discard in the buffer due to channel bit rate constraints on allocating fragment header bits. We propose a slice discard scheme using frame importance and slice CMSE contribution to control error propagation effects. Packet fragmentation is extended to slice fragmentation by modifying the conventional H.264 decoder to handle partial slice decoding. Priority-aware slice fragmentation combined with the proposed slice discard scheme provides considerable PSNR and VQM gains as compared to priority-agnostic fragmentation.
C1 [Kambhatla, Kashyap K. R.] San Diego State Univ, Coll Engn, San Diego, CA 92182 USA.
   [Kambhatla, Kashyap K. R.; Cosman, Pamela C.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
   [Kumar, Sunil] San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.
   [Paluri, Seethal] San Diego State Univ, Computat Sci Res Ctr, San Diego, CA 92182 USA.
C3 California State University System; San Diego State University;
   University of California System; University of California San Diego;
   California State University System; San Diego State University;
   California State University System; San Diego State University
RP Kambhatla, KKR (corresponding author), San Diego State Univ, Coll Engn, San Diego, CA 92182 USA.
EM kkambhat@ucsd.edu; skumar@mail.sdsu.edu; spaluri@sciences.sdsu.edu;
   pcosman@eng.ucsd.edu
RI Kumar, Sunil/AAT-4942-2020
OI Kumar, Sunil/0000-0001-9957-5661; Cosman, Pamela/0000-0002-4012-0176
FU US Air Force Research Laboratory at San Diego State University
   [FA8750-08-1-0078, FA8750-11-1-0048]
FX This material is based upon work that was supported in part by the US
   Air Force Research Laboratory under agreements FA8750-08-1-0078 and
   FA8750-11-1-0048 at San Diego State University.
CR Alefeld G., 1999, ELSEVIER J COMPUTATI, V121, P421
   [Anonymous], H 264 AVC REF SOFTW
   [Anonymous], 2001, RFC3095
   Chang Y., 2007, IEEE TRIDENTCOM, P1
   Connie AT, 2008, CONSUM COMM NETWORK, P800, DOI 10.1109/ccnc08.2007.185
   Daji Qiao, 2002, IEEE Transactions on Mobile Computing, V1, P278, DOI 10.1109/TMC.2002.1175541
   Fallah YP, 2007, CONSUM COMM NETWORK, P875, DOI 10.1109/CCNC.2007.177
   Fallah YP, 2008, EURASIP J WIREL COMM, V2008, P1
   Feng Zheng, 2008, 2008 6th International Symposium on Modeling and Optimization in Mobile, Ad Hoc, and Wireless Networks and Workshops (WiOPT), P437, DOI 10.1109/WIOPT.2008.4586104
   Forouzan BA, 2007, FOROUZAN NETWORKING
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   ICHIDA K, 1979, COMPUTING, V23, P85, DOI 10.1007/BF02252616
   Jelenkovic PR, 2008, MOBIHOC'08: PROCEEDINGS OF THE NINTH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P73
   Kambhatla K. K. R., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3233, DOI 10.1109/ICIP.2011.6116358
   Kim BS, 2005, IEEE T VEH TECHNOL, V54, P1415, DOI 10.1109/TVT.2005.851361
   Ksentini A, 2006, IEEE COMMUN MAG, V44, P107, DOI 10.1109/MCOM.2006.1580940
   Kumar S, 2006, J VIS COMMUN IMAGE R, V17, P183, DOI 10.1016/j.jvcir.2005.12.002
   Kumar S, 2009, IEEE MILIT COMMUN C, P1702
   Lettieri P, 1998, IEEE INFOCOM SER, P564, DOI 10.1109/INFCOM.1998.665076
   MUNACK H, 1992, COMPUTING, V48, P319, DOI 10.1007/BF02238641
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rajan Dinesh, 2007, 2007 International Conference on Wireless Algorithms, Systems and Applications, P158
   Setton E, 2005, IEEE WIREL COMMUN, V12, P59, DOI 10.1109/MWC.2005.1497859
   Shakkottai S, 2003, IEEE COMMUN MAG, V41, P74, DOI 10.1109/MCOM.2003.1235598
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Sun Y., 2005, ACM WiTMeMo, P25
   SUPERIORI L, 2006, P INT C ADV MOB COMP
   Tanenbaum A.S., 2002, COMPUT NETW, VFourth
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Wang XD, 2006, MOBILE NETW APPL, V11, P279, DOI 10.1007/s11036-005-4479-8
   Wang YM, 2004, VTC2004-SPRING: 2004 IEEE 59TH VEHICULAR TECHNOLOGY CONFERENCE, VOLS 1-5, PROCEEDINGS, P2205
   Wenger S., 2005, 3894 RFC NETW WORK G
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wolf S., 2002, Video quality measurement techniques
   Yin J, 2004, IEEE WCNC, P1654, DOI 10.1109/WCNC.2004.1311801
NR 35
TC 16
Z9 20
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1480
EP 1495
DI 10.1109/TMM.2012.2196508
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600009
OA Green Published
DA 2024-07-18
ER

PT J
AU Saini, M
   Wang, XY
   Atrey, PK
   Kankanhalli, M
AF Saini, Mukesh
   Wang, Xiangyu
   Atrey, Pradeep K.
   Kankanhalli, Mohan
TI Adaptive Workload Equalization in Multi-Camera Surveillance Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Cloud; dynamic; model; surveillance; workload
AB Surveillance and monitoring systems generally employ a large number of cameras to capture people's activities in the environment. These activities are analyzed by hosts (human operators and/or computers) for threat detection. Threat detection is a target centric task in which the behavior of each target is analyzed separately, which requires a significant amount of human attention and is a computationally intensive task for automatic analysis. In order to meet the real-time requirements of surveillance, it is necessary to distribute the video processing load over multiple hosts. In general, cameras are statically assigned to the hosts; we show that this is not a desirable solution as the workload for a particular camera may vary over time depending on the number of targets in its view. In the future, this uneven distribution of workload will become more critical as the sensing infrastructures are being deployed on the cloud. In this paper, we model the camera workload as a function of the number of targets, and use that to dynamically assign video feeds to the hosts. Experimental results show that the proposed model successfully captures the variability of the workload, and that the dynamic workload assignment provides better results than a static assignment.
C1 [Saini, Mukesh; Wang, Xiangyu; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Atrey, Pradeep K.] Univ Winnipeg, Dept Appl Comp Sci, Winnipeg, MB R3B 2E9, Canada.
C3 National University of Singapore; University of Winnipeg
RP Saini, M (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015
CR [Anonymous], 2009, DEP ELECT ENG COMPUT
   Calderara S., 2007, 2007 First ACM/IEEE International Conference on Distributed Smart Cameras, P364, DOI 10.1109/ICDSC.2007.4357545
   Chandra Ramesh., 2010, Proceedings of the 2010 USENIX Conference on Web Application Development, WebApps'10, P1, DOI DOI 10.1109/IPDPSW.2010.5470740
   Collins R., 2000, CMURITR0012
   Detmold H., 2006, P IEEE INT C VIDEO S, P103
   Dias H, 2005, 2005 PORTUGUESE CONFERENCE ON ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P257, DOI 10.1109/EPIA.2005.341225
   Kruegle H., 2006, CCTV SURVEILLANCE AN
   Marcenaro L, 2001, P IEEE, V89, P1419, DOI 10.1109/5.959339
   Marchesotti L, 2002, IEEE IMAGE PROC, P892
   Maxiaguine A, 2004, DESIGN, AUTOMATION AND TEST IN EUROPE CONFERENCE AND EXHIBITION, VOLS 1 AND 2, PROCEEDINGS, P1040, DOI 10.1109/DATE.2004.1269030
   Saini M., 2011, P ICME
   Saini M. K., 2010, Proceedings 7th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS 2010), P72, DOI 10.1109/AVSS.2010.58
   Saini M, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P179, DOI 10.1109/ISM.2009.27
   SOLDATINI F, 2000, MULTIMEDIA VIDEO BAS, P143
   Song B., 2005, JOB SCHEDULING STRAT, P9
   Trivedi MM, 2005, IEEE INTELL SYST, V20, P58, DOI 10.1109/MIS.2005.86
   Valera M, 2005, IEE P-VIS IMAGE SIGN, V152, P192, DOI 10.1049/ip-vis:20041147
   2000, PETS PERFORMANCE EVA
NR 18
TC 25
Z9 25
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 555
EP 562
DI 10.1109/TMM.2012.2186957
PN 1
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300007
DA 2024-07-18
ER

PT J
AU Li, JX
   Shao, B
   Li, T
   Ogihara, M
AF Li, Jingxuan
   Shao, Bo
   Li, Tao
   Ogihara, Mitsunori
TI Hierarchical Co-Clustering: A New Way to Organize the Music Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-clustering; hierarchical clustering; user tags
ID ERROR-VARIANCE APPROACH; ALGORITHMS
AB In music information retrieval (MIR) an important research topic, which has attracted much attention recently, is the utilization of user-assigned tags, artist-related style, and mood labels, which can be extracted from music listening web sites, such as Last. fm (http:www.last.fm/) and All Music Guide (http:www.allmusic.com/). A fundamental research problem in the area is how to understand the relationships among artists/songs and these different pieces of information.
   Co-clustering is the problem of simultaneously clustering two types of data (e.g., documents and words, and webpages and urls). We can naturally bring this idea to the situation at hand and consider clustering artists and tags together, artists and styles together, or artists and mood labels together. Once such co-clustering has been successfully completed, one can identify co-existing clusters of artists and tags, styles, or mood labels (T/S/M). For simplicity, we use the acronym T/S/M to refer to tag(s), style(s), or mood(s) for the rest of the paper. When dealing with tags it is worth noticing that some tags are more specific versions of others. This naturally suggests that the tags could be organized in hierarchical clusters. Such hierarchical organizations exist for styles andmood labels, so we will consider hierarchical co-clustering of artists and T/S/M.
   In this paper, we systematically study the application of hierarchical co-clustering (HCC) methods for organizing the music data. There are two standard strategies for hierarchical clustering. One is the divisive strategy, in which we attempt to divide the input data set into smaller groups recursively, and the other is the agglomerative strategy, in which we attempt to combine initially individually separated data points into larger groups by finding the most closely related pair at each iteration. We will compare these two strategies against each other. We apply a previously known divisive hierarchical co-clustering method and a novel agglomerative hierarchical co-clustering. In addition, we demonstrate that these two methods have the capability of incorporating instance-level constraints to achieve better performance. We perform experiments to show that these two hierarchical co-clustering methods can be effectively deployed for organizing the music data and they present reasonable clustering performance comparing with the other clustering methods. A case study is also conducted to show that HCC provides us a new method to quantify the artist similarity.
C1 [Li, Jingxuan; Shao, Bo; Li, Tao] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
   [Ogihara, Mitsunori] Univ Miami, Dept Comp Sci, Coral Gables, FL 33124 USA.
C3 State University System of Florida; Florida International University;
   University of Miami
RP Li, JX (corresponding author), Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
EM taoli@cs.fiu.edu
RI Ogihara, Mitsunori/AAB-8275-2020; guo, ppdop/KAL-9865-2024
OI Ogihara, Mitsunori/0000-0002-5690-7854; 
FU NSF [IIS-0546280, CCF-0939179]
FX Manuscript received August 26, 2011; revised November 17, 2011; accepted
   December 01, 2011. Date of publication December 22, 2011; date of
   current version March 21, 2012. This work was supported in part by NSF
   grants IIS-0546280 and CCF-0939179. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Z. Jane Wang.
CR [Anonymous], P 21 ACM INT C MACH
   Bade K., 2008, Proceedings of the SIAM International Conference on Data Mining, SDM 2008, P13
   Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654
   Basu S, 2009, CH CRC DATA MIN KNOW, P1
   Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25
   Bosteels K., 2008, LAT BREAK SESS ISMIR
   Cho H, 2004, SIAM PROC S, P114
   CUTTING DR, 1992, SIGIR 92 : PROCEEDINGS OF THE FIFTEENTH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P318
   Davidson I, 2009, DATA MIN KNOWL DISC, V18, P257, DOI 10.1007/s10618-008-0103-4
   Dhillon IS, 2001, P 7 ACM SIGKDD INT C, P269, DOI DOI 10.1145/502512.502550
   Dhillon IS, 2003, P 9 ACM SIGKDD INT C, P89, DOI DOI 10.1145/956750.956764
   Dunn J. C., 1973, Journal of Cybernetics, V3, P32, DOI 10.1080/01969727308546046
   ECKES T, 1993, J CLASSIF, V10, P51, DOI 10.1007/BF02638453
   Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863
   Foote J., 2001, Multimedia and Expo, IEEE International Confer- ence on, P224
   Fung BCM, 2003, SIAM PROC S, P59
   Gaul W., 1996, Data analysis and information systems
   Getz G, 2000, P NATL ACAD SCI USA, V97, P12079, DOI 10.1073/pnas.210134797
   Gilpin S, 2011, P 17 ACM SIGKDD INT, P1136
   Gu Xu, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P300, DOI 10.1145/1148170.1148224
   Hosseini M, 2007, LECT NOTES COMPUT SC, V4831, P653
   Ienco D, 2009, LECT NOTES ARTIF INT, V5781, P580, DOI 10.1007/978-3-642-04180-8_55
   Jiang J, 1997, INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, 1997 DIGEST OF TECHNICAL PAPERS, P94
   Kumar V., 2006, Introduction to Data Mining
   Li J., P 11 INT SOC MUS INF
   Li JX, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P861
   Li T, 2003, P 26 ANN INT ACM SIG, P282, DOI [DOI 10.1145/860435.860487, 10.1145/860484.860487, DOI 10.1145/860484.860487]
   Li T., 2011, MUSIC DATA MINING, V21
   Li T, 2006, IEEE T MULTIMEDIA, V8, P564, DOI 10.1109/TMM.2006.870730
   Lin D, 1998, P 15 INT C MACH LEAR, V98, P296
   Logan B., 2001, A content-based music similarity function
   Long B., 2006, SIGKDD, P317, DOI DOI 10.1145/1150402.1150439
   Madeira SC, 2004, IEEE ACM T COMPUT BI, V1, P24, DOI 10.1109/TCBB.2004.2
   MILLIGAN GW, 1986, MULTIVAR BEHAV RES, V21, P441, DOI 10.1207/s15327906mbr2104_5
   MIRKIN B, 1995, J CLASSIF, V12, P243, DOI 10.1007/BF03040857
   Pham DL, 2001, COMPUT VIS IMAGE UND, V84, P285, DOI 10.1006/cviu.2001.0951
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   Schlicker A, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-302
   Shao B., 2008, P ACM WORKSH WEB INF, P119, DOI DOI 10.1145/1458502.1458522
   SOKAL ROBERT R., 1962, TAXON, V11, P33, DOI 10.2307/1217208
   Strehl A, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P93, DOI 10.1162/153244303321897735
   Symeonidis P., 2008, Proceedings of the 9th International Conference on Music Information Retrieval (ISMIR 2008), P219
   Tang L, 2009, MINES 2009: FIRST INTERNATIONAL CONFERENCE ON MULTIMEDIA INFORMATION NETWORKING AND SECURITY, VOL 1, PROCEEDINGS, P244, DOI 10.1109/MINES.2009.194
   Turnbull D, 2008, P 9 INT C MUS INF RE
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wang Fei., 2009, International Society for Music Information Retrieval, P363
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Zhao Y, 2005, DATA MIN KNOWL DISC, V10, P141, DOI 10.1007/s10618-005-0361-3
   Zheng L., 2011, P 2011 IEEE INT C DA
NR 49
TC 11
Z9 12
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 471
EP 481
DI 10.1109/TMM.2011.2181151
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500019
DA 2024-07-18
ER

PT J
AU Jansen, J
   Cesar, P
   Bulterman, DCA
   Stevens, T
   Kegel, I
   Issing, J
AF Jansen, Jack
   Cesar, Pablo
   Bulterman, Dick C. A.
   Stevens, Tim
   Kegel, Ian
   Issing, Jochen
TI Enabling Composition-Based Video-Conferencing for the Home
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compression and coding; consumer electronics and entertainment;
   presentation of content in multimedia sessions; standards and related
   issues; videoconferencing and collaboration environments
AB This paper describes a videoconferencing system that meets performance constraints and functional requirements for use in consumer homes. Our system improves existing home technologies (such as video chat) by providing high-quality audiovisual communication, efficient encoding mechanisms, and low end-to-end delay. Moreover, the system includes a control interface that is capable of dynamically manipulating and compositing audiovisual content streams. This innovative architectural component is required for a domestic setting, where the television acts as the main screen and multiple people gather around it. Apart from the requirements and architecture, this paper analyses the performance of our system. The results validate our architectural decisions and provide a valuable input for further research in domestic videoconferencing.
C1 [Jansen, Jack; Cesar, Pablo; Bulterman, Dick C. A.] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
   [Kegel, Ian] BT Res & Technol, Future Content Grp, Ipswich, Suffolk, England.
   [Issing, Jochen] Univ Erlangen Nurnberg, Dept Comp Sci 7, Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Jansen, J (corresponding author), Ctr Wiskunde & Informat, Amsterdam, Netherlands.
EM Jack.Jansen@cwi.nl; p.s.cesar@cwi.nl; dick.bulterman@cwi.nl
RI Jansen, Jack/KHZ-0382-2024
OI Jansen, Jack/0000-0002-7006-2560; Cesar, Pablo/0000-0003-1752-6837
FU European Community [FP7/2007-2013, ICT-2007-214793]
FX Manuscript received September 30, 2010; revised March 09, 2011; accepted
   May 22, 2011. Date of publication June 13, 2011; date of current version
   September 16, 2011. This work was supported in part by funding from the
   European Community's Seventh Framework Programme (FP7/2007-2013) under
   grant agreement no. ICT-2007-214793. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   S.-H. Gary Chan.
CR Ames MorganG., 2010, P 2010 ACM C COMPUTE, P145, DOI DOI 10.1145/1718918.1718946
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], P ACM C COMP SUPP CO
   [Anonymous], P 9 ACM INT C MULT M
   [Anonymous], 2000, E MOD COMP MOD US TR
   Baker H.Harlyn., 2005, ACM Trans. Multimedia Comput. Commun. Appl, V1, P190, DOI DOI 10.1145/1062253.1062258
   Baldi M, 2000, IEEE ACM T NETWORK, V8, P479, DOI 10.1109/90.865076
   BATCHELLER AL, 2007, P CHI, P849
   Boyaci O, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P194, DOI 10.1109/ISM.2009.46
   Bulterman Dick., 2008, SYNCHRONIZED MULTIME, p3.0
   Duffner S., 2011, P INT C SIGN ACQ PRO
   FAVROT A, 2008, P INT WORKSH AC ECH
   FRANTZIS M, 2010, INTERACTION MODELLIN
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   GAVER W, 1993, HUMAN FACTORS IN COMPUTING SYSTEMS, P335
   HAN I, 2008, P IEEE INT C CONS EL, P233
   ISSING J, 2008, P 124 AES CONV
   *ITU R, 1934, BS1534 ITUR
   *ITU T, 1997, H231 ITUT
   ITU-T, 2003, Recommendation I.371
   JANSEN J, 2008, P 8 ACM S DOC ENG SA, P18
   KAISER R, 2010, P ACM INT WORKSH EV, P29
   KALLINGER M, 2008, P 124 AES CONV
   Kernchen R, 2010, IEEE MULTIMEDIA, V17, P52, DOI 10.1109/MMUL.2009.75
   KUECH F, 2008, P INT WORKSH AC ECH
   Lampi F., 2008, P ACM INT C MULT MM, P1103
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   NGUYEN DT, 2009, P ACM CHI, P423
   Ott DavidE., 2004, MULTIMEDIA '04: Proceedings of the 12th annual ACM inter- national conference on Multimedia, P596
   Pinson MH, 2010, IEEE T BROADCAST, V56, P86, DOI 10.1109/TBC.2009.2034511
   POLTROCK SE, 2005, P ANN HAW INT C SYST, V4
   POSTEL J, 1980, 0006 IETF
   Pulkki V, 2007, J AUDIO ENG SOC, V55, P503
   RANJAN A, 2008, P ACM CHI, P227, DOI DOI 10.1145/1357054.1357095
   Roberts D, 2009, IEEE ACM DIS SIM, P89, DOI 10.1109/DS-RT.2009.43
   Rogge B, 2004, IEEE T MULTIMEDIA, V6, P910, DOI 10.1109/TMM.2004.835213
   SCHNELL M, 2007, P 122 AES CONV
   SCHULTZAMLING R, 2008, P 124 AES CONV
   SCHULZRINNE H., 2003, RFC 3550 RTP: A transport protocol for real-time applications
   Williams D, 2009, EUROITV'09: PROCEEDINGS OF THE SEVENTH EUROPEAN INTERACTIVE TELEVISION CONFERENCE, P19
   YAMASHITA N, 2008, P CSCW2008 ACM, P177, DOI DOI 10.1145/1460563.1460591
   Yang ZY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671963
   Yarosh S., 2010, P CHI, P1251
   2010, P DTSCH TEL AG TSG S
NR 44
TC 21
Z9 23
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 869
EP 881
DI 10.1109/TMM.2011.2159369
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300004
DA 2024-07-18
ER

PT J
AU Kim, E
   Huang, XL
   Tan, G
AF Kim, Edward
   Huang, Xiaolei
   Tan, Gang
TI Markup SVG-An Online Content-Aware Image Abstraction and Annotation Tool
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data structures; image annotation; image processing; image
   representation; scalable vector graphics (SVG)
ID FEATURES
AB Suppose you want to effectively search through millions of images, train an algorithm to perform image and video object recognition, or research the complex patterns and relationships that exist in our visual world. A common and essential component for any of these tasks is a large annotated image dataset. However, obtaining labeled image data is a complex and tedious task that requires methods for annotating and structuring content. Therefore, we developed a comprehensive online tool and data structure, Markup SVG, that simplifies the collection of annotated image data by leveraging state-of-the-art image processing techniques. As the core data structure of our tool, we adopt scalable vector graphics (SVG), an extensible and versatile language built upon XML. Given the extensibility of our framework, we are able to encode low-level image features, high-level semantics, and further define interactions with the data to assist the user with image annotation. We also demonstrate the ability to merge multiple online and offline datasets into our system in an effort to standardize image collection and its data representation. Lastly, we present our modular design; each component acts as a plug-in to our system. We developed several novel components and algorithms to highlight the possibilities of semi-supervised segmentation and automatic annotation within our proposed framework. Further, our modular design provides the necessary capabilities to incorporate future image features, methods, or algorithms. Our results show that our tool is able to greatly simplify the process of obtaining large annotated image collections in an online collaborative platform.
C1 [Kim, Edward; Huang, Xiaolei; Tan, Gang] Lehigh Univ, Dept Comp Sci & Engn, PC Rossin Coll Engn & Appl Sci, Bethlehem, PA 18105 USA.
C3 Lehigh University
RP Kim, E (corresponding author), Lehigh Univ, Dept Comp Sci & Engn, PC Rossin Coll Engn & Appl Sci, Bethlehem, PA 18105 USA.
EM edk208@lehigh.edu; xih206@lehigh.edu; gtan@cse.lehigh.edu
OI Huang, Sharon Xiaolei/0000-0003-2338-6535; Kim,
   Edward/0000-0001-5345-3781; Tan, Gang/0000-0001-6109-6091
FU National Science Foundation NSF [IIS-0812120]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [0812120]
   Funding Source: National Science Foundation; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [0854606]
   Funding Source: National Science Foundation
FX Manuscript received September 17, 2010; revised March 11, 2011; accepted
   June 12, 2011. Date of publication July 05, 2011; date of current
   version September 16, 2011. This work was supported in part by a grant
   from the National Science Foundation under contract NSF IIS-0812120. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ajay Divakaran.
CR [Anonymous], P 14 ANN ACM INT C M
   [Anonymous], P MED IMAGE COMPUTIN
   [Anonymous], 2001, Interactive graph cuts for optimal boundary & region segmentation of objects in nd images, DOI DOI 10.1109/ICCV.2001.937505
   [Anonymous], P MICCAI GRID HPC WO
   [Anonymous], SVG SCALABLE VECTOR
   [Anonymous], EUR J COMPUT VISION
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P INT C COMP VIS
   [Anonymous], P 15 ACM INT C MULT
   [Anonymous], BEAUTIFUL CODE
   [Anonymous], P INT C COMP VIS
   [Anonymous], P IEEE C COMP VIS PA
   Barrett W A, 1997, Med Image Anal, V1, P331, DOI 10.1016/S1361-8415(97)85005-0
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6
   DeLoache JS, 1998, PSYCHOL SCI, V9, P205, DOI 10.1111/1467-9280.00039
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Ferrari V, 2006, LECT NOTES COMPUT SC, V3953, P14, DOI 10.1007/11744078_2
   Grady L, 2005, LECT NOTES COMPUT SC, V3750, P773, DOI 10.1007/11566489_95
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Jia J., 2008, ACM International Conference on Multimedia, P459
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Lafferty John, 2001, INT C MACH LEARN ICM
   Lefohn AE, 2003, LECT NOTES COMPUT SC, V2878, P564
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Liu Y, 2007, PATTERN RECOGN, V40, P262, DOI 10.1016/j.patcog.2006.04.045
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   MALLADI R, 1995, IEEE T PATTERN ANAL, V17, P158, DOI 10.1109/34.368173
   Meer P, 2001, IEEE T PATTERN ANAL, V23, P1351, DOI 10.1109/34.977560
   MOKHTARIAN F, 1992, IEEE T PATTERN ANAL, V14, P789, DOI 10.1109/34.149591
   Mortensen EN, 1998, GRAPH MODEL IM PROC, V60, P349, DOI 10.1006/gmip.1998.0480
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shi JB, 1997, PROC CVPR IEEE, P731, DOI 10.1109/CVPR.1997.609407
   Stork DG, 1999, IEEE INTELL SYST APP, V14, P19
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   Xu CY, 1998, IEEE T IMAGE PROCESS, V7, P359, DOI 10.1109/83.661186
   Yao B, 2007, LECT NOTES COMPUT SC, V4679, P169
   Zhao Rong., 2001, DistributedMultimedia Databases: Techniques andApplications, P14
NR 45
TC 4
Z9 5
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 993
EP 1006
DI 10.1109/TMM.2011.2161275
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300013
DA 2024-07-18
ER

PT J
AU Thomos, N
   Chakareski, J
   Frossard, P
AF Thomos, Nikolaos
   Chakareski, Jacob
   Frossard, Pascal
TI Prioritized Distributed Video Delivery With Randomized Network Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Network coding; overlay networks; peer-to-peer (P2P) systems; rate
   allocation; scalable video delivery; unequal error protection; video
   streaming
AB We address the problem of prioritized video streaming over lossy overlay networks. We propose to exploit network path diversity via a novel randomized network coding (RNC) approach that provides unequal error protection (UEP) to the packets conveying the video content. We design a distributed receiver-driven streaming solution, where a client requests packets from the different priority classes from its neighbors in the overlay. Based on the received requests, a node in turn forwards combinations of the selected packets to the requesting peers. Choosing a network coding strategy at every node can be cast as an optimization problem that determines the rate allocation between the different packet classes such that the average distortion at the requesting peer is minimized. As the optimization problem has log-concavity properties, it can be solved with low complexity by an iterative algorithm. Our simulation results demonstrate that the proposed scheme respects the relative priorities of the different packet classes and achieves a graceful quality adaptation to network resource constraints. Therefore, our scheme substantially outperforms reference schemes such as baseline network coding techniques as well as solutions that employ rateless codes with built-in UEP properties. The performance evaluation provides additional evidence of the substantial robustness of the proposed scheme in a variety of transmission scenarios.
C1 [Thomos, Nikolaos; Chakareski, Jacob; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Thomos, N (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Lausanne, Switzerland.
EM nikolaos.thomos@epfl.ch; jakov.cakareski@epfl.ch;
   pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019; Thomos, Nikolaos/AAU-2328-2020
OI Thomos, Nikolaos/0000-0001-7266-2642
FU Swiss National Science Foundation [PZ00P2-121906, PZ00P2-126416]; Swiss
   National Science Foundation (SNF) [PZ00P2_121906, PZ00P2_126416] Funding
   Source: Swiss National Science Foundation (SNF)
FX This work was supported by the Swiss National Science Foundation, under
   grants PZ00P2-121906 and PZ00P2-126416. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Z. Jane Wang.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   *AK INC, 2008, STAT INT
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], P 41 ALL C COMM CONT
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cao Y, 2010, IEEE INT SYMP INFO, P2438, DOI 10.1109/ISIT.2010.5513792
   Chakareski J, 2004, IEEE T COMMUN, V52, P1675, DOI 10.1109/TCOMM.2004.836436
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Chou PA, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1221, DOI 10.1109/ICME.2000.870987
   Chou PA, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.904818
   *CISC INC, 2008, APPR ZETT ER CISC VI
   Deb S, 2005, 2005 IEEE International Symposium on Information Theory (ISIT), Vols 1 and 2, P278
   FRAGOULI C, 2004, P C INF SCI SYST CIS
   Ho T., 2003, P IEEE INT S INF THE
   *ITU T, 2005, ADV VID COD IN PRESS
   Limmanee A, 2008, 2008 5TH INTERNATIONAL SYMPOSIUM ON TURBO CODES AND RELATED TOPICS, P333, DOI 10.1109/TURBOCODING.2008.4658721
   Lin Y., 2007, Proc. ICDCS, P47
   LIU X, 2008, P IEEE INT WORKSH MU
   Luby M, 2002, ANN IEEE SYMP FOUND, P271, DOI 10.1109/SFCS.2002.1181950
   MAYMOUKOV P, 2006, P 44 ALL C COMM CONT
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   RAMASUBRAMANIAN AK, 2009, P SPIE VCIP SAN JOS
   SEJDINOVIC D, 2007, P 41 ANN AS 2007 C S
   Shokrollahi A, 2006, IEEE T INFORM THEORY, V52, P2551, DOI 10.1109/TIT.2006.874390
   Talari A, 2010, IEEE INT SYMP INFO, P2453, DOI 10.1109/ISIT.2010.5513758
   Tarjan R., 1972, SIAM Journal on Computing, V1, P146, DOI 10.1137/0201010
   THOMOS N, 2007, P 1 ACM INT WORKSH M
   THOMOS N, 2009, P INT C MULT EXP 200
   Thomos N, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P497, DOI 10.1109/ICME.2008.4607480
   VUKOBRATOVIC D, 2007, P 45 ANN ALL 2007 C
   Wang H, 2007, GLOB TELECOMM CONF, P2129
   WANG M, 2007, P IEEE INFOCOM ANCH
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wang M, 2007, IEEE T MULTIMEDIA, V9, P1554, DOI 10.1109/TMM.2007.907460
   WU Y, 2008, P 5 IEEE ANN COMM SO, P1
   WU Y, 2008, P IEEE INT S INF THE, P1349
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 40
TC 47
Z9 49
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 776
EP 787
DI 10.1109/TMM.2011.2111364
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300016
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU De Silva, V
   Fernando, A
   Worrall, S
   Kodikara, H
   Kondoz, A
AF De Silva, Varuna
   Fernando, Anil
   Worrall, Stewart
   Kodikara, Hemantha
   Kondoz, Ahmet
TI Sensitivity Analysis of the Human Visual System for Depth Cues in
   Stereoscopic 3-D Displays
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D video; depth perception; just noticeable difference; stereoscopic
   displays
AB Three-dimensional (3-D) displays provide a more realistic experience of entertainment by providing its viewers an added sensation of depth by artificially exploiting light rays to stimulate certain depth cues in the human visual system, especially binocular stereopsis. Due to its close relationship with human visual perception, mass market deployment of 3-D displays will be significantly dependant upon addressing the related perceptual factors such as visual comfort. In order to address the perceptual factors, it is very important to understand how humans experience depth on 3-D displays and how sensitive they are for different depth cues. In this paper, the sensitivity of humans for different depth cues is analyzed as applicable to 3-D viewing on stereoscopic displays. Mathematical models are derived to explain the just noticeable difference in depth (JNDD) for three different depth cues, namely binocular disparity, retinal blur, and relative size. Extensive subjective assessments are performed on a stereoscopic display with passive polarized glasses and on an auto-stereoscopic display to validate the mathematical models for JNDD. It is expected that the proposed models will have important use cases in 3-D display designing as well as 3-D content production.
C1 [De Silva, Varuna; Fernando, Anil; Worrall, Stewart; Kodikara, Hemantha; Kondoz, Ahmet] Univ Surrey, Lab Multimedia Commun Res 1, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP De Silva, V (corresponding author), Univ Surrey, Lab Multimedia Commun Res 1, Guildford GU2 7XH, Surrey, England.
EM D.De-silva@surrey.ac.uk; W.Fernando@surrey.ac.uk;
   S.Worrall@surrey.ac.uk; H.Kodikaraarachchi@surrey.ac.uk;
   A.Kondoz@surrey.ac.uk
OI Kodikara Arachchi, Hemantha/0000-0002-5631-3239; De Silva,
   Varuna/0000-0001-7535-141X
FU European Commission
FX This work is supported by the MUSCADE Integrating Project funded under
   the 7th Framework program of the European Commission. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yap-Peng Tan.
CR Bruce V., 2003, VISUAL PERCEPTION
   Cutting JE, 1995, PERCEPTION SPACE MOT, P69, DOI [DOI 10.1016/B978-012240530-3/50005-5, 10.1016/B978-012240530-3/50005-5]
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Hewage CTER, 2009, IEEE J-STSP, V3, P304, DOI 10.1109/JSTSP.2009.2014805
   Howard Ian P, 1995, Binocular Vision and Stereopsis
   Inoue T, 1997, APPL OPTICS, V36, P4509, DOI 10.1364/AO.36.004509
   *ISO IEC, 2007, JTC1SC29WG11 ISOIEC
   *ISO IEC, 2006, 1SC29WG11 ISOIEC JTC
   Lambooij MTM, 2007, PROC SPIE, V6490, DOI 10.1117/12.705527
   Leon Gustavo, 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P301, DOI 10.1109/3DTV.2008.4547868
   Mather G, 1997, PERCEPTION, V26, P1147, DOI 10.1068/p261147
   Meesters LMJ, 2004, IEEE T CIRC SYST VID, V14, P381, DOI 10.1109/TCSVT.2004.823398
   MIKKOLA M, 2010, P ACM MULT
   Onural L, 2007, P IEEE, V95, P1143, DOI 10.1109/JPROC.2007.896490
   Patterson R, 2007, J SOC INF DISPLAY, V15, P861, DOI 10.1889/1.2812986
   PENTLAND AP, 1987, IEEE T PATTERN ANAL, V9, P523, DOI 10.1109/TPAMI.1987.4767940
   *PHIL 3D SOL, 2008, 3D INT SPEC
   REICHELT S, 2010, P SOC PHOTOOPTICAL I
   Wang B, 2004, VISION RES, V44, P1115, DOI 10.1016/j.visres.2004.01.001
NR 19
TC 40
Z9 45
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 498
EP 506
DI 10.1109/TMM.2011.2129500
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700009
DA 2024-07-18
ER

PT J
AU Vasudevan, R
   Kurillo, G
   Lobaton, E
   Bernardin, T
   Kreylos, O
   Bajcsy, R
   Nahrstedt, K
AF Vasudevan, Ramanarayan
   Kurillo, Gregorij
   Lobaton, Edgar
   Bernardin, Tony
   Kreylos, Oliver
   Bajcsy, Ruzena
   Nahrstedt, Klara
TI High-Quality Visualization for Geographically Distributed 3-D
   Teleimmersive Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-computer interaction; real-time; stereo reconstruction; virtual
   reality; visualization; 3-D teleimmersion; 3-D video
AB The growing popularity of 3-D movies has led to the rapid development of numerous affordable consumer 3-D displays. In contrast, the development of technology to generate 3-D content has lagged behind considerably. In spite of significant improvements to the quality of imaging devices, the accuracy of the algorithms that generate 3-D data, and the hardware available to render such data, the algorithms available to calibrate, reconstruct, and then visualize such data remain difficult to use, extremely noise sensitive, and unreasonably slow. In this paper, we present a multi-camera system that creates a highly accurate (on the order of a centimeter), 3-D reconstruction of an environment in real-time (under 30 ms) that allows for remote interaction between users. This paper focuses on addressing the aforementioned deficiencies by describing algorithms to calibrate, reconstruct, and render objects in the system. We demonstrate the accuracy and speed of our results on a variety of benchmarks and data collected from our own system.
C1 [Vasudevan, Ramanarayan; Kurillo, Gregorij; Bajcsy, Ruzena] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Lobaton, Edgar] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27599 USA.
   [Bernardin, Tony; Kreylos, Oliver] Univ Calif Davis, Inst Data Anal & Visualizat, Davis, CA 95616 USA.
   [Nahrstedt, Klara] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.
C3 University of California System; University of California Berkeley;
   University of North Carolina; University of North Carolina Chapel Hill;
   University of California System; University of California Davis;
   University of Illinois System; University of Illinois Urbana-Champaign
RP Vasudevan, R (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM ramv@eecs.berkeley.edu; gregorij@eecs.berkeley.edu; lobaton@cs.unc.edu;
   tbernardin@cs.ucdavis.edu; kreylos@cs.ucdavis.edu;
   bajcsy@eecs.berkeley.edu; klara@cs.uiuc.edu
RI ; Vasudevan, Ramanarayan/F-2863-2016
OI Lobaton, Edgar/0000-0002-4056-8309; Vasudevan,
   Ramanarayan/0000-0003-1978-0572
FU NSF [0703787, 0724681, 0937060]; Direct For Computer & Info Scie &
   Enginr [0703787] Funding Source: National Science Foundation; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [0724681] Funding Source: National Science Foundation; Directorate For
   Engineering [0931437] Funding Source: National Science Foundation;
   Directorate For Engineering; Div Of Electrical, Commun & Cyber Sys
   [0941382] Funding Source: National Science Foundation; Div Of
   Electrical, Commun & Cyber Sys [0931437] Funding Source: National
   Science Foundation; Div Of Information & Intelligent Systems [0703787]
   Funding Source: National Science Foundation
FX This work was supported in part by NSF (grants: 0703787, 0724681,
   0937060), HP Labs, EADS, and CITRIS at University of California,
   Berkeley. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhengyou Zhang.
CR [Anonymous], 2004, An invitation to 3-D vision
   [Anonymous], P EUR C VIS MED PROD
   [Anonymous], P INT WORKSH IMM TEL
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], J MULTIMEDIA
   BAILENSON JN, MEDIA PSYCH IN PRESS
   Bleyer M, 2005, ISPRS J PHOTOGRAMM, V59, P128, DOI 10.1016/j.isprsjprs.2005.02.008
   Chan TF, 2005, IMAGE PROCESSING AND ANALYSIS, P1, DOI 10.1137/1.9780898717877
   CHENG X, 2000, P IEEE C COMP VIS PA
   DEFANTI T, 1999, P EC NSF WORKSH RES
   FORTE M, 2010, P 16 INT C VIRT SYST
   Gross M, 2003, ACM T GRAPHIC, V22, P819, DOI 10.1145/882262.882350
   Hasenfratz J.-M., 2004, Proceedings of the Tenth Eurographics Conference on Virtual Environments, EGVE'04, (Aire-la-Ville, Switzerland, Switzerland), P147
   Hauberg S, 2010, LECT NOTES COMPUT SC, V6311, P425, DOI 10.1007/978-3-642-15549-9_31
   IHRKE I., 2004, J WSCG, V12, P537
   Kanade T, 1999, MIXED REALITY, P41
   KLAUS A, 2006, P INT C PATT REC, V2
   Kurillo G, 2008, P 2 ACM IEEE INT C D
   Kurillo G, 2008, IEEE VIRTUAL REALITY 2008, PROCEEDINGS, P269
   Kurillo G, 2011, STUD HEALTH TECHNOL, V163, P290, DOI 10.3233/978-1-60750-706-2-290
   Lourakis M., 2004, 340 FORTH I COMP SCI
   Lourakis MIA, 2004, LEVMAR LEVENBERG MAR
   MAUBACH JM, 1995, SIAM J SCI COMPUT, V16, P210, DOI 10.1137/0916014
   Mulligan J, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P959, DOI 10.1109/ICIP.2001.958284
   Nguyen D., 2005, Proceedings of the sigchi conference on human factors in computing systems, P799
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Ponce J, 2002, COMPUTER VISION MODE, DOI 10.5555/580035
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   SCHARSTEIN D, 2003, P 2003 IEEE COMP SOC, V1
   Seitz S.M., 2006, 2006 IEEE COMP SOC C, V1, P519, DOI https://doi.org/10.1109/CVPR.2006.19
   Svoboda T, 2005, PRESENCE-VIRTUAL AUG, V14, P407, DOI 10.1162/105474605774785325
   WANG Z, 2008, P IEEE COMP SOC C CO
   YANG Z, ACM T MULTI IN PRESS
   ZHANG D, 1991, P IEEE RSJ INT WORKS, P292
NR 34
TC 30
Z9 40
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 573
EP 584
DI 10.1109/TMM.2011.2123871
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700016
DA 2024-07-18
ER

PT J
AU Shao, MK
   Dumitrescu, S
   Wu, XL
AF Shao, Mingkai
   Dumitrescu, Sorina
   Wu, Xiaolin
TI Layered Multicast With Inter-Layer Network Coding for Multimedia
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multicast algorithms; multimedia communication; network coding;
   optimization; scalable source coding
AB Multirate multicast is a powerful methodology of multimedia communication in heterogenous networks. A variant of multirate multicast motivated by scalable multimedia streaming is layered multicast, where the transmitted signal is presented in successive data layers. With recent advances of network coding theory, many layered multicast schemes using network coding have been proposed to improve the performance of traditional routing-based layered multicast. They divide the network into different layers and construct a unirate multicast network code for each layer. However, these schemes do not perform network coding between data layers, and consequently cannot realize the full potential of network coding. In this paper, we propose a novel approach to layered multicast that allows network coding of data in different layers. This relaxation lends the proposed scheme greater flexibility in optimizing the data flow than previous layered solutions, and thus achieves higher throughput.
C1 [Shao, Mingkai; Dumitrescu, Sorina; Wu, Xiaolin] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON, Canada.
C3 McMaster University
RP Shao, MK (corresponding author), McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON, Canada.
EM shaomk@grads.ece.mc-master.ca; sorina@mail.ece.mcmaster.ca;
   xwu@ece.mcmaster.ca
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   AHLSWEDE R, 2003, IEEE T NETWORKING, V11, P782
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   Cataldi P, 2010, IEEE T IMAGE PROCESS, V19, P1491, DOI 10.1109/TIP.2010.2042985
   Chen L, 2007, IEEE INFOCOM SER, P1163, DOI 10.1109/INFCOM.2007.139
   Dumitrescu S, 2004, IEEE T MULTIMEDIA, V6, P230, DOI 10.1109/TMM.2003.822793
   Dumitrescu S, 2009, IEEE INFOCOM SER, P442, DOI 10.1109/INFCOM.2009.5061949
   Jaggi S, 2005, IEEE T INFORM THEORY, V51, P1973, DOI 10.1109/TIT.2005.847712
   KAR K, 2001, P IEEE INFOCOM 2001, V1
   LI X, 1998, P IEEE INFORCOM 1998, V3
   MCCANNE S, 1996, P AXM SIGCOMM 96
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   SHAO M, 2008, P ACM MULT 08 OCT 27
   SHAO M, 2008, P NETCOD 2008
   SUNDARAM N, 2005, P ALL 05
   VUVUKOBRATOVIC D, 2009, IEEE T MULTIMEDIA, V11, P1094
   WU Y, 2008, P IEEE INT WORKSH WI
   YEUNG RW, 1995, IEEE T INFORM THEORY, V41, P412, DOI 10.1109/18.370142
NR 19
TC 23
Z9 25
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 353
EP 365
DI 10.1109/TMM.2010.2095833
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800016
DA 2024-07-18
ER

PT J
AU Tindale, A
   Kapur, A
   Tzanetakis, G
AF Tindale, Adam
   Kapur, Ajay
   Tzanetakis, George
TI Training Surrogate Sensors in Musical Gesture Acquisition Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gesture recognition; machine learning; new interfaces for musical
   expression; surrogate sensors; virtual sensors
AB Capturing the gestures of music performers is a common task in interactive electroacoustic music. The captured gestures can be mapped to sounds, synthesis algorithms, visuals, etc., or used for music transcription. Two of the most common approaches for acquiring musical gestures are: 1) "hyper-instruments" which are "traditional" musical instruments enhanced with sensors for directly detecting the gestures and 2) "indirect acquisition" in which the only sensor is a microphone capturing the audio signal. Hyper-instruments require invasive modification of existing instruments which is frequently undesirable. However, they provide relatively straightforward and reliable sensor measurements. On the other hand, indirect acquisition approaches typically require sophisticated signal processing and possibly machine learning algorithms in order to extract the relevant information from the audio signal. The idea of using direct sensor(s) to train a machine learning model for indirect acquisition is proposed in this paper. The resulting trained "surrogate" sensor can then be used in place of the original direct invasive sensor(s) that were used for training. That way, the instrument can be used unmodified in performance while still providing the gesture information that a hyper-instrument would provide. In addition, using this approach, large amounts of training data can be collected with minimum effort. Experimental results supporting this idea are provided in two detection contexts: 1) strike position on a drum surface and 2) strum direction on a sitar.
C1 [Tindale, Adam; Kapur, Ajay; Tzanetakis, George] Univ Victoria, Dept Comp Sci, Dept Elect Engn, Victoria, BC V8S 1P2, Canada.
   [Tindale, Adam; Kapur, Ajay; Tzanetakis, George] Univ Victoria, Fac Mus, Victoria, BC V8S 1P2, Canada.
C3 University of Victoria; University of Victoria
RP Tindale, A (corresponding author), Univ Victoria, Dept Comp Sci, Dept Elect Engn, Victoria, BC V8S 1P2, Canada.
EM art@uvic.ca; akapur@alumni.princeton.edu; gtzan@cs.uvic.ca
RI Tzanetakis, George/I-6593-2013
OI Tzanetakis, George/0000-0002-6844-7912; Tindale,
   Adam/0000-0002-3712-4061
FU Natural Sciences and Engineering Research Council of Canada (NSERC)
FX Manuscript received March 22, 2010; revised July 20, 2010; accepted
   October 08, 2010. Date of publication October 28, 2010; date of current
   version January 19, 2011. This work was supported by the Natural
   Sciences and Engineering Research Council of Canada (NSERC). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Nicu Sebe.
CR [Anonymous], 2000, ISMIR
   [Anonymous], 2000, Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations
   Bagchee Sandeep., 1998, UNDERSTANDING RAGA M
   EGOZY EB, 1995, THESIS MIT CAMBRIDGE
   Frank Eibe, 2001, EUR C MACH LEARN, P145, DOI 10.1007/3-540-44795-413
   GIMENES CJM, 2007, P NEW INT MUS EXPR
   GOUYON F, 2001, P MOSART WORKSH CURR
   Hanzevack EL, 1997, P AMER CONTR CONF, P669, DOI 10.1109/ACC.1997.611885
   Holmes G, 1999, LECT NOTES ARTIF INT, V1747, P1
   Kabadayi Sanem., 2006, P 2006 INT S WORLD W, P587
   KAHN A, 1998, CLASSICAL MUSIC N IN
   KAPUR A, 2004, P C DIG AUD EFF
   KAPUR A, 2004, P C DIG AUD EFF, P17
   KAPUR A, 2004, P INT COMP MUS C ICM
   King D, 2004, IEEE SENS J, V4, P21, DOI 10.1109/JSEN.2003.820344
   Klapuri A, 2006, SIGNAL PROCESSING ME
   Krishnapuram B, 2003, IEEE SENS J, V3, P147, DOI 10.1109/JSEN.2002.805552
   Machover Tod., 1992, Hyperinstruments: A Progress Report, 1987-1991
   MARKO K, 1996, P ART NEUR NETW ICAN, P191
   MATHEWS M, 1989, P INT COMP MUS C ICM
   Miranda, 2006, New Digital Musical Instruments: Control and Interaction Beyond the Keyboard" (Computer Music and Digital Audio Series)
   Mitchell T. M., 1997, MACHINE LEARNING
   ORIO N, 1999, P INT COMP MUS C ICM
   Prokhorov D, 2005, PROCEEDINGS OF THE 2005 INTELLIGENT SENSORS, SENSOR NETWORKS & INFORMATION PROCESSING CONFERENCE, P411
   SILPANPAA J, 2000, DRUM STROKE RECOGNIT
   TINDALE A, 2004, P INT S MUS INF RETR
   TINDALE A, 2004, THESIS MCGILL U MONT
   TRAUBE C, 2000, P C DIG AUD EFF
   Traube C., 2003, P 2003 C NEW INTERFA, P42
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Tzanetakis G., 2008, INTELLIGENT MUSIC IN, P31
   Tzanetakis G, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P37, DOI 10.1109/MMSP.2006.285264
   VALLIS O, 2008, P IEEE INT S MULT
   Wanderley MM, 2004, P IEEE, V92, P632, DOI 10.1109/JPROC.2004.825882
   YOUNG D, 2004, P NEW INT MUS EXPR N
NR 35
TC 4
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 50
EP 59
DI 10.1109/TMM.2010.2089786
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900006
DA 2024-07-18
ER

PT J
AU Lavoué, G
   Corsini, M
AF Lavoue, Guillaume
   Corsini, Massimiliano
TI A Comparison of Perceptually-Based Metrics for Objective Evaluation of
   Geometry Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geometry processing; objective evaluation; perceptual metrics; quality
   evaluation
ID WATERMARKING; MESHES; ERROR
AB Recent advances in 3-D graphics technologies have led to an increasing use of processing techniques on 3-D meshes, such as filtering, compression, watermarking, simplification, deformation, and so forth. Since these processes may modify the visual appearance of the 3-D objects, several metrics have been introduced to properly drive or evaluate them, from classic geometric ones such as Hausdorff distance, to more complex perceptually-based measures. This paper presents a survey on existing perceptually-based metrics for visual impairment of 3-D objects and provides an extensive comparison between them. In particular, different scenarios which correspond to different perceptual and cognitive mechanisms are analyzed. The objective is twofold: 1) catching the behavior of existing measures to help Perception researchers for designing new 3-D metrics and 2) providing a comparison between them to inform and help computer graphics researchers for choosing the most accurate tool for the design and the evaluation of their mesh processing algorithms.
C1 [Lavoue, Guillaume] Univ Lyon, CNRS, F-69621 Villeurbanne, France.
   [Lavoue, Guillaume] INSA Lyon, LIRIS UMR 5205, F-69621 Villeurbanne, France.
   [Corsini, Massimiliano] ISTI CNR, Visual Comp Lab, I-56124 Pisa, Italy.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Consiglio Nazionale delle
   Ricerche (CNR); Istituto di Scienza e Tecnologie dell'Informazione
   "Alessandro Faedo" (ISTI-CNR)
RP Lavoué, G (corresponding author), Univ Lyon, CNRS, F-69621 Villeurbanne, France.
EM glavoue@liris.cnrs.fr; massimiliano.corsini@isti.cnr.it
RI Corsini, Massimiliano/B-6375-2015
OI Corsini, Massimiliano/0000-0003-0543-1638
FU French National Research Agency (ANR) [ANR-07-MDCO-015]; EC IST IP
   [IST-2008-231809]
FX Manuscript received December 15, 2009; revised May 14, 2010; accepted
   July 05, 2010. Date of publication July 23, 2010; date of current
   version October 15, 2010. This work was supported in part by the French
   National Research Agency (ANR) through MADRAS project (ANR-07-MDCO-015)
   and in part by the EC IST IP project "3D-COFORM" (IST-2008-231809). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Nadia Magnenat-Thalmann.
CR Bian Z, 2008, LECT NOTES COMPUT SC, V4975, P62
   Bian Z, 2009, J COMPUT SCI TECH-CH, V24, P65, DOI 10.1007/s11390-009-9198-3
   BOLIN MR, 1998, P 25 ANN C COMP GRAP, P299
   Cho JW, 2007, IEEE T SIGNAL PROCES, V55, P142, DOI 10.1109/TSP.2006.882111
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   CORSINI M, 2005, P WORKSH IM AN MULT
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Daniel W. W., 1999, Biostatistics: A Foundation for Analysis in the Health Sciences, V9 th, DOI DOI 10.2307/2531929
   Dumont R, 2003, ACM T GRAPHIC, V22, P152, DOI 10.1145/636886.636888
   Eckert MP, 1998, SIGNAL PROCESS, V70, P177, DOI 10.1016/S0165-1684(98)00124-8
   FERWERDA J, 2008, P IS T 16 COL IM C
   Ferwerda J. A., 1997, Proc. ACM SIGGRAPH, P143
   Garland M., 1997, PROC 24 C COMPUTER G, P209, DOI DOI 10.1145/258734.258849
   GELASCA ED, 2005, P IEEE INT C IM PROC
   GELASCA ED, 2005, THESIS EPFL LAUSANNE
   Hildebrandt K, 2004, COMPUT GRAPH FORUM, V23, P391, DOI 10.1111/j.1467-8659.2004.00770.x
   Howlett S., 2004, Proceedings of the 1st Symposium on Applied perception in graphics and visualization, APGV'04, P57
   *ITU, 2000, BT50010 ITU
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kim SJ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P276, DOI 10.1109/PCCGA.2002.1167871
   Lavoué G, 2006, PROC SPIE, V6312, DOI 10.1117/12.686964
   Lavoué G, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462052
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Lindstrom P, 2000, ACM T GRAPHIC, V19, P204, DOI 10.1145/353981.353995
   Lubin J., 1995, Vision Models for Target Detection and Recognition, P245
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Myszkowski K., 2002, SCCG'02: Proceedings of the 18th spring conference on Computer graphics, P13
   Ohbuchi R, 2002, COMPUT GRAPH FORUM, V21, P373, DOI 10.1111/1467-8659.t01-1-00597
   Pan YX, 2005, IEEE T MULTIMEDIA, V7, P269, DOI 10.1109/TMM.2005.843364
   Qu LJ, 2008, IEEE T VIS COMPUT GR, V14, P1015, DOI 10.1109/TVCG.2008.51
   Ramanarayanan G, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276472, 10.1145/1239451.1239527]
   Ramasubramanian M, 1999, COMP GRAPH, P73, DOI 10.1145/311535.311543
   REDDY M, 1997, THESIS U EDINBURGH E
   Rogowitz BE, 2001, P SOC PHOTO-OPT INS, V4299, P340, DOI 10.1117/12.429504
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   SORKINE O, 2003, P EUR ACM SIGGRAPH S, P42
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Vallet B, 2008, COMPUT GRAPH FORUM, V27, P251, DOI 10.1111/j.1467-8659.2008.01122.x
   Wang K, 2008, IEEE T INF FOREN SEC, V3, P620, DOI 10.1109/TIFS.2008.2007229
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z., 2006, Modern Image Quality Assessment, DOI 10.2200/S00010ED1V01Y200508IVM003
   Watson A.B., 1993, DIGITAL IMAGES HUMAN, P179
   WILLIAMS N, 2003, P 2003 S INT 3D GRAP, P113
   Wu JH, 2001, NINTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P12, DOI 10.1109/PCCGA.2001.962853
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 45
TC 62
Z9 70
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 636
EP 649
DI 10.1109/TMM.2010.2060475
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500003
DA 2024-07-18
ER

PT J
AU Luo, JC
   Ahmad, I
   Sun, Y
AF Luo, Jiancong
   Ahmad, Ishfaq
   Sun, Yu
TI Controlling the Bit Rate of Multi-Object Videos With Noncooperative Game
   Theory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Game theory; rate control; video compression
ID MPEG-4 RATE CONTROL; RATE CONTROL SCHEME; ALLOCATION
AB This paper proposes an object-level rate control algorithm to jointly controlling the bit rates of multiple video objects. Utilizing noncooperative game theory, the proposed rate control algorithm mimics the behaviors of players representing video objects. Each player competes for available bits to optimize its visual quality. The algorithm finds an "optimal solution" in that it conforms to the mixed strategy Nash equilibrium, which is the probability distribution of the actions carried by the players that maximizes their expected payoffs (the number of bits). The game is played iteratively, and the expected payoff of each play is accumulated. The game terminates when all of the available bits for the specific time instant have been distributed to video object planes (VOPs). The advantage of the proposed scheme is that the bidding objects divide the bits among themselves automatically and fairly, according to their encoding complexity, and with an overall solution that is strategically optimal under the given circumstances. To minimize buffer fluctuation and avoid buffer overflow and under-flow, a proportional-integral-derivative (PID) control based buffer policy is utilized.
C1 [Luo, Jiancong; Ahmad, Ishfaq] Univ Texas Arlington, Dept Comp Sci, Arlington, TX 76010 USA.
   [Sun, Yu] Univ Cent Arkansas, Dept Comp Sci, Conway, AR 72035 USA.
C3 University of Texas System; University of Texas Arlington; University of
   Central Arkansas
RP Luo, JC (corresponding author), Thomson, Princeton, NJ USA.
EM Daniel.luo@thomson.net; iahmad@cse.uta.edu; yusun@uca.edu
CR Ahmad I, 2006, IEEE T CIRC SYST VID, V16, P209, DOI 10.1109/TCSVT.2005.856899
   [Anonymous], 1980, THEORY GAMES EC BEHA
   Chen ZZ, 2006, IEEE T MULTIMEDIA, V8, P1117, DOI 10.1109/TMM.2006.884633
   CHIANG T, 1997, JTCSC29WG11 ISOIEC
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   D'Souza A.F., 1988, DESIGN CONTROL SYSTE
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   Hofbauer J., 1998, EVOLUTIONARY GAMES P
   *ITU T, 2005, H264 ITUT
   Khan S.U., 2006, INT J COMPUTATIONAL, V3, P14
   Khan SU, 2005, 8TH INTERNATIONAL SYMPOSIUM ON PARALLEL ARCHITECTURES, ALGORITHMS AND NETWORKS, PROCEEDINGS, P160, DOI 10.1109/ISPAN.2005.72
   Kim HM, 2003, IEEE T CIRC SYST VID, V13, P432, DOI 10.1109/TCSVT.2003.811606
   KOENEN R, 1999, JTC1SC29WG11 ISOIEC
   Krishna Vijay, 2002, AUCTION THEORY
   LEE CH, 2000, NEUTRON NETWORK NEWS, V10, P6
   LEE HJ, 1997, P 1997 INT C IM PROC, V2, P768
   Lee JW, 2003, IEEE T CIRC SYST VID, V13, P488, DOI 10.1109/TCSVT.2003.813421
   LEMKE CE, 1964, J SOC IND APPL MATH, V12, P413, DOI 10.1137/0112033
   MONTET C, 2004, GAME THEORY EC
   NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529
   NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48
   Nash J, 1953, ECONOMETRICA, V21, P128, DOI 10.2307/1906951
   Nash JF, 1950, ECONOMETRICA, V18, P155, DOI 10.2307/1907266
   Pan F, 2003, IEEE T CIRC SYST VID, V13, P440, DOI 10.1109/TCSVT.2003.811603
   PHILLIPS CL, 1999, BASIC FEEDBACK CONTR
   Ronda JI, 1999, IEEE T CIRC SYST VID, V9, P1243, DOI 10.1109/76.809159
   Roughgarden Tim., 2001, P 33 ANN ACM S THEOR, P104
   Sun Y, 2004, IEEE T CIRC SYST VID, V14, P1167, DOI 10.1109/TCSVT.2004.833164
   Tao B, 2000, IEEE T CIRC SYST VID, V10, P147, DOI 10.1109/76.825868
   Tourapis A. M., 2000, JTC1SC29WG11 ISOIEC
   Vetro A, 1999, IEEE T CIRC SYST VID, V9, P186, DOI 10.1109/76.744285
   VETRO A, 1997, JTC1SC29WG11 ISOIEC
   VISCITO E, 1991, SPIE VISUAL COMMUN I, P58
   von Neumann J, 1928, MATH ANN, V100, P295
   Wang HH, 2005, IEEE T CIRC SYST VID, V15, P1113, DOI 10.1109/TCSVT.2005.852629
NR 36
TC 12
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2010
VL 12
IS 2
BP 97
EP 107
DI 10.1109/TMM.2009.2037385
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 573OA
UT WOS:000275922000001
DA 2024-07-18
ER

PT J
AU De Cock, J
   Notebaert, S
   Lambert, P
   de Walle, RV
AF De Cock, Jan
   Notebaert, Stijn
   Lambert, Peter
   Van de Walle, Rik
TI Architectures for Fast Transcoding of H.264/AVC to Quality-Scalable SVC
   Streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; scalable video coding (SVC); video transcoding
ID VIDEO
AB The scalable extension of H.264/AVC (SVC) was recently standardized, and offers scalability at a minor penalty in rate-distortion efficiency when compared to single-layer H.264/AVC coding. In SVC, a scaled version of the original video sequence can easily be extracted by dropping layers from the stream. However, most of the video content nowadays is still produced in a single-layer format. While decoding and reencoding is a possible solution to introduce scalability in the existing bitstreams, this is an approach which requires a tremendous amount of time and effort. In this paper, we show that transcoding can be used to intelligently derive scalable bitstreams from existing single-layer streams. We focus on SNR scalability, and introduce techniques that are able to create multiple quality layers in the bitstreams. We also discuss bitstream rewriting from SVC to H.264/AVC, and examine how our newly proposed architectures can benefit from the changes that were introduced for bitstream rewriting. Architectures with different rate distribution flexibility and computational complexity are discussed. Rate-distortion performance of transcoding is shown to be comparable to that of reencoding at a fraction of the time needed for the latter.
C1 [De Cock, Jan; Notebaert, Stijn; Lambert, Peter; Van de Walle, Rik] Univ Ghent, IBBT, Dept Elect & Informat Syst, Multimedia Lab Res Grp, B-9050 Ledeberg Ghent, Belgium.
C3 Ghent University
RP De Cock, J (corresponding author), Univ Ghent, IBBT, Dept Elect & Informat Syst, Multimedia Lab Res Grp, B-9050 Ledeberg Ghent, Belgium.
EM Jan.DeCock@UGent.be; Stijn.Notebaert@UGent.be; Peter.Lambert@UGent.be;
   Rik.VandeWalle@UGent.be
RI Lambert, Peter/D-7776-2016
OI Lambert, Peter/0000-0001-5313-4158
FU Ghent University; Interdisciplinary Institute for Broadband Technology
   (IBBT); Institute for the Promotion of Innovation by Science and
   Technology in Flanders (IWT-Flanders); Fund for Scientific
   Research-Flanders (FWO-Flanders); European Union
FX Manuscript received August 12, 2008; revised April 17, 2009. First
   published August 21, 2009; current version published October 16, 2009.
   This work was supported in part by Ghent University, in part by the
   Interdisciplinary Institute for Broadband Technology (IBBT), in part by
   the Institute for the Promotion of Innovation by Science and Technology
   in Flanders (IWT-Flanders), in part by the Fund for Scientific
   Research-Flanders (FWO-Flanders), and in part by the European Union. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Beatrice Pesquet-Popescu.
CR Ahmad I, 2005, IEEE T MULTIMEDIA, V7, P793, DOI 10.1109/TMM.2005.854472
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], 2007, H264 ITUT
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   BARRAU E, 2002, P IEEE INT C IM PROC
   Chen CC., 2005, P IEEE INT S CIRC SY, DOI 10.1109/ISCAS.2005.1464649
   DECOCK J, 2007, P INT S SIGN PROC IT
   DECOCK J, 2008, P IEEE INT C IM PROC
   DECOCK J, 2007, P IEEE INT C IM PROC
   Eleftheriadis A, 2004, IEEE T CIRC SYST VID, V14, P1195, DOI 10.1109/TCSVT.2004.835149
   ELEFTHERIADIS A, 1994, P IEEE INT C IM PROC
   *ITU T, 2003, H264 ITUT
   *ITU T, 2006, H264 ITUT
   *ITU T, 2005, H264 ITUT
   *ITU T, 2004, H264 ITUT
   Lefol D, 2006, IEEE T CONSUM ELECTR, V52, P215
   LEFOL D, 2006, P IEEE INT S CIRC SY
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   NOTEBAERT S, 2006, P PAC RIM C MULT PCM
   SCHWARZ H, 2007, P IEEE INT C IM PROC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SEGALL A, 2006, JVTT061
   SHEN H, 2006, P IEEE INT C IM PROC
   Tan T. K., 2007, VCEGAE10
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   WINKEN M, 2007, P IEEE INT C IM PROC
   Xin J, 2005, P IEEE, V93, P84, DOI 10.1109/JPROC.2004.839620
NR 27
TC 41
Z9 48
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1209
EP 1224
DI 10.1109/TMM.2009.2030606
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300001
OA Green Published
DA 2024-07-18
ER

PT J
AU Akhaee, MA
   Saberian, MJ
   Feizi, S
   Marvasti, F
AF Akhaee, Mohammad A.
   Saberian, Mohammad J.
   Feizi, Soheil
   Marvasti, Farokh
TI Robust Audio Data Hiding Using Correlated Quantization With
   Histogram-Based Detector
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; hard quantization; point-to-point graph (PPG); soft
   quantization
ID WATERMARKING
AB In this paper, two blind audio watermarking methods using correlated quantization for data embedding with histogram-based detector have been proposed. First, a novel mapping called the point-to-point graph (PPG) is introduced. In this mapping, the value of samples is important as well as the correlation among them. As this mapping increases the dimension of the signal, the data embedding procedure (quantization) will be diversified more securely than that of the 1-D domains such as the time or frequency domains. Hence, two watermarking techniques coined as hard and soft quantization methods based on the quantization of the PPG point radii are suggested. The performance of both techniques is analyzed by obtaining the radii distribution of PPG points after watermarking. Experimental results against AWGN attack confirm the validity of theoretical analysis. Moreover, the robustness of the proposed methods against other common attacks such as echo, low pass, resampling, and MP3 are investigated through extensive simulations.
C1 [Akhaee, Mohammad A.; Saberian, Mohammad J.; Feizi, Soheil; Marvasti, Farokh] Sharif Univ Technol, Dept Elect Engn, ACRI, Tehran 1458889694, Iran.
C3 Sharif University of Technology
RP Akhaee, MA (corresponding author), Sharif Univ Technol, Dept Elect Engn, ACRI, Tehran 1458889694, Iran.
EM akhaee@ee.sharif.edu; mjsaberian@ee.sharif.edu; sfeiz@ee.sharif.edu;
   marvasti@sharif.edu
FU Iran Telecommunication Research Center (ITRC)
FX This work was supported by Iran Telecommunication Research Center
   (ITRC). The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Wen Gao.
CR Balado F, 2005, IEEE T SIGNAL PROCES, V53, P4006, DOI 10.1109/TSP.2005.855412
   Barni M., 2004, WATERMARKING SYSTEMS, V1st
   Cayre F, 2005, IEEE T SIGNAL PROCES, V53, P3976, DOI 10.1109/TSP.2005.855418
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   CHEN B, 1999, P IEEE INT C MULT CO, V1
   Chen OTC, 2008, IEEE T AUDIO SPEECH, V16, P629, DOI 10.1109/TASL.2007.913022
   Cohen AS, 2002, IEEE T INFORM THEORY, V48, P1639, DOI 10.1109/TIT.2002.1003844
   Conway J.H., 1988, SPHERE PACKING LATTI
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   Eggers JJ, 2003, IEEE T SIGNAL PROCES, V51, P1003, DOI 10.1109/TSP.2003.809366
   Huang JW, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, PROCEEDINGS, P627
   KIM HO, WAVELET BASED AUDIO
   Malik HMA, 2007, IEEE T AUDIO SPEECH, V15, P1296, DOI 10.1109/TASL.2007.894509
   Moulin P, 2003, IEEE T SIGNAL PROCES, V51, P1098, DOI 10.1109/TSP.2003.809370
   Moulin P, 2003, IEEE T INFORM THEORY, V49, P563, DOI 10.1109/TIT.2002.808134
   Proakis J.G., 2004, Digital Communications
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   Trappe W, 2003, IEEE T SIGNAL PROCES, V51, P1069, DOI 10.1109/TSP.2003.809378
   Wu SQ, 2005, IEEE T BROADCAST, V51, P69, DOI 10.1109/TBC.2004.838265
   Yeung MM, 1998, COMMUN ACM, V41, P30
   Zamir R, 2002, IEEE T INFORM THEORY, V48, P1250, DOI 10.1109/TIT.2002.1003821
   ZHANG Q, 2003, P 41 ANN ALL C COMM
NR 22
TC 31
Z9 32
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 834
EP 842
DI 10.1109/TMM.2009.2012923
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300004
DA 2024-07-18
ER

PT J
AU Tan, WT
   Cheung, G
   Ortega, A
   Shen, B
AF Tan, Wai-Tian
   Cheung, Gene
   Ortega, Antonio
   Shen, Bo
TI Community Streaming With Interactive Visual Overlays: System and
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; video streaming
ID VIDEO
AB Community streaming is an enhanced form of joint content viewing where a sense of community is reinforced by the addition of interactive visual overlays, controlled in real-time by viewers, on top of a shared video stream. As a concrete example, we describe a community video system called ECHO, where personalized avatars are overlaid on top of a real-time encoded video stream of an Internet game for multicast consumption. Recognizing that only the visual overlays are generated live, we propose schemes that encode and schedule the live and non-live portions of the overlaid video separately in order to exploit the difference in delay sensitivity of the two, leading to video streams that contain two sub-streams with different delay constraints. We show that, in the known channel case, a low complexity "earliest deadline first" packet scheduling algorithm minimizes receiver buffer delay. We also analyze the case where multiple streams are multiplexed, which allows us to quantify the potential gains of allowing different delay constraints for different sub-streams. We show that a "water filling" strategy maximizes the total number of streams that can be supported. Simulation results show that the bandwidth necessary to maintain low-latency for visual overlays is reduced by about 40% when our proposed sub-stream approach is used. For multiplexing of multiple streams, our approach can increase the number of supported streams ( e. g., a 30% increase when around ten streams are multiplexed).
C1 [Tan, Wai-Tian] Hewlett Packard Corp, Palo Alto, CA 94304 USA.
   [Cheung, Gene] Hewlett Packard Labs Japan, Tokyo, Japan.
   [Ortega, Antonio] Univ So Calif, Los Angeles, CA 90089 USA.
   [Shen, Bo] Vuclip Com, Milpitas, CA 95035 USA.
C3 Hewlett-Packard; Hewlett-Packard; University of Southern California
RP Tan, WT (corresponding author), Hewlett Packard Corp, 3500 Deer Creek Rd, Palo Alto, CA 94304 USA.
EM wai-tian.tan@hp.com
RI Ortega, Antonio/B-6252-2009; Ortega, Antonio/N-4542-2019; Cheung,
   Gene/AAB-9284-2020
OI Ortega, Antonio/0000-0001-5403-0940; Ortega,
   Antonio/0000-0001-5403-0940; Cheung, Gene/0000-0002-5571-4137
CR *2 LIF, OFF SIT 3D ONL VIRT
   [Anonymous], P IS T SPIE VIS COMM
   Berger AW, 1998, IEEE ACM T NETWORK, V6, P447, DOI 10.1109/90.720887
   CHANG YC, 1998, P IEEE INT C IM PROC, V3, P27
   CHANG YC, 1998, P ICASSP 1998 SEATTL
   CHEUNG G, 2008, P IS T SPIE 15 ANN M
   CHEUNG G, 2004, P IEEE INT C IM PROC
   Hsu CY, 1997, IEEE J SEL AREA COMM, V15, P1016, DOI 10.1109/49.611156
   HUI JY, 1988, IEEE J SEL AREA COMM, V6, P1598, DOI 10.1109/49.12887
   Kahmann V, 2006, COMMUN ACM, V49, P58, DOI 10.1145/1167838.1167864
   Kelly F.P., 1996, Stochastic Networks: Theory and Applications, P141
   Kesidis G, 1993, IEEE ACM T NETWORK, V1, P424, DOI 10.1109/90.251894
   LAKHSMAN TV, 1998, P IEEE, V86, P952
   Salehi JD, 1998, IEEE ACM T NETWORK, V6, P397, DOI [10.1109/90.720873, 10.1142/S0218213097000219]
   Shacham R, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1230812.1230818
   TSE DNC, 1995, IEEE J SEL AREA COMM, V13, P1028, DOI 10.1109/49.400658
   VECIANA G, 1995, QUEUEING SYST, V20, P37
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 18
TC 3
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 986
EP 997
DI 10.1109/TMM.2009.2021797
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, X
   Cheung, G
   Chuah, CN
AF Liu, Xin (Leo)
   Cheung, Gene
   Chuah, Chen-Nee
TI Structured Network Coding and Cooperative Wireless Ad-Hoc Peer-to-Peer
   Repair for WWAN Video Broadcast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cooperative peer-to-peer repair; network coding; wireless wide area
   network (WWAN) video broadcast
AB In a scenario where each peer of an ad-hoc wireless local area network (WLAN) receives one of many available video streams from a wireless wide area network (WWAN), we propose a network-coding-based cooperative repair framework for the ad-hoc peer group to improve broadcast video quality during channel losses. Specifically, we first impose network coding structures globally, and then select the appropriate video streams and network coding types within the structures locally, so that repair can be optimized for broadcast video in a rate-distortion manner. Innovative probability-the likelihood that a repair packet is useful in data recovery to a receiving peer-is analyzed in this setting for accurate optimization of the network codes. Our simulation results show that by using our framework, video quality can be improved by up to 19.71 dB over un-repaired video stream and by up to 5.39 dB over video stream using traditional unstructured network coding.
C1 [Liu, Xin (Leo); Chuah, Chen-Nee] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
   [Cheung, Gene] Hewlett Packard Labs Japan, Tokyo 1680072, Japan.
C3 University of California System; University of California Davis;
   Hewlett-Packard
RP Liu, X (corresponding author), Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
EM xyzliu@ucdavis.edu; gene-cs.cheung@hp.com; chuah@ucdavis.edu
RI Cheung, Gene/AAB-9284-2020
OI Cheung, Gene/0000-0002-5571-4137; Chuah, Chen-Nee/0000-0002-2772-387X
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], QUALNET
   [Anonymous], TS26246 3GPP
   [Anonymous], P 5 INT C MOB SYST A
   [Anonymous], TML PROJECT WEB PAGE
   Chandra S, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK, P329
   Cheung G, 2004, IEEE T MULTIMEDIA, V6, P304, DOI 10.1109/TMM.2003.822794
   Cheung G, 2007, IEEE T CIRC SYST VID, V17, P649, DOI 10.1109/TCSVT.2007.896620
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CROWCROFT J, 1988, P ACM SIGCOMM NEW YO
   Deb S, 2006, IEEE T INFORM THEORY, V52, P2486, DOI 10.1109/TIT.2006.874532
   Dong Nguyen X.Y., 2007, Proc. Packet Video, P326
   Gkantsidis C, 2005, IEEE INFOCOM SER, P2235
   HALLOUSH M, 2008, P IEEE INT C COMM MA
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   Li LE, 2007, IEEE INFOCOM SER, P1739, DOI 10.1109/INFCOM.2007.203
   LIN Y, IEEE T PARR IN PRESS
   LIU X, 2008, P 1 IEEE WORKSH MOB
   LIU X, 2008, P INT WORKSH MULT SI
   LIU X, 2008, P IEEE INT C COMM IC
   Nguyen K, 2008, SPRINGER J SIGNA FEB
   Pering Trevor., 2006, P 4 INT C MOBILE SYS, P220, DOI DOI 10.1145/1134680.1134704
   Raza S, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1075
   Seferoglu H., 2007, Proc. 16th International Packet Video Workshop, Lausanne, P191
   SHARMA P, 2005, P IEEE INT C COMM SE
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   ZFZAL J, 2006, J MULTIMEDIA, V1, P25
NR 27
TC 40
Z9 47
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 730
EP 741
DI 10.1109/TMM.2009.2017636
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900014
DA 2024-07-18
ER

PT J
AU Soltani, S
   Misra, K
   Radha, H
AF Soltani, Sohraab
   Misra, Kiran
   Radha, Hayder
TI Delay Constraint Error Control Protocol for Real-Time Video
   Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Channel coding; forward error correction; link-layer; multimedia
   communication; wireless LAN
AB Real-time video communication over wireless channels is subject to information loss since wireless links are error-prone and susceptible to noise. Popular wireless link-layer protocols, such as retransmission (ARQ) based 802.11 and hybrid ARQ methods provide some level of reliability while largely ignoring the latency issue which is critical for real-time applications. Therefore, they suffer from low throughput (under high-error rates) and large waiting-times leading to serious degradation of video playback quality. In this paper, we develop an analytical framework for video communication which captures the behavior of real-time video traffic at the wireless link-layer while taking into consideration both reliability and latency conditions. Using this framework, we introduce a delay constraint packet embedded error control (DC-PEEC) protocol for wireless link-layer. DC-PEEC ensures reliable and rapid delivery of video packets by employing various channel codes to minimize fluctuations in throughput and provide timely arrival of video. In addition to theoretically analyzing DC-PEEC, the performance of the proposed scheme is analyzed by simulating real-time video communication over "real" channel traces collected on 802.11b WLANs using H.264/AVC JM14.0 video codec. The experimental results demonstrate performance gains of 5-10 dB for different real-time video scenarios.
C1 [Soltani, Sohraab] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
   [Misra, Kiran; Radha, Hayder] Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
C3 Michigan State University; Michigan State University
RP Soltani, S (corresponding author), Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
EM soltanis@cse.msu.edu; misrakir@egr.msu.edu; radha@egr.msu.edu
FU NSF [CNS-0721550, CCF-0515253]
FX Manuscript received September 02, 2008; revised January 13, 2009. First
   published April 17. 2009; current version published May 15, 2009. This
   work was supported in part by NSF Awards CNS-0721550 and CCF-0515253.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Beatrice Pesquet-Popescu.
CR [Anonymous], OMNET COMMUNITY
   [Anonymous], 1999, 80211 IEEE WG 11
   [Anonymous], SOFTWARE LOW DENSITY
   [Anonymous], 2003, Introduction to Probability Models
   [Anonymous], REED SOLOMON SOURCE
   Caire G, 2001, IEEE T INFORM THEORY, V47, P1971, DOI 10.1109/18.930931
   Cover T. M., 1991, ELEMENTS INFORM THEO
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   KARANDE S, 2007, P IEEE INT C COMM IC
   Karande SS, 2007, IEEE T MULTIMEDIA, V9, P307, DOI 10.1109/TMM.2006.886280
   Mandelbamn M, 2007, TOP, V15, P281, DOI 10.1007/s11750-007-0018-z
   RICHARDSON EG, 2002, VIDEO CODEC DESIGN D
   Shakkottai S, 2003, IEEE COMMUN MAG, V41, P74, DOI 10.1109/MCOM.2003.1235598
   SOLJANIN E, 2003, P DIMACS WORKSH NETW
   Soltani S, 2008, IEEE J SEL AREA COMM, V26, P1376, DOI 10.1109/JSAC.2008.081004
   STRINATI EC, 2003, IEEE VTC, V4, P2735
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   ZHAO JA, 2004, ACM KLUWER J WIRELES, V10, P130
   H 264 AVC SOFTWARE C
NR 19
TC 23
Z9 23
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 742
EP 751
DI 10.1109/TMM.2009.2017622
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900015
DA 2024-07-18
ER

PT J
AU Chi, Y
   Zhu, SH
   Hino, KJ
   Gong, YH
   Zhang, Y
AF Chi, Yun
   Zhu, Shenghuo
   Hino, Koji
   Gong, Yihong
   Zhang, Yi
TI iOLAP: A Framework for Analyzing the Internet, Social Networks, and
   Other Networked Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Information filtering; knowledge management applications; modeling
   structured; personalization; textual and multimedia data; web mining
ID LATENT SEMANTIC ANALYSIS
AB As the amount of noisy, unorganized, linked data on the Internet increases dramatically, how to efficiently analyze such data becomes a challenging research problem. In this paper, we propose a framework, iOLAP, that offers functionalities for analyzing networked data from Internet, social networks, scientific paper citations, etc. We first identify four main data dimensions that are common in most of networked data, namely people, relation, content, and time. Motivated by the fact that various dimensions of data jointly affect each other, we propose a polyadic factorization approach to directly model all the dimensions simultaneously in a unified framework. We provide detailed theoretical analysis of the new modeling framework. In addition to the theoretical framework, we also present an efficient implementation of the algorithm that takes advantage of the sparseness of data and has time complexity linear in the number of data records in a dataset. We then apply the proposed models to analyzing the ogosphere and personalizing recommendation in paper citations. Extensive experimental studies showed that our framework is able to provide deep insights jointed obtained from various dimensions of networked data.
C1 [Chi, Yun; Zhu, Shenghuo; Hino, Koji; Gong, Yihong] NEC Labs Amer, Cupertino, CA 95014 USA.
   [Zhang, Yi] Univ Calif Santa Cruz, Sch Engn, Santa Cruz, CA 95064 USA.
C3 NEC Corporation; University of California System; University of
   California Santa Cruz
RP Zhang, Y (corresponding author), NEC Labs Amer, Cupertino, CA 95014 USA.
EM ychi@sv.nec-labs.com; zsh@sv.nec-labs.com; hino@sv.nec-labs.com;
   ygong@sv.nec-labs.com; yiz@soe.ucsc.edu
OI Zhang, Yi/0000-0003-4299-1511
FU National Science Foundation [IIS-0713111]; AFOSR
FX Manuscript received June 06, 2008; revised November 13, 2008. Current
   version published March 18, 2009. The work of Y. Zhang was supported in
   part by National Science Foundation IIS-0713111 and in part by AFOSR.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lexing Xie.
CR [Anonymous], TR98042 INT COMP SCI
   [Anonymous], P 7 SIAM INT C DAT M
   ASUR S, 2007, P 13 ACM SIGKDD C
   Bader BW, 2006, ACM T MATH SOFTWARE, V32, P635, DOI 10.1145/1186785.1186794
   CHI Y, 2008, P 17 ACM CIKM C
   De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696
   De Nooy W., 2018, EXPLORATORY SOCIAL N
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   DING C, 2005, P SIAM SDM
   Gaussier E., 2005, P SIGIR
   GRAY JAM, 1997, EVIDENCE BASED HLTH, V1, P1
   Han J., 2005, DATA MINING CONCEPTS
   Harshman Richard A., 1970, FDN PARAFAC PROCEDUR
   HAZAN T, 2005, P 10 IEEE INT C COMP
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Lee D. D., 2000, P NIPS
   LIN YR, 2008, P 17 WWW C
   MEI Q, 2008, P 17 WWW C
   MORUP M, 2007, ALGORITHMS SPARSE NO
   Palla G, 2007, NATURE, V446, P664, DOI 10.1038/nature05670
   SHASHUA A, 2005, P 22 INT C MACH LEAR
   TOYODA M, 2003, P 14 ACM C HYP HYP H
   TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464
   ZHOU D, 2008, P WWW
   Zhu S., 2007, P SIGIR
NR 25
TC 11
Z9 11
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 372
EP 382
DI 10.1109/TMM.2009.2012912
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yu, HF
   Ho, PH
   Yang, HC
AF Yu, Hsiang-Fu
   Ho, Pin-Han
   Yang, Hung-Chang
TI Generalized Sequence-Based and Reverse Sequence-Based Models for
   Broadcasting Hot Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hot-video broadcasting; video-on-demand (VOD); buffers; cable TV
ID SEAMLESS CHANNEL TRANSITION; ON-DEMAND; RECEIVING SCHEME; POPULAR
   VIDEOS; SERVICE
AB It has been well recognized as an efficient approach for broadcasting popular videos by partitioning a video data stream into multiple segments and launching each segment through an individual channel simultaneously and periodically. Based on the design premises, some recent studies, including skyscraper broadcasting (SkB), client-centric approach (CCA), greedy disk-conserving broadcasting (GDB), and reverse fast broadcasting (RFB) schemes, etc., have been reported. To study the client segment downloading process, this paper first introduces an applicable sequence-based broadcasting model that can be used to minimize the required buffer size. By extending RFB, this paper further proposes a reverse sequence-based broadcasting model, which can generally improve the existing schemes such as SkB, CCA, GDB, and FB in terms of the relaxed client buffer size. To have a deeper understanding on the proposed reverse model, the upper bound of the client buffer requirement is obtained through a comprehensive analysis, which is proved to be much smaller than the conventional sequence model by 25% to 50%. Based on the proposed reverse model, a reverse sequence-based broadcasting scheme is developed for achieving smaller delay than CCA and GDB.
C1 [Yu, Hsiang-Fu] Natl Taipei Univ Educ, Dept Comp Sci, Taipei 10671, Taiwan.
   [Ho, Pin-Han] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
   [Yang, Hung-Chang] Natl Cent Univ, Dept Comp Sci & Informat Engn, Jhongli 32001, Taoyuan County, Taiwan.
C3 National Taipei University of Education; University of Waterloo;
   National Central University
RP Yu, HF (corresponding author), Natl Taipei Univ Educ, Dept Comp Sci, Taipei 10671, Taiwan.
EM yu@tea.ntue.edu.tw; pinhan@bbcr.uwaterloo.ca; cyht@dslab.csie.ncu.edu.tw
FU National Science Council of Taiwan [NSC 97-2221-E-152-002]
FX Manuscript received March 16, 2008; revised September 10, 2008. First
   published December 22, 2008; current version published January 09, 2009.
   This work was supported in part by the National Science Council of
   Taiwan under Contract NSC 97-2221-E-152-002. The associate editor
   coordinating the review of his manuscript and approving it for
   publication was Dr. Gene Cheung.
CR Cai YD, 2001, BMC BIOINFORMATICS, V2, DOI 10.1186/1471-2105-2-3
   Chien WD, 2005, IEEE T BROADCAST, V51, P360, DOI 10.1109/TBC.2005.852251
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   Gao LX, 2002, MULTIMEDIA SYST, V8, P284, DOI 10.1007/s005300100049
   Guo Y, 2004, IEEE T MULTIMEDIA, V6, P387, DOI 10.1109/TMM.2003.822786
   Hu AL, 2001, IEEE INFOCOM SER, P508, DOI 10.1109/INFCOM.2001.916754
   HUA KA, 1997, ACM SIGCOMM      SEP
   Juhn LS, 1997, IEEE T CONSUM ELECTR, V43, P1110, DOI 10.1109/30.642378
   Juhn LS, 1998, IEEE T BROADCAST, V44, P100, DOI 10.1109/11.713059
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Pâris JF, 1998, IEEE IC COMP COM NET, P690, DOI 10.1109/ICCCN.1998.998831
   Sheu JP, 2004, IEEE T BROADCAST, V50, P120, DOI 10.1109/TBC.2004.828754
   Tantaoui MA, 2004, IEEE T BROADCAST, V50, P289, DOI 10.1109/TBC.2004.834202
   Tian RX, 2005, IEEE T CIRC SYST VID, V15, P961, DOI 10.1109/TCSVT.2005.852416
   Tseng YC, 2004, IEEE ACM T NETWORK, V12, P559, DOI 10.1109/TNET.2004.828965
   Tseng YC, 2002, IEEE T COMMUN, V50, P1348, DOI 10.1109/TCOMM.2002.801466
   Tseng YC, 2001, IEEE T COMMUN, V49, P863, DOI 10.1109/26.923810
   Yang ZY, 1999, IEEE T BROADCAST, V45, P318, DOI 10.1109/11.796274
   YU HF, MULTIMEDIA  IN PRESS
   YU HF, 2008, IEEE T BROADCASTING, V54
   Yu HF, 2007, IEEE T BROADCAST, V53, P103, DOI 10.1109/TBC.2006.888917
NR 21
TC 5
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 152
EP 165
DI 10.1109/TMM.2008.2008931
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700013
DA 2024-07-18
ER

PT J
AU Atrey, PK
   El Saddik, A
AF Atrey, Pradeep K.
   El Saddik, Abdulmotaleb
TI Confidence Evolution in Multimedia Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Agreement coefficient; confidence evolution; multimedia assimilation
AB Multimedia systems utilize multiple media streams, each of which have different confidence levels in accomplishing various detection tasks. For example, in a multimedia surveillance system, one would usually have higher confidence in an audio stream compared to a video stream for detecting human shouting events. The pre-computation of these confidence levels is cumbersome especially when new media streams are dynamically added to the system. This paper proposes a novel method, which dynamically computes the confidence levels of new streams based on the past history of their agreement/disagreement with the already trusted streams. To demonstrate the utility of the proposed method, we provide the experimental results for detecting events in a multimedia surveillance scenario.
C1 [Atrey, Pradeep K.; El Saddik, Abdulmotaleb] Univ Ottawa, Fac Engn, Multimedia Commun Res Lab, Sch Informat Technol & Engn, Ottawa, ON K1N 6N5, Canada.
C3 University of Ottawa
RP Atrey, PK (corresponding author), Univ Ottawa, Fac Engn, Multimedia Commun Res Lab, Sch Informat Technol & Engn, Ottawa, ON K1N 6N5, Canada.
EM patrey@mcrlab.uottawa.ca; abed@mcrlab.uottawa.ca
RI /D-4159-2009
OI /0000-0002-7690-8547
CR [Anonymous], P 2 INT WORKSH NETW
   ATREY PK, 2007, P 13 INT C MULT MOD, P155
   Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   CONAIRE CO, 2006, INT C COMP VIS THEOR
   De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397, DOI 10.1109/ICICS.1997.647126
   Frolik J, 2001, IEEE T INSTRUM MEAS, V50, P1761, DOI 10.1109/19.982977
   Hsu WHM, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1091, DOI 10.1109/ICME.2004.1394400
   HUGHES K, 1993, P IEEE INT C ROB AUT, V2, P136
   IOANNOU S, 2005, FUZZ IEEE
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Kankanhalli MS, 2006, IEEE T MULTIMEDIA, V8, P937, DOI 10.1109/TMM.2006.879876
   LUO J, 2004, P IEEE COMP SOC C CO
   Oommen BJ, 2005, ARTIF INTELL, V164, P1, DOI 10.1016/j.artint.2002.02.001
   SIEGEL M, 2004, P IEEE INT WORKSH RO, P96
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   TATBUL N, 2004, WORKSH DAT MAN SENS
   YU D, 2005, P 43 ACM SE C KENN G
NR 17
TC 7
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1288
EP 1298
DI 10.1109/TMM.2008.2004907
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700006
DA 2024-07-18
ER

PT J
AU Jang, U
   Lee, H
   Lee, S
AF Jang, Uk
   Lee, Hyungkeuk
   Lee, Sanghoon
TI Optimal Carrier Loading Control for the Enhancement of Visual Quality
   over OFDMA Cellular Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer optimization; orthogonal frequency division multiplexing
   (OFDM); orthogonal frequency division multiple access (OFDMA); layered
   video coding; loading ratio; visual throughput; wireless video
ID VIDEO TRANSMISSION; WIRELESS NETWORKS; RESOURCE-ALLOCATION; SCALABLE
   VIDEO; POWER ALLOCATION; MULTIUSER OFDM; CDMA NETWORKS; SYSTEMS; VISION;
   MODEL
AB A recent dynamic increase in demand for wireless multimedia services has greatly accelerated the research on dynamic channel adaptation of high quality video applications. In this paper, we explore a theoretical approach to cross-layer optimization between multimedia and wireless networks by means of a quality criterion termed "visual throughput" for downlink video transmission using a layered coding algorithm. We obtain the optimal loading ratio of orthogonal frequency division multiple access (OFDMA) subcarriers through an optimization problem balancing the tradeoff relationship between inter-cell interference (ICI) and channel throughput. Through numerical link capacity analysis, we show that the upper bounds of the visual throughput gain at the cell boundary is obtained at about 27%.
C1 [Jang, Uk; Lee, Hyungkeuk; Lee, Sanghoon] Yonsei Univ, Wireless Network Lab, Dept Elect & Elect Engn, Seoul 120749, South Korea.
C3 Yonsei University
RP Jang, U (corresponding author), Yonsei Univ, Wireless Network Lab, Dept Elect & Elect Engn, Seoul 120749, South Korea.
EM ukjang@yonsei.ac.kr; punktank@yonsei.ac.kr; slee@yonsei.ac.kr
RI Lee, Sanghoon/A-3430-2019
OI Lee, Sanghoon/0000-0001-9895-5347
CR *3GPP, 25913V720 3GPP TR
   [Anonymous], 80216A IEEE
   [Anonymous], 2013, Wiley Series in Discrete Mathematics and Optimization
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], DIGITAL COMMUNICATIO
   Arnow TL, 1996, P SOC PHOTO-OPT INS, V2674, P119, DOI 10.1117/12.237500
   Atarashi H, 2003, IEICE T COMMUN, VE86B, P291
   Chan YS, 2003, IEEE J SEL AREA COMM, V21, P1516, DOI 10.1109/JSAC.2003.815228
   CHOI YJ, 2006, P ICC 2006, V10, pR30
   CHUANG J, 1998, P IEEE GLOB TEL C GL, V2, P974
   CONCI N, 2005, IEEE INT C IM PROC G, V1, P11
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   Geisler W. S., 1998, P SOC PHOTO-OPT INS, V3299
   Hui SY, 2003, IEEE COMMUN MAG, V41, P54, DOI 10.1109/MCOM.2003.1252799
   *IEEE, 1997, 80111 IEEE
   Jang JH, 2003, IEEE J SEL AREA COMM, V21, P171, DOI 10.1109/JSAC.2002.807348
   Jiang H, 2005, IEEE COMMUN MAG, V43, P120, DOI 10.1109/MCOM.2005.1561929
   Kondi LP, 2002, IEEE T IMAGE PROCESS, V11, P1043, DOI 10.1109/TIP.2002.802507
   Lambrecht CJV, 1998, SIGNAL PROCESS, V67, P255, DOI 10.1016/S0165-1684(98)00043-7
   Lee C, 2006, OPT ENG, V45, DOI 10.1117/1.2160515
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   Sabir MF, 2006, IEEE T IMAGE PROCESS, V15, P1349, DOI 10.1109/TIP.2006.871118
   Schwarz H, 2006, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2006.312374
   Song GC, 2005, IEEE T WIREL COMMUN, V4, P614, DOI 10.1109/TWC.2004.843065
   Wang HH, 2005, IEEE T CIRC SYST VID, V15, P1505, DOI 10.1109/TCSVT.2005.857305
   Wang Z, 2001, IEEE T IMAGE PROCESS, V10, P1397, DOI 10.1109/83.951527
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   Winkler S, 1999, SIGNAL PROCESS, V78, P231, DOI 10.1016/S0165-1684(99)00062-6
   Wong CY, 1999, IEEE J SEL AREA COMM, V17, P1747, DOI 10.1109/49.793310
   Wu DP, 2001, P IEEE, V89, P6, DOI 10.1109/5.904503
   Zhang Jianhai, 2007, Marine Science Bulletin (Beijing), V9, P3
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   Zhao SJ, 2002, IEEE T CIRC SYST VID, V12, P425, DOI 10.1109/TCSVT.2002.800336
NR 34
TC 18
Z9 18
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1181
EP 1196
DI 10.1109/TMM.2008.2001380
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600020
DA 2024-07-18
ER

PT J
AU De Vieeschouwer, C
   Frossard, P
AF De Vieeschouwer, Christophe
   Frossard, Pascal
TI Dependent packet transmission policies in rate-distortion optimized
   media scheduling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID IMAGE COMPRESSION; VIDEO
AB This paper addresses the problem of streaming packetized media over a lossy packet network, with sender-driven (re)transmissions and receiver acknowledgments. It extends the Markovian formulation of the rate-distortion optimized (RaDiO) streaming framework by allowing the transmission schedule of a media data unit to become contingent on the acknowledgements relative to other data units. Media decoding dependencies are generally considered in state-of-the-art rate-distortion optimized scheduling algorithms. However, the set of eligible packet schedules are restricted to independent streaming policies, where the transmission strategy envisioned for a data unit at future transmission opportunities only depends on its own acknowledgment, and not on the acknowledgments received for other data units. This paper questions the validity of this assumption in the design of rate-distortion optimal streaming solutions, and provides a first attempt in the formal derivation of the benefit offered by dependent policies. One of the main contributions of our paper is to propose a methodology that limits the search space of dependent policies to relevant dependencies that are likely to bring a rate-distortion benefit, in order to solve an optimization problem that is a priori computationally intractable. Extensive simulations validate the proposed approach that focuses on relevant dependencies between streaming policies. We further show that the benefit of dependent streaming policies is actually marginal in practical scenarios where the gain in distortion per unit of rate decreases along the media decoding dependency path. It represents the first demonstration that the common assumption of independent streaming policies is valid in many common streaming scenarios. However, experimental results also demonstrate significant benefits and encourage a careful investigation of dependent policies when the content is characterized by an increase of the benefit per transmission unit brought along the data unit dependency path.
C1 UCL, TELE, B-1348 Louvaine La Neuve, Belgium.
   Ecole Polytech Fed Lausanne, FSTI, ITS, LTS4, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP De Vieeschouwer, C (corresponding author), UCL, TELE, B-1348 Louvaine La Neuve, Belgium.
EM devlees@tele.ucl.ac.be; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
CR BEGEN A, 2003, P IEEE INT C IM PROC
   Chakareski J, 2005, IEEE T CIRC SYST VID, V15, P1257, DOI 10.1109/TCSVT.2005.854227
   CHAKARESKI J, 2003, P IEEE INT C IM PROC
   CHAKARESKI J, 2003, P ACM MULTIMEDIA
   CHAKARESKI J, 2004, P IEEE DAT COMPR C S
   CHAKARESKI J, 2003, P IEEE DAT COMPR C D
   CHAKARESKI J, 2002, IEEE WORKSH MULT SIG
   CHAKARESKI J, 2004, P PACK VID WORKSH IR
   CHANG CL, 2004, P IEEE C MULT EXP IC
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   De Vleeschouwer C, 2007, IEEE T MULTIMEDIA, V9, P348, DOI 10.1109/TMM.2006.886283
   DEVLEESCHOUWER C, 2006, TRTS2006008
   GIROD B, 2002, P IEEE INT C IM PROC
   KALMAN M, 2004, P PACKET VID WORKSH
   LI D, 2004, P IEEE ICIP SING OCT
   LIANG Y, 2003, P IEEE INT C IM PROC
   MATSUYAM T, 2004, IEEE T CIRCUITS SYST, V9, P357
   MATTHEW G, 2001, J VLSI SIGNAL PROC, V27, P81
   MIAO Z, 2000, P AS C SIGN SYST COM
   MULLER K, 2005, P IEEE INT C IM PROC
   Podolsky M, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P591, DOI 10.1109/MMSP.1998.739045
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   RAMANATHAN P, 2004, P INT PICT COD S PCS
   Röder M, 2004, IEEE DATA COMPR CONF, P192
   SETTON E, 2004, P IEEE WORKSH MULT S
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   TIAN D, 2004, P IEEE WIR COMM NETW
   Waschbüsch M, 2005, VISUAL COMPUT, V21, P629, DOI 10.1007/s00371-005-0346-7
NR 28
TC 6
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1241
EP 1258
DI 10.1109/TMM.2007.902869
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000014
OA Green Published
DA 2024-07-18
ER

PT J
AU Snoek, CGM
   Huurnink, B
   Hollink, L
   de Rijke, M
   Schreiber, G
   Worring, M
AF Snoek, Cees G. M.
   Huurnink, Bouke
   Hollink, Laura
   de Rijke, Maarten
   Schreiber, Guus
   Worring, Marcel
TI Adding semantics to detectors for video retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE concept learning; content analysis and indexing; knowledge modeling;
   multimedia information systems; video retrieval
ID IMAGE RETRIEVAL; FEATURES; QUERY; MODEL; GAP
AB In this paper, we propose an automatic video retrieval method based on high-level concept detectors. Research in video analysis has reached the point where over 100 concept detectors can be learned in a generic fashion, albeit with mixed performance. Such a set of detectors is very small still compared to ontologies aiming to capture the full vocabulary a user has. We aim to throw a bridge between the two fields by building a multimedia thesaurus, i.e., a set of machine learned concept detectors that is enriched with semantic descriptions and semantic structure obtained from WordNet. Given a multimodal user query, we identify three strategies to select a relevant detector from this thesaurus, namely: text matching, ontology querying, and semantic visual querying. We evaluate the methods against the automatic search task of the TRECVID 2005 video retrieval benchmark, using a news video archive of 85 h in combination with a thesaurus of 363 machine learned concept detectors. We assess the influence of thesaurus size on video search performance, evaluate and compare the multimodal selection strategies for concept detectors, and finally discuss their combined potential using oracle fusion. The set of queries in the TRECVID 2005 corpus is too small for us to be definite in our conclusions, but the results suggest promising new lines of research.
C1 Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 SJ Amsterdam, Netherlands.
   Free Univ Amsterdam, FEW, Dept Comp Sci, NL-1081 HV Amsterdam, Netherlands.
C3 University of Amsterdam; Vrije Universiteit Amsterdam
RP Snoek, CGM (corresponding author), Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM cgmsnoek@science.uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Snoek, Cees/0000-0001-9092-1556; de Rijke, Maarten/0000-0002-1086-0202;
   Worring, Marcel/0000-0003-4097-4136
CR ADAMS B, 2002, P 11 TEXT RETR C GAI
   AMIR A, 2003, P TRECVID WORKSH GAI
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 1997, An Algorithm for Suffix Stripping, page
   [Anonymous], 1993, Proceedings of the 3rd DARPA Worshop on Human Language Technology
   [Anonymous], P 13 ACM INT C MULT
   [Anonymous], 1990, Building Large Knowledge-Based Systems: Representation and Inference in the CYC Project
   BERTINI M, 2005, P 13 ANN ACM INT C M, P395
   Brown M. G., 1995, P ACM MULT SAN FRANC
   Budanitsky A, 2006, COMPUT LINGUIST, V32, P13, DOI 10.1162/coli.2006.32.1.13
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   CHRISTE Y, 1995, CAH CIVILIS MEDIEVAL, V38, P4
   CHUA TS, 2004, P TRECVID WORKSH GAI
   DelBimbo A, 1997, IEEE T PATTERN ANAL, V19, P121, DOI 10.1109/34.574790
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Fox E. A., 1994, Second Text REtrieval Conference (TREC-2) (NIST-SP 500-215), P243
   Gevers T, 2000, IEEE T IMAGE PROCESS, V9, P102, DOI 10.1109/83.817602
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   HOLLINK L, 2005, P 13 ANN ACM INT C M, P479
   HOLLINK L, 2003, P K CAP 2003 WORKSH
   HOLLINK L, 2006, THESIS VRIJE U AMSTE
   Hollink V, 2004, INFORM RETRIEVAL, V7, P33, DOI 10.1023/B:INRT.0000009439.19151.4c
   Hoogs A, 2003, PROC CVPR IEEE, P327
   HYVONEN E, 2004, P ECAI WORKSH APPL S
   IYENGAR G, 2005, P 13 ACM INT C MULT, P21
   KAMPS J, 2004, P 19 ANN ACM S APPL, P1073
   KENNEDY L, 2005, P ACM INT C MULT, P882
   Lienhart R, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P509, DOI 10.1109/MMCS.1997.609763
   Lin C., 2003, P TRECVID WORKSH GAI
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Ma WY, 1999, MULTIMEDIA SYST, V7, P184, DOI 10.1007/s005300050121
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Neo SY, 2006, LECT NOTES COMPUT SC, V4071, P143
   Platt JC, 2000, ADV NEUR IN, P61
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Resnik P, 1995, INT JOINT CONF ARTIF, P448
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   SALTON G, 1972, COMMUN ACM, V15, P658, DOI 10.1145/361454.361509
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Sang EFTK, 2002, J MACH LEARN RES, V2, P559, DOI 10.1162/153244302320884542
   Schroer R, 2001, IEEE AERO EL SYS MAG, V16, P3
   SMEATON A, 2005, CIVR, V3568, P19
   Smeaton A. F., 1996, SIGIR Forum, P174
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Smith E, 1997, ENVIRON TOXICOL PHAR, V4, P3, DOI 10.1016/S1382-6689(97)10035-7
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P280, DOI 10.1109/TMM.2006.886275
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   VANGEMERT JC, 2006, P INT WORKSH SEM LEA
   Vapnik V.N., 2000, The Nature of Statistical Learning Theory, DOI DOI 10.1007/978-1-4757-3264-1_1
   Volkmer T., 2005, PROC 13 ANN ACM INT, P892, DOI DOI 10.1145/1101149.1101341
   Volkmer T, 2007, IEEE T MULTIMEDIA, V9, P967, DOI 10.1109/TMM.2007.900153
   Westerveld T, 2003, EURASIP J APPL SIG P, V2003, P186, DOI 10.1155/S111086570321101X
   Worring M, 2007, IEEE T MULTIMEDIA, V9, P909, DOI 10.1109/TMM.2007.898913
   Yan R., 2004, PROC ACM INT C MULTI, P548
   2006, LUCENE SEARCH ENGINE
NR 58
TC 88
Z9 94
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 975
EP 986
DI 10.1109/TMM.2007.900156
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800007
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Wong, HS
   Ma, B
   Yu, ZW
   Yeung, PF
   Ip, HHS
AF Wong, Hau-Sad
   Ma, Bo
   Yu, Zhiwen
   Yeung, Pui Fong
   Ip, Horace H. S.
TI 3-D head model retrieval using a single face view query
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D model retrieval; kernel principal component analysis; neural network
ID FUZZY C-MEANS
AB In this paper, a novel 3-D head model retrieval approach is proposed, in which only a single 2-D face view query is required. The proposed approach will be important for multimedia application areas such as virtual world construction and game design, in which 3-D virtual characters with a given set of facial features can be rapidly constructed based on 2-D view queries, instead of having to generate each model anew. To achieve this objective, we construct an adaptive mapping through which each 2-D view feature vector is associated with its corresponding 3-D model feature vector. Given this estimated 3-D model feature vector, similarity matching can then be performed in the 3-D model feature space. To avoid the explicit specification of the complex relationship between the 2-D and 3-D feature spaces, a neural network approach is adopted in which the required mapping is implicitly specified through a set of training examples. In addition, for efficient feature representation, principal component analysis (PCA) is adopted to achieve dimensionality reduction for facilitating both the mapping construction and the similarity matching process. Since the linear nature of the original PCA formulation may not be adequate to capture the complex characteristics of 3-D models, we also consider the adoption of its nonlinear counterpart, i.e., the so-called kernel PCA approach, in this work. Experimental results show that the proposed approach is capable of successfully retrieving the set of 3-D models which are similar in appearance to a given 2-D face view.
C1 City Univ Hong Kong, Dept Comp Sci, Virtual Real Lab, Image Comp Grp, Hong Kong, Hong Kong, Peoples R China.
   Beijing Inst Technol, Dept Comp Sci, Beijing 100081, Peoples R China.
C3 City University of Hong Kong; Beijing Institute of Technology
RP Wong, HS (corresponding author), City Univ Hong Kong, Dept Comp Sci, Virtual Real Lab, Image Comp Grp, Hong Kong, Hong Kong, Peoples R China.
EM cshswong@cityu.edu.hk
OI WONG, Hau-San/0000-0002-1530-7529; IP, Ho Shing
   Horace/0000-0002-1509-9002
CR BEZDEK JC, 1984, COMPUT GEOSCI, V10, P191, DOI 10.1016/0098-3004(84)90020-7
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Blanz Volker., 1999, P 26 ANN C COMPUTER, P187, DOI DOI 10.1145/311535.311556
   CHEUNG KKT, 2003, P 6 INT C VIS INF SY, P420
   DAI DQ, 2003, PATTERN RECOGN, V36, P95
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   Draper BA, 2003, COMPUT VIS IMAGE UND, V91, P115, DOI 10.1016/S1077-3142(03)00077-8
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   HERBRICH R, 2002, LEARNING KERNAL CLAS
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HLAVATY T, 2003, P GEOM GRAPH TEACH C, P5
   Ip HHS, 2002, P 15 INT C VIS INT, P314
   Jollife I., 1986, PRINCIPAL COMPONENT
   Keim DA, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P419, DOI 10.1145/304181.304219
   Kohonen T., 1997, Self-Organizing Maps
   Lau R. W. H., 2002, World Wide Web, V5, P193, DOI 10.1023/A:1020984612969
   Lin SH, 1997, IEEE T NEURAL NETWOR, V8, P114, DOI 10.1109/72.554196
   Ma Y, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P339
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI [10.1162/neco.1992.4.3.415, 10.1162/neco.1992.4.3.448]
   Mahmoudi S, 2002, INT C PATT RECOG, P457, DOI 10.1109/ICPR.2002.1048337
   *MAX PLANCK I BIOL, 2004, MPI FAC DAT
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Müller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517
   Novotni M, 2001, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDING, P167, DOI 10.1109/SMA.2001.923387
   Novotni M., 2003, P 8 ACM S SOL MOD AP, P216, DOI DOI 10.1145/781606.781639
   Pascual-Marqui RD, 2001, PATTERN RECOGN, V34, P2395, DOI 10.1016/S0031-3203(00)00167-9
   Pastor L, 2001, PATTERN RECOGN, V34, P2497, DOI 10.1016/S0031-3203(00)00170-9
   Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Shawe-Taylor J., 2004, KERNEL METHODS PATTE
   Sun M, 2002, P ANN INT IEEE EMBS, P2027
   Suzuki MT, 2003, IEEE SYS MAN CYBERN, P3846
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   VILLALOBOS L, 1994, IEEE INT CONF ROBOT, P1965
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Wang P, 2005, PROC CVPR IEEE, P373
   Wong HS, 2004, INT C PATT RECOG, P613
   Wong HS, 2004, PATTERN RECOGN, V37, P2307, DOI 10.1016/j.patcog.2004.05.004
   Yang MH, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P215
   Yu M, 2003, PROC CVPR IEEE, P656
   [No title captured]
NR 43
TC 21
Z9 21
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 1026
EP 1036
DI 10.1109/TMM.2007.898915
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800012
DA 2024-07-18
ER

PT J
AU Somasundaram, K
   Domnic, S
AF Somasundaram, K.
   Domnic, S.
TI Extended Golomb code for Integer representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Burrows-Wheeler compression; Eilas root code; Fibonacci code; Golomb
   code; Huffman code; integer coding; Punctured Elias code
AB In this paper, we have proposed two methods to represent nonnegative integers based on the principle used in Golomb code (GC). In both methods, the given integer is successively divided with a divisor, the quotient and the remainders are then used to represent the integer. One of our methods is best suited for representing short integers and gives bit length comparable to that of Elias root code which is best for representing short-range integers. Another of our methods is best suited for representing both short and long integers and gives a bit length comparable to that of Fibonacci code which is best for representing long integers. Application of our methods as a final stage encoder of the Burrows-Wheeler Transform Compressor shows that our codes give a better compression rate than the Elias, Fibonacci, Punctured, and GC codes.
C1 Gandhigram Rural Inst, Dept Comp Sci & Appl, Gandhigram 624302, India.
C3 Gandhigram Rural Institute
RP Somasundaram, K (corresponding author), Gandhigram Rural Inst, Dept Comp Sci & Appl, Gandhigram 624302, India.
EM somasundaramk@yahoo.com; to_domnic@yahoo.co.in
RI S, Domnic/AAX-7673-2021; Somasundaram, Karuppanagounder/ABE-1074-2020;
   Sandanam, Domnic/AAX-7666-2021
OI Somasundaram, Karuppanagounder/0000-0003-0932-3538; Sandanam,
   Domnic/0000-0002-9986-2561
CR [Anonymous], 1972, B SOC ROY SCI LIEGE
   DAVID S, 2000, DATA COMPRESSION BOO, P41
   ELIAS P, 1975, IEEE T INFORM THEORY, V21, P194, DOI 10.1109/TIT.1975.1055349
   Fenwick P, 2002, SOFTWARE PRACT EXPER, V32, P1307, DOI 10.1002/spe.484
   FENWICK P, 1996, PUNCTURED ELIAS CODE
   GOLOMB SW, 1966, IEEE T INFORM THEORY, V12, P399, DOI 10.1109/TIT.1966.1053907
   HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898
   Williams HE, 1999, COMPUT J, V42, P193, DOI 10.1093/comjnl/42.3.193
   [No title captured]
NR 9
TC 7
Z9 9
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 239
EP 246
DI 10.1109/TMM.2006.886260
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900004
DA 2024-07-18
ER

PT J
AU Amir, Y
   Danilov, C
   Goose, S
   Hedqvist, D
   Terzis, A
AF Amir, Yair
   Danilov, Claudiu
   Goose, Stuart
   Hedqvist, David
   Terzis, Andreas
TI An overlay architecture for high-quality VoIP streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE overlay networks; VoIP
AB The cost savings and novel features associated with voice over IP (VoIP) are driving its adoption by service providers. Unfortunately, the Internet's best effort service model provides no quality of service guarantees. Because low latency and jitter are the key requirements for supporting high-quality interactive conversations, VoIP applications use UDP to transfer data, thereby subjecting themselves to quality degradations caused by packet loss and network failures. In this paper, we describe an architecture to improve the performance of such VoIP applications. Two protocols are used for localized packet loss recovery and rapid rerouting in the event of network failures. The protocols are deployed on the nodes of an application-level overlay network and require no changes to the underlying infrastructure. Experimental results indicate that the architecture and protocols can be combined to yield voice quality on par with the Public Switched Telephone Network.
C1 Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
   Siemens Corp Res Inc, Princeton, NJ 08540 USA.
C3 Johns Hopkins University; Siemens AG
RP Danilov, C (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.
EM yairamir@cs.jhu.edu; claudiu@cs.jhu.edu; stuart.goose@scr.siemens.com;
   david.hedqvist@scr.siemens.com; terzis@csjhu.edu
RI Terzis, Andreas/A-3348-2010; Amir, Yair/A-3261-2010
CR Amir Y., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P51, DOI 10.1145/1065983.1065997
   Amir Y, 2003, 2003 INTERNATIONAL CONFERENCE ON DEPENDABLE SYSTEMS AND NETWORKS, PROCEEDINGS, P511, DOI 10.1109/DSN.2003.1209961
   Andersen D., 2001, Proc. of SOSP, P131, DOI DOI 10.1145/502059.502048
   ANDERSEN DG, 2003, P IMC 2003 OCT
   [Anonymous], 2001, 3031 RFC
   Banerjee S., 2002, P ACM SIGCOMM
   Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   BOUTREMANS C, P NOSSDAV 2002
   CHANDRA B, 2001, P 3 USISTS MAR
   CHU YH, 2001, ACM SIGCOMM 2001 AUG
   Gulbrandsen A., 2000, document RFC 2782
   Iannaccone G., 2001, TR01ATL062917
   Labovitz C, 1998, IEEE ACM T NETWORK, V6, P515, DOI 10.1109/90.731185
   LABOVITZ C, 2000, P SIGCOMM 2000 AUG
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   MEDINA A, 2001, INT WORKSH MOD AN SI
   Paxson V, 1999, IEEE ACM T NETWORK, V7, P277, DOI 10.1109/90.779192
   PETERSON L, 2002, P 1 WORKSH HOT TOP N
   Schooler E., 2002, 3261 RFC
   Schulzrinne H., 1996, 1889 IETF RFC
   SUBRAMANIAN L, 2004, P USENIX NSDI 04 MAR
   TAO S, 2005, P INFOCOM 2005 MAR
   THERNELIUS F, 2000, THESIS KUNGL TEKNISK
   TOUCH J, 1998, P 3 GLOB INT MIN C G
   White B, 2002, USENIX ASSOCIATION PROCEEDINGS OF THE FIFTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P255, DOI 10.1145/1060289.1060313
   ZHANG Y, 2001, P ACM SIGCOMM INT ME
NR 26
TC 29
Z9 104
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1250
EP 1262
DI 10.1109/TMM.2006.884609
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Balter, R
   Gioia, P
   Morin, L
AF Balter, Raphaele
   Gioia, Patrick
   Morin, Luce
TI Scalable and efficient video coding using 3-D modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D model-based coding; 3-D reconstruction; second-generation wavelets
ID RECONSTRUCTION; COMPRESSION
AB In this paper, we present a three-dimensional (3-D) model-based video coding scheme for streaming static scene video in a compact way but also enabling time and spatial scalability according to network or terminal capability and providing 3-D functionalities. The proposed format is based on encoding the sequence of reconstructed models using second-generation wavelets, and efficiently multiplexing the resulting geometric, topological, texture, and camera motion binary representations. The wavelets decomposition can be adaptive in order to fit to images and scene contents. To ensure time scalability, this representation is based on a common connectivity for all 3-1) models, which also allows straight-forward morphing between successive models ensuring visual continuity at no additional cost. The method proves to be better than previous methods for video encoding of static scenes, even better than state-of-the-art video coders such as H264 (also known as MPEG AVC). Another applications of our approach are smoothing camera path for suppression of jitter from hand-held acquisition and the fast transmission and real-time visualization of virtual environments obtained by video capture, for virtual or augmented reality and interactive walk-through in photo-realistic 3-1) environments around the original camera path.
C1 France Telecom R&D, F-35512 Cesson Sevigne, France.
   INRIA, IRISA, F-35042 Rennes, France.
C3 Orange SA; Universite de Rennes; Inria
RP Balter, R (corresponding author), France Telecom R&D, Campus Beaulieu, F-35512 Cesson Sevigne, France.
EM raphaele.balter@orange-ft.com; patrick.gioia@orange-ft.com;
   luce.morin@irisa.fr
CR ALLIEZ P, 2003, P S MULT GEOM MOD CA
   [Anonymous], JTC1SC29WG11
   [Anonymous], 1988, P ALVEY VISION C
   BALTER R, 2004, P 2 INT S 3DPVT 3D D
   CORNELIS K, 2000, P 2 WORKSH STRUCT MU
   Daubechies I., 1992, 10 LECT WAVELETS
   Galpin F, 2002, EURASIP J APPL SIG P, V2002, P1088, DOI 10.1155/S1110865702206095
   GALPIN F, 2004, P C COMP VIS PATT RE
   Gioia P, 2004, IEEE T CIRC SYST VID, V14, P1009, DOI 10.1109/TCSVT.2004.830672
   GIROD B, 1999, P ICIP KOB JAP OCT
   GUEN BL, 2004, J TUD CHANG CORESA 2
   KHODAKOVSKY A, 2000, SIGGRAPH 2000 C P NE
   Koch R, 2000, J VISUAL COMP ANIMAT, V11, P115, DOI 10.1002/1099-1778(200007)11:3<115::AID-VIS228>3.0.CO;2-2
   Lamboray E, 2004, P IEEE VIRT REAL ANN, P91, DOI 10.1109/VR.2004.1310060
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Magnor M, 2003, IEEE T CIRC SYST VID, V13, P1092, DOI 10.1109/TCSVT.2003.817630
   NISTER D, 2000, P 5 EUR C COMP VIS E
   Paris S, 2006, INT J COMPUT VISION, V66, P141, DOI 10.1007/s11263-005-3953-x
   PATEUX S, 2001, P PICT COD S
   POLLEFEYS M, 2001, P 3 INT WORKSH AUT E
   PRETEUX F, 1998, P SPIE C MATH MOD ES, P94
   RAMANATHAN P, 2004, P C COMP VIS PATT RE
   Rossignac J, 1999, IEEE T VIS COMPUT GR, V5, P47, DOI 10.1109/2945.764870
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Schruder P., 1995, Proc. 22nd Ann. Conf. Comput. Graphics Interactive Techniques (SIGGRAPH'95), P161
   SCHWARZ H, 2002, P IBC AMST NETH
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   Zisserman A, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P51, DOI 10.1109/MMCS.1999.779119
NR 28
TC 12
Z9 16
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1147
EP 1155
DI 10.1109/TMM.2006.879873
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vinciarelli, A
   Odobez, JM
AF Vinciarelli, Alessandro
   Odobez, Jean-Marc
TI Application of information retrieval technologies to presentation slides
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE indexing; information retrieval; noisy text; optical character
   recognition; presentations; slides
ID SEGMENTING TEXT
AB Presentations are becoming an increasingly more common means of communication in working environments, and slides are often the necessary supporting material on which the presentations rely. In this paper, we describe a slide indexing and retrieval system in which the slides are captured as images (through a framegrabber) at the moment they are displayed during a presentation and then transcribed with an optical character recognition (OCR) system. In this context, we show that such an approach presents several advantages over the use of commercial software (API based) to obtain the slide transcriptions. We report a set of retrieval experiments conducted on a database of 26 real presentations (570 slides) collected at a workshop. The experiments show that the overall retrieval performance is close to that obtained using either a manual transcription of the slides or the API software. Moreover, the experiments show that the OCR-based approach outperforms significantly the API in extracting the text embedded in images and figures.
C1 IDIAP Res Inst, CH-1920 Martigny, Switzerland.
RP Vinciarelli, A (corresponding author), IDIAP Res Inst, CH-1920 Martigny, Switzerland.
EM vincia@idiap.ch; odobez@idiap.ch
RI Odobez, Jean-Marc/B-1426-2010; Vinciarelli, Alessandro/HZI-8274-2023;
   Vinciarelli, Alessandro/C-1651-2012
OI Odobez, Jean-Marc/0000-0002-9537-9898; Vinciarelli,
   Alessandro/0000-0002-9048-0524
CR Abowd GD, 1999, IBM SYST J, V38, P508, DOI 10.1147/sj.384.0508
   AMIR A, 2001, P 34 HAW INT C SYST, P1662
   Anderson Richard., 2004, MULTIMEDIA '04: Proceedings of the 12th annual ACM international conference on Multimedia, P796
   [Anonymous], 2018, The Visual Display of Quantitative Information
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   BOURLAND H, 2005, LECT NOTES COMPUTER, V3361
   Chen JY, 2004, LETHAIA, V37, P3, DOI 10.1080/00241160410004764
   COHEN G, 2000, P 33 HAW INT C SYST, P3046
   Deshpande S. G., 2001, IEEE Transactions on Multimedia, V3, P432, DOI 10.1109/6046.966115
   DUMAIS ST, 1991, BEHAV RES METH INS C, V23, P229, DOI 10.3758/BF03203370
   Fox C., 1992, INFORM RETRIEVAL DAT, P102
   Frakes W.B., 1992, INFORM RETRIEVAL DAT, P131
   GAN CK, 1988, IEEE T ACOUST SPEECH, V36, P924, DOI 10.1109/29.1605
   GAROFOLO JS, 1999, P TREC 9, P107
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   Hearst MA, 1997, COMPUT LINGUIST, V23, P33
   Hürst W, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P333, DOI 10.1145/319463.319639
   HURST W, 2003, IADIS INT J WWW INTE, V1, P43
   HURST W, 2004, P ED MEDIA 2004
   Jain AK, 1998, PATTERN RECOGN, V31, P2055, DOI 10.1016/S0031-3203(98)00067-3
   Ju SX, 1998, IEEE T CIRC SYST VID, V8, P686, DOI 10.1109/76.718513
   Kawahara T, 2004, IEEE T SPEECH AUDI P, V12, P409, DOI 10.1109/TSA.2004.828701
   LI CF, 2000, P SIGCHI C HUM FACT, P169
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   LI W, 2004, P 2 IEEE INT WORKSH
   Lienhart R, 2002, IEEE T CIRC SYST VID, V12, P256, DOI 10.1109/76.999203
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   Ngo CW, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA533
   Niblack W, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P114, DOI 10.1109/IVL.1999.781134
   PAUL DB, 1992, P DARPA SLS WORKSH
   Pimentel MD, 2001, INTERACT COMPUT, V13, P353, DOI 10.1016/S0953-5438(00)00042-4
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Robertson SE, 2000, INFORM PROCESS MANAG, V36, P95, DOI 10.1016/S0306-4573(99)00046-1
   ROGINA I, 2002, P INT C SPEECH LANG, P333
   SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0
   Sato T, 1999, MULTIMEDIA SYST, V7, P385, DOI 10.1007/s005300050140
   Singhal A, 1996, INFORM PROCESS MANAG, V32, P619, DOI 10.1016/0306-4573(96)00008-8
   Syeda-Mahmood T., 2000, Proceedings ACM Multimedia 2000, P85, DOI 10.1145/354384.354433
   Wang F., 2003, MULTIMEDIA '03: Proceedings of the eleventh ACM international conference on Multimedia, P315
   Wolf C, 2002, INT C PATT RECOG, P1037, DOI 10.1109/ICPR.2002.1048482
   Wu V, 1999, IEEE T PATTERN ANAL, V21, P1224, DOI 10.1109/34.809116
   Zhang DS, 2004, IEEE T MULTIMEDIA, V6, P450, DOI 10.1109/TMM.2004.827505
   ZHU Z, 2004, P 4 INT WORKSH MULT
   ZIEWER P, 2004, P E LEARN 2004
   ZUPANCIC B, 2002, P 7 ANN C INN TECHN, P24
NR 45
TC 10
Z9 11
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 981
EP 995
DI 10.1109/TMM.2006.879870
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gavrielides, MA
   Sikudová, E
   Pitas, I
AF Gavrielides, Marios A.
   Sikudova, Elena
   Pitas, Ioannis
TI Color-based descriptors for image fingerprinting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE color histogram; color quantization; image fingerprinting; image
   indexing; image representation; retrieval; spatial chromatic histogram
ID RETRIEVAL
AB Typically, content-based image retrieval (CBIR) systems receive an image or an image description as input and retrieve images from a database that are similar to the query image in regard to properties such as color, texture, shape, or layout. A kind of system that did not receive much attention compared to CBIR systems, is one that searches for images that are not similar but exact copies of the same image that have undergone some transformation. In this paper, we present such a system referred to as an image fingerprinting system, since it aims to extract unique and robust image descriptors (in analogy to human fingerprints). We examine the use of color-based descriptors and provide comparisons for different quantization methods, histograms calculated using color-only and/or spatial-color information with different similarity measures. The system was evaluated with receiver operating characteristic (ROC) analysis on a large database of 919 original images consisting of randomly drawn art images and similar images from specific categories, along with 30 transformed images for each original, totaling 27570 images. The transformed images were produced with attacks that typically occur during digital image distribution, including different degrees of scaling, rotation, cropping, smoothing, additive noise and compression, as well as illumination contrast changes. Results showed a sensitivity of 96% at the small false positive fraction of 4% and a reduced sensitivity of 88% when 13% of all transformations involved changing the illuminance of the images. The overall performance of the system is encouraging for the use of color, and particularly spatial chromatic descriptors for image fingerprinting.
C1 Aristotle Univ Thessaloniki, Dept Informat, Artificial Intelligence & Informat Anal Lab, Thessaloniki 54124, Greece.
C3 Aristotle University of Thessaloniki
RP Gavrielides, MA (corresponding author), US FDA, Div Imaging & Appl Math, Off Sci, Ctr Devices & Radiol Hlth, Rockville, MD 20852 USA.
EM marios.gavrielides@fda.hhs.gov; sikudova@fmph.uniba.sk;
   pitas@zeus.csd.auth.gr
RI Šikudová, Elena/T-4763-2017; Šikudová, Elena/S-1078-2019
OI Šikudová, Elena/0000-0003-4572-4064; 
CR Cinque L, 2001, IMAGE VISION COMPUT, V19, P979, DOI 10.1016/S0262-8856(01)00060-9
   DATTA A, 2003, IEEE INT C MULT EXP, V2, P221
   GLASSNER A, 1995, MORGAN KAUFMAN SERIE
   HAFNER J, 1995, IEEE T PATTERN ANAL, V17, P729, DOI 10.1109/34.391417
   JOHNSON M, 2003, P 2003 INT C IM PROC, P495
   JOHNSON N, 1999, 5 INT WORKSH MULT IN
   McCamy C. S., 1976, Journal of Applied Photographic Engineering, V2, P95
   Mojsilovic A, 2001, IEEE T IMAGE PROCESS, V10, P1712, DOI 10.1109/83.967399
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Schneider M, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P227, DOI 10.1109/ICIP.1996.560425
   SEO J, 2003, IEEE INT C AC SPEECH, V3, P61
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   STRICKER M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P704, DOI 10.1109/CVPR.1994.323774
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Swets John., 1982, EVALUATION DIAGNOSTI
   VENKATESAN R, 2000, P 2000 INT C IM PROC
   WOLFGANG RB, 1997, P SPIE INT C MULT NE, V3228, P297, DOI DOI 10.1117/12.300900
NR 17
TC 19
Z9 21
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 740
EP 748
DI 10.1109/TMM.2006.876290
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300009
DA 2024-07-18
ER

PT J
AU Bao, P
   Gourlay, D
   Li, YF
AF Bao, Paul
   Gourlay, Douglas
   Li, Youfu
TI Deep compression of remotely rendered views
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE context modeling; epipolar geometry; image-based-rendering; image
   compression and streaming; virtual reality; 3-D image warping
AB Three-dimensional (3-D) models are information-rich and provide compelling visualization effects. However downloading and viewing 3-D scenes over the network may be excessive. In addition low-end devices typically have insufficient power and/or memory to render the scene interactively in real-time. Alternatively, 3-D image warping, an image-based-rendering technique that renders a two-dimensional (2-D) depth view to form new views intended from different viewpoints and/or orientations, may be employed on a limited device. In a networked 3-D environment, the warped views may be further compensated by the graphically rendered views and transmitted to clients at times. Depth views can be considered as a compact model of 3-D scenes enabling the remote rendering of complex 3-D environment on relatively low-end devices. The major overhead of the 3-D image warping environment is the transmission of the depth views of the initial and subsequent references. This paper addresses the issue by presenting an effective remote rendering environment based on the deep compression of depth views utilizing the context statistics structure present in depth views. The warped image quality is also explored by reducing the resolution of the depth map. It is shown that proposed deep compression of the remote rendered view significantly outperforms the JPEG2000 and enables the realtime rendering of remote 3-D scene while the degradation of warped image quality is visually imperceptible for the benchmark scenes.
C1 Univ S Florida, Dept Informat Technol, Tampa, FL 33260 USA.
   Chongqing Univ, Sch Comp, Chongqing 630044, Peoples R China.
   Chinese Univ Hong Kong, Dept Manufacture Engn & Engn Management, Hong Kong, Hong Kong, Peoples R China.
C3 State University System of Florida; University of South Florida;
   Chongqing University; Chinese University of Hong Kong
RP Bao, P (corresponding author), Univ S Florida, Dept Informat Technol, Tampa, FL 33260 USA.
EM pbao@lakeland.usf.edu; douglas@ie.cuhk.edu.hk; meyfli@cityu.edu.hk
RI Liu, Liu/JXM-8208-2024
OI LI, You Fu/0000-0002-5227-1326
CR [Anonymous], P IEEE SPIE MULT COM
   [Anonymous], THESIS U N CAROLINA
   Bao P, 1998, COMPUT GRAPH-UK, V22, P217, DOI 10.1016/S0097-8493(98)00010-7
   Bao P, 2004, IEEE T MULTIMEDIA, V6, P786, DOI 10.1109/TMM.2004.837248
   Bayakovski Y, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P25, DOI 10.1109/ICIP.2002.1038894
   Cohen-Or D, 1999, COMP GRAPH, P261, DOI 10.1145/311535.311564
   CohenOr D, 1997, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P104, DOI 10.1109/CGI.1997.601282
   Duan JG, 2001, IEEE DATA COMPR CONF, P331, DOI 10.1109/DCC.2001.917164
   HUDSON T, 1999, TR99024 COMP SCI
   Koller D, 2004, ACM T GRAPHIC, V23, P695, DOI 10.1145/1015706.1015782
   Krishnamurthy R, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P828, DOI 10.1109/ICIP.2001.958248
   Levoy M., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P21, DOI 10.1145/218380.218392
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Magnor M, 2000, IEEE T CIRC SYST VID, V10, P338, DOI 10.1109/76.836278
   Mann Y, 1997, COMPUT GRAPH FORUM, V16, pC201, DOI 10.1111/1467-8659.00157
   MARK WR, 1998, TR98011 COMP SCI
   McMillan L, 1997, IMAGE BASED APPROACH
   POPESCU V, 2001, THESIS U N CAROLINA
   Shade J., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P231, DOI 10.1145/280814.280882
   SHADE L, 1998, P SIGGRAPH 98, P231
   WELCH TA, 1984, COMPUTER, V17, P8, DOI 10.1109/MC.1984.1659158
   Wu XL, 2000, IEEE T IMAGE PROCESS, V9, P536, DOI 10.1109/83.841931
   Wu XL, 1997, IEEE T COMMUN, V45, P437, DOI 10.1109/26.585919
   ZWICKER M, 2000, 200009 MITS EL RES L
NR 24
TC 3
Z9 3
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 444
EP 456
DI 10.1109/TMM.2006.870746
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000002
DA 2024-07-18
ER

PT J
AU Chan, SHG
   Zheng, X
   Zhang, Q
   Zhu, WW
   Zhang, YQ
AF Chan, SHG
   Zheng, X
   Zhang, Q
   Zhu, WW
   Zhang, YQ
TI Video loss recovery with FEC and stream replication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE fast approximation; feedback-free error recovery; forward error
   correction (FEC); layered video multicast; stream replication
ID ERROR-PROTECTION; MULTICAST; TRANSMISSION; ALLOCATION; FRAMEWORK;
   FEEDBACK
AB Packet loss is inevitable in video multicast. In this paper, we propose and study an effective feedback-free loss recovery scheme for layered video which combines forward error correction (FEC) and stream replication. In our scheme, the server multicasts the video in parallel with FEC packets and a number of replicated delayed (ReD) version of the stream. Receivers autonomously and dynamically join the FEC and ReD streams to repair their losses. On the server side, we analyze and optimize the number of replicated streams and FEC packets to meet a certain residual loss requirement (i.e., error after correction). On the receiver side, we analyze the optimal combination of FEC and ReD packets to minimize its loss. We also present a fast yet accurate approximation algorithm for receiver to make such decision. We show that FEC combined with merely one or two replicated streams can effectively reduce the residual error rate (by as much as 50%) as compared with pure FEC or replication alone. Both subjective and objective video measures confirm that our recovery scheme achieves much better visual quality.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   Intel Commun Technol China Lab, Beijing 100080, Peoples R China.
   Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Hong Kong University of Science & Technology; Intel Corporation;
   Microsoft Research Asia; Microsoft
RP Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM gchan@cs.ust.hk; wenwu.zhu@intel.com
RI Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881; Chan, Gary Shueng
   Han/0000-0003-4207-764X
CR BOYD A, 1959, REPORTS STAT APPL RE
   Byers J.W., 1998, P ACM SIGCOMM 98 C A, P56
   CACERES R, 1999, P 18 ANN JOINT C IEE, V1, P371
   Cai JF, 2000, IEEE WCNC, P1243, DOI 10.1109/WCNC.2000.904809
   Carle G, 1997, IEEE NETWORK, V11, P24, DOI 10.1109/65.642357
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   DEERING S, 1993, MULTIMEDIA INTEGRATE
   Dubuc C., 2001, IEEE Transactions on Multimedia, V3, P424, DOI 10.1109/6046.966114
   Floyd S, 1997, IEEE ACM T NETWORK, V5, P784, DOI 10.1109/90.650139
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   Hagenauer J, 1999, P IEEE, V87, P1764, DOI 10.1109/5.790636
   ISHIBASHI Y, 2000, P 2000 IEEE INT C CO, V2, P846
   JONGTAE S, 1998, P IEEE ATM WORKSH MA, P360
   Kasera SK, 1998, IEEE INFOCOM SER, P988, DOI 10.1109/INFCOM.1998.662908
   Kasera SK, 2000, IEEE ACM T NETWORK, V8, P294, DOI 10.1109/90.851976
   KERMODE R, 1998, P ACM SIGCOMM 98 VAN, P278
   Kim J, 2004, IEEE T IMAGE PROCESS, V13, P1547, DOI 10.1109/TIP.2004.837552
   Lee TWA, 2002, IEEE T CIRC SYST VID, V12, P1059, DOI 10.1109/TCSVT.2002.806816
   Lee TWA, 2001, GLOB TELECOMM CONF, P1994, DOI 10.1109/GLOCOM.2001.965922
   Li B, 2001, CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING 2001, VOLS I AND II, CONFERENCE PROCEEDINGS, P1145, DOI 10.1109/CCECE.2001.933603
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   Li X, 1996, PROCEEDINGS OF THE FIFTH IEEE INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, P356, DOI 10.1109/HPDC.1996.546206
   Li X, 1999, IEEE NETWORK, V13, P46, DOI 10.1109/65.768488
   NOGUCHI T, 2001, P ICC 2001, V8, P2348
   Nonnenmacher J, 1998, IEEE INFOCOM SER, P972, DOI 10.1109/INFCOM.1998.662906
   Nonnenmacher J, 1998, IEEE ACM T NETWORK, V6, P349, DOI 10.1109/90.720869
   NONNENMACHER J, 1997, P ACM SIGCOMM SEP
   Papadopoulos C, 1998, IEEE INFOCOM SER, P1188, DOI 10.1109/INFCOM.1998.662932
   Paul S, 1997, IEEE J SEL AREA COMM, V15, P407, DOI 10.1109/49.564138
   Prasad R, 2003, IEEE NETWORK, V17, P27, DOI 10.1109/MNET.2003.1248658
   Ren GL, 2004, IEEE T CONSUM ELECTR, V50, P478, DOI 10.1109/TCE.2004.1309411
   RHEE I, 2000, P IEEE INFOCOM 00, V2, P805
   RHEE I, 1998, P ACM SIGCOMM 98 C A, P290
   Stockhammer T, 2002, IEEE T CIRC SYST VID, V12, P465, DOI [10.1109/TCSVT.2002.800317, 10.1109/TCSVT.2002.806317]
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Xu LH, 2003, IEEE T COMMUN, V51, P63, DOI 10.1109/TCOMM.2002.807616
   Yan B, 2004, IEEE T CIRC SYST VID, V14, P874, DOI 10.1109/TCSVT.2004.828332
   YANG GH, 2004, P IEEE INF MAR, V23, P1201
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
   ZHENG X, 2003, P IEEE ICC MAY 11 15
NR 44
TC 27
Z9 31
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 370
EP 381
DI 10.1109/TMM.2005.864340
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, JH
   Chau, LP
AF Zheng, JH
   Chau, LP
TI Error-concealment algorithm for H.26L using first-order plane estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; H.264
ID MOTION COMPENSATION; VIDEO; INTERPOLATION
AB In this paper, we propose a new error-concealment algorithm for the forthcoming video coding standard H.26L, which makes use of a first-order plane to estimate motion vectors. In H.26L, a 16 x 16 inter macroblok can be divided into variant block shape for motion prediction, and there are up to sixteen sets of motion vector in one macroblock. For nature image, the motions within a small area are likely to move in the same direction. By using the motion vectors that are next to the vertices of lost macroblock, we can constitute a first-order plane that indicates the movement tendency in this small area, and estimate the motion vector of vertices. Then we use the motion vectors of vertices to interpolate motion vector for each pixel separately. The interpolation function we selected makes the motion change smoothly within the lost macroblock. The simulation results show that our method can efficiently improve the video quality over different macroblock lost rate.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM lpchau@ieee.org
RI 金华, 郑/GWM-7529-2022; Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593
CR Al-Mualla M, 1999, ELECTRON LETT, V35, P215, DOI 10.1049/el:19990174
   Atzori L, 2001, IEEE T MULTIMEDIA, V3, P326, DOI 10.1109/6046.944476
   Bjontegaard Gisle, 2001, ITU TELECOMMUNICATIO
   Chen MJ, 1997, IEEE T CIRC SYST VID, V7, P560, DOI 10.1109/76.585936
   Han YH, 1998, IEEE T CIRC SYST VID, V8, P221, DOI 10.1109/76.664106
   Heising G, 2001, IEE P-VIS IMAGE SIGN, V148, P93, DOI 10.1049/ip-vis:20010075
   Lee Marshall H., 1990, Annals of Epidemiology, V1, P1
   Park JW, 1999, IEEE T CIRC SYST VID, V9, P1003, DOI 10.1109/76.795052
   SULLIVAN G, 2001, P IEEE INT C IM PROC, V3, P573
   Tsekeridou S, 2000, IEEE T CIRC SYST VID, V10, P646, DOI 10.1109/76.845010
   Turaga DS, 2002, IEEE T CIRC SYST VID, V12, P483, DOI 10.1109/TCSVT.2002.800318
   Valente S, 2001, IEEE T CONSUM ELECTR, V47, P568, DOI 10.1109/30.964147
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wang Z, 1998, IEEE T IMAGE PROCESS, V7, P1056, DOI 10.1109/83.701166
   Zhu QF, 1993, IEEE T CIRC SYST VID, V3, P248, DOI 10.1109/76.224235
NR 15
TC 12
Z9 15
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 801
EP 805
DI 10.1109/TMM.2004.837246
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200003
DA 2024-07-18
ER

PT J
AU Huang, YL
   Shieh, S
   Ho, FS
   Wang, JC
AF Huang, YL
   Shieh, S
   Ho, FS
   Wang, JC
TI Efficient key distribution schemes for secure media delivery in pay-TV
   systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE channel protection; conditional access system; key distribution scheme;
   secure media delivery
ID SERVICE
AB To provide secure media delivery in pay-TV systems, a large number of messages are exchanged for key updates in the conventional key distribution schemes. This is inefficient and costly when the client side (set-top box) uses a smart card with limited computing power. In this paper, we present three key distribution schemes for channel protection and secure media delivery in pay-TV systems. With the proposed schemes, encryption keys of the subscribed programs can be efficiently and securely distributed to the authorized subscribers. Only one message is needed to renew key in the key distribution schemes for subscription channel protection. In addition, we use simpler computation functions, including one-way hash function and exclusive-OR operation, for key updates to reduce the computation cost. With our key distribution schemes, only authorized subscribers can watch the subscribed programs correctly. Unauthorized subscribers have no information to retrieve the correct programs over the networks. Thus, service providers can charge their subscribers according to their subscriptions, and the illegal access of the media and video programs from networks can be prevented, based on the proposed schemes.
C1 Natl Chiao Tung Univ, Dept Comp Sci & Informat Engn, Hsinchu 30050, Taiwan.
C3 National Yang Ming Chiao Tung University
RP Natl Chiao Tung Univ, Dept Comp Sci & Informat Engn, Hsinchu 30050, Taiwan.
EM ylhuang@csie.nctu.edu.tw; ssp@csie.nctu.edu.tw; fsho@csie.nctu.edu.tw;
   jcwang@csie.nctu.edu.tw
CR ALMEROTH KC, 1996, IEEE J SELECTED AREA, V14
   [Anonymous], IEEE GLOB TEL C
   FU FK, 1998 IEEE INT S CONS, V45, P151
   Juhn LS, 1997, IEEE T CONSUM ELECTR, V43, P1110, DOI 10.1109/30.642378
   Kim WH, 1997, IEEE T CONSUM ELECTR, V43, P980, DOI 10.1109/30.628778
   LAI XJ, 1991, LECT NOTES COMPUT SC, V473, P389
   LEE JW, 1996, P INT C CRYPT INF SE, P82
   Little T. D. C., 1994, IEEE Multimedia, V1, P14, DOI 10.1109/MMUL.1994.318978
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   *NBS FIPS, 1977, NBS FIPS PUB, V461
   ROBSHAW MJB, 1994, TR601 RSA LAC
   Sakakibara H., 1994, Proceedings. 1994 International Conference on Network Protocols (Cat. No.94TH8002), P91, DOI 10.1109/ICNP.1994.344372
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   Wallner D.M., 1999, KEY MANAGEMENT MULTI
NR 14
TC 43
Z9 45
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 760
EP 769
DI 10.1109/TMM.2004.834861
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800008
DA 2024-07-18
ER

PT J
AU Grecos, C
AF Grecos, C
TI A low cost algorithm for fast inverse motion compensation in the MPEG-2
   standard
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE fast algorithms; inverse motion compensation; MPEG-2 standard
AB The most advanced of inverse motion compensation (IMC) algorithms exploit shared information in the macroblock level rather than operating on 8 x 8 blocks, achieving speed-tips of up to 44%. A variable complexity algorithm (VCA) is presented that jointly exploits macroblock shared information and DCT block sparseness, thus resulting in further reductions of up to 6% on the average in the number of operations required.
C1 Univ Loughborough, Dept Elect & Elect Engn, Loughborough LE11 3TU, Leics, England.
C3 Loughborough University
RP Grecos, C (corresponding author), Univ Loughborough, Dept Elect & Elect Engn, Loughborough LE11 3TU, Leics, England.
EM C.Grecos@lboro.ac.uk
CR Assuncao PAA, 1997, INT CONF ACOUST SPEE, P2633, DOI 10.1109/ICASSP.1997.595329
   CHANG SF, 1995, IEEE J SEL AREA COMM, V13, P1, DOI 10.1109/49.363151
   LEGALL D, 1991, COMMUN ACM, V34
   MERHAV N, 1996, P IEEE INT C AC SPEE, V4, P2307
   Patterson DavidA., 1996, Computer architecture: a quantitative approach, V2nd
   Song JH, 2000, IEEE T CIRC SYST VID, V10, P767, DOI 10.1109/76.856453
NR 6
TC 0
Z9 0
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 510
EP 513
DI 10.1109/TMM.2004.827512
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200014
DA 2024-07-18
ER

PT J
AU Marolt, M
AF Marolt, M
TI A connectionist approach to automatic transcription of polyphonic piano
   music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive oscillators; music transcription; neural networks
AB In this paper, we present a connectionist approach to automatic transcription of polyphonic piano music. We first compare the performance of several neural network models on the task of recognizing tones from time-frequency representation of a musical signal. We then propose a new partial tracking technique, based on a combination of an auditory model and adaptive oscillator networks. We show how synchronization of adaptive oscillators can be exploited to track partials in a musical signal. We also present an extension of our technique for tracking individual partials to a method for tracking groups of partials by joining adaptive oscillators into networks. We show that oscillator networks improve the accuracy of transcription with neural networks. We also provide a short overview of our entire transcription system and present its performance on transcriptions of several synthesized and real piano recordings. Results show that our approach represents a viable alternative to existing transcription systems.
C1 Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1000, Slovenia.
C3 University of Ljubljana
RP Univ Ljubljana, Fac Comp & Informat Sci, Ljubljana 1000, Slovenia.
EM matija.marolt@fri.uni-lj.si
CR [Anonymous], 1995, THESIS INDIANA U BLO
   [Anonymous], COMPUT MUSIC J
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   CARPENTER GA, 1992, IEEE T NEURAL NETWOR, V3, P698, DOI 10.1109/72.159059
   Dixon S., 2000, COMPUTER RECOGNITION, P31
   GABRIJEL I, 1997, P 2 INT ICSC S SOFT, P164
   Haykin S., 1999, NEURAL NETWORKS COMP, DOI [10.1017/S0269888998214044, DOI 10.1017/S0269888998214044]
   KASHINO K, 1995, P INT JOINT C AI WOR
   KLAPURI A, 2001, P INT C AC SPEECH SI
   KLAPURI A, 1997, THESIS TAMPERE U TEC
   Large E. W., 1994, Connection Science, V6, P177, DOI 10.1080/09540099408915723
   MEDDIS R, 1991, J ACOUST SOC AM, V89, P2866, DOI 10.1121/1.400725
   MEDDIS R, 1986, J ACOUST SOC AM, V79, P702, DOI 10.1121/1.393460
   MOORE BCJ, 1983, J ACOUST SOC AM, V74, P750, DOI 10.1121/1.389861
   PATTERSON RD, 1990, ADV SPEECH HEARING A, V3
   Roads Curtis., 1996, The Computer Music Tutorial
   ROSSI L, 1998, THESIS U CORSE FRANC
   Slaney M., 1993, An efficient implementation of the patterson-holdsworth auditory filterbank, V35
   Smith LS, 1996, ADV NEUR IN, V8, P729
   STERIAN AD, 1999, THESIS U MI ANN ARBO
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wang DL, 1996, COGNITIVE SCI, V20, P409, DOI 10.1207/s15516709cog2003_3
   ZELL A, 1997, SNNS STUTTGART NEURA
NR 23
TC 85
Z9 97
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 439
EP 449
DI 10.1109/TMM.2004.827507
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200007
DA 2024-07-18
ER

PT J
AU Wang, B
   Sen, S
   Adler, M
   Towsley, D
AF Wang, B
   Sen, S
   Adler, M
   Towsley, D
TI Optimal proxy cache allocation for efficient streaming media
   distribution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE computer networks; multimedia communication; multimedia streaming;
   prefix caching; proxy caching; streaming media distribution
AB In this paper, we address the problem of efficiently streaming a set of heterogeneous videos from a remote server through a proxy to multiple asynchronous clients so that they can experience playback with low startup delays. We determine the optimal proxy prefix cache allocation to the videos that minimizes the aggregate network bandwidth cost. We integrate proxy caching with traditional server-based reactive transmission schemes such as hatching, patching and stream merging to develop a set of proxy-assisted delivery schemes. We quantitatively explore the impact of the choice of transmission scheme, cache allocation policy, proxy cache size, and availability of unicast versus multicast capability, on the resulting transmission cost. Our evaluations show that even a relatively small prefix cache (10%-20% of the video repository) is sufficient to realize substantial savings in transmission cost. We find that carefully designed proxy-assisted reactive transmission schemes can produce significant cost savings even in a predominantly unicast environment such as the Internet.
C1 Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
   AT&T Labs Res, Florham Pk, NJ 07932 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   AT&T
RP Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.
EM sen@research.att.ocm
OI Towsley, Donald/0000-0002-7808-7375
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P253, DOI 10.1109/MMCS.1996.534983
   ALMEIDA J, 2002, P IEEE INFOCOM NEW Y
   ALMEIDA JM, 2001, P SPIE ACM C MULT CO
   CARTER S, 1997, P INT C COMP COMM NE
   Diot C, 2000, IEEE NETWORK, V14, P78, DOI 10.1109/65.819174
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Eager D, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P199, DOI 10.1145/319463.319601
   EAGER D, 1999, P MULT COMP NETW MMC
   Gao L., 2001, IEEE T MULTIMEDIA, V3, P405
   Gao LX, 2003, IEEE ACM T NETWORK, V11, P884, DOI 10.1109/TNET.2003.820423
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   RAMESH S, 2001, P IEEE INFOCOM ANCH
   Sen S, 1999, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1999.752149
   SEN S, 2001, P IEEE INT PERF COMP
   TEWARI R, 1998, P SPIE ACM C MULT CO
   VENKATRAMANI C, 2002, P INT WORKSH NETW OP, P147
   VERSCHEURE O, 2001, P 6 INT WORKSH WEB C
   WANG B, 2001, 0105 U MASS DEP COMP
   WANG Y, 1998, P IEEE INFOCOM SAN F
NR 19
TC 45
Z9 58
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 366
EP 374
DI 10.1109/TMM.2003.822788
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Korkmaz, T
   Krunz, MM
AF Korkmaz, T
   Krunz, MM
TI Routing multimedia traffic with QoS guarantees
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE INFOCOM 2001 Conference
CY APR, 2001
CL Anchorage, AK
SP IEEE
DE multiconstrained path selection; QoS routing; scalable routing
ID PATH SUBJECT; ALGORITHM; QUALITY; NETWORKS
AB One of the challenging issues in exchanging multimedia information over a network is how to determine a feasible path that satisfies all the quality-of-service (QoS) requirements of multimedia applications while maintaining high utilization of network resources. The latter objective implies the need to impose an additional optimality requirement on the feasibility problem. This can be done through a primary cost function (e.g., administrative weight, hop-count) according to which the selected feasible path is optimal. In general, multiconstrained path selection, with or without optimization, is an NP-complete problem that cannot be exactly solved in polynomial time. Heuristics and approximation algorithms with polynomial- and pseudo-polynomial-time complexities are often used to deal with this problem. However, existing solutions suffer either from excessive computational complexities that cannot be used for online network operation or from low performance. Moreover, they only deal with special cases of the problem (e.g., two constraints without optimization, one constraint with optimization, etc.). For the feasibility problem under multiple constraints, some researchers have recently proposed a nonlinear cost function whose minimization provides a continuous spectrum of solutions ranging from a generalized linear approximation (GLA) to an asymptotically exact solution. In this paper, we propose an efficient heuristic algorithm for the most general form of the problem. We first formalize the theoretical properties of the above nonlinear cost function. We then introduce our heuristic algorithm (H-MCOP), which attempts to minimize both the nonlinear cost function (for the feasibility part) and the primary cost function (for the optimality part). We prove that HMCOP guarantees at least the performance of GLA and often improves upon it. H-MCOP has the same order of complexity as Dijkstra's algorithm. Using extensive simulations on random graphs and realistic network topologies with correlated and uncorrelated link weights from several distributions including uniform, normal, and exponential, we show the efficiency of H-MCOP over its (less general) contenders in terms of finding feasible paths and minimizing their costs under the same level of computational complexity.
C1 Univ Texas, Dept Comp Sci, San Antonio, TX 78249 USA.
   Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 University of Texas System; University of Texas Health Science Center at
   San Antonio; University of Arizona
RP Univ Texas, Dept Comp Sci, San Antonio, TX 78249 USA.
EM korkmaz@cs.utsa.edu; krunz@ece.arizona.edu
RI Korkmaz, Turgay/L-8078-2015
OI Korkmaz, Turgay/0000-0002-5529-673X; Krunz, Marwan/0000-0001-7137-2985
CR Ahuja R. K., 1993, Network flows: theory, algorithms, and applications
   ALLES A, 1995, ATM INTERNETWORKING
   ANEJA YP, 1978, NAV RES LOG, V25, P549, DOI 10.1002/nav.3800250314
   ANEJA YP, 1983, NETWORKS, V13, P295, DOI 10.1002/net.3230130212
   [Anonymous], 1998, 2386 IETF RFC
   APOSTOLOPOULOS G, 1999, 2676 IETF RFC
   *ATM FOR, 1996, PRIV NETW TO NETW IN
   Bennett JCR, 1996, IEEE INFOCOM SER, P120, DOI 10.1109/INFCOM.1996.497885
   Blokh D., 1996, AUSTRALAS J COMB, V14, P157
   Calvert KL, 1997, IEEE COMMUN MAG, V35, P160, DOI 10.1109/35.587723
   CHEN S, 1997, UIUCDCSR972026
   Chen SG, 1998, IEEE NETWORK, V12, P64, DOI 10.1109/65.752646
   Chen SG, 1998, ICC 98 - 1998 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS VOLS 1-3, P874, DOI 10.1109/ICC.1998.685137
   Chen TM, 1999, IEEE COMMUN MAG, V37, P58, DOI 10.1109/35.809386
   Cidon I, 1997, IEEE INFOCOM SER, P92, DOI 10.1109/INFCOM.1997.635118
   Clark D, 1996, ACM COMPUT SURV, V28, P679, DOI 10.1145/242223.242273
   Comer D.E., 1995, INTERNETWORKING TCP, V1
   Cormen T.H., 1996, INTRO ALGORITHMS
   De Neve H, 1998, 1998 IEEE ATM WORKSHOP PROCEEDINGS, P324, DOI 10.1109/ATM.1998.675192
   Eppstein D., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P154, DOI 10.1109/SFCS.1994.365697
   ERGUN F, 2000, P INFOCOM 2000 C IEE, V1, P137
   Fortz B., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P519, DOI 10.1109/INFCOM.2000.832225
   Garey M.R., 1979, COMPUTERS INTRACTABI
   Goel A, 2001, IEEE INFOCOM SER, P854, DOI 10.1109/INFCOM.2001.916276
   GOLESTANI SJ, 1994, IEEE INFOCOM SER, P636, DOI 10.1109/INFCOM.1994.337677
   Guerin R. A., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P118, DOI 10.1109/INFCOM.2000.832180
   Guérin RA, 1999, IEEE ACM T NETWORK, V7, P350, DOI 10.1109/90.779203
   Guo L, 1999, INT CON DISTR COMP S, P142, DOI 10.1109/ICDCS.1999.776515
   HANDLER GY, 1980, NETWORKS, V10, P293, DOI 10.1002/net.3230100403
   HASSIN R, 1992, MATH OPER RES, V17, P36, DOI 10.1287/moor.17.1.36
   IETF, 1998, 2328 IETF RFC
   Ishida K, 1998, FIFTH INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING SYSTEMS AND APPLICATIONS, PROCEEDINGS, P58, DOI 10.1109/RTCSA.1998.726352
   Iwata A, 1996, IEICE T COMMUN, VE79B, P999
   JAFFE JM, 1984, NETWORKS, V14, P95, DOI 10.1002/net.3230140109
   Jüttner A, 2001, IEEE INFOCOM SER, P859, DOI 10.1109/INFCOM.2001.916277
   Kodialam M., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P902, DOI 10.1109/INFCOM.2000.832265
   Korkmaz T, 2002, COMPUT COMMUN, V25, P225, DOI 10.1016/S0140-3664(01)00358-9
   Korkmaz T, 2001, COMPUT NETW, V36, P251, DOI 10.1016/S1389-1286(00)00209-7
   LEE WC, 1995, IEEE NETWORK, V9, P46, DOI 10.1109/65.397043
   Lorenz DH, 2000, INT WORKSH QUAL SERV, P75, DOI 10.1109/IWQOS.2000.847940
   MA Q, 1998, P NOSSDAV 98 JUL
   Ma QM, 1997, 1997 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS - PROCEEDINGS, P191, DOI 10.1109/ICNP.1997.643714
   Orda A, 1999, IEEE ACM T NETWORK, V7, P365, DOI 10.1109/90.779205
   Phillips C. A., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P776, DOI 10.1145/167088.167286
   Pornavalai C, 1997, 1997 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS - PROCEEDINGS, P167, DOI 10.1109/ICNP.1997.643711
   Raghavan S.V., 1998, NETWORKED MULTIMEDIA
   Reeves DS, 2000, IEEE ACM T NETWORK, V8, P239, DOI 10.1109/90.842145
   Sriram R, 1998, COMPUT COMMUN, V21, P1655, DOI 10.1016/S0140-3664(98)00194-7
   Stoica I, 1999, COMP COMM R, V29, P81, DOI 10.1145/316194.316208
   Sun Q, 1998, COMPUT COMMUN, V21, P572, DOI 10.1016/S0140-3664(98)00127-3
   Taft-Plotkin N, 1999, INT WORKSH QUAL SERV, P119, DOI 10.1109/IWQOS.1999.766485
   Verma S, 1998, PERFORM EVALUATION, V34, P273, DOI 10.1016/S0166-5316(98)00041-8
   Vogel R, 1996, IEEE J SEL AREA COMM, V14, P1235, DOI 10.1109/49.536365
   Wang Z, 1996, IEEE J SEL AREA COMM, V14, P1228, DOI 10.1109/49.536364
   Wang Z, 1995, GLOB TELECOMM CONF, P2129, DOI 10.1109/GLOCOM.1995.502780
   Wang Z, 1999, INFORM PROCESS LETT, V69, P111, DOI 10.1016/S0020-0190(98)00206-3
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Widyono R., 1994, Technical Report TR-94-024
   Xiao XP, 1999, IEEE NETWORK, V13, P8, DOI 10.1109/65.768484
   YUAN X, 1999, P 8 INT C COMP COMM, P304
   [No title captured]
   [No title captured]
   [No title captured]
NR 63
TC 35
Z9 39
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 429
EP 443
DI 10.1109/TMM.2003.811627
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500014
DA 2024-07-18
ER

PT J
AU Lan, TH
   Tewfik, AH
AF Lan, TH
   Tewfik, AH
TI A resource management strategy in wireless multimedia communications -
   Total power saving in mobile terminals with a guaranteed QoS
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE complexity scalable coding; energy-aware design; low power; video over
   wireless channel; wireless multimedia communications
AB The integration of multimedia services into wireless communication networks is a major source of future technological advances. One of the main challenging issues in this endeavor is the resource optimization strategy. This paper addresses this issue from the perspective of minimizing the total power consumption of a mobile terminal while maintaining a guaranteed quality-of-service (QoS). For many years, the management strategy has dealt primarily with bandwidth allocation, network capacity, and QoS. However, due to the integration of multimedia services, the increasing energy consumption of a mobile unit is also becoming a dominant factor in the design of communication systems. Here, we describe two technologies that can make a wireless multimedia communication system more energy-efficient while ensuring QoS. These technologies consist of an energy-efficient communication protocol for the uplink channel and a low-complexity multirate transmission scheme. We also provide a video transmission example using the H.263 standard in the proposed system to demonstrate the importance of our total power optimization strategy. The simulation results show that a savings of 10-32% is achieved in the total energy consumption of the mobile unit.
C1 Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
   Philips Res, Briarcliff Manor, NY 10510 USA.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Philips; Philips Research
RP EGT, Atlanta, GA 30318 USA.
EM jlan@egtinc.com; tewfik@ece.umn.edu
CR ABIDI AA, 1995, P IEEE, V83, P544, DOI 10.1109/5.371966
   AONO K, 1992, IEEE J SOLID-ST CIRC, V27, P1886, DOI 10.1109/4.173119
   CHANDRAKASAN AP, 1995, P IEEE, V83, P498, DOI 10.1109/5.371964
   CHEUNG G, 1996, P 1996 IEEE INT C IM
   DERTOUZOS M, 1999, SCI AM           AUG
   FARVARDIN N, 1990, IEEE T INFORM THEORY, V36, P799, DOI 10.1109/18.53739
   FOSCHINI GJ, 1993, IEEE T VEH TECHNOL, V42, P641, DOI 10.1109/25.260747
   Gonzalez R, 1996, IEEE J SOLID-ST CIRC, V31, P1277, DOI 10.1109/4.535411
   GRANDHI SA, 1993, IEEE T VEH TECHNOL, V42, P466, DOI 10.1109/25.260766
   *ITU, 1995, ITU T DRAFT H263
   KOHNO R, 1995, IEEE COMMUN MAG  JAN
   Kozintsev I, 1996, INT CONF ACOUST SPEE, P2343, DOI 10.1109/ICASSP.1996.547752
   KUHN P, IPROF
   LAN T, 1999, P 1999 IEEE INT C IM
   Lan TH, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P597, DOI 10.1109/MMSP.1998.739046
   LAN TH, 1997, P IEEE 1 WORKSH MULT
   LAN TH, 1997, THESIS
   LETTIERI P, 1997, P MOBICOM 97
   NARAYANSAWAMY S, 1995, IEEE PERS COMMUN APR, P4
   PADOVANI R, 1994, IEEE PERS COMMUN
   RAMCHANDRAN K, 1993, IEEE J SEL AREA COMM, V11, P6, DOI 10.1109/49.210540
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   *TELENOR, H 263 SOFTW SIM
   Tiwari V., 1994, IEEE Transactions on Very Large Scale Integration (VLSI) Systems, V2, P437, DOI 10.1109/92.335012
   TURNER C, 1997, CALCULATION TMS320LC
   VITERBI AJ, 1994, IEEE PERS COMMUN
   Webb WilliamT., 1994, MODERN QUADRATURE AM
   Woesner H, 1998, IEEE PERS COMMUN, V5, P40, DOI 10.1109/98.683738
   ZANDER J, 1992, IEEE T VEH TECHNOL, V41, P57, DOI 10.1109/25.120145
   Zorzi M, 1997, IEEE T COMPUT, V46, P279, DOI 10.1109/12.580424
NR 30
TC 20
Z9 20
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 267
EP 281
DI 10.1109/TMM.2003.812714
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100011
DA 2024-07-18
ER

PT J
AU Cen, S
   Cosman, PC
AF Cen, S
   Cosman, PC
TI Decision trees for error concealment in video decoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE Wireless Communications and Networking Conference
CY SEP 21-24, 1999
CL NEW ORLEANS, LA
SP IEEE
DE adaptive error concealment; decision trees; MPEG video
ID RECOVERY
AB When macroblocks are lost in a video decoder such as MPEG-2, the decoder can try to conceal the error by estimating or interpolating the missing area. Many different methods for this type of post-processing concealment have been proposed, operating in the spatial, frequency, or temporal domains, or some hybrid combination of them. In this paper, we show how the use of a decision tree that can adaptively choose among several different error concealment methods can outperform each single method. We also propose two promising new methods for temporal error concealment.
C1 Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM pcosman@code.ucsd.edu
OI Cosman, Pamela/0000-0002-4012-0176
CR AIGN S, 1998, P VCIP 98 JAN, P405
   Alexandre C, 1997, SIGNAL PROCESS-IMAGE, V11, P105, DOI 10.1016/S0923-5965(96)00039-2
   Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   Cen S, 1999, IEEE DATA COMPR CONF, P384, DOI 10.1109/DCC.1999.755688
   CEN S, 1999, P 1999 IEEE WIR COMM, P329
   Cuenca P, 1997, IEEE PACIF, P912, DOI 10.1109/PACRIM.1997.620408
   Cuenca P, 1997, 1997 CANADIAN CONFERENCE ON ELECTRICAL AND COMPUTER ENGINEERING, CONFERENCE PROCEEDINGS, VOLS I AND II, P118, DOI 10.1109/CCECE.1997.614804
   Ghanbari M, 1993, IEEE T CIRC SYST VID, V3, P238, DOI 10.1109/76.224234
   HEMAMI SS, 1995, IEEE T IMAGE PROCESS, V4, P1023, DOI 10.1109/83.392344
   Hong MC, 1999, SIGNAL PROCESS-IMAGE, V14, P473, DOI 10.1016/S0923-5965(98)00061-7
   LUO WJ, 1995, P SOC PHOTO-OPT INS, V2501, P1358, DOI 10.1117/12.206670
   Park JW, 1997, IEEE T CIRC SYST VID, V7, P845, DOI 10.1109/76.644064
   SALAMA P, 1995, P IEEE ICIP, P405
   Suh JW, 1997, IEEE T CONSUM ELECTR, V43, P295, DOI 10.1109/30.628616
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wiegand T., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P51, DOI 10.1109/ICIP.1999.821563
   Zhu QF, 1993, IEEE T CIRC SYST VID, V3, P248, DOI 10.1109/76.224235
NR 17
TC 24
Z9 27
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 1
EP 7
DI 10.1109/TMM.2003.808825
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200001
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Feng, TT
   Qi, Q
   Wang, JY
   Liao, JX
   Liu, JC
AF Feng, Tongtong
   Qi, Qi
   Wang, Jingyu
   Liao, Jianxin
   Liu, Jiangchuan
TI Timely and Accurate Bitrate Switching in HTTP Adaptive Streaming With
   Date-Driven I-Frame Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Streaming media; Switches; Servers; Bandwidth; Standards;
   Quality of experience; HTTP adaptive streaming; segmentation; bitrate
   switching; I-frame prediction; reinforcement learning
ID VIDEO
AB In today's Internet, bandwidth dynamics are inevitable, and hence, the bitrate for live streaming applications should also be dynamically adjusted. However, in existing HTTP-based adaptive streaming (HAS), bitrate switching can only be performed at segment boundaries, making decisions unresponsive and often inaccurate. In this paper, we start from a close investigation on the impact of the segment length in HAS and accordingly present VHAS, an extension towards intelligent variable-length segmentation, which makes client-side decisions based on the massive amount of real-time information from the network and viewers. VHAS implements a smart trigger mechanism that balances accuracy and overhead for variablelength segmentation. We further develop an adaptive bitrate switching algorithm with data-driven I-frame prediction, which is tailored to individual viewers to minimize bitrate mismatches. We evaluate VHAS via extensive trace-driven simulations, and our results demonstrate that compared with state-of-the-art solutions, VHAS achieves 15%-49% gains in QoE, with a noticeable bandwidth reduction of 37%-57%.
C1 [Feng, Tongtong; Qi, Qi; Wang, Jingyu; Liao, Jianxin] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
   [Feng, Tongtong; Qi, Qi; Wang, Jingyu; Liao, Jianxin] EBUPT COM, Beijing 100191, Peoples R China.
   [Liu, Jiangchuan] Simon Fraser Univ, TASC I 9005, Burnaby, BC, Canada.
C3 Beijing University of Posts & Telecommunications; Simon Fraser
   University
RP Wang, JY; Liao, JX (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.; Liu, JC (corresponding author), Simon Fraser Univ, TASC I 9005, Burnaby, BC, Canada.
EM ftt@bupt.edu.cn; QiQi8266@bupt.edu.cn; Wangjingyu@bupt.edu.cn;
   liaojx@bupt.edu.cn; jcliu@cs.sfu.ca
RI Wang, Jingyu/JFK-6346-2023; Feng, Tongtong/ABD-4886-2020; qi,
   li/JFE-7167-2023
OI Wang, Jingyu/0000-0002-2182-2228; Feng, Tongtong/0000-0003-4734-5607;
   Qi, Qi/0000-0003-0829-4624
FU National Natural Science Foundation of China [62071067, 62171057,
   62101064, 62001054]; Ministry of Education [MCM20200202]; China Mobile
   [MCM20200202]; BUPT Excellent Ph.D. Students Foundation [CX2020230]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62071067, 62171057, 62101064, and
   62001054, in part by the Ministry of Education and China Mobile Joint
   Fund MCM20200202, and in part by BUPT Excellent Ph.D. Students
   Foundation under Grant CX2020230.
CR Beben A, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P13, DOI 10.1145/2910017.2910596
   Chen H, 2020, IEEE T MULTIMEDIA, V22, P459, DOI 10.1109/TMM.2019.2928497
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Feng TT, 2020, IEEE T MULTIMEDIA, V22, P2963, DOI 10.1109/TMM.2019.2962313
   Guo J, 2020, IEEE T CIRC SYST VID, V30, P4355, DOI 10.1109/TCSVT.2019.2955136
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Huang TC, 2020, IEEE J SEL AREA COMM, V38, P2324, DOI 10.1109/JSAC.2020.3000363
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Jiang ZB, 2019, IEEE T MULTIMEDIA, V21, P1577, DOI 10.1109/TMM.2018.2881095
   Kim S, 2019, IEEE T MULTIMEDIA, V21, P442, DOI 10.1109/TMM.2018.2856626
   Li Z.-N., 2004, Fundamentals of Multimedia
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   LU Z, 2018, P CHI C HUM FACTORS, P1
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mnih V., 2016, International Conference on Machine Learning, P1928, DOI DOI 10.48550/ARXIV.1602.01783
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Schulman J., 2017, ARXIV
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   SHEN M, 2020, 2020 IEEEACM 28 INT
   Song JR, 2020, IEEE T MULTIMEDIA, V22, P2366, DOI 10.1109/TMM.2019.2957976
   Spiteri K, 2020, IEEE ACM T NETWORK, V28, P1698, DOI 10.1109/TNET.2020.2996964
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang FX, 2019, IEEE INFOCOM SER, P910, DOI [10.1109/infocom.2019.8737456, 10.1109/INFOCOM.2019.8737456]
   Wang XF, 2020, IEEE COMMUN SURV TUT, V22, P869, DOI 10.1109/COMST.2020.2970550
   Wu YH, 2017, ADV NEUR IN, V30
   Xiao KF, 2020, IEEE T MULTIMEDIA, V22, P474, DOI 10.1109/TMM.2019.2929929
   Xiao XD, 2020, IEEE T MULTIMEDIA, V22, P1567, DOI 10.1109/TMM.2019.2945167
   Xu ZM, 2019, IEEE T CIRC SYST VID, V29, P1781, DOI 10.1109/TCSVT.2018.2849015
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhan C, 2020, IEEE T MULTIMEDIA, V22, P795, DOI 10.1109/TMM.2019.2931441
   Zhang T, 2020, IEEE T MOBILE COMPUT, V19, P1715, DOI 10.1109/TMC.2019.2912750
NR 35
TC 4
Z9 4
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3753
EP 3762
DI 10.1109/TMM.2022.3165381
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500017
DA 2024-07-18
ER

PT J
AU Gao, GW
   Xu, GA
   Li, JC
   Yu, Y
   Lu, HM
   Yang, J
AF Gao, Guangwei
   Xu, Guoan
   Li, Juncheng
   Yu, Yi
   Lu, Huimin
   Yang, Jian
TI FBSNet: A Fast Bilateral Symmetrical Network for Real-Time Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature aggregation; local dependencies; real-time; semantic
   segmentation
AB Real-time semantic segmentation, which can be visually understood as the pixel-level classification task on the input image, currently has broad application prospects, especially in the fast-developing fields of autonomous driving and drone navigation. However, the huge burden of calculation together with redundant parameters are still the obstacles to its technological development. In this article, we propose a Fast Bilateral Symmetrical Network (FBSNet) to alleviate the above challenges. Specifically, FBSNet employs a symmetrical encoder-decoder structure with two branches, semantic information branch and spatial detail branch. The Semantic Information Branch (SIB) is the main branch with semantic architecture to acquire the contextual information of the input image and meanwhile acquire sufficient receptive field. While the Spatial Detail Branch (SDB) is a shallow and simple network used to establish local dependencies of each pixel for preserving details, which is essential for restoring the original resolution during the decoding phase. Meanwhile, a Feature Aggregation Module (FAM) is designed to effectively combine the output of these two branches. Experimental results of Cityscapes and CamVid show that the proposed FBSNet can strike a good balance between accuracy and efficiency. Specifically, it obtains 70.9% and 68.9% mIoU along with the inference speed of 90 fps and 120 fps on these two test datasets, respectively, with only 0.62 million parameters on a single RTX 2080Ti GPU. The code is available at https://github.com/IVIPLab/FBSNet.
C1 [Gao, Guangwei; Xu, Guoan] Nanjing Univ Posts & Telecommun, Inst Adv Technol, Nanjing 210023, Peoples R China.
   [Gao, Guangwei; Xu, Guoan; Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
   [Li, Juncheng] Chinese Univ Hong Kong, Ctr Math Artificial Intelligence, Dept Math, Hong Kong, Peoples R China.
   [Lu, Huimin] Kyushu Inst Technol, Dept Mech & Control Engn, Kitakyushu 8048550, Japan.
   [Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Research Organization
   of Information & Systems (ROIS); National Institute of Informatics (NII)
   - Japan; Chinese University of Hong Kong; Kyushu Institute of
   Technology; Nanjing University of Science & Technology
RP Yu, Y (corresponding author), Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.; Li, JC (corresponding author), Chinese Univ Hong Kong, Ctr Math Artificial Intelligence, Dept Math, Hong Kong, Peoples R China.
EM csggao@gmail.com; xga_njupt@163.com; cvjunchengli@gmail.com;
   yiyu@nii.ac.jp; dr.huimin.lu@ieee.org; csjyang@njust.edu.cn
RI Li, Juncheng/AHA-3971-2022; WANG, SHIHAO/KHC-8263-2024; li,
   xiaomin/KCX-9845-2024
OI Lu, Huimin/0000-0001-9794-3221; Li, Juncheng/0000-0001-7314-6754
FU National Natural Science Foundation of China [61972212, 61772568,
   61833011]; Natural Science Foundation of Jiangsu Province [BK20190089];
   Six Talent Peaks Project in Jiangsu Province [RJFW-011]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61972212, 61772568, and 61833011, in
   part by the Natural Science Foundation of Jiangsu Province under Grant
   BK20190089, and in part by the Six Talent Peaks Project in Jiangsu
   Province under Grant RJFW-011. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was
   Dr.ChangXu.(Guangwei Gao and Guoan Xu contributed equally to this work.)
   (Corresponding authors: Juncheng Li; Yi Yu.)
CR [Anonymous], 2010, P INT C MACH LEARN
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   CHEN LC, 2017, ARXIV PREPRINT ARXIV, V1706, P5587, DOI DOI 10.48550/ARXIV.1706.05587
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao GW, 2022, IEEE T INTELL TRANSP, V23, P25489, DOI 10.1109/TITS.2021.3098355
   Gao GW, 2022, IEEE T CIRC SYST VID, V32, P2550, DOI 10.1109/TCSVT.2020.3042178
   Gao GW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107539
   Gao GW, 2017, PATTERN RECOGN, V66, P129, DOI 10.1016/j.patcog.2016.12.021
   Han HY, 2021, IEEE T INTELL TRANSP, V22, P1041, DOI 10.1109/TITS.2019.2962094
   Hao SJ, 2022, Arxiv, DOI arXiv:2005.11034
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2021, PROC CVPR IEEE, P13708, DOI 10.1109/CVPR46437.2021.01350
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li G., 2019, P BRIT MACH VIS C BM
   Li HC, 2019, PROC CVPR IEEE, P9514, DOI 10.1109/CVPR.2019.00975
   Li Y, 2017, IEEE T SYST MAN CY-S, V47, P648, DOI 10.1109/TSMC.2016.2623683
   Liu J, 2020, INT CONF ACOUST SPEE, P2373, DOI [10.1109/icassp40776.2020.9053838, 10.1109/ICASSP40776.2020.9053838]
   Liu W, 2016, INT C LEARN REPR ICL
   Lo S. Y., 2019, P ACM MULTIMEDIA ASI, P1, DOI DOI 10.1145/3338533.3366558
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Mehta S, 2019, PROC CVPR IEEE, P9182, DOI 10.1109/CVPR.2019.00941
   Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34
   Paszke A., 2017, NAT COMMUN, P1, DOI DOI 10.1038/S41467-016-0009-6
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Poudel R.P.K., 2019, ARXIV190204502, P289
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Treml M., 2016, P NIPS, P1
   Wang JW, 2020, APPL INTELL, V50, P1045, DOI 10.1007/s10489-019-01587-1
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y, 2019, IEEE IMAGE PROC, P1860, DOI [10.1109/icip.2019.8803154, 10.1109/ICIP.2019.8803154]
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu TY, 2021, IEEE T IMAGE PROCESS, V30, P1169, DOI 10.1109/TIP.2020.3042065
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yang ZG, 2021, IEEE T INTELL TRANSP, V22, P5508, DOI 10.1109/TITS.2020.2987816
   Yu CQ, 2021, INT J COMPUT VISION, V129, P3051, DOI 10.1007/s11263-021-01515-2
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XT, 2019, IEEE T IND INFORM, V15, P1183, DOI 10.1109/TII.2018.2849348
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 54
TC 32
Z9 32
U1 22
U2 51
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3273
EP 3283
DI 10.1109/TMM.2022.3157995
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200024
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Junayed, MS
   Islam, MB
AF Junayed, Masum Shah
   Islam, Md Baharul
TI Consistent Video Inpainting Using Axial Attention-Based Style
   Transformer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Index Terms-Video Inpainting; Deep Encoder; Axial Attention Block;
   Transformer; Style Manipulation Block; Relative Positional Encoding
AB Maintaining spatial and temporal consistency in the inpainted video area of the video is a challenging problem. Recent research focuses on flow information for synthesizing temporally smooth pixels while neglecting semantic structural coherence across the video frames. Thus, it suffers from over-smoothing and shadowy outlines that significantly degrade the inpainted video quality. We propose an end-to-end consistent video inpainting model that will substantially improve the inpainted video region to overcome this problem. The model employs a deep encoder (DE), axial attention block (AAB), style transformer, and decoder to enhance video inpainting with a realistic structure. A deep encoder (DE) encodes features effectively while the axial attention block (AAB) recreates all retrieved attributes by merging recoverable multi-scale characteristics with local spatial structures. Then, a novel-style transformer with the style manipulation block (SMB) fills the missing area with rich visual elements and temporal coherence. We use two publicly available benchmark datasets to assess the model's performance. Experimental results demonstrate that our method performs better than the state-of-the-art methods by a large margin. Besides, an extensive ablation study validates the model's performance.
C1 [Junayed, Masum Shah; Islam, Md Baharul] Bahcesehir Univ, Comp Engn, TR-34349 Istanbul, Turkiye.
C3 Bahcesehir University
RP Islam, MB (corresponding author), Bahcesehir Univ, Comp Engn, TR-34349 Istanbul, Turkiye.
EM masumshahjunayed@gmail.com; bislam.eng@gmail.com
RI Islam, Md Baharul/R-3751-2019
OI Islam, Md Baharul/0000-0002-9928-5776; Junayed, Masum
   Shah/0000-0003-3592-4601
FU Scientific and Technological Research Council of Turkey through 2232
   Outstanding Researchers Program [118C301]
FX This work was supported by the Scientific and Technological Research
   Council of Turkey through 2232 Outstanding Researchers Program under
   Project 118C301.
CR Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Chang YL, 2019, Arxiv, DOI arXiv:1907.01131
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P713, DOI 10.1007/978-3-030-58610-2_42
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Dehan L, 2022, IEEE COMPUT SOC CONF, P686, DOI 10.1109/CVPRW56347.2022.00084
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   He Pengcheng, 2020, arXiv, DOI 10.48550/arXiv.2006.03654
   Huang JB, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982398
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim D, 2020, IEEE T PATTERN ANAL, V42, P1038, DOI 10.1109/TPAMI.2019.2958083
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li HD, 2019, IEEE I CONF COMP VIS, P8300, DOI 10.1109/ICCV.2019.00839
   Ling CH, 2011, IEEE T MULTIMEDIA, V13, P292, DOI 10.1109/TMM.2010.2095000
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Liu R, 2021, Arxiv, DOI arXiv:2104.06637
   Liu R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14020, DOI 10.1109/ICCV48922.2021.01378
   Liu YL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2279, DOI 10.1109/ICCV48922.2021.00230
   Maggia B., 2021, Video outpainting using conditional generative adverarial networks
   Miao Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P1, DOI 10.1007/978-3-030-58589-1_1
   Newson A, 2014, SIAM J IMAGING SCI, V7, P1993, DOI 10.1137/140954933
   Oh SW, 2019, IEEE I CONF COMP VIS, P4402, DOI 10.1109/ICCV.2019.00450
   Patwardhan K. A., 2005, IEEE INT C IMAGE PRO, V2, pII
   Patwardhan KA, 2007, IEEE T IMAGE PROCESS, V16, P545, DOI 10.1109/TIP.2006.888343
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Strobel M, 2014, LECT NOTES COMPUT SC, V8753, P293, DOI 10.1007/978-3-319-11752-2_23
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, AAAI CONF ARTIF INTE, P5232
   Wang T., 2018, ARXIV
   Wang YQ, 2021, PROC CVPR IEEE, P8737, DOI 10.1109/CVPR46437.2021.00863
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zhang HT, 2019, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2019.00281
   Zhao YH, 2022, Arxiv, DOI arXiv:2201.08131
   Zhou Peng., 2021, arXiv
   Zhou YQ, 2021, PROC CVPR IEEE, P2266, DOI 10.1109/CVPR46437.2021.00230
   Zhu XS, 2018, SIGNAL PROCESS-IMAGE, V67, P90, DOI 10.1016/j.image.2018.05.015
NR 52
TC 2
Z9 2
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7494
EP 7504
DI 10.1109/TMM.2022.3222932
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000055
DA 2024-07-18
ER

PT J
AU Liang, DK
   Xu, W
   Zhu, YY
   Zhou, Y
AF Liang, Dingkang
   Xu, Wei
   Zhu, Yingying
   Zhou, Yu
TI Focal Inverse Distance Transform Maps for Crowd Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowd localization; crowd counting; crowd analysis; distance transform;
   FIDT map
ID SCALE; DEEP
AB In this paper, we focus on the crowd localization task, a crucial topic of crowd analysis. Most regression-based methods utilize convolution neural networks (CNN) to regress a density map, which can not accurately locate the instance in the extremely dense scene, attributed to two crucial reasons: 1) the density map consists of a series of blurry Gaussian blobs, 2) severe overlaps exist in the dense region of the density map. To tackle this issue, we propose a novel Focal Inverse Distance Transform (FIDT) map for the crowd localization task. Compared with the density maps, the FIDT maps accurately describe the persons' locations without overlapping in dense regions. Based on the FIDT maps, a Local-Maxima-Detection-Strategy (LMDS) is derived to effectively extract the center point for each individual. Furthermore, we introduce an Independent SSIM (I-SSIM) loss to make the model tend to learn the local structural information, better recognizing local maxima. Extensive experiments demonstrate that the proposed method reports state-of-the-art localization performance on six crowd datasets and one vehicle dataset. Additionally, we find that the proposed method shows superior robustness on the negative and extremely dense scenes, which further verifies the effectiveness of the FIDT maps.
C1 [Liang, Dingkang; Zhu, Yingying; Zhou, Yu] Huazhong Univ Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
   [Xu, Wei] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.
C3 Huazhong University of Science & Technology; Beijing University of Posts
   & Telecommunications
RP Zhu, YY; Zhou, Y (corresponding author), Huazhong Univ Sci & Technol, Wuhan 430074, Hubei, Peoples R China.
EM dkliang@hust.edu.cn; xuwei2020@bupt.edu.cn; yyzhu@hust.edu.cn;
   yuzhou@hust.edu.cn
RI Xu, Wei/F-8338-2011; zhu, yingying/K-8170-2018; liang,
   dingkang/ABG-6365-2021
OI Zhou, Yu/0000-0002-6674-6484; liang, dingkang/0000-0003-3035-1373
FU National Key Research and Development Program of China [2018AAA0100400];
   National Natural Science Foundation of China [62176098, 61703049]; Young
   Scientists Fund of the National Natural Science Foundation of China
   [62206103]; Natural Science Foundation of Hubei Province of China
   [2019CFA022]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100400, in part by the
   National Natural Science Foundation of China under Grants 62176098 and
   61703049, in part by the Young Scientists Fund of the National Natural
   Science Foundation of China under Grant 62206103, and in part by the
   Natural Science Foundation of Hubei Province of China under Grant
   2019CFA022.
CR Abousamra S, 2021, AAAI CONF ARTIF INTE, V35, P872
   Arteta C, 2016, LECT NOTES COMPUT SC, V9911, P483, DOI 10.1007/978-3-319-46478-7_30
   Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Gao JY, 2021, Arxiv, DOI arXiv:2012.04164
   Gao JY, 2019, Arxiv, DOI arXiv:1907.02724
   Gao JY, 2023, IEEE T NEUR NET LEAR, V34, P4803, DOI 10.1109/TNNLS.2021.3124272
   Gao JY, 2019, NEUROCOMPUTING, V363, P1, DOI 10.1016/j.neucom.2019.08.018
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   Han T., 2021, arXiv
   Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Laradji IH, 2018, LECT NOTES COMPUT SC, V11206, P560, DOI 10.1007/978-3-030-01216-8_34
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Liang DK, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3445-y
   Liang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P164, DOI 10.1007/978-3-030-58607-2_10
   Liu CC, 2019, PROC CVPR IEEE, P1217, DOI 10.1109/CVPR.2019.00131
   Liu LB, 2019, IEEE I CONF COMP VIS, P1774, DOI 10.1109/ICCV.2019.00186
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Mazzeo PL, 2020, J IMAGING, V6, DOI 10.3390/jimaging6070062
   Meng YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15529, DOI 10.1109/ICCV48922.2021.01526
   Olmschenk G, 2020, PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 5: VISAPP, P185, DOI 10.5220/0009156201850195
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ribera J, 2019, PROC CVPR IEEE, P6472, DOI 10.1109/CVPR.2019.00664
   ROSENFELD A, 1968, PATTERN RECOGN, V1, P33, DOI 10.1016/0031-3203(68)90013-7
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shi MJ, 2019, PROC CVPR IEEE, P7271, DOI 10.1109/CVPR.2019.00745
   Sindagi VA, 2022, IEEE T PATTERN ANAL, V44, P2594, DOI 10.1109/TPAMI.2020.3035969
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1221, DOI 10.1109/ICCV.2019.00131
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Wan J., 2020, P ADV NEUR INF PROC, P3386
   Wan J, 2021, PROC CVPR IEEE, P1974, DOI 10.1109/CVPR46437.2021.00201
   Wan J, 2022, IEEE T PATTERN ANAL, V44, P1357, DOI 10.1109/TPAMI.2020.3022878
   Wang B., 2020, Advances in Neural Information Processing Systems, V33, P1595, DOI DOI 10.48550/ARXIV.2009.13077
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang Q, 2021, IEEE T PATTERN ANAL, V43, P2141, DOI 10.1109/TPAMI.2020.3013269
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wang Y, 2020, PROC CVPR IEEE, P3832, DOI 10.1109/CVPR42600.2020.00389
   Wang Y, 2021, IEEE INT CONF MULTI, DOI 10.1109/ICMEW53276.2021.9455954
   Wang Y, 2021, IEEE T IMAGE PROCESS, V30, P2876, DOI 10.1109/TIP.2021.3055632
   Wang YK, 2019, PROC CVPR IEEE, P5282, DOI 10.1109/CVPR.2019.00543
   Xu CF, 2022, INT J COMPUT VISION, V130, P405, DOI 10.1007/s11263-021-01542-z
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Xu YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15550, DOI 10.1109/ICCV48922.2021.01528
   Yan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P242, DOI 10.1007/978-3-030-58555-6_15
   Yang YF, 2020, PROC CVPR IEEE, P4373, DOI 10.1109/CVPR42600.2020.00443
   Yifan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P1, DOI 10.1007/978-3-030-58598-3_1
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689
   Zhang A, 2019, IEEE I CONF COMP VIS, P5713, DOI 10.1109/ICCV.2019.00581
   Zhang C, 2016, IEEE T MULTIMEDIA, V18, P1048, DOI 10.1109/TMM.2016.2542585
   Zhang JL, 2021, IEEE T MULTIMEDIA, V23, P3085, DOI 10.1109/TMM.2020.3020691
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 66
TC 42
Z9 42
U1 8
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6040
EP 6052
DI 10.1109/TMM.2022.3203870
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500029
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, AA
   Zhou, HY
   Li, XY
   Wang, LJ
AF Liu, An-An
   Zhou, He-Yu
   Li, Xuanya
   Wang, Lanjun
TI Vulnerability of Feature Extractors in 2D Image-Based 3D Object
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D object retrieval; adversarial example; adversarial defense
ID NETWORKS
AB Recent advances in 3D modeling software and 3D capture devices contribute to the availability of large-scale 3D objects. Together with the prevalence of deep neural networks (DNNs), DNN-based 3D object retrieval systems are widely applied, especially by inputting 2D images to retrieve 3D objects. Although DNNs have shown vulnerable to adversarial attacks in classification, the vulnerability of DNN-based 3D object retrieval system remains under-explored. In this paper, we formulate the problem of attacking against DNN-based feature extractors in the 2D image-based 3D object retrieval system. Specifically, we consider the attack happens under a reasonable scenario that the candidate 3D object database is unknown to the adversary, which challenges adversarial example generation. To tackle this difficulty, we set up a reasonable hypothesis on the information which the adversary can be accessible, and then propose two effective perturbation generation methods: one is to corrupt domain-level alignment (CDA) and the other one is to corrupt class-level alignment (CCA). In converse, we propose a novel progressive adversarial training (PAT) method to improve the feature extractor robustness, which can effectively and stably mitigate both CDA and CCA attacks. Experimental results demonstrate that a typical feature extractor can be effectively compromised by attacks. Moreover, the transferability of the adversarial query illustrates the possibility of realistic black-box attacks. The successful defense against both CDA and CCA attacks by PAT can validate the superiority of the proposed defense method.
C1 [Liu, An-An; Zhou, He-Yu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Li, Xuanya] Baidu Inc, Beijing 100105, Peoples R China.
   [Wang, Lanjun] Tianjin Univ, Sch New Media & Commun, Tianjin 300072, Peoples R China.
C3 Tianjin University; Baidu; Tianjin University
RP Wang, LJ (corresponding author), Tianjin Univ, Sch New Media & Commun, Tianjin 300072, Peoples R China.
EM anan0422@gmail.com; zhy_std@163.com; lixuanya@baidu.com;
   wang.lanjun@outlook.com
RI Wang, Lanjun/AAM-7339-2020
OI Wang, Lanjun/0000-0002-7696-5330; , Heyu Zhou/0000-0001-9451-5600
FU National Natural Science Foundation of China [U21B2024]; National Key
   Research and Development Program of China [2021YFF0901602]; Baidu
   Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U21B2024, in part by the National Key
   Research and Development Program of China under Grant 2021YFF0901602,
   and in part by the Baidu Program.
CR Abdul-Rashid H., 2018, P EUR WORKSH 3D OBJ, P37
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Athalye A, 2018, PR MACH LEARN RES, V80
   Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10
   Cai QZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3740
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cherabier I, 2016, INT CONF 3D VISION, P601, DOI 10.1109/3DV.2016.68
   Dai GX, 2018, IEEE T IMAGE PROCESS, V27, P3374, DOI 10.1109/TIP.2018.2817042
   Ding G. W., 2019, INT C LEARN REPR
   Ding L, 2021, AAAI CONF ARTIF INTE, V35, P1236
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Feng Y, 2020, AAAI CONF ARTIF INTE, V34, P10786
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Feng YT, 2019, AAAI CONF ARTIF INTE, P8279
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   He XW, 2018, PROC CVPR IEEE, P1945, DOI 10.1109/CVPR.2018.00208
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Inkawhich N, 2019, PROC CVPR IEEE, P7059, DOI 10.1109/CVPR.2019.00723
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J, 2019, IEEE I CONF COMP VIS, P4898, DOI 10.1109/ICCV.2019.00500
   Li W, 2019, 12 EUR WORKSH 3D OBJ, P103, DOI DOI 10.2312/3DOR.20191068
   Li W., 2020, 3DOR, P37
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Madry A., 2018, ARXIV
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mo Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P781, DOI 10.1007/978-3-030-58568-6_46
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Papernot N., 2016, CORR
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Qi CR, 2017, ADV NEUR IN, V30
   Qin Y, 2019, PR MACH LEARN RES, V97
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Sehwag V, 2020, Advances in Neural Information Processing Systems (NeurIPS)
   Shafahi A, 2019, ADV NEUR IN, V32
   Song D, 2021, IEEE T MULTIMEDIA, V23, P2721, DOI 10.1109/TMM.2020.3015554
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2020, IEEE T CIRC SYST VID, V30, P3765, DOI 10.1109/TCSVT.2019.2942688
   Sun XL, 2021, ENG APPL ARTIF INTEL, V97, DOI 10.1016/j.engappai.2020.104085
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang Y., 2020, P INT C LEARN REPR
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong E., 2020, INT C LEARN REPR
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xu L, 2019, PROC CVPR IEEE, P3328, DOI 10.1109/CVPR.2019.00345
   Xu RJ, 2019, IEEE I CONF COMP VIS, P1426, DOI 10.1109/ICCV.2019.00151
   Ye SK, 2019, IEEE I CONF COMP VIS, P111, DOI 10.1109/ICCV.2019.00020
   Yin X., 2019, P INT C LEARN REPR
   Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017
   Zhou HY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P925, DOI 10.1145/3394171.3413631
   Zhou HY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P839
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
NR 61
TC 1
Z9 1
U1 6
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5065
EP 5076
DI 10.1109/TMM.2022.3186740
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300033
DA 2024-07-18
ER

PT J
AU Liu, H
   Liu, WT
   Chi, ZX
   Wang, Y
   Yu, YH
   Chen, J
   Tang, J
AF Liu, Huan
   Liu, Wentao
   Chi, Zhixiang
   Wang, Yang
   Yu, Yuanhao
   Chen, Jun
   Tang, Jin
TI Fast Human Pose Estimation in Compressed Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Pose estimation; Task analysis; Image coding; Semantics; Motion
   segmentation; Real-time systems; Human pose estimation; compressed
   video; deep neural network
ID MPEG
AB Current approaches for human pose estimation in videos can be categorized into per-frame and warping-based methods. Both approaches have their pros and cons. For example, per-frame methods are generally more accurate, but they are often slow. Warping-based approaches are more efficient, but the performance is usually not good. To bridge the gap, in this paper, we propose a novel fast framework for human pose estimation to meet the real-time inference with controllable accuracy degradation in compressed video domain. Our approach takes advantage of the motion representation (called "motion vector") that is readily available in a compressed video. Pose joints in a frame are obtained by directly warping the pose joints from the previous frame using the motion vectors. We also propose modules to correct possible errors introduced by the pose warping when needed. Extensive experimental results demonstrate the effectiveness of our proposed framework for accelerating the speed of top-down human pose estimation in videos.
C1 [Liu, Huan; Chen, Jun] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 2K1, Canada.
   [Liu, Wentao; Chi, Zhixiang; Yu, Yuanhao; Tang, Jin] Huawei Technol Canada, Noahs Ark Lab, Markham, ON L3R 5A4, Canada.
   [Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
   [Wang, Yang] Huawei Technol Canada, Markhan, ON L3R 5A4, Canada.
C3 McMaster University; Huawei Technologies; University of Manitoba; Huawei
   Technologies
RP Liu, H (corresponding author), McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 2K1, Canada.
EM liuh127@mcmaster.ca; wentao.liu2@huawei.com; zhixiang.chi@huawei.com;
   ywang@cs.umanitoba.ca; yuanhao.yu@huawei.com; chenjun@mcmaster.ca;
   tangjin@huawei.com
RI Qi, Ling/KHE-3068-2024
OI Liu, Huan/0000-0001-7155-3663; Yu, Yuanhao/0000-0001-8176-9716; Chi,
   Zhixiang/0000-0003-4560-4986
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156
   Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754
   [Anonymous], 2014, P ADV NEUR INF PROC
   Barjatya A., 2004, IEEE T EVOLUTION COM, V8, P225, DOI DOI 10.1109/TEVC.2004.826069
   Bertasius G., 2019, ADV NEUR IN
   Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chadha A, 2019, IEEE T CIRC SYST VID, V29, P475, DOI 10.1109/TCSVT.2017.2786999
   Chadha A, 2017, IEEE IMAGE PROC, P1832, DOI 10.1109/ICIP.2017.8296598
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Chen YM, 2011, IEEE T MULTIMEDIA, V13, P421, DOI 10.1109/TMM.2011.2127464
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Doering A., 2018, BMVC, P261
   Everingham M., 2010, BMVC, V2, P5
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Feng JY, 2022, IEEE T PATTERN ANAL, V44, P1591, DOI 10.1109/TPAMI.2020.3024646
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Guo H., 2018, PROC EUR C COMPUT VI
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kim ST, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10186497
   Krull A, 2015, IEEE I CONF COMP VIS, P954, DOI 10.1109/ICCV.2015.115
   Lee SW, 2000, IEEE T MULTIMEDIA, V2, P240, DOI 10.1109/6046.890059
   LEGALL D, 1991, COMMUN ACM, V34, P46, DOI 10.1145/103085.103090
   Li A., 2019, IEEE INT WORKSH MULT, P1
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Mukherjee D., 2015, SMPTE Motion Imaging Journal, V124, P44
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Ouyang WL, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.299
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Paszke A, 2019, ADV NEUR IN, V32
   Pei SC, 2002, IEEE T MULTIMEDIA, V4, P309, DOI 10.1109/TMM.2002.802841
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433
   Rafi U., 2016, BMVC, V1, P2
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Song J, 2017, PROC CVPR IEEE, P5563, DOI 10.1109/CVPR.2017.590
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang Z., 2018, P JOINT REC CHALL WO, V5
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiu Y., 2018, BMVC
   Zhang DW, 2018, PROC CVPR IEEE, P6762, DOI 10.1109/CVPR.2018.00707
   Zhang F, 2019, PROC CVPR IEEE, P3512, DOI 10.1109/CVPR.2019.00363
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   Zhang Z, 2020, Arxiv, DOI arXiv:1911.10346
   Zheng C, 2022, Arxiv, DOI [arXiv:2012.13392, DOI 10.48550/ARXIV.2012.13392]
NR 59
TC 9
Z9 9
U1 4
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1390
EP 1400
DI 10.1109/TMM.2022.3141888
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100028
DA 2024-07-18
ER

PT J
AU Liu, WD
   Zhang, C
   Ding, HH
   Hung, TY
   Lin, GS
AF Liu, Weide
   Zhang, Chi
   Ding, Henghui
   Hung, Tzu-Yi
   Lin, Guosheng
TI Few-Shot Segmentation With Optimal Transport Matching and Message Flow
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Task analysis; Semantics; Feature extraction;
   Electronic mail; Testing; Prototypes; CMNet; correspondence matching
   network; few-shot learning; message flow; optimal transport matching;
   segmentation
ID NETWORK
AB We tackle the challenging task of few-shot segmentation in this work. It is essential for few-shot semantic segmentation to fully utilize the support information. Previous methods typically adopt masked average pooling over the support feature to extract the support clues as a global vector, usually dominated by the salient part and lost certain essential clues. In this work, we argue that every support pixel's information is desired to be transferred to all query pixels and propose a Correspondence Matching Network (CMNet) with an Optimal Transport Matching module to mine out the correspondence between the query and support images. Besides, it is critical to fully utilize both local and global information from the annotated support images. To this end, we propose a Message Flow module to propagate the message along the inner-flow inside the same image and cross-flow between support and query images, which greatly helps enhance the local feature representations. Experiments on PASCAL VOC 2012, MS COCO, and FSS-1000 datasets show that our network achieves new state-of-the-art few-shot segmentation performance.
C1 [Liu, Weide] Nanyang Technol Univ NTU, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Zhang, Chi; Ding, Henghui; Lin, Guosheng] Inst Infocomm Res ASTAR, Singapore 138632, Singapore.
   [Hung, Tzu-Yi] Delta Res Ctr, Singapore 58357, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Lin, GS (corresponding author), Inst Infocomm Res ASTAR, Singapore 138632, Singapore.
EM weide001@e.ntu.edu.sg; chi007@e.ntu.edu.sg; ding0093@e.ntu.edu.sg;
   tzuyi.hung@deltaww.com; gslin@ntu.edu.sg
RI Ding, Henghui/C-7486-2019
OI Ding, Henghui/0000-0003-4868-6526; LIU, WEIDE/0000-0002-9855-4479
FU National Research Foundation, Singapore through AI Singapore Programme
   [AISG-RP-2018-003]; Academic Research Fund, Ministry of Education,
   Singapore [MOE-T2EP20220-0007, RG95/20]
FX This work was supported in part by the National Research Foundation,
   Singapore through AI Singapore Programme under Award AISG-RP-2018-003
   and in part by the Academic Research Fund, Ministry of Education,
   Singapore under Grants MOE-T2EP20220-0007 and RG95/20.
CR Azad R, 2021, IEEE WINT CONF APPL, P2673, DOI 10.1109/WACV48630.2021.00272
   Battaglia, 2018, ARXIV180601261
   Boudiaf M, 2021, PROC CVPR IEEE, P13974, DOI 10.1109/CVPR46437.2021.01376
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chi Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12200, DOI 10.1109/CVPR42600.2020.01222
   Cordonnier J.-B., 2019, P 8 INT C LEARN REPR
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Cuturi M., 2013, ADV NEURAL INFORM PR, P2292, DOI DOI 10.48550/ARXIV.1306.0895
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong N., 2018, P BRIT MACH VIS C
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Guolei Sun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P347, DOI 10.1007/978-3-030-58536-5_21
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hendryx S. M., 2019, arXiv
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Nguyen K, 2019, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2019.00071
   Li G, 2021, PROC CVPR IEEE, P8330, DOI 10.1109/CVPR46437.2021.00823
   Li L., 2022, PROC IEEE C COMPUT V, P8719
   Li X, 2020, PROC CVPR IEEE, P2866, DOI 10.1109/CVPR42600.2020.00294
   Lin GS, 2020, IEEE T PATTERN ANAL, V42, P1228, DOI 10.1109/TPAMI.2019.2893630
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WD, 2020, PROC CVPR IEEE, P4164, DOI 10.1109/CVPR42600.2020.00422
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu XK, 2022, IEEE T PATTERN ANAL, V44, P7885, DOI 10.1109/TPAMI.2021.3115815
   Rakelly K., 2018, P ICLR WORKSH
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Shaban A, 2017, ARXIV PREPRINT ARXIV, DOI 10.5244/C.31.167
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Siam M., 2019, P INT C LEARN REPR
   Siam M, 2019, IEEE I CONF COMP VIS, P5248, DOI 10.1109/ICCV.2019.00535
   Su ZY, 2015, IEEE T PATTERN ANAL, V37, P2246, DOI 10.1109/TPAMI.2015.2408346
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang WG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7283, DOI 10.1109/ICCV48922.2021.00721
   Zhang BF, 2021, PROC CVPR IEEE, P8308, DOI 10.1109/CVPR46437.2021.00821
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang C, 2019, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR.2019.00536
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XL, 2020, IEEE T CYBERNETICS, V50, P3855, DOI 10.1109/TCYB.2020.2992433
   Zhao Q, 2010, IEEE T PATTERN ANAL, V32, P274, DOI 10.1109/TPAMI.2008.299
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhou TF, 2021, PROC CVPR IEEE, P1622, DOI 10.1109/CVPR46437.2021.00167
NR 48
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5130
EP 5141
DI 10.1109/TMM.2022.3187855
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300037
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nawala, J
   Janowski, L
   Cmiel, B
   Rusek, K
   Pérez, P
AF Nawala, Jakub
   Janowski, Lucjan
   Cmiel, Bogdan
   Rusek, Krzysztof
   Perez, Pablo
TI Generalized Score Distribution: A Two-Parameter Discrete Distribution
   Accurately Describing Responses From Quality of Experience Subjective
   Experiments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discrete distribution; generalised score distribution; GSD; subjective
   experiments; quality of experience
AB Subjective responses from Multimedia Quality Assessment (MQA) experiments are conventionally analyzed with methods not suitable for the data type these responses represent. Furthermore, obtaining subjective responses is resource intensive. Thus, a method that allows the reuse of existing responses would be beneficial. Applying improper data analysis methods leads to difficulty in interpreting results. This increases the probability of drawing erroneous conclusions. Building upon existing subjective responses is resource friendly and helps develop machine learning (ML) based visual quality predictors. In this work, we show that using a discrete model for analyzing responses from MQA subjective experiments is feasible. We indicate that our proposed Generalized Score Distribution (GSD) properly describes response distributions observed in typical MQA experiments. We also highlight interpretability of GSD parameters and indicate that the GSD outperforms the approach based on sample empirical distribution when it comes to bootstrapping. Furthermore, we provide evidence that the GSD outcompetes the state-of-the-art model both in terms of goodness-of-fit and bootstrapping capabilities. To accomplish the aforementioned objectives, we analyze more than one million subjective responses from over 30 subjective experiments.
C1 [Nawala, Jakub; Janowski, Lucjan; Rusek, Krzysztof] AGH Univ Sci & Technol, Inst Telecommun, PL-30059 Krakow, Poland.
   [Cmiel, Bogdan] AGH Univ Sci & Technol, Dept Math Anal Computat Math & Probabil Methods, PL-30059 Krakow, Poland.
   [Perez, Pablo] Nokia Bell Labs, Applicat & Platforms Software Syst, Madrid 28050, Spain.
C3 AGH University of Krakow; AGH University of Krakow
RP Nawala, J (corresponding author), AGH Univ Sci & Technol, Inst Telecommun, PL-30059 Krakow, Poland.
EM jnawala@agh.edu.pl; janowski@kt.agh.edu.pl; cmielbog@mat.agh.edu.pl;
   krusek@agh.edu.pl; pablo.perez@nokia-bell-labs.com
RI Janowski, Lucjan/B-2264-2013; Rusek, Krzysztof/H-6694-2012
OI Janowski, Lucjan/0000-0002-3151-2944; Perez, Pablo/0000-0002-3502-6791;
   Rusek, Krzysztof/0000-0003-4336-7841
FU Norwegian Financial Mechanism [2019/34/H/ST6/00599]; PL-Grid
   Infrastructure
FX The work was supported by Norwegian Financial Mechanism 2014-2021 under
   Project 2019/34/H/ST6/00599 and in part by PL-Grid Infrastructure.
CR AGRESTI A., 2002, Categorical Data Analysis, DOI [10.1002/0471249688, DOI 10.1002/0471249688]
   AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   BECKER WE, 1992, ECONOMET THEOR, V8, P127, DOI 10.1017/S0266466600010781
   Efron B., 1993, INTRO BOOTSTRAP, DOI 10.1007/978-1-4899-4541-9
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Gao YX, 2022, Arxiv, DOI arXiv:2203.00926
   Gao YX, 2021, IEEE IMAGE PROC, P1574, DOI 10.1109/ICIP42928.2021.9506196
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Hossfeld T, 2020, PROCEEDINGS OF THE 2020 6TH IEEE CONFERENCE ON NETWORK SOFTWARIZATION (NETSOFT 2020): BRIDGING THE GAP BETWEEN AI AND NETWORK SOFTWARIZATION, P51, DOI 10.1109/NetSoft48620.2020.9165426
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   ITU-R, 2020, Rec. ITUR BT.500-14
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T, 2021, Recommendation P.913
   ITU-T Study Group 12, 1998, About International Telecommunication Union, Rec. P.Sup23 (02/98), DOI [11.1002/1000/4415, DOI 11.1002/1000/4415]
   Janowski L, 2015, IEEE T MULTIMEDIA, V17, P2210, DOI 10.1109/TMM.2015.2484963
   Li J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3339, DOI 10.1145/3394171.3413619
   Li Z., 2020, Electron. Imag., P1
   Li Z, 2017, IEEE DATA COMPR CONF, P52, DOI 10.1109/DCC.2017.26
   Liddell TM, 2018, J EXP SOC PSYCHOL, V79, P328, DOI 10.1016/j.jesp.2018.08.009
   miel B. C, 2022, arXiv
   Moller S, 2014, T-LAB SER TELECOMMUN, P1, DOI 10.1007/978-3-319-02681-7
   Naderi B, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123115
   Nawala J, 2022, Arxiv, DOI [arXiv:2204.07131, 10.48550/arXiv.2204.07131, DOI 10.48550/ARXIV.2204.07131]
   Nawala J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P852, DOI 10.1145/3394171.3413749
   Nguyen TT, 2018, INFORM SYST FRONT, V20, P1173, DOI 10.1007/s10796-017-9782-y
   Pagano M., 2018, Principles of Biostatistics
   Pérez P, 2021, IEEE T MULTIMEDIA, V24, P3442, DOI 10.1109/TMM.2021.3098450
   Pezzulli S, 2021, IEEE T MULTIMEDIA, V23, P2505, DOI 10.1109/TMM.2020.3013349
   Pinson M, 2010, REPORT VALIDATION VI
   Pinson M. H., 2014, Tech. Memo TM-14-505
   Pinson M. H., 2018, Tech. Memo TM-18-532
   Pinson M. H., 2019, Tech. Memo TM-19-537
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   SCHWEDER T, 1982, BIOMETRIKA, V69, P493
   Seufert M, 2021, Qual. User Experience, V6, P1, DOI DOI 10.1007/S41233-020-00044-Z
   Wickham H, 2014, J STAT SOFTW, V59, P1
   Ying ZQ, 2021, PROC CVPR IEEE, P14014, DOI 10.1109/CVPR46437.2021.01380
   Ying ZQ, 2020, PROC CVPR IEEE, P3572, DOI 10.1109/CVPR42600.2020.00363
NR 39
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6090
EP 6104
DI 10.1109/TMM.2022.3205444
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500032
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Qian, XL
   Zeng, YF
   Wang, W
   Zhang, QW
AF Qian, Xiaoliang
   Zeng, Yinfeng
   Wang, Wei
   Zhang, Qiuwen
TI Co-Saliency Detection Guided by Group Weakly Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-saliency detection; end-to-end training; group class activation maps;
   group weakly supervised learning
ID OBJECT DETECTION; DEEP
AB The detection results of many existing co-saliency detection methods are easily interfered by the unrelated salient objects, which have similar appearance characteristics to co-salient objects. Therefore, mining the inter-saliency cues which contain the common category information of multiple related images is the core of co-saliency detection. To address above concern, a novel group weakly supervised learning induced co-saliency detection (GWSCoSal) model is proposed in this paper. First of all, a novel group class activation maps (GCAM) network is constructed and trained through a group weakly supervised learning scheme, which adopts the common category of a group of related images as the ground truth. The GCAM produced by the trained GCAM network are considered as the inter-saliency cues, which can only highlight the regions covered by the objects with common category. Afterwards, the GCAM are integrated into a feature pyramid networks (FPN) based backbone trained by the pixel-level labels to infer the co-saliency maps. The group weakly supervised and the pixel-level learning are jointly implemented for end-to-end training of GWSCoSal model. The comprehensive comparisons with 13 state-of-the-art methods demonstrate that, our GWSCoSal model can detect the co-salient objects more accurately under the condition of being interfered by the similar unrelated salient objects, and the overall performance of which has achieved the level of state-of-the-art methods. The ablation study of our GWSCoSal model validates the effectiveness of proposed GCAM network.
C1 [Qian, Xiaoliang; Zeng, Yinfeng; Wang, Wei] Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou 450002, Peoples R China.
   [Zhang, Qiuwen] Zhengzhou Univ Light Ind, Coll Comp & Commun Engn, Zhengzhou 450002, Peoples R China.
C3 Zhengzhou University of Light Industry; Zhengzhou University of Light
   Industry
RP Wang, W (corresponding author), Zhengzhou Univ Light Ind, Coll Elect & Informat Engn, Zhengzhou 450002, Peoples R China.; Zhang, QW (corresponding author), Zhengzhou Univ Light Ind, Coll Comp & Commun Engn, Zhengzhou 450002, Peoples R China.
EM qxl_sunshine@163.com; zyf_0217@163.com; wangwei-zzuli@zzuli.edu.cn;
   zhangqwen@126.com
RI Qian, Xiaoliang/AAV-1480-2020; Zeng, Yinfeng/JEP-6871-2023
OI Zeng, Yinfeng/0009-0001-9251-9015; Qian, Xiaoliang/0000-0002-4328-6411
FU National Science Foundation of China [62076223, 61771432]; Key Science
   and Technology Program of Henan Province [202102210347, 202102210143];
   Basic Research Projects of Education Department of Henan [21zx003]
FX This work was supported in part by the National Science Foundation of
   China under Grants 62076223 and 61771432, in part by the Key Science and
   Technology Program of Henan Province under Grants 202102210347 and
   202102210143, and in part by the Basic Research Projects of Education
   Department of Henan under Grant 21zx003.
CR Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P2811, DOI 10.1109/TGRS.2017.2783902
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan Q, 2021, PROC CVPR IEEE, P12283, DOI 10.1109/CVPR46437.2021.01211
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Guo G., 2021, CVPR, P7403
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   Han JW, 2013, IEEE T IMAGE PROCESS, V22, P2723, DOI 10.1109/TIP.2013.2256919
   Hsu KJ, 2018, LECT NOTES COMPUT SC, V11209, P502, DOI 10.1007/978-3-030-01228-1_30
   Jacobs D.E., 2010, ACM S USER INTERFACE, P219
   Jerripothula KR, 2019, IEEE T CIRC SYST VID, V29, P744, DOI 10.1109/TCSVT.2018.2805811
   Jerripothula KR, 2016, LECT NOTES COMPUT SC, V9911, P187, DOI 10.1007/978-3-319-46478-7_12
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kingma D. P., 2014, arXiv
   Li B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Li JX, 2021, IEEE T MULTIMEDIA, V23, P1397, DOI 10.1109/TMM.2020.2997192
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Li ZW, 2018, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2018.00067
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720
   Qian XL, 2021, IEEE SIGNAL PROC LET, V28, P180, DOI 10.1109/LSP.2021.3049997
   Qian XL, 2020, NEURAL NETWORKS, V127, P132, DOI 10.1016/j.neunet.2020.04.012
   Qian XL, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12010143
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang L, 2021, IEEE T MULTIMEDIA, V23, P1287, DOI 10.1109/TMM.2020.2995266
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wei LN, 2019, IEEE T IMAGE PROCESS, V28, P5052, DOI 10.1109/TIP.2019.2909649
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Wei SK, 2019, IEEE T IMAGE PROCESS, V28, P4580, DOI 10.1109/TIP.2019.2913513
   Yao XW, 2021, IEEE T GEOSCI REMOTE, V59, P675, DOI 10.1109/TGRS.2020.2991407
   Yao XW, 2016, IEEE T GEOSCI REMOTE, V54, P3660, DOI 10.1109/TGRS.2016.2523563
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Zhang DW, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3158674
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang Kai, 2019, CVPR
   Zhang KH, 2020, AAAI CONF ARTIF INTE, V34, P12813
   Zhang QJ, 2021, IEEE T IMAGE PROCESS, V30, P1305, DOI 10.1109/TIP.2020.3042084
   Zhang QL, 2021, Arxiv, DOI arXiv:2103.13859
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 62
TC 48
Z9 48
U1 6
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1810
EP 1818
DI 10.1109/TMM.2022.3167805
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100017
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Qiu, CR
   Zhang, DH
   Hu, Y
   Li, HQ
   Sun, QB
   Chen, Y
AF Qiu, Chengrun
   Zhang, Dongheng
   Hu, Yang
   Li, Houqiang
   Sun, Qibin
   Chen, Yan
TI Radio-Assisted Human Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anchor-based one-stage detector; human detection; radio localization;
   two-stage detector
ID AOA LOCALIZATION; INTERNET
AB In this paper, we propose a radio-assisted human detection framework by incorporating radio information into the state-of-the-art detection methods, including anchor-based one-stage detectors and two-stage detectors. We extract the radio localization and identifier information from the radio signals to assist the human detection, due to which the problem of false positives and false negatives can be greatly alleviated. For both detectors, we use the confidence score revision based on the radio localization to improve the detection performance. For two-stage detection methods, we propose to utilize the region proposals generated from radio localization rather than relying on region proposal network (RPN). Moreover, with the radio identifier information, a non-max suppression method with the radio localization constraint has also been proposed to further suppress the false detections and reduce miss detections. Experiments on the simulative Microsoft COCO dataset and Caltech pedestrian datasets show that the mean average precision (mAP) and the miss rate of the state-of-the-art detection methods can be improved with the aid of radio information. Finally, we conduct experiments in real-world scenarios to demonstrate the feasibility of our proposed method in practice.
C1 [Qiu, Chengrun; Zhang, Dongheng; Sun, Qibin; Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Anhui, Peoples R China.
   [Hu, Yang; Li, Houqiang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Chen, Y (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Anhui, Peoples R China.
EM q940890508@gmail.com; dongheng@ustc.edu.cn; eeyhu@ustc.edu.cn;
   lihq@ustc.edu.cn; qibinsun@ustc.edu.cn; eecyan@ustc.edu.cn
RI Li, Houqiang Li/B-6259-2013
OI Zhang, Dongheng/0000-0001-6309-6626
FU National Natural Science Foundation of China [62172381]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172381.
CR Al-Sadoon MAG, 2020, IEEE T ANTENN PROPAG, V68, P6330, DOI 10.1109/TAP.2020.2981676
   Alahi A, 2015, IEEE I CONF COMP VIS, P3289, DOI 10.1109/ICCV.2015.376
   Amendola S, 2014, IEEE INTERNET THINGS, V1, P144, DOI 10.1109/JIOT.2014.2313981
   Bahl P., 2000, IEEE INFOCOM
   Bai L, 2022, IEEE T WIREL COMMUN, V21, P2051, DOI 10.1109/TWC.2021.3109146
   Bianchi V, 2019, IEEE T INSTRUM MEAS, V68, P566, DOI 10.1109/TIM.2018.2851675
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen Z, 2021, IEEE T MOBILE COMPUT, V20, P588, DOI 10.1109/TMC.2019.2950315
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dollár P, 2009, PROC CVPR IEEE, P304, DOI 10.1109/CVPRW.2009.5206631
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gao MF, 2018, LECT NOTES COMPUT SC, V11205, P155, DOI 10.1007/978-3-030-01246-5_10
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Golden SA, 2007, IEEE T MOBILE COMPUT, V6, P1185, DOI 10.1109/TMC.2007.1002
   Gong W, 2019, IEEE T MOBILE COMPUT, V18, P1380, DOI 10.1109/TMC.2018.2860018
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   Ishihara T, 2018, IEEE WINT CONF APPL, P596, DOI 10.1109/WACV.2018.00071
   Jara AJ, 2013, IEEE J SEL AREA COMM, V31, P47, DOI 10.1109/JSAC.2013.SUP.0513005
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li TH, 2019, IEEE I CONF COMP VIS, P872, DOI 10.1109/ICCV.2019.00096
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Lu X, 2019, Arxiv, DOI arXiv:1906.05688
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Marcaletti A, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P13, DOI 10.1145/2674005.2674998
   Hoang MT, 2019, IEEE INTERNET THINGS, V6, P10639, DOI 10.1109/JIOT.2019.2940368
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Papaioannou S, 2017, IEEE T MOBILE COMPUT, V16, P2351, DOI 10.1109/TMC.2016.2613523
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shao HJ, 2014, IEEE T SIGNAL PROCES, V62, P2580, DOI 10.1109/TSP.2014.2314064
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sun YM, 2020, IEEE T SIGNAL PROCES, V68, P2256, DOI 10.1109/TSP.2020.2981773
   Tian Z, 2022, IEEE T PATTERN ANAL, V44, P1922, DOI 10.1109/TPAMI.2020.3032166
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Vasish D, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P165
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang J, 2016, IEEE T IND INFORM, V12, P158, DOI 10.1109/TII.2015.2501225
   Wang ZQ, 2021, IEEE T MOBILE COMPUT, V20, P893, DOI 10.1109/TMC.2019.2954830
   Wen FX, 2015, IEEE SENS J, V15, P1538, DOI 10.1109/JSEN.2014.2364121
   Zanella A, 2014, IEEE INTERNET THINGS, V1, P22, DOI 10.1109/JIOT.2014.2306328
   Zhang DH, 2019, IEEE INTERNET THINGS, V6, P3899, DOI 10.1109/JIOT.2019.2893330
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhao YC, 2020, IEEE INTERNET THINGS, V7, P1024, DOI 10.1109/JIOT.2019.2948605
   Zhou XY, 2019, PROC CVPR IEEE, P850, DOI 10.1109/CVPR.2019.00094
NR 54
TC 5
Z9 5
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2613
EP 2623
DI 10.1109/TMM.2022.3149129
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sheng, ZH
   Liu, XW
   Cao, SY
   Shen, HL
   Zhang, HQ
AF Sheng, Zehua
   Liu, Xiongwei
   Cao, Si-Yuan
   Shen, Hui-Liang
   Zhang, Huaqi
TI Frequency-Domain Deep Guided Image Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frequency decomposition; guided image denoising; convolutional neural
   network
ID SPARSE; REPRESENTATION; RESTORATION; CNN
AB Despite the tremendous advances in denoising techniques, it's still challenging to restore a clean image with salient structures based on one noisy observation, especially at high noise levels. In this work, we propose a frequency-domain guided denoising algorithm to conduct denoising with the help of a well-aligned guidance image. Thanks to their structural correlations, the frequency characteristics of the guidance image can indicate whether the frequency coefficients of the noisy target image are contributed by noise or textures. Therefore, the explicit frequency decomposition enables our denoising model to avoid over-smoothing detailed contents. However, as two input images are usually captured in different fields, their structures are not always consistent. Therefore, we model guided denoising with an optimization problem which considers both the representation model of the guidance image and the fidelity to the noisy target. Further, we design a convolutional neural network, called as FGDNet, to explore the optimal solution. Due to the visual masking phenomenon, human eyes are sensitive to noise in the flat areas, but may not perceive noise around edges or textures. Therefore, we expect to remove as much noise as possible to guarantee the spatial smoothness of flat contents, while also preserving high-frequency structures. Through frequency decomposition, our model can process the low-frequency and high-frequency contents separately. We also adopt a frequency-relevant loss function to train the network. Experimental results show that, compared with state-of-the-art guided and non-guided denoisers, our FGDNet achieves higher denoising accuracy and better visual quality in both flat and texture-rich regions.
C1 [Sheng, Zehua; Liu, Xiongwei; Cao, Si-Yuan; Shen, Hui-Liang] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Zhang, Huaqi] Vivo Mobile Commun Co Ltd, Hangzhou 310030, Peoples R China.
C3 Zhejiang University
RP Shen, HL (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
EM shengzehua@zju.edu.cn; liuxw11@zju.edu.cn; karlcao@hotmail.com;
   shenhl@zju.edu.cn; zhanghuaqi@vivo.com
OI Sheng, Zehua/0000-0002-1721-9143; Shen, Hui-Liang/0000-0001-8469-019X
FU Ten Thousand Talents Program of Zhejiang Province [2020R52003]; ZJU-vivo
   Information Technology Joint Research Center
FX This work was supported in part by the Ten Thousand Talents Program of
   Zhejiang Province under Grant 2020R52003 and in part by the ZJU-vivo
   Information Technology Joint Research Center.
CR Aksoy Y, 2018, LECT NOTES COMPUT SC, V11213, P644, DOI 10.1007/978-3-030-01240-3_39
   Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Byun J, 2021, PROC CVPR IEEE, P5764, DOI 10.1109/CVPR46437.2021.00571
   Chen LY, 2022, LECT NOTES COMPUT SC, V13667, P17, DOI 10.1007/978-3-031-20071-7_2
   Chen LY, 2021, IEEE COMPUT SOC CONF, P182, DOI 10.1109/CVPRW53098.2021.00027
   Cheng S, 2021, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR46437.2021.00486
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Deng X, 2021, IEEE T PATTERN ANAL, V43, P3333, DOI 10.1109/TPAMI.2020.2984244
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729
   Du Y, 2021, IEEE T MULTIMEDIA, V23, P2139, DOI 10.1109/TMM.2020.3008057
   El Helou Majed, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P749, DOI 10.1007/978-3-030-58517-4_44
   Elad M, 2002, IEEE T IMAGE PROCESS, V11, P1141, DOI 10.1109/TIP.2002.801126
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Foi A, 2008, IEEE T IMAGE PROCESS, V17, P1737, DOI 10.1109/TIP.2008.2001399
   Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   HADDAD RA, 1991, IEEE T SIGNAL PROCES, V39, P723, DOI 10.1109/78.80892
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hou YK, 2020, IEEE T IMAGE PROCESS, V29, P5121, DOI 10.1109/TIP.2020.2980116
   KAHNEMAN D, 1968, PSYCHOL BULL, V70, P404, DOI 10.1037/h0026731
   Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z
   Kingma D. P., 2014, arXiv
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Krishnan D., 2009, P ADV NEURAL INFORM, V22, P1033
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491
   Lee W, 2022, PROC CVPR IEEE, P17704, DOI 10.1109/CVPR52688.2022.01720
   Li YJ, 2016, LECT NOTES COMPUT SC, V9908, P154, DOI 10.1007/978-3-319-46493-0_10
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Ma JY, 2022, IEEE T MULTIMEDIA, V24, P3157, DOI 10.1109/TMM.2021.3094058
   Ma RJ, 2022, IEEE T MULTIMEDIA, V24, P2366, DOI 10.1109/TMM.2021.3079697
   Mao XT, 2022, Arxiv, DOI [arXiv:2111.11745, DOI 10.48550/ARXIV.2111.11745]
   Mo H., 2020, P AS C COMP VIS, P168
   Mou C, 2022, PROC CVPR IEEE, P17378, DOI 10.1109/CVPR52688.2022.01688
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Pan JS, 2019, PROC CVPR IEEE, P1702, DOI 10.1109/CVPR.2019.00180
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Shi ZL, 2021, IEEE T IMAGE PROCESS, V30, P7472, DOI 10.1109/TIP.2021.3106812
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Soltanayev S, 2018, ADV NEUR IN, V31
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Tian CW, 2020, NEURAL NETWORKS, V124, P117, DOI 10.1016/j.neunet.2019.12.024
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wu HK, 2018, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2018.00197
   Xie W., 2021, P IEEE CVF INT C COM, P4308
   Xu J, 2018, LECT NOTES COMPUT SC, V11212, P21, DOI 10.1007/978-3-030-01237-3_2
   Yan Q, 2013, IEEE I CONF COMP VIS, P1537, DOI 10.1109/ICCV.2013.194
   Yin D., 2019, Adv. Neural Inform. Process. Syst, V32, P13276, DOI [10.48550/arXiv.1906.08988, DOI 10.48550/ARXIV.1906.08988]
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X., 2008, PROC IEEE C COMPUT V, P1
   Zou WB, 2021, IEEE INT CONF COMP V, P1895, DOI 10.1109/ICCVW54120.2021.00216
NR 60
TC 2
Z9 2
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6767
EP 6781
DI 10.1109/TMM.2022.3214375
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000005
DA 2024-07-18
ER

PT J
AU Tu, YB
   Li, L
   Su, L
   Lu, K
   Huang, QM
AF Tu, Yunbin
   Li, Liang
   Su, Li
   Lu, Ke
   Huang, Qingming
TI Neighborhood Contrastive Transformer for Change Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Change captioning; neighborhood contrastive transformer; syntax
   dependencies
ID IMAGE; ATTENTION; NETWORK
AB Change captioning is to describe the semantic change between a pair of similar images in natural language. It is more challenging than general image captioning, because it requires capturing fine-grained change information while being immune to irrelevant viewpoint changes, and solving syntax ambiguity in change descriptions. In this paper, we propose a neighborhood contrastive transformer to improve the model's perceiving ability for various changes under different scenes and cognition ability for complex syntax structure. Concretely, we first design a neighboring feature aggregating to integrate neighboring context into each feature, which helps quickly locate the inconspicuous changes under the guidance of conspicuous referents. Then, we devise a common feature distilling to compare two images at neighborhood level and extract common properties from each image, so as to learn effective contrastive information between them. Finally, we introduce the explicit dependencies between words to calibrate the transformer decoder, which helps better understand complex syntax structure during training. Extensive experimental results demonstrate that the proposed method achieves the state-of-the-art performance on three public datasets with different change scenarios.
C1 [Tu, Yunbin; Su, Li; Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Li, Liang] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Li, Liang] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Zhejiang, Peoples R China.
   [Lu, Ke] Univ Chinese Acad Sci, Sch Engn Sci, Beijing 101408, Peoples R China.
   [Lu, Ke] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Computing Technology,
   CAS; Hangzhou Dianzi University; Chinese Academy of Sciences; University
   of Chinese Academy of Sciences, CAS; Peng Cheng Laboratory
RP Su, L (corresponding author), Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.; Li, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM tuyunbin22@mails.ucas.ac.cn; liang.li@ict.ac.cn; suli@ucas.ac.cn;
   luk@ucas.ac.cn; qmhuang@ucas.ac.cn
OI Tu, Yunbin/0000-0002-9525-9060
FU National Key Ramp;D Program of China
FX No Statement Available
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bochkovskiy A, 2020, Arxiv, DOI arXiv:2004.10934
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Deng JC, 2022, IEEE T CIRC SYST VID, V32, P880, DOI 10.1109/TCSVT.2021.3063423
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dozat T., 2017, 5 INT C LEARN REPR I
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hosseinzadeh M, 2021, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR46437.2021.00275
   Hou JY, 2019, IEEE I CONF COMP VIS, P8917, DOI 10.1109/ICCV.2019.00901
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Jhamtani H, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P4024
   Ji JY, 2023, IEEE T MULTIMEDIA, V25, P3962, DOI 10.1109/TMM.2022.3169061
   Kim H., 2021, ICCV, P2095
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lei Ba J., 2016, arXiv
   Li L, 2022, IEEE T IMAGE PROCESS, V31, P2726, DOI 10.1109/TIP.2022.3158546
   Li ZW, 2020, PROC CVPR IEEE, P3437, DOI 10.1109/CVPR42600.2020.00350
   Liao ZM, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5074, DOI 10.1145/3474085.3475712
   Lin Chin-Yew, 2004, Text summarization branches out, P74, DOI DOI 10.2307/3105454
   Liu FL, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P269
   Liu S, 2023, IEEE T MULTIMEDIA, V25, P4494, DOI 10.1109/TMM.2022.3177308
   Liu XJ, 2023, IEEE T PATTERN ANAL, V45, P3003, DOI 10.1109/TPAMI.2022.3186410
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park DH, 2019, IEEE I CONF COMP VIS, P4623, DOI 10.1109/ICCV.2019.00472
   Paszke A, 2019, ADV NEUR IN, V32
   Qiu Y., 2021, P IEEE CVF INT C COM, P1971
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Tan H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1873
   Tu YB, 2023, PATTERN RECOGN, V136, DOI 10.1016/j.patcog.2022.109204
   Tu YB, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P63
   Tu YB, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9319
   Tu YB, 2022, IEEE T IMAGE PROCESS, V31, P3565, DOI 10.1109/TIP.2022.3159472
   Tu YB, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107702
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vo N, 2019, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2019.00660
   Wang BR, 2019, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2019.00273
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu M., 2022, P IEEECVF C COMPUTER, P18020
   Xiangxi Shi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P574, DOI 10.1007/978-3-030-58568-6_34
   Yao LL, 2022, AAAI CONF ARTIF INTE, P3108
   Yu LT, 2022, IEEE T MULTIMEDIA, V24, P1775, DOI 10.1109/TMM.2021.3072479
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang ZJ, 2022, IEEE T MULTIMEDIA, V24, P3101, DOI 10.1109/TMM.2021.3093725
   Zhao D., 2021, P IEEE CVF INT C COM, P3354, DOI DOI 10.48550/ARXIV.2108.00211
   Zhao Wentian, 2021, ADV NEUR IN, V34
   Zheng Q., 2020, P IEEE CVF C COMP VI, P13096
NR 50
TC 2
Z9 2
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9518
EP 9529
DI 10.1109/TMM.2023.3254162
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200036
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tu, ZZ
   Ma, Y
   Li, Z
   Li, CL
   Xu, JM
   Liu, YT
AF Tu, Zhengzheng
   Ma, Yan
   Li, Zhun
   Li, Chenglong
   Xu, Jieming
   Liu, Yongtao
TI RGBT Salient Object Detection: A Large-Scale Dataset and Benchmark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; Feature extraction; Imaging; Task analysis; Saliency
   detection; Cameras; Benchmark testing; Salient object detection;
   attention; VT5000 dataset
ID FUSION; TRACKING; NETWORK
AB Salient object detection (SOD) in complex scenes and environments is a challenging research topic. Most works focus on RGB-based SOD, which limits its performance of real-life applications when confronted with adverse conditions such as dark environments and complex backgrounds. Since thermal infrared spectrum provides the complementary information, RGBT SOD has become a new research direction. However, current research for RGBT SOD is limited by the lack of a large-scale dataset and comprehensive benchmark. This work contributes such a RGBT image dataset named VT5000, including 5000 spatially aligned RGBT image pairs with ground truth annotations. VT5000 has 11 challenges collected in different scenes and environments for exploring the robustness of algorithms. With this dataset, we propose a powerful baseline approach, which extracts multilevel features of each modality and aggregates these features of all modalities with the attention mechanism for accurate RGBT SOD. To further solve the problem of blur boundaries of salient objects, we also use an edge loss to refine the boundaries. Extensive experiments show that the proposed baseline approach outperforms the state-of-the-art methods on VT5000 dataset and other two public datasets. In addition, we carry out a comprehensive analysis of different algorithms of RGBT SOD on VT5000 dataset, and then make several valuable conclusions and provide some potential research directions for RGBT SOD.
C1 [Tu, Zhengzheng; Ma, Yan; Li, Zhun; Li, Chenglong; Xu, Jieming; Liu, Yongtao] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
   [Li, Chenglong] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University
RP Li, CL (corresponding author), Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.; Li, CL (corresponding author), Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230601, Peoples R China.
EM zhengzhengahu@163.com; m17856174397@163.com; 18355607109@163.com;
   lcl1314@foxmail.com; m17730263582@163.com; lyt494131144@163.com
RI Li, Chenglong/AAH-4234-2019; Liu, Yongtao/HNO-8621-2023
OI Tu, Zhengzheng/0000-0002-9689-8657
FU Natural Science Foundation of Anhui Higher Education Institution of
   China [KJ2020A0033]; Anhui Provincial Natural Science foundation
   [2108085MF211,2020]; Anhui Energy Internet Joint Fund Project
   [2008085UD07]; National Natural Science Foundation of China [61876002];
   Anhui Provincial Key Research and Development Program [202104d07020008];
   NSFC Key Project of International (Regional) Cooperation and Exchanges
   [61860206004]; Natural Science Foundation of Anhui Higher Education
   Institution of China [KJ2020A0033]; Anhui Provincial Natural Science
   foundation [2108085MF211,2020]; Anhui Energy Internet Joint Fund Project
   [2008085UD07]; National Natural Science Foundation of China [61876002];
   Anhui Provincial Key Research and Development Program [202104d07020008];
   NSFC Key Project of International (Regional) Cooperation and Exchanges
   [61860206004]
FX This work was supported in part by the Natural Science Foundation of
   Anhui Higher Education Institution of China under Grant KJ2020A0033, in
   part by Anhui Provincial Natural Science foundation under Grant
   2108085MF211,2020, in part by Anhui Energy Internet Joint Fund Project
   under Grant 2008085UD07, in part by the National Natural Science
   Foundation of China under Grant 61876002, in part by Anhui Provincial
   Key Research and Development Program under Grant 202104d07020008, and in
   part by the NSFC Key Project of International (Regional) Cooperation and
   Exchanges under Grant 61860206004
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Cheng Y, 2014, IEEE INT CON MULTI
   Chu X., 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, P1831, DOI DOI 10.1109/CVPR.2017.601
   Ciptadi A, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.112
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Jiang HZ, 2019, FRONT COMPUT SCI-CHI, V13, P778, DOI 10.1007/s11704-017-6613-8
   Jiang J., 2018, REDNET RESIDUAL ENCO
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kingma D. P., 2014, arXiv
   Li CL, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.106977
   Li CL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1856, DOI 10.1145/3123266.3123289
   Li CL, 2018, LECT NOTES COMPUT SC, V11217, P831, DOI 10.1007/978-3-030-01261-8_49
   Li CL, 2016, IEEE T IMAGE PROCESS, V25, P5743, DOI 10.1109/TIP.2016.2614135
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu HP, 2012, SCI CHINA INFORM SCI, V55, P590, DOI 10.1007/s11432-011-4536-9
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Simonyan K, 2015, IEEE INT C ICLR
   Tang J, 2020, IEEE T CIRC SYST VID, V30, P4421, DOI 10.1109/TCSVT.2019.2951621
   Tatler BW, 2005, VISION RES, V45, P643, DOI 10.1016/j.visres.2004.09.017
   Trudinger, 2001, CLASSICS MATH
   Tu ZZ, 2020, IEEE T MULTIMEDIA, V22, P160, DOI 10.1109/TMM.2019.2924578
   Tu ZZ, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P141, DOI 10.1109/MIPR.2019.00032
   Wang G., 2018, CHINESE C IMAGE GRAP, P359
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Z, 2019, PROC CVPR IEEE, P3902, DOI 10.1109/CVPR.2019.00403
   Xia CQ, 2017, PROC CVPR IEEE, P4399, DOI 10.1109/CVPR.2017.468
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang S, 2018, IEEE T CIRC SYST VID, V28, P2574, DOI 10.1109/TCSVT.2017.2721460
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhu CB, 2017, IEEE INT CONF COMP V, P3008, DOI 10.1109/ICCVW.2017.355
   Zhu S., 2019, OGNET SALIENT OBJECT
NR 63
TC 38
Z9 38
U1 15
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4163
EP 4176
DI 10.1109/TMM.2022.3171688
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200007
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Wang, SK
   Gan, T
   Liu, Y
   Wu, JL
   Cheng, Y
   Nie, LQ
AF Wang, Shaokun
   Gan, Tian
   Liu, Yuan
   Wu, Jianlong
   Cheng, Yuan
   Nie, Liqiang
TI Micro-Influencer Recommendation by Multi-Perspective Account
   Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Influencer marketing; multi-modal; social media information
ID USER; BEHAVIOR
AB Influencer marketing is emerging as a new marketing method, changing the marketing strategies of brands profoundly. In order to help brands find suitable micro-influencers as marketing partners, the micro-influencer recommendation is regarded as an indispensable part of influencer marketing. However, previous works only focus on modeling the individual image of brands/micro-influencers, which is insufficient to represent the characteristics of brands/micro-influencers over the marketing scenarios. In this case, we propose a micro-influencer ranking joint learning framework which models brands/micro-influencers from the perspective of individual image, target audiences, and cooperation preferences. Specifically, to model accounts' individual image, we extract topics information and images semantic information from historical content information, and fuse them to learn the account content representation. We introduce target audiences as a new kind of marketing role in the micro-influencer recommendation, in which audiences information of brand/micro-influencer is leveraged to learn the multi-modal account audiences representation. Afterward, we build the attribute co-occurrence graph network to mine cooperation preferences from social media interaction information. Based on account attributes, the cooperation preferences between brands and micro-influencers are refined to attributes' co-occurrence information. The attribute node embeddings learned in the attribute co-occurrence graph network are further utilized to construct the account attribute representation. Finally, the global ranking function is designed to generate ranking scores for all brand-micro-influencer pairs from the three perspectives jointly. The extensive experiments on a publicly available dataset demonstrate the effectiveness of our proposed model over the state-of-the-art methods.
C1 [Wang, Shaokun; Gan, Tian; Liu, Yuan; Wu, Jianlong; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Cheng, Yuan] Ant Financial Serv Grp, Hangzhou 310013, Peoples R China.
C3 Shandong University
RP Gan, T; Wu, JL (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM shaokunwang.sdu@gmail.com; gantian@sdu.edu.cn; yuanliu@mail.sdu.edu.cn;
   jlwu1992@sdu.edu.cn; chengyuan.c@antgroup.com; nieliqiang@gmail.com
RI wang, shaokun/KIE-8291-2024
OI Wang, Shaokun/0000-0001-8945-1200; Liu, Yu-An/0000-0002-9125-5097
FU National Natural Science Foundation of China [62176137, 62006140];
   Shandong Provincial Natural Science and Foundation [ZR2019JQ23,
   ZR2020QF106]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62176137 and 62006140, and in part by
   the Shandong Provincial Natural Science and Foundation under Grants
   ZR2019JQ23 and ZR2020QF106.
CR Ang L., 2014, Principles of integrated marketing communications
   Anger I., 2011, P 11 INT C KNOWL TEC, P1, DOI DOI 10.1145/2024288.2024326
   Ashley C, 2015, PSYCHOL MARKET, V32, P15, DOI 10.1002/mar.20761
   Berger J., 2017, RES SHOWS MICROINFLU
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bonomo M, 2019, Arxiv, DOI arXiv:1907.01326
   Cha M., 2010, P INT AAAI C WEB SOC, V4, P10, DOI DOI 10.1145/2897659.2897663
   Cossu JV, 2015, SECOND EUROPEAN NETWORK INTELLIGENCE CONFERENCE (ENIC 2015), P83, DOI 10.1109/ENIC.2015.20
   Dolan R, 2016, J STRATEG MARK, V24, P261, DOI 10.1080/0965254X.2015.1095222
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P1031, DOI 10.1109/TMM.2015.2430819
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Gan T, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1933, DOI 10.1145/3343031.3351080
   Gelli F, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3385413
   Gelli F, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P465, DOI 10.1145/3240508.3240689
   Georgiou T, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1432, DOI 10.1145/2998181.2998259
   Goh KY, 2013, INFORM SYST RES, V24, P88, DOI 10.1287/isre.1120.0469
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Hajian B., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P497, DOI 10.1109/PASSAT/SocialCom.2011.118
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hollebeek LD, 2019, J ACAD MARKET SCI, V47, P161, DOI 10.1007/s11747-016-0494-5
   Kapoor PS, 2022, J TRAVEL RES, V61, P1138, DOI 10.1177/00472875211019469
   Kay S, 2020, J MARKET MANAG-UK, V36, P248, DOI 10.1080/0267257X.2020.1718740
   Kim AJ, 2016, COMPUT HUM BEHAV, V58, P98, DOI 10.1016/j.chb.2015.12.047
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Li YM, 2011, INFORM SCIENCES, V181, P5143, DOI 10.1016/j.ins.2011.07.023
   Lou C., 2019, J INTERACTIVE ADVERT, V19, P58, DOI DOI 10.1080/15252019.2018.1533501
   Ma WZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3484
   Maecker O., 2016, BUS RES, V9, P133, DOI [10.1007/s40685-016-0027-6, DOI 10.1007/S40685-016-0027-6]
   Marques IR, 2021, STRATEGIC CORPORATE COMMUNICATION IN THE DIGITAL AGE, P131, DOI 10.1108/978-1-80071-264-520211008
   Mazloom M., 2016, P ACM MULT, P197, DOI [10.1145/2964284.2967210, 10]
   Morisio M., 2019, THESIS DEPT COMPUT E
   Morone F, 2016, SCI REP-UK, V6, DOI 10.1038/srep30062
   Provost F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P707
   Rafailidis D, 2017, EXPERT SYST APPL, V74, P11, DOI 10.1016/j.eswa.2017.01.005
   Rao A, 2015, PROCEEDINGS 2015 IEEE INTERNATIONAL CONFERENCE ON BIG DATA, P2282, DOI 10.1109/BigData.2015.7364017
   Segev N, 2018, ACM/SIGIR PROCEEDINGS 2018, P1009, DOI 10.1145/3209978.3210134
   Shen JL, 2021, INFORM SCIENCES, V569, P469, DOI 10.1016/j.ins.2020.11.026
   Si S., 2015, BUS EC J, DOI [10.4172/2151-6219.1000203, DOI 10.4172/2151-6219.1000203, 10.4172/2151-6219.100020]
   Stevens K., 2012, P 2012 JOINT C EMPIR
   Sweet T, 2019, Arxiv, DOI arXiv:1901.05949
   Tan SL, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037679
   Tuten T.L., 2017, SOCIAL MEDIA MARKETI
   Wang SK, 2022, IEEE T MULTIMEDIA, V24, P2595, DOI 10.1109/TMM.2021.3087038
   Wang Z., 2015, P 24 ACM INT C INFOR, P653
   Woods Steven., 2016, #Sponsored: The Emergence of Influencer Marketing
   Wu MF, 2016, 2016 3RD INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE), P379, DOI 10.1109/ICISCE.2016.90
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xiao WY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P235, DOI 10.1145/3292500.3330965
   Yesiloglu S., 2020, Influencer marketing: Building brand communities and engagement
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhang Y, 2019, P ACM INT C MULTIMED, P1
   Zhang YW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3106, DOI 10.1145/3474085.3475453
   Zietek N., 2016, Influencer Marketing: the characteristics and components of fashion influencer marketing
NR 54
TC 4
Z9 4
U1 11
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2749
EP 2760
DI 10.1109/TMM.2022.3151029
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600023
DA 2024-07-18
ER

PT J
AU Wang, Z
   Zhang, L
   Shen, Y
   Zhou, YC
AF Wang, Zhong
   Zhang, Lin
   Shen, Ying
   Zhou, Yicong
TI D-LIOM: Tightly-Coupled Direct LiDAR-Inertial Odometry and Mapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Laser radar; Simultaneous localization and mapping;
   Point cloud compression; Sensors; Three-dimensional displays; Gravity;
   LiDAR-Inertial odometry; SLAM; loop detection; data fusion
ID REAL-TIME; ROBUST
AB Simultaneous localization and mapping via LiDARInertial fusion is a crucial technology in many automation-related applications. Recently, a number of approaches based on geometric features have evolved, yielding impressive results via tightly-coupled estimation. This sort of feature-based techniques, however, are inextricably linked to the scanning mechanism of the LiDAR, relying on stable feature detection, and thus are difficult to adapt to multi-LiDAR systems. A few "direct" solutions, on the other hand, register the raw point cloud with the built probability map, which is more computationally efficient and easy to be extended. But, the existing direct approaches are all loosely-coupled, lacking correction of the IMU biases, and thus only work well in 2D cases. To this end, we present D-LIOM, a tightly-coupled Direct LiDAR-Inertial Odometry and Mapping framework. In D-LIOM, a scan is directly registered to a probability submap, and the LiDAR odometry, the IMU pre-integration, and the gravity constraint are integrated to build a local factor graph in the submap's time window, allowing the system to perform real-time high-precision pose estimation. Furthermore, to eliminate accumulated errors in time, we detect loops and adjust the sparse pose graph based on mutual matching of projected 2D submaps, allowing D-LIOM to run stably in large-scale scenes. In addition, to improve its flexibility to varied sensor combinations, D-LIOM supports multi-LiDAR inputs and facilitates the initialization with a common 6-axis IMU. Extensive experiments demonstrate that D-LIOM largely outperforms the existing state-of-the-art counterparts in mapping effect and localization accuracy as well as with high time efficiency. Lastly, to ensure that our results are entirely reproducible, all necessary data and codes are made open-source available. One introduction video can also be found on the online website.
C1 [Wang, Zhong; Zhang, Lin; Shen, Ying] Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
   [Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 Tongji University; University of Macau
RP Zhang, L (corresponding author), Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
EM 2010194@tongji.edu.cn; cslinzhang@tongji.edu.cn; yingshen@tongji.edu.cn;
   yicongzhou@um.edu.mo
RI ; Zhou, Yicong/A-8017-2009
OI Wang, Zhong/0000-0002-6206-526X; Zhang, Lin/0000-0002-4360-5523; Shen,
   Ying/0000-0002-2966-7955; Zhou, Yicong/0000-0002-4487-6384
FU National Key Research and Development Project [2020YFB2103900]; National
   Natural Science Foundation of China [61973235, 61972285]; Shanghai
   Science and Technology Innovation Plan [20510760400]; Dawn Program of
   Shanghai Municipal Education Commission [21SG23]; Shanghai Municipal
   Science and Technology Major Project [2021SHZDZX0100]; Fundamental
   Research Funds for the Central Universities
FX This work was supported in part by the National Key Research and
   Development Project underGrant 2020YFB2103900, in part by the National
   Natural Science Foundation of China under Grants 61973235 and 61972285,
   in part by the Shanghai Science and Technology Innovation Plan under
   Grant 20510760400, in part by the Dawn Program of Shanghai Municipal
   Education Commission under Grant 21SG23, in part by the Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0100, and in part by the Fundamental Research Funds for the
   Central Universities.
CR ANTONI B, 2009, AUTON ROBOT, V26, P203
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biber P, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P2743, DOI 10.1109/iros.2003.1249285
   Cadena C, 2016, IEEE T ROBOT, V32, P1309, DOI 10.1109/TRO.2016.2624754
   Chen XYL, 2022, AUTON ROBOT, V46, P61, DOI 10.1007/s10514-021-09999-0
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forster C, 2017, IEEE T ROBOT, V33, P1, DOI 10.1109/TRO.2016.2597321
   HAOYANGYE ML, 2019, P IEEE INT C ROBOT A, P3144
   Hess W, 2016, IEEE INT CONF ROBOT, P1271, DOI 10.1109/ICRA.2016.7487258
   Hornung A, 2013, AUTON ROBOT, V34, P189, DOI 10.1007/s10514-012-9321-0
   Jeong J, 2019, INT J ROBOT RES, V38, P642, DOI 10.1177/0278364919843996
   Kaess Michael, 2011, 2011 IEEE International Conference on Robotics and Automation, P3281
   Kaess M, 2008, IEEE T ROBOT, V24, P1365, DOI 10.1109/TRO.2008.2006706
   Kim G, 2018, IEEE INT C INT ROBOT, P4802, DOI 10.1109/IROS.2018.8593953
   Kohlbrecher S., 2011, 2011 Proceedings of IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR 2011), P155, DOI 10.1109/SSRR.2011.6106777
   KRIVOKUCA M, IEEE T MULTIMEDIA
   Levenberg K., 1944, Quarterly of Applied Mathematics, V2, P164, DOI [10.1090/QAM/10666, DOI 10.1090/QAM/10666]
   LI L, IEEE T MULTIMEDIA
   Lin JR, 2021, IEEE ROBOT AUTOM LET, V6, P7469, DOI 10.1109/LRA.2021.3095515
   Lin JR, 2020, IEEE INT CONF ROBOT, P3126, DOI 10.1109/ICRA40945.2020.9197440
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   LV J, 2021, P IEEE RSJ INT C INT, P9968
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   Mourikis AI, 2007, IEEE INT CONF ROBOT, P3565, DOI 10.1109/ROBOT.2007.364024
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Mur-Artal R, 2017, IEEE ROBOT AUTOM LET, V2, P796, DOI 10.1109/LRA.2017.2653359
   Nam DV, 2021, INT CONF BIG DATA, P302, DOI 10.1109/BigComp51126.2021.00064
   Nguyen TM, 2022, INT J ROBOT RES, V41, P270, DOI 10.1177/02783649211052312
   Olson E. B., 2009, IEEE INT C ROB AUT I, P4387
   Qin C, 2020, IEEE INT CONF ROBOT, P8899, DOI [10.1109/ICRA40945.2020.9197567, 10.1109/icra40945.2020.9197567]
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Shan TX, 2020, IEEE INT C INT ROBOT, P5135, DOI 10.1109/IROS45743.2020.9341176
   Shan TX, 2018, IEEE INT C INT ROBOT, P4758, DOI 10.1109/IROS.2018.8594299
   Sola J., 2017, Quaternion kinematics for the error-state kalman filter
   Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754
   UMEYAMA S, 1991, IEEE T PATTERN ANAL, V13, P376, DOI 10.1109/34.88573
   Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463
   Wang Y, 2020, IEEE INT C INT ROBOT, P5769, DOI 10.1109/IROS45743.2020.9341010
   WOODMAN OJ, 2007, AN INTRODUCTION TO I
   Xu W, 2021, IEEE ROBOT AUTOM LET, V6, P3317, DOI 10.1109/LRA.2021.3064227
   Zhan J, 2014, DES AUT CON, DOI 10.1145/2593069.2593165
   Zuo XX, 2020, IEEE INT C INT ROBOT, P5112, DOI 10.1109/IROS45743.2020.9340704
   Zuo XX, 2019, IEEE INT C INT ROBOT, P5848, DOI [10.1109/iros40897.2019.8967746, 10.1109/IROS40897.2019.8967746]
NR 46
TC 8
Z9 8
U1 8
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3905
EP 3920
DI 10.1109/TMM.2022.3168423
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500028
DA 2024-07-18
ER

PT J
AU Zeng, JD
   Zhou, JT
   Liu, TY
AF Zeng, Jiandian
   Zhou, Jiantao
   Liu, Tianyi
TI Robust Multimodal Sentiment Analysis via Tag Encoding of Uncertain
   Missing Modalities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sentiment analysis; Feature extraction; Transformers; Visualization;
   Acoustics; Encoding; Training; Multimodal sentiment analysis; missing
   modality; joint representation
AB Multimodal sentiment analysis aims to extract emotions with multiple data sources, usually under the assumption that all modalities are available. In practice, such a strong assumption does not always hold, and most of multimodal sentiment analysis methods may fail when partial modalities are missing. Some existing works have started to address the missing modality problem; but only considered the singlemodalitymissing case, while ignoring the practically more general cases of multiple modalities missing. To this end, in this paper, we propose a Tag-Assisted Transformer Encoder (TATE) network to handle the problem of missing uncertain modalities. Specifically, we design a tag encoding module to cover both the single modality and multiple modalities missing cases, so as to guide the network's attention to those missing modalities. Besides, a new space projection pattern is adopted to align common vectors, taking into account the different importance of each modality. Afterwards, a Transformer encoderdecoder network is utilized to learn the missing modality features, and the outputs of the Transformer encoder are extracted for the final sentiment classification. Extensive experiments and analyses are conducted on CMU-MOSI, IEMOCAP, and MELD datasets, which show that the proposed method can achieve significant improvements compared with several baselines.
C1 [Zeng, Jiandian; Zhou, Jiantao] Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
   [Zeng, Jiandian; Zhou, Jiantao] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Liu, Tianyi] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
C3 University of Macau; University of Macau; Shanghai Jiao Tong University
RP Zhou, JT (corresponding author), Univ Macau, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.; Zhou, JT (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM yb87470@um.edu.mo; jtzhou@umac.mo; liutianyi@sjtu.edu.cn
FU Macau Science and Technology Development Fund [SKLIOTSC-2021-2023,
   0072/2020/AMJ]; Research Committee at University of Macau
   [MYRG2018-00029-FST, MYRG2019-00023-FST, MYRG2020-00101-FST,
   MYRG2022-00152-FST]; Natural Science Foundation of China [61971476];
   Alibaba Group through Alibaba Innovative Research Program
FX This work was supported in part by Macau Science and Technology
   Development Fund under Grants SKLIOTSC-2021-2023 and 0072/2020/AMJ, in
   part by Research Committee at University of Macau under Grants
   MYRG2018-00029-FST, MYRG2019-00023-FST, MYRG2020-00101-FST, and
   MYRG2022-00152-FST, in part by the Natural Science Foundation of China
   under Grant 61971476, and in part by Alibaba Group through Alibaba
   Innovative Research Program.
CR Akbari H., 2021, Proc. Neural Inf. Process. Syst., V34, P1
   Baldi P., 2012, P ICML WORKSH UNS TR, P37
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cai L, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1158, DOI 10.1145/3219819.3219963
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Du CD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P108, DOI 10.1145/3240508.3240528
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo D, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152121
   Guo WY, 2021, IEEE T MULTIMEDIA, V23, P1785, DOI 10.1109/TMM.2020.3003648
   Guo XB, 2023, IEEE T MULTIMEDIA, V25, P2085, DOI 10.1109/TMM.2022.3142448
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kim Eun-Sol, 2020, P IEEE CVF C COMP VI, P14581
   Kingma D, 2014, ICLR P, V2014, P1
   Kingma D. P., 2014, arXiv
   Tran L, 2017, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2017.528
   Majumder N, 2018, KNOWL-BASED SYST, V161, P124, DOI 10.1016/j.knosys.2018.07.041
   Mazumder R, 2010, J MACH LEARN RES, V11, P2287
   McFee B., 2015, P PYTH SCI C AUST TX, P18, DOI 10.25080/Majora-7b98e3ed-003
   Mikolov T, 2013, P WORKSHOP ICLR 2013
   Nie WZ, 2022, IEEE T MULTIMEDIA, V24, P4471, DOI 10.1109/TMM.2021.3118881
   Parthasarathy S, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P400, DOI 10.1145/3395035.3425202
   Peng W, 2021, IEEE INTELL SYST, V36, P82, DOI 10.1109/MIS.2021.3057757
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2018, IEEE INTELL SYST, V33, P17, DOI 10.1109/MIS.2018.2882362
   Ruan SL, 2024, IEEE T MULTIMEDIA, V26, P4097, DOI 10.1109/TMM.2021.3118208
   Shang C, 2017, IEEE INT CONF BIG DA, P766, DOI 10.1109/BigData.2017.8257992
   Shao WX, 2013, IEEE DATA MINING, P1181, DOI 10.1109/ICDM.2013.117
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang ZL, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2514, DOI 10.1145/3366423.3380000
   Wu HW, 2022, PROC CVPR IEEE, P13430, DOI 10.1109/CVPR52688.2022.01308
   Xu J, 2021, IEEE T IND INFORM, V17, P2974, DOI 10.1109/TII.2020.3005405
   Xu N, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3777
   Xu N, 2018, ACM/SIGIR PROCEEDINGS 2018, P929, DOI 10.1145/3209978.3210093
   Yang KY, 2022, ACM T KNOWL DISCOV D, V16, DOI 10.1145/3457216
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yu WM, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P3718
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng J., 2022, PROC INT ACM SIGIR C, P1
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
   Zhu T, 2023, IEEE T MULTIMEDIA, V25, P3375, DOI 10.1109/TMM.2022.3160060
NR 49
TC 6
Z9 6
U1 19
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6301
EP 6314
DI 10.1109/TMM.2022.3207572
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500047
DA 2024-07-18
ER

PT J
AU Zhang, HR
   Meng, YD
   Zhao, YT
   Qian, XS
   Qiao, YH
   Yang, XY
   Zheng, YL
AF Zhang, Hongrun
   Meng, Yanda
   Zhao, Yitian
   Qian, Xuesheng
   Qiao, Yihong
   Yang, Xiaoyun
   Zheng, Yalin
TI 3D Human Pose and Shape Reconstruction From Videos via Confidence-Aware
   Temporal Feature Aggregation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Shape; Training;
   Correlation; Solid modeling; Videos; Human pose; temporal estimation;
   uncertainty
AB Estimating 3D human body shapes and poses from videos is a challenging computer vision task. The intrinsic temporal information embedded in adjacent frames is helpful in making accurate estimations. Existing approaches learn temporal features of the target frames simply by aggregating features of their adjacent frames, using off-the-shelf deep neural networks. Consequently these approaches cannot explicitly and effectively use the correlations between adjacent frames to help infer the parameters of the target frames. In this paper, we propose a novel framework that can measure the correlations amongst adjacent frames in the form of an estimated confidence metric. The confidence value will indicate to what extent the adjacent frames can help predict the target frames' 3D shapes and poses. Based on the estimated confidence values, temporally aggregated features are then obtained by adaptively allocating different weights to the temporal predicted features from the adjacent frames. The final 3D shapes and poses are estimated by regressing from the temporally aggregated features. Experimental results on three benchmark datasets show that the proposed method outperforms state-ofthe-art approaches (even without the motion priors involved in training). In particular, the proposed method is more robust against corrupted frames.
C1 [Zhang, Hongrun; Meng, Yanda; Zheng, Yalin] Univ Liverpool, Inst Life Course & Med Sci, Liverpool L7 8TX, Merseyside, England.
   [Zhao, Yitian] Chinese Acad Sci, Ningbo Inst Mat Technol & Engn, Cixi Inst Biomed Engn, Ningbo 315201, Peoples R China.
   [Qian, Xuesheng; Qiao, Yihong] China IntelliCloud Co, Shanghai, Peoples R China.
   [Yang, Xiaoyun] Remark AI UK Ltd, London SE1 9PD, England.
C3 University of Liverpool; Chinese Academy of Sciences; Ningbo Institute
   of Materials Technology and Engineering, CAS
RP Zheng, YL (corresponding author), Univ Liverpool, Inst Life Course & Med Sci, Liverpool L7 8TX, Merseyside, England.
EM hongrun.zhang@liverpool.ac.uk; yanda.meng@liverpool.ac.uk;
   yitian.zhao@nimte.ac.cn; xuesheng.qian@intellicloud.ai;
   yihong.qiao@intellicloud.ai; xyang@remarkholdings.com;
   yalin.zheng@liverpool.ac.uk
RI Qi, Ling/KHE-3068-2024; Meng, Yanda/IUQ-7187-2023; Zhao,
   Yitian/AAM-4907-2021; Zheng, Yalin/N-6432-2017
OI Meng, Yanda/0000-0001-7344-2174; Zhao, Yitian/0000-0002-0962-8377;
   Zheng, Yalin/0000-0002-7873-0922; Qian, xuesheng/0000-0001-6225-7889
FU China IntelliCloud Co., Shanghai, China
FX This work was supported by the China IntelliCloud Co., Shanghai, China.
CR Agarwal A, 2006, IEEE T PATTERN ANAL, V28, P44, DOI 10.1109/TPAMI.2006.21
   Akhter I, 2015, PROC CVPR IEEE, P1446, DOI 10.1109/CVPR.2015.7298751
   Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bütepage J, 2017, PROC CVPR IEEE, P1591, DOI 10.1109/CVPR.2017.173
   Chao YW, 2017, PROC CVPR IEEE, P3643, DOI 10.1109/CVPR.2017.388
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Coskun H, 2017, IEEE I CONF COMP VIS, P5525, DOI 10.1109/ICCV.2017.589
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Doersch, 2019, ARXIV190702499, P12949
   Everingham M., 2010, BMVC, V2, P5
   Fragkiadaki K, 2015, IEEE I CONF COMP VIS, P4346, DOI 10.1109/ICCV.2015.494
   GAL Y, 2016, UNCERTAINTY IN DEEP, V1
   Grauman K, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P641
   Guan P, 2009, IEEE I CONF COMP VIS, P1381, DOI 10.1109/iccv.2009.5459300
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Gyeongsik Moon, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12352), P752, DOI 10.1007/978-3-030-58571-6_44
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2019, PROC CVPR IEEE, P2883, DOI 10.1109/CVPR.2019.00300
   Hsieh JW, 2008, IEEE T MULTIMEDIA, V10, P372, DOI 10.1109/TMM.2008.917403
   Huang YH, 2017, INT CONF 3D VISION, P421, DOI 10.1109/3DV.2017.00055
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kanazawa A, 2019, PROC CVPR IEEE, P5597, DOI 10.1109/CVPR.2019.00576
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Lassner C., 2017, PROC CVPR IEEE, V2, P3, DOI DOI 10.1109/CVPR.2017.500
   LEE HJ, 1985, COMPUT VISION GRAPH, V30, P148, DOI 10.1016/0734-189X(85)90094-5
   Lin K, 2021, PROC CVPR IEEE, P1954, DOI 10.1109/CVPR46437.2021.00199
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P3657, DOI 10.1109/TMM.2020.3029941
   Martinez Julieta, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Meng PF, 2018, INT CONF LIGHTN PROT
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Omran M, 2018, INT CONF 3D VISION, P484, DOI 10.1109/3DV.2018.00062
   PARAMESWARAN V, 2004, P IEEE COMPUT SOC C
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Peng XB, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275014
   Ramakrishna V., 2012, LECT NOTES COMPUT SC, P573, DOI DOI 10.1007/978-3-642-33765-9
   Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31
   Sigal L., 2008, ADV NEURAL INFORM PR, P1337
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tan Jun Kai Vince, 2017, Indirect deep structured learning for 3D human body shape and pose prediction, V3
   Tung HYF, 2017, ADV NEUR IN, V30
   Varol G, 2018, LECT NOTES COMPUT SC, V11211, P20, DOI 10.1007/978-3-030-01234-2_2
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wandt B, 2016, IEEE T PATTERN ANAL, V38, P1505, DOI 10.1109/TPAMI.2016.2553028
   Xiangyu Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P284, DOI 10.1007/978-3-030-58545-7_17
   Zhang JY, 2019, IEEE I CONF COMP VIS, P7113, DOI 10.1109/ICCV.2019.00721
   Zhe Wang, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12536), P523, DOI 10.1007/978-3-030-66096-3_36
   Zhou SZ, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778863
   2019, P IEEE C COMPUT VIS, P10975
NR 60
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3868
EP 3880
DI 10.1109/TMM.2022.3167887
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500025
DA 2024-07-18
ER

PT J
AU Chen, TY
   Wu, S
   Yang, XH
   Xu, Y
   Wong, HS
AF Chen, Tianyi
   Wu, Si
   Yang, Xuhui
   Xu, Yong
   Wong, Hau-San
TI Semantic Regularized Class-Conditional GANs for Semi-Supervised
   Fine-Grained Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Semantics; Image synthesis; Training; Task analysis;
   Generative adversarial networks; Data models; Semi-supervised learning;
   fine-grained image synthesis; generative adversarial networks; semantic
   regularization
AB Learning effective generative models for natural image synthesis is a promising way to reduce the dependence of deep models on massive training data. This work focuses on Fine-Grained Image Synthesis (FGIS) in the semi-supervised setting where a small number of training instances are labeled. Different from generic image synthesis tasks, the available fine-grained data may be inadequate, and the differences among the object categories are typically subtle. To address these issues, we propose a Semantic Regularized class-conditional Generative Adversarial Network, which is referred to as SReGAN. We incorporate an additional discriminator and classifier into the generator-discriminator minimax game. Competing with two discriminators enforces the generator to model both marginal and class-conditional data distributions, which alleviates the problem of limited training data and labels. However, the discriminators may overlook the class separability. To induce the generator to discover the distinctions between classes, we construct semantically congruent and incongruent pairs in the generation process, and further regularize the generator by encouraging high similarities of congruent pairs, while penalizing that of incongruent ones in the classifier's feature space. We have conducted extensive experiments to verify the capability of SReGAN in generating high-fidelity images on a variety of FGIS benchmarks.
C1 [Chen, Tianyi; Wu, Si; Xu, Yong] South China Univ Technol, Sch Comp Sci Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Yang, Xuhui; Xu, Yong] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
   [Xu, Yong] Commun & Comp Network Lab Guangdong, Guangzhou 510006, Guangdong, Peoples R China.
   [Wong, Hau-San] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 South China University of Technology; Peng Cheng Laboratory; City
   University of Hong Kong
RP Wu, S (corresponding author), South China Univ Technol, Sch Comp Sci Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM csttychen@mail.scut.edu.cn; ez.wusi@gmail.com; yangxh@pcl.ac.cn;
   yxu@scut.edu.cn; cshswong@cityu.edu.hk
OI Yang, Xuhui/0000-0003-1151-5045; WONG, Hau-San/0000-0002-1530-7529
FU National Natural Science Foundation of China [62072188, 62072189];
   Research Grants Council of the Hong Kong Special Administration Region
   [CityU 11201220]; Natural Science Foundation of Guangdong Province
   [2019A050510010, 2020A1515010484]; China Postdoctoral Science Foundation
   [2021M691682]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072188 and 62072189, in part by the
   Research Grants Council of the Hong Kong Special Administration Region
   under Grant CityU 11201220, in part by the Natural Science Foundation of
   Guangdong Province under Grants 2019A050510010 and 2020A1515010484, and
   in part by the China Postdoctoral Science Foundation under Grant
   2021M691682.
CR Abid A., ARXIV190204601
   Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   [Anonymous], 2018, P INT C LEARN REPR I
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Benny Y., 2020, PROC EURO C COMPUT V, P59
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Dai Z. H., 2017, NIPS
   Dauphin YN, 2017, PR MACH LEARN RES, V70
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng Z., 2017, PROC NEURAL INF PROC, P3901
   Dong J., 2019, Advances in neural information processing systems, P10440
   Gan Z., 2017, PROC NEURAL INF PROC, P5248
   Gong MM, 2019, ADV NEUR IN, V32
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hensel M, 2017, ADV NEUR IN, V30
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Khosla A., 2011, PROC 1 WORKSHOP FINE
   Kingma D. P., 2014, arXiv
   Kingma DP, 2014, ADV NEUR IN, V27
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lamb A., 2019, ARXIV160203220
   Li CX, 2017, ADV NEUR IN, V30
   LI Z, 2020, IN PRESS, DOI DOI 10.1109/TMM.2020.3015015
   Liu Y, 2020, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR42600.2020.00576
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Ng HW, 2014, IEEE IMAGE PROC, P343, DOI 10.1109/ICIP.2014.7025068
   Pagnoni Artidoro, 2018, ARXIV181204405
   Radford A., 2015, ARXIV
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701
   Salimans T, 2016, ADV NEUR IN, V29
   Singh KK, 2019, PROC CVPR IEEE, P6483, DOI 10.1109/CVPR.2019.00665
   Sohn K., 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628
   Springenberg J., 2016, PROC INT C LEARN REP
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, PROC TECH REPORTCNS
   Wang TM, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5233
   Wei X., 2018, PROC INT C LEARN REP
   Wu S, 2019, PROC CVPR IEEE, P10083, DOI [10.1109/CVPR.2019.00666, 10.1109/CVPR.2019.01033]
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
NR 52
TC 6
Z9 6
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2975
EP 2985
DI 10.1109/TMM.2021.3091859
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000023
DA 2024-07-18
ER

PT J
AU Huang, JJ
   Yan, W
   Li, T
   Liu, S
   Li, G
AF Huang, Jingjia
   Yan, Wei
   Li, Thomas
   Liu, Shan
   Li, Ge
TI Learning the Global Descriptor for 3-D Object Recognition Based on
   Multiple Views Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Object recognition; Task
   analysis; Two dimensional displays; Shape; Virtual machine monitors;
   Latent views; mixture model; multi-view 3 d object; object recognition
ID MODEL RETRIEVAL; 3D; NETWORKS; FEATURES
AB The key point of view based strategies for the analysis of 3D object is to obtain a global descriptor from a collection of its rendered views on 2D images. The views are always redundantly sampled as to ensure the completeness of the information. In this paper, we bring new insight into the study of multi-view object recognition, which models an object as a View Mixture Model (VMM). We argue that each object represented by the multiple views can be decomposed into just a few latent views. Based on the VMM, we introduce a decomposition module to mine the representations of these latent views for the construction of a compact and comprehensive descriptor. After that, we further propose a view alignment module to ensure the descriptor is robust to the variation of view permutation. We evaluate our method on the ModelNet-40, ModelNet-10 and ShapeNetCore55 datasets. The experimental results show that our method can learn efficient and comprehensive representation for 3D objects, and achieves state-of-the-art performance on both the 3D object classification and retrieval tasks. Lastly, experiments are conducted for benchmarking various popular CNN backbones on the 3D object recognition task, with a view to achieving fair comparisons and promoting the future research in this area. Codes for our paper are released: "https://github.com/hjjpku/multi_view_sort ".
C1 [Huang, Jingjia; Yan, Wei; Li, Ge] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Huang, Jingjia; Yan, Wei] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Li, Thomas] Peking Univ, AIIT, Hangzhou 100871, Peoples R China.
   [Liu, Shan] Tencent Media Lab, Palo Alto, CA 94301 USA.
C3 Peking University; Peng Cheng Laboratory; Peking University
RP Li, G (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM jjhuang@pku.edu.cn; yanwe@pku.edu.cn; tli@aiit.org.cn;
   shanl@tencent.com; geli@ece.pku.edu.cn
RI huang, shan/JVN-1240-2024
OI , Shan/0000-0002-1442-1207
FU National Key R&D Program of China [2020AAA0103500]; Shenzhen Municipal
   Science and Technology Program
FX This work is supported by National Key R&D Program of China
   (No.2020AAA0103500) and Shenzhen Municipal Science and Technology
   Program (No.JCYJ20170818141146428). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Marta Mrak
CR [Anonymous], 2014, ABS14053531 CORR
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cho K., 2014, ARXIV14061078
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Greff K., 2017, Advances in Neural Information Processing Systems (NeurIPS), P6691
   Han Z., 2019, P 28 INT JOINT C ART, P758
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Han ZZ, 2019, IEEE T IMAGE PROCESS, V28, P658, DOI 10.1109/TIP.2018.2868426
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FC, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073654, 10.1145/3137609]
   Huang JJ, 2019, IEEE I CONF COMP VIS, P6489, DOI 10.1109/ICCV.2019.00658
   Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414
   Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kearns M, 1998, NATO ADV SCI I D-BEH, V89, P495
   Kingma D. P., 2014, arXiv
   Klokov R, 2017, IEEE I CONF COMP VIS, P863, DOI 10.1109/ICCV.2017.99
   Knopp J, 2010, LECT NOTES COMPUT SC, V6316, P589, DOI 10.1007/978-3-642-15567-3_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2013, MULTIMED TOOLS APPL, V62, P821, DOI 10.1007/s11042-011-0873-3
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Lindsay B., 1995, NSF CBMS REGIONAL C
   Liu XH, 2020, COMPUT AIDED GEOM D, V79, DOI 10.1016/j.cagd.2020.101859
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Savva M., 2017, EUR WORKSH 3D OBJ RE, P39
   Savva M., 2016, Proceedings of the eurographics workshop on 3D object retrieval, P89
   Sfikas K., 2017, EXPLOITING PANORAMA, P1, DOI 10.2312/3dor.20171045
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su Jong-Chyi, 2018, P EUR C COMP VIS ECC, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Wang C., 2019, ARXIV190601592
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xie J, 2017, IEEE T MULTIMEDIA, V19, P2463, DOI 10.1109/TMM.2017.2698200
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
NR 50
TC 9
Z9 9
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 188
EP 201
DI 10.1109/TMM.2020.3047762
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300015
DA 2024-07-18
ER

PT J
AU Jiang, NF
   Chen, WL
   Lin, YT
   Zhao, TS
   Lin, CW
AF Jiang, Nanfeng
   Chen, Weiling
   Lin, Yuting
   Zhao, Tiesong
   Lin, Chia-Wen
TI Underwater Image Enhancement With Lightweight Cascaded Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Task analysis; Visualization; Physics;
   Mathematical models; Complexity theory; Laplace equations; Underwater
   images; image enhancement; oceanic image processing
ID HAZE REMOVAL; TRACKING
AB Due to light scatter and absorption in waterbody, underwater imaging can be easily impaired with low contrast and visual distortion. The resulting images are often unable to meet the quality requirements of human perception and computer processing. Therefore, Underwater Image Enhancement (UIE) has been attracting extensive research efforts. Although deep learning has demonstrated its great success in many vision tasks, its huge amounts of parameters and computations are not conducive to UIE in resource-limited scenarios. In this paper, we address this issue by proposing a Lightweight Cascaded Network (LCNet) based on Laplacian image pyramids. At each pyramid level, we implement cascaded blocks upon a residual network. Specifically, high quality residuals can be progressively predicted with significantly reduced complexity in a coarse-to-fine fashion. Furthermore, these sub-networks are recursively nested to build our LCNet, thereby reducing the overall computational complexity with reused parameters. Extensive experiments demonstrate that the proposed method performs favorably against the state-of-the-arts in terms of visual quality, model parameters and complexity.
C1 [Jiang, Nanfeng; Chen, Weiling; Lin, Yuting; Zhao, Tiesong] Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
C3 Fuzhou University; National Tsing Hua University
RP Jiang, NF (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou, Peoples R China.
RI Weiling, Chen/JAA-9972-2023; lin, yt/IQT-6771-2023; Lin,
   Yuting/HPE-4176-2023; Lin, Chia-Wen/M-4571-2013; Jiang,
   Nanfeng/AEO-9860-2022
OI Jiang, Nanfeng/0000-0003-1810-8311
CR Akkaynak D, 2019, PROC CVPR IEEE, P1682, DOI 10.1109/CVPR.2019.00178
   Ancuti CO, 2018, IEEE T IMAGE PROCESS, V27, P379, DOI 10.1109/TIP.2017.2759252
   Ancuti C, 2016, INT C PATT RECOG, P4202, DOI 10.1109/ICPR.2016.7900293
   Ancuti C, 2012, PROC CVPR IEEE, P81, DOI 10.1109/CVPR.2012.6247661
   Berman D, 2017, PROC BRIT MACH VIS C, V1, P203
   Berman D, 2021, IEEE T PATTERN ANAL, V43, P2822, DOI 10.1109/TPAMI.2020.2977624
   Carlevaris-Bianco N, 2010, OCEANS-IEEE
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Codruta AO, 2020, IEEE T IMAGE PROCESS, V29, P2653, DOI 10.1109/TIP.2019.2951304
   Drews PLJ, 2016, IEEE COMPUT GRAPH, V36, P24, DOI 10.1109/MCG.2016.26
   Fabbri C, 2018, IEEE INT CONF ROBOT, P7159
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2017, I S INTELL SIG PROC, P789, DOI 10.1109/ISPACS.2017.8266583
   Fu XY, 2017, PROC CVPR IEEE, P1715, DOI 10.1109/CVPR.2017.186
   Fu XY, 2014, IEEE IMAGE PROC, P4572, DOI 10.1109/ICIP.2014.7025927
   Galdran A, 2015, J VIS COMMUN IMAGE R, V26, P132, DOI 10.1016/j.jvcir.2014.11.006
   Ghani ASA, 2015, APPL SOFT COMPUT, V27, P219, DOI 10.1016/j.asoc.2014.11.020
   Guo YC, 2020, IEEE J OCEANIC ENG, V45, P862, DOI 10.1109/JOE.2019.2911447
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hou MJ, 2018, IEEE IMAGE PROC, P4043, DOI 10.1109/ICIP.2018.8451209
   Iqbal K, 2010, IEEE INT C SYSTEMS M
   Iqbal M, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2020.3021134
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kingma D. P, 2015, International Conference on Learning Representations
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lee D, 2012, OCEAN ENG, V48, P59, DOI 10.1016/j.oceaneng.2012.04.006
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li CY, 2021, IEEE T IMAGE PROCESS, V30, P4985, DOI 10.1109/TIP.2021.3076367
   Li CY, 2021, IEEE T CYBERNETICS, V51, P88, DOI 10.1109/TCYB.2020.2969255
   Li CY, 2020, IEEE T IMAGE PROCESS, V29, P4376, DOI 10.1109/TIP.2019.2955241
   Li CY, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107038
   Li J, 2018, IEEE ROBOT AUTOM LET, V3, P387, DOI 10.1109/LRA.2017.2730363
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Li RD, 2018, PROC CVPR IEEE, P8202, DOI 10.1109/CVPR.2018.00856
   Li YJ, 2016, COMPUT ELECTR ENG, V54, P68, DOI 10.1016/j.compeleceng.2016.08.008
   Liu RS, 2020, IEEE T CIRC SYST VID, V30, P4861, DOI 10.1109/TCSVT.2019.2963772
   Liu RS, 2019, AAAI CONF ARTIF INTE, P4368
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu YC, 2016, NEUROCOMPUTING, V196, P1, DOI 10.1016/j.neucom.2016.02.042
   Lu HM, 2018, FUTURE GENER COMP SY, V82, P142, DOI 10.1016/j.future.2018.01.001
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Novak B, 2020, 2020 ZOOMING INNOVATION IN CONSUMER TECHNOLOGIES CONFERENCE (ZINC), P165, DOI [10.1109/zinc50678.2020.9161446, 10.1109/ZINC50678.2020.9161446]
   Panetta K, 2016, IEEE J OCEANIC ENG, V41, P541, DOI 10.1109/JOE.2015.2469915
   Peng YT, 2018, IEEE T IMAGE PROCESS, V27, P2856, DOI 10.1109/TIP.2018.2813092
   Peng YT, 2017, IEEE T IMAGE PROCESS, V26, P1579, DOI 10.1109/TIP.2017.2663846
   Serikawa S, 2014, COMPUT ELECTR ENG, V40, P41, DOI 10.1016/j.compeleceng.2013.10.016
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang Y, 2018, COMPUT ELECTR ENG, V70, P904, DOI 10.1016/j.compeleceng.2017.12.006
   Wang YL, 2021, IEEE T MULTIMEDIA, V23, P2481, DOI 10.1109/TMM.2020.3013383
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wen HC, 2013, IEEE INT SYMP CIRC S, P753, DOI 10.1109/ISCAS.2013.6571956
   Yang M, 2015, IEEE T IMAGE PROCESS, V24, P6062, DOI 10.1109/TIP.2015.2491020
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2545, DOI 10.1109/TMM.2019.2908375
   Zhang H, 2020, IEEE T CIRC SYST VID, V30, P3943, DOI 10.1109/TCSVT.2019.2920407
   Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang S, 2017, NEUROCOMPUTING, V245, P1, DOI 10.1016/j.neucom.2017.03.029
   Zheng YQ, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2019.107180
   Zhou SC, 2019, IEEE I CONF COMP VIS, P2482, DOI 10.1109/ICCV.2019.00257
NR 65
TC 21
Z9 21
U1 11
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
DI 10.1109/TMM.2021.3115442
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 5C3SZ
UT WOS:000864185400003
DA 2024-07-18
ER

PT J
AU Liu, YX
   Wu, JJ
   Li, AB
   Li, LD
   Dong, WS
   Shi, GM
   Lin, WS
AF Liu, Yongxu
   Wu, Jinjian
   Li, Aobo
   Li, Leida
   Dong, Weisheng
   Shi, Guangming
   Lin, Weisi
TI Video Quality Assessment With Serial Dependence Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Degradation; Data mining; Video Quality
   Assessment; Serial Dependence; Long-Short Term Memory; Attention
ID IMAGE; SIMILARITY; PERCEPTION
AB Video quality assessment (VQA) is much more challenging than image quality assessment, due to the difficulty of modeling temporal influence among frames. Most of the existing VQA methods usually isolate each moment within the video (i.e., it neglects the sequential nature), leading to a large gap from the subjective perception. Recent research on neuroscience suggests a serially dependent perception (SDP) mechanism in the human visual system (HVS). Namely, the HVS tends to incorporate the recent past visual experience to predict the present perception. Inspired by the SDP, we suggest that the HVS prefers stable and continuous degradations in videos due to their predictability, and exhibits less tolerance to interrupted and unpredictable disturbances. Thus, we introduce a novel serial dependence modeling (SDM) framework for full-reference VQA in this paper. Firstly, the instantaneous degradation is measured on both the static appearance and motion information for each glimpse of scenes. Since motion plays an important role in videos, two types of structures are extracted for motion representation, namely, an explicit content-based 3D structure and an implicit feature-based 2D structure. Next, an assessment-directed long-short term memory (A-LSTM) is proposed to capture the serial dependence among instantaneous degradations. With the consideration of the perceptual effect from the previous moment on the current one, especially the effect from the perceptually worst moment, the serially dependent degradation is characterized. Finally, by mimicking the subjective rating for video-viewing, an attention-based quality decision procedure is presented to acquire the final video quality. Experimental results on publicly available VQA databases demonstrate that the proposed method maintains good consistency with the subjective perception.
C1 [Liu, Yongxu; Wu, Jinjian; Li, Aobo; Li, Leida; Dong, Weisheng; Shi, Guangming] Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Xidian University; Nanyang Technological University
RP Wu, JJ (corresponding author), Xidian Univ, Sch Artificial Intelligence, Xian 710071, Peoples R China.
EM yongxu.liu@stu.xidian.edu.cn; jinjian.wu@mail.xidian.edu.cn;
   abli@stu.xidian.edu.cn; ldli@xidian.edu.cn; wsdong@mail.xidian.edu.cn;
   gmshi@xidian.edu.cn; wslin@ntu.edu.sg
RI Wu, Jinjian/GQH-0222-2022; li, li/HII-4157-2022; Lin, Wei/D-3353-2012;
   Li, aoyu/GXV-4925-2022; Li, Li/AEM-3636-2022; Liu, Yongxu/JVE-2493-2024;
   Li, Aoyu/JCE-1542-2023; Li, Ao/GRR-5311-2022; Lin, Weisi/A-3696-2011
OI Liu, Yongxu/0009-0008-5719-1107; Li, Aoyu/0000-0001-5249-9964; Lin,
   Weisi/0000-0001-9866-1947
FU National Key R&D Program of China [2018AAA0101400]; National Natural
   Science Foundation of China [62022063, 61772388, 61632019]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0101400 and in part by the National Natural Science
   Foundation of China under Grants 62022063, 61772388, and 61632019.
CR [Anonymous], 2011, IVP Subjective Quality Video Database
   Bampis CG, 2017, IEEE SIGNAL PROC LET, V24, P1333, DOI 10.1109/LSP.2017.2726542
   Duanmu ZF, 2017, IEEE J-STSP, V11, P154, DOI 10.1109/JSTSP.2016.2608329
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Fischer J, 2014, NAT NEUROSCI, V17, P738, DOI 10.1038/nn.3689
   Francis G, 1996, PERCEPT PSYCHOPHYS, V58, P1203, DOI 10.3758/BF03207553
   Gu K, 2020, IEEE T MULTIMEDIA, V22, P311, DOI 10.1109/TMM.2019.2929009
   He LH, 2017, NEUROCOMPUTING, V269, P108, DOI 10.1016/j.neucom.2016.08.143
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P224, DOI 10.1007/978-3-030-01246-5_14
   Kiyonaga A, 2017, TRENDS COGN SCI, V21, P493, DOI 10.1016/j.tics.2017.04.011
   Li DQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2351, DOI 10.1145/3343031.3351028
   Li Z., 2016, NETFLIX TECH BL 0606, V6
   Liberman A, 2014, CURR BIOL, V24, P2569, DOI 10.1016/j.cub.2014.09.025
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu YT, 2018, IEEE T MULTIMEDIA, V20, P379, DOI 10.1109/TMM.2017.2729020
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Park J, 2013, IEEE T IMAGE PROCESS, V22, P610, DOI 10.1109/TIP.2012.2219551
   Peng P, 2017, PATTERN RECOGN, V70, P1, DOI 10.1016/j.patcog.2017.04.031
   Pitrey Y, 2012, PROC SPIE, V8291, DOI 10.1117/12.912180
   Rahnev D, 2015, PSYCHOL SCI, V26, P1664, DOI 10.1177/0956797615595037
   Saad MA, 2014, IEEE T IMAGE PROCESS, V23, P1352, DOI 10.1109/TIP.2014.2299154
   Seshadrinathan K, 2011, INT CONF ACOUST SPEE, P1153
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1
   Simonyan K, 2014, ADV NEUR IN, V27
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Summerfield C, 2014, NAT REV NEUROSCI, V15, P745, DOI 10.1038/nrn3838
   Tu ZZ, 2020, IEEE IMAGE PROC, P141, DOI 10.1109/ICIP40778.2020.9191169
   Vaswani A, 2017, ADV NEUR IN, V30
   Vu PV, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013016
   Wang Y, 2012, IEEE T CIRC SYST VID, V22, P989, DOI 10.1109/TCSVT.2012.2186745
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Whitney D, 2000, VISION RES, V40, P137, DOI 10.1016/S0042-6989(99)00166-2
   Wolf S., 2011, Tech. Memo TM-11-482
   Wu JJ, 2019, IEEE T MULTIMEDIA, V21, P2738, DOI 10.1109/TMM.2019.2908377
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Xu MN, 2020, INT CONF ACOUST SPEE, P4447, DOI [10.1109/ICASSP40776.2020.9053031, 10.1109/icassp40776.2020.9053031]
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   You JY, 2019, IEEE IMAGE PROC, P2349, DOI [10.1109/icip.2019.8803395, 10.1109/ICIP.2019.8803395]
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
   Zhang Y, 2020, IEEE T NEUR NET LEAR, V31, P2716, DOI 10.1109/TNNLS.2018.2890310
NR 46
TC 5
Z9 6
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3754
EP 3768
DI 10.1109/TMM.2021.3107148
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400007
DA 2024-07-18
ER

PT J
AU Qu, LF
   Chen, F
   Zhang, SJ
   He, HJ
AF Qu, Lingfeng
   Chen, Fan
   Zhang, Shanjun
   He, Hongjie
TI Cryptanalysis of Reversible Data Hiding in Encrypted Images by Block
   Permutation and Co-Modulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encryption; Security; Streaming media; Ciphers; Cloud computing;
   Resists; Estimation; Reversible data hiding; known plaintext attack;
   block permutation and co-modulation; security analysis
ID ONLY MULTIMEDIA CIPHERS; QUANTITATIVE CRYPTANALYSIS; SECURITY
AB Reversible data hiding in encrypted images (RDH-EI) technology is commonly used in cloud storage images for privacy protection. Most existing RDH-EI techniques reported in the literature applied block permutation and co-modulation (BPCM) encryption to generate encrypted images. This work analyses the security of the RDH-EI algorithm based on BPCM encryption under known plaintext attacks (KPAs). Different from the existing KPAs, this paper considers that attackers can perform KPAs based on marked encrypted images and shows that BPCM encryption has the risk of information leakage. To find the constant features of a block before and after co-modulation, the first-pixel difference block (FDB) of a block is first defined. Then, a pseudo cypher difference image of the cyphertext image is constructed to eliminate the changed FDBs so that the differences in the cyphertext FDBs are the same as the FDBs in the corresponding plaintext difference image. Finally, we design an FDB-based block permutation key estimation method according to the plaintext difference image and pseudocyphertext difference image. The influence of block size on key estimation accuracy and the time complexity of the proposed KPA algorithm are analysed and discussed. Experimental results show that the correct rate of key estimation is positively correlated with the block size and the number of plain-cyphertext pairs. The average correct rate of key estimation reaches 63% when the block size is greater than 3x3.
C1 [Qu, Lingfeng; He, Hongjie] Southwest Jiaotong Univ, Key Lab Signal & Informat Proc, Chengdu 611756, Peoples R China.
   [Chen, Fan] Southwest Jiaotong Univ, Sch Comp & Artificial Intelligence, Chengdu 611756, Peoples R China.
   [Zhang, Shanjun] Kanagawa Univ, Dept Informat Sci, Fac Sci, Kanazawa, Ishikawa 9201192, Japan.
C3 Southwest Jiaotong University; Southwest Jiaotong University; Kanagawa
   University
RP He, HJ (corresponding author), Southwest Jiaotong Univ, Key Lab Signal & Informat Proc, Chengdu 611756, Peoples R China.
EM 792443987@qq.com; fchen@swjtu.edu.cn; zhang@info.kanagawa-u.ac.jp;
   hjhe@swjtu.edu.cn
RI Lingfeng, Qu/GLT-0330-2022
OI Lingfeng, Qu/0000-0002-2544-4324
FU National Natural Science Foundation of China (NSFC) [U1936113,
   61872303]; Technology Department of Sichuan Province [2021004]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants U1936113 and 61872303, and in
   part by the Technology Department of Sichuan Province under Grant
   2021004.
CR [Anonymous], 1977, FEDERAL INFONNATION, V46
   [Anonymous], 1883, J SCI MILITAIRES
   Chen F, 2021, IEEE T CIRC SYST VID, V31, P905, DOI 10.1109/TCSVT.2020.2992817
   Chen K, 2018, INT J DIGIT CRIME FO, V10, P1, DOI 10.4018/IJDCF.2018040101
   Chen YY, 2018, CMC-COMPUT MATER CON, V56, P299, DOI 10.3970/cmc.2018.03179
   Dawen Xu, 2018, Security and Communication Networks, V2018, DOI 10.1155/2018/1734961
   Dawen Xu, 2016, Digital Forensics and Watermarking. 14th International Workshop, IWDW 2015. Revised Selected Papers: LNCS 9569, P365, DOI 10.1007/978-3-319-31960-5_30
   Dragoi IC, 2021, IEEE T INF FOREN SEC, V16, P187, DOI 10.1109/TIFS.2020.3006382
   Ge HL, 2019, IEEE T CIRC SYST VID, V29, P2285, DOI 10.1109/TCSVT.2018.2863029
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Jolfaei A, 2016, IEEE T INF FOREN SEC, V11, P235, DOI 10.1109/TIFS.2015.2489178
   Khelifi F, 2018, SIGNAL PROCESS, V143, P336, DOI 10.1016/j.sigpro.2017.09.020
   Li CQ, 2011, SIGNAL PROCESS, V91, P949, DOI 10.1016/j.sigpro.2010.09.014
   Li SJ, 2008, SIGNAL PROCESS-IMAGE, V23, P212, DOI 10.1016/j.image.2008.01.003
   Liu ZL, 2018, INFORM SCIENCES, V433, P188, DOI 10.1016/j.ins.2017.12.044
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qu LF, 2020, MULTIMED TOOLS APPL, V79, P29451, DOI 10.1007/s11042-020-09379-3
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Wang P, 2020, IEEE ACCESS, V8, P28902, DOI 10.1109/ACCESS.2020.2972622
   Wu YQ, 2020, IEEE T MULTIMEDIA, V22, P1929, DOI 10.1109/TMM.2019.2952979
   Xu DW, 2019, SECUR COMMUN NETW, DOI 10.1155/2019/7480147
   Xu DW, 2016, SIGNAL PROCESS, V123, P9, DOI 10.1016/j.sigpro.2015.12.012
   Yi S, 2019, IEEE T MULTIMEDIA, V21, P51, DOI 10.1109/TMM.2018.2844679
   Yi S, 2018, SIGNAL PROCESS, V150, P171, DOI 10.1016/j.sigpro.2018.04.016
   Yi S, 2017, IEEE IMAGE PROC, P4322, DOI 10.1109/ICIP.2017.8297098
   Yin ZX, 2020, IEEE T MULTIMEDIA, V22, P874, DOI 10.1109/TMM.2019.2936314
   Yin ZX, 2017, MULTIMED TOOLS APPL, V76, P3899, DOI 10.1007/s11042-016-4049-z
   Yin ZX, 2016, INT CONF ACOUST SPEE, P2129, DOI 10.1109/ICASSP.2016.7472053
   Zhang LY, 2018, INFORM SCIENCES, V430, P228, DOI 10.1016/j.ins.2017.11.021
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
NR 35
TC 26
Z9 27
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2924
EP 2937
DI 10.1109/TMM.2021.3090588
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000019
DA 2024-07-18
ER

PT J
AU Song, SJ
   Liu, JY
   Lin, LL
   Guo, ZM
AF Song, Sijie
   Liu, Jiaying
   Lin, Lilang
   Guo, Zongming
TI Learning to Recognize Human Actions From Noisy Skeleton Data Via Noise
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Noise measurement; Adaptation models; Feature extraction;
   Cameras; Three-dimensional displays; Pose estimation; Action
   recognition; noisy skeletons; regression model; generative model; noise
   adaptation
ID NETWORK
AB Recent studies have made great progress on skeleton-based action recognition. However, most of them are developed with relatively clean skeletons without the presence of intensive noise. We argue that the models learned from relatively clean data are not well generalizable to handle noisy skeletons commonly appeared in the real world. In this paper, we address the challenge of recognizing human actions from noisy skeletons, which is seldom explored by previous methods. Beyond exploring the new problem, we further take a new perspective to address it, i.e., noise adaptation, which gets rid of explicit skeleton noise modeling and reliance on skeleton ground truths. Specifically, we develop regression-based and generation-based adaptation models according to whether pairs of noisy skeletons are available. The regression-based model aims to learn noise-suppressed intrinsic feature representations by mapping pairs of noisy skeletons into a noise-robust space. When only unpaired skeletons are accessible, the generation-based model aims to adapt the features from noisy skeletons to a low-noise space by adversarial learning. To verify our proposed model and facilitate research on noisy skeletons, we collect a new dataset Noisy Skeleton Dataset (NSD), the skeletons of which are with much noise and more similar to daily-life data than previous datasets. Extensive experiments are conducted on the NSD, VV-RGBD and N-UCLA datasets, and results consistently show the outstanding performance of our proposed model.
C1 [Song, Sijie; Liu, Jiaying; Lin, Lilang; Guo, Zongming] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100080, Peoples R China.
EM ssj940920@pku.edu.cn; liujiaying@pku.edu.cn; linlilang@pku.edu.cn;
   guozongming@pku.edu.cn
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576; Song, Sijie/0000-0002-2085-6370
FU National Key Research and Development Program of China [2018AAA0102702];
   Fundamental Research Funds for the Central Universities; National
   Natural Science Foundation of China [61772043]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102702, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   the National Natural Science Foundation of China under Grant 61772043.
   The Guest Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Xian-Sheng Hua.
CR [Anonymous], 2017, Proc. of British Machine Vision Conference BMVC
   Bousmalis K, 2016, ADV NEUR IN, V29
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen JW, 2018, PROC CVPR IEEE, P3155, DOI 10.1109/CVPR.2018.00333
   Demisse GG, 2018, IEEE COMPUT SOC CONF, P301, DOI 10.1109/CVPRW.2018.00056
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han F, 2017, COMPUT VIS IMAGE UND, V158, P85, DOI 10.1016/j.cviu.2017.01.011
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Ji YL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1510, DOI 10.1145/3240508.3240675
   JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378
   Kaneko T, 2019, PROC CVPR IEEE, P2462, DOI 10.1109/CVPR.2019.00257
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kingma D. P., 2014, arXiv
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li C, 2018, IEEE INT CONF SENS, P1, DOI 10.1109/TFUZZ.2018.2878200
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Moran Nick, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12061, DOI 10.1109/CVPR42600.2020.01208
   Nie Q, 2019, IEEE T IMAGE PROCESS, V28, P3959, DOI 10.1109/TIP.2019.2907048
   Odena A, 2017, PR MACH LEARN RES, V70
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Taigman Y., 2017, INT C LEARN REPR ICL, P1
   Thekumparampil K.K., 2018, NEURIPS, P10271
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vemulapalli R, 2016, PROC CVPR IEEE, P4471, DOI 10.1109/CVPR.2016.484
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yun K., 2012, 2012 IEEE COMP SOC C, P28, DOI DOI 10.1109/CVPRW.2012.6239234
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2116, DOI 10.1109/ICCV.2017.231
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhou H, 2018, PROC CVPR IEEE, P6238, DOI 10.1109/CVPR.2018.00653
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu WT, 2016, AAAI CONF ARTIF INTE, P3697
NR 51
TC 4
Z9 4
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1152
EP 1163
DI 10.1109/TMM.2021.3120631
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800012
DA 2024-07-18
ER

PT J
AU Xia, RJ
   Li, YS
   Luo, WH
AF Xia, Rongjie
   Li, Yanshan
   Luo, Wenhan
TI LAGA-Net: Local-and-Global Attention Network for Skeleton Based Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton; Feature extraction; Joints; Adaptation models;
   Three-dimensional displays; Kernel; Computational modeling; Action
   Recognition; Skeleton Sequence; Spatio-temporal Feature; Attention
   Mechanism; Motion Enhancement
AB Skeleton-based action recognition has attracted significant attention and obtained widespread applications due to the robustness of 3D skeleton data. One of the key challenges is how to extract discriminative and robust spatio-temporal features from sparse skeleton data to describe actions and improve recognition accuracy. To address this issue, this paper combines convolutions with attention mechanisms and proposes a deep network for skeleton-based action recognition, termed as local-and-global attention network (LAGA-Net). First, we encode skeleton sequences into joint feature evolution maps to compactly describe the spatial and temporal characteristics of skeleton sequences. Then, a motion guided channel attention module (MGCAM) is proposed to model the interdependencies between feature channels by calculating temporal frame-level motion and enhance motion-salient features in a channel-wise way. Further, a spatio-temporal attention module (STAM) is proposed to model spatio-temporal context-aware collaboration at sequence level and extract spatio-temporal attention features that involve long-range dependencies. Together, MGCAM and STAM are combined to form LAGA-Net, which extracts discriminative features integrating both local and global representations of skeleton sequences. Moreover, a two-stream architecture is proposed to learn complementary features from joint and bone aspects. We conduct extensive experiments to verify the effectiveness and superiority of our proposed method over state-of-the-art approaches on several benchmarks (e.g., NTU RGB+D, Northwestern-UCLA, UTD-MHAD and NTU RGB+D 120).
C1 [Xia, Rongjie; Li, Yanshan] Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518060, Peoples R China.
   [Luo, Wenhan] Tencent, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Tencent
RP Xia, RJ; Li, YS (corresponding author), Shenzhen Univ, Coll Elect & Informat Engn, Shenzhen 518060, Peoples R China.
EM xiarongjie2017@email.szu.edu.cn; lys@szu.edu.cn; whluo.china@gmail.com
RI Luo, Wenhan/GZL-0535-2022
OI Luo, Wenhan/0000-0002-5697-4168
FU National Natural Science Foundation of China [61771319, 62076165,
   61871154]; Natural Science Foundation of Guangdong Province
   [2019A1515011307]; Shenzhen Science and Technology Project
   [JCYJ20180507182259896, 20200826154022001, 2020KCXTD004,
   WDZC20195500201]
FX This work was supported in part by National Natural Science Foundation
   of China under Grants 61771319, 62076165, 61871154, in part by the
   Natural Science Foundation of Guangdong Province under Grants
   2019A1515011307, and in part by the Shenzhen Science and Technology
   Project (Nos. JCYJ20180507182259896, 20200826154022001) and the other
   project (Nos. 2020KCXTD004, WDZC20195500201). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Andrew Bagdanov.
CR Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Chen C, 2015, IEEE IMAGE PROC, P168, DOI 10.1109/ICIP.2015.7350781
   Du Y, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P579, DOI 10.1109/ACPR.2015.7486569
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Ke QH, 2018, IEEE T IMAGE PROCESS, V27, P2842, DOI 10.1109/TIP.2018.2812099
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li B, 2017, IEEE INT C COMPUT, P187, DOI 10.1109/CSE-EUC.2017.217
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li CK, 2019, IEEE T HUM-MACH SYST, V49, P95, DOI 10.1109/THMS.2018.2883001
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li S., 2019, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P3595
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YS, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107293
   Li YS, 2019, IEEE INT CON MULTI, P1066, DOI 10.1109/ICME.2019.00187
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu X., 2020, NEUROCOMPUTING, P288
   Liu YH, 2019, AAAI CONF ARTIF INTE, P8786
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Nie Q, 2019, IEEE T IMAGE PROCESS, V28, P3959, DOI 10.1109/TIP.2019.2907048
   Ren B., 2020, SURVEY 3D SKELETON B
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Si CY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107511
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Thien HT, 2020, INFORM SCIENCES, V513, P112, DOI 10.1016/j.ins.2019.10.047
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2018, PATTERN RECOGN, V81, P23, DOI 10.1016/j.patcog.2018.03.030
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu S., 2020, IEEE Internet of Things Journal, P1
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Zhang XK, 2020, IEEE T NEUR NET LEAR, V31, P3047, DOI 10.1109/TNNLS.2019.2935173
   Zhou Y, 2014, IEEE INT CONGR BIG, P1, DOI 10.1109/BigData.Congress.2014.11
NR 48
TC 17
Z9 19
U1 10
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2648
EP 2661
DI 10.1109/TMM.2021.3086758
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600030
DA 2024-07-18
ER

PT J
AU Zhang, XK
   Zhu, YL
   Chen, WT
   Liu, WS
   Shen, LL
AF Zhang, Xiaokang
   Zhu, Yuanlue
   Chen, Wenting
   Liu, Wenshuang
   Shen, Linlin
TI Gated SwitchGAN for Multi-Domain Facial Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Switches; Generators; Logic gates; Faces; Control systems; Feature
   extraction; Task analysis; GANs; Image translation; Feature switching;
   Attribute intensity control
AB Recent studies on multi-domain facial image translation have achieved impressive results. The existing methods generally provide a discriminator with an auxiliary classifier to impose domain translation. However, these methods neglect important information regarding domain distribution matching. To solve this problem, we propose a switch generative adversarial network (SwitchGAN) with a more adaptive discriminator structure and a matched generator to perform delicate image translation among multiple domains. A feature-switching operation is proposed to achieve feature selection and fusion in our conditional modules. We demonstrate the effectiveness of our model. Furthermore, we also introduce a new capability of our generator that represents attribute intensity control and extracts content information without tailored training. Experiments on the Morph, RaFD and CelebA databases visually and quantitatively show that our extended SwitchGAN (i.e., Gated SwitchGAN) can achieve better translation results than StarGAN, AttGAN and STGAN. The attribute classification accuracy achieved using the trained ResNet-18 model and the FID score obtained using the ImageNet pretrained Inception-v3 model also quantitatively demonstrate the superior performance of our models.
C1 [Zhang, Xiaokang; Zhu, Yuanlue; Chen, Wenting; Liu, Wenshuang; Shen, Linlin] Shenzhen Univ, Sch Comp Sci & Software Engn, Comp Vis Inst, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Zhang, Xiaokang; Zhu, Yuanlue; Chen, Wenting; Liu, Wenshuang; Shen, Linlin] Shenzhen Inst Artificial Intelligence & Robot Soc, Shenzhen, Peoples R China.
C3 Shenzhen University; Shenzhen Institute of Artificial Intelligence &
   Robotics for Society
RP Shen, LL (corresponding author), Shenzhen Univ, Sch Comp Sci & Software Engn, Comp Vis Inst, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM zhangxiaokang2019@email.szu.edu.cn; zhuyuanlue2017@email.szu.edu.cn;
   chenwenting2017@email.szu.edu.cn; liuwenshuang2018@email.szu.edu.cn;
   llshen@szu.edu.cn
RI Shen, Linlin/AEX-9392-2022
OI Shen, Linlin/0000-0003-1420-0815; Chen, Wenting/0000-0002-7457-9540
FU National Natural Science Foundation of China [91959108, andU1713214]
FX This work was supported in part by the National Natural Science
   Foundation of China underGrants 91959108, andU1713214.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Anoosheh A, 2018, IEEE COMPUT SOC CONF, P896, DOI 10.1109/CVPRW.2018.00122
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen X, 2016, ADV NEUR IN, V29
   Chen YC, 2018, PROC CVPR IEEE, P3541, DOI 10.1109/CVPR.2018.00373
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Ding H, 2018, AAAI CONF ARTIF INTE, P6781
   Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Huang J., 2020, IEEE T MULTIMEDIA, P1
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Lample Guillaume, 2017, P ANN C NEUR INF PRO, P5967
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lim J. H., 2017, COMPUT RES REPOSITOR
   Liu M.-Y., 2017, NIPS
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu T., 2019, NEURAL COMPUT APPL, P1
   Lu T, 2020, NEUROCOMPUTING, V387, P309, DOI 10.1016/j.neucom.2020.01.015
   Mirza M., 2014, COMPUT RES REPOSITOR
   Odena A, 2017, PR MACH LEARN RES, V70
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Perarnau G., 2016, COMPUT RES REPOSITOR
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Shi CL, 2020, PATTERN RECOGN LETT, V138, P520, DOI 10.1016/j.patrec.2020.08.021
   Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Yuanlue Zhu, 2019, 2019 IEEE International Conference on Multimedia and Expo (ICME). Proceedings, P1198, DOI 10.1109/ICME.2019.00209
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang ZF, 2017, PROC CVPR IEEE, P4352, DOI 10.1109/CVPR.2017.463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 46
TC 3
Z9 4
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1990
EP 2003
DI 10.1109/TMM.2021.3074807
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wan, RT
   Zhou, JJ
   Huang, BW
   Zeng, H
   Fan, YB
AF Wan, Rentao
   Zhou, Jinjia
   Huang, Bowen
   Zeng, Hui
   Fan, Yibo
TI APMC: Adjacent Pixels Based Measurement Coding System for Compressively
   Sensed Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image coding; Prediction algorithms; Encoding; Correlation; Image
   reconstruction; Current measurement; Compressed sensing; Compressed
   sensing; deterministic sampling matrix; measurement coding;
   measurement-domain prediction
ID RESTRICTED ISOMETRY PROPERTY; SENSING MATRIX DESIGN; SIGNAL
   RECONSTRUCTION; TOEPLITZ MATRICES
AB Compressed sensing is now regarded as an effective method for dimension reduction during signal acquisition. One significant, yet under-addressed issue regarding the transmission of compressively sensed images is how to compress and code measurements, the output of a compressed sensing sensor. As the spatially adjacent correlation in the measurement domain is weak, conventional encoding algorithms cannot be applied directly for measurements. In this paper, we propose the adjacent pixels based measurement coding system (APMC) to generate compressed image bit-streams. Firstly, an adjacent pixels based measurement matrix (APMM) is applied to embed the pixel-domain boundary information of each block to the measurement domain. By adopting APMM, the pixel-domain information can be efficiently used for measurement-domain intra prediction. Moreover, to avoid the interference of pixels that are far apart and achieve a high prediction accuracy, we employ boundary measurements of neighboring blocks as references for prediction. Finally, we propose a rate control algorithm to process the residuals between measurements and predictions, to generate a coded bit sequence for transmitting. Experimental results demonstrated superiority in rate-distortion performance and bandwidth costs in transmitting as compared to previous schemes. Compared to the state-of-the-art, this work achieves a 24% decrease in bitrate and a 1.68 dB increase in Peak Signal-to-Noise Ratio (PSNR) on average.
C1 [Wan, Rentao; Huang, Bowen; Fan, Yibo] Fudan Univ, State Key Lab ASIC & Syst, Shanghai 200433, Peoples R China.
   [Zhou, Jinjia] Hosei Univ, Grad Sch Sci & Engn, Tokyo 1848584, Japan.
   [Zeng, Hui] Zhejiang Gongshang Univ, Sch Stat & Math, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Fudan University; Hosei University; Zhejiang Gongshang University
RP Fan, YB (corresponding author), Fudan Univ, State Key Lab ASIC & Syst, Shanghai 200433, Peoples R China.
EM 16300720035@fudan.edu.cn; jinjia.zhou.35@hosei.ac.jp;
   bwhuang19@fudan.edu.cn; zenghui@zjgsu.edu.cn; fanyibo@fudan.edu.cn
RI fan, yi/GYU-1036-2022
OI Wan, Rentao/0000-0003-4146-9684
FU Japan Science and Technology Agency (JST) PRESTO Japan [JPMJPR1757];
   National Natural Science Foundation of China [62031009]; Shanghai
   Science and Technology Committee (STCSM) [19511104300]; Alibaba
   Innovative Research (AIR) Program; Innovation Program of Shanghai
   Municipal Education Commission; Fudan University-CIOMP Joint Fund
   [FC2019-001]; Fudan-ZTE Joint Lab
FX This work was supported in part by Japan Science and Technology Agency
   (JST) PRESTO under Grant JPMJPR1757 Japan, in part by the National
   Natural Science Foundation of China under Grant 62031009, in part by
   Shanghai Science and Technology Committee (STCSM) under Grant
   19511104300, in part by Alibaba Innovative Research (AIR) Program, in
   part by the Innovation Program of Shanghai Municipal Education
   Commission, in part by the Fudan University-CIOMP Joint Fund
   (FC2019-001), and in part by the Fudan-ZTE Joint Lab. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. S. Mehrotra.
CR Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Candes E., 2005, l1-magic: Recovery of sparse signals via convex programming, P14
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candès EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014
   Dadkhah M, 2015, IEEE SENS J, V15, P3699, DOI 10.1109/JSEN.2015.2397874
   Dehghan H, 2015, IEEE T SIGNAL PROCES, V63, P5665, DOI 10.1109/TSP.2015.2457391
   Dolatabadi HM, 2019, IEEE SIGNAL PROC LET, V26, P1501, DOI 10.1109/LSP.2019.2923843
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gesen Zhang, 2010, Proceedings of the 2010 International Conference on Information and Automation (ICIA 2010), P455, DOI 10.1109/ICINFA.2010.5512379
   Kingsbury N, 2001, APPL COMPUT HARMON A, V10, P234, DOI 10.1006/acha.2000.0343
   Li LX, 2020, IEEE T MULTIMEDIA, V22, P82, DOI 10.1109/TMM.2019.2923111
   Lotfi M, 2020, IEEE T SIGNAL PROCES, V68, P3008, DOI 10.1109/TSP.2020.2990154
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Moshtaghpour A, 2020, IEEE T INFORM THEORY, V66, P7253, DOI 10.1109/TIT.2020.2992852
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Obermeier R, 2019, IEEE T COMPUT IMAG, V5, P27, DOI 10.1109/TCI.2018.2884291
   Obermeier R, 2017, IEEE T COMPUT IMAG, V3, P217, DOI 10.1109/TCI.2017.2671398
   Oike Y, 2013, IEEE J SOLID-ST CIRC, V48, P318, DOI 10.1109/JSSC.2012.2214851
   Peetakul J, 2019, IEEE DATA COMPR CONF, P599, DOI 10.1109/DCC.2019.00111
   Rauhut H., 2009, MATHEMATICS-BASEL
   Shi W., 2019, IEEE T CIRC SYST VID, V31, P425
   Tran Thuy T. T., 2020, IEEE INT WORKSH MULT, P1, DOI [10.1109/MMSP48831.2020.9287074, DOI 10.1109/mmsp48831.2020.9287074]
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yu NY, 2018, IEEE T INF FOREN SEC, V13, P1722, DOI 10.1109/TIFS.2018.2800726
   Zhang J, 2013, IEEE IMAGE PROC, P1021, DOI 10.1109/ICIP.2013.6738211
   Zhou JB, 2017, IEICE T FUND ELECTR, VE100A, P2869, DOI 10.1587/transfun.E100.A.2869
NR 29
TC 3
Z9 4
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 6
PY 2021
VL 24
BP 3558
EP 3569
DI 10.1109/TMM.2021.3102394
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NO
UT WOS:000824707300002
DA 2024-07-18
ER

PT J
AU Ouyang, NL
   Huang, QB
   Li, PJ
   Cai, Y
   Liu, B
   Leung, HF
   Li, Q
AF Ouyang, Ninglin
   Huang, Qingbao
   Li, Pijian
   Cai, Yi
   Liu, Bin
   Leung, Ho-fung
   Li, Qing
TI Suppressing Biased Samples for Robust VQA
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Visualization; Training data; Image color analysis; Sports;
   Knowledge discovery; Annotations; Visual Question Answering; Language
   Priors; Language Bias; Robust VQA
AB Most existing visual question answering (VQA) models strongly rely on language bias to answer questions, i.e., they always tend to fit question-answer pairs on the train split and perform poorly on the test spilt when the answer distributions are different. This behavior makes them hard to be applied in real scenarios. To reduce the language biases, previous studies mainly integrate modules to overcome language priors (ensemble-based methods) or generate additional training data to balance dataset biases (data-balanced methods). However, all the existing ensemble-based methods drop their accuracies on the VQA v2 dataset, while data-balanced methods may introduce new biases and cannot guarantee the quality of the generated data. In this paper, we propose a model-agnostic training scheme called Suppressing Biased Samples (SBS) to overcome language priors. SBS consists of two collaborative parts, i.e., a Data Classifier Module to divide the dataset into biased samples and unbiased samples by utilizing the similarity in the semantic space, and a Bias Penalty Module to suppress the biased samples to weaken their influence. As a new way of balancing data to address language bias, SBS overcomes the shortcomings of previous data-balanced methods. Experimental results show that our method can be merged into other bias-reduction methods and achieves a new state-of-the-art performance on the commonly used VQA-CP v2 dataset.
C1 [Ouyang, Ninglin; Li, Pijian; Liu, Bin] Guangxi Univ, Sch Elect Engn, Nanning 530005, Peoples R China.
   [Huang, Qingbao] Guangxi Univ, Sch Elect Engn, Inst Artificial Intelligence, Guangxi Key Lab Multimedia Commun & Network Techn, Nanning 530004, Peoples R China.
   [Huang, Qingbao; Cai, Yi] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
   [Cai, Yi] MOE China, Key Lab Big Data & Intelligent Robot SCUT, Guangzhou 510006, Peoples R China.
   [Leung, Ho-fung] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
C3 Guangxi University; Guangxi University; South China University of
   Technology; Chinese University of Hong Kong; Hong Kong Polytechnic
   University
RP Huang, QB (corresponding author), Guangxi Univ, Sch Elect Engn, Inst Artificial Intelligence, Guangxi Key Lab Multimedia Commun & Network Techn, Nanning 530004, Peoples R China.
EM 985449541@qq.com; qbhuang@gxu.edu.cn; 1912302005@st.gxu.edu.cn;
   ycai@scut.edu.cn; bingo.liu@csu.edu.cn; lhf@cuhk.edu.hk;
   csqli@comp.polyu.edu.hk
RI Leung, Ho-fung/F-2878-2011; Li, Qing/JMH-1365-2023
OI Leung, Ho-fung/0000-0003-4914-2934; Li, Qing/0000-0003-3370-471X; Huang,
   Qingbao/0000-0001-7691-347X; Li, Pijian/0009-0005-1924-5248
FU National Natural Science Foundation of China [62076100, 51767005];
   Fundamental Research Funds for the Central Universities, SCUT [D2210010,
   D2200150, D2201300]; Science and Technology Planning Project of
   Guangdong Province [2020B0101100002]; Science and Technology Programs of
   Guangzhou [201704030076, 201707010223, 201802010027, 201902010046];
   HongKong Research Grants Council, China [PolyU1121417, C1031-18 G]; Hong
   Kong Polytechnic University, China [1.9B0V]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62076100 and 51767005, in part by the collaborative
   research grants from the Fundamental Research Funds for the Central
   Universities, SCUT under Grants D2210010, D2200150, and D2201300, in
   part by the Science and Technology Planning Project of Guangdong
   Province under Grant 2020B0101100002, in part by the Science and
   Technology Programs of Guangzhou under Grants 201704030076,
   201707010223, 201802010027, and 201902010046, in part by theHongKong
   Research Grants Council, China under Grants PolyU1121417 and C1031-18 G,
   and in part by an internal research grant from the Hong Kong Polytechnic
   University, China under Grant 1.9B0V. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Vasileios Mezaris.
CR Agarwal Vedika, 2020, P IEEE CVF C COMP VI, P9690
   Agrawal A., 2016, P C EMP METH NAT LAN, P1955
   Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 1997, NEURAL COMPUT
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Cadene R., 2019, P INT C NEUR INF PRO, P839
   Chen Long, 2020, P IEEE CVF C COMP VI
   Clark C, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4069
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Gan C, 2017, IEEE I CONF COMP VIS, P1829, DOI 10.1109/ICCV.2017.201
   Gao LL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1742, DOI 10.1145/3240508.3240687
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Grand Gabriel, 2019, P 2 WORKSH SHORTC VI, P1
   Huang Q., 2020, P 58 ANN M ASS COMP, P7166
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Jing CC, 2020, AAAI CONF ARTIF INTE, V34, P11181
   Khademi M, 2020, P 58 ANN M ASS COMP, P7177, DOI 10.18653/v1/2020.acl-main.643
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li L., 2020, IEEE T MULTIMEDIA, V23, P1264
   Li Y, 2019, PROC CVPR IEEE, P9564, DOI 10.1109/CVPR.2019.00980
   Liu F., 2020, IEEE Transactions on Multimedia, P1
   Lu JS, 2019, ADV NEUR IN, V32
   Mao J., 2019, PROC INT C LEARN REP
   Qing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P141, DOI 10.1007/978-3-030-58536-5_9
   Ramakrishnan S, 2018, ADV NEUR IN, V31
   Ren S., 2015, NEURAL INFORM PROCES, V28, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Selvaraju RR, 2019, IEEE I CONF COMP VIS, P2591, DOI 10.1109/ICCV.2019.00268
   Shrestha Robik, 2020, P 58 ANN M ASS COMPU, P8172
   Teney Damien, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P580, DOI 10.1007/978-3-030-58607-2_34
   Wu JL, 2019, ADV NEUR IN, V32
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang Jianwei, 2020, ARXIV201211587
   Yi K., 2020, PROC INT C LEARN REP
   Yi KX, 2018, ADV NEUR IN, V31
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu Z., 2020, ARXIV200412070, P3743
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zhang LY, 2021, IEEE T NEUR NET LEAR, V32, P4362, DOI 10.1109/TNNLS.2020.3017530
   Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhi X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1083
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 49
TC 12
Z9 12
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 16
PY 2021
VL 24
BP 3405
EP 3415
DI 10.1109/TMM.2021.3097502
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NK
UT WOS:000824706900003
DA 2024-07-18
ER

PT J
AU Azzam, M
   Wu, WH
   Cao, WM
   Wu, S
   Wong, HS
AF Azzam, Mohamed
   Wu, Wenhao
   Cao, Wenming
   Wu, Si
   Wong, Hau-San
TI KTransGAN: Variational Inference-Based Knowledge Transfer for
   Unsupervised Conditional Generative Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Data models; Generators; Gallium nitride; Adaptation models;
   Task analysis; Generative adversarial networks; Knowledge transfer;
   generative learning; domain adaptation; image classification
ID ATTENTION
AB Class-conditional generative models have gained popularity due to their characteristics of learning disentangled representations. However, these models typically require labeled examples in training. In this paper, we explore the feasibility of training these models on completely unlabeled data, under the assumption that we have access to other labeled data. The labeled data share the same label space, while their domain is shifted. Our model, which we refer to as KTransGAN, incorporates a classifier to transfer knowledge from the labeled data and performs collaborative learning with the conditional generator. By adopting these measures, KTransGAN is able to approximate the conditional distribution of the unlabeled data and simultaneously introduces a new solution to the unsupervised domain adaptation problem. To mitigate the training difficulty of our generative adversarial networks-based model, variational encoding and feature matching are also considered. From the empirical results, KTransGAN exhibits outstanding performance on a number of synthetic datasets and multiple real-world benchmarks. The quality of the synthesized instances is far superior to the pure variational autoencoding model. For example, on the CIFAR-10 dataset, our model scores 35.3 in FID, while the other model scores 128.45. In addition, the synthesis quality is close to the case when the model is trained in a fully supervised setting over the same number of training iterations. Regarding the classification performance, for instance, our model surpasses the highest state-of-the-art results (89.19%) by a large margin and achieves a test accuracy of 95.31% on the unlabeled data SVHN, while MNIST represents the labeled data. These results highlight the effectiveness of our proposed framework.
C1 [Azzam, Mohamed; Wu, Si; Wong, Hau-San] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong 999077, Peoples R China.
   [Wu, Wenhao; Wu, Si] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Cao, Wenming] Univ Hong Kong, Dept Stat & Actuarial Sci, Hong Kong 999077, Peoples R China.
C3 City University of Hong Kong; South China University of Technology;
   University of Hong Kong
RP Wu, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong 999077, Peoples R China.; Wu, S (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM m.azzam@my.cityu.edu.hk; wenhaowu.chn@gmail.com; wmingcao@hku.hk;
   cswusi@scut.edu.cn; cshswong@cityu.edu.hk
RI cao, wenming/Y-5293-2019
OI WONG, Hau-San/0000-0002-1530-7529; Azzam, Mohamed/0000-0002-9007-936X
FU City University of Hong Kong [7005055]; National Natural Science
   Foundation of China [U1611461]; Natural Science Foundation of Guangdong
   Province [2016A030310422]; Fundamental Research Funds for the Central
   Universities [2018ZD33]; Hong Kong PhD Fellowship Scheme
FX This work was supported in part by the City University of Hong Kong
   (Project 7005055), in part by the National Natural Science Foundation of
   China (Project U1611461), in part by the Natural Science Foundation of
   Guangdong Province (Project 2016A030310422), and in part by the
   Fundamental Research Funds for the Central Universities (Project
   2018ZD33). The work of Mohamed Azzam was supported by the Hong Kong PhD
   Fellowship Scheme. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. David Crandall.
   (Corresponding author: Si Wu.)
CR [Anonymous], 2016, ARXIV E PRINTS
   [Anonymous], 2015, P INT C LEARNING REP
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bao JM, 2017, IEEE I CONF COMP VIS, P2764, DOI 10.1109/ICCV.2017.299
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Benaim Sagie, 2017, Advances in neural information processing systems, P752
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Brock A., 2019, INT C LEARN REPR
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Cicek S, 2019, IEEE I CONF COMP VIS, P1416, DOI 10.1109/ICCV.2019.00150
   Donahue J., 2016, ARXIV160509782
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   French Geoffrey, 2017, ARXIV170605208
   Gan Z, 2017, 31 ANN C NEURAL INFO, V30
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grandvalet Y., 2005, CAP, V367, P281
   Harwath David, 2016, Advances in Neural Information Processing Systems, P1858
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Hoffman J., 2017, CORR
   Huang H., 2018, ADV NEURAL INFORM PR, P52
   Gulrajani I, 2017, ADV NEUR IN, V30
   Karras T., 2018, INT C LEARN REPR, P1, Patent No. 171010196
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2013, ARXIV13126114
   Kumar A, 2018, ADV NEUR IN, V31
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2019, PR MACH LEARN RES, V97
   Liu Ming Yu, 2016, ADV NEURAL INF PROCE, P469
   Liu MY, 2017, ADV NEUR IN, V30
   Lucas J, 2019, ADV NEUR IN, V32
   Lyu F, 2019, IEEE T MULTIMEDIA, V21, P1971, DOI 10.1109/TMM.2019.2894964
   Ma X., 2019, INT C LEARN REPR
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Makhzani A., 2015, ARXIV
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, Proceedings of the 6th International Conference on Learning Representations, P1
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Radford A., 2015, ARXIV151106434
   Redmon J., 2016, P IEEE C COMP VIS PA, P779, DOI DOI 10.1109/CVPR.2016.91
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shu R., 2018, P 6 INT C LEARN REPR
   Sohn K., 2015, Neural Information Processing Systems, P3483
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang Hongyi, 2017, ARXIV171009412
   Zhao Han, 2019, P 36 INT C MACH LEAR
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 60
TC 5
Z9 5
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3318
EP 3331
DI 10.1109/TMM.2020.3023792
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000029
DA 2024-07-18
ER

PT J
AU He, PS
   Li, HL
   Wang, HX
   Wang, SQ
   Jiang, XH
   Zhang, RM
AF He, Peisong
   Li, Haoliang
   Wang, Hongxia
   Wang, Shiqi
   Jiang, Xinghao
   Zhang, Ruimei
TI Frame-Wise Detection of Double HEVC Compression by Learning Deep
   Spatio-Temporal Representations in Compression Domain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Encoding; Standards; Feature extraction; Neural networks; Bit
   rate; Forensics; Coding information map; double HEVC compression; hybrid
   neural network; spatio-temporal representation; video forensics
ID FORENSICS; DECISION
AB Detection of double compression is regarded as one primary step in analyzing the integrity of digital videos, which is of prominent importance in video forensics. However, current methods are vulnerable with the severe lossy quantization in the recompression process such that it is challenging to obtain reliable frame-wise detection results, especially for the high efficiency video coding (HEVC) standard. In view of these issues, in this paper, a hybrid neural network is proposed to reveal abnormal frames in HEVC videos with double compression by learning robust spatio-temporal representations from coding information in the compression domain. Based on the statistical analysis of Coding Units (CUs), it is interesting to find that HEVC video streams contain "rich" coding information that could be leveraged to identify abnormal traces caused by double compression. Two types of coding information maps, including CU Size Map (CSM) and CU Prediction mode Map (CPM), are exploited. In contrast with the conventional paradigm relying on pixel-level representations of decoded frames, CSMs and CPMs of a short-time video clip are treated as the input, aiming to achieve high robustness against recompression of low quality. In our hybrid neural network, an attention-based two-stream residual network is proposed to learn hierarchical representations from CSM and CPM, which are then jointly optimized by the attention-based fusion module. Finally, the temporal variation is modeled by Long Short-Term Memory (LSTM) to obtain frame-wise detection results. We have conducted extensive experiments considering various video content and coding parameters, such as bitrates and sizes of Group of Picture. Experimental results show that our approach can obtain state-of-the-art performance compared with conventional methods, especially when videos are recompressed in the low bitrate coding scenarios.
C1 [He, Peisong; Wang, Hongxia; Zhang, Ruimei] Sichuan Univ, Coll Cybersecur, Chengdu 610065, Peoples R China.
   [Li, Haoliang] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Li, Haoliang] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Shenzhen Inst, Shenzhen 518057, Peoples R China.
   [Jiang, Xinghao] Shanghai Jiao Tong Univ, Dept Cyber Sci & Engn, Shanghai 200240, Peoples R China.
C3 Sichuan University; Nanyang Technological University; City University of
   Hong Kong; City University of Hong Kong; Shanghai Jiao Tong University
RP Wang, SQ (corresponding author), City Univ Hong Kong, Shenzhen Inst, Shenzhen 518057, Peoples R China.
EM gokeyhps@scu.edu.cn; hxwang@scu.edu.cn; shiqwang@cityu.edu.hk;
   xhjiang@sjtu.edu.cn; ruimeizhang163@163.com
RI He, Peisong/AAE-2082-2022; Wang, Hongxia/AAE-2135-2022
OI Li, Haoliang/0000-0002-8723-8112; He, Peisong/0000-0003-3121-0599
FU National Natural Science Foundation of China [61902263, 61972269]; China
   Postdoctoral Science Foundation [2020M673276]; Fundamental Research
   Funds for the Central Universities [2020SCU12066, YJ201881]; Science,
   Technology, and Innovation Commission of Shenzhen Municipality
   [JCYJ20180307123934031]; Science and Technology Foundation of Guangzhou
   Huangpu Development District [2017GH22]; Sino-Singapore International
   Joint Research Institute Project [206-A017023]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61902263 and 61972269, in part by the
   China Postdoctoral Science Foundation (2020M673276), in part by the
   Fundamental Research Funds for the Central Universities (2020SCU12066,
   YJ201881), in part by by the Science, Technology, and Innovation
   Commission of Shenzhen Municipality under Project JCYJ20180307123934031,
   in part by the Science and Technology Foundation of Guangzhou Huangpu
   Development District under Grant 2017GH22, and in part by Sino-Singapore
   International Joint Research Institute Project under Grant 206-A017023.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Raouf Hamzaoui.
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Bian S, 2017, PROCEEDINGS OF 2017 INTERNATIONAL CONFERENCE ON VIDEO AND IMAGE PROCESSING (ICVIP 2017), P44, DOI 10.1145/3177404.3177420
   Bian S, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11010067
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Chen W, 2009, LECT NOTES COMPUT SC, V5450, P16, DOI 10.1007/978-3-642-04438-0_2
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Cruz G, 2019, IEEE T GEOSCI REMOTE, V57, P6565, DOI 10.1109/TGRS.2019.2907277
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fang Q, 2019, P 12 INT C IM SIGN P, P1
   Gulli A., 2017, DEEP LEARNING KERAS, P45
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P., 2019, IEEE T CIRC SYST VID, P1
   He PS, 2017, J VIS COMMUN IMAGE R, V48, P149, DOI 10.1016/j.jvcir.2017.06.010
   He PS, 2017, NEUROCOMPUTING, V228, P84, DOI 10.1016/j.neucom.2016.09.084
   He PS, 2016, J VIS COMMUN IMAGE R, V35, P55, DOI 10.1016/j.jvcir.2015.11.014
   He PS, 2015, LECT NOTES ARTIF INT, V9227, P787, DOI 10.1007/978-3-319-22053-6_84
   Huang M, 2015, P INT WORKSH DIG WAT, P61
   Jiang XH, 2020, IEEE T INF FOREN SEC, V15, P250, DOI 10.1109/TIFS.2019.2918085
   Jiang XH, 2019, IEEE ACCESS, V7, P95364, DOI 10.1109/ACCESS.2019.2928857
   Jiang XH, 2013, IEEE SIGNAL PROC LET, V20, P447, DOI 10.1109/LSP.2013.2251632
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Kingma D. P., 2015, INT C LEARNING REPRE
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li Q, 2019, IET INFORM SECUR, V13, P1, DOI 10.1049/iet-ifs.2017.0555
   Li ZH, 2017, ITM WEB CONF, V12, DOI 10.1051/itmconf/20171201020
   Nam SH, 2019, IEEE IMAGE PROC, P111, DOI [10.1109/ICIP.2019.8802966, 10.1109/icip.2019.8802966]
   Niu Y, 2019, IEEE SIGNAL PROC LET, V26, P1907, DOI 10.1109/LSP.2019.2953953
   Ravi H, 2014, IEEE IMAGE PROC, P5352, DOI 10.1109/ICIP.2014.7026083
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Stamm MC, 2012, IEEE T INF FOREN SEC, V7, P1315, DOI 10.1109/TIFS.2012.2205568
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun TF, 2012, INT CONF ACOUST SPEE, P1389, DOI 10.1109/ICASSP.2012.6288150
   Vázquez-Padín D, 2012, IEEE INT WORKS INFOR, P151, DOI 10.1109/WIFS.2012.6412641
   Vazquez-Padin D., 2019, IEEE T INF FORENSICS
   Wang J, 2020, IEEE-ACM T AUDIO SPE, V28, P581, DOI 10.1109/TASLP.2019.2959251
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu Q., 2019, P 12 INT C IM SIGN P, P1
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu QY, 2017, LECT NOTES COMPUT SC, V10431, P3, DOI 10.1007/978-3-319-64185-0_1
   Yu LF, 2019, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-019-0468-x
   Zhang ZZ, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11111343
NR 44
TC 14
Z9 14
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3179
EP 3192
DI 10.1109/TMM.2020.3021234
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000018
DA 2024-07-18
ER

PT J
AU Katsenou, AV
   Dimitrov, G
   Ma, D
   Bull, DR
AF Katsenou, Angeliki V.
   Dimitrov, Goce
   Ma, Di
   Bull, David R.
TI BVI-SynTex: A Synthetic Video Texture Dataset for Video Compression and
   Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Quality assessment; Cameras; Encoding; Spatial
   resolution; Video compression; Video recording; Video texture; CGI video
   dataset; video content analysis and compression; HEVC
ID DATABASE
AB Highly textured video content is challenging to compress since the bit-rate to video quality trade-off is high and complex perceptual masking influences performance. Test datasets that cover a wide range of texture types are thus important for codec evaluation, but few exist. In order to study the properties of video texture, this paper introduces a Synthetic video Texture dataset (BVI-SynTex) that was generated using a Computer-Generated Imagery (CGI) environment. It contains 196 sequences clustered in three different texture types and offers the capability of being able to generate many versions of the same scene with different video parameters. It therefore provides a flexible basis for studying the influence of texture type and parameters on video compression and perceived video quality. A thorough validation and comparison of BVI-SynTex with similarly textured natural video content is performed. The comparisons show that BVI-SynTex exhibits a comparable coverage over the spatial and temporal domain and that it produces similar encoding statistics to real video datasets. A subset of the BVI-SynTex dataset was selected to perform a subjective evaluation of compression using the MPEG HEVC codec.The results show the impact of the content parameters to both the compression efficiency and the perceived quality. The publicly available BVI-SynTex dataset contains all source sequences, the objective and subjective analysis results, providing a valuable resource for the research community.
C1 [Katsenou, Angeliki V.; Dimitrov, Goce; Ma, Di; Bull, David R.] Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
C3 University of Bristol
RP Katsenou, AV (corresponding author), Univ Bristol, Dept Elect & Elect Engn, Bristol BS8 1UB, Avon, England.
EM angeliki.katsenou@bristol.ac.uk; gd14470@bristol.ac.uk;
   di.ma@bristol.ac.uk; dave.bull@bristol.ac.uk
OI Bull, David/0000-0001-7634-190X
FU Leverhulme Early Career Fellowship [ECF-2017-413]; Engineering and
   Physical Sciences Research Council (EPSRC) [EP/L016656/1, EP/M000885/1];
   EPSRC [EP/M000885/1] Funding Source: UKRI
FX This work was supported in part by the Leverhulme Early Career
   Fellowship ECF-2017-413 and in part by the Engineering and Physical
   Sciences Research Council (EPSRC), under Grants EP/L016656/1 and
   EP/M000885/1.
CR Afonso M., 2016, PROC PICTURE CODING
   [Anonymous], 2018, UNREAL ENGINE UE4
   [Anonymous], 2017, CISCO VISUAL NETWORK
   [Anonymous], 2012, TECH REP
   [Anonymous], 2017, JCTVCAC1100
   [Anonymous], 2015, High Efficiency Video Coding: coding tools and specifications
   Barman N, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P7, DOI 10.1145/3210424.3210434
   Bull DR, 2014, COMMUNICATING PICTURES: A COURSE IN IMAGE AND VIDEO CODING, P1
   Butterworth B., 2013, BBC INTERNET BLOG
   Chen D, 2019, INT CONF ACOUST SPEE, P1622, DOI [10.1109/ICASSP.2019.8682641, 10.1109/icassp.2019.8682641]
   Cho S, 2013, IEEE T CIRC SYST VID, V23, P1555, DOI 10.1109/TCSVT.2013.2249017
   Chubach O, 2018, PICT COD SYMP, P218, DOI 10.1109/PCS.2018.8456271
   Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261
   Dimirov G., 2019, SVQA SUBJECTIVE VIDE
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Ghadiyaram D, 2019, IEEE T CIRC SYST VID, V29, P183, DOI 10.1109/TCSVT.2017.2768542
   ITU-T, 2012, TECH REP
   ITU-T, 2008, TECH REP
   Katsenou A. V., 2016, VIL HOMOGENEOUS VIDE
   Katsenou A. V., 2019, BVI SYNTHETIC VIDEO
   Katsenou AV, 2017, IEEE INT WORKSH MULT
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Li Z., NETFLIX TECH BLOG PR
   Ma D, 2019, IEEE IMAGE PROC, P1094, DOI [10.1109/icip.2019.8803798, 10.1109/ICIP.2019.8803798]
   Mackin A, 2019, IEEE T MULTIMEDIA, V21, P1499, DOI 10.1109/TMM.2018.2880603
   Mackin A, 2015, IEEE IMAGE PROC, P3407, DOI 10.1109/ICIP.2015.7351436
   Naser K, 2016, IEEE IMAGE PROC, P3787, DOI 10.1109/ICIP.2016.7533068
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Papadopoulos MA, 2017, IEEE IMAGE PROC, P2741, DOI 10.1109/ICIP.2017.8296781
   Papadopoulos MA, 2015, IEEE IMAGE PROC, P2781, DOI 10.1109/ICIP.2015.7351309
   Pechard Stephane, 2008, P INT WORKSH IM MED
   Pendit UC, 2017, PROC INT CONF INF TE, P160
   Péteri R, 2010, PATTERN RECOGN LETT, V31, P1627, DOI 10.1016/j.patrec.2010.05.009
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Ponomarenko N., 2007, INT WORKSH VID PROC
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Simone F. D., 2009, EPFL POLIMI VIDEO QU
   Song L, 2013, INT WORK QUAL MULTIM, P34, DOI 10.1109/QoMEX.2013.6603201
   Tian GF, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P405, DOI 10.1109/PCS.2012.6213317
   V. Q. E. Group, 2000, TECH REP
   V. Q. E. Group, 2010, TECH REP
   Vu P. V., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2505, DOI 10.1109/ICIP.2011.6116171
   Wandt B, 2018, PICT COD SYMP, P144, DOI 10.1109/PCS.2018.8456248
   Wang Y, 2016, IEEE IMAGE PROC, P539, DOI 10.1109/ICIP.2016.7532415
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2018, IEEE DATA COMPR CONF, P431, DOI 10.1109/DCC.2018.00084
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
   Zhang F, 2018, IEEE T MULTIMEDIA, V20, P2620, DOI 10.1109/TMM.2018.2817070
   Zhang F, 2011, IEEE J-STSP, V5, P1378, DOI 10.1109/JSTSP.2011.2165201
   Zhang FJ, 2015, PROC TECH, V20, P249, DOI 10.1016/j.protcy.2015.07.040
   Zhang Y, 2018, INT CONF 3D VISION, P228, DOI 10.1109/3DV.2018.00035
NR 56
TC 9
Z9 9
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 26
EP 38
DI 10.1109/TMM.2020.2976591
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, JC
   Yan, B
   Lin, Q
   Li, A
   Ma, CX
AF Li, Jichun
   Yan, Bo
   Lin, Qing
   Li, Ang
   Ma, Chenxi
TI Motion Blur Removal With Quality Assessment Guidance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Image restoration; Measurement; Kernel; Optimization;
   Quality assessment; Convolution; Deep learning; image quality assessment
   (IQA); low-level vision; single image deblurring; task-driven
AB Non-uniform blind motion deblurring is a challenging yet fundamental task in the computer vision field, which aims to restore the latent sharp image from the blurry input. Recently, deep-learning-based methods have made significant improvement and progress, on the metric of PSNR. They achieve good results mainly because they adopt Mean Squared Error (MSE) as the optimization objective, in addition to their good model design. However, simple adoption of the PSNR metric and the MSE loss, has non-ignorable disadvantages. PSNR cannot always succeed in assessing the deblurred quality in accordance with the human visual system (HVS), and MSE guides the network to generate over-smoothed images. To address these problems, we are the first to propose the deep-learning-based multi-scale non-reference quality assessment network (Deep DEBLUR-IQA) for assessing the quality of deblurred results. Moreover, a deblurring network of high efficiency is presented. It is more than 50 times faster than other SOTA multi-scale Convolution Neural Network (CNN) methods, with the newly propose Residual Dilated Block (RDB) and Light ResBlock (LRB). The deblurring network's performance can be further boosted with Multiple Dilation Block (MDB), with an acceptable speed decrease. Finally, and most importantly, we are the first to let Deep DEBLUR-IQA guide the deblurring network's optimization. This IQA-guided enhancement paradigm can significantly improve the deblurring results' subjective quality while achieving excellent PSNR. Experimental results demonstrate that the proposed method performs favorably against state-of-the-art methods quantitatively and qualitatively.
C1 [Li, Jichun; Yan, Bo; Lin, Qing; Li, Ang; Ma, Chenxi] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270; Li, Jichun/0000-0003-4906-8244; ,
   chenxi/0000-0002-5577-5773; Lin, Qing/0000-0002-3808-3492
FU NSFC [U2001209, 61772137]
FX This work was supported by NSFC under Grants U2001209 and 61772137. The
   Guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. J. Zhang.
CR Bare B, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1223, DOI 10.1109/ICASSP.2018.8461931
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang YM, 2018, MULTIMED TOOLS APPL, V77, P29829, DOI 10.1007/s11042-018-5805-z
   Gao HY, 2019, PROC CVPR IEEE, P3843, DOI 10.1109/CVPR.2019.00397
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Lai WS, 2016, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2016.188
   Li A, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102899
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Liu YM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508391
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Pan JS, 2016, PROC CVPR IEEE, P1628, DOI 10.1109/CVPR.2016.180
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun J, 2015, PROC CVPR IEEE, P769, DOI 10.1109/CVPR.2015.7298677
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu L, 2013, PROC CVPR IEEE, P1107, DOI 10.1109/CVPR.2013.147
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
NR 24
TC 9
Z9 9
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2986
EP 2997
DI 10.1109/TMM.2021.3068561
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000003
DA 2024-07-18
ER

PT J
AU Nacakli, S
   Tekalp, AM
AF Nacakli, Selin
   Tekalp, A. Murat
TI Controlling P2P-CDN Live Streaming Services at SDN-Enabled Multi-Access
   Edge Datacenters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Peer-to-peer computing; Servers; Computer architecture; Over-the-top
   media services; Cloud computing; Software; Content distribution networks
   (CDN); cross network-service provider traffic; managed peer-to-peer
   (P2P) service; multi-access edge datacenters; software-defined network
   (SDN); video server load; video service quality
ID VIDEO
AB Recognizing the shortcomings of current hybrid peer-to-peer (P2P) content-distribution network (CDN) video solutions and the potential of emerging multi-access edge datacenters, we propose a novel P2P-CDN service model that is hosted at software defined networks (SDN)-enabled multi-access edge datacenters operated by network service providers (NSP). An important feature of the proposed service architecture is that both CDN access by peers and P2P video streaming between peers within edge access networks are fully controlled by cooperation of the video content provider (VCP) and NSP to optimize video service key performance indicators (KPI). The proposed fully controlled P2P-CDN architecture with P2P group formation and chunk scheduling managed at edge datacenters reduces the load on CDN servers while overcoming quality of experience (QoE) fluctuations per flow and unfairness between multiple heterogeneous video-resolution clients over reserved access network slices. Other advantages of this service include: i) better video quality and lower delay for clients; ii) better use of edge network resources; iii) avoiding illegal, unauthorized P2P content sharing. To the best of our knowledge, there are no solutions in the literature that address P2P-CDN services managed at NSP-edge datacenters combining P2P-assisted CDN, SDN-assisted edge computing, and premium service over reserved slices. Experimental results show that the proposed P2P-CDN service deployed at SDN-enabled edge datacenters provides excellent service KPI compared to other state-of-the-art solutions.
C1 [Nacakli, Selin; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Tekalp, AM (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM seyilmaz@ku.edu.tr; mtekalp@ku.edu.tr
RI Nacakli, Selin/AAD-3477-2021
OI Nacakli, Selin/0000-0002-6022-8192
FU TUBITAK Project [115E299]
FX Manuscript received June 23, 2020; revised August 23, 2020; accepted
   October 12, 2020. Date of publication October 19, 2020; date of current
   version October 19, 2021. This work was supported by TUBITAK Project
   115E299. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Chonggang Wang. (Corresponding
   author: A. Murat Tekalp.)
CR Alimi R, 2014, RFC 7285
   Antony I., 2001, LECT NOTES COMPUTER, P329, DOI DOI 10.1007/3-540-45518-3_18
   Arthur D., 2006, Proceedings of the Twenty-Second Annual Symposium on Computational Geometry (SCG'06), P144, DOI 10.1145/1137856.1137880
   Bagci KT, 2018, IEEE T MULTIMEDIA, V20, P3084, DOI 10.1109/TMM.2018.2823907
   Bagci KT, 2016, 2016 IEEE SIXTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS (ICCE), P33, DOI 10.1109/CCE.2016.7562609
   Bagci KT, 2017, IEEE T MULTIMEDIA, V19, P2152, DOI 10.1109/TMM.2017.2736638
   Cha M., 2008, P 7 INT C PEER TO PE, P5
   Datta S, 2009, IEEE T KNOWL DATA EN, V21, P1372, DOI 10.1109/TKDE.2008.222
   Huang R., 2014, SURVEY WEBRTC BASED
   ITEC, 2020, DYN AD STREAM HTTP D
   James S., 2010, P IEEE 10 INT C PEER, P1
   Jeon S, 2009, LECT NOTES COMPUT SC, V5764, P229, DOI 10.1007/978-3-642-04190-7_21
   Kirmizioglu RA, 2020, IEEE T MULTIMEDIA, V22, P1005, DOI 10.1109/TMM.2019.2937170
   Longwell A, 2018, IHS MARKIT 85 OPERAT
   Maymounkov P, 2002, LECT NOTES COMPUT SC, V2429, P53
   Nogueira Barbosa F. R., 2014, S BRAS RED COMP
   ONF, CORD REINV CENTR OFF
   ONF, 2019, SEBA SDN EANBL BROAD
   Peterson L, 2019, ACM SIGCOMM COMP COM, V49, P31, DOI 10.1145/3336937.3336942
   Sahin KE, 2017, INT CONF NETW SER
   Seif G., 2018, TOWARDSDATASCIENCE
   Shen HY, 2015, IEEE T PARALL DISTR, V26, P1509, DOI 10.1109/TPDS.2014.2327033
   Shibasaki R., 2016, P 10 INT C DIG SOC E
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Streamroot, 2020, DISC CDN MESH DEL LI
   Thinh Nguyen Kim, 2011, 2011 IEEE 15th International Symposium on Consumer Electronics, P438, DOI 10.1109/ISCE.2011.5973865
   Trajkovska Irena, 2014, 2014 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-TW), P33, DOI 10.1109/ICCE-TW.2014.6904089
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Xie HY, 2008, ACM SIGCOMM COMP COM, V38, P351, DOI 10.1145/1402946.1402999
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Zhang M., 2005, Proc. ACM Workshop on Advances in Peer-to-Peer Multimedia Streaming (P2PMMS), P21
   Zhihui Lu, 2012, Journal of Communications, V7, P232, DOI 10.4304/jcm.7.3.232-245
NR 32
TC 12
Z9 13
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3805
EP 3816
DI 10.1109/TMM.2020.3032042
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100029
DA 2024-07-18
ER

PT J
AU Wang, LC
   Li, S
   Wang, SF
   Kong, DH
   Yin, BC
AF Wang, Lichun
   Li, Shuang
   Wang, Shaofan
   Kong, Dehui
   Yin, Baocai
TI Hardness-Aware Dictionary Learning: Boosting Dictionary for Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionaries; Training; Boosting; Task analysis; Face recognition;
   Visualization; Sparse representation; classification; AdaBoost;
   dictionary learning
ID DISCRIMINATIVE DICTIONARY; FACE RECOGNITION; SPARSE REPRESENTATION;
   CONVOLUTIONAL SPARSE; K-SVD; IMAGE; CLASSIFICATION; ALGORITHM; ADABOOST;
   MODELS
AB Sparse representation is a powerful tool in many visual applications since images can be represented effectively and efficiently with a dictionary. Conventional dictionary learning methods usually treat each training sample equally, which would lead to the degradation of recognition performance when the samples from same category distribute dispersedly. This is because the dictionary focuses more on easy samples (known as highly clustered samples), and those hard samples (known as widely distributed samples) are easily ignored. As a result, the test samples which exhibit high dissimilarities to most of intra-category samples tend to be misclassified. To circumvent this issue, this paper proposes a simple and effective hardness-aware dictionary learning (HADL) method, which considers training samples discriminatively based on the AdaBoost mechanism. Different from learning one optimal dictionary, HADL learns a set of dictionaries and corresponding sub-classifiers jointly in an iterative fashion. In each iteration, HADL learns a dictionary and a sub-classifier, and updates the weights based on the classification errors given by current sub-classifier. Those correctly classified samples are assigned with small weights while those incorrectly classified samples are assigned with large weights. Through the iterated learning procedure, the hard samples are associated with different dictionaries. Finally, HADL combines the learned sub-classifiers linearly to form a strong classifier, which improves the overall recognition accuracy effectively. Experiments on well-known benchmarks show that HADL achieves promising classification results.
C1 [Wang, Lichun; Li, Shuang; Wang, Shaofan; Kong, Dehui; Yin, Baocai] Beijing Univ Technol, Fac Informat Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Wang, SF (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM wanglc@bjut.edu.cn; shuangli@emails.bjut.edu.cn;
   wangshaofan@bjut.edu.cn; kdh@bjut.edu.cn; ybc@bjut.edu.cn
OI WANG, SHAOFAN/0000-0002-3045-624X; Li, Shuang/0000-0003-2792-8303
FU National Natural Science Foundation of China [61632006, 61876012,
   61772049]; Beijing Natural Science Foundation [4202003]; Beijing
   Outstanding Young Scientists Projects [BJJWZYJH01201910005018]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61632006, 61876012, and 61772049, in
   part by the Beijing Natural Science Foundation under Grant 4202003, and
   in part by the Beijing Outstanding Young Scientists Projects under Grant
   BJJWZYJH01201910005018.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Akhtar N, 2017, PROC CVPR IEEE, P3919, DOI 10.1109/CVPR.2017.417
   Bach F, 2012, STAT SCI, V27, P450, DOI 10.1214/12-STS394
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bo LF, 2013, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2013.91
   Cai SJ, 2014, LECT NOTES COMPUT SC, V8692, P624, DOI 10.1007/978-3-319-10593-2_41
   Cao Wanpeng, 2018, Computer Engineering and Applications, V54, P132, DOI 10.3778/j.issn.1002-8331.1610-0308
   Chen YC, 2012, LECT NOTES COMPUT SC, V7577, P766, DOI 10.1007/978-3-642-33783-3_55
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Feng QX, 2016, IEEE T MULTIMEDIA, V18, P1956, DOI 10.1109/TMM.2016.2602062
   FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Guha T, 2012, IEEE T PATTERN ANAL, V34, P1576, DOI 10.1109/TPAMI.2011.253
   Huang Y, 2019, IEEE SIGNAL PROC LET, V26, P327, DOI 10.1109/LSP.2018.2890765
   Ji SH, 2008, IEEE T SIGNAL PROCES, V56, P2346, DOI 10.1109/TSP.2007.914345
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Jiang ZL, 2013, IEEE T PATTERN ANAL, V35, P2651, DOI 10.1109/TPAMI.2013.88
   Kearns M., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P433, DOI 10.1145/73007.73049
   Kearns M., 2018, TR1488 HARVARD U AIK, P1
   Kong S, 2012, LECT NOTES COMPUT SC, V7572, P186, DOI 10.1007/978-3-642-33718-5_14
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Lee J, 2010, PROCEEDINGS OF THE 17TH INTERNATIONAL CONGRESS ON SOUND AND VIBRATION
   Lei Zhang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1237, DOI 10.1109/ICPR.2010.308
   Li AH, 2018, J AM STAT ASSOC, V113, P660, DOI 10.1080/01621459.2016.1273116
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li MH, 2018, PROC CVPR IEEE, P6644, DOI 10.1109/CVPR.2018.00695
   Li XC, 2008, ENG APPL ARTIF INTEL, V21, P785, DOI 10.1016/j.engappai.2007.07.001
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977
   Mahdizadehaghdam S, 2019, IEEE T IMAGE PROCESS, V28, P4790, DOI 10.1109/TIP.2019.2914376
   Marcellin M. W., 2000, Proceedings DCC 2000. Data Compression Conference, P523, DOI 10.1109/DCC.2000.838192
   Nejati M, 2016, IEEE T IMAGE PROCESS, V25, P4900, DOI 10.1109/TIP.2016.2598483
   Quan YH, 2016, PROC CVPR IEEE, P5839, DOI 10.1109/CVPR.2016.629
   Quan YH, 2016, PATTERN RECOGN, V55, P247, DOI 10.1016/j.patcog.2016.01.028
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Ramzi P, 2014, IEEE J-STARS, V7, P2066, DOI 10.1109/JSTARS.2013.2292901
   Ren J, 2013, IEEE T IMAGE PROCESS, V22, P1454, DOI 10.1109/TIP.2012.2231690
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760
   Shen L, 2013, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2013.56
   Song JQ, 2019, PATTERN RECOGN, V91, P135, DOI 10.1016/j.patcog.2019.02.018
   Sun B, 2016, KNOWL-BASED SYST, V102, P87, DOI 10.1016/j.knosys.2016.03.024
   Sun SL, 2018, LECT NOTES COMPUT SC, V10862, P590, DOI 10.1007/978-3-319-93713-7_55
   Theodorakopoulos I, 2014, J VIS COMMUN IMAGE R, V25, P12, DOI 10.1016/j.jvcir.2013.03.008
   Toka O, 2019, SOFT COMPUT, V23, P7081, DOI 10.1007/s00500-018-3350-3
   Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiao Z, 2018, IEEE T NEUR NET LEAR, V29, P3069, DOI 10.1109/TNNLS.2017.2711028
   Xing HJ, 2020, INFORM FUSION, V55, P45, DOI 10.1016/j.inffus.2019.08.002
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2018, IEEE T IMAGE PROCESS, V27, P2121, DOI 10.1109/TIP.2017.2786469
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhang SZ, 2017, PATTERN RECOGN, V64, P130, DOI 10.1016/j.patcog.2016.10.032
   Zhang W., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1241
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu F, 2016, IEEE INTELL SYST, V31, P6, DOI 10.1109/MIS.2016.30
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 61
TC 7
Z9 7
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2857
EP 2867
DI 10.1109/TMM.2020.3017916
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600025
DA 2024-07-18
ER

PT J
AU Zhang, ML
   Ling, Q
AF Zhang, Menglei
   Ling, Qiang
TI Supervised Pixel-Wise GAN for Face Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face image super-resolution; supervised; generative adversarial nets
   (GAN); face recognition; pixel-wise GAN
ID RECOGNITION; RESOLUTION
AB For many face-related multimedia applications, low-resolution face images may greatly degrade the face recognition performance and necessitate face super-resolution (SR). Among the current SR methods, MSE-oriented SR methods often produce over-smoothed outputs and could miss some texture details while GAN-oriented SR methods may generate artifacts which are harmful to face recognition. To resolve the above issues, this paper presents a supervised pixel-wise Generative Adversarial Network (SPGAN) that can resolve a very low-resolution face image of 16 x 16 or smaller pixel-size to its larger version of multiple scaling factors (2x, 4x, 8x and even 16x) in a unified framework. Being different from traditional unsupervised discriminators which generate a single number to represent the likelihood whether the input image is real or fake, the proposed supervised pixel-wise discriminator mainly focus on whether each pixel of the generated SR face image is as photo-realistic as its corresponding pixel in the ground-truth HR (high-resolution) face image. To further improve the face recognition performance of SPGAN, we take advantage of the face identity prior by sending two inputs to the discriminator, including an input face image (either a real HR face image or its corresponding SR face image) and its face features which are extracted from a pre-trained face recognition model. Due to the introduced face identity prior, the identity-based discriminator can pay more attention to texture details which are closely related to face recognition. Extensive experiments demonstrate that the proposed SPGAN can achieve more photo-realistic SRimages and higher face recognition accuracy than some state-of-the-art methods.
C1 [Zhang, Menglei; Ling, Qiang] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Ling, Q (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
EM zml123@mail.ustc.edu.cn; qling@ustc.edu.cn
OI Ling, Qiang/0000-0001-5688-4130
FU Technological Innovation Project for New Energy and Intelligent
   Networked Automobile Industry of Anhui Province; National Key Research
   and Development Program of China [2016YFC0201003]
FX Manuscript received October 10, 2019; revised January 17, 2020 and April
   8, 2020; accepted June 22, 2020. Date of publication July 2, 2020; date
   of current version June 25, 2021. This work was supported in part by the
   Technological Innovation Project for New Energy and Intelligent
   Networked Automobile Industry of Anhui Province and in part by the
   National Key Research and Development Program of China under Grant
   2016YFC0201003. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Fatih Porikli.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bai YC, 2018, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2018.00010
   Brock Andrew, 2018, ARXIV180911096
   Bulat A, 2018, PROC CVPR IEEE, P109, DOI 10.1109/CVPR.2018.00019
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen S, 2018, LECT NOTES COMPUT SC, V10996, P428, DOI 10.1007/978-3-319-97909-0_46
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fookes C, 2012, J VIS COMMUN IMAGE R, V23, P75, DOI 10.1016/j.jvcir.2011.06.004
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grm K, 2020, IEEE T IMAGE PROCESS, V29, P2150, DOI 10.1109/TIP.2019.2945835
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Miyato T, 2018, INT C LEARN REPR
   Moschoglou S, 2017, IEEE COMPUT SOC CONF, P1997, DOI 10.1109/CVPRW.2017.250
   Mudunuri SP, 2016, IEEE T PATTERN ANAL, V38, P1034, DOI 10.1109/TPAMI.2015.2469282
   Peng Y., 2012, P 33 WIC S INF THEOR, P36
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Salimans T, 2016, ADV NEUR IN, V29
   Sengupta S, 2016, IEEE WINT CONF APPL
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh M., 2018, P IEEE C COMP VIS PA, P479
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang KP, 2018, LECT NOTES COMPUT SC, V11215, P196, DOI 10.1007/978-3-030-01252-6_12
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 46
TC 47
Z9 49
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1938
EP 1950
DI 10.1109/TMM.2020.3006414
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100009
DA 2024-07-18
ER

PT J
AU Zheng, S
   Chen, J
   Zhang, XP
   Kuo, YH
AF Zheng, Shuai
   Chen, Jian
   Zhang, Xiao-Ping
   Kuo, Yonghong
TI A New Multihypothesis-Based Compressed Video Sensing Reconstruction
   System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Microsoft Windows; Prediction algorithms; Decoding; Image
   reconstruction; Encoding; Sensors; Optimal matching; Compressed sensing;
   hypotheses acquisition; residual transforming; weight prediction
ID FRAMEWORK
AB Multihypothesis-based compressed video sensing scheme attracts wide attention in the research of resource-constrained video application scenarios. However, high-accuracy weight prediction of hypotheses is always challenging especially for the high-motion sequences. To solve this problem, this paper proposes a novel multihypothesis-based distributed compressed video sensing (NMH-DCVS) system. The new multihypothesis system contains two components: hypotheses acquisition, and weight prediction. First, to acquire more high-quality hypotheses, a new hypotheses acquisition scheme is proposed by constructing the search window based on the temporal, and spatial correlation of the image blocks, respectively. The optimal matching block can be quickly determined. Second, to improve the accuracy of the multihypothesis weight prediction, a new residual transforming preprocessing-based weight prediction algorithm is proposed by transforming the original hypothesis set to residual hypothesis set. The influence of the quality fluctuation of the hypotheses on prediction accuracy is effectively suppressed. Moreover, the improved hypotheses further improve the sparsity of the residual hypothesis set, leading to the additional improvement of the accuracy of the proposed residual-based weight prediction algorithm. Experiment results show that compared with the state-of-the-art methods reported in the literature, the proposed new multihypothesis system significantly improves the decoding performance both in objective, and subjective quality.
C1 [Zheng, Shuai] China Elect Technol Grp Corp, Inst 20, Xian 710071, Peoples R China.
   [Chen, Jian; Kuo, Yonghong] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 China Electronics Technology Group; Xidian University; Toronto
   Metropolitan University
RP Zhang, XP (corresponding author), Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
EM zhs_xd@163.com; jianchen@mail.xidian.edu.cn; xzhang@ee.ryerson.ca;
   yhkuo@mail.xidian.edu.cn
RI KUO, Yong-Hong/M-9078-2015; Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069; Zheng,
   Shuai/0000-0002-2570-0105
FU National Natural Science Foundation of China (NSFC) [61771366]; "111"
   Project [B08038]; Natural Sciences, and Engineering Research Council of
   Canada (NSERC) [RGPIN-2020-04661]; Fundamental Research Funds for the
   Central Universities; Innovation Fund of Xidian University
   [5001-20109195456]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61771366, "111" Project under
   Grant B08038, in part by the Natural Sciences, and Engineering Research
   Council of Canada (NSERC) under Grant RGPIN-2020-04661, in part by the
   Fundamental Research Funds for the Central Universities, and in part by
   the Innovation Fund of Xidian University under Grant 5001-20109195456.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zixiang Xiong. (Corresponding
   author: Xiao-Ping Zhang.)
CR Azghani M, 2016, IEEE T CIRC SYST VID, V26, P627, DOI 10.1109/TCSVT.2015.2418586
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Chen C, 2020, IEEE T CIRC SYST VID, V30, P1, DOI 10.1109/TCSVT.2018.2886310
   Chen C, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P520, DOI 10.1109/SIPROCESS.2018.8600454
   Chen C, 2018, IET IMAGE PROCESS, V12, P210, DOI 10.1049/iet-ipr.2017.0354
   Chen J, 2018, MULTIMED TOOLS APPL, V77, P14873, DOI 10.1007/s11042-017-5071-5
   Chen J, 2017, MULTIMED TOOLS APPL, V76, P15735, DOI 10.1007/s11042-016-3866-4
   Chen J, 2015, MULTIMED TOOLS APPL, V74, P2085, DOI 10.1007/s11042-013-1743-y
   Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   Dong W, 2013, ELECTRON LETT, V49, P184, DOI 10.1049/el.2012.2536
   Donoho DL, 2012, IEEE T INFORM THEORY, V58, P1094, DOI 10.1109/TIT.2011.2173241
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Duarte MF, 2008, IEEE SIGNAL PROC MAG, V25, P83, DOI 10.1109/MSP.2007.914730
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Fowler JE, 2010, FOUND TRENDS SIGNAL, V4, P297, DOI 10.1561/2000000033
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Girod B, 2005, P IEEE, V93, P71, DOI 10.1109/JPROC.2004.839619
   Guo J, 2016, IEEE T MULTIMEDIA, V18, P1297, DOI 10.1109/TMM.2016.2564100
   Hirokawa S, 2017, ASIAPAC SIGN INFO PR, P1701, DOI 10.1109/APSIPA.2017.8282307
   Hitomi Y, 2011, IEEE I CONF COMP VIS, P287, DOI 10.1109/ICCV.2011.6126254
   Hosseini MS, 2014, IEEE T IMAGE PROCESS, V23, P3869, DOI 10.1109/TIP.2014.2332755
   Kang LW, 2009, INT CONF ACOUST SPEE, P1169, DOI 10.1109/ICASSP.2009.4959797
   Khwaja AS, 2014, IEEE GEOSCI REMOTE S, V11, P1350, DOI 10.1109/LGRS.2013.2293475
   Kuo YH, 2017, MULTIDIM SYST SIGN P, V28, P129, DOI 10.1007/s11045-015-0337-4
   Lappalainen V, 2003, IEEE T CIRC SYST VID, V13, P717, DOI 10.1109/TCSVT.2003.814968
   Li CB, 2013, IEEE T BROADCAST, V59, P197, DOI 10.1109/TBC.2012.2226509
   Li Chengbo., 2009, Tval3: Tv minimization by augmented lagrangian and alternating direction algorithm
   Li J, 2018, IEEE J-STARS, V11, P4932, DOI 10.1109/JSTARS.2018.2879363
   Li ZJ, 2018, IEEE INT CONF COMMUN, P819, DOI 10.1109/ICCChina.2018.8641167
   Liu Q, 2020, IEEE T CLOUD COMPUT, V8, P1250, DOI 10.1109/TCC.2016.2630684
   Liu ZR, 2011, IEEE T CIRC SYST VID, V21, P1704, DOI 10.1109/TCSVT.2011.2133890
   Llull P, 2013, OPT EXPRESS, V21, P10526, DOI 10.1364/OE.21.010526
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Narayanan S, 2016, PROCEEDINGS OF THE 2016 IEEE REGION 10 CONFERENCE (TENCON), P993, DOI 10.1109/TENCON.2016.7848154
   Ou WF, 2016, IEEE IMAGE PROC, P2494, DOI 10.1109/ICIP.2016.7532808
   Pudlewski S, 2012, IEEE T MOBILE COMPUT, V11, P1060, DOI 10.1109/TMC.2011.175
   Rehman AU, 2016, INT WIREL COMMUN, P170, DOI 10.1109/IWCMC.2016.7577052
   Roohi S, 2013, IRAN CONF MACH, P53, DOI 10.1109/IranianMVIP.2013.6779949
   Sarwer Mohammed Golam, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3465, DOI 10.1109/ICIP.2011.6116459
   Sarwer M, 2013, SIGNAL IMAGE VIDEO P, V7, P777, DOI 10.1007/s11760-011-0267-z
   Shaban M, 2018, 2018 IEEE 4TH WORLD FORUM ON INTERNET OF THINGS (WF-IOT), P173, DOI 10.1109/WF-IoT.2018.8355095
   Song XD, 2017, IEEE T MULTIMEDIA, V19, P1351, DOI 10.1109/TMM.2017.2654123
   Takhar D, 2006, PROC SPIE, V6065, DOI 10.1117/12.659602
   Tramel EW, 2011, IEEE DATA COMPR CONF, P193, DOI 10.1109/DCC.2011.26
   Wan YL, 2014, IEEE T MULTIMEDIA, V16, P637, DOI 10.1109/TMM.2014.2299515
   Wang LJ, 2014, INT SYM COMPUT INTEL, P529, DOI 10.1109/ISCID.2014.73
   Xiao YH, 2012, J MATH IMAGING VIS, V44, P114, DOI 10.1007/s10851-011-0314-y
   Xiaoda Jiang, 2018, IEEE INFOCOM 2018 - IEEE Conference on Computer Communications, P1952, DOI 10.1109/INFOCOM.2018.8486377
   Xu J, 2018, J ELECTRON IMAGING, V27, DOI 10.1117/1.JEI.27.5.053042
   Yang Y, 2018, J NETW COMPUT APPL, V117, P72, DOI 10.1016/j.jnca.2018.05.018
   Zhang DY, 2018, IEEE 20TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS / IEEE 16TH INTERNATIONAL CONFERENCE ON SMART CITY / IEEE 4TH INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS), P1387, DOI 10.1109/HPCC/SmartCity/DSS.2018.00229
   Zhang DY, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P546, DOI 10.1109/ICIVC.2018.8492840
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhao C, 2017, IEEE T CIRC SYST VID, V27, P1182, DOI 10.1109/TCSVT.2016.2527181
   Zheng S, 2019, IEEE INT SYMP CIRC S
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
NR 60
TC 7
Z9 7
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3577
EP 3589
DI 10.1109/TMM.2020.3028479
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100012
DA 2024-07-18
ER

PT J
AU Zhou, GL
   Yan, Y
   Wang, DM
   Chen, QJ
AF Zhou, Guangliang
   Yan, Yi
   Wang, Deming
   Chen, Qijun
TI A Novel Depth and Color Feature Fusion Framework for 6D Object Pose
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Pose estimation; Image color analysis;
   Feature extraction; Color; Two dimensional displays; Deep learning;
   Object pose estimation; color and depth feature fusion; region-level
   feature
AB This paper aims to solve the problem of estimating the 6D pose of an object under occlusion using RGB-D images. Most existing methods typically use the information of color and depth images separately to make predictions, which limits their performances in the presence of occlusion. Instead, we propose a pipeline to effectively fuse color and depth information and perform region-level pose estimation. Our method first uses a CNN to extract the color features, and then we obtain the fusion features by combining the color features into the point cloud. Unlike existing methods, the fusion features are in the form of point sets instead of feature maps. We further use a PointNet++-like network to process the fusion features, obtaining several region-level features. Each region-level feature can predict a pose with confidence. The pose with the highest confidence is chosen as the final output. Experiments show that the proposed method outperforms the state-of-the-art methods on both the LINEMOD and Occlusion LINEMOD datasets, indicating that the proposed pipeline can obtain accurate pose estimation results and is robust to occlusion.
C1 [Zhou, Guangliang; Yan, Yi; Wang, Deming; Chen, Qijun] Tongji Univ, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
C3 Tongji University
RP Chen, QJ (corresponding author), Tongji Univ, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
EM 1910064@tongji.edu.cn; 1710822@tongji.edu.cn; wangdeming@tongji.edu.cn;
   qjchen@tongji.edu.cn
OI Wang, Deming/0000-0003-3486-4176; , Guangl/0000-0001-7845-7068
FU National Natural Science Foundation of China [61733013]
FX Thisworkwas supported by theNational Natural Science Foundation of China
   under Grant 61733013. The associate editor coordinating the review of
   this manuscript and approving it for publication was Engin Erzin.
CR Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Boukhayma A, 2019, PROC CVPR IEEE, P10835, DOI 10.1109/CVPR.2019.01110
   Brachmann E, 2016, PROC CVPR IEEE, P3364, DOI 10.1109/CVPR.2016.366
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Cai H., 2013, P 9 INT C COMPUTER V, P103
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dou YK, 2019, INT CONF ACOUST SPEE, P1982, DOI 10.1109/ICASSP.2019.8683356
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Ge LH, 2018, PROC CVPR IEEE, P8417, DOI 10.1109/CVPR.2018.00878
   Gu CH, 2010, LECT NOTES COMPUT SC, V6315, P408
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinterstoisser S, 2016, LECT NOTES COMPUT SC, V9907, P834, DOI 10.1007/978-3-319-46487-9_51
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005
   Hu YL, 2019, PROC CVPR IEEE, P3380, DOI 10.1109/CVPR.2019.00350
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kehl W, 2016, LECT NOTES COMPUT SC, V9907, P205, DOI 10.1007/978-3-319-46487-9_13
   Lepetit V, 2009, INT J COMPUT VISION, V81, P155, DOI 10.1007/s11263-008-0152-6
   Li C, 2018, LECT NOTES COMPUT SC, V11220, P263, DOI 10.1007/978-3-030-01270-0_16
   Li Y, 2018, LECT NOTES COMPUT SC, V11210, P695, DOI [10.1007/s11263-019-01250-9, 10.1007/978-3-030-01231-1_42]
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu YP, 2019, IEEE T MULTIMEDIA, V21, P2776, DOI 10.1109/TMM.2019.2913321
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MICHEL F, 2017, P IEEE C COMP VIS PA, P462, DOI DOI 10.1109/CVPR.2017.20
   Mousavian A, 2017, PROC CVPR IEEE, P5632, DOI 10.1109/CVPR.2017.597
   Oberweger M, 2018, LECT NOTES COMPUT SC, V11219, P125, DOI 10.1007/978-3-030-01267-0_8
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Pavlakos G., 2017, P IEEE C COMPUTER VI, P2011, DOI DOI 10.1109/CVPR.2017.139
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Qi C.R., 2017, ABS170602413 CORR, P5099
   Rad M, 2017, IEEE I CONF COMP VIS, P3848, DOI 10.1109/ICCV.2017.413
   Rios-Cabrera R, 2013, IEEE I CONF COMP VIS, P2048, DOI 10.1109/ICCV.2013.256
   Rothganger F, 2006, INT J COMPUT VISION, V66, P231, DOI 10.1007/s11263-005-3674-1
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30
   Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038
   Tulsiani S, 2015, PROC CVPR IEEE, P1510, DOI 10.1109/CVPR.2015.7298758
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Xiang Yu, 2017, ARXIV171100199
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   ZHANG ZY, 1994, INT J COMPUT VISION, V13, P119, DOI 10.1007/BF01427149
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu ML, 2014, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2014.6907430
NR 52
TC 15
Z9 18
U1 5
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1630
EP 1639
DI 10.1109/TMM.2020.3001533
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300012
DA 2024-07-18
ER

PT J
AU Sun, GM
   Shi, BF
   Chen, XD
   Krylov, AS
   Ding, Y
AF Sun, Guangming
   Shi, Bufan
   Chen, Xiaodong
   Krylov, Andrey S.
   Ding, Yong
TI Learning Local Quality-Aware Structures of Salient Regions for
   Stereoscopic Images via Deep Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; stereoscopic image quality assessment;
   three-column deep model; visual saliency
ID VIDEO QUALITY; PREDICTION; MODELS
AB The perceptual quality of stereoscopic images plays an essential role in the human perception of visual information. However, most available stereoscopic image quality assessment (SIQA) methods evaluate 3D visual experience using hand-crafted features or shallow architectures, which cannot model the visual properties of stereo images well. In this paper, we use convolutional neural networks (CNNs) to learn deeper local quality-aware structures for stereo images. With different inputs, two CNN models are designed for no-reference SIQA tasks. The one-column CNN model directly accepts a cyclopean view as the input, and the three-column CNN model jointly considers the cyclopean, left and right views as CNN inputs. The two SIQA frameworks share the same implementation approach: First, to overcome the obstacle of limited SIQA datasets, we accept image patches that have been cropped from corresponding stereopairs as inputs for local quality-sensitive feature extraction. Next, a local feature selection algorithm is used to remove related features on non-salient patches, which could cause large prediction errors. Finally, the reserved local visual structures of salient regions are aggregated into a final quality score in an end-to-end manner. Experimental results on three public SIQA databases demonstrate that our method outperforms most state-of-the-art no-reference (NR) SIQA methods. The results of a cross-database experiment also show the robustness and generality of the proposed method.
C1 [Sun, Guangming; Ding, Yong] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Shi, Bufan] Tongji Univ, Sch Automobile Studies, Shanghai 200092, Peoples R China.
   [Chen, Xiaodong] China Elect Technol Grp Corp, Res Inst 14, Nanjing 410111, Peoples R China.
   [Krylov, Andrey S.] Lomonosov Moscow State Univ, Lab Math Methods Image Proc, Moscow 119991, Russia.
C3 Zhejiang University; Tongji University; China Electronics Technology
   Group; Lomonosov Moscow State University
RP Ding, Y (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
EM gm96sun@gmail.com; shibf2018@gmail.com; ndcxd2000@163.com;
   kryl@cs.msu.ru; dingy@vlsi.zju.edu.cn
RI Shi, Bufan/CAF-4222-2022; Shi, Bufan/IAP-3899-2023; Krylov, Andrey
   S/B-9651-2014
OI Shi, Bufan/0000-0002-6793-8410; 
FU National Science and Technology Major Project [2016ZX01012101-003];
   Fundamental Research Funds for the Central Universities; National Key
   Research and Development Program of China [2018YFE0183900]
FX This work was supported in part by the National Science and Technology
   Major Project under Grant 2016ZX01012101-003 and the Fundamental
   Research Funds for the Central Universities, and in part by the National
   Key Research and Development Program of China under Grant
   2018YFE0183900. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Manoranjan Paul.
CR Akhter R, 2010, PROC SPIE, V7524, DOI 10.1117/12.838775
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Bosse S, 2016, IEEE IMAGE PROC, P3773, DOI 10.1109/ICIP.2016.7533065
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding J, 2013, J VISION, V13, DOI 10.1167/13.2.13
   Ding Y, 2018, IEEE ACCESS, V6, P37595, DOI 10.1109/ACCESS.2018.2851255
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hewage CTER, 2008, ELECTRON LETT, V44, P963, DOI 10.1049/el:20081562
   Jiang GY, 2018, IET IMAGE PROCESS, V12, P810, DOI 10.1049/iet-ipr.2017.0650
   Jiang QP, 2018, PATTERN RECOGN, V76, P242, DOI 10.1016/j.patcog.2017.11.001
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li YM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING (DSP), P685, DOI 10.1109/ICDSP.2016.7868646
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lv YQ, 2016, SIGNAL PROCESS-IMAGE, V47, P346, DOI 10.1016/j.image.2016.07.003
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Oh H, 2017, IEEE T IMAGE PROCESS, V26, P4923, DOI 10.1109/TIP.2017.2725584
   Qiuping Jiang, 2014, Journal of Software, V9, P1841, DOI 10.4304/jsw.9.7.1841-1847
   Shao F, 2017, IEEE ACCESS, V5, P15706, DOI 10.1109/ACCESS.2017.2733161
   Shao F, 2016, IEEE T MULTIMEDIA, V18, P2104, DOI 10.1109/TMM.2016.2594142
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2015, IEEE SIGNAL PROC LET, V22, P1548, DOI 10.1109/LSP.2015.2413946
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Shen LL, 2018, MULTIMED TOOLS APPL, V77, P8195, DOI 10.1007/s11042-017-4709-7
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xing LY, 2012, IEEE T MULTIMEDIA, V14, P326, DOI 10.1109/TMM.2011.2172402
   Yang JC, 2019, INFORM SCIENCES, V474, P1, DOI 10.1016/j.ins.2018.08.066
   Yasakethu SLP, 2008, IEEE T CONSUM ELECTR, V54, P1969, DOI 10.1109/TCE.2008.4711260
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
NR 44
TC 9
Z9 9
U1 4
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2938
EP 2949
DI 10.1109/TMM.2020.2965461
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900014
DA 2024-07-18
ER

PT J
AU Dai, QQ
   Chopp, H
   Pouyet, E
   Cossairt, O
   Walton, M
   Katsaggelos, AK
AF Dai, Qiqin
   Chopp, Henry
   Pouyet, Emeline
   Cossairt, Oliver
   Walton, Marc
   Katsaggelos, Aggelos K.
TI Adaptive Image Sampling Using Deep Learning and Its Application on X-Ray
   Fluorescence Image Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive sampling; convolutional neural network; X-Ray fluorescence;
   inpainting
ID SCANNING MACRO-XRF; SUPERRESOLUTION; REPRESENTATION
AB This paper presents an adaptive image sampling algorithm based on Deep Learning (DL). It consists of an adaptive sampling mask generation network which is jointly trained with an image inpainting network. The sampling rate is controlled by the mask generation network, and a binarization strategy is investigated to make the sampling mask binary. In addition to the image sampling and reconstruction process, we show how it can be extended and used to speed up raster scanning such as the X-Ray fluorescence (XRF) image scanning process. Recently XRF laboratory-based systems have evolved into lightweight and portable instruments thanks to technological advancements in both X-Ray generation and detection. However, the scanning time of an XRF image is usually long due to the long exposure requirements (e.g., 100 mu s - 1 ms per point). We propose an XRF image inpainting approach to address the long scanning times, thus speeding up the scanning process, while being able to reconstruct a high quality XRF image. The proposed adaptive image sampling algorithm is applied to the RGB image of the scanning target to generate the sampling mask. The XRF scanner is then driven according to the sampling mask to scan a subset of the total image pixels. Finally, we inpaint the scanned XRF image by fusing the RGB image to reconstruct the full scan XRF image. The experiments show that the proposed adaptive sampling algorithm is able to effectively sample the image and achieve a better reconstruction accuracy than that of existing methods.
C1 [Dai, Qiqin; Chopp, Henry; Cossairt, Oliver; Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
   [Pouyet, Emeline; Walton, Marc] Northwestern Univ, Chicago Ctr Sci Studies Arts NU ACCESS, Art Inst, Evanston, IL 60208 USA.
C3 Northwestern University; Northwestern University
RP Katsaggelos, AK (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM qiqindai2012@u.northwestern.edu; HenryChopp2017@u.northwestern.edu;
   emeline.pouyet@northwestern.edu; ollie@eecs.northwestern.edu;
   marc.walton@northwestern.edu; aggk@eecs.northwestern.edu
RI Li, Mengqi/AAG-6804-2021; Pouyet, Emeline/AAK-5772-2021; Cossairt,
   Oliver/I-5647-2012; Katsaggelos, Aggelos K/B-7233-2009; Pouyet,
   Emeline/AAA-8038-2022
OI Pouyet, Emeline/0000-0001-5824-2940; Chopp, Henry/0000-0003-2243-3467;
   Katsaggelos, Aggelos K/0000-0003-4554-0070
FU NSF PIRE [1743748]; Office Of The Director; Office Of Internatl Science
   &Engineering [1743748] Funding Source: National Science Foundation
FX This work was supported by an NSF PIRE under Grant 1743748 through
   Computationally-Based Imaging of Structure and Material (CuBISM).
CR Akhtar N, 2014, LECT NOTES COMPUT SC, V8695, P63, DOI 10.1007/978-3-319-10584-0_5
   Alfeld M, 2013, J ANAL ATOM SPECTROM, V28, P760, DOI 10.1039/c3ja30341a
   Alfeld M, 2013, J ANAL ATOM SPECTROM, V28, P40, DOI 10.1039/c2ja30119a
   Anitha A, 2013, SIGNAL PROCESS, V93, P592, DOI 10.1016/j.sigpro.2012.09.027
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2015, P 3 INT C LEARN REPR
   Babacan SD, 2008, IEEE IMAGE PROC, P641, DOI 10.1109/ICIP.2008.4711836
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Bioucas-Dias JM, 2012, IEEE J-STARS, V5, P354, DOI 10.1109/JSTARS.2012.2194696
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Chan TF, 2001, J VIS COMMUN IMAGE R, V12, P436, DOI 10.1006/jvci.2001.0487
   Chan TF, 2002, SIAM J APPL MATH, V62, P1019, DOI 10.1137/S0036139900368844
   Chen Z, 2018, IEEE T MULTIMEDIA, V20, P1610, DOI 10.1109/TMM.2017.2774004
   COOK RL, 1986, ACM T GRAPHIC, V5, P51, DOI 10.1145/7529.8927
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dai Q., 2016, PROC IEEE 12 WORKSHO, P1
   Dai QQ, 2017, IEEE T COMPUT IMAG, V3, P432, DOI 10.1109/TCI.2017.2703987
   Demaret L, 2006, SIGNAL PROCESS, V86, P1604, DOI 10.1016/j.sigpro.2005.09.003
   Deng CW, 2012, IEEE T MULTIMEDIA, V14, P278, DOI 10.1109/TMM.2011.2181491
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2016, IEEE T IMAGE PROCESS, V25, P2337, DOI 10.1109/TIP.2016.2542360
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Eldar Y, 1997, IEEE T IMAGE PROCESS, V6, P1305, DOI 10.1109/83.623193
   Esedoglu S, 2002, EUR J APPL MATH, V13, P353, DOI 10.1017/S0956792501004904
   Gao RH, 2017, IEEE I CONF COMP VIS, P1095, DOI 10.1109/ICCV.2017.124
   Iliadis M, 2020, DIGIT SIGNAL PROCESS, V96, DOI 10.1016/j.dsp.2019.102591
   Iliadis M, 2018, DIGIT SIGNAL PROCESS, V72, P9, DOI 10.1016/j.dsp.2017.09.010
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kappeler A, 2016, IEEE T COMPUT IMAG, V2, P109, DOI 10.1109/TCI.2016.2532323
   Keshava N, 2002, IEEE SIGNAL PROC MAG, V19, P44, DOI 10.1109/79.974727
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lanaras C, 2015, IEEE I CONF COMP VIS, P3586, DOI 10.1109/ICCV.2015.409
   Lin AS, 2015, EUR SIGNAL PR CONF, P604, DOI 10.1109/EUSIPCO.2015.7362454
   Liu HX, 2014, IEEE T MULTIMEDIA, V16, P1549, DOI 10.1109/TMM.2014.2328324
   Liu JX, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS (VISAPP), VOL 1, P25
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maas A, 2013, MOD PHYS LETT A, V28, DOI 10.1142/S0217732313501034
   Manolakis D, 2001, IEEE T GEOSCI REMOTE, V39, P1392, DOI 10.1109/36.934072
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Rajesh S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, PROCEEDINGS, VOLS 1-8, P1645, DOI 10.1109/ISIE.2007.4374851
   Ramponi G, 2001, IMAGE VISION COMPUT, V19, P451, DOI 10.1016/S0262-8856(00)00090-1
   Sam G., 2016, TRAINING INVESTIGATI
   Shivakumar SS, 2019, IEEE INT C INTELL TR, P13, DOI [10.1109/ITSC.2019.8917294, 10.1109/itsc.2019.8917294]
   Song P., 2018, P IEEE 88 VEH TECHN, P1
   Soumekh M, 1998, IEEE T IMAGE PROCESS, V7, P1627, DOI 10.1109/83.725371
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Taimori A, 2018, IEEE TRANS COMPUT IM, V4, P311, DOI 10.1109/TCI.2018.2833625
   TZOU KH, 1987, OPT ENG, V26, P581, DOI 10.1117/12.7974121
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Xie HT, 2019, IEEE T MULTIMEDIA, V21, P1248, DOI 10.1109/TMM.2018.2872898
   Xie HT, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3231737
   Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031
   Xu B, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/832093
   Yang J., 2008, PROC IEEE C COMPUT V, P1
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yuhas R., 1992, SUMMARIES 4 ANN JPL, P147
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang LY, 2016, IEEE T MULTIMEDIA, V18, P1720, DOI 10.1109/TMM.2016.2581593
   Zhou MY, 2012, IEEE T IMAGE PROCESS, V21, P130, DOI 10.1109/TIP.2011.2160072
NR 65
TC 22
Z9 23
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2564
EP 2578
DI 10.1109/TMM.2019.2958760
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NU6VI
UT WOS:000573779600001
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Krasula, L
   Fliegel, K
   Le Callet, P
AF Krasula, Lukas
   Fliegel, Karel
   Le Callet, Patrick
TI FFTMI: Features Fusion for Natural Tone-Mapped Images Quality Evaluation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Indexes; Image quality; Dynamic range; Quality
   assessment; Reliability; High dynamic range imaging; tone-mapping;
   quality assessment; feature selection
ID SCENE STATISTICS; CONTRAST; ALGORITHMS; BLUR
AB Tone-mapping is a crucial step in the task towards displaying high dynamic range (HDR) images on standard displays. Given the number of possible ways to tone-map such images, development of an objective quality criterion, enabling selection of the most suitable tone-mapping operator (TMO) and setting its parameters in order to maximize the quality of the reproduction, is of high interest. In this paper, a new objective metric for natural tone-mapped images is proposed. It is based on a fusion of several perceptually relevant features that have been carefully selected using an appropriate feature selection procedure. The outcome of the selection also provides a valuable insight into the importance of particular perceptual aspects when judging the quality of tone-mapped HDR content. The performance of the resulting combination of features is thoroughly evaluated with respect to three publicly available databases and compared to several relevant state-of-the-art criteria. The proposed approach is shown to significantly outperform the tested metrics and can, therefore, be considered a competitive alternative for tone-mapped images evaluation.
C1 [Krasula, Lukas; Le Callet, Patrick] Univ Nantes, F-44306 Nantes, France.
   [Krasula, Lukas; Fliegel, Karel] Czech Tech Univ, Fac Elect Engn, Dept Radioelect, Prague 16636 6, Czech Republic.
C3 Nantes Universite; Czech Technical University Prague
RP Krasula, L (corresponding author), Univ Nantes, F-44306 Nantes, France.
EM lukas.krasula@univ-nantes.fr; fliegek@fel.cvut.cz;
   patrick.lecallet@univ-nantes.fr
RI Le Callet, Patrick/F-5772-2010; Fliegel, Karel/D-7115-2013
FU Czech Science Foundation [GA17-05840S]
FX This work was supported in part by the Czech Science Foundation within
   Project GA17-05840S "Multicriteria optimization of shift-variant imaging
   system models." The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Agaian SS, 2007, IEEE T IMAGE PROCESS, V16, P741, DOI 10.1109/TIP.2006.888338
   [Anonymous], P INT C SIGN PROC CO
   [Anonymous], 2016, P IEEE INT C QUAL MU
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Aydin TO, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360668
   Banterle F, 2011, ADVANCED HIGH DYNAMIC RANGE IMAGING: THEORY AND PRACTICE, P1
   Batten C. F., THESIS
   Brassard G., 1996, Fundamentals of Algorithmics
   Cadík M, 2008, COMPUT GRAPH-UK, V32, P330, DOI 10.1016/j.cag.2008.04.003
   CONN AR, 1991, SIAM J NUMER ANAL, V28, P545, DOI 10.1137/0728030
   Crozier WJ, 1936, J GEN PHYSIOL, V19, P0503, DOI 10.1085/jgp.19.3.503
   ERASMUS SJ, 1982, J MICROSC-OXFORD, V127, P185, DOI 10.1111/j.1365-2818.1982.tb00412.x
   FERZLI R, 2005, P 1 INT WORKSH VID P, P1
   Ferzli R., 2007, Third International Workshop on Video Processing and Quality Metrics for Consumer Electronics, P25
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   FIRESTONE L, 1991, CYTOMETRY, V12, P195, DOI 10.1002/cyto.990120302
   Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521
   Fu Y.-Y., 2003, THESIS
   Granza M. H., 2015, PROC IEEE 13 BRAZILI, P1
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Hadizadeh H, 2018, IEEE T MULTIMEDIA, V20, P392, DOI 10.1109/TMM.2017.2740023
   HANLEY JA, 1983, RADIOLOGY, V148, P839, DOI 10.1148/radiology.148.3.6878708
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou XH, 2008, I C WIREL COMM NETW, P6009
   Hou YT, 2007, IEEE INFOCOM SER, P1, DOI 10.1109/INFCOM.2007.9
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   ITU-R, 2015, Tech. Rep., ITU-R Rec, BT.2020-2
   KELLY DH, 1970, J OPT SOC AM, V60, P98, DOI 10.1364/JOSA.60.000098
   Krasula L, 2017, IEEE T IMAGE PROCESS, V26, P1496, DOI 10.1109/TIP.2017.2651374
   Krasula L, 2017, IEEE J-STSP, V11, P64, DOI 10.1109/JSTSP.2016.2637168
   Krasula L, 2015, PROC SPIE, V9599, DOI 10.1117/12.2186388
   Krasula L, 2014, PROC SPIE, V9217, DOI 10.1117/12.2075270
   Krasula L, 2014, PROC SPIE, V9138, DOI 10.1117/12.2054504
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P4725, DOI 10.1109/TIP.2017.2713945
   Kundu D, 2017, IEEE T IMAGE PROCESS, V26, P2957, DOI 10.1109/TIP.2017.2685941
   Lewis RM, 2007, SIAM J SCI COMPUT, V29, P2507, DOI 10.1137/050635432
   Liu H, 2005, IEEE T KNOWL DATA EN, V17, P491, DOI 10.1109/TKDE.2005.66
   Liu H., 1998, Feature Extraction, Construction and Selection: A Data Mining Perspective, V241, P259
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P494, DOI 10.1016/j.image.2014.02.004
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   MANNOS JL, 1974, IEEE T INFORM THEORY, V20, P525, DOI 10.1109/TIT.1974.1055250
   Marichal X., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P386, DOI 10.1109/ICIP.1999.822923
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Matkovic K., 2005, Computational Aesthetics, P159
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.75
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Nafchi HZ, 2015, IEEE SIGNAL PROC LET, V22, P1026, DOI 10.1109/LSP.2014.2381458
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Nuutinen M, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.061111
   Panetta K, 2013, IEEE T CONSUM ELECTR, V59, P643, DOI 10.1109/TCE.2013.6626251
   Panetta K, 2011, IEEE T INF TECHNOL B, V15, P918, DOI 10.1109/TITB.2011.2164259
   Petit J, 2013, J VIS COMMUN IMAGE R, V24, P1020, DOI 10.1016/j.jvcir.2013.06.014
   Richter T, 2011, PROC SPIE, V8135, DOI 10.1117/12.896091
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sang QB, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.061104
   Shaked D, 2005, IEEE IMAGE PROC, P841
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z., 2002, P IEEE INT C IM PROC, V1, pI
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Yeganeh H, 2013, IEEE T IMAGE PROCESS, V22, P657, DOI 10.1109/TIP.2012.2221725
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang N., 2003, Proceedings of Section of Physical and Engineering Sciences of American Statistical Society, P4730
NR 67
TC 11
Z9 12
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2038
EP 2047
DI 10.1109/TMM.2019.2952256
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500010
DA 2024-07-18
ER

PT J
AU She, DY
   Yang, JF
   Cheng, MM
   Lai, YK
   Rosin, PL
   Wang, L
AF She, Dongyu
   Yang, Jufeng
   Cheng, Ming-Ming
   Lai, Yu-Kun
   Rosin, Paul L.
   Wang, Liang
TI WSCNet: Weakly Supervised Coupled Networks for Visual Sentiment
   Classification and Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Proposals; Task analysis; Feature extraction; Sentiment
   analysis; Training; Convolutional neural networks; Visual sentiment
   analysis; weakly supervised detection; convolutional neural networks
ID EMOTION; DEEP; CATEGORIZATION; PREDICTION; ATTENTION; IMAGES
AB Automatic assessment of sentiment from visual content has gained considerable attention with the increasing tendency of expressing opinions online. In this paper, we solve the problem of visual sentiment analysis, which is challenging due to the high-level abstraction in the recognition process. Existing methods based on convolutional neural networks learn sentiment representations from the holistic image, despite the fact that different image regions can have different influence on the evoked sentiment. In this paper, we introduce a weakly supervised coupled convolutional network (WSCNet). Our method is dedicated to automatically selecting relevant soft proposals given weak annotations (e.g., global image labels), thereby significantly reducing the annotation burden, and encompasses the following contributions. First, the proposed WSCNet detects a sentiment-specific soft map by training a fully convolutional network with the cross spatial pooling strategy in the detection branch. Second, both the holistic and localized information are utilized by coupling the sentiment map with deep features as semantic vector in the classification branch. The sentiment detection and classification branches are integrated into a unified deep framework optimized in an end-to-end manner. Extensive experiments demonstrate that the proposed WSCNet outperforms the state-of-the-art results on seven benchmark datasets.
C1 [She, Dongyu; Yang, Jufeng; Cheng, Ming-Ming] Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
   [Lai, Yu-Kun; Rosin, Paul L.] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Wang, Liang] Chinese Acad Sci, Natl Lab Pattern Recognit, CAS Ctr Excellence Brain Sci & Intelligence Techn, Beijing 100190, Peoples R China.
C3 Nankai University; Cardiff University; Chinese Academy of Sciences
RP Yang, JF (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin 300350, Peoples R China.
EM sherry6656@163.com; yangjufeng@nankai.edu.cn; cmm@nankai.edu.cn;
   laiy4@cardiff.ac.uk; Paul.Rosin@cs.cf.ac.uk; wangliang@nlpr.ia.ac.cn
RI Lai, Yu-Kun/D-2343-2010; Cheng, Ming-Ming/A-2527-2009
OI Cheng, Ming-Ming/0000-0001-5550-8758; Rosin, Paul/0000-0002-4965-3884;
   Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China (NSFC) [61876094,
   U1933114]; Natural Science Foundation of Tianjin, China [18JCYBJC15400,
   18ZXZNGX00110]; Open Project Program of the National Laboratory of
   Pattern Recognition (NLPR); Fundamental Research Funds for the Central
   Universities
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grants 61876094 and U1933114, in part
   by the Natural Science Foundation of Tianjin, China, under Grants
   18JCYBJC15400 and 18ZXZNGX00110, in part by the Open Project Program of
   the National Laboratory of Pattern Recognition (NLPR), and in part by
   the Fundamental Research Funds for the Central Universities.
CR Alameda-Pineda X, 2017, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2017.59
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2012, P 20 ACM INT C MULT
   [Anonymous], 2016, 2016 IEEE International Conference on Multimedia and Expo
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   [Anonymous], CHINESE GEOLOGICAL E, DOI DOI 10.1145/2502069.2502079
   [Anonymous], 2012, P 20 ACM INT C MULT
   BILEN H, 2016, PROC CVPR IEEE, P3034, DOI [10.1109/CVPR.2016.331, DOI 10.1109/CVPR.2016.331]
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Boehner K, 2007, INT J HUM-COMPUT ST, V65, P275, DOI 10.1016/j.ijhcs.2006.11.016
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   Brosch T, 2010, COGNITION EMOTION, V24, P377, DOI 10.1080/02699930902975754
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen T., 2014, DeepSentiBank: Visual Sentiment Concept Classification with Deep Convolutional Neural Networks
   Chen YY, 2015, IEEE T AFFECT COMPUT, V6, P298, DOI 10.1109/TAFFC.2014.2388370
   Cheng G, 2016, IEEE T GEOSCI REMOTE, V54, P7405, DOI 10.1109/TGRS.2016.2601622
   Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231
   Cinbis RG, 2014, PROC CVPR IEEE, P2409, DOI 10.1109/CVPR.2014.309
   Compton Rebecca J, 2003, Behav Cogn Neurosci Rev, V2, P115, DOI 10.1177/1534582303002002003
   Dabkowski Piotr., 2017, P 31 INT C NEUR INF, DOI DOI 10.48550/ARXIV.1705.07857
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545
   Durand T, 2017, P IEEE C COMP VIS PA
   Fan SJ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P217, DOI 10.1145/3123266.3123445
   Fan SJ, 2018, PROC CVPR IEEE, P7521, DOI 10.1109/CVPR.2018.00785
   Gao J., 2017, P ACM INT C MULT RET
   Gupta R, 2016, FRONT PSYCHOL, V7, DOI 10.3389/fpsyg.2016.01613
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang XG, 2015, 2015 IEEE International Conference on Applied Superconductivity and Electromagnetic Devices (ASEMD), P262, DOI 10.1109/ASEMD.2015.7453564
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Ji RR, 2016, FRONT COMPUT SCI-CHI, V10, P602, DOI 10.1007/s11704-016-5453-2
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Joyce J.M., 2011, Kullback-Leibler Divergence, P720, DOI [DOI 10.1007/978-3-642-04898-2_327, DOI 10.1007/978-3-642-04898-2327, 10.1007/978-3-642-04898-2327]
   Katsurai M, 2016, INT CONF ACOUST SPEE, P2837, DOI 10.1109/ICASSP.2016.7472195
   Kosti R, 2017, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2017.212
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Lin KH, 2006, IEEE INT SYMP CIRC S, P3534
   Machajdik J., 2010, P 18 ACM INT C MULT, P83, DOI DOI 10.1145/1873951.1873965
   Murray HA, 1945, GENET PSYCHOL MONOGR, V32, P153
   Öhman A, 2001, J EXP PSYCHOL GEN, V130, P466, DOI 10.1037/0096-3445.130.3.466
   Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668
   Paszke Adam, 2017, Pytorch
   PENG KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Peng KC, 2016, IEEE IMAGE PROC, P614, DOI 10.1109/ICIP.2016.7532430
   Porzi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P139, DOI 10.1145/2733373.2806273
   Truong QT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1274, DOI 10.1145/3123266.3123374
   Rao TR, 2019, NEUROCOMPUTING, V333, P429, DOI 10.1016/j.neucom.2018.12.053
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren WQ, 2016, IEEE T PATTERN ANAL, V38, P405, DOI 10.1109/TPAMI.2015.2456908
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   Sartori A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P311, DOI 10.1145/2733373.2806250
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   She D., P ACM T MULT COMP
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Vuilleumier P, 2005, TRENDS COGN SCI, V9, P585, DOI 10.1016/j.tics.2005.10.011
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wu JJ, 2015, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2015.7298968
   Wu LF, 2017, IEEE IMAGE PROC, P1322, DOI 10.1109/ICIP.2017.8296496
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Yan Q, 2017, 2017 INTERNATIONAL CONFERENCE ON FRONTIERS IN EDUCATIONAL TECHNOLOGIES AND MANAGEMENT SCIENCES (FETMS 2017), P359
   Yang JF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3266
   Yang JF, 2018, AAAI CONF ARTIF INTE, P491
   Yang JF, 2018, PROC CVPR IEEE, P7584, DOI 10.1109/CVPR.2018.00791
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   Yanulevskaya V, 2008, IEEE IMAGE PROC, P101, DOI 10.1109/ICIP.2008.4711701
   Yao X., 2019, P INT C COMP VIS
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Zeiler M. D., 2014, P EUR C COMPUT VIS
   Zhan C., 2019, P INT C COMP VIS
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang HM, 2016, IEEE IMAGE PROC, P629, DOI 10.1109/ICIP.2016.7532433
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhao SC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4669
   Zhao SC, 2020, IEEE T AFFECT COMPUT, V11, P574, DOI [10.1109/TAFFC.2018.2818685, 10.1109/TAFFC.2016.2628787]
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zhu Y, 2017, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2017.204
NR 90
TC 58
Z9 64
U1 1
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1358
EP 1371
DI 10.1109/TMM.2019.2939744
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200020
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Xie, L
   Lee, FF
   Liu, L
   Yin, Z
   Chen, Q
AF Xie, Lin
   Lee, Feifei
   Liu, Li
   Yin, Zhong
   Chen, Qiu
TI Hierarchical Coding of Convolutional Features for Scene Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Convolutional codes; Encoding; Image representation;
   Feature extraction; Image recognition; Image coding; Convolutional
   feature; Inter-class linear coding; Non-negative sparse decomposition;
   Scene recognition
ID IMAGE CLASSIFICATION; REPRESENTATION; NETWORK; SCALE
AB Convolutional neural networks (CNNs) have achieved great success in visual recognition because of the availability of large-scale image datasets, such as the ImageNet. The transfer of convolutional features to challenging scene recognition remains an open problem. Multiple non-linear transforms endow the convolutional features with abundant information. On the other side, CNNs are adept at capturing the holistic appearances of scenes, whereas the lack of some critical local details may reduce the recognition accuracy. To address these problems, we propose a novel hierarchical coding algorithm to learn effective representations. To adapt the scale variations, many useful patches with various scales sampled from the whole image are considered to provide the sufficient details. Non-negative sparse decomposition model (NNSD) based on convolutional features is proposed to learn the sharable components for each scale and further produce global signatures. Based on the global signatures, inter-class linear coding (ICLC) is proposed to learn the discriminative components and ultimate image representations. Experimental results indicate that our approach significantly improves the recognition accuracy compared with general CNN models and achieves excellent performance on five standard benchmarks.
C1 [Xie, Lin; Lee, Feifei; Yin, Zhong] Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.
   [Liu, Li] Nanchang Univ, Sch Informat Engn, Nanchang 330500, Jiangxi, Peoples R China.
   [Chen, Qiu] Kogakuin Univ, Elect Engn & Elect, Grad Sch Engn, Tokyo 1638677, Japan.
C3 University of Shanghai for Science & Technology; Nanchang University;
   Kogakuin University
RP Lee, FF (corresponding author), Univ Shanghai Sci & Technol, Sch Opt Elect & Comp Engn, Shanghai 200093, Peoples R China.; Chen, Q (corresponding author), Kogakuin Univ, Elect Engn & Elect, Grad Sch Engn, Tokyo 1638677, Japan.
EM linxiestd@126.com; feifeilee@ieee.org; liuli_033@163.com;
   yinzhong@usst.edu.cn; q.chen@ieee.org
RI CHEN, QIU/G-7959-2012
OI CHEN, QIU/0000-0003-3079-9207
FU Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning; JSPS KAKENHI [15K00159];
   Grants-in-Aid for Scientific Research [15K00159] Funding Source: KAKEN
FX This work was supported in part by the Program for Professor of Special
   Appointment (Eastern Scholar) at Shanghai Institutions of Higher
   Learning, and in part by the JSPS KAKENHI under Grant 15K00159. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Wenwu Zhu.
CR [Anonymous], 2014, P NIPS
   [Anonymous], 2013, NIPS
   Bai S, 2016, IEEE T MULTIMEDIA, V18, P1351, DOI 10.1109/TMM.2016.2557071
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Chaib S, 2017, IEEE T GEOSCI REMOTE, V55, P4775, DOI 10.1109/TGRS.2017.2700322
   Chen SZ, 2015, IEEE T GEOSCI REMOTE, V53, P1947, DOI 10.1109/TGRS.2014.2351395
   Cheng XJ, 2018, PATTERN RECOGN, V74, P474, DOI 10.1016/j.patcog.2017.09.025
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dixit M, 2015, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2015.7298916
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Guo S, 2017, IEEE T IMAGE PROCESS, V26, P808, DOI 10.1109/TIP.2016.2629443
   Guo YM, 2018, IEEE T MULTIMEDIA, V20, P1525, DOI 10.1109/TMM.2017.2766842
   Hayat M, 2016, IEEE T IMAGE PROCESS, V25, P4829, DOI 10.1109/TIP.2016.2599292
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jiang YN, 2012, LECT NOTES COMPUT SC, V7573, P730, DOI 10.1007/978-3-642-33709-3_52
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Khan SH, 2016, IEEE T IMAGE PROCESS, V25, P3372, DOI 10.1109/TIP.2016.2567076
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwitt Roland, 2012, Computer Vision - ECCV 2012. Proceedings of the 12th European Conference on Computer Vision, P359, DOI 10.1007/978-3-642-33765-9_26
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li L, 2010, AAAI CONF ARTIF INTE, P1377
   Li LJ, 2007, IEEE I CONF COMP VIS, P345
   Li LJ, 2010, IMECE2009: PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, VOL 13, P51
   Lin D, 2014, PROC CVPR IEEE, P3726, DOI 10.1109/CVPR.2014.476
   Liu Y, 2018, AAAI CONF ARTIF INTE, P7178
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu XQ, 2017, IEEE T GEOSCI REMOTE, V55, P5148, DOI 10.1109/TGRS.2017.2702596
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Margolin R, 2014, LECT NOTES COMPUT SC, V8695, P377, DOI 10.1007/978-3-319-10584-0_25
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Othman E, 2017, IEEE T GEOSCI REMOTE, V55, P4441, DOI 10.1109/TGRS.2017.2692281
   Parikh D, 2011, PROC CVPR IEEE, P1681, DOI 10.1109/CVPR.2011.5995451
   Parizi SN, 2012, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2012.6248001
   Qiao YZ, 2017, AIP CONF PROC, V1821, DOI 10.1063/1.4977617
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shabou A, 2012, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2012.6248107
   Shi J, 2019, IEEE ACCESS, V7, P45230, DOI 10.1109/ACCESS.2019.2908448
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422
   Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang JZ, 2016, IEEE T MULTIMEDIA, V18, P1000, DOI 10.1109/TMM.2016.2544099
   Wang LM, 2017, IEEE T IMAGE PROCESS, V26, P2055, DOI 10.1109/TIP.2017.2675339
   Wang Z, 2017, IEEE T IMAGE PROCESS, V26, P2028, DOI 10.1109/TIP.2017.2666739
   Wang Z, 2016, INT CONF ACOUST SPEE, P1258, DOI 10.1109/ICASSP.2016.7471878
   Weng CQ, 2017, IEEE SIGNAL PROC LET, V24, P1143, DOI 10.1109/LSP.2016.2641020
   Wu JX, 2009, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2009.5459178
   Wu JX, 2011, IEEE T PATTERN ANAL, V33, P1489, DOI 10.1109/TPAMI.2010.224
   Wu RB, 2015, IEEE I CONF COMP VIS, P1287, DOI 10.1109/ICCV.2015.152
   Xie GS, 2017, IEEE T CIRC SYST VID, V27, P1263, DOI 10.1109/TCSVT.2015.2511543
   Xie L, 2017, 2017 2ND IEEE INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (ICCIA), P345, DOI 10.1109/CIAPP.2017.8167236
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang M, 2015, IEEE IMAGE PROC, P402, DOI 10.1109/ICIP.2015.7350829
   Yang SF, 2015, IEEE I CONF COMP VIS, P1215, DOI 10.1109/ICCV.2015.144
   Yang Y, 2011, IEEE I CONF COMP VIS, P1465, DOI 10.1109/ICCV.2011.6126403
   Zhang F, 2016, IEEE T GEOSCI REMOTE, V54, P1793, DOI 10.1109/TGRS.2015.2488681
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3241, DOI 10.1109/TIP.2014.2328894
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 71
TC 22
Z9 22
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1182
EP 1192
DI 10.1109/TMM.2019.2942478
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200006
DA 2024-07-18
ER

PT J
AU Liu, HJ
   Wang, SG
   Wang, W
   Cheng, J
AF Liu, Haijun
   Wang, Shiguang
   Wang, Wen
   Cheng, Jian
TI Multi-Scale Based Context-Aware Net for Action Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Feature extraction; Object detection; Logic gates;
   Three-dimensional displays; Message passing; Streaming media; Action
   detection; multiple scales; context-aware; gate function
ID ACTION RECOGNITION; NETWORK
AB We address the problem of action detection in continuous untrimmed video streams, based on the two-stage framework: one stage for action proposals generation and the other for proposals classification and refinement. The context features inside and outside a candidate region (proposal) are critical for classification in action detection. Therefore, effective integration of these features with different scales has become a fundamental problem. We contend that different action instances and candidate proposals may need different context features. To address this issue, we present a novel multiple scales based context-aware net (MSCA-Net) to effectively classify the action proposals for action detection in this paper. For each candidate action proposal, MSCA-Net takes its multiple regions with different temporal scales as input and then generates suitable context features. Based on the "candidate-control" mechanism of LSTM, the proposed MSCA-Net specially adopts the two-branch structure: Branch1 generates multi-scale context features for each candidate proposal, whereas Branch2 utilizes the context-aware gate function to control the message passing. Extensive experiments on THUMOS'14, Charades daily and ActivityNet action detection datasets, demonstrate the effectiveness of the designed structure and show how these context features influence the detection results.
C1 [Liu, Haijun; Wang, Shiguang; Wang, Wen; Cheng, Jian] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Cheng, J (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM haijun_liu@126.com; xiaohu_wyyx@163.com; uestc_wangwen@163.com;
   chengjian@uestc.edu.cn
RI Wang, Shiguang/A-5476-2018
OI Cheng, Jian/0000-0001-6966-0531; wang, wen/0000-0002-3737-3201
FU National Natural Science Foundation of China [61671125, 61201271,
   61301269]
FX This work was supported in part by the National Natural Science
   Foundation of China (61671125, 61201271, and 61301269). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Elisa Ricci.
CR [Anonymous], 2016, P NIPS
   [Anonymous], 2017, CORR
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2012, CoRR
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao J., 2017, P BRIT MACHINE VISIO
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Jiang Y., 2014, ECCV WORKSH
   Kuehne H, 2013, HIGH PERFORMANCE COMPUTING IN SCIENCE AND ENGINEERING '12: TRANSACTIONS OF THE HIGH PERFORMANCE COMPUTING CENTER, STUTTGART (HLRS) 2012, P571, DOI 10.1007/978-3-642-33374-3_41
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2017, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR.2017.599
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh G., 2016, ARXIV160701979
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2014, IEEE T IMAGE PROCESS, V23, P810, DOI 10.1109/TIP.2013.2295753
   Wang SG, 2018, IEEE T MULTIMEDIA, V20, P3148, DOI 10.1109/TMM.2018.2829602
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y, 2017, IEEE INT CONF COMP V, P2129, DOI 10.1109/ICCVW.2017.249
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Yang K, 2018, AAAI CONF ARTIF INTE, P7477
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Yuan Z., 2017, P IEEE C COMP VIS PA, P3684
   Zeng XY, 2018, IEEE T PATTERN ANAL, V40, P2109, DOI 10.1109/TPAMI.2017.2745563
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou K, 2016, DESTECH TRANS COMP
NR 47
TC 7
Z9 7
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2020
VL 22
IS 2
BP 337
EP 348
DI 10.1109/TMM.2019.2929923
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KH5FZ
UT WOS:000510676300005
DA 2024-07-18
ER

PT J
AU Lin, XX
   Liang, YY
   Wan, J
   Lin, C
   Li, T
AF Lin, Xuxin
   Liang, Yanyan
   Wan, Jun
   Lin, Chi
   Li, Stan Z.
TI Region-Based Context Enhanced Network for Robust Multiple Face Alignment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial landmark localization; face alignment; convolutional network;
   point distribution model
ID FACIAL LANDMARK DETECTION
AB The recent studies for face alignment have involved developing an isolated algorithm on well-cropped face images. It is difficult to obtain the expected input by using an off-the-shelf face detector in practical applications. In this paper, we attempt to bridge between face detection and face alignment by establishing a novel joint multi-task model, which allows us to simultaneously detect multiple faces and their landmarks on a given scene image. In contrast to the pipeline-based framework by cascading separate models, we aim to propose an end-to-end convolutional network by sharing and transform feature representations between the task-specific modules. To learn a robust landmark estimator for unconstrained face alignment, three types of context enhanced blocks are designed to encode feature maps with multi-level context, multi-scale context, and global context. In the post-processing step, we develop a shape reconstruction algorithm based on point distribution model to refine the landmark outliers. Extensive experiments demonstrate that our results are robust for the landmark location task and insensitive to the location of estimated face regions. Furthermore, our method significantly outperforms recent state-of-the-art methods on several challenging datasets including 300 W, AFLW, and COFW.
C1 [Lin, Xuxin; Liang, Yanyan] Macau Univ Sci & Technol, Fac Informat Technol, Taipa, Macau, Peoples R China.
   [Wan, Jun; Li, Stan Z.] Chinese Acad Sci, Inst Automat, Res & Natl Lab Pattern Recognit, Ctr Biometr & Secur, Beijing 100190, Peoples R China.
   [Lin, Chi] Univ Southern Calif, Los Angeles, CA 90007 USA.
C3 Macau University of Science & Technology; Chinese Academy of Sciences;
   Institute of Automation, CAS; University of Southern California
RP Liang, YY (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Taipa, Macau, Peoples R China.
EM linxuxin6@gmail.com; yyliang@must.edu.mo; jun.wan@nlpr.ia.ac.cn;
   linchi@usc.edu; szli@nlpr.ia.ac.cn
RI Liang, Yanyan/ACD-0082-2022; wu, jd/IST-2336-2023; Lin,
   Xuxin/HZJ-4566-2023; Liang, Yanyan/C-4132-2014
OI Liang, Yanyan/0000-0002-5780-8540; Lin, Xuxin/0000-0002-5179-6569; wan,
   jun/0000-0002-4735-2885
FU National Key Research and Development Plan [2016YFC0801002]; Chinese
   National Natural Science Foundation [61876179, 61872367]; Science and
   Technology Development Fund of Macau [152/2017/A, 0025/2018/A1,
   008/2019/A1]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801002, in part by the Chinese
   National Natural Science Foundation Projects 61876179 and 61872367, and
   in part by the Science and Technology Development Fund of Macau
   (152/2017/A, 0025/2018/A1, and 008/2019/A1). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lei Zhang.
CR Alabort-i-Medina J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P679, DOI 10.1145/2647868.2654890
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2017, PROC IEEE C COMPUT V
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P INT C LEARN REPR W
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46484-8_29
   [Anonymous], 2015, P INT C LEARN REP IC
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2018, P EUR C COMP VIS MUN
   [Anonymous], P INT C LEARN REPR W
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], FACE R CNN 2017
   [Anonymous], 2017, P IEEE C COMP VIS PA
   [Anonymous], 2014, IEEE C COMP VIS PATT
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Buitelaar P, 2018, IEEE T MULTIMEDIA, V20, P2454, DOI 10.1109/TMM.2018.2798287
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3
   Cao Z., 2017, P IEEE C COMP VIS PA, P7291
   Cech J, 2016, IMAGE VISION COMPUT, V47, P60, DOI 10.1016/j.imavis.2015.11.003
   Chen X, 2014, BIOINFORMATICS 2014: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON BIOINFORMATICS MODELS, METHODS AND ALGORITHMS, P109
   Cootes T. F., 1992, BMVC92. Proceedings of the British Machine Vision Conference, P266
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267
   Deng JK, 2016, IMAGE VISION COMPUT, V47, P19, DOI 10.1016/j.imavis.2015.11.005
   Dong Chen, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9909, P122, DOI 10.1007/978-3-319-46454-1_8
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Feng ZH, 2015, IEEE T IMAGE PROCESS, V24, P3425, DOI 10.1109/TIP.2015.2446944
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Haoqiang Fan, 2016, Image and Vision Computing, V47, P27, DOI 10.1016/j.imavis.2015.11.004
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hou QQ, 2018, PATTERN RECOGN, V74, P448, DOI 10.1016/j.patcog.2017.09.028
   Hsu GS, 2018, IEEE T CIRC SYST VID, V28, P3194, DOI 10.1109/TCSVT.2017.2748379
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jourabloo A, 2017, IEEE I CONF COMP VIS, P3219, DOI 10.1109/ICCV.2017.347
   Jourabloo A, 2016, PROC CVPR IEEE, P4188, DOI 10.1109/CVPR.2016.454
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Kumar A, 2017, IEEE INT CONF AUTOMA, P258, DOI 10.1109/FG.2017.149
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Liu YJ, 2017, IEEE INT CONF COMP V, P1619, DOI 10.1109/ICCVW.2017.190
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Martinez B, 2016, IMAGE VISION COMPUT, V47, P36, DOI 10.1016/j.imavis.2015.09.003
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Merget D, 2018, PROC CVPR IEEE, P781, DOI 10.1109/CVPR.2018.00088
   Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Ranjan R, 2019, IEEE T PATTERN ANAL, V41, P121, DOI 10.1109/TPAMI.2017.2781233
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Shen XH, 2013, PROC CVPR IEEE, P3460, DOI 10.1109/CVPR.2013.444
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Tao Y, 2017, CHIN CONTR CONF, P4288, DOI 10.23919/ChiCC.2017.8028032
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Uricár M, 2016, IMAGE VISION COMPUT, V47, P45, DOI 10.1016/j.imavis.2016.02.004
   Wang X, 2014, IEEE T MULTIMEDIA, V16, P2130, DOI 10.1109/TMM.2014.2355134
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1880, DOI 10.1109/TMM.2013.2269314
   Wu Y, 2017, PROC CVPR IEEE, P5719, DOI 10.1109/CVPR.2017.606
   Wu Y, 2015, IEEE I CONF COMP VIS, P3658, DOI 10.1109/ICCV.2015.417
   Xiao ST, 2017, IEEE I CONF COMP VIS, P1642, DOI 10.1109/ICCV.2017.181
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P2393, DOI 10.1109/TIP.2015.2421438
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   YANG S, 2016, PROC CVPR IEEE, P5525, DOI DOI 10.1109/CVPR.2016.596
   Yu X, 2013, IEEE I CONF COMP VIS, P1944, DOI 10.1109/ICCV.2013.244
   Zhan J, 2015, VISUAL COMPUT, V31, P575, DOI 10.1007/s00371-014-0984-8
   Zhang HW, 2018, IEEE T INF FOREN SEC, V13, P2409, DOI 10.1109/TIFS.2018.2800901
   Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhu CC, 2018, PROC CVPR IEEE, P5127, DOI 10.1109/CVPR.2018.00538
   Zhu CC, 2017, ADV COMPUT VIS PATT, P57, DOI 10.1007/978-3-319-61657-5_3
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 88
TC 11
Z9 11
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3053
EP 3067
DI 10.1109/TMM.2019.2916455
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200007
DA 2024-07-18
ER

PT J
AU Liu, YP
   Zhou, LS
   Zong, H
   Gong, XX
   Wu, QY
   Liang, QX
   Wang, J
AF Liu, Yuanpeng
   Zhou, Laishui
   Zong, Hua
   Gong, Xiaoxi
   Wu, Qiaoyun
   Liang, Qingxiao
   Wang, Jun
TI Regression-Based Three-Dimensional Pose Estimation for Texture-Less
   Objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Pose estimation; Feature extraction;
   Training; Image edge detection; Correlation; Cost function; Texture-less
   objects; pose estimation; triplet network; deep learning; pose
   regression
ID HEAVY CLUTTER
AB 3-D pose estimation for texture-less objects remains a challenging problem. Previous works either focus on a template matching method to find the nearest template as a candidate, or construct a Hough forest, which utilizes the offset of patches to vote for the object location and pose. By contrast, in this paper, we propose a comprehensive framework to directly regress 3-D poses for the candidates, in which a convolutional neural network-based triplet network is trained to extract discriminating features from the binary images. To make the features suitable for the regression task, a pose-guided method and a regression constraint are employed with the constructed triplet network. We show that the constraint reaches the goal of creating the correlation between the features and 3-D poses. Once the expected features are obtained, the object pose could be efficiently regressed, by training a regression network with a simple structure. For symmetric objects, depth images are treated as an additional channel to feed the triplet network. Experiments on the LineMOD and our own datasets demonstrate our method with high regression precision and efficiency.
C1 [Liu, Yuanpeng; Zhou, Laishui; Gong, Xiaoxi; Wu, Qiaoyun; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210007, Jiangsu, Peoples R China.
   [Zong, Hua] Natl Key Lab Sci & Technol, Aerosp Intelligent Control, Beijing 100854, Peoples R China.
   [Liang, Qingxiao] Avic Xian Aircraft Ind Grp Co Ltd, Xian 710089, Shaanxi, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Aviation Industry
   Corporation of China (AVIC)
RP Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210007, Jiangsu, Peoples R China.
EM xypengliu@gmail.com; zlsme@nuaa.edu.cn; zonghua3@sina.cn;
   xiaoxigong.nuaa@gmail.com; wuqiaoyun@nuaa.edu.cn; lqxasia@163.com;
   junwang@outlook.com
RI Wang, Jun/AAM-6868-2021; Liu, Yuanpeng/HNB-6644-2023
OI Wang, Jun/0000-0001-9223-2615; 
FU National Natural Science Foundation of China [61772267]; Fundamental
   Research Funds for the Central Universities [NE2014402, NE2016004];
   Nanjing University of Aeronautics and Astronautics Fundamental Research
   Funds [NS2015053]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772267, in part by the Fundamental
   Research Funds for the Central Universities under Grant NE2014402 and
   Grant NE2016004, and in part by the Nanjing University of Aeronautics
   and Astronautics Fundamental Research Funds under Grant NS2015053. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR [Anonymous], 2016, ARXIV160702257
   [Anonymous], 2014, REGISTRATION RECOGNI
   Balntas V, 2017, IEEE I CONF COMP VIS, P3876, DOI 10.1109/ICCV.2017.416
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   BORGEFORS G, 1988, IEEE T PATTERN ANAL, V10, P849, DOI 10.1109/34.9107
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Bratanic B, 2015, COMPUT VIS IMAGE UND, V141, P38, DOI 10.1016/j.cviu.2015.09.002
   Cai H., 2013, P 9 INT C COMPUTER V, P103
   Chan J, 2016, PROC CVPR IEEE, P2855, DOI 10.1109/CVPR.2016.312
   Chang MMY, 2005, IEEE T MULTIMEDIA, V7, P253, DOI 10.1109/TMM.2005.843344
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Damen D, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.23
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Ferrari V, 2010, INT J COMPUT VISION, V87, P284, DOI 10.1007/s11263-009-0270-9
   Gavrila D. M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P87, DOI 10.1109/ICCV.1999.791202
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hinterstoisser S, 2012, IEEE T PATTERN ANAL, V34, P876, DOI 10.1109/TPAMI.2011.206
   Hinterstoisser V., 2012, P COMP VIS ACCV 2012, P548
   Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005
   Hsu HW, 2019, IEEE T MULTIMEDIA, V21, P1035, DOI 10.1109/TMM.2018.2866770
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kim G, 2007, INNOVATIVE ALGORITHMS AND TECHNIQUES IN AUTOMATION, INDUSTRIAL ELECTRONICS AND TELECOMMUNICATIONS, P251, DOI 10.1007/978-1-4020-6266-7_46
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Liu HL, 2016, IEEE T MULTIMEDIA, V18, P1233, DOI 10.1109/TMM.2016.2556859
   Liu MY, 2012, INT J ROBOT RES, V31, P951, DOI 10.1177/0278364911436018
   Liu MY, 2010, IEEE INT CONF ROBOT, P2028, DOI 10.1109/ROBOT.2010.5509897
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu GY, 2017, IEEE T MULTIMEDIA, V19, P2117, DOI 10.1109/TMM.2017.2731044
   Mahendran S, 2017, IEEE COMPUT SOC CONF, P494, DOI 10.1109/CVPRW.2017.73
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P2107, DOI 10.1109/TKDE.2015.2399298
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sahin C, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4113, DOI 10.1109/IROS.2016.7759605
   Seo BK, 2014, IEEE T VIS COMPUT GR, V20, P99, DOI 10.1109/TVCG.2013.94
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun G, 2015, IEEE INT CONF ADV LE, P462, DOI 10.1109/ICALT.2015.26
   Sun X, 2017, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2017.284
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Tejani A, 2018, IEEE T PATTERN ANAL, V40, P119, DOI 10.1109/TPAMI.2017.2665623
   Tombari F, 2013, IEEE I CONF COMP VIS, P1265, DOI 10.1109/ICCV.2013.160
   Ulrich M, 2012, IEEE T PATTERN ANAL, V34, P1902, DOI 10.1109/TPAMI.2011.266
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Xiang Yu, 2017, ARXIV171100199
   Zeng Andy, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1386, DOI 10.1109/ICRA.2017.7989165
   Zhu ZT, 2014, 2014 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P279, DOI 10.1109/SPAC.2014.6982699
NR 50
TC 14
Z9 16
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2776
EP 2789
DI 10.1109/TMM.2019.2913321
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000007
DA 2024-07-18
ER

PT J
AU Li, SQ
   Liu, W
   Ma, HD
AF Li, Shuangqun
   Liu, Wu
   Ma, Huadong
TI Attentive Spatial-Temporal Summary Networks for Feature Learning in
   Irregular Gait Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; attention mechanism; gait cycle; irregular gait
   sequence
ID MODEL
AB Gait recognition is an attractive human recognition technology. However, existing gait recognition methods mainly focus on the regular gait cycles, which ignore the irregular situation. In real-world surveillance, human gait is almost irregular, which contains arbitrary dynamic characteristics (e.g., duration, speed, and phase) and varied viewpoints. In this paper, we propose the attentive spatial-temporal summary networks to learn salient spatial-temporal and view-independence features for irregular gait recognition. First of all, we design the gate mechanism with attentive spatial-temporal summary to extract the discriminative sequence-level features for representing the periodic motion cues of irregular gait sequences. The designed general attention and residual attention components can concentrate on the discriminative identity-related semantic regions from the spatial feature maps. The proposed attentive temporal summary component can automatically assign adaptive attention to enhance the discriminative gait timesteps and suppress the redundant ones. Furthermore, to improve the accuracy of cross-view gait recognition, we combine the Siamese structure and Null Foley-Sammon transform to obtain the view-invariant gait features from irregular gait sequences. Finally, we quantitatively evaluate the impact of the irregular gait and viewpoint interval between matching pairs on gait recognition accuracy. Experimental results show that our method achieves state-of-the-art performance in irregular gait recognition on the OULP and CASIA-B datasets.
C1 [Li, Shuangqun; Liu, Wu; Ma, Huadong] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
   [Liu, Wu] JD AI Res, Beijing 100105, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Ma, HD (corresponding author), Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
EM shuangqunli@hotmail.com; liuwu@live.cn; mhd@bupt.edu.cn
RI Liu, Wu/AAG-3615-2019
OI Liu, Wu/0000-0003-1633-7575
FU International Cooperation and Exchange of the National Natural Science
   Foundation of China (NSFC) [61720106007]; National Natural Science
   Foundation of China [61602049]; NSFC-Guangdong Joint Fund [U1501254];
   111 Project [B18008]
FX This work was supported in part by the International Cooperation and
   Exchange of the National Natural Science Foundation of China (NSFC)
   under Grant 61720106007, in part by the National Natural Science
   Foundation of China under Grant 61602049, in part by the NSFC-Guangdong
   Joint Fund under Grant U1501254, and in part by the 111 Project under
   Grant B18008. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR [Anonymous], 2018, NEUROINFORMATICS, DOI DOI 10.1007/s12021-018-9362-4
   [Anonymous], 2016, IEEE T PATTERN ANAL
   Aqmar MR, 2014, COMPUT VIS IMAGE UND, V126, P38, DOI 10.1016/j.cviu.2014.05.004
   Ariyanto G., 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117582
   Ben XY, 2016, NEUROCOMPUTING, V208, P153, DOI 10.1016/j.neucom.2016.01.098
   Chen SH, 2018, INT C PATT RECOG, P1169, DOI 10.1109/ICPR.2018.8545436
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Connie T, 2017, IEEE T CYBERNETICS, V47, P1395, DOI 10.1109/TCYB.2016.2545693
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Goffredo M, 2010, IEEE T SYST MAN CY B, V40, P997, DOI 10.1109/TSMCB.2009.2031091
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ruiz AH, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1087, DOI 10.1145/3123266.3123299
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Khamsemanan N, 2018, IEEE T INF FOREN SEC, V13, P119, DOI 10.1109/TIFS.2017.2738611
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2010, PROC CVPR IEEE, P974, DOI 10.1109/CVPR.2010.5540113
   Lam THW, 2011, PATTERN RECOGN, V44, P973, DOI 10.1016/j.patcog.2010.10.011
   Li SQ, 2018, IEEE INT CON MULTI
   Lombardi S, 2013, IEEE I CONF COMP VIS, P1041, DOI 10.1109/ICCV.2013.133
   Ma HD, 2018, IEEE MULTIMEDIA, V25, P76, DOI 10.1109/MMUL.2017.265091429
   Makihara Y, 2017, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR.2017.718
   McLaughlin N, 2016, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2016.148
   Montero-Odasso M, 2005, J GERONTOL A-BIOL, V60, P1304, DOI 10.1093/gerona/60.10.1304
   Muramatsu D, 2015, IEEE T IMAGE PROCESS, V24, P140, DOI 10.1109/TIP.2014.2371335
   Padole C, 2017, PATTERN ANAL APPL, V20, P73, DOI 10.1007/s10044-015-0468-0
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Shen C, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1942, DOI 10.1145/3123266.3123452
   Sivapalan S, 2013, IEEE COMPUT SOC CONF, P125, DOI 10.1109/CVPRW.2013.26
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Tang J, 2017, IEEE T IMAGE PROCESS, V26, P7, DOI 10.1109/TIP.2016.2612823
   Wei L, 2015, AAAI CONF ARTIF INTE, P1882
   Wu HM, 2018, J VIS COMMUN IMAGE R, V55, P424, DOI 10.1016/j.jvcir.2018.06.019
   Xing XL, 2016, PATTERN RECOGN, V50, P107, DOI 10.1016/j.patcog.2015.08.011
   Xiong F, 2014, LECT NOTES COMPUT SC, V8695, P1, DOI 10.1007/978-3-319-10584-0_1
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P146, DOI 10.1145/3123266.3123327
   Yu SQ, 2006, INT C PATT RECOG, P441
   Yu SQ, 2017, NEUROCOMPUTING, V239, P81, DOI 10.1016/j.neucom.2017.02.006
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang ZX, 2018, IEEE T CYBERNETICS, V48, P2935, DOI 10.1109/TCYB.2017.2752759
   Zhao N, 2016, PROCEEDINGS OF 2016 8TH INTERNATIONAL CONFERENCE ON MODELLING, IDENTIFICATION & CONTROL (ICMIC 2016), P989, DOI 10.1109/ICMIC.2016.7804258
   Zhao SC, 2018, IEEE T CIRC SYST VID, V28, P1839, DOI 10.1109/TCSVT.2017.2682196
NR 50
TC 32
Z9 35
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2361
EP 2375
DI 10.1109/TMM.2019.2900134
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200016
DA 2024-07-18
ER

PT J
AU Zhu, Y
   Deng, XQ
   Newsam, S
AF Zhu, Yi
   Deng, Xueqing
   Newsam, Shawn
TI Fine-Grained Land Use Classification at the City Scale Using
   Ground-Level Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo-referenced images; land use classification; convolutional neural
   networks; proximate sensing
ID FEATURES; RECOMMENDATION; CONTEXT; PHOTOS; COVER
AB Multimedia researchers have exploited large collections of community-contributed geo-referenced images to better understand a particular image, such as its subject matter or where it was taken, as well as to better understand a geographic location, such as the most visited tourist spots in a city or what the local cuisine is like. The goal of this paper is to better understand location. In particular, we use geo-referenced image collections to better understand what occurs in different parts of a city at fine spatial and activity class scales. This problem is known as land use mapping in the geographical sciences. We propose a novel framework to perform fine-grained land use mapping at the city scale using ground-level images. Mapping land use is considerably more difficult than mapping land cover and is generally not possible using overhead imagery as it requires close-up views and seeing inside buildings. We postulate that the growing collections of geo-referenced, ground-level images suggest an alternate approach to this geographic knowledge discovery problem. We develop a general framework that uses Flickr images to map 45 different land-use classes for the city of San Francisco, CA, USA. Individual images are classified using a novel convolutional neural network containing two streams: one for recognizing objects and another for recognizing scenes. This network is trained in an end-to-end manner directly on the labeled training images. We propose several novel strategies to overcome the noisiness of our user-generated data including search-based training set augmentation and online adaptive training. We derive a ground truth map of San Francisco in order to evaluate our method. We demonstrate the effectiveness of our approach through geovisualization and quantitative analysis. Our framework achieves over 29% recall at the individual land parcel level that represents a strong baseline for the challenging 45-way land use classification problem, especially given the noisiness of the image data.
C1 [Zhu, Yi; Deng, Xueqing; Newsam, Shawn] Univ Calif Merced, Dept Elect Engn & Comp Sci, Merced, CA 95343 USA.
C3 University of California System; University of California Merced
RP Zhu, Y (corresponding author), Univ Calif Merced, Dept Elect Engn & Comp Sci, Merced, CA 95343 USA.
EM yzhu25@ucmerced.edu; xdeng7@ucmerced.edu; snewsam@ucmerced.edu
OI Newsam, Shawn/0000-0001-6803-5291; Zhu, Yi/0000-0002-6482-6712
FU National Science Foundation CAREER [IIS-1150115]; Seed Grant from the
   Center for Information Technology in the Interest of Society; NVIDIA
   Corporation
FX This work was supported in part by the National Science Foundation
   CAREER Grant IIS-1150115, in part by the Seed Grant from the Center for
   Information Technology in the Interest of Society, and in part by the
   NVIDIA Corporation through the donation of the Titan X GPUs used in this
   work.
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P ACM SIGSPATIAL INT
   [Anonymous], 2017, ARXIV170900029
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P ACM SIGSPATIAL INT
   [Anonymous], 2015, ARXIV
   [Anonymous], GEOGRAPH BRITAIN IRE
   [Anonymous], J APPL REMOTE SENS
   [Anonymous], 2020, SAN FRANCISCO OPEN D
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], 2015, Proceedings of the 23rd SIGSPATIAL International Conference on Advances in Geographic Information Systems, page
   [Anonymous], 2014, P NIPS
   [Anonymous], CHINA REMOTE SENS
   [Anonymous], ARXIV161106474
   Ayeh JK, 2012, INFORMATION AND COMMUNICATION TECHNOLOGIES IN TOURISM 2012, P1
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1388, DOI 10.1109/TMM.2013.2250492
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fisher P., 2005, Representing GIS
   Hang RL, 2017, IEEE J-STARS, V10, P2002, DOI 10.1109/JSTARS.2017.2658948
   Hang RL, 2016, IEEE T GEOSCI REMOTE, V54, P783, DOI 10.1109/TGRS.2015.2465899
   Hays J, 2008, PROC CVPR IEEE, P3436
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Hu JW, 2017, AER ADV ENG RES, V126, P1
   Hu TY, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8020151
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang S, 2015, COMPUT ENVIRON URBAN, V53, P36, DOI 10.1016/j.compenvurbsys.2014.12.001
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kang J, 2018, ISPRS J PHOTOGRAMM, V145, P44, DOI 10.1016/j.isprsjprs.2018.02.006
   Leung D, 2010, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2010.5540040
   Li MM, 2017, IEEE J-STARS, V10, P4930, DOI 10.1109/JSTARS.2017.2737702
   Li SL, 2010, MODELLING SIMULATION, P270
   Li XC, 2018, IEEE T MULTIMEDIA, V20, P1179, DOI 10.1109/TMM.2017.2763323
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Liu Q., 2016, ARXIV161103589
   Liu QS, 2018, IEEE T GEOSCI REMOTE, V56, P117, DOI 10.1109/TGRS.2017.2743243
   Liu XP, 2017, INT J GEOGR INF SCI, V31, P1675, DOI 10.1080/13658816.2017.1324976
   Liu YL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0157728
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luus FPS, 2015, IEEE GEOSCI REMOTE S, V12, P2448, DOI 10.1109/LGRS.2015.2483680
   Marmanis D, 2016, IEEE GEOSCI REMOTE S, V13, P105, DOI 10.1109/LGRS.2015.2499239
   Marsal-Llacuna ML, 2014, J URBAN TECHNOL, V21, P39, DOI 10.1080/10630732.2014.884385
   Oba H, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P320, DOI 10.1109/ISM.2014.78
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Paldino S, 2015, EPJ DATA SCI, V4, DOI 10.1140/epjds/s13688-015-0043-3
   Pei T, 2014, INT J GEOGR INF SCI, V28, P1988, DOI 10.1080/13658816.2014.913794
   Penatti Otavio A. B., 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P44, DOI 10.1109/CVPRW.2015.7301382
   Qian XM, 2017, IEEE T MULTIMEDIA, V19, P813, DOI 10.1109/TMM.2016.2638207
   Rawat JS, 2015, EGYPT J REMOTE SENS, V18, P77, DOI 10.1016/j.ejrs.2015.02.002
   Shekhar S, 2002, IEEE T MULTIMEDIA, V4, P174, DOI 10.1109/TMM.2002.1017732
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K., 2014, 14091556 ARXIV
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Song YF, 2016, IEEE T MULTIMEDIA, V18, P1542, DOI 10.1109/TMM.2016.2568743
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang J, 2018, IEEE T MULTIMEDIA, V20, P1008, DOI 10.1109/TMM.2017.2760627
   Theobald DM, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094628
   Tracewski L, 2017, GEO-SPAT INF SCI, V20, P252, DOI 10.1080/10095020.2017.1373955
   Untenecker J, 2016, LAND USE POLICY, V57, P164, DOI 10.1016/j.landusepol.2016.04.016
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Weng Q, 2017, IEEE GEOSCI REMOTE S, V14, P704, DOI 10.1109/LGRS.2017.2672643
   Wickham JD, 2013, REMOTE SENS ENVIRON, V130, P294, DOI 10.1016/j.rse.2012.12.001
   Workman S, 2017, IEEE I CONF COMP VIS, P2707, DOI 10.1109/ICCV.2017.293
   Wu Y, 2016, IEEE T MULTIMEDIA, V18, P2206, DOI 10.1109/TMM.2016.2614185
   Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Yamagata Y, 2013, APPL ENERG, V112, P1466, DOI 10.1016/j.apenergy.2013.01.061
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Yin YF, 2015, IEEE T MULTIMEDIA, V17, P1760, DOI 10.1109/TMM.2015.2458042
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang XM, 2016, IEEE T MULTIMEDIA, V18, P1855, DOI 10.1109/TMM.2016.2574122
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhang Y, 2016, IEEE T MULTIMEDIA, V18, P418, DOI 10.1109/TMM.2016.2520827
   Zhao B, 2017, IEEE GEOSCI REMOTE S, V14, P1436, DOI 10.1109/LGRS.2017.2691013
   Zhu Y, 2016, 24TH ACM SIGSPATIAL INTERNATIONAL CONFERENCE ON ADVANCES IN GEOGRAPHIC INFORMATION SYSTEMS (ACM SIGSPATIAL GIS 2016), DOI 10.1145/2996913.2996978
NR 81
TC 30
Z9 34
U1 4
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1825
EP 1838
DI 10.1109/TMM.2019.2891999
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700017
OA hybrid, Green Submitted
DA 2024-07-18
ER

PT J
AU Choi, O
   Son, YJ
   Lim, H
   Ahn, SC
AF Choi, Ouk
   Son, Young-Jun
   Lim, Hwasup
   Ahn, Sang Chul
TI Co-Recognition of Multiple Fingertips for Tabletop Human-Projector
   Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-projector interaction; depth camera; random forest; fingertip
ID HAND GESTURE RECOGNITION; TRACKING
AB We present a depth-based fingertip recognition method for interactive projectors. We use a depth camera attached to a projector, so it is possible to change the relative pose between the projector and the projection surface without manual recalibration. For detection and classification of fingertips, we propose using cascaded random forests boosted by our 3-D pose-normalized pixel-difference features. The ensemble probabilities from the cascaded random forests are used to define a score function of a subset of detected fingertips. By finding the subset maximizing the score function, the fingertips in the subset are correctly classified, and the remaining incorrectly detected fingertips are rejected. Experiments show that the proposed method outperforms conventional random forest and convolutional neural network classifiers. In addition, our developed applications show the advantage of the proposed method in assigning different roles to different fingers.
C1 [Choi, Ouk; Son, Young-Jun] Incheon Natl Univ, Dept Elect Engn, Incheon 22012, South Korea.
   [Lim, Hwasup; Ahn, Sang Chul] Korea Inst Sci & Technol, Ctr Imaging Media Res, Seoul 02792, South Korea.
C3 Incheon National University; Korea Institute of Science & Technology
   (KIST)
RP Choi, O (corresponding author), Incheon Natl Univ, Dept Elect Engn, Incheon 22012, South Korea.
EM ouk.choi@inu.ac.kr; yj.mi.son@gmail.com; hslim@kist.re.kr;
   asc@kist.re.kr
OI Choi, Ouk/0000-0001-9860-9145
FU Incheon National University Research Grant in 2016; Korea Institute of
   Science and Technology Institutional Program
FX This work was supported in part by the Incheon National University
   Research Grant in 2016 and in part by the Korea Institute of Science and
   Technology Institutional Program. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Hatice Gunes.
CR [Anonymous], INTELLIGENT USER INT
   [Anonymous], 2012, Decision forests: A unified framework for classification, regression, density estimation, manifold learning and semi-supervised learning
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Badshah A. M., 2016, P INT INT C CONC CON
   Baumann F, 2013, LECT NOTES COMPUT SC, V7944, P131
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Carfagni M, 2017, IEEE SENS J, V17, P4508, DOI 10.1109/JSEN.2017.2703829
   Chang HJ, 2016, COMPUT VIS IMAGE UND, V148, P87, DOI 10.1016/j.cviu.2016.01.010
   Chen C, 2004, U CALIF BERKELEY
   Erol A, 2007, COMPUT VIS IMAGE UND, V108, P52, DOI 10.1016/j.cviu.2006.10.012
   Ge LH, 2017, PROC CVPR IEEE, P5679, DOI 10.1109/CVPR.2017.602
   Guo H., 2016, ARXIV161207978
   Hackenberg G, 2011, P IEEE VIRT REAL ANN, P19, DOI 10.1109/VR.2011.5759431
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Huang YC, 2016, IEEE COMPUT SOC CONF, P370, DOI 10.1109/CVPRW.2016.53
   Intel, 2016, REALSENSE CAM SR300
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Koike H., 2000, CHI 2000 Conference Proceedings. Conference on Human Factors in Computing Systems. CHI 2000. The Future is Here, P121, DOI 10.1145/332040.332415
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee T, 2007, ELEVENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P83
   Li ZR, 2011, IEEE T MULTIMEDIA, V13, P155, DOI 10.1109/TMM.2010.2092421
   Liang H, 2014, IEEE T MULTIMEDIA, V16, P1241, DOI 10.1109/TMM.2014.2306177
   MacCormick J., 2000, IEEE European Conference on Computer Vision, P3, DOI DOI 10.1007/3-540-45053-X1
   Maiero J, 2017, IEEE T MULTIMEDIA, V19, P1521, DOI 10.1109/TMM.2017.2673410
   Malik S., 2004, ACM INT C MULTIMODAL, P289, DOI DOI 10.1145/1027933.1027980
   Melax S., 2013, P ACM SIGGRAPH S INT, P63, DOI DOI 10.1145/2448196.2448232
   Mhatre Madhura, 2015, INT J RES ENG TECHNO, V4, P108
   NIERBO, 2016, SC20 FING TOUCH PROJ
   Oberweger M, 2017, IEEE INT CONF COMP V, P585, DOI 10.1109/ICCVW.2017.75
   Oikonomidis I, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.101
   Oka K, 2002, IEEE COMPUT GRAPH, V22, P64, DOI 10.1109/MCG.2002.1046630
   Pan ZG, 2010, P IEEE VIRT REAL ANN, P219, DOI 10.1109/VR.2010.5444787
   Qian C., 2014, PROC CVPR IEEE, P1106, DOI DOI 10.1109/CVPR.2014.145
   Raheja J. L., 2011, 2011 Third International Conference on Computational Intelligence, Modelling and Simulation, P248, DOI 10.1109/CIMSim.2011.51
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Sanghi A., 2008, P INT CAR C DEV CIRC
   Sharp T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P3633, DOI 10.1145/2702123.2702179
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simon T, 2017, PROC CVPR IEEE, P4645, DOI 10.1109/CVPR.2017.494
   Slossberg R., 2015, P BRIT MACH VIS C BM
   Son Y.-J., 2016, P IEEE C CONS EL AS, P182
   SONY, 2017, XPER TOUCH
   Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stockman George, 2001, Computer Vision
   Sun X, 2015, PROC CVPR IEEE, P824, DOI 10.1109/CVPR.2015.7298683
   Tang DH, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.58
   Tang DH, 2015, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2015.380
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Von Hardenberg Christian, 2001, Proceedings of the 2001 workshop on Perceptive user interfaces, PUI'01, P1, DOI DOI 10.1145/971478.971513
   Wan CD, 2017, PROC CVPR IEEE, P1196, DOI 10.1109/CVPR.2017.132
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wellner P., 1993, COMMUN ACM, V36, P87
   Wu GL, 2017, IEEE T MULTIMEDIA, V19, P1730, DOI 10.1109/TMM.2017.2691538
   Wu GL, 2016, IEEE T MULTIMEDIA, V18, P978, DOI 10.1109/TMM.2016.2545401
   Yang DD, 2005, PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9, P4991
   Zhang X, 2013, IEEE MULTIMEDIA, V20, P85, DOI 10.1109/MMUL.2013.50
   Zhou ZH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3553
NR 60
TC 4
Z9 4
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1487
EP 1498
DI 10.1109/TMM.2018.2880608
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400012
DA 2024-07-18
ER

PT J
AU Li, J
   Xiang, Y
   Hou, JY
   Xu, D
AF Li, Jie
   Xiang, Yong
   Hou, Jingyu
   Xu, Dan
TI Non-Local Texture Optimization With Wasserstein Regularization Under
   Convolutional Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Texture synthesis; non-local operator; Wasserstein distance; convolution
   neural network
ID COMPLETION; STATISTICS
AB Example-based texture synthesis aims to generate a new texture from an exemplar texture and has long been drawing attention in the fields of computer graphics, computer vision, and image processing. Nevertheless, synthesizing structured textures remains a challenging task. Most previous methods rely on additional guidance channels, which encode the structured features of textures. However, estimating the guidance channel is very difficult, and often fails when a texture has unpronounced features. In this paper, we propose a novel texture synthesis method, based on non-local operators, which captures the long-range structure of a texture without the additional guidance channel. The synthesized texture is generated by minimizing non-local texture energy through an expectation-maximization like optimization algorithm. A statistical constraint based on the Wasserstein distance is also proposed to ensure that the synthesized texture preserves the global statistics of the exemplar texture. Extensive experiments show that the proposed method can stably handle textures with different scale structures.
C1 [Li, Jie; Xu, Dan] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
   [Xiang, Yong; Hou, Jingyu] Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia.
C3 Yunnan University; Deakin University
RP Xu, D (corresponding author), Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Yunnan, Peoples R China.
EM lightstr@outlook.com; yxiang@deakin.edu.au; jingyu.hou@deakin.edu.au;
   danxu@ynu.edu.cn
RI Xu, Dan/KPA-7396-2024
OI Xu, Dan/0000-0003-4602-3550; Hou, Jingyu/0000-0002-6403-9786; Li,
   Jie/0000-0002-2916-4960; Xiang, Yong/0000-0003-3545-7863
FU Research Natural Science Foundation of China [61163019, 61540062]; Key
   Research Foundation of Yunnan Province [2014FA021]; Scientific Research
   Fund and Industrialization Cultivation Project of Yunnan Provincial
   Education Department [2016CYH03]; Australian Research Council
   [LP170100458]; Research Innovation Fund for Graduate Students of Yunnan
   University; China Scholarship Council
FX This work was supported in part by the Research Natural Science
   Foundation of China under Grant 61163019 and Grant 61540062, in part by
   the Key Research Foundation of Yunnan Province under Grant 2014FA021, in
   part by the Scientific Research Fund and Industrialization Cultivation
   Project of Yunnan Provincial Education Department under Grant 2016CYH03,
   in part by the Australian Research Council under Grant LP170100458, and
   in part by the Research Innovation Fund for Graduate Students of Yunnan
   University. Thework of J. Liwas supported by the China Scholarship
   Council. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Fatih Porikli.
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2009, State of the Art in Example-Based Texture Synthesis R
   [Anonymous], 2001, Schooling for Tomorrow
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Berger G., 2017, P INT C LEARN REPR, P3234
   Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Burkard R., 2009, Assignment Problems
   Cornet E., 2011, RESYNTHESIZER PLUGIN
   Darabi S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185578
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Galerne B, 2011, IEEE T IMAGE PROCESS, V20, P257, DOI 10.1109/TIP.2010.2052822
   Gatys L., 2015, NIPS
   He KM, 2014, IEEE T PATTERN ANAL, V36, P2423, DOI 10.1109/TPAMI.2014.2330611
   Hidane M, 2016, IEEE T IMAGE PROCESS, V25, P3505, DOI 10.1109/TIP.2016.2571061
   Jamriska O, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766983
   Kaspar A, 2015, COMPUT GRAPH FORUM, V34, P349, DOI 10.1111/cgf.12565
   Kolár M, 2017, COMPUT GRAPH FORUM, V36, P189, DOI 10.1111/cgf.13118
   Kwatra V, 2005, ACM T GRAPHIC, V24, P795, DOI 10.1145/1073204.1073263
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Liu G, 2016, INT C PATT RECOG, P3234, DOI 10.1109/ICPR.2016.7900133
   Lockerman YD, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925964
   Peyre G, 2008, LECT NOTES COMPUT SC, V5304, P57, DOI 10.1007/978-3-540-88690-7_5
   Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983
   Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37
   Sendik O, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3015461
   Shechtman E, 2010, PROC CVPR IEEE, P615, DOI 10.1109/CVPR.2010.5540159
   Simonyan K., 2014, 14091556 ARXIV
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Tsai TH, 2011, IEEE T MULTIMEDIA, V13, P29, DOI 10.1109/TMM.2010.2091497
   Villani C., 2003, AMS GRADUATE STUDIES, P370
   Wexler Y, 2007, IEEE T PATTERN ANAL, V29, P463, DOI 10.1109/TPAMI.2007.60
   Wu RB, 2014, IEEE T VIS COMPUT GR, V20, P436, DOI 10.1109/TVCG.2013.113
   Yang C, 2017, PROC CVPR IEEE, P4076, DOI 10.1109/CVPR.2017.434
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420
   Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401
NR 38
TC 2
Z9 2
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1437
EP 1449
DI 10.1109/TMM.2018.2880604
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400008
DA 2024-07-18
ER

PT J
AU Wu, JY
   Tan, R
   Wang, M
AF Wu, Jiyan
   Tan, Rui
   Wang, Ming
TI Energy-Efficient Multipath TCP for Quality-Guaranteed Video Over
   Heterogeneous Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multipath TCP; energy efficiency; real-time video; quality-awareness;
   heterogeneous wireless networks
ID BANDWIDTH AGGREGATION; TRANSPORT PROTOCOL; CONGESTION CONTROL;
   TRANSMISSION; MANAGEMENT; COMMUNICATION; DIVERSITY; SCHEME
AB Prompted by technological advancements in wireless systems and handheld devices, concurrent multipath transfer is a promising solution to stream high-quality mobile videos in heterogeneous access medium. Multipath TCP (MPTCP) is a transport-layer protocol recommended by the Internet Engineering Task Force (IETF) for concurrent data transmission to multi-radio terminals. However, it is still challenging to stream high-quality real-time videos with the existing MPTCP solutions because of the tradeoff between energy efficiency and video quality. To deliver real-time video in an energy-efficient manner, this paper presents a Delay-Energy-quAlity-aware MPTCP (DEAM) solution. First, an analytical framework is developed to characterize the delay-constrained energy-quality tradeoff for multipath video delivery over heterogeneous access networks. Second, a subflow allocation algorithm is proposed to minimize the device energy consumption while achieving target video quality within the imposed deadline. The performance of the proposed DEAM is verified by means of extensive Exata emulations with real-time streaming videos. Evaluation results demonstrate that DEAM achieves appreciable improvements over the reference MPTCP solutions in mobile energy conservation and user-perceived video quality.
C1 [Wu, Jiyan] Tensor Vis Private Ltd, Dept Software Engn, Singapore 139953, Singapore.
   [Tan, Rui] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wang, Ming] Beijing Univ Posts & Telecommun, State Key Lab Networking & Switching Technol, Beijing 100876, Peoples R China.
C3 Nanyang Technological University; Beijing University of Posts &
   Telecommunications
RP Wu, JY (corresponding author), Tensor Vis Private Ltd, Dept Software Engn, Singapore 139953, Singapore.
EM wujiyan@ieee.org; tanrui@ntu.edu.sg; wangming_bupt@bupt.edu.cn
RI Chen, John/GPW-8839-2022
FU research project in Tensor Vision Private Ltd.
FX This research was supported by a research project in Tensor Vision
   Private Ltd. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Christian Timmerer.
CR [Anonymous], 2012, 9 USENIX S NETW SYST
   [Anonymous], 2013, P C INT MEAS C
   [Anonymous], MULTIPATH TCP LINUX
   [Anonymous], 2011, 6182 RFC
   [Anonymous], P 7 INT C MULT SYST
   [Anonymous], 2017, VIS NETW IND GLOB MO
   [Anonymous], 2007, STREAM CONTROL TRANS
   [Anonymous], 2002, COMPUTERS INTRACTABI
   [Anonymous], 2001, T1TR742001 ANSI
   [Anonymous], P 19 ANN INT C MOB C
   [Anonymous], 2013, 6824 RFC
   Aparicio-Pardo R, 2017, 2017 PROCEEDINGS OF THE 29TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 29), VOL 1, P28, DOI [10.23919/ITC.2017.8064336, 10.1109/ITC.29.89]
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Borgnat P, 2009, IEEE INFOCOM SER, P711, DOI 10.1109/INFCOM.2009.5061979
   Bui DH, 2013, REAL TIM SYST SYMP P, P57, DOI 10.1109/RTSS.2013.14
   Cai L, 2005, IEEE T MULTIMEDIA, V7, P339, DOI 10.1109/TMM.2005.843360
   Cen S, 2003, IEEE ACM T NETWORK, V11, P703, DOI 10.1109/TNET.2003.818187
   Chen SY, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P1291
   Cui Y, 2012, INT CON DISTR COMP S, P366, DOI 10.1109/ICDCS.2012.23
   Floyd S, 1999, IEEE ACM T NETWORK, V7, P458, DOI 10.1109/90.793002
   Freris NM, 2013, IEEE ACM T NETWORK, V21, P469, DOI 10.1109/TNET.2012.2203608
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Han B, 2016, PROCEEDINGS OF THE 12TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'16), P129, DOI 10.1145/2999572.2999606
   Han HZ, 2006, IEEE ACM T NETWORK, V14, P1260, DOI 10.1109/TNET.2006.886738
   Harjula E, 2012, CONSUM COMM NETWORK, P532, DOI 10.1109/CCNC.2012.6181134
   Hasslinger G, 2013, IEEE INFOCOM SER, P1438
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   Hesmans B., 2015, P 11 ACM C EMERGING, P28
   Ho D, 2018, IEEE T MOBILE COMPUT, V17, P1090, DOI 10.1109/TMC.2017.2748592
   Hoque MA, 2014, IEEE COMMUN SURV TUT, V16, P579, DOI 10.1109/SURV.2012.111412.00051
   Huang J., 2012, P 10 INT C MOB SYST, P225, DOI DOI 10.1145/2307636.2307658
   Ismail M, 2014, IEEE T WIREL COMMUN, V13, P4616, DOI 10.1109/TWC.2014.2320726
   Ismail M, 2013, IEEE T WIREL COMMUN, V12, P3600, DOI 10.1109/TWC.2013.062713.130302
   ITU-T, 2008, ITU T P910 SUBJECTIV
   Jurca D, 2007, IEEE T MULTIMEDIA, V9, P629, DOI 10.1109/TMM.2006.888017
   Khalili R., 2012, P 8 INT C EMERGING N, P1
   Lee J, 2013, IEEE T MOBILE COMPUT, V12, P1737, DOI 10.1109/TMC.2012.139
   Li CL, 2014, IEEE T CIRC SYST VID, V24, P1170, DOI 10.1109/TCSVT.2014.2302517
   Li M, 2014, COMPUT NETW, V64, P1, DOI 10.1016/j.comnet.2014.01.011
   Lim YS, 2014, IEEE INFOCOM SER, P1815, DOI 10.1109/INFOCOM.2014.6848120
   Mamatha AS, 2013, 2013 IEEE RECENT ADVANCES IN INTELLIGENT COMPUTATIONAL SYSTEMS (RAICS), P193, DOI 10.1109/RAICS.2013.6745472
   Oliveira T, 2011, IEEE INFOCOM SER, P2390, DOI 10.1109/INFCOM.2011.5935059
   Peng QY, 2014, MOBIHOC'14: PROCEEDINGS OF THE 15TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P257, DOI 10.1145/2632951.2632971
   Piamrat K, 2011, COMPUT COMMUN, V34, P1066, DOI 10.1016/j.comcom.2010.02.015
   Raiciu C., 2011, 6356 RFC INT ENG TAS
   Sharma V, 2012, IEEE ACM T NETWORK, V20, P1024, DOI 10.1109/TNET.2011.2181979
   Song W., 1985, IEEE T WIREL COMMUN, V11, P1554
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Van der Auwera G, 2009, IEEE T BROADCAST, V55, P541, DOI 10.1109/TBC.2009.2027399
   Wenger S., 2005, 3984 RFC INT ENG TAS
   Wenjie Hu, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1185, DOI 10.1109/INFOCOM.2015.7218493
   Wischik D., 2011, USENIX NSDI, P99
   Wu JY, 2016, INT CON DISTR COMP S, P487, DOI 10.1109/ICDCS.2016.25
   Wu JY, 2016, IEEE T MOBILE COMPUT, V15, P2345, DOI 10.1109/TMC.2015.2497238
   Wu JY, 2016, IEEE T CIRC SYST VID, V26, P711, DOI 10.1109/TCSVT.2015.2412774
   Wu JY, 2013, IEEE GLOB COMM CONF, P1723, DOI 10.1109/GLOCOM.2013.6831322
   Wu JY, 2015, IEEE T PARALL DISTR, V26, P2286, DOI 10.1109/TPDS.2014.2347031
   Wu JY, 2015, IEEE T MOBILE COMPUT, V14, P688, DOI 10.1109/TMC.2014.2334592
   Wu JY, 2014, WIRELESS PERS COMMUN, V75, P1265, DOI 10.1007/s11277-013-1422-3
   Wu JY, 2013, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2013-283
   Wu R, 2014, IEEE ASIAN SOLID STA, P181
   Xing M, 2014, IEEE J SEL AREA COMM, V32, P795, DOI 10.1109/JSAC.2014.140411
   Xu CQ, 2015, IEEE T CIRC SYST VID, V25, P1175, DOI 10.1109/TCSVT.2014.2376138
   Yoon J, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P209
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
   Zhu XQ, 2009, IEEE T MULTIMEDIA, V11, P752, DOI 10.1109/TMM.2009.2017641
NR 66
TC 24
Z9 25
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1593
EP 1608
DI 10.1109/TMM.2018.2879748
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400020
DA 2024-07-18
ER

PT J
AU Yi, SY
   Liang, YY
   He, ZY
   Li, Y
   Cheung, YM
AF Yi, Shuangyan
   Liang, Yingyi
   He, Zhenyu
   Li, Yi
   Cheung, Yiu-Ming
TI Dual Pursuit for Subspace Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Low-rank representation; dual pursuit; graph-regularization technique
ID NONNEGATIVE LOW-RANK; MOTION SEGMENTATION; SALIENCY DETECTION; SPARSE
   GRAPH; REPRESENTATION
AB In general, low-rank representation (LRR) aims to find the lowest rank representation with respect to a dictionary. In fact, the dictionary is a key aspect of low-rank representation. However, a lot of low-rank representation methods usually use the data itself as a dictionary (i.e., a fixed dictionary), which may degrade their performances due to the lack of clustering ability of a fixed dictionary. To this end, we propose learning a locality-preserving dictionary instead of the fixed dictionary for low-rank representation, where the locality-preserving dictionary is constructed by using a graph regularization technique to capture the intrinsic geometric structure of the dictionary and, hence, the locality-preserving dictionary has an underlying clustering ability. In this way, the obtained low-rank representation via the locality-preserving dictionary has a better grouping-effect representation. Inversely, a better grouping-effect representation can help to learn a good dictionary. The locality-preserving dictionary and the grouping-effect representation interact with each other, where dual pursuit is called. The proposed method, namely, Dual Pursuit for Subspace Learning, provides us with a robust method for clustering and classification simultaneously, and compares favorably with the other state-of-the-art methods.
C1 [Yi, Shuangyan; Liang, Yingyi; He, Zhenyu; Li, Yi] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Cheung, Yiu-Ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
   [Cheung, Yiu-Ming] Hong Kong Baptist Univ, Inst Res & Continuing Educ, Hong Kong, Peoples R China.
   [Cheung, Yiu-Ming] Beijing Normal Univ HKBU, United Int Coll, Zhuhai 519000, Peoples R China.
C3 Harbin Institute of Technology; Hong Kong Baptist University; Hong Kong
   Baptist University; Beijing Normal University - Hong Kong Baptist
   University United International College
RP He, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM shuangyanshuangfei@163.com; liangyingyi002@foxmail.com;
   zhenyuhe@hit.edu.cn; ly_res@163.com; ymc@comp.hkbu.edu.hk
RI Liang, Yingyi/AAB-7976-2020; Cheung, Yiu-ming/E-2050-2015
OI Liang, Yingyi/0000-0001-8196-2348; Cheung, Yiu-ming/0000-0001-7629-4648
FU National Natural Science Foundation of China [61672183, 61272366,
   61672444, 61772141]; Natural Science Foundation of Guangdong Province
   [2016B090918047]; Shenzhen Research Council [JCYJ20170413104556946,
   JCYJ20160406161948211, JCYJ20160226201453085, JSGG20150331152017052,
   JCYJ20160531194006833]; Shenzhen science and technology plan
   [KJYY20170724152625446]; Scientific Research Platform Cultivation
   Project of SZIIT [PT201704]; Faculty Research Grant of Hong Kong Baptist
   University [FRG2/17-18/082]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61672183, Grant 61272366, Grant
   61672444, Grant 61772141, in part by the Natural Science Foundation of
   Guangdong Province under Grant 2016B090918047, in part by the Shenzhen
   Research Council under Grant JCYJ20170413104556946, Grant
   JCYJ20160406161948211, Grant JCYJ20160226201453085, Grant
   JSGG20150331152017052, Grant JCYJ20160531194006833, in part by the
   Shenzhen science and technology plan underGrant KJYY20170724152625446,
   in part by the Scientific Research Platform Cultivation Project of SZIIT
   (PT201704), and in part by the Faculty Research Grant of Hong Kong
   Baptist University under Grant FRG2/17-18/082. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Dong Xu.
CR [Anonymous], 2002, THESIS STANFORD U
   Bao BK, 2012, IEEE T IMAGE PROCESS, V21, P3794, DOI 10.1109/TIP.2012.2192742
   Cai D, 2007, IEEE DATA MINING, P73, DOI 10.1109/ICDM.2007.89
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen CF, 2012, PROC CVPR IEEE, P2618, DOI 10.1109/CVPR.2012.6247981
   Chen L, 2013, C ELECT INSUL DIEL P, P164, DOI 10.1109/CEIDP.2013.6748232
   Chen X., 2017, IEEE ACM T COMPUT BI, V52, P2
   Chen Y, 2018, IEEE T MULTIMEDIA, V20, P1823, DOI 10.1109/TMM.2017.2775220
   Chung F. R. K., 1997, AM MATH SOC, V92, DOI DOI 10.1090/CBMS/092
   Ding ZM, 2016, AAAI CONF ARTIF INTE, P1181
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Frank A, 2005, NEUROLOGIA, P2
   Gu QQ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P359
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Hu H, 2014, PROC CVPR IEEE, P3834, DOI 10.1109/CVPR.2014.484
   Huang D, 2012, LECT NOTES COMPUT SC, V7575, P616, DOI 10.1007/978-3-642-33765-9_44
   Jian M, 2016, IEEE T MULTIMEDIA, V18, P458, DOI 10.1109/TMM.2016.2515367
   Lang CY, 2016, IEEE T NEUR NET LEAR, V27, P1190, DOI 10.1109/TNNLS.2015.2513393
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Lin ZW, 2009, CHINA PET PROCESS PE, P9
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2017, IEEE T PATTERN ANAL, V39, P47, DOI 10.1109/TPAMI.2016.2539946
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu JM, 2014, IEEE T IMAGE PROCESS, V23, P4022, DOI 10.1109/TIP.2014.2343458
   Liu Q, 2010, 2010 THIRD INTERNATIONAL SYMPOSIUM ON INTELLIGENT INFORMATION TECHNOLOGY AND SECURITY INFORMATICS (IITSI 2010), P663, DOI 10.1109/IITSI.2010.40
   Lu XQ, 2013, IEEE T GEOSCI REMOTE, V51, P4009, DOI 10.1109/TGRS.2012.2226730
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Luo CZ, 2016, IEEE T MULTIMEDIA, V18, P40, DOI 10.1109/TMM.2015.2495248
   Ma L, 2012, PROC CVPR IEEE, P2586, DOI 10.1109/CVPR.2012.6247977
   Rao S, 2010, IEEE T PATTERN ANAL, V32, P1832, DOI 10.1109/TPAMI.2009.191
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shao M, 2014, INT J COMPUT VISION, V109, P74, DOI 10.1007/s11263-014-0696-6
   Talwalkar A, 2013, IEEE I CONF COMP VIS, P3543, DOI 10.1109/ICCV.2013.440
   Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974
   Wen ZW, 2010, MATH PROGRAM COMPUT, V2, P203, DOI 10.1007/s12532-010-0017-1
   Wright J, 2009, ADV NEURAL INFORM PR, P2080, DOI DOI 10.1109/NNSP.2000.889420
   Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94
   Yan YG, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3252
   Yang JF, 2010, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL SYMPOSIUM ON STRUCTURAL ENGINEERING, VOL I AND II, P19
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Ye YM, 2013, PATTERN RECOGN, V46, P769, DOI 10.1016/j.patcog.2012.09.005
   Yin M, 2016, IEEE T PATTERN ANAL, V38, P504, DOI 10.1109/TPAMI.2015.2462360
   Yin M, 2015, IEEE T IMAGE PROCESS, V24, P4918, DOI 10.1109/TIP.2015.2472277
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhang TZ, 2013, IEEE I CONF COMP VIS, P281, DOI 10.1109/ICCV.2013.42
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhuang LS, 2015, IEEE T IMAGE PROCESS, V24, P3717, DOI 10.1109/TIP.2015.2441632
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 52
TC 19
Z9 19
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1399
EP 1411
DI 10.1109/TMM.2018.2877888
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400005
DA 2024-07-18
ER

PT J
AU Yang, CL
   Yue, YF
   Zhang, J
   Wen, MX
   Wang, DW
AF Yang, Chule
   Yue, Yufeng
   Zhang, Jun
   Wen, Mingxing
   Wang, Danwei
TI Probabilistic Reasoning for Unique Role Recognition Based on the Fusion
   of Semantic-Interaction and Spatio-Temporal Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Probabilistic inference; multimodal information fusion; decision making;
   unique role recognition
ID SOCIAL ROLES; KNOWLEDGE
AB This paper deals with the problem of recognizing the unique role in dynamic environments. Different from social roles, the unique role refers to those who are unusual in their carrying items or movements in the scene. In this paper, we propose a hierarchical probabilistic reasoning method that relates spatial relationships between interested objects and humans with their temporal changes to recognize the unique individual. Two observation models, Object Existence Model (OEM) and Human Action Model (HAM), are established to support role inference by analyzing the corresponding semantic-interaction features and spatio-temporal features. Then, OEM and HAM results of each person are compared with the overall distribution in the scene, respectively. Finally, we can determine the role through the fusion of two observation models. Experiments are conducted in both indoor and outdoor environments concerning different settings, degrees of clutter, and occlusions. The results show that the proposed method can adapt to a variety of scenarios and outperforms othermethods on accuracy and robustness, moreover, exhibiting stable performance even in complex scenes.
C1 [Yang, Chule; Yue, Yufeng; Zhang, Jun; Wen, Mingxing; Wang, Danwei] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Yue, YF (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM yang0438@e.ntu.edu.sg; yyue001@e.ntu.edu.sg; jzhang061@e.ntu.edu.sg;
   mingxing001@e.ntu.edu.sg; edwwang@ntu.edu.sg
RI zhang, jun/M-5962-2016; wang, dan/JEF-0836-2023; Yue,
   Yufeng/AAE-1720-2019; Wen, Mingxing/KFC-0310-2024
OI zhang, jun/0000-0001-7406-5243; 
FU ST Engineering-NTU Corporate Laboratory through the NRF Corporate
   Laboratory@UniversityScheme
FX This work was supported in part by the ST Engineering-NTU Corporate
   Laboratory through the NRF Corporate Laboratory@UniversityScheme.
CR Aloulou H, 2015, KNOWL-BASED SYST, V77, P16, DOI 10.1016/j.knosys.2014.12.025
   [Anonymous], CVPR 2011 WORKSHOPS, DOI DOI 10.1109/CVPRW.2011.5981811
   BIDDLE BJ, 1986, ANNU REV SOCIOL, V12, P67, DOI 10.1146/annurev.so.12.080186.000435
   Borges PVK, 2013, IEEE T CIRC SYST VID, V23, P1993, DOI 10.1109/TCSVT.2013.2270402
   Cagalaban G, 2010, LECT NOTES ARTIF INT, V6232, P103, DOI 10.1007/978-3-642-15037-1_10
   Charalampous K, 2017, ROBOT AUTON SYST, V93, P85, DOI 10.1016/j.robot.2017.03.002
   Cunningham AG, 2015, IEEE INT CONF ROBOT, P1670, DOI 10.1109/ICRA.2015.7139412
   Dapoigny R, 2013, INFORM FUSION, V14, P87, DOI 10.1016/j.inffus.2012.02.006
   Dondrup C, 2016, IEEE ROMAN, P586, DOI 10.1109/ROMAN.2016.7745177
   Everingham M, 2012, PASCAL VISUAL OBJECT
   Fu CH, 2015, INT CONF UNMAN AIRCR, P957, DOI 10.1109/ICUAS.2015.7152384
   Geiger A, 2012, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2012.6224570
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Huang JJ, 2017, LECT NOTES COMPUT SC, V10425, P218, DOI 10.1007/978-3-319-64698-5_19
   Ramírez OAI, 2016, IEEE ROMAN, P347, DOI 10.1109/ROMAN.2016.7745154
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Khaleghi B, 2013, INFORM FUSION, V14, P28, DOI 10.1016/j.inffus.2011.08.001
   Kollar T, 2009, IEEE INT CONF ROBOT, P4116
   Lan T, 2012, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2012.6247821
   Leite I, 2013, INT J SOC ROBOT, V5, P291, DOI 10.1007/s12369-013-0178-y
   Lin K, 2015, IEEE T INF FOREN SEC, V10, P1359, DOI 10.1109/TIFS.2015.2408263
   Lin YW, 2017, IEEE T CIRC SYST VID, V27, P1208, DOI 10.1109/TCSVT.2016.2527258
   Little EG, 2009, INFORM FUSION, V10, P70, DOI 10.1016/j.inffus.2008.05.006
   Liu DT, 2015, INT J MULTIMED DATA, V6, P1, DOI 10.4018/ijmdem.2015010101
   Mehta D, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P1190, DOI 10.1109/IROS.2016.7759200
   Meyer Johannes, 2011, RoboCup 2010: Robot Soccer World Cup XIV, P180, DOI 10.1007/978-3-642-20217-9_16
   Minaeian S, 2016, IEEE T SYST MAN CY-S, V46, P1005, DOI 10.1109/TSMC.2015.2491878
   Oh J, 2015, AAAI CONF ARTIF INTE, P1371
   Pronobis  A., 2010, 11 INT C INT AUT SYS, P133
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ruiz-Sarmiento JR, 2015, KNOWL-BASED SYST, V86, P131, DOI 10.1016/j.knosys.2015.05.032
   Salamin H, 2012, IEEE T MULTIMEDIA, V14, P338, DOI 10.1109/TMM.2011.2173927
   Sapru A, 2015, IEEE T MULTIMEDIA, V17, P746, DOI 10.1109/TMM.2015.2408437
   Shen SJ, 2013, IEEE INT CONF ROBOT, P1758, DOI 10.1109/ICRA.2013.6630808
   Sheng WH, 2015, ROBOT AUTON SYST, V68, P47, DOI 10.1016/j.robot.2015.02.002
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Smirnov A, 2015, INFORM FUSION, V21, P114, DOI 10.1016/j.inffus.2013.10.010
   Snidaro L, 2015, INFORM FUSION, V25, P16, DOI 10.1016/j.inffus.2015.01.002
   Song Z, 2011, IEEE I CONF COMP VIS, P1084, DOI 10.1109/ICCV.2011.6126355
   Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754
   Vidas S, 2013, ENERG BUILDINGS, V66, P445, DOI 10.1016/j.enbuild.2013.07.030
   Vidas S, 2013, IEEE INT CONF ROBOT, P2311, DOI 10.1109/ICRA.2013.6630890
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Williams C.K.I., PASCAL VISUAL OBJECT
   Yang CL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (IEEE ROBIO 2017), P159, DOI 10.1109/ROBIO.2017.8324411
   Yue YF, 2018, IEEE SENS J, V18, P8933, DOI 10.1109/JSEN.2018.2867854
   Yue YF, 2018, 2018 21ST INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P488, DOI 10.23919/ICIF.2018.8455670
   2015, INT J COMPUT VISION, V112, P133, DOI DOI 10.1007/S11263-014-0777-6
   2017, IEEE T CIRCUITS SYST, V27, P683, DOI DOI 10.1109/TCSVT.2016.2589859
   2015, EXPERT SYST APPL, V42, P8805, DOI DOI 10.1016/J.ESWA.2015.07.033
   2013, ROBOT AUTON SYST, V61, P1726, DOI DOI 10.1016/J.ROBOT.2013.05.007
   2016, J SYST SOFTW, V117, P55, DOI DOI 10.1016/J.JSS.2016.02.010
NR 57
TC 3
Z9 3
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1195
EP 1208
DI 10.1109/TMM.2018.2875513
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Mo, HY
   Liu, LB
   Zhu, WP
   Yin, SY
   Wei, SJ
AF Mo, Huiyu
   Liu, Leibo
   Zhu, Wenping
   Yin, Shouyi
   Wei, Shaojun
TI Face Alignment With Expression- and Pose-Based Adaptive Initialization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive initial shape; expression recognition; facial alignment; pose
   estimation
ID REGRESSION; FORESTS
AB Face alignment is a critical task in many multimedia and vision applications that use face-based algorithms. Recent research has focused on achieving efficient initialization to improve performance; however, the use of facial attributes and the extent of their correlation with initialization have not been fully exploited. This paper presents a lightweight method called expression- and pose-based adaptive initialization (EXPAI), in which facial attributes, that is, expression and pose information, are used as priors. This approach can significantly improve the face alignment performance. In addition, reliable expression and head pose information can be derived simultaneously in the same framework. First, an expression-and pose-based template dictionary is formed by augmenting the mean shape across three degrees of freedom, thereby substantially improving the robustness of the initial shape with respect to large head pose variations. Second, each the template corresponds to an image of interest, which is jointly determined using a shape-constrained multiclass classifier and binary classifiers, and is assigned a pretrained confidence coefficient. The initial shape that is thus generated for subsequent cascaded regression is more adaptive and enables higher accuracy. Furthermore, EXPAI enables initialization with significantly increased computational efficiency because of its independence from the original dataset. The experimental results obtained on the widely used 300-W dataset show that our method achieves very competitive performance compared with that of state-of-the-art methods. In particular, for the challenging subset of 300-W, EXPAI reduces errors by more than 14% compared with coarse-to-fine shape searching (CFSS), which currently exhibits the best performance among regression-based approaches. Furthermore, a speed increase of more than 10 times compared with CFSS is achieved.
C1 [Mo, Huiyu; Liu, Leibo; Zhu, Wenping; Yin, Shouyi; Wei, Shaojun] Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
   [Zhu, Wenping] Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of
   Semiconductors, CAS
RP Liu, LB (corresponding author), Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
EM mohy15@mails.tsinghua.edu.cn; liulb@tsinghua.edu.cn; zhuwp@semi.ac.cn;
   yinsy@tsinghua.edu.cn; wsj@tsinghua.edu.cn
RI Liu, Leibo/AAE-1973-2020; Mo, Huiyu/AAC-6418-2019
OI Liu, Leibo/0000-0001-7548-4116; Mo, Huiyu/0000-0002-3373-7178
FU National High Technology Research and Development Program of China
   [2012AA012701]; National Natural Science Foundation of China [61672317]
FX This work was supported in part by the National High Technology Research
   and Development Program of China under Grant 2012AA012701 and in part by
   the National Natural Science Foundation of China under Grant 61672317.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Lei Zhang. (Corresponding author:
   Leibo Liu.)
CR [Anonymous], ROBUST INCREMENTAL C
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P IEEE INT C COMP VI
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Drouard V, 2015, IEEE IMAGE PROC, P4624, DOI 10.1109/ICIP.2015.7351683
   Elaiwat S, 2016, PATTERN RECOGN, V49, P152, DOI 10.1016/j.patcog.2015.07.006
   Fan X, 2018, IEEE T MULTIMEDIA, V20, P567, DOI 10.1109/TMM.2017.2751143
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Kowalski M, 2016, IEEE SIGNAL PROC LET, V23, P1567, DOI 10.1109/LSP.2016.2608139
   Kstinger M., 2012, IEEE INT C COMP VIS, P2144
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P1666, DOI 10.1109/TIP.2017.2657118
   Liu QS, 2016, IEEE T IMAGE PROCESS, V25, P700, DOI 10.1109/TIP.2015.2502485
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Ranjan R., 2018, IEEE T PATTERN ANAL, P1
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Shao XH, 2017, IEEE COMPUT SOC CONF, P2069, DOI 10.1109/CVPRW.2017.258
   Siddiqi MH, 2015, IEEE T IMAGE PROCESS, V24, P1386, DOI 10.1109/TIP.2015.2405346
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Tsai TJ, 2015, IEEE T MULTIMEDIA, V17, P1550, DOI 10.1109/TMM.2015.2454332
   Tuzel O, 2016, LECT NOTES COMPUT SC, V9909, P825, DOI 10.1007/978-3-319-46454-1_50
   Tzimiropoulos G, 2014, PROC CVPR IEEE, P1851, DOI 10.1109/CVPR.2014.239
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Zadeh A, 2017, IEEE COMPUT SOC CONF, P2051, DOI 10.1109/CVPRW.2017.256
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhan J, 2014, DES AUT CON, DOI 10.1145/2593069.2593165
   Zhang ZP, 2016, IEEE T PATTERN ANAL, V38, P918, DOI 10.1109/TPAMI.2015.2469286
   Zhao XW, 2014, PROC CVPR IEEE, P1765, DOI 10.1109/CVPR.2014.228
   Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu X., 2018, IEEE T PATTERN ANAL, P1
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
NR 45
TC 13
Z9 14
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 943
EP 956
DI 10.1109/TMM.2018.2867262
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700011
DA 2024-07-18
ER

PT J
AU Ren, C
   He, XH
   Nguyen, TQ
AF Ren, Chao
   He, Xiaohai
   Nguyen, Truong Q.
TI Adjusted Non-Local Regression and Directional Smoothness for Image
   Restoration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adjusted non-local regression; directional smoothness; image
   restoration; deblurring; super-resolution
ID SPLIT BREGMAN METHOD; SPARSE REPRESENTATION; SUPERRESOLUTION;
   REGULARIZATION; OPTIMIZATION; ALGORITHM
AB Image restoration (IR) problems are very important in many low-level vision tasks. Due to their ill-posed natures, image priors are widely used to regularize the solution spaces. Recently, patch-based non-local self-similarity has shown great potential in IR problems, leading to many effective non-local priors. Their performance largely depends on whether the non-local self-similarity of the underlying image can be fully exploited. However, most of these priors, including non-local regression (NLR), only utilize the center pixel of each patch to model the non-local feature, which is suboptimal. We propose an effective overlap-based non-local regression (ONLR) to fully exploit the non-local similar patches: first, the concept of overlap-based similar pixels group (OSPG) is introduced; second, for each pixel within an OSPG, the non-local weight is obtained via a novel similarity measurement method; third, based on the consistency assumption, the non-local fitting deviations (NLFDs) by using OSPGs are uniformly constrained. Because of the uniform constraints, the restoration may be poor in regions where OSPGs are not reliable. Consequently, a weighting scheme is proposed to measure the OSPG reliability, leading to a novel adjusted non-local regression (ANLR). In addition, the integral image technique (IIT) is adopted to speed up the similar patches search process. To further boost the ANLR, a local directional smoothness (DS) prior is proposed as a good complement of the non-local feature. Finally, a fast split Bregman iteration algorithm is designed to solve the ANLR-DS minimization problem. Extensive experiments on two typical IR problems, that is, image deblurring and super resolution, demonstrate the superiority of the proposed method compared to many state-of-the-art IR methods.
C1 [Ren, Chao; He, Xiaohai] Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
   [Nguyen, Truong Q.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 Sichuan University; University of California System; University of
   California San Diego
RP Ren, C (corresponding author), Sichuan Univ, Coll Elect & Informat Engn, Chengdu 610065, Sichuan, Peoples R China.
EM chaoren@scu.edu.cn; hxh@scu.edu.cn; tqn001@eng.ucsd.edu
RI Ren, Chao/L-8078-2019; Nguyen, Truong/JXN-9786-2024
OI Ren, Chao/0000-0002-5347-2728
FU National Natural Science Foundation of China [61471248]; National
   Postdoctoral Program for Innovative Talents of China [BX201700163];
   Postdoctoral Research and Development Foundation of Sichuan University
   [2017SCU12003]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61801316, in part by the National Postdoctoral Program
   for Innovative Talents of China under Grant BX201700163, in part by the
   Postdoctoral Research and Development Foundation of Sichuan University
   under Grant 2017SCU12003, and in part by the National Natural Science
   Foundation of China under Grant 61471248. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Enrico Magli. (Corresponding author: Chao Ren.)
CR [Anonymous], 2014, MATHEMATICS-BASEL, DOI DOI 10.48550/ARXIV.1402.4371
   [Anonymous], P EUR C COMPUT VIS
   [Anonymous], TR0710 CAAMM RIC U D
   Arias P, 2011, INT J COMPUT VISION, V93, P319, DOI 10.1007/s11263-010-0418-7
   Beck A, 2009, IEEE T IMAGE PROCESS, V18, P2419, DOI 10.1109/TIP.2009.2028250
   Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38
   Chantas G, 2010, IEEE T IMAGE PROCESS, V19, P351, DOI 10.1109/TIP.2009.2033398
   Chen HG, 2017, IEEE T MULTIMEDIA, V19, P1702, DOI 10.1109/TMM.2017.2688920
   Chen XW, 2013, PROC CVPR IEEE, P1902, DOI 10.1109/CVPR.2013.248
   Chierchia G, 2015, SIGNAL IMAGE VIDEO P, V9, P1737, DOI 10.1007/s11760-014-0664-1
   Cho TS, 2012, IEEE T PATTERN ANAL, V34, P683, DOI 10.1109/TPAMI.2011.166
   Couprie C, 2013, SIAM J IMAGING SCI, V6, P1246, DOI 10.1137/120895068
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Facciolo G, 2014, IMAGE PROCESS ON LIN, V4, P344, DOI 10.5201/ipol.2014.57
   GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331
   Gilboa G, 2008, MULTISCALE MODEL SIM, V7, P1005, DOI 10.1137/070698592
   Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P15, DOI 10.1109/TMM.2016.2599145
   Lefkimmiatis S, 2015, IEEE T COMPUT IMAG, V1, P16, DOI 10.1109/TCI.2015.2434616
   Li T, 2018, IEEE T MULTIMEDIA, V20, P1305, DOI 10.1109/TMM.2017.2766889
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Mignotte M, 2008, PATTERN RECOGN LETT, V29, P2206, DOI 10.1016/j.patrec.2008.08.004
   Milanfar P, 2013, IEEE SIGNAL PROC MAG, V30, P106, DOI 10.1109/MSP.2011.2179329
   Ng K.M., 2002, THESIS
   Ni J, 2011, IEEE T IMAGE PROCESS, V20, P3086, DOI 10.1109/TIP.2011.2145386
   Papyan V, 2016, IEEE T IMAGE PROCESS, V25, P249, DOI 10.1109/TIP.2015.2499698
   Portilla J, 2009, IEEE IMAGE PROC, P3909, DOI 10.1109/ICIP.2009.5413975
   Protter M, 2009, IEEE T IMAGE PROCESS, V18, P36, DOI 10.1109/TIP.2008.2008067
   Ren C, 2016, IEEE T IMAGE PROCESS, V25, P2168, DOI 10.1109/TIP.2016.2542442
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Salmon J, 2010, IEEE IMAGE PROC, P1929, DOI 10.1109/ICIP.2010.5650780
   Salmon J, 2012, SIGNAL PROCESS, V92, P477, DOI 10.1016/j.sigpro.2011.08.011
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Sun J., 2008, PROC IEEE C COMPUT V, P1
   Tai YW, 2010, PROC CVPR IEEE, P2400, DOI 10.1109/CVPR.2010.5539933
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   Tang S, 2014, SIGNAL PROCESS, V94, P339, DOI 10.1016/j.sigpro.2013.07.005
   Tikhonov A.N., 1963, SOV MATH DOKL, V5, P1035, DOI DOI 10.1111/J.1365-246X.2012.05699.X
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tofighi M, 2016, IEEE J-STSP, V10, P81, DOI 10.1109/JSTSP.2015.2502541
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang LF, 2013, IEEE T CIRC SYST VID, V23, P1289, DOI 10.1109/TCSVT.2013.2240915
   Wang YL, 2008, SIAM J IMAGING SCI, V1, P248, DOI 10.1137/080724265
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang Y, 2013, MATH COMPUT, V82, P2061, DOI 10.1090/S0025-5718-2013-02700-7
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang KB, 2013, IEEE T NEUR NET LEAR, V24, P1648, DOI 10.1109/TNNLS.2013.2262001
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379
   Zhang XF, 2016, IEEE T IMAGE PROCESS, V25, P4158, DOI 10.1109/TIP.2016.2588326
   Zhu SY, 2016, IEEE T MULTIMEDIA, V18, P1707, DOI 10.1109/TMM.2016.2593039
NR 59
TC 29
Z9 32
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 731
EP 745
DI 10.1109/TMM.2018.2866362
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800017
DA 2024-07-18
ER

PT J
AU Rente, PD
   Brites, C
   Ascenso, J
   Pereira, F
AF Rente, Paulo de Oliveira
   Brites, Catarina
   Ascenso, Joao
   Pereira, Fernando
TI Graph-Based Static 3D Point Clouds Geometry Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud coding; graph-based transform; octree coding; rendering;
   point cloud quality assessment
ID GAME-THEORY; COMPRESSION; REPRESENTATION; FIELD
AB Recently, 3D visual representation models such as light fields and point clouds are becoming popular due to their capability to represent the real world in a more complete and immersive way, paving the road for new and more advanced visual experiences. The point cloud representation model is able to efficiently represent the surface of objects/scenes by means of a set of 3D points and associated attributes and is increasingly being used from autonomous cars to augmented reality. Emerging imaging sensors have made it easier to perform richer and denser point cloud acquisitions, notably with millions of points, making it impossible to store and transmit these very high amounts of data without appropriate coding. This bottleneck has raised the need for efficient point cloud coding solutions in order to offer more immersive visual experiences and better quality of experience to the users. In this context, this paper proposes an efficient lossy coding solution for the geometry of static point clouds. The proposed coding solution uses an octree-based approach for a base layer and a graph-based transform approach for the enhancement layer where an Inter-layer residual is coded. The performance assessment shows very significant compression gains regarding the state-of-the-art, especially for the most relevant lower and medium rates.
C1 [Rente, Paulo de Oliveira; Brites, Catarina; Ascenso, Joao; Pereira, Fernando] Univ Lisbon, Inst Super Tecn, P-1049001 Lisbon, Portugal.
   [Rente, Paulo de Oliveira; Brites, Catarina; Ascenso, Joao; Pereira, Fernando] Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa; Instituto de Telecomunicacoes
RP Brites, C (corresponding author), Univ Lisbon, Inst Super Tecn, P-1049001 Lisbon, Portugal.; Brites, C (corresponding author), Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
EM paulo.rente.92@gmail.com; catarina.brites@lx.it.pt;
   joao.ascenso@lx.it.pt; fp@lx.it.pt
RI Pereira, Fernando/K-4046-2012; Pereira, Fernando/HNR-7786-2023; Brites,
   Catarina/L-6191-2013; Ascenso, Joao/B-9024-2008
OI Bernardo Pereira, Fernando Manuel/0000-0001-6100-947X; Brites,
   Catarina/0000-0002-6011-4574; Ascenso, Joao/0000-0001-9902-5926
FU Fundacao para a Cieneia e a Tecnologia through the Project entitled
   Progressive Point Cloud Representation [PTDC/EEI-PRO/7237/2014];
   Fundação para a Ciência e a Tecnologia [PTDC/EEI-PRO/7237/2014] Funding
   Source: FCT
FX This work was supported by the Fundacao para a Cieneia e a Tecnologia
   through the Project entitled Progressive Point Cloud Representation
   under Grant PTDC/EEI-PRO/7237/2014. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Sanjeev Mehrotra.
CR Adelson Edward H, 1991, PLENOPTIC FUNCTION E
   Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   [Anonymous], 2015, 1SC29WG1N6922 ISOIEC
   [Anonymous], 2017, JTC1SC29WG11N16763 I
   [Anonymous], 1979, VIDEO DATA REC C SOU
   [Anonymous], 2015, HUNGARIAN ALGORITHM
   Champel M.-L., 2017, JTC1SC29WG11N17067 I
   Chen JY, 2014, IEEE T MULTIMEDIA, V16, P337, DOI 10.1109/TMM.2013.2286580
   CloudCompare Z., 2020, CloudCompare: 3D point cloud and mesh processing software
   Doulamis A, 2015, ISPRS ANN PHOTO REM, P61, DOI 10.5194/isprsannals-II-5-W3-61-2015
   Fracastoro G., 2016, P PICT COD S PCS DEC, P1
   Gao W, 2017, IEEE T IMAGE PROCESS, V26, P6074, DOI 10.1109/TIP.2017.2745099
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Golla T, 2015, IEEE INT C INT ROBOT, P5087, DOI 10.1109/IROS.2015.7354093
   Gortler S. J., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P43, DOI 10.1145/237170.237200
   Guede C., 2017, JTC1SC29WG11N16902 I
   Howard PG, 1998, IEEE T CIRC SYST VID, V8, P838, DOI 10.1109/76.735380
   Hu W, 2012, IEEE IMAGE PROC, P1297, DOI 10.1109/ICIP.2012.6467105
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   JPEG Convenor, 2016, JTC1SC29WG11N73013 I
   Kammerl J, 2012, IEEE INT CONF ROBOT, P778, DOI 10.1109/ICRA.2012.6224647
   Kyriakaki G., 2014, Int. J. Herit. Digit. Era, V3, P431, DOI DOI 10.1260/2047-4970.3.2.431
   Liu D, 2018, IEEE SIGNAL PROC LET, V25, P10, DOI 10.1109/LSP.2017.2769002
   Ma R, 2018, IEEE T MULTIMEDIA, V20, P1595, DOI 10.1109/TMM.2017.2779039
   Morell V, 2014, PATTERN RECOGN LETT, V50, P55, DOI 10.1016/j.patrec.2014.05.016
   MPEG, 2017, MPEG POINT CLOUD DAT
   MPEG Requirements Subgroup, 2016, JTC1SC29WG11N16332 I
   MPEG Requirements Subgroup, 2016, JTC1SC29WG11N16331 I
   MPEG Requirements Subgroup, 2016, JTC1SC29WG11N16330 I
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Pavez E, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P199, DOI 10.1109/PCS.2015.7170075
   Pereira F., 2016, P 15 BRAZILIAN S HUM, P1, DOI DOI 10.1145/3033701.3033720
   Remondino F, 2006, PHOTOGRAMM REC, V21, P269, DOI 10.1111/j.1477-9730.2006.00383.x
   Rothermel M., 2012, P LC3D WORKSH BERL G
   Schnabel R., 2006, S POINT BAS GRAPH 20, P111
   Shidanshidi H, 2015, IEEE T MULTIMEDIA, V17, P1677, DOI 10.1109/TMM.2015.2447274
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Stankiewicz O, 2018, IEEE T MULTIMEDIA, V20, P2182, DOI 10.1109/TMM.2018.2790162
   Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187
   Toni L, 2017, IEEE T MULTIMEDIA, V19, P2775, DOI 10.1109/TMM.2017.2713644
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
NR 43
TC 76
Z9 80
U1 19
U2 104
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 284
EP 299
DI 10.1109/TMM.2018.2859591
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400002
DA 2024-07-18
ER

PT J
AU Yi, S
   Zhou, YC
AF Yi, Shuang
   Zhou, Yicong
TI Separable and Reversible Data Hiding in Encrypted Images Using
   Parametric Binary Tree Labeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reversible data hiding; encrypted images; parametric binary tree
   labeling scheme; privacy protection
ID DIFFERENCE
AB This paper first introduces a parametric binary tree labeling scheme (PBTL) to label image pixels in two different categories. Using PBTL, a data embedding method (PBTL-DE) is proposed to embed secret data to an image by exploiting spatial redundancy within small image blocks. We then apply PBTL-DE into the encrypted domain and propose a PBTL-based reversible data hiding method in encrypted images (PBTL-RDHEI). PBTL-RDHEI is a separable and reversible method that both the original image and secret data can be recovered and extracted losslessly and independently. Experiment results and analysis show that PBTL-RDHEI is able to achieve an average embedding rate as large as 1.752 bpp and 2.003 bpp when block size is set to 2 x 2 and 3 x 3, respectively.
C1 [Yi, Shuang; Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
C3 University of Macau
RP Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM yishuang0227@gmail.com; yicongzhou@umac.mo
RI Zhou, Yicong/A-8017-2009
OI Zhou, Yicong/0000-0002-4487-6384
FU Macau Science and Technology Development Fund [FDCT/189/2017/A3];
   Research Committee at University of Macau [MYRG2016-00123-FST,
   MYRG2018-00136-FST]
FX This work was supported in part by the Macau Science and Technology
   Development Fund under Grant FDCT/189/2017/A3, and in part by the
   Research Committee at University of Macau under Grant MYRG2016-00123-FST
   and Grant MYRG2018-00136-FST. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1132, DOI 10.1109/TCYB.2015.2423678
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Chen YC, 2014, J VIS COMMUN IMAGE R, V25, P1164, DOI 10.1016/j.jvcir.2014.04.003
   Fridrich J, 2002, EURASIP J APPL SIG P, V2002, P185, DOI 10.1155/S1110865702000537
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu YJ, 2009, IEEE T CIRC SYST VID, V19, P250, DOI 10.1109/TCSVT.2008.2009252
   Hua ZY, 2018, SIGNAL PROCESS, V144, P134, DOI 10.1016/j.sigpro.2017.10.004
   Hua ZY, 2015, INFORM SCIENCES, V297, P80, DOI 10.1016/j.ins.2014.11.018
   Huang FJ, 2016, IEEE T INF FOREN SEC, V11, P2777, DOI 10.1109/TIFS.2016.2598528
   Li M, 2015, SIGNAL PROCESS-IMAGE, V39, P234, DOI 10.1016/j.image.2015.10.001
   Li M, 2014, ETRI J, V36, P325, DOI 10.4218/etrij.14.0213.0449
   Liao X, 2017, MULTIMED TOOLS APPL, V76, P20739, DOI 10.1007/s11042-016-3971-4
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Mathew T, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P839, DOI 10.1109/IC3I.2014.7019628
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Puech W., 2008, P SPIE SECURITY FORE, VX
   Qian ZX, 2016, IEEE T CIRC SYST VID, V26, P636, DOI 10.1109/TCSVT.2015.2418611
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Shiu CW, 2015, SIGNAL PROCESS-IMAGE, V39, P226, DOI 10.1016/j.image.2015.09.014
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Yi S, 2017, SIGNAL PROCESS, V133, P40, DOI 10.1016/j.sigpro.2016.10.017
   Yi S, 2015, 2015 IEEE CHINA SUMMIT & INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING, P225, DOI 10.1109/ChinaSIP.2015.7230396
   Yin ZX, 2017, MULTIMED TOOLS APPL, V76, P3899, DOI 10.1007/s11042-016-4049-z
   Yin Zhaoxia, 2015, Cloud Computing and Security. First International Conference, ICCCS 2015. Revised Selected Papers: LNCS 9483, P101, DOI 10.1007/978-3-319-27051-7_9
   Yin ZX, 2014, SCI WORLD J, DOI 10.1155/2014/604876
   Zhang WM, 2014, SIGNAL PROCESS, V94, P118, DOI 10.1016/j.sigpro.2013.06.023
   Zhang X., 2013, DIGITAL FORENSICS WA, V7809, P358
   Zhang XP, 2016, IEEE T CIRC SYST VID, V26, P1622, DOI 10.1109/TCSVT.2015.2433194
   Zhang XP, 2014, J VIS COMMUN IMAGE R, V25, P322, DOI 10.1016/j.jvcir.2013.11.001
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2012, IEEE T INF FOREN SEC, V7, P826, DOI 10.1109/TIFS.2011.2176120
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zheng SL, 2016, MULTIMED TOOLS APPL, V75, P13765, DOI 10.1007/s11042-015-2920-y
   Zhou JT, 2016, IEEE T CIRC SYST VID, V26, P441, DOI 10.1109/TCSVT.2015.2416591
   Zhou YC, 2014, SIGNAL PROCESS, V97, P172, DOI 10.1016/j.sigpro.2013.10.034
NR 38
TC 137
Z9 158
U1 3
U2 59
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 51
EP 64
DI 10.1109/TMM.2018.2844679
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700005
DA 2024-07-18
ER

PT J
AU Wang, HL
   Xiao, B
   Wu, J
   Kwong, S
   Kuo, CCJ
AF Wang, Hanli
   Xiao, Bo
   Wu, Jun
   Kwong, Sam
   Kuo, C-C Jay
TI A Collaborative Scheduling-Based Parallel Solution for HEVC Encoding on
   Multicore Platforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding; wavefront parallel processing;
   collaborative scheduling; multicore platform; parallelization
   scalability
ID VIDEO; EFFICIENCY; DECISION; IMPLEMENTATION; FRAMEWORK; DECODERS
AB In order to meet the high computational demand to achieve superior coding efficiency and to explore the parallelism of parallel processing architectures, the emerging high efficiency video coding (HEVC) standard has been designed to be more parallelizable than previous video coding standards. However, it is still desirable to design an efficient parallel HEVC encoder to fully exploit the parallelism of the increasingly powerful multicore platforms, especially when considering the amount of parallelism, the scalability of parallelization, and the coding efficiency. In this work, a performance model of HEVC encoding is first introduced to investigate the speedup and the limitations of the technique of wavefront parallel processing (WPP) under various conditions. Then, a collaborative scheduling-based parallel solution (CSPS) for HEVC encoding is proposed, which includes adaptive parallel mode decision, asynchronous frame-level pixel interpolation, and multigrained task scheduling. The goal of the proposed CSPS is to defeat the disadvantages of WPP and further improve the parallelization of HEVC encoding on multicore platforms. Extensive experimental results demonstrate the efficiency of the proposed CSPS for parallelizing HEVC encoding as the computing resources of multicore architectures can be fully utilized.
C1 [Wang, Hanli; Xiao, Bo; Wu, Jun] Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   [Wang, Hanli; Xiao, Bo; Wu, Jun] Tongji Univ, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Kuo, C-C Jay] Univ Southern Calif, Dept Elect Engn, Signal & Image Proc Inst, Los Angeles, CA 90089 USA.
C3 Tongji University; Tongji University; City University of Hong Kong;
   University of Southern California
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
EM hanliwang@tongji.edu.cn; 1314xiaobo@tongji.edu.cn; wujun@tongji.edu.cn;
   cssamk@cityu.edu.hk; cckuo@sipi.usc.edu
RI Kwong, Sam/C-9319-2012; Wang, Hanli/G-5111-2014; Kuo, C.-C.
   Jay/A-7110-2011
OI Kwong, Sam/0000-0001-7484-7261; Wang, Hanli/0000-0002-9999-4871; Kuo,
   C.-C. Jay/0000-0001-9474-5035
FU National Natural Science Foundation of China [61622115, 61472281];
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning [GZ2015005]; Shanghai
   Engineering Research Center of Industrial Vision Perception and
   Intelligent Computing [17DZ2251600]; IBM Shared University Research
   Awards Program
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61622115 and 61472281, in part by the
   Program for Professor of Special Appointment (Eastern Scholar) at
   Shanghai Institutions of Higher Learning (No. GZ2015005), in part by the
   Shanghai Engineering Research Center of Industrial Vision Perception and
   Intelligent Computing (17DZ2251600), and in part by the IBM Shared
   University Research Awards Program.
CR Ahmad I, 2002, PARALLEL COMPUT, V28, P1039, DOI 10.1016/S0167-8191(02)00100-X
   Bjontegaard G., 2001, Document VCEG-M33
   Borkar S, 2011, COMMUN ACM, V54, P67, DOI 10.1145/1941487.1941507
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Chen KJ, 2016, IEEE T CIRC SYST VID, V26, P181, DOI 10.1109/TCSVT.2015.2418651
   Chen KH., 2012, SAE Int., V1, P1, DOI DOI 10.4271/2012-01-0645
   Chen YK, 2006, J VIS COMMUN IMAGE R, V17, P509, DOI 10.1016/j.jvcir.2005.05.004
   Chi CC, 2012, IEEE T CIRC SYST VID, V22, P1827, DOI 10.1109/TCSVT.2012.2223056
   Cho S, 2015, IEEE T MULTIMEDIA, V17, P778, DOI 10.1109/TMM.2015.2418995
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Fuldseth A., 2011, JCTVCE408
   Gweon R., 2011, JCTVCF045
   Henry F., 2011, JCTVCE196
   Hsiao HF, 2013, IEEE T PARALL DISTR, V24, P2355, DOI 10.1109/TPDS.2013.7
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Jung B, 2008, J VIS COMMUN IMAGE R, V19, P558, DOI 10.1016/j.jvcir.2008.09.004
   Li Y, 2017, IEEE T MULTIMEDIA, V19, P1431, DOI 10.1109/TMM.2017.2669863
   Madan N, 2011, INT S HIGH PERF COMP, P291, DOI 10.1109/HPCA.2011.5749737
   Mastronarde N, 2013, IEEE T MULTIMEDIA, V15, P268, DOI 10.1109/TMM.2012.2231668
   McCann K., 2014, document JCTVC-R1002
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Radicke S, 2014, 2014 INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING AND MULTIMEDIA APPLICATIONS (SIGMAP), P90
   Reinders J., 2007, INTEL THREADING BUIL
   Rodríguez A, 2006, PAR ELEC 2006: INTERNATIONAL SYMPOSIUM ON PARALLEL COMPUTING IN ELECTRICAL ENGINEERING, PROCEEDINGS, P363
   Sankaraiah S, 2014, INT J PARALLEL PROG, V42, P931, DOI 10.1007/s10766-013-0267-4
   Sankaraiah S., 2013, P 1 INT C ADV DAT IN, P317
   Sankaraiah S, 2014, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2014-145
   Sanz-Rodríguez S, 2015, SIGNAL PROCESS-IMAGE, V30, P89, DOI 10.1016/j.image.2014.10.003
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vieron J., 2012, JCTVCI0198
   Wang ZY, 2015, J VIS COMMUN IMAGE R, V28, P36, DOI 10.1016/j.jvcir.2015.01.005
   Wang ZY, 2011, J SIGNAL PROCESS SYS, V65, P129, DOI 10.1007/s11265-010-0543-0
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao W, 2015, IEEE T CIRC SYST VID, V25, P1830, DOI 10.1109/TCSVT.2015.2406199
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yen-Kuang Chen, 2004, Proceedings. 18th International Parallel and Distributed Processing Symposium
   Zhang YD, 2012, IEEE T MULTIMEDIA, V14, P510, DOI 10.1109/TMM.2012.2190391
   Zhao Z., 2006, IEEE International Conference on Acoustics, Speech and Signal Processing, V5, P489
NR 41
TC 8
Z9 9
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2935
EP 2948
DI 10.1109/TMM.2018.2830120
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800006
DA 2024-07-18
ER

PT J
AU Li, S
   Zhu, C
   Sun, MT
AF Li, Shuai
   Zhu, Ce
   Sun, Ming-Ting
TI Hole Filling With Multiple Reference Views in DIBR View Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D video; depth-image-based rendering (DIBR); view synthesis; hole
   generation; hole filling
ID DEPTH; VIDEO
AB Depth-image-based rendering (DIBR) oriented view synthesis has been widely employed in the current depth-based 3-D video systems by synthesizing a virtual view from an arbitrary viewpoint. However, holes may appear in the synthesized view due to disocclusion, thus significantly degrading the quality. Consequently, efforts have been made on developing effective and efficient hole-filling algorithms. Current hole-filling techniques generally extrapolate/interpolate the hole regions with the neighboring information based on an assumption that the texture pattern in the holes is similar to that of the neighboring background information. However, in many scenarios, especially of complex texture, the assumption may not hold. In other words, hole-filling techniques can only provide an estimation for a hole which may not be good enough or may even be erroneous considering a wide variety of complex scene of images. In this paper, we first examine the view interpolation with multiple reference views, demonstrating that the problem of emerging holes in a target virtual view can he greatly alleviated by making good use of other neighboring complementary views in addition to its two (commonly used) most neighboring primary views. The effects of using multiple views for view extrapolation in reducing holes are also investigated in this paper. In view of the 3D Video and ongoing free-viewpoint TV standardization, we propose a new view synthesis framework, which employs multiple views to synthesize output virtual views. Furthermore, a scheme of selective warping of complementary views is developed by efficiently locating a small number of useful pixels in the complementary views for hole reduction, to avoid full warping of additional complementary views thus lowering greatly the warping complexity. Experimental results show that the hole size based on two primary reference views may he reduced by up to about 70% with the help of two complementary reference views in the case of view interpolation, while the hole size based on one primary reference view may he reduced by about 27% with the help of one more complementary reference view in view extrapolation. Moreover, it is shown that by using one more pair of views in view interpolation and one more view in view extrapolation, 10% hole pixels may be reduced additionally.
C1 [Li, Shuai; Zhu, Ce] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610051, Sichuan, Peoples R China.
   [Li, Shuai; Zhu, Ce] Univ Elect Sci & Technol China, Ctr Robot, Chengdu 610051, Sichuan, Peoples R China.
   [Sun, Ming-Ting] Univ Washington, Seattle, WA 98195 USA.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; University of Washington;
   University of Washington Seattle
RP Zhu, C (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610051, Sichuan, Peoples R China.
EM shuailichn@gmail.com; eczhu@uestc.edu.cn; mts@uw.edu
RI Li, Shuai/AAE-6907-2019; Zhu, Ce/AEN-1875-2022
OI Li, Shuai/0000-0002-9938-0917; 
FU National Natural Science Foundation of China [61571102]; Fundamental
   Research Funds for the Central Universities [ZYGX2014Z003]; National
   High Technology Research and Development Program of China [2015AA015903]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571102, in part by the Fundamental
   Research Funds for the Central Universities under Grant ZYGX2014Z003,
   and in part by the National High Technology Research and Development
   Program of China under Grant 2015AA015903. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Hantao Liu.
CR [Anonymous], 2014, IEEE SYST J
   [Anonymous], 2013, M30229 ISOIEC JTC1SC
   [Anonymous], INT SYM MIX AUGMENT
   [Anonymous], 2005, P IEEE INT C IM PROC
   [Anonymous], 2008, N9595 ISOIEC JTC1SC2
   [Anonymous], 2011, N12188 ISOIEC JTC1SC
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   Levoy M, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.270
   Lipski C, 2014, IEEE T CIRC SYST VID, V24, P942, DOI 10.1109/TCSVT.2014.2302379
   Mori Y, 2009, SIGNAL PROCESS-IMAGE, V24, P65, DOI 10.1016/j.image.2008.10.013
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Rusanovskyy D., 2013, JCT3VF1100
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Senoh T., 2014, M32201 ISOIEC JTC1SC
   Tanimoto M, 2012, P IEEE, V100, P905, DOI 10.1109/JPROC.2011.2182101
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Telea A., 2004, Journal of Graphics Tools, V9, P23, DOI 10.1080/10867651.2004.10487596
   Tian D., 2009, P SOC PHOTO-OPT INS, V7443
   Wegner K., 2014, N14512 ISOIEC JTC1SC
   Yao C, 2014, IEEE T BROADCAST, V60, P394, DOI 10.1109/TBC.2014.2321671
   Zhao Y, 2011, IEEE T BROADCAST, V57, P510, DOI 10.1109/TBC.2011.2120730
   Zhu C., 2013, 3D-TV System with Depth-Image-Based Rendering - Architectures, Techniques and Challenges
   Zhu C, 2016, IEEE T BROADCAST, V62, P82, DOI 10.1109/TBC.2015.2475697
   Zhu C, 2013, 2013 NINTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING (IIH-MSP 2013), P607, DOI 10.1109/IIH-MSP.2013.156
   Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766
NR 27
TC 62
Z9 66
U1 0
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 1948
EP 1959
DI 10.1109/TMM.2018.2791810
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liang, F
   Luo, C
   Xiong, RQ
   Zeng, WJ
   Wu, F
AF Liang, Fei
   Luo, Chong
   Xiong, Ruiqin
   Zeng, Wenjun
   Wu, Feng
TI Hybrid Digital-Analog Video Delivery With Shannon-Kotel'nikov Mapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hybrid digital-analog video transmission; Shannon-Kotel'nikov mapping
ID TRANSMISSION; EFFICIENCY; BROADCAST
AB Hybrid digital-analog (HDA) transmission is becoming an attractive solution for mobile video delivery because it not only has graceful degradation with channel variations but also yields high power efficiency. However, the heavy bandwidth demand of analog transmission is still an unsolved problem, limiting the received video quality when the bandwidth is not sufficient. To address this problem, we propose adopting Shannon-Kotel'nikov (SK) mapping for HDA video transmission and design an HDA scheme called SK-Cast. SK-Cast consists of a digital and an analog branch. In the digital branch, SK-Cast compresses the video sequence using an high efficiency video coding digital encoder to produce a base layer. The base layer is transmitted through digital methods with strong protection. The residual signals are then decorrelated using three-dimensional discrete cosine transform transform. The SK mapping is exploited to transmit these coefficients, as they can achieve efficient bandwidth compression. We address the resource allocation problems in SK-Cast, including the allocation between digital and analog branches and the allocation among analog symbols. The simulation results show that the SK-Cast outperforms the state-of-the-art HDA systems, including WSVC and Sharp-Cast, and a digital scalable video coding system.
C1 [Liang, Fei] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei, Anhui, Peoples R China.
   [Luo, Chong; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Xiong, Ruiqin] Peking Univ, Beijing 100080, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft Research Asia; Microsoft; Peking University;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Luo, C (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM lfbeyond@mail.ustc.edu.cn; cluo@microsoft.com; rqxiong@pku.edu.cn;
   wezeng@microsoft.com; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024
OI Xiong, Ruiqin/0000-0001-9796-0478; Liang, Fei/0000-0002-7639-1584
FU National Natural Science Foundation of China [61631017]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61631017. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Lingfen Sun.
CR [Anonymous], 2014, HM SOFTWARE VERSION
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Brante G, 2013, IEEE T COMMUN, V61, P301, DOI 10.1109/TCOMM.2012.091212.110773
   Cagnazzo M, 2015, IEEE IMAGE PROC, P1085, DOI 10.1109/ICIP.2015.7350967
   Cisco, 2016, CISC VIS NETW IND GL
   Cui H, 2014, IEEE INFOCOM SER, P73, DOI 10.1109/INFOCOM.2014.6847926
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   Fan XP, 2012, IEEE DATA COMPR CONF, P199, DOI 10.1109/DCC.2012.27
   Fresnedo O, 2013, IEEE COMMUN LETT, V17, P745, DOI 10.1109/LCOMM.2013.021913.122782
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   Hekland F, 2009, IEEE T COMMUN, V57, P94, DOI 10.1109/TCOMM.2009.0901.070075
   Hu YC, 2011, IEEE T COMMUN, V59, P3016, DOI 10.1109/TCOMM.2011.081711.090298
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Jakubczak S., 2011, PROC MOBICOM, P289
   Kotelnikov V. A., 1959, The Theory of Optimum Noise Immunity
   Kratochvil T., 2009, LECT NOTES ELECT ENG, P333
   Li PL, 2012, IEEE ACM T NETWORK, V20, P57, DOI 10.1109/TNET.2011.2157700
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Lu J., 2014, PROC 48 ANN C INF SC, P1
   Prabhakaran VM, 2011, IEEE T INFORM THEORY, V57, P4573, DOI 10.1109/TIT.2011.2145210
   Schierl T, 2007, IEEE T CIRC SYST VID, V17, P1204, DOI 10.1109/TCSVT.2007.905528
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Singhal C, 2014, IEEE T MOBILE COMPUT, V13, P1522, DOI 10.1109/TMC.2013.138
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang CY, 2015, IEEE T WIREL COMMUN, V14, P2353, DOI 10.1109/TWC.2014.2385773
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu F, 2014, IEEE T IMAGE PROCESS, V23, P1015, DOI 10.1109/TIP.2014.2298972
   Xiong RG, 2014, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2014.55
   Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2621478
   Xiong RQ, 2014, IEEE IMAGE PROC, P2542, DOI 10.1109/ICIP.2014.7025514
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Xiong RQ, 2013, IEEE INT SYMP CIRC S, P1159, DOI 10.1109/ISCAS.2013.6572057
   Yu L, 2015, IEEE T CIRC SYST VID, V25, P436, DOI 10.1109/TCSVT.2014.2347532
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhao X, 2016, IEEE T CIRC SYST VID, V26, P1117, DOI 10.1109/TCSVT.2015.2444753
NR 36
TC 10
Z9 10
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2138
EP 2152
DI 10.1109/TMM.2017.2785264
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600018
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
TI Deep-Structured Event Modeling for User-Generated Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event analysis; unusual event detection; deep learning
ID VIDEO; RECOGNITION
AB Vision-based event analysis is difficult because of the following challenges. The first challenge is intraclass variation. Photos uploaded by users are sparsely sampled visual appearances of an event over time. Thus, each photo may only capture a single object or scene of a specific complex event. The second challenge is interclass confusion. Photos related to different events may contain similar objects or scenes. Third, unusual events are characterized by scarcity, and only a few samples are available for use in learning event patterns. In this paper, by considering the photo timestamp, we propose a structured event modeling (SEM) framework for event analysis that exploits the temporal information of visual features and event classes in a photo sequence. Specifically, the temporal event patterns of the photo sequence and the relationships of different photos are jointly learned using deep neural networks (convolutional neural networks and recurrent neural networks) and a conditional random field. We evaluate the proposed SEM framework in two applications: multiclass event recognition and unusual event detection in photo sequences. The results of extensive experiments performed on a public event recognition dataset and a collected unusual event dataset demonstrate the effectiveness of the proposed method.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Yang, XS (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM xiaoshan.yang@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Zhang, Tianzhu/AGY-9389-2022
OI Zhang, Tianzhu/0000-0003-0764-6106; xu, chang sheng/0000-0001-8343-9665;
   zhang, tian zhu/0000-0003-1856-9564
FU National Natural Science Foundation of China [61432019, 61572498,
   61532009, 61702511, 61720106006, 61711530243]; Beijing Natural Science
   Foundation [4172062]; Key Research Program of Frontier Sciences, Chinese
   Academy of Sciences [QYZDJ-SSW-JSC039]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61432019, 61572498, 61532009, 61702511,
   61720106006, and 61711530243; in part by Beijing Natural Science
   Foundation (4172062); and in part by the Key Research Program of
   Frontier Sciences, Chinese Academy of Sciences under Grant
   QYZDJ-SSW-JSC039. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Martha Larson.
CR [Anonymous], 2009, P 18 INT C WORLD WID
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], P IEEE COMP SOC C CO
   [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.348
   [Anonymous], P INT C MULT
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P CVPR
   [Anonymous], P 2009 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2009.5206557
   [Anonymous], 2014, P 4 INT C WEB INT MI
   [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631371
   [Anonymous], P 18 INT C MULT FIR
   [Anonymous], JOINT ACM WORKSH MOD
   [Anonymous], 2012, ARXIV12115590
   [Anonymous], 2001, PROC 18 INT C MACH L
   Bergstra J., 2010, Proc. Python Sci. Comput. Conf., V1, P3
   Bhattacharya S, 2014, IEEE T MULTIMEDIA, V16, P686, DOI 10.1109/TMM.2014.2300833
   Bin Zhao, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3313, DOI 10.1109/CVPR.2011.5995524
   Bossard L, 2013, IEEE I CONF COMP VIS, P1193, DOI 10.1109/ICCV.2013.151
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen CY, 2011, PROC CVPR IEEE, P1569, DOI 10.1109/CVPR.2011.5995412
   Chen LC, 2015, PR MACH LEARN RES, V37, P1785
   Chung JY, 2015, PR MACH LEARN RES, V37, P2067
   Chung Junyoung, 2014, ARXIV14123555
   Cristani M., 2008, PROC IEEE C COMPUT V, P1
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Firan C.S., 2010, Proceedings of the 19th ACM international conference on Information and knowledge management, P189
   Gaidon A, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.30
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kaisheng Yao, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4077, DOI 10.1109/ICASSP.2014.6854368
   Kalogerakis E, 2009, IEEE I CONF COMP VIS, P253, DOI 10.1109/ICCV.2009.5459259
   Kim G, 2014, PROC CVPR IEEE, P3882, DOI 10.1109/CVPR.2014.496
   Kim G, 2013, PROC CVPR IEEE, P620, DOI 10.1109/CVPR.2013.86
   Kim G, 2010, LECT NOTES COMPUT SC, V6315, P85
   Kiros Ryan., 2015, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, P3294
   Koller D., 2009, Probabilistic graphical models: principles and techniques
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Lu CW, 2013, IEEE I CONF COMP VIS, P2720, DOI 10.1109/ICCV.2013.338
   Marchi E, 2015, INT CONF ACOUST SPEE, P1996, DOI 10.1109/ICASSP.2015.7178320
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Obrador P., 2010, PROC 18 ACM INT C MU, P561
   Peng J., 2009, NIPS
   Petkos G., 2012, P 2 ACM INT C MULT R, P231, DOI 10. 1145/2324796.2324825.
   Reuter T., 2012, PROC ANN ACM INT C M, P22
   Reuter Timo., 2013, P MEDIAEVAL 2013 MUL, P1
   Rosani A, 2015, IEEE T MULTIMEDIA, V17, P1359, DOI 10.1109/TMM.2015.2441003
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salvador A, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301334
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9909, P71, DOI 10.1007/978-3-319-46454-1_5
   Simonyan K., 2014, 14091556 ARXIV
   Snavely N, 2010, P IEEE, V98, P1370, DOI 10.1109/JPROC.2010.2049330
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Sutton Charles., 2005, PROC C UNCERTAINTY A, P568
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tsai S.-F., 2011, ACM Multimedia, P1361
   Tsai SW., 2011, Proceedings of ICCM-18, P1
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang Dingding., 2012, AAAI
   Wang F, 2014, IEEE T MULTIMEDIA, V16, P1303, DOI 10.1109/TMM.2014.2315780
   Weyand T, 2016, LECT NOTES COMPUT SC, V9912, P37, DOI 10.1007/978-3-319-46484-8_3
   Wu JZ, 2014, IEEE T MULTIMEDIA, V16, P147, DOI 10.1109/TMM.2013.2283846
   Xiong B, 2015, IEEE I CONF COMP VIS, P4525, DOI 10.1109/ICCV.2015.514
   Xiong YJ, 2015, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2015.7298768
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Yang XS, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P47, DOI 10.1145/2964284.2975215
   Yang XS, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2962719
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang W, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P33, DOI 10.1109/3dpvt.2006.80
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1127
NR 83
TC 2
Z9 2
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2100
EP 2113
DI 10.1109/TMM.2017.2788210
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600015
DA 2024-07-18
ER

PT J
AU Kurdoglu, E
   Liu, Y
   Wang, Y
AF Kurdoglu, Eymen
   Liu, Yong
   Wang, Yao
TI Perceptual Quality Maximization for Video Calls With Packet Losses by
   Optimizing FEC, Frame Rate, and Quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia communication; real-time systems; video codecs
ID ERROR-CORRECTION; SCALABLE VIDEO; TRANSMISSION; PROTECTION; AWARE
AB We consider video calls affected by bursty packet losses, where frame-level forward error correction (FEC) is employed clue to delay constraints, and damaged frames and others predicted from them are discarded. Here, a high frame rate (FR) at low Nitrates leads to large quantization step sizes (QS), small frames, and suboptimal FEC, whereas a low FR at high Nitrates reduces the perceptual quality. To mitigate frame losses and freezing, hierarchical-P (hierP) temporal layering can be used with lower coding efficiency than IPPP coding. We study the received quality maximization for hierP and IPPP, by jointly optimizing the encoding FR, QS, and the FEC redundancy, under sending bitrate constraints. Building upon Q-STAR perceptual quality and R-STAR bitrate models, which depend on QS, and the decoded and encoding FR, respectively, we cast the problem as a combinatorial optimization problem. We solve for the encoding FR and the Nitrate using exhaustive search, hill-climbing, and a greedy FEC distribution algorithm to determine the FEC redundancies. We show that, fir random losses, the FEC Nitrate ratio is an affine function of the packet-loss rate; low encoding FR is preferred at wider sending bitrate ranges with higher packet lass; layer protection is more even at higher bitrates; and IPPP, while achieving higher Q-STAR scores, is prone to freezing. For bursty losses, we show that layer redundancies are higher, rising with the mean burst length, reaching 80%; and hierP achieves higher Q-STAR scores than IPPP for longer bursts, and a smaller mean and variance of decoded frame distances.
C1 [Kurdoglu, Eymen; Liu, Yong; Wang, Yao] NYU, Tandon Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Kurdoglu, E (corresponding author), NYU, Tandon Sch Engn, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM eymen.kurdoglu@nyu.edu; yongliu@nyu.edu; yw523@nyu.edu
OI Kurdoglu, Eymen/0000-0002-7770-3148
CR Andren J, 1998, GLOBECOM 98: IEEE GLOBECOM 1998 - CONFERENCE RECORD, VOLS 1-6, P1118, DOI 10.1109/GLOCOM.1998.776899
   [Anonymous], SKYP
   Apple, FAC
   Bergkvist A., 2012, WEBRTC 1 0 REAL TIME, V91
   Blahut R.E., 1983, THEORY PRACTICE ERRO, V126
   Bolot J.-C., 1993, Computer Communication Review, V23, P289, DOI 10.1145/167954.166265
   C. Montgomery, XIPH ORG TEST MED
   Chen W, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2983634
   Cover T. M., 2012, ELEMENTS INFORM THEO
   De Cicco L., 2013, Proceedings_of_the_2013_ACM_SIGCOMM Workshop_on_Future_Human-Centric_Multimedia_Networking, FhMN'13, page, P21, DOI DOI 10.1145/2491172.2491182
   Freris NM, 2013, IEEE ACM T NETWORK, V21, P469, DOI 10.1109/TNET.2012.2203608
   Frossard P, 2001, IEEE COMMUN LETT, V5, P122, DOI 10.1109/4234.913160
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Hameed A, 2016, IEEE T MULTIMEDIA, V18, P764, DOI 10.1109/TMM.2016.2525862
   Hellge C, 2011, IEEE T MULTIMEDIA, V13, P551, DOI 10.1109/TMM.2011.2129499
   Hong D., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P146, DOI 10.1109/PCS.2010.5702445
   Hu H, 2012, IEEE IMAGE PROC, P717, DOI 10.1109/ICIP.2012.6466960
   Kurdoglu E., 2016, P 7 INT C MULT SYST, P1
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Lin CH, 2013, IEEE T MULTIMEDIA, V15, P195, DOI 10.1109/TMM.2012.2225028
   Ma Z., 2013, Power Electronics and Applications (EPE), 2013 15th European Conference On, P1
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Merritt L., 2006, X264 HIGH PERFORMANC
   NYU Video Lab, FEC4RT PROJ PAG
   Ou YF, 2014, IEEE T IMAGE PROCESS, V23, P2473, DOI 10.1109/TIP.2014.2303636
   Paxson V., 1997, Computer Communication Review, V27, P139, DOI 10.1145/263109.263155
   Richardson Iain E, 2004, H. 264 and MPEG-4 Video Compression: Video Coding for Next-Generation Multimedia
   Salamatian K., 2001, Performance Evaluation Review, V29, P92, DOI 10.1145/384268.378439
   Shih CH, 2016, COMPUT NETW, V105, P89, DOI 10.1016/j.comnet.2016.05.016
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tencent Inc, WECHAT
   Winstein Keith, 2013, 10 USENIX S NETW SYS, P459
   Wu Huahui, 2005, ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), V1, P315
   Wu JY, 2017, IEEE T CIRC SYST VID, V27, P32, DOI 10.1109/TCSVT.2016.2527398
   Xiao JM, 2012, IEEE T MULTIMEDIA, V14, P1298, DOI 10.1109/TMM.2012.2194274
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   Yang XK, 2003, SIGNAL PROCESS-IMAGE, V18, P157, DOI 10.1016/S0923-5965(02)00128-5
   Zhang XG, 2013, IEEE T MULTIMEDIA, V15, P1446, DOI 10.1109/TMM.2013.2247988
NR 38
TC 6
Z9 6
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1876
EP 1887
DI 10.1109/TMM.2017.2781362
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100022
DA 2024-07-18
ER

PT J
AU Wang, PC
   Li, WQ
   Gao, ZM
   Tang, C
   Ogunbona, PO
AF Wang, Pichao
   Li, Wanqing
   Gao, Zhimin
   Tang, Chang
   Ogunbona, Philip O.
TI Depth Pooling Based Large-Scale 3-D Action Recognition With
   Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Large-scale; depth; action recognition; convolutional neural networks
ID GESTURE RECOGNITION
AB This paper proposes three simple, compact yet effective representations of depth sequences, referred to respectively as dynamic depth images (DDI), dynamic depth normal images (DDNI), and dynamic depth motion normal images (DDMNI), for both isolated and continuous action recognition. These dynamic images are constructed from a segmented sequence of depth maps using hierarchical bidirectional rank pooling to effectively capture the spatial-temporal information. Specifically, DDI exploits the dynamics of postures over time, and DDNI and DDMNI exploit the 3-D structural information captured by depth maps. Upon the proposed representations, a convolutional neural network (ConvNet)-based method is developed for action recognition. The image-based representations enable us to fine-tune the existing ConvNet models trained on image data without training a large number of parameters from scratch. The proposed method achieved the state-of-art results on three large datasets, namely, the large-scale continuous gesture recognition dataset (means the Jaccard index 0.4109), the large-scale isolated gesture recognition dataset (59.21%), and the NTU RGB+D dataset (87.08% cross-subject and 84.22% cross-view) even though only the depth modality was used.
C1 [Wang, Pichao; Li, Wanqing; Gao, Zhimin; Ogunbona, Philip O.] Univ Wollongong, Adv Multimedia Res Lab, Wollongong, NSW 2522, Australia.
   [Tang, Chang] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Hubei, Peoples R China.
C3 University of Wollongong; China University of Geosciences
RP Gao, ZM (corresponding author), Univ Wollongong, Adv Multimedia Res Lab, Wollongong, NSW 2522, Australia.
EM pw212@uowmail.edu.au; wanqing@uow.edu.au; zg126@uowmail.edu.au;
   happytangchang@gmail.com; philipo@uow.edu.au
RI Tang, Chang/AAU-8995-2020
OI Tang, Chang/0000-0002-6515-7696; Li, Wanqing/0000-0002-4427-2687; Wang,
   Pichao/0000-0002-1430-0237; Ogunbona, Philip O./0000-0003-4119-2873
CR Aggarwal JK, 2014, PATTERN RECOGN LETT, V48, P70, DOI 10.1016/j.patrec.2014.04.011
   [Anonymous], P EUR C COMP VIS ECC
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], P 2016 ACM MUL TIM C
   [Anonymous], 2018, AAAI
   [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], 2012, 2012 IEEE COMP SOC C, DOI DOI 10.1109/CVPRW.2012.6239179
   [Anonymous], 2018, INVERTED RESIDUALS L
   Bilen H, 2016, PROC CVPR IEEE, P3034, DOI 10.1109/CVPR.2016.331
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Chai XJ, 2016, INT C PATT RECOG, P31, DOI 10.1109/ICPR.2016.7899603
   Cheng H, 2016, IEEE T CIRC SYST VID, V26, P1659, DOI 10.1109/TCSVT.2015.2469551
   D'Orazio T, 2016, IMAGE VISION COMPUT, V52, P56, DOI 10.1016/j.imavis.2016.05.007
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Escalante H.J., 2015, Pattern Anal. Appl, P1
   Escalera S, 2016, J MACH LEARN RES, V17
   Fernando B, 2016, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR.2016.212
   Fernando B, 2015, PROC CVPR IEEE, P5378, DOI 10.1109/CVPR.2015.7299176
   FORNEY GD, 1973, P IEEE, V61, P268, DOI 10.1109/PROC.1973.9030
   Guyon I, 2014, MACH VISION APPL, V25, P1929, DOI 10.1007/s00138-014-0596-3
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Hou YH, 2018, IEEE ACCESS, V6, P2206, DOI 10.1109/ACCESS.2017.2782258
   Hou YH, 2018, IEEE T CIRC SYST VID, V28, P807, DOI 10.1109/TCSVT.2016.2628339
   Escalante HJ, 2016, INT C PATT RECOG, P67, DOI 10.1109/ICPR.2016.7899609
   Jayaraman D, 2016, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2016.418
   Ji XP, 2017, KNOWL-BASED SYST, V122, P64, DOI 10.1016/j.knosys.2017.01.035
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Li WQ, 2008, IEEE T CIRC SYST VID, V18, P1499, DOI 10.1109/TCSVT.2008.2005597
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li YN, 2016, INT C PATT RECOG, P25, DOI 10.1109/ICPR.2016.7899602
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Lo Presti L, 2016, PATTERN RECOGN, V53, P130, DOI 10.1016/j.patcog.2015.11.019
   Lu CW, 2014, PROC CVPR IEEE, P772, DOI 10.1109/CVPR.2014.104
   Lui YM, 2012, J MACH LEARN RES, V13, P3297
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Oreifej O, 2013, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2013.98
   Pisharady PK, 2015, COMPUT VIS IMAGE UND, V141, P152, DOI 10.1016/j.cviu.2015.08.004
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Simonyan K., 2014, 14091556 ARXIV
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wan J, 2016, IEEE T PATTERN ANAL, V38, P1626, DOI 10.1109/TPAMI.2015.2513479
   Wan J, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.2.023017
   Wan J, 2014, IEEE T IMAGE PROCESS, V23, P3152, DOI 10.1109/TIP.2014.2328181
   Wan J, 2013, J MACH LEARN RES, V14, P2549
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang PC, 2017, IEEE INT CONF COMP V, P1005, DOI 10.1109/ICCVW.2017.123
   Wang PC, 2017, PROC CVPR IEEE, P416, DOI 10.1109/CVPR.2017.52
   Wang PC, 2016, INT C PATT RECOG, P13, DOI 10.1109/ICPR.2016.7899600
   Wang PC, 2016, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2016.7899599
   Wang PC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1119, DOI 10.1145/2733373.2806296
   Wang PC, 2016, IEEE T HUM-MACH SYST, V46, P498, DOI 10.1109/THMS.2015.2504550
   Wu D, 2016, IEEE T PATTERN ANAL, V38, P1583, DOI 10.1109/TPAMI.2016.2537340
   Yang X., 2012, P 20 ACM INT C MULT, P1057, DOI DOI 10.1145/2393347.2396382
   Yang XD, 2014, PROC CVPR IEEE, P804, DOI 10.1109/CVPR.2014.108
   Zhang J, 2016, PATTERN RECOGN, V60, P86, DOI 10.1016/j.patcog.2016.05.019
   Zhu GM, 2016, INT C PATT RECOG, P19, DOI 10.1109/ICPR.2016.7899601
NR 63
TC 116
Z9 121
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1051
EP 1061
DI 10.1109/TMM.2018.2818329
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhao, LP
   Zhou, KL
   Guo, J
   Wang, SH
   Lin, T
AF Zhao, Liping
   Zhou, Kailun
   Guo, Jing
   Wang, Shuhui
   Lin, Tao
TI A Universal String Matching Approach to Screen Content Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); audio video coding standard (AVS);
   screen content coding (SCC); string matching; hash table
ID ALGORITHM
AB This paper proposes a universal string matching (USM) approach to screen content coding (SCC). USM uses a primary reference buffer and a secondary reference buffer for string matching and includes three modes: general string (GS) mode, constrained string 1 (CS1) mode, and constrained string 2 (CS2) mode. The CS1 mode and CS2 mode are constrained cases of the GS mode. Due to the diversity of the screen content, each of the three modes plays an indispensable role in coding some types of screen content. When using USM to code a coding unit (CU), one of the three modes is selected to code the CU. Compared with high-efficiency video coding (HEVC) SCC reference software HM-16.6 + SCM-5.2 of full frame search range for intrablock copy, USM achieves an average Y BD-rate of -28.4% for five text and graphics with motion (TGM) sequences from the audio video coding standard SCC common test condition (CTC) test suite and -5.8% for eight TGM test sequences from the HEVC SCC CTC test suite in all intraconfigurations, with a nearly 10% decrease in encoding runtime and almost the same decoding runtime.
C1 [Zhao, Liping] Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.
   [Zhao, Liping; Zhou, Kailun; Guo, Jing; Wang, Shuhui; Lin, Tao] Tongji Univ, Inst VLSI, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
C3 Shaoxing University; Tongji University
RP Zhao, LP (corresponding author), Shaoxing Univ, Dept Comp Sci & Engn, Shaoxing 312000, Peoples R China.; Lin, T (corresponding author), Tongji Univ, Inst VLSI, Coll Elect & Informat Engn, Shanghai 200092, Peoples R China.
EM zhaoliping_jian@126.com; kailun_zh@tongji.edu.cn; hpzfbmguo@163.com;
   shw@dongji.edu.cn; lintao@tongji.edu.cn
RI zhao, liping/N-4269-2017
FU National Natural Science Foundation of China [61601200]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61601200. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof, Zhu Li.
CR [Anonymous], 2014, FUND GUID CALL PROP
   [Anonymous], N2282 AVS
   [Anonymous], 2011, JCTVCE145
   [Anonymous], 2015, JCTVCU1015
   AVS, 2015, N2203 AVS
   Bjontegaard G, 2001, VCEG-M33 ITU-T SG 16/Q 6
   Bjontegaard G., 2008, VCEGAI11 ITUT SG16 Q
   Chen CC, 2017, IEEE T CIRC SYST VID, V27, P1568, DOI 10.1109/TCSVT.2016.2543098
   [陈先义 Chen Xianyi], 2015, [电子与信息学报, Journal of Electronics & Information Technology], V37, P2685
   Gao P, 2015, IEEE T MULTIMEDIA, V17, P1153, DOI 10.1109/TMM.2015.2438711
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Guo J., 2015, JCTVCV0097
   Guo L., 2013, P IEEE INT C IM PROC, P5556
   Joshi R., 2015, JCTVCU1005
   Kang JW, 2016, IEEE T MULTIMEDIA, V18, P2054, DOI 10.1109/TMM.2016.2595259
   Li B, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P530, DOI 10.1109/VCIP.2014.7051623
   Li L, 2016, IEEE T MULTIMEDIA, V18, P2023, DOI 10.1109/TMM.2016.2595264
   Lin T., 2013, P IEEE INT C MULT EX, P1
   Lin T, 2017, J ELECTRON INF TECHN, V39, P351, DOI 10.11999/JEIT160560
   Lin T, 2013, PICT COD SYMP, P369, DOI 10.1109/PCS.2013.6737760
   Lin T, 2013, INT J AD HOC UBIQ CO, V13, P96, DOI 10.1504/IJAHUC.2013.054174
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Lin T, 2009, IEEE INT CON MULTI, P1801
   Lin T, 2009, IEEE SIGNAL PROC LET, V16, P323, DOI 10.1109/LSP.2009.2014285
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Van LP, 2016, IEEE T MULTIMEDIA, V18, P364, DOI 10.1109/TMM.2015.2512231
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Marpe D, 2006, IEEE IMAGE PROC, P3157, DOI 10.1109/ICIP.2006.313039
   Miyazawa K., 2014, JCTVCR0074
   Pang C., 2014, JCTVCR0309
   Peng W., 2016, IEEE J EMERGING SEL, V6, P339
   RAMABADRAN TV, 1988, IEEE MICRO, V8, P62, DOI 10.1109/40.7773
   Shuhui Wang, 2010, Proceedings of the 2010 3rd International Congress on Image and Signal Processing (CISP 2010), P566, DOI 10.1109/CISP.2010.5647270
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   'Tsai C.-M., 2014, JCTVCR0168
   Wang SQ, 2009, PROCEEDINGS OF THE 2009 CHINESE CONFERENCE ON PATTERN RECOGNITION AND THE FIRST CJK JOINT WORKSHOP ON PATTERN RECOGNITION, VOLS 1 AND 2, P1, DOI 10.1109/PLASMA.2009.5227540
   Wang SH, 2015, MULTIMED TOOLS APPL, V74, P7753, DOI 10.1007/s11042-014-2021-3
   Wang SH, 2014, MULTIMED TOOLS APPL, V71, P1263, DOI 10.1007/s11042-012-1274-y
   Wang SH, 2013, IET IMAGE PROCESS, V7, P484, DOI 10.1049/iet-ipr.2012.0439
   Wang W, 2015, JCTVCT0126
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Yeh CH, 2015, IEEE T MULTIMEDIA, V17, P1508, DOI 10.1109/TMM.2015.2449659
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zeng WJ, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P476, DOI 10.1109/ICIP.2000.899448
   [张培君 Zhang Peijun], 2013, [电子与信息学报, Journal of Electronics & Information Technology], V35, P196
   Zhao L., 2002, CHIN J COMP IN PRESS
   Zhao L., 2016, JVETB0048
   Zhao L., 2017, CHIN J COMPUT, V40, P1
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhao LP, 2015, JCTVCV0095
   Zhou KL, 2016, IEEE J EM SEL TOP C, V6, P560, DOI 10.1109/JETCAS.2016.2599876
   Zhou KL, 2015, JCTVCV0094
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
   Zhu WJ, 2013, PICT COD SYMP, P373, DOI 10.1109/PCS.2013.6737761
   ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714
NR 56
TC 25
Z9 29
U1 0
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 796
EP 809
DI 10.1109/TMM.2017.2758519
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000003
DA 2024-07-18
ER

PT J
AU Song, XD
   Peng, XL
   Xu, JZ
   Shi, GM
   Wu, F
AF Song, Xiaodan
   Peng, Xiulian
   Xu, Jizheng
   Shi, Guangming
   Wu, Feng
TI Unequal Error Protection for Scalable Video Storage in the Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Unequal error protection; local reconstruction codes; scalable videos;
   simulcast
ID DISTRIBUTED STORAGE; REGENERATING CODES; H.264/AVC; AWARE
AB Redundancy is necessary for a storage system to achieve reliability. Frequent errors in large-scale storage systems, for example, cloud, make it desirable to reduce the cost of recovery. Among all types of data in cloud storage, videos generally occupy significant amounts of space due to high volumes and the rapid development of video sharing and video-on-demand services. Unlike general data, videos can tolerate a certain level of quality degradation. This paper investigates multilayer video representations, such as scalable videos and simulcast streaming, and proposes an unequal error protection scheme based on local reconstruction codes (LRC) for video storage. By providing less protection for less important layers or video copies, a better tradeoff between storage and repair cost is achieved. Both theoretical and simulation results show that such a tradeoff can he achieved over the LRC with equal error protection, though the recovered video quality might be slightly lower in rare cases.
C1 [Song, Xiaodan; Shi, Guangming] Xidian Univ, Xian 710071, Shaanxi, Peoples R China.
   [Peng, Xiulian; Xu, Jizheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
C3 Xidian University; Microsoft; Microsoft Research Asia; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Peng, XL (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM xiaodansong@outlook.com; xipe@microsoft.com; jzxu@microsoft.com;
   gmshi@xidian.edu.cn; fengwu@ustc.edu.cn
RI Xu, Jizheng/JDD-5152-2023; Wu, Feng/KCY-3017-2024; Song,
   Xiaodan/GSE-2144-2022
OI Song, Xiaodan/0000-0002-8049-1828
FU National Natural Science Foundation of China [61632019]; Foundation for
   Innovative Research Groups of the National Natural Science Foundation of
   China [61621005]
FX This work was supported in part by the Major Project under Grant
   61632019 from the National Natural Science Foundation of China and in
   part by the Foundation for Innovative Research Groups of the National
   Natural Science Foundation of China under Grant 61621005.
CR Ahmad S, 2011, IEEE T MULTIMEDIA, V13, P92, DOI 10.1109/TMM.2010.2093511
   [Anonymous], 2011, P 3 USENIX C HOT TOP
   [Anonymous], 1994, GEN COD MOV PICT A 2
   [Anonymous], 2001, 144962 ISOIEC
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Calder B, 2011, SOSP 11: PROCEEDINGS OF THE TWENTY-THIRD ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P143
   Che XH, 2015, IEEE MULTIMEDIA, V22, P56, DOI 10.1109/MMUL.2015.34
   Dimakis AG, 2011, P IEEE, V99, P476, DOI 10.1109/JPROC.2010.2096170
   Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295
   Doulamis ND, 2000, IEEE T CIRC SYST VID, V10, P93, DOI 10.1109/76.825864
   Ghemawat S., 2003, ACM SIGOPS OPERATING, V37, P29, DOI [DOI 10.1145/945445.945450, DOI 10.1145/1165389.945450, 10.1145/1165389.945450]
   Gopalan P, 2012, IEEE T INFORM THEORY, V58, P6925, DOI 10.1109/TIT.2012.2208937
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hellge C, 2011, IEEE T MULTIMEDIA, V13, P551, DOI 10.1109/TMM.2011.2129499
   Hoyland A., 2009, System reliability theory: models and statistical methods, V420
   Huang Cheng, 2012, P USENIX ATC
   *ITU T, 1998, H263 ITUT
   Kamath GM, 2014, IEEE T INFORM THEORY, V60, P4637, DOI 10.1109/TIT.2014.2329872
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   MASNICK B, 1967, IEEE T INFORM THEORY, V13, P600, DOI 10.1109/TIT.1967.1054054
   Memos VA, 2016, J REAL-TIME IMAGE PR, V12, P473, DOI 10.1007/s11554-015-0509-3
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   Papailiopoulos DS, 2014, IEEE T INFORM THEORY, V60, P5843, DOI 10.1109/TIT.2014.2325570
   Papailiopoulos DS, 2013, IEEE T INFORM THEORY, V59, P3021, DOI 10.1109/TIT.2013.2241819
   Papailiopoulos DS, 2012, IEEE INFOCOM SER, P2801, DOI 10.1109/INFCOM.2012.6195703
   Psannis KE, 2006, IEEE T CIRC SYST VID, V16, P280, DOI 10.1109/TCSVT.2005.859933
   Psannis KE, 2016, J REAL-TIME IMAGE PR, V12, P509, DOI 10.1007/s11554-015-0514-6
   Rahnavard N, 2006, IEEE COMMUN LETT, V10, P43, DOI 10.1109/LCOMM.2006.1576564
   Rahnavard N, 2007, IEEE T INFORM THEORY, V53, P1521, DOI 10.1109/TIT.2007.892814
   Rashmi K., 2015, USENIX C FIL STOR TE, P81
   Rashmi KV, 2011, IEEE T INFORM THEORY, V57, P5227, DOI 10.1109/TIT.2011.2159049
   Rashmi K.V., 2013, P 5 USENIX C HOT TOP, P8
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Schierl T, 2012, IEEE T CIRC SYST VID, V22, P1871, DOI 10.1109/TCSVT.2012.2223054
   Schroeder B, 2007, USENIX ASSOCIATION PROCEEDINGS OF THE 5TH USENIX CONFERENCE ON FILE AND STORAGE TECHNOLOGIES ( FAST '07), P1
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sejdinovic D, 2009, IEEE T COMMUN, V57, P2510, DOI 10.1109/TCOMM.2009.09.070616
   Shvachko K., 2010, 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST), P1
   Sprjan N, 2005, INT CONF ACOUST SPEE, P741
   Stockhammer T, 2003, IEEE T CIRC SYST VID, V13, P657, DOI 10.1109/TCSVT.2003.815167
   Stütz T, 2012, IEEE T CIRC SYST VID, V22, P325, DOI 10.1109/TCSVT.2011.2162290
   Suh C., 2010, ARXIV10010107V2
   Suh CH, 2011, IEEE T INFORM THEORY, V57, P1425, DOI 10.1109/TIT.2011.2105003
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tamo I, 2013, IEEE T INFORM THEORY, V59, P1597, DOI 10.1109/TIT.2012.2227110
   Vukobratovic D, 2012, IEEE T COMMUN, V60, P1243, DOI 10.1109/TCOMM.2012.030712.100454
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   Wang X, 2013, IEEE WIREL COMMUN, V20, P72, DOI 10.1109/MWC.2013.6549285
   Wang Z., 2010, ARXIV10093291
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Xiaoqing Song, 2015, 2015 17th European Conference on Power Electronics and Applications (EPE'15 ECCE-Europe), P1, DOI 10.1109/EPE.2015.7309243
   Yekhanin S, 2010, FOUND TRENDS THEOR C, V6, P139, DOI 10.1561/0400000030
NR 52
TC 2
Z9 2
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 699
EP 710
DI 10.1109/TMM.2017.2751147
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500015
DA 2024-07-18
ER

PT J
AU Jeong, SW
   Sim, JY
AF Jeong, Se-Won
   Sim, Jae-Young
TI Saliency Detection for 3D Surface Geometry Using Semi-regular Meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mesh saliency; random walk; saliency detection; semi-regular meshes;
   view-dependent saliency
ID MODEL
AB In this paper, a unified detection algorithm of view independent and view-dependent saliency for three-dimensional mesh models is proposed. While the conventional techniques use the irregular meshes, we adopt the semi-regular meshes to overcome the drawback of irregular connectivity for saliency computation. We employ the angular deviation of normal vectors between neighboring faces as geometric curvature features, which are evaluated at hierarchically structured triangle faces. We construct a fully connected graph at each level of semi-regular mesh, where the face patches serve as graph nodes. At the base mesh level, we estimate the saliency as the stationary distribution of random walk. At the higher level meshes, we take the maximum value between the stationary distribution of random walk at the current level and an upsampled saliency map from the previous coarser scale. Moreover, we also propose a view-dependent saliency detection method that employs the visibility feature in addition to the geometric features to estimate the saliency with respect to a selected viewpoint. Experimental results demonstrate that the proposed saliency detection algorithm captures global conspicuous regions reliably and detects locally detailed geometric features faithfully, compared with the conventional techniques.
C1 [Jeong, Se-Won; Sim, Jae-Young] Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, South Korea.
C3 Ulsan National Institute of Science & Technology (UNIST)
RP Sim, JY (corresponding author), Ulsan Natl Inst Sci & Technol, Sch Elect & Comp Engn, Ulsan 44919, South Korea.
EM swjeong@unist.ac.kr; jysim@unist.ac.kr
FU Global Ph.D Fellowship Program through National Research Foundation of
   Korea(NRF) - Ministry of Education [2015H1A2A1029729]; NRF
   [2016R1D1A1A09919618]
FX This work was supported in part by Global Ph.D Fellowship Program
   through the National Research Foundation of Korea(NRF) funded by the
   Ministry of Education (2015H1A2A1029729), and in part by the NRF under
   Grant 2016R1D1A1A09919618. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Jingdong
   Wang. (Corresponding author: Jae-Young Sim.)
CR [Anonymous], 2005, Markov Chains
   Borji A, 2012, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2012.6247711
   Chen BW, 2009, IEEE T MULTIMEDIA, V11, P295, DOI 10.1109/TMM.2008.2009703
   Chen DY, 2013, IEEE T MULTIMEDIA, V15, P1616, DOI 10.1109/TMM.2013.2267725
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Costa L. da Fontoura, 2007, CORR
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   Feixas M, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462056
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Garland M., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P209, DOI 10.1145/258734.258849
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Gomes L, 2014, PATTERN RECOGN LETT, V50, P3, DOI 10.1016/j.patrec.2014.03.023
   Gopalakrishnan V, 2010, IEEE T IMAGE PROCESS, V19, P3232, DOI 10.1109/TIP.2010.2053940
   Hou XD, 2007, PROC CVPR IEEE, P2280
   Hubeli A, 2001, IEEE VISUAL, P287, DOI 10.1109/VISUAL.2001.964523
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jeong S.-W., 2014, P AS PAC SIGN INF PR, P455
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Kim JS, 2014, IEEE T CIRC SYST VID, V24, P198, DOI 10.1109/TCSVT.2013.2270366
   Kiyokawa K., 2012, 2012 International Symposium on Ubiquitous Virtual Reality (ISUVR), P14, DOI 10.1109/ISUVR.2012.11
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Liu T, 2007, PROC CVPR IEEE, P596
   Liu XP, 2016, VISUAL COMPUT, V32, P1121, DOI 10.1007/s00371-015-1184-x
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Serfozo R., 2013, BASICS APPL STOCHAST
   Shilane P, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1243980.1243981
   Sim JY, 2005, IEEE T CIRC SYST VID, V15, P854, DOI 10.1109/TCSVT.2005.848349
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Wu JL, 2013, GRAPH MODELS, V75, P255, DOI 10.1016/j.gmod.2013.05.002
   You CH, 2009, IEEE SIGNAL PROC LET, V16, P49, DOI 10.1109/LSP.2008.2006711
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
NR 35
TC 15
Z9 17
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2692
EP 2705
DI 10.1109/TMM.2017.2710802
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200005
DA 2024-07-18
ER

PT J
AU Jing, LP
   Liu, B
   Choi, J
   Janin, A
   Bernd, J
   Mahoney, MW
   Friedland, G
AF Jing, Liping
   Liu, Bo
   Choi, Jaeyoung
   Janin, Adam
   Bernd, Julia
   Mahoney, Michael W.
   Friedland, Gerald
TI DCAR: A Discriminative and Compact Audio Representation for Audio
   Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio representation; audio scene classification; compact
   representation; discriminativity; event detection
ID CLASSIFICATION
AB This paper presents a novel two-phase method for audio representation, discriminative and compact audio representation (DCAR), and evaluates its performance at detecting events and scenes in consumer-produced videos. In the first phase of DCAR, each audio track is modeled using a Gaussian mixture model (GMM) that includes several components to capture the variability within that track. The second phase takes into account both global structure and local structure. In this phase, the components are rendered more discriminative and compact by formulating an optimization problem on a Grassmannian manifold. The learned components can effectively represent the structure of audio. Our experiments used the YLI-MED and DCASE Acoustic Scenes datasets. The results show that variants on the proposed DCAR representation consistently outperform four popular audio representations (mv-vector, i-vector, GMM, and HEM-GMM). The advantage is significant for both easier and harder discrimination tasks; we discuss how these performance differences across tasks follow from how each type of model leverages (or does not leverage) the intrinsic structure of the data.
C1 [Jing, Liping; Liu, Bo] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Liu, Bo] Agr Univ Hebei, Coll Informat Sci & Technol, Baoding 071000, Hebei, Peoples R China.
   [Choi, Jaeyoung; Janin, Adam; Bernd, Julia; Mahoney, Michael W.] Int Comp Sci Inst, Berkeley, CA 94704 USA.
   [Choi, Jaeyoung] Delft Univ Technol, NL-2628 Delft, Netherlands.
   [Mahoney, Michael W.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.
   [Friedland, Gerald] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
   [Friedland, Gerald] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 Beijing Jiaotong University; Hebei Agricultural University; Delft
   University of Technology; University of California System; University of
   California Berkeley; University of California System; University of
   California Berkeley; United States Department of Energy (DOE); Lawrence
   Livermore National Laboratory
RP Jing, LP (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM lpjing@bjtu.edu.cn; 12112082@bjtu.edu.cn; jaeyoung.icsi@gmail.com;
   janin@icsi.berkeley.edu; jbernd@icsi.berkeley.edu;
   mmahoney@stat.berkeley.edu; fractor@eecs.berkeley.edu
RI Liu, Bo/AAT-4011-2020
OI Liu, Bo/0000-0001-6615-1096; Bernd, Julia/0000-0002-9792-2743
FU NSFC [61370129, 61375062, 61632004]; PCSIRT [IRT201206]; collaborative
   Laboratory Directed Research & Development grant by Lawrence Livermore
   National Laboratory (U.S. Dept. of Energy) [DE-AC52-07NA27344]; Science
   and Technology Bureau of Baoding City [16ZG026]
FX This work was supported in part by the NSFC under Grant 61370129, Grant
   61375062, and Grant 61632004, in part by the PCSIRT under Grant
   IRT201206, in part by a collaborative Laboratory Directed Research &
   Development grant led by the Lawrence Livermore National Laboratory
   (U.S. Dept. of Energy Contract DE-AC52-07NA27344), and in part by the
   Science and Technology Bureau of Baoding City under Grant 16ZG026. This
   paper was presented in part at the ACM Multimedia Conference, Amsterdam,
   The Netherlands, October 2016. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Yi-Hsuan Yang. (Corresponding author: Liping Jing.)
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Barchiesi D, 2015, IEEE SIGNAL PROC MAG, V32, P16, DOI 10.1109/MSP.2014.2326181
   Bernd J., 2015, TR15001 INT COMP SCI
   Brent R. P., 1973, Algorithms for Minimization without Derivatives
   Cai R, 2008, IEEE T MULTIMEDIA, V10, P596, DOI 10.1109/TMM.2008.921739
   Casanovas AL, 2010, IEEE T MULTIMEDIA, V12, P358, DOI 10.1109/TMM.2010.2050650
   Cherian A, 2011, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2011.6126523
   Coviello E, 2011, IEEE T AUDIO SPEECH, V19, P1343, DOI 10.1109/TASL.2010.2090148
   Coviello Emanuele., 2012, ISMIR, P547
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Eghbal-Zadeh H., 2016, IEEE AASP CHALL DET
   Elizalde B, 2013, IEEE INT SYM MULTIM, P114, DOI 10.1109/ISM.2013.27
   Eronen A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P529
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Huang Z, 2013, INTERSPEECH, P2281
   Jin Q, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P2083
   Jing LP, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P57, DOI 10.1145/2964284.2970377
   Jing LP, 2012, IEEE T IMAGE PROCESS, V21, P4508, DOI 10.1109/TIP.2012.2206040
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Lan M, 2009, IEEE T PATTERN ANAL, V31, P721, DOI 10.1109/TPAMI.2008.110
   Li H., 2003, Advances in Neural Information Processing Systems, V16, P97
   Manor LZ., 2005, Proceedings of the Advances in Neural Information Processing Systems, V27, P1601
   Mclachlan GJ., 2005, DISCRIMINANT ANAL ST
   Mertens Robert., 2011, Joint ACM Workshop on Modeling and Representing Events, P19
   Mesaros A, 2016, EUR SIGNAL PR CONF, P1128, DOI 10.1109/EUSIPCO.2016.7760424
   Over P., 2012, TRHAL00763912 NAT I
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z
   Ravanelli M, 2015, MMCOMMONS'15: PROCEEDINGS OF THE 2015 WORKSHOP ON COMMUNITY-ORGANIZED MULTIMODAL MINING: OPPORTUNITIES FOR NOVEL SOLUTIONS, P19, DOI 10.1145/2814815.2814816
   Roma G., 2013, IEEE AASP CHALLENGE, P1
   Scholkopf B., 2002, Learning with Kernels
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   Valero X, 2012, IEEE T MULTIMEDIA, V14, P1684, DOI 10.1109/TMM.2012.2199972
   Wang W, 2015, PROC CVPR IEEE, P3395, DOI 10.1109/CVPR.2015.7298816
NR 38
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2637
EP 2650
DI 10.1109/TMM.2017.2703939
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200001
DA 2024-07-18
ER

PT J
AU Vonikakis, V
   Subramanian, R
   Arnfred, J
   Winkler, S
AF Vonikakis, Vassilios
   Subramanian, Ramanathan
   Arnfred, Jonas
   Winkler, Stefan
TI A Probabilistic Approach to People-Centric Photo Selection and
   Sequencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing (CS); image appeal; mixed integer linear programming
   (MILP); personal photo libraries; slideshow creation
ID RECOGNITION; COLLECTION; FACES
AB We present a crowdsourcing (CS) study to examine how specific attributes probabilistically affect the selection and sequencing of images from personal photo collections. Thirteen image attributes are explored, including seven people-centric properties. We first propose a novel dataset shaping technique based on mixed integer linear programming (MILP) to identify a subset of photos in which the attributes of interest are uniformly distributed and minimally correlated. Shaping enables the synthesis of compact, balanced, and representative datasets for CS, and facilitates effective learning of the selection likelihood of an image as well as its relative position in a sequence, given its attributes. We further present an ILP-based slideshow creation framework to select and arrange (a subset of) appealing images froma personal photo library. Quantitative and qualitative evaluations confirm that our method outperforms regression-based and greedy approaches for photo selection and sequencing, generating slideshows similar in quality to those created by humans.
C1 [Vonikakis, Vassilios; Winkler, Stefan] Univ Illinois, Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Subramanian, Ramanathan] Int Inst Informat Technol, Hyderabad 500032, Andhra Pradesh, India.
   [Arnfred, Jonas] Amazon, London SL1 1QP, Berks, England.
C3 International Institute of Information Technology Hyderabad
RP Vonikakis, V (corresponding author), Univ Illinois, Adv Digital Sci Ctr, Singapore 138632, Singapore.
EM bbonik@adsc.com.sg; s.ramanathan@iiit.ac.in; jonas@ifany.org;
   ste-fan.winkler@adsc.com.sg
RI Winkler, Stefan/ACL-6097-2022
OI Winkler, Stefan/0000-0003-4305-8408; Subramanian,
   Ramanathan/0000-0001-9441-7074
FU Singapore's Agency for Science, Technology, and Research (A*STAR)
FX This work was supported by the Human-Centered Cyber-physical Systems
   research grant from Singapore's Agency for Science, Technology, and
   Research (A*STAR). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Tao Mei.
   (Corresponding author: Vassilios Vonikakis.)
CR [Anonymous], 2010, P ACM INT C MULT INF, DOI 10.1145/1743384.1743457
   [Anonymous], P 2 ACM INT WORKSH C
   [Anonymous], 2012, PROC 20 ACM INT C MU
   [Anonymous], P SIGGRAPH
   [Anonymous], 2014, MSRTR201491
   [Anonymous], 2015, P IEEE INT C AUT FAC, DOI DOI 10.1109/FG.2015.7163082
   [Anonymous], P ACM INT WORKSH CON
   [Anonymous], P SPIE
   [Anonymous], 2010, P 18 ACM INT C MULTI
   [Anonymous], 2007, SNC'07. Proceedings of the 2007 International Workshop on Symbolic-Numeric Computation
   [Anonymous], 2014, P 2014 INT ACM WORKS, DOI [DOI 10.1145/2660114.2660126, 10.1145/2660114.2660126]
   Atamtürk A, 2005, ANN OPER RES, V140, P67, DOI 10.1007/s10479-005-3968-2
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Bakhshi S, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P965, DOI 10.1145/2556288.2557403
   Birmingham E, 2009, VISION RES, V49, P2992, DOI 10.1016/j.visres.2009.09.014
   Choi JY, 2011, IEEE T MULTIMEDIA, V13, P14, DOI 10.1109/TMM.2010.2087320
   Clausen J, 1997, APPL OPTIMIZAT, V7, P239
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gallagher A., 2008, PROC IEEE C COMPUT V, P1
   Gilani SO, 2013, IEEE IMAGE PROC, P231, DOI 10.1109/ICIP.2013.6738048
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Hays J, 2008, PROC CVPR IEEE, P3436
   Hossfeld T, 2014, IEEE T MULTIMEDIA, V16, P541, DOI 10.1109/TMM.2013.2291663
   Isola P, 2014, IEEE T PATTERN ANAL, V36, P1469, DOI 10.1109/TPAMI.2013.200
   Jacob P, 2011, J STAT COMPUT SIM, V81, P109, DOI 10.1080/00949650903218861
   Kapoor A, 2009, IEEE I CONF COMP VIS, P1058, DOI 10.1109/ICCV.2009.5459392
   Khan S.S., 2012, P 8 ANN S COMPUTATIO, P55
   Kherfi ML, 2007, IEEE T MULTIMEDIA, V9, P893, DOI 10.1109/TMM.2007.893349
   Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Li CC, 2010, IEEE IMAGE PROC, P3221, DOI 10.1109/ICIP.2010.5651833
   Lin DH, 2010, LECT NOTES COMPUT SC, V6311, P243
   Liu Q., 2013, Advances in Neural Information Processing Systems (NIPS), P1914
   Liu Y, 2017, AAAI CONF ARTIF INTE, P1445
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Obrador P., 2010, PROC 18 ACM INT C MU, P561
   Obrador P, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P700
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P1231, DOI 10.1109/TMM.2013.2261481
   Ryu DS, 2012, MULTIMED TOOLS APPL, V61, P523, DOI 10.1007/s11042-010-0700-2
   Saad MA, 2016, J ELECTRON IMAGING, V25, DOI 10.1117/1.JEI.25.4.043009
   San Pedro Jose., 2009, WWW, P771, DOI DOI 10.1145/1526709.1526813
   Savakis AE, 2000, P SOC PHOTO-OPT INS, V3959, P111, DOI 10.1117/12.387147
   Siahaan E, 2016, IEEE T MULTIMEDIA, V18, P1338, DOI 10.1109/TMM.2016.2559942
   Strong G, 2011, IMAGE VISION COMPUT, V29, P774, DOI 10.1016/j.imavis.2011.08.007
   Subramanian Ramanathan., 2011, Proceedings of the 19th ACM International Conference on Multimedia, P33
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Vonikakis V, 2016, IEEE IMAGE PROC, P3753, DOI 10.1109/ICIP.2016.7533061
   Vonikakis V, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P153
   Walber T, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2065, DOI 10.1145/2556288.2557025
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Wild B, 2001, PSYCHIAT RES, V102, P109, DOI 10.1016/S0165-1781(01)00225-6
   Wu Y, 2016, IEEE T MULTIMEDIA, V18, P2206, DOI 10.1109/TMM.2016.2614185
   Yang JC, 2012, IEEE T MULTIMEDIA, V14, P1642, DOI 10.1109/TMM.2012.2198458
   Yu KM, 2013, PATTERN RECOGN, V46, P2144, DOI 10.1016/j.patcog.2013.01.032
   Yuan JS, 2010, IEEE T MULTIMEDIA, V12, P705, DOI 10.1109/TMM.2010.2051868
   Zhang LG, 2014, IMAGE VISION COMPUT, V32, P1067, DOI 10.1016/j.imavis.2014.09.005
   Zhang N, 2015, PROC CVPR IEEE, P4804, DOI 10.1109/CVPR.2015.7299113
   Zhi CH, 2011, PROC CVPR IEEE, P481, DOI 10.1109/CVPR.2011.5995680
NR 61
TC 15
Z9 16
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2609
EP 2624
DI 10.1109/TMM.2017.2699859
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200020
DA 2024-07-18
ER

PT J
AU Ma, Z
   Yue, T
   Cao, X
   Xu, YL
   Li, X
   Wang, YJ
AF Ma, Zhan
   Yue, Tao
   Cao, Xun
   Xu, Yiling
   Li, Xin
   Wang, Yongjin
TI Interactive Screen Video Streaming-Based Pervasive Mobile Workstyle
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High-efficiency video coding; pervasive computing; screen content
   coding; virtual desktop infrastructure
ID HEVC STANDARD; CODING HEVC; EFFICIENCY; COMPRESSION; QUALITY
AB In this paper, we develop an interactive screen video streaming-based system to enable the ubiquitous mobile workstyle, which is referred to as personal computer to pervasive computing (PC2PC). The desktop screens of virtualized systems are compressed in the PC2PC servers and delivered to remote end users for stream decoding, rendering, and interactions. We have implemented a system from the scratch, where the emerging screen content coding extension of high-efficiency video coding is implemented to compress and stream the desktop screens of the virtualized system in real time. Three core asset channels, system, display, and inputs, are defined to enable systematic end-to-end communication. Compared with Red Hat SPICE virtual desktop infrastructure scheme, the proposed PC2PC could save network bandwidth consumption by a factor of 2, 7, and 4, respectively, in terms of typical video streaming, web browsing, and stationary office applications at the same visual quality. Meanwhile, we have also measured the delays of the system and presented preliminary results on the user experience aspect. A simple network estimation is applied to optimize the quality bandwidth adaptation for both single user and multiuser scenarios to consider the network dynamics.
C1 [Ma, Zhan] Nanjing Univ, Fac Elect Sci & Engn, Nanjing 210008, Jiangsu, Peoples R China.
   [Yue, Tao] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210008, Jiangsu, Peoples R China.
   [Cao, Xun] Nanjing Univ, Nanjing 210008, Jiangsu, Peoples R China.
   [Xu, Yiling] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Li, Xin] Yun Ge Zhi Li Inc, Richardson, TX 75082 USA.
   [Wang, Yongjin] Nanjing Univ Posts & Telecommun, Nanjing 210028, Jiangsu, Peoples R China.
C3 Nanjing University; Nanjing University; Nanjing University; Shanghai
   Jiao Tong University; Nanjing University of Posts & Telecommunications
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM mazhan@nju.edu.cn; yuetao@nju.edu.cn; caoxun@nju.edu.cn;
   yl.xu@sjtu.edu.cn; lixin@gwecom.com; wangyj@njupt.edu.cn
RI 岳, 涛/IST-6884-2023; Sun, Yuchen/JZD-1692-2024; Ma, Zhan/HKW-2859-2023
OI Ma, Zhan/0000-0003-3686-4057
FU National Natural Science Foundation of China (NSFC) [61371166,
   61422107]; NSFC [61571215, 61650101, 61322112, 61531166004]; National
   Science Foundation for Young Scholar of Jiangsu Province, China
   [BK20140610]; Natural Science Foundation of Jiangsu Province [BE2016186]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) Projects under Grant 61371166 and Grant
   61422107, in part by the NSFC Projects under Grant 61571215, Grant
   61650101, Grant 61322112, and Grant 61531166004, in part by the National
   Science Foundation for Young Scholar of Jiangsu Province, China, under
   Grant BK20140610, and in part by the Natural Science Foundation of
   Jiangsu Province under Grant BE2016186. The guest editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Shiwen Mao. (Corresponding author: Yiling Xu.)
CR [Anonymous], 2017, PC2PC
   [Anonymous], 2016, FIREFLY RK3288
   [Anonymous], 2017, VMware Horizon
   [Anonymous], JCTVCQ0034
   [Anonymous], 2014, MICROSOFT RDP
   [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], JCTVCQ0031
   [Anonymous], JCTVCN0287
   [Anonymous], 2017, AUDIO VIDEO CODING S
   Assi CM, 2003, IEEE J SEL AREA COMM, V21, P1467, DOI 10.1109/JSAC.2003.818837
   Boyce J., 2014, JCTVCS0304
   Chen KT, 2014, IEEE T MULTIMEDIA, V16, P480, DOI 10.1109/TMM.2013.2291532
   Citrix Mobile Workstyles Survey, 2014, WORKPL FUT GLOB MARK
   Fang L, 2013, IEEE SIGNAL PROC MAG, V30, P177, DOI 10.1109/MSP.2013.2241311
   Grange A., 2012, OVERVIEW VP NEXT
   Guo L., 2013, JCTVCN0247
   Hu H, 2012, IEEE IMAGE PROC, P717, DOI 10.1109/ICIP.2012.6466960
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Kwon D. -K., 2013, JCTVCN0205
   Li B., 2014, document JCTVC-S0085
   Lin T, 2013, IEEE T CIRC SYST VID, V23, P173, DOI 10.1109/TCSVT.2012.2223871
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Ma Z, 2014, IEEE T IMAGE PROCESS, V23, P4399, DOI 10.1109/TIP.2014.2346995
   Ma Z, 2012, IEEE T CIRC SYST VID, V22, P671, DOI 10.1109/TCSVT.2011.2177143
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Ou YF, 2008, IEEE IMAGE PROC, P689, DOI 10.1109/ICIP.2008.4711848
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Pang C, 2013, JCTVCN0256
   Pu W., 2014, JCTVCQ0094
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   VMware Brief, 2014, MOB SEC DESKT SOL MO
   Wang SQ, 2017, IEEE T MULTIMEDIA, V19, P660, DOI 10.1109/TMM.2016.2625276
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xin Feng, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457932
   Yaïche H, 2000, IEEE ACM T NETWORK, V8, P667, DOI 10.1109/90.879352
   ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934
NR 37
TC 6
Z9 6
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2322
EP 2332
DI 10.1109/TMM.2017.2737944
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600015
OA Bronze
DA 2024-07-18
ER

PT J
AU Liong, VE
   Lu, JW
   Tan, YP
   Zhou, J
AF Liong, Venice Erin
   Lu, Jiwen
   Tan, Yap-Peng
   Zhou, Jie
TI Deep Coupled Metric Learning for Cross-Modal Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coupled learning; cross-modal matching; deep model; metric learning;
   multimedia retrieval
ID SPECTRAL REGRESSION; FACE; IMAGES
AB In this paper, we propose a new deep coupled metric learning (DCML) method for cross-modal matching, which aims to match samples captured from two different modalities (e.g., texts versus images, visible versus near infrared images). Unlike existing cross-modal matching methods which learn a linear common space to reduce the modality gap, our DCML designs two feedforward neural networks which learn two sets of hierarchical nonlinear transformations (one set for each modality) to nonlinearly map samples from different modalities into a shared latent feature subspace, under which the intraclass variation is minimized and the interclass variation is maximized, and the difference of each data pair captured from two modalities of the same class is minimized, respectively. Experimental results on four different cross-modal matching datasets validate the efficacy of the proposed approach.
C1 [Liong, Venice Erin] Nanyang Technol Univ, Interdisciplinary Grad Sch, Rapid Rich Object Search Lab, Singapore 639798, Singapore.
   [Lu, Jiwen; Zhou, Jie] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University; Tsinghua University; Nanyang
   Technological University
RP Lu, JW (corresponding author), Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM veniceer001@e.ntu.edu.sg; lujiwen@tsinghua.edu.cn; eyptan@ntu.edu.sg;
   jzhou@tsinghua.edu.cn
RI Tan, Yap-Peng/A-5158-2011; Lu, Jiwen/C-5291-2009
OI Lu, Jiwen/0000-0002-6121-5529
FU National Key Research and Development Program of China [2016YFB1001001];
   National Natural Science Foundation of China [61672306, 61225008,
   61572271, 61527808, 61373074, 61373090]; National 1000 Young Talents
   Plan Program; National Basic Research Program of China [2014CB349304];
   Ministry of Education of China [20120002110033]; Tsinghua University
   Initiative Scientific Research Program
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001001, in part by the
   National Natural Science Foundation of China under Grant 61672306, Grant
   61225008, Grant 61572271, Grant 61527808, Grant 61373074, and Grant
   61373090, in part by the National 1000 Young Talents Plan Program, in
   part by the National Basic Research Program of China under Grant
   2014CB349304, in part by the Ministry of Education of China under Grant
   20120002110033, and in part by the Tsinghua University Initiative
   Scientific Research Program. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran. (Corresponding author: Jiwen Lu.)
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2015, P INT C INF KNOWL MA
   [Anonymous], P 3 INT WORKSH CONT
   [Anonymous], 2008, P 8 IEEE INT C AUT F
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2015, IEEE C WORKSH AUT FA
   [Anonymous], FACEVACS SOFTWARE DE
   [Anonymous], 2013, P IEEE 6 INT C BIOM
   [Anonymous], 2012, AS C COMP VIS
   Ballan L., 2014, P INT C MULTIMEDIA R, P73
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Pereira JC, 2014, IEEE T PATTERN ANAL, V36, P521, DOI 10.1109/TPAMI.2013.142
   Dhamecha TI, 2014, INT C PATT RECOG, P1788, DOI 10.1109/ICPR.2014.314
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Guillaumin M, 2010, PROC CVPR IEEE, P902, DOI 10.1109/CVPR.2010.5540120
   Habibian A, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P131, DOI 10.1145/2671188.2749403
   Hinton G. E., 2012, Neural networks: tricks of the trade, P599
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Huang XS, 2013, IEEE T IMAGE PROCESS, V22, P353, DOI 10.1109/TIP.2012.2215617
   Hwang S. J., 2010, PROC BRIT MACH VIS C, P1
   Hwang SJ, 2012, INT J COMPUT VISION, V100, P134, DOI 10.1007/s11263-011-0494-3
   Jin Y, 2015, IEEE T INF FOREN SEC, V10, P640, DOI 10.1109/TIFS.2015.2390414
   Juefei-Xu Felix, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P141, DOI 10.1109/CVPRW.2015.7301308
   Kan MN, 2012, LECT NOTES COMPUT SC, V7572, P808, DOI 10.1007/978-3-642-33718-5_58
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kim J., 2012, PROC COLING, P579
   Klare BF, 2013, IEEE T PATTERN ANAL, V35, P1410, DOI 10.1109/TPAMI.2012.229
   Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lei Z, 2012, IEEE T INF FOREN SEC, V7, P1707, DOI 10.1109/TIFS.2012.2210041
   Lei Z, 2009, PROC CVPR IEEE, P1123, DOI 10.1109/CVPRW.2009.5206860
   Li AN, 2011, PATTERN RECOGN LETT, V32, P1948, DOI 10.1016/j.patrec.2011.07.020
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Lin DH, 2006, LECT NOTES COMPUT SC, V3954, P13
   Liu QS, 2005, PROC CVPR IEEE, P1005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Sharma A, 2011, PROC CVPR IEEE, P593, DOI 10.1109/CVPR.2011.5995350
   Siena S, 2012, LECT NOTES COMPUT SC, V7584, P240, DOI 10.1007/978-3-642-33868-7_24
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Sun Y, 2013, IEEE I CONF COMP VIS, P1489, DOI 10.1109/ICCV.2013.188
   Szegedy C., 2013, Advances in Neural Information Processing Systems, V26, P2553
   Tang XO, 2004, IEEE T CIRC SYST VID, V14, P50, DOI 10.1109/TCSVT.2003.818353
   Wang C, 2016, MULTIMED TOOLS APPL, V75, P9255, DOI 10.1007/s11042-016-3380-8
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Xu Y, 2015, IEEE INT CONF RFID, P1, DOI 10.1109/RFID.2015.7113066
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
NR 59
TC 92
Z9 100
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1234
EP 1244
DI 10.1109/TMM.2016.2646180
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400010
DA 2024-07-18
ER

PT J
AU Baraldi, L
   Grana, C
   Cucchiara, R
AF Baraldi, Lorenzo
   Grana, Costantino
   Cucchiara, Rita
TI Recognizing and Presenting the Storytelling Video Structure With Deep
   Multimodal Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep networks; performance evaluation; scene detection; temporal video
   segmentation
ID SCENE DETECTION; SEGMENTATION; RETRIEVAL
AB In this paper, we propose a novel scene detection algorithm which employs semantic, visual, textual, and audio cues. We also show how the hierarchical decomposition of the storytelling video structure can improve retrieval results presentation with semantically and aesthetically effective thumbnails. Our method is built upon two advancements of the state of the art: first is semantic feature extraction which builds video-specific concept detectors; and second is multimodal feature embedding learning that maps the feature vector of a shot to a space in which the Euclidean distance has task specific semantic properties. The proposed method is able to decompose the video in annotated temporal segments which allow us for a query specific thumbnail extraction. Extensive experiments are performed on different data sets to demonstrate the effectiveness of our algorithm. An in-depth discussion on how to deal with the subjectivity of the task is conducted and a strategy to overcome the problem is suggested.
C1 [Baraldi, Lorenzo; Grana, Costantino; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, I-41121 Modena, Italy.
C3 Universita di Modena e Reggio Emilia
RP Baraldi, L (corresponding author), Univ Modena & Reggio Emilia, Dept Engn Enzo Ferrari, I-41121 Modena, Italy.
EM lorenzo.baraldi@unimore.it; costantino.grana@unimore.it;
   rita.cucchiara@unimore.it
RI Cucchiara, Rita/L-3006-2015; Grana, Costantino/B-4555-2012
OI Grana, Costantino/0000-0002-4792-2358; Baraldi,
   Lorenzo/0000-0001-5125-4957
FU project "Citta educante" of the National Technological Cluster on Smart
   Communities - Italian Ministry of Education, University and Research
   (MIUR) [CTN01_00034_393801]
FX This work was supported by the project "Citta educante"
   (CTN01_00034_393801) of the National Technological Cluster on Smart
   Communities that is cofunded by the Italian Ministry of Education,
   University and Research (MIUR). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Tao
   Mei.
CR Aner A, 2002, LECT NOTES COMPUT SC, V2353, P388
   [Anonymous], 2016, GLOBAL INTERNET PHEN
   [Anonymous], 2000, ISMIR
   [Anonymous], 2016, BBC PLANET EARTH SER
   [Anonymous], 2006, PROC 5 INT C LANGUAG
   [Anonymous], 2015, 3 ANN MILLENNIAL VID
   [Anonymous], P 2006 IEEE COMP SOC
   Apostolidis Evlampios, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6583, DOI 10.1109/ICASSP.2014.6854873
   Ballan L, 2015, COMPUT VIS IMAGE UND, V140, P58, DOI 10.1016/j.cviu.2015.05.009
   Baraldi L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1199, DOI 10.1145/2733373.2806316
   Chasanis VT, 2009, IEEE T MULTIMEDIA, V11, P89, DOI 10.1109/TMM.2008.2008924
   Chunxi Liu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2449, DOI 10.1109/ICIP.2011.6116155
   Craggs B, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P1217
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ercolessi P., 2011, P WORKSHOP IMAGE ANA, P13
   Friedman J., 2001, The Elements of Statistical Learning, V10, DOI 10.1007/978-0-387-21606-5
   Glorot X., 2010, P INT C ART INT STAT, P249
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Habibian A., 2015, CORR
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P580, DOI 10.1109/76.767124
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   Hong-Wen Kang, 2005, 13th Annual ACM International Conference on Multimedia, P423, DOI 10.1145/1101149.1101242
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lin DH, 2014, PROC CVPR IEEE, P2657, DOI 10.1109/CVPR.2014.340
   Liu CL, 2013, IEEE T MULTIMEDIA, V15, P884, DOI 10.1109/TMM.2013.2238522
   Liu W, 2015, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2015.7298994
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Petersohn C., 2010, Temporal Video Segmentation
   Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sidiropoulos P, 2012, IEEE T CIRC SYST VID, V22, P904, DOI 10.1109/TCSVT.2011.2181231
   Sidiropoulos P, 2011, IEEE T CIRC SYST VID, V21, P1163, DOI 10.1109/TCSVT.2011.2138830
   Simonyan K., 2014, CORR
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   Wu Y, 2016, IEEE T MULTIMEDIA, V18, P2206, DOI 10.1109/TMM.2016.2614185
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   YEUNG MM, 1995, P SOC PHOTO-OPT INS, V2417, P399, DOI 10.1117/12.206067
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu SH, 2009, EXPERT SYST APPL, V36, P5976, DOI 10.1016/j.eswa.2008.07.009
NR 47
TC 31
Z9 32
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 955
EP 968
DI 10.1109/TMM.2016.2644872
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Gao, GY
   Hu, H
   Wen, YG
   Westphal, C
AF Gao, Guanyu
   Hu, Han
   Wen, Yonggang
   Westphal, Cedric
TI Resource Provisioning and Profit Maximization for Transcoding in Clouds:
   A Two-Timescale Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud computing; profit maximization; resource provisioning; scheduling;
   transcoding
ID DATA CENTERS; SERVICE; ENERGY
AB Transcoding is widely adopted for content adaptation; however, it may incur excessive resource consumption and processing delays. Taking advantage of cloud infrastructure, cloud-based transcoding can elastically allocate resources under time-varying workloads and perform multiple transcodings in parallel to reduce delays. To provide transcoding as a cloud service, cloud transcoding systems require some intelligent mechanisms to provision resources and schedule tasks to satisfy user requirements while maximizing financial profit. To this end, we propose a two-timescale stochastic optimization framework for maximizing service profit while achieving performance requirements by jointly provisioning resources and scheduling tasks under a hierarchical control architecture. Our method analytically integrates service revenue, processing delay, and resource consumption in one optimization framework. We derive the offline exact solution and design some approximate online solutions for task scheduling and resource provisioning. We implement an open source cloud transcoding system, called Morph, and evaluate the performance of our method in a real environment. Empirical studies verify that our method can reduce resource consumption and achieve a higher profit compared with baseline schemes.
C1 [Gao, Guanyu] Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
   [Hu, Han; Wen, Yonggang] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Westphal, Cedric] Huawei Innovat Ctr, Santa Clara, CA 95050 USA.
   [Westphal, Cedric] Univ Calif Santa Cruz, Dept Comp Engn, Santa Cruz, CA 95064 USA.
C3 Nanyang Technological University; Nanyang Technological University;
   Huawei Technologies; University of California System; University of
   California Santa Cruz
RP Gao, GY (corresponding author), Nanyang Technol Univ, Interdisciplinary Grad Sch, Singapore 639798, Singapore.
EM ggao001@ntu.edu.sg; hhu@ntu.edu.sg; ygwen@ntu.edu.sg;
   cwestphal@huawei.com
RI Gao, Guanyu/ACR-3456-2022; Wen, Yonggang/P-9406-2017; Wen,
   Yonggang/B-8848-2011
OI Wen, Yonggang/0000-0002-2751-5114; 
FU Singapore MOE tier-1 fund [RG 17/14]; Singapore-MIT Alliance for
   Research and Technology Innovation [ING148077-ICT]; Singapore EMA
   [EIRP02]
FX This work was supported in part by the Singapore MOE tier-1 fund under
   Grant RG 17/14, in part by the Singapore-MIT Alliance for Research and
   Technology Innovation under Grant ING148077-ICT (IGN), and in part by
   the Singapore EMA under Grant EIRP02. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Balakrishnan Prabhakaran.
CR [Anonymous], 1998, THESIS
   [Anonymous], 2012, 2012 VISUAL COMMUNIC
   [Anonymous], 2010, P IEEE INFOCOM
   [Anonymous], 1998, REINFORCEMENT LEARNI
   Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953
   Bertsekas DP, 2017, DYNAMIC PROGRAMMING
   Cao JW, 2013, IEEE T PARALL DISTR, V24, P1087, DOI 10.1109/TPDS.2012.203
   Careless J., 2012, STREAMING MEDIA
   Chang HS, 2003, IEEE T AUTOMAT CONTR, V48, P976, DOI 10.1109/TAC.2003.812782
   Duan Q, 2015, DIGIT COMMUN NETW, V1, P181, DOI 10.1016/j.dcan.2015.05.003
   Gao GY, 2015, IEEE T MULTIMEDIA, V17, P1286, DOI 10.1109/TMM.2015.2438713
   Gao Guanyu., 2016, MM, P1160, DOI DOI 10.1145/2964284.2973792
   Ghamkhari M, 2013, IEEE T SMART GRID, V4, P1017, DOI 10.1109/TSG.2013.2237929
   Guanyu Gao, 2016, 2016 IEEE Conference on Computer Communications: Workshops (INFOCOM WKSHPS), P97, DOI 10.1109/INFCOMW.2016.7562053
   Hu H, 2014, IEEE MULTIMEDIA, V21, P10, DOI 10.1109/MMUL.2014.2
   Jokhio F, 2013, EUROMICRO WORKSHOP P, P254, DOI 10.1109/PDP.2013.44
   Li Z., 2012, Proc. of ACM Workshop on Network and Operating System Support for Digital Audio and Video (NOSSDAV), P33, DOI DOI 10.1145/2229087.2229097
   Lin MH, 2013, IEEE ACM T NETWORK, V21, P1378, DOI 10.1109/TNET.2012.2226216
   Lin S, 2013, IEEE INT SYMP CIRC S, P2864, DOI 10.1109/ISCAS.2013.6572476
   Ma H., 2014, Proc. of ACM Multimedia Systems Conference. ACM, P283
   MAHMOUD MS, 1977, IEEE T SYST MAN CYB, V7, P125, DOI 10.1109/TSMC.1977.4309677
   Niyato D, 2007, IEEE COMMUN MAG, V45, P140, DOI 10.1109/MCOM.2007.358861
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   Pinedo ML, 2012, SCHEDULING: THEORY, ALGORITHMS, AND SYSTEMS, FOURTH EDITION, P1, DOI 10.1007/978-1-4614-2361-4
   Ran Y., IEEE T SERV IN PRESS
   Song M, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700282
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Tang JH, 2014, IEEE T MULTIMEDIA, V16, P1434, DOI 10.1109/TMM.2014.2308726
   Timmerer C., 2015, IEEE COMSOC MMTC E L, V10, P7
   Tsakalozos K, 2011, PROC INT CONF DATA, P75, DOI 10.1109/ICDE.2011.5767932
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang SW, 2015, DIGIT COMMUN NETW, V1, P161, DOI 10.1016/j.dcan.2015.09.004
   Wang W., 2012, EURASIP J WIREL COMM, V2012, P1, DOI DOI 10.1109/HPEC.2012.6408660
   Wang Z, 2014, IEEE INFOCOM SER, P91, DOI 10.1109/INFOCOM.2014.6847928
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wernz C, 2013, EURO J DECIS PROCESS, V1, P299, DOI 10.1007/s40070-013-0020-7
   Wu Y, 2013, IEEE T MULTIMEDIA, V15, P821, DOI 10.1109/TMM.2013.2240670
   Xiao WH, 2016, IEEE T PARALL DISTR, V27, P1954, DOI 10.1109/TPDS.2015.2470676
   Yao Y, 2012, IEEE INFOCOM SER, P1431, DOI 10.1109/INFCOM.2012.6195508
   Zhang Q, 2014, IEEE T CLOUD COMPUT, V2, P14, DOI 10.1109/TCC.2014.2306427
   Zhang WW, 2014, IEEE T VEH TECHNOL, V63, P2002, DOI 10.1109/TVT.2014.2310394
   Zhao J, 2014, IEEE INFOCOM SER, P118, DOI 10.1109/INFOCOM.2014.6847931
NR 43
TC 28
Z9 28
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 836
EP 848
DI 10.1109/TMM.2016.2635019
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500014
DA 2024-07-18
ER

PT J
AU Wang, SQ
   Zhang, XF
   Liu, XM
   Zhang, J
   Ma, SW
   Gao, W
AF Wang, Shiqi
   Zhang, Xinfeng
   Liu, Xianming
   Zhang, Jian
   Ma, Siwei
   Gao, Wen
TI Utility-Driven Adaptive Preprocessing for Screen Content Video
   Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Block type identification; screen content video (SCV); temporal masking;
   utility information
ID QUALITY ASSESSMENT; BLUR
AB In this work, we propose a utility-driven preprocessing technique for high-efficiency screen content video (SCV) compression based on the temporal masking effect, which was found to be a fundamental attribute that plays an important role in human visual perception of video quality, but has not been fully exploited in the context of SCV coding. Specifically, we investigate the temporal masking effect from the perspective of perceived utility, which allows us to preserve the quality of the high utility content and substitute the low utility regions with the corresponding smooth version. To distinguish the regional utilities, a specifically designed block type identification algorithm for screen content is employed to measure the local properties. Subsequently, the Gaussian filter is applied to smooth out the high-frequency components in the detected low utility regions to save consumption bits. Validations based on subjective testings show that the proposed approach is capable of achieving significant bitrate savings with little sacrifice on the final utility compared with the conventional SCV coding scheme.
C1 [Wang, Shiqi; Zhang, Xinfeng] Nanyang Technol Univ, Rapid Rich Object Search Lab, Singapore 637553, Singapore.
   [Liu, Xianming] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Zhang, Jian; Ma, Siwei; Gao, Wen] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Nanyang Technological University; Harbin Institute of Technology; Peking
   University
RP Liu, XM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM wang-shiqi@ntu.edu.sg; xfzhang@ntu.edu.sg; xmliu.hit@gmail.com;
   jian.zhang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019
OI Wang, Shiqi/0000-0002-3583-959X; Zhang, Jian/0000-0001-5486-3125
FU National Natural Science Foundation of China [61322106, 61632001,
   61571017, 61300110, 61672193]; National Basic Research Program of China
   (973 Program) [2015CB351800]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61322106, Grant 61632001, Grant
   61571017, Grant 61300110, and Grant 61672193, and in part by the
   National Basic Research Program of China (973 Program) under Grant
   2015CB351800.
CR [Anonymous], 2014, P DES
   [Anonymous], [No title captured]
   Baratto RicardoA., 2005, Proceedings of the 20th ACM Symposium on Operating Systems Principles, SOSP '05, P277, DOI DOI 10.1145/1095810.1095837
   Brostow GJ, 2001, COMP GRAPH, P561, DOI 10.1145/383259.383325
   Budagavi M, 2005, IEEE IMAGE PROC, P1565
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Christiansen BO, 2002, IEEE DATA COMPR CONF, P332, DOI 10.1109/DCC.2002.999971
   Gabriellini A, 2013, SIGNAL PROCESS-IMAGE, V28, P197, DOI 10.1016/j.image.2012.11.007
   Gabriellini A, 2012, IEEE IMAGE PROC, P185, DOI 10.1109/ICIP.2012.6466826
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   HARRINGTON TL, 1981, ACTA PSYCHOL, V48, P227, DOI 10.1016/0001-6918(81)90064-0
   Khorbotly S, 2013, MIDWEST SYMP CIRCUIT, P1399, DOI 10.1109/MWSCAS.2013.6674918
   Kwon D. K., 2013, P 14 M VIENN AUSTR, P2013
   Lan CL, 2010, IEEE T IMAGE PROCESS, V19, P946, DOI 10.1109/TIP.2009.2038636
   Li B., 2014, SCI WORLD J, V2014, P1, DOI DOI 10.1155/2014/906861
   Li HQ, 2012, IEEE T CIRC SYST VID, V22, P1844, DOI 10.1109/TCSVT.2012.2223038
   Lu Y, 2011, IEEE MULTIMEDIA, V18, P4, DOI 10.1109/MMUL.2011.33
   Miao D, 2014, IEEE INT SYMP CIRC S, P2157, DOI 10.1109/ISCAS.2014.6865595
   Mrak M, 2012, EUR SIGNAL PR CONF, P1209
   Pan ZT, 2013, IEEE T CIRC SYST VID, V23, P949, DOI 10.1109/TCSVT.2013.2243056
   Potmesil M., 1983, Computer Graphics, V17, P389, DOI 10.1145/964967.801169
   Rehman A., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P497, DOI 10.1109/ICME.2012.175
   Rouse DM, 2010, IEEE IMAGE PROC, P2505, DOI 10.1109/ICIP.2010.5649182
   Rouse DM, 2009, IEEE IMAGE PROC, P2217, DOI 10.1109/ICIP.2009.5413882
   Segall CA, 2004, IEEE T IMAGE PROCESS, V13, P898, DOI 10.1109/TIP.2004.827230
   Shen H. L., 2009, P ISES WORLD C 2007, VI-V, P1
   Shen HF, 2007, IEEE T IMAGE PROCESS, V16, P479, DOI 10.1109/TIP.2006.888334
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Wang SQ, 2016, IEEE T CIRC SYST VID, V26, P1595, DOI 10.1109/TCSVT.2015.2461891
   Wang SQ, 2015, IEEE IMAGE PROC, P1434, DOI 10.1109/ICIP.2015.7351037
   Wang SQ, 2012, IEEE INT SYMP CIRC S, P145
   Wang SQ, 2013, IEEE T IMAGE PROCESS, V22, P1418, DOI 10.1109/TIP.2012.2231090
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Xu L, 2013, IEEE T CIRC SYST VID, V23, P975, DOI 10.1109/TCSVT.2013.2243657
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yuan QQ, 2010, IEEE T IMAGE PROCESS, V19, P3157, DOI 10.1109/TIP.2010.2055571
   Zhang L, 2015, IEEE DATA COMPR CONF, P233, DOI 10.1109/DCC.2015.33
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 43
TC 46
Z9 47
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 660
EP 667
DI 10.1109/TMM.2016.2625276
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400019
DA 2024-07-18
ER

PT J
AU Yang, YH
   Deng, C
   Gao, SQ
   Liu, W
   Tao, DP
   Gao, XB
AF Yang, Yanhua
   Deng, Cheng
   Gao, Shangqian
   Liu, Wei
   Tao, Dapeng
   Gao, Xinbo
TI Discriminative Multi-instance Multitask Learning for 3D Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D action recognition; discriminative multi-instance learning; group
   sparsity; joint configuration; multitask learning
ID EVENT DETECTION; DEPTH; JOINTS; SPARSITY; FEATURES; POSE
AB As the prosperity of low-cost and easy-operating depth cameras, skeleton-based human action recognition has been extensively studied recently. However, most of the existingmethods partially consider that all 3D joints of a human skeleton are identical. Actually, these 3D joints exhibit diverse responses to different action classes, and some joint configurations are more discriminative to distinguish a certain action. In this paper, we propose a discriminative multi-instance multitask learning (MIMTL) framework to discover the intrinsic relationship between joint configurations and action classes. First, a set of discriminative and informative joint configurations for the corresponding action class is captured in multi-instance learning model by regarding the action and the joint configurations as a bag and its instances, respectively. Then, a multitask learning modelwith group structure constraints is exploited to further reveal the intrinsic relationship between the joint configurations and different action classes. We conduct extensive evaluations of MIMTL using three benchmark 3D action recognition datasets. Experimental results show that our proposed MIMTL framework performs favorably compared with several state-of-the-art approaches.
C1 [Yang, Yanhua; Deng, Cheng; Liu, Wei; Gao, Xinbo] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Shangqian] Northeastern Univ, Coll Engn, Boston, MA 02115 USA.
   [Tao, Dapeng] Yunnan Univ, Sch Informat Sci & Engn, Kunming 650091, Peoples R China.
C3 Xidian University; Northeastern University; Yunnan University
RP Yang, YH (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
EM yangyanhua.xd@gmail.com; chdeng@mail.xidian.edu.cn;
   gao.sh@husky.neu.edu; weiliu.xidian@gmail.com; dapeng.tao@gmail.com;
   xbgao@mail.xidian.edu.cn
RI Liu, Wei/L-1951-2019; Zhang, Xiaoyu/JXR-6386-2024; Gao,
   Shangqian/AAE-5993-2022; Tao, Dapeng/E-8649-2013; Gao, Xinbo/Q-8622-2016
OI Gao, Xinbo/0000-0003-1443-0776; Liu, Wei/0000-0002-3865-8145; Deng,
   Cheng/0000-0003-2620-3247
FU National Natural Science Foundation of China [61572388, 61432014];
   National High Technology Research and Development Program of China
   [2013AA01A602]; Key Industrial Innovation Chain Project in Industrial
   Domain [2016KTZDGY-02]; Program for Changjiang Scholars and Innovative
   Research Team in University [IRT13088]; Shenzhen Technology Project
   [JCYJ20140901003939001]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61572388 and Grant 61432014, by the National High
   Technology Research and Development Program of China under Grant
   2013AA01A602, by the Key Industrial Innovation Chain Project in
   Industrial Domain under Grant 2016KTZDGY-02, by the Program for
   Changjiang Scholars and Innovative Research Team in University under
   Grant IRT13088, and by the Shenzhen Technology Project
   JCYJ20140901003939001.
CR [Anonymous], 2009, PROGR NATURAL SCI
   [Anonymous], IEEE T CYBE IN PRESS
   Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8
   Chaudhry R, 2013, IEEE COMPUT SOC CONF, P471, DOI 10.1109/CVPRW.2013.153
   Chen X., 2012, CORR
   Devanne M, 2015, IEEE T CYBERNETICS, V45, P1340, DOI 10.1109/TCYB.2014.2350774
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Eweiwi A, 2015, LECT NOTES COMPUT SC, V9007, P428, DOI 10.1007/978-3-319-16814-2_28
   Foulds J, 2010, KNOWL ENG REV, V25, P1, DOI 10.1017/S026988890999035X
   Hajimirsadeghi H, 2015, PROC CVPR IEEE, P2596, DOI 10.1109/CVPR.2015.7298875
   Hou R, 2014, LECT NOTES COMPUT SC, V8691, P721, DOI 10.1007/978-3-319-10578-9_47
   Hussein, 2013, INT JOINT C ART INT
   Jacob L., 2009, ADV NEURAL INFORM PR, P745
   Jayaraman D, 2014, PROC CVPR IEEE, P1629, DOI 10.1109/CVPR.2014.211
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549
   Kim Seyoung, 2010, ICML, P543
   Kumar A., 2012, P 29 INT C MACH LEAR, P1383
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li W, 2011, IEEE I CONF COMP VIS, P2049, DOI 10.1109/ICCV.2011.6126478
   Lin YY, 2014, PROC CVPR IEEE, P2617, DOI 10.1109/CVPR.2014.335
   Luo JJ, 2013, IEEE I CONF COMP VIS, P1809, DOI 10.1109/ICCV.2013.227
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Ma ZG, 2014, IEEE T PATTERN ANAL, V36, P1789, DOI 10.1109/TPAMI.2014.2306419
   Mahasseni B, 2013, IEEE I CONF COMP VIS, P3128, DOI 10.1109/ICCV.2013.388
   Maron O., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P341
   Ofli F, 2014, J VIS COMMUN IMAGE R, V25, P24, DOI 10.1016/j.jvcir.2013.04.007
   Parameswaran S., 2010, ADV NEURAL INFORM PR, P1867
   Pazhoumand-Dar H, 2015, J VIS COMMUN IMAGE R, V30, P10, DOI 10.1016/j.jvcir.2015.03.002
   Seidenari L, 2013, IEEE COMPUT SOC CONF, P479, DOI 10.1109/CVPRW.2013.77
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Sharaf A, 2015, IEEE WINT CONF APPL, P998, DOI 10.1109/WACV.2015.138
   Sheikh Y, 2005, IEEE I CONF COMP VIS, P144
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wang J, 2013, IEEE I CONF COMP VIS, P2688, DOI 10.1109/ICCV.2013.334
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang L, 2015, IEEE I CONF COMP VIS, P4570, DOI 10.1109/ICCV.2015.519
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2013, IEEE I CONF COMP VIS, P2680, DOI 10.1109/ICCV.2013.333
   Wei P, 2013, IEEE I CONF COMP VIS, P3136, DOI 10.1109/ICCV.2013.389
   Wu D, 2014, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2014.98
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Yan Y, 2014, IEEE T IMAGE PROCESS, V23, P5599, DOI 10.1109/TIP.2014.2365699
   Yang X., 2012, P IEEE C COMP VIS PA, P14
   Yang XD, 2014, J VIS COMMUN IMAGE R, V25, P2, DOI 10.1016/j.jvcir.2013.03.001
   Yang YH, 2016, SIGNAL PROCESS, V124, P36, DOI 10.1016/j.sigpro.2015.10.035
   Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Yu T., 2009, P 26 ANN INT C MACH, P1169, DOI DOI 10.1145/1553374.1553523
   Zanfir M, 2013, IEEE I CONF COMP VIS, P2752, DOI 10.1109/ICCV.2013.342
   Zhang Qi, 2002, International Conference on Machine Learning (ICML), P682
   Zhou Jiayu, 2011, Adv Neural Inf Process Syst, V2011, P702
   Zhou Q, 2013, IEEE I CONF COMP VIS, P2264, DOI 10.1109/ICCV.2013.281
   Zhou Y, 2015, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR.2015.7298953
   Zhu Y, 2013, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2013.78
NR 62
TC 93
Z9 93
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 519
EP 529
DI 10.1109/TMM.2016.2626959
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400008
DA 2024-07-18
ER

PT J
AU Zhang, T
   Zheng, WM
   Cui, Z
   Zong, Y
   Yan, JW
   Yan, KY
AF Zhang, Tong
   Zheng, Wenming
   Cui, Zhen
   Zong, Yuan
   Yan, Jingwei
   Yan, Keyu
TI A Deep Neural Network-Driven Feature Learning Method for Multi-view
   Facial Expression Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep neural network (DNN); multi-view facial expression recognition;
   scale invariant feature transform (SIFT)
AB In this paper, a novel deep neural network (DNN)driven feature learning method is proposed and applied to multi-view facial expression recognition (FER). In this method, scale invariant feature transform (SIFT) features corresponding to a set of landmark points are first extracted from each facial image. Then, a feature matrix consisting of the extracted SIFT feature vectors is used as input data and sent to a well-designed DNN model for learning optimal discriminative features for expression classification. The proposed DNN model employs several layers to characterize the corresponding relationship between the SIFT feature vectors and their corresponding high-level semantic information. By training the DNN model, we are able to learn a set of optimal features that are well suitable for classifying the facial expressions across different facial views. To evaluate the effectiveness of the proposed method, two nonfrontal facial expression databases, namely BU-3DFE and Multi-PIE, are respectively used to testify our method and the experimental results show that our algorithm outperforms the state-of-the-art methods.
C1 [Zhang, Tong; Zheng, Wenming; Cui, Zhen; Zong, Yuan; Yan, Jingwei; Yan, Keyu] Southeast Univ, Key Lab Child Dev & Learning Sci, Minist Educ, Res Ctr Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.
   [Zhang, Tong; Yan, Keyu] Southeast Univ, Sch Informat Sci & Engn, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China; Southeast University - China
RP Zheng, WM (corresponding author), Southeast Univ, Key Lab Child Dev & Learning Sci, Minist Educ, Res Ctr Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.
EM tongzhang@seu.edu.cn; 230139081@seu.edu.cn; wenming_zheng@seu.edu.cn;
   zhen.cui@seu.edu.cn; xhzongyuan@seu.edu.cn; yanjingwei1989@126.com
RI zhang, tong/JAO-3571-2023; Zhang, tong/IAP-2587-2023; Yan,
   Keyu/IXX-0343-2023; yan, keyu/GXW-2126-2022; ZHANG, TAO/ITV-6162-2023
FU National Basic Research Program of China [2015CB351704]; National
   Natural Science Foundation of China [61231002, 61572009]; Natural
   Science Foundation of Jiangsu Province [BK20130020]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB351704, in part by the National Natural
   Science Foundation of China under Grant 61231002 and Grant 61572009, and
   in part by the Natural Science Foundation of Jiangsu Province under
   Grant BK20130020. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Chengcui Zhang.
   (Corresponding author: Wenming Zheng.)
CR Adolphs R, 2002, CURR OPIN NEUROBIOL, V12, P169, DOI 10.1016/S0959-4388(02)00301-X
   [Anonymous], CORR
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2009, P 26 ANN INT C MACH
   [Anonymous], 1976, Pictures of facial affect
   Berger J., 2010, Proceedings of the Python for Scientific Computing Conference (SciPy), number Scipy, P1
   Chen H, 2015, IEEE J BIOMED HEALTH, V19, P1627, DOI 10.1109/JBHI.2015.2425041
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Ekman P., 2002, FACIAL ACTION CODING
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kumano S, 2009, INT J COMPUT VISION, V83, P178, DOI 10.1007/s11263-008-0185-x
   Tran L, 2016, AAAI CONF ARTIF INTE, P1317
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee H., 2009, P 26 ANN INT C MACH, P609
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Moore S., 2009, POWER PEDAGOGY PRAXI, P1
   Pantic M, 2006, IEEE T SYST MAN CY B, V36, P433, DOI 10.1109/TSMCB.2005.859075
   Ranzato M, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995710
   Rudovic O, 2010, LECT NOTES COMPUT SC, V6312, P350, DOI 10.1007/978-3-642-15552-9_26
   Susskind Joshua M., 2008, Generating facial expressions with deep belief nets
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zheng WM, 2010, LECT NOTES COMPUT SC, V6316, P490, DOI 10.1007/978-3-642-15567-3_36
NR 34
TC 220
Z9 223
U1 8
U2 139
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2528
EP 2536
DI 10.1109/TMM.2016.2598092
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200018
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Lu, F
   Gao, Y
   Chen, XW
AF Lu, Feng
   Gao, Yue
   Chen, Xiaowu
TI Estimating 3D Gaze Directions Using Unlabeled Eye Images via Synthetic
   Iris Appearance Fitting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze estimation; iris fitting; three-dimensional (3D) human gaze;
   unlabeled eye images
ID CROSS-RATIO; MOVEMENT; TRACKING; PREDICTION
AB Estimating three-dimensional (3D) human eye gaze by capturing a single eye image without active illumination is challenging. Although the elliptical iris shape provides a useful cue, existing methods face difficulties in ellipse fitting due to unreliable iris contour detection. These methods may fail frequently especially with low resolution eye images. In this paper, we propose a synthetic iris appearance fitting (SIAF) method that is model-driven to compute 3D gaze direction from iris shape. Instead of fitting an ellipse based on exactly detected iris contour, our method first synthesizes a set of physically possible iris appearances and then optimizes inside this synthetic space to find the best solution to explain the captured eye image. In this way, the solution is highly constrained and guaranteed to be physically feasible. In addition, the proposed advanced image analysis techniques also help the SIAF method be robust to the unreliable iris contour detection. Furthermore, with multiple eye images, we propose a SIAF-joint method that can further reduce the gaze error by half, and it also resolves the binary ambiguity which is inevitable in conventional methods based on simple ellipse fitting.
C1 [Lu, Feng; Chen, Xiaowu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Lu, Feng] Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing 100191, Peoples R China.
   [Gao, Yue] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
C3 Beihang University; Beihang University; Tsinghua University
RP Chen, XW (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM lufeng@buaa.edu.cn; gaoyue@tsinghua.edu.cn; chen@buaa.edu.cn
RI Gao, Yue/B-3376-2012
FU Joint Funds of National Natural Science Foundation of China-CARFC
   [U1533129]; National Natural Science Foundation of China [61325011,
   61532003]; Fundamental Research Funds for the Central Universities
FX This work was supported in part by the Joint Funds of National Natural
   Science Foundation of China-CARFC under Grant U1533129, in part by the
   National Natural Science Foundation of China under Grant 61325011 and
   Grant 61532003, and in part by the Fundamental Research Funds for the
   Central Universities. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Balakrishnan
   Prabhakaran. (Corresponding author: Xiaowu Chen.)
CR [Anonymous], 2014, P ACM INT JOINT C PE, DOI DOI 10.1145/2638728.2641694
   [Anonymous], P 2 INT C POS CONT A
   [Anonymous], 1994, P 6 INT C NEUR INF P
   Arar NM, 2015, IEEE WINT CONF APPL, P642, DOI 10.1109/WACV.2015.91
   Beymer D, 2003, PROC CVPR IEEE, P451
   Brolly X.L. C., 2004, Proc. of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPRW'04), V8, P134, DOI DOI 10.1109/CVPR.2004.92
   Chen JX, 2015, IEEE T IMAGE PROCESS, V24, P1076, DOI 10.1109/TIP.2014.2383326
   Coutinho FL, 2013, INT J COMPUT VISION, V101, P459, DOI 10.1007/s11263-012-0541-8
   Davies SJC, 2009, IEEE T MULTIMEDIA, V11, P39, DOI 10.1109/TMM.2008.2008916
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   Feng YL, 2013, IEEE T MULTIMEDIA, V15, P1865, DOI 10.1109/TMM.2013.2272918
   Guestrin ED, 2006, IEEE T BIO-MED ENG, V53, P1124, DOI 10.1109/TBME.2005.863952
   Hansen DW, 2010, IEEE T PATTERN ANAL, V32, P478, DOI 10.1109/TPAMI.2009.30
   Ishikawa Takahiro, 2004, CMURITR0408
   Li JF, 2014, IEEE COMPUT SOC CONF, P606, DOI 10.1109/CVPRW.2014.93
   Li-Qun Xu, 1998, BMVC 98. Proceedings of the Ninth British Machine Vision Conference, P428
   Lu F, 2016, NEUROCOMPUTING, V182, P10, DOI 10.1016/j.neucom.2015.07.125
   Lu F, 2015, IEEE T IMAGE PROCESS, V24, P3680, DOI 10.1109/TIP.2015.2445295
   Lu F, 2014, IEEE T PATTERN ANAL, V36, P2033, DOI 10.1109/TPAMI.2014.2313123
   Lu F, 2011, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2011.6126237
   Mora K. A. F., 2014, P S EYE TRACK RES AP, P255, DOI [10.1145/2578153.2578190, 10.1145/2578153]
   Mora KAF, 2014, PROC CVPR IEEE, P1773, DOI 10.1109/CVPR.2014.229
   Morimoto CH, 2005, COMPUT VIS IMAGE UND, V98, P4, DOI 10.1016/j.cviu.2004.07.010
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Nakazawa A, 2012, LECT NOTES COMPUT SC, V7573, P159, DOI 10.1007/978-3-642-33709-3_12
   Papadopoulos GT, 2014, IEEE T MULTIMEDIA, V16, P440, DOI 10.1109/TMM.2013.2291535
   Park KR, 2007, IEEE T SYST MAN CY B, V37, P199, DOI 10.1109/TSMCB.2006.883426
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Sugano Y, 2013, IEEE T PATTERN ANAL, V35, P329, DOI 10.1109/TPAMI.2012.101
   Sun XS, 2014, IEEE T IMAGE PROCESS, V23, P4649, DOI 10.1109/TIP.2014.2337758
   Tan KH, 2002, SIXTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P191, DOI 10.1109/ACV.2002.1182180
   Tawari A, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P988, DOI 10.1109/ITSC.2014.6957817
   Tsukada A., 2012, P S EYE TRACK RES AP, P213
   Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740
   Wang JG, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P136
   Williams O., 2006, P IEEE C CVPR, V1, P230, DOI DOI 10.1109/CVPR.2006.285
   Wood E., 2014, P S EYE TRACK RES AP, P207, DOI DOI 10.1145/2578153.2578185
   Yamazoe H, 2008, PROCEEDINGS OF THE EYE TRACKING RESEARCH AND APPLICATIONS SYMPOSIUM (ETRA 2008), P245, DOI 10.1145/1344471.1344527
   Yoo DH, 2005, COMPUT VIS IMAGE UND, V98, P25, DOI 10.1016/j.cviu.2004.07.011
   YOUNG LR, 1975, BEHAV RES METH INSTR, V7, P397, DOI 10.3758/BF03201553
NR 42
TC 23
Z9 24
U1 3
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1772
EP 1782
DI 10.1109/TMM.2016.2576284
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800008
DA 2024-07-18
ER

PT J
AU Zupancic, I
   Blasi, SG
   Peixoto, E
   Izquierdo, E
AF Zupancic, Ivan
   Blasi, Saverio G.
   Peixoto, Eduardo
   Izquierdo, Ebroul
TI Inter-Prediction Optimizations for Video Coding Using Adaptive Coding
   Unit Visiting Order
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficiency video coding (HEVC); mode decision; video coding
ID MODE DECISION; HEVC
AB The flexible partitioning scheme and increased number of prediction modes in the high efficiency video coding (HEVC) standard are largely responsible for both its high compression efficiency and computational complexity. In typical HEVC encoder implementations, coding units (CUs) in a coding tree unit (CTU) are visited from top to bottom at each level of recursion to select the optimal coding configuration. In this paper, a novel approach is presented in which CUs in a CTU can be adaptively visited also in a reverse, bottom to top visiting order. This reverse CU (RCU) visiting order allows for different algorithmic optimizations for further complexity reduction of many HEVC encoding steps, especially under challenging conditions, such as highly textured or fast moving content. In particular, algorithms to reduce complexity of HEVC depth selection, mode decision, and inter-prediction are presented here based on the coding information obtained from higher depths when using the RCU visiting order. Experimental results show that enabling different stages of the proposed algorithm can achieve average speedups from 16.3% to 36.6% compared to fast reference HEVC implementation with pre-built speed-ups enabled (up to 51.2% in some cases), for 0.3% to 2.2% BD-rate penalty.
C1 [Zupancic, Ivan; Blasi, Saverio G.; Izquierdo, Ebroul] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
   [Blasi, Saverio G.] British Broadcasting Corp, Res & Dev, London W12 7SB, England.
   [Peixoto, Eduardo] Univ Brasilia, Dept Elect Engn, BR-70910900 Brasilia, DF, Brazil.
C3 University of London; Queen Mary University London; Universidade de
   Brasilia
RP Zupancic, I (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London E1 4NS, England.
EM i.zupancic@qmul.ac.uk; saverio.blasi@bbc.co.uk; eduardopeixoto@ieee.org;
   ebroul.izquierdo@qmul.ac.uk
FU QMUL Research-IT; EPSRC [EP/K000128/1]; French State (FUI/Oseo); French
   local Authorities (Region Bretagne); EPSRC [EP/K000128/1, EP/K000233/1]
   Funding Source: UKRI
FX This research utilized Queen Mary's MidPlus computational facilities,
   supported by QMUL Research-IT and by EPSRC under Grant EP/K000128/1.
   Some of the content belongs to the 4EVER consortium, rights reserved to
   the 4EVER partners and their licensors. The 4EVER Research Project is
   coordinated by Orange Laboratories and is supported by the French State
   (FUI/Oseo) and French local Authorities (Region Bretagne) associated to
   the European funds FEDER.
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   [Anonymous], 1997, ART COMPUTER PROGRAM
   [Anonymous], P 2013 NEM SUMM
   Bjontegaard G., 2008, VCEGM33
   Blasi SG, 2015, IEEE IMAGE PROC, P1478, DOI 10.1109/ICIP.2015.7351046
   Blasi SG, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P144, DOI 10.1109/PCS.2015.7170064
   Blasi SG, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P50, DOI 10.1109/PCS.2015.7170045
   Bossen F, 2012, JCTVCL1100
   Choi K., 2011, JCTVC F092       JUL
   Choi WI, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P371
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   Even S., 2011, Graph Algorithms
   Franche JF, 2015, IEEE IMAGE PROC, P477, DOI 10.1109/ICIP.2015.7350844
   Gweon R. H., 2011, JCTVC F045       JUL
   Haglund L., 2006, SVT HIGH DEFINITION
   ITU, 2016, HM REF SOFTW
   Marpe D, 2006, IEEE COMMUN MAG, V44, P134, DOI 10.1109/MCOM.2006.1678121
   Mitchell T. M., 1997, MACHINE LEARNING
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Palomino D, 2013, PICT COD SYMP, P209, DOI 10.1109/PCS.2013.6737720
   Peixoto E, 2012, IEEE IMAGE PROC, P737, DOI 10.1109/ICIP.2012.6466965
   Powers DMW, 2020, J MACH LEARN TECHNOL, P37, DOI DOI 10.9735/2229-3981
   Shen LQ, 2014, IEEE T CIRC SYST VID, V24, P1709, DOI 10.1109/TCSVT.2014.2313892
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   SONG L, 2013, P INT WORKSH QUAL MU, P34
   Tourapis AM, 2002, IEEE T CIRC SYST VID, V12, P934, DOI 10.1109/TCSVT.2002.804894
   Vanne J, 2014, IEEE T CIRC SYST VID, V24, P1579, DOI 10.1109/TCSVT.2014.2308453
   Vanne J, 2012, IEEE T CIRC SYST VID, V22, P1885, DOI 10.1109/TCSVT.2012.2223013
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weerakkody R., 2013, JCTVCO0332
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Yang J., 2011, JCTVC G543       NOV
   Zhang YF, 2013, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2013.13
   Zhang YF, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP), DOI 10.1109/VCIP.2012.6410739
   Zupancic I, 2015, INT CONF ACOUST SPEE, P1419, DOI 10.1109/ICASSP.2015.7178204
NR 36
TC 25
Z9 25
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1677
EP 1690
DI 10.1109/TMM.2016.2579505
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, WJ
   Yu, L
AF Zhou, Wujie
   Yu, Lu
TI Binocular Responses for No-Reference 3D Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D image quality assessment (3D-IQA); binocular energy response (BER);
   binocular rivalry response (BRR); local binary pattern
ID STEREOSCOPIC IMAGES; JOINT STATISTICS
AB Perceptual quality assessment of distorted three-dimensional (3D) images has become a fundamental yet challenging issue in the field of 3D imaging. In this paper, we propose a general-purpose blind/no-reference (NR) 3D image quality assessment (IQA) metric that utilizes the complementary local patterns (the local magnitude pattern and the proposed generalized local directional pattern) of binocular energy response (BER) and binocular rivalry response (BRR). The main technical contribution of this research is that binocular visual perception and local structural distribution are considered for NR 3D-IQA. More specifically, the metric simulates the binocular visual perception using BER and BRR. Subsequently, the local patterns of the binocular responses' encoding maps are used to form various binocular quality-predictive features, which will change in the presence of distortions. After feature extraction, we use k-nearest neighbors-based machine learning to drive the overall quality score. We tested our proposed metric against two publicly available 3D databases; these tests confirm that the proposed metric's results consistently align with human subjective judgments.
C1 [Zhou, Wujie] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Zhejiang, Peoples R China.
   [Zhou, Wujie; Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Yu, Lu] Zhejiang Prov Key Lab Informat Network Technol, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University of Science & Technology; Zhejiang University
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Zhejiang, Peoples R China.; Yu, L (corresponding author), Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Zhejiang, Peoples R China.
EM wujiezhou@163.com; yulu@zju.edu.cn
OI zhou, wujie/0000-0002-3055-2493; Yu, Lu/0000-0002-0550-7754
FU Natural Science Foundation of China [61431015, 61371162, 61502429];
   Zhejiang Provincial Natural Science Foundation of China [LQ15F020010]
FX This work was supported by the Natural Science Foundation of China under
   Grant 61431015, Grant 61371162, and Grant 61502429, and by the Zhejiang
   Provincial Natural Science Foundation of China under Grant LQ15F020010.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Christian Timmerer.
CR Akhter R., 2010, P SOC PHOTO-OPT INS, V7525
   [Anonymous], 1980, VISION
   [Anonymous], 2010, P INT WORKSH VID PRO
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Chang Y, 2015, IEEE T CIRC SYST VID, V25, P99, DOI 10.1109/TCSVT.2014.2330658
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Fleet DJ, 1996, VISION RES, V36, P1839, DOI 10.1016/0042-6989(95)00313-4
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Jabid T, 2010, ETRI J, V32, P784, DOI 10.4218/etrij.10.1510.0132
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Levelt W.J. M., 1968, BINOCULAR RIVALRY
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Park H, 2014, IEEE T MULTIMEDIA, V16, P326, DOI 10.1109/TMM.2013.2286567
   Phan R, 2014, IEEE T MULTIMEDIA, V16, P122, DOI 10.1109/TMM.2013.2283451
   Pinson MH, 2015, IEEE SIGNAL PROC MAG, V32, P101, DOI 10.1109/MSP.2013.2292535
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Shao F, 2015, IEEE SIGNAL PROC LET, V22, P1548, DOI 10.1109/LSP.2015.2413946
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Zhang L, 2015, J HEMATOL ONCOL, V8, DOI 10.1186/s13045-014-0103-3
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang M, 2015, IEEE SIGNAL PROC LET, V22, P207, DOI 10.1109/LSP.2014.2326399
   Zhang Y, 2014, SIGNAL PROCESS-IMAGE, V29, P725, DOI 10.1016/j.image.2014.05.004
   Zhou WJ, 2015, OPT EXPRESS, V23, P23710, DOI 10.1364/OE.23.023710
   Zhou WJ, 2014, COMPUT ELECTR ENG, V40, P104, DOI 10.1016/j.compeleceng.2014.06.007
NR 33
TC 60
Z9 62
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1077
EP 1084
DI 10.1109/TMM.2016.2542580
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100011
DA 2024-07-18
ER

PT J
AU Du, J
   Jiang, CX
   Qian, Y
   Han, Z
   Ren, Y
AF Du, Jun
   Jiang, Chunxiao
   Qian, Yi
   Han, Zhu
   Ren, Yong
TI Resource Allocation With Video Traffic Prediction in Cloud-Based Space
   Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud service; predictive backpressure (PBP); queueing theory; resource
   allocation; space-based information network (SBIN); video traffic
   prediction
ID COGNITIVE RADIO NETWORKS; DYNAMIC SPECTRUM ACCESS; WIRELESS NETWORKS;
   GAME; TRANSMISSION; DOMAIN
AB This paper considers the resource allocation problems for video transmission in space-based information networks. The queueing system analyzed in this study is constituted by multiple users and a single server. The server is operated as a cloud that can sense the traffic arrivals to each user's queue and then allocates the transmission resource and service rate for users. The objectives are to make configurations over time to minimize the time average cost of the system, and to minimize the waiting time of packets after they enter the queue. Meanwhile, the constraints on the queue stability of the system must be satisfied. In this paper, we introduce a predictive backpressure algorithm, which considers the future arrivals with a certain prediction window size into the consideration of resource allocation to make decisions on which packets to be served first. In addition, this paper designs a multiresolution wavelet decomposition-based backpropagation network for the prediction of video traffic, which exhibits the long-range dependence property. Simulation results indicate that the delay of the queueing system can be reduced through this prediction-based resource allocation, and the prediction accuracy for the video traffic is improved according to the proposed prediction system.
C1 [Du, Jun; Jiang, Chunxiao; Ren, Yong] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Qian, Yi] Univ Nebraska Lincoln, Dept Elect & Comp Engn, Omaha, NE 68182 USA.
   [Han, Zhu] Univ Houston, Dept Elect & Comp Engn, Houston, TX 77004 USA.
C3 Tsinghua University; University of Nebraska System; University of
   Nebraska Lincoln; University of Houston System; University of Houston
RP Jiang, CX (corresponding author), Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
EM blgdujun@gmail.com; jchx@tsinghua.edu.cn; yqian2@unl.edu; zhan2@uh.edu;
   reny@tsinghua.edu.cn
RI Jiang, Chunxiao/W-3904-2017; Han, Zhu/ABG-1222-2021; qian,
   yi/HZH-4175-2023; Du, Jun/AAY-6649-2021; Qian, Yi/KEI-0952-2024
OI Du, Jun/0000-0002-5213-8808; 
FU Grand Foundational Research Program of China (863 Program)
   [2015AA015701]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1456921] Funding Source: National Science
   Foundation
FX This work was supported by the Grand Foundational Research Program of
   China (863 Program) under Grant 2015AA015701.
CR Abdi A, 2001, IEEE COMMUN LETT, V5, P92, DOI 10.1109/4234.913150
   [Anonymous], 2013, FUNDAMENTALS STAT SI
   Argyriou A, 2015, IEEE T MULTIMEDIA, V17, P736, DOI 10.1109/TMM.2015.2408254
   Asif MT, 2014, IEEE T INTELL TRANSP, V15, P794, DOI 10.1109/TITS.2013.2290285
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Huang LB, 2014, MOBIHOC'14: PROCEEDINGS OF THE 15TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P33, DOI 10.1145/2632951.2632983
   Huang LB, 2013, IEEE ACM T NETWORK, V21, P831, DOI 10.1109/TNET.2012.2226215
   Ilow A, 2000, INT CONF ACOUST SPEE, P3814, DOI 10.1109/ICASSP.2000.860234
   Jiang CX, 2014, IEEE T WIREL COMMUN, V13, P2176, DOI 10.1109/TWC.2014.022014.131209
   Jiang CX, 2014, IEEE T WIREL COMMUN, V13, P1960, DOI 10.1109/TWC.2014.030314.130632
   Jiang CX, 2013, IEEE T WIREL COMMUN, V12, P2470, DOI 10.1109/TWC.2013.031813.121135
   Jiang CX, 2013, IEEE J SEL AREA COMM, V31, P406, DOI 10.1109/JSAC.2013.130307
   Kang SH, 2005, IEEE T MULTIMEDIA, V7, P1139, DOI 10.1109/TMM.2005.858401
   Kholaif AM, 2010, IEEE T MULTIMEDIA, V12, P142, DOI 10.1109/TMM.2009.2037380
   Krunz MM, 2000, IEEE T MULTIMEDIA, V2, P27, DOI 10.1109/6046.825792
   Lalbakhsh P, 2009, INTERNATIONAL CONFERENCE ON FUTURE COMPUTER AND COMMUNICATIONS, PROCEEDINGS, P47, DOI 10.1109/ICFCC.2009.8
   Lehmann E.L., 1998, Theory of point estimation, V31
   Liu CH, 2013, IEEE J SEL AREA COMM, V31, P2312, DOI 10.1109/JSAC.2013.131123
   Ma S, 2001, IEEE ACM T NETWORK, V9, P634, DOI 10.1109/90.958331
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Mastronarde NH, 2008, IEEE T CIRC SYST VID, V18, P453, DOI 10.1109/TCSVT.2008.918440
   Neely MJ, 2006, IEEE T INFORM THEORY, V52, P2915, DOI 10.1109/TIT.2006.876219
   Neely MJ, 2009, AD HOC NETW, V7, P862, DOI 10.1016/j.adhoc.2008.07.009
   Östring SAM, 2001, IEEE T COMMUN, V49, P1092, DOI 10.1109/26.930639
   Saki H, 2015, IEEE T MULTIMEDIA, V17, P333, DOI 10.1109/TMM.2015.2389032
   Szolgayová E, 2014, J HYDROL HYDROMECH, V62, P24, DOI 10.2478/johh-2014-0011
   Tamma B. R., 2009, P IEEE ICC 2009 JUN, P1
   TASSIULAS L, 1992, IEEE T AUTOMAT CONTR, V37, P1936, DOI 10.1109/9.182479
   Vanajakshi L, 2004, 2004 IEEE INTELLIGENT VEHICLES SYMPOSIUM, P194
   Wu M, 2001, IEEE T MULTIMEDIA, V3, P186, DOI 10.1109/6046.923818
   Yoo SJ, 2002, IEEE T BROADCAST, V48, P10, DOI 10.1109/11.992849
   Zhang YS, 2010, IEEE T MULTIMEDIA, V12, P886, DOI 10.1109/TMM.2010.2065217
NR 32
TC 52
Z9 53
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 820
EP 830
DI 10.1109/TMM.2016.2537781
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200003
DA 2024-07-18
ER

PT J
AU Zhou, L
AF Zhou, Liang
TI On Data-Driven Delay Estimation for Media Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay announcement; media cloud; user response; user satisfaction
ID WIRELESS NETWORKS; PROTECTION
AB It is well known that delay announcement is an economical and efficient way to improve the user satisfaction since the waiting time (delay) is an important performance metric for media cloud. However, how to accurately estimate the delay in an online-implementation manner is still an open and challenging problem. In this study, we study the data-driven delay estimation in a practical cloud media with heavy traffic, and propose an accurate estimation strategy only with a small amount of dataset. Importantly, we explicitly model the subjective announcement-dependent user response via an objective response function through the elaborate data analysis and model. On the theoretical end, the user response in terms of the estimated delay is characterized by the time window data-cleaning, where an appropriate dataset is set up through the window function analysis. On the technical end, we analyze the conditions for data-driven delay estimation, and prove that the proposed method is able to obtain a near-optimal solution within a finite time period. Extensive simulation results demonstrate the efficiency of the proposed delay estimation method.
C1 [Zhou, Liang] Natl Engn Res Ctr Commun & Network Technol NUPT, Nanjing 210003, Jiangsu, Peoples R China.
RP Zhou, L (corresponding author), Natl Engn Res Ctr Commun & Network Technol NUPT, Nanjing 210003, Jiangsu, Peoples R China.
EM lzhou1105@163.com
FU State Key Development Program of Basic Research of China [2013CB329005];
   National Natural Science Foundation of China [61571240, 61322104];
   Priority Academic Program Development of Jiangsu Higher Education
   Institutions
FX This work was supported in part by the State Key Development Program of
   Basic Research of China under Grant 2013CB329005, in part by the
   National Natural Science Foundation of China under Grant 61571240 and
   Grant 61322104, and in part by the Priority Academic Program Development
   of Jiangsu Higher Education Institutions. The guest editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Dapeng Oliver Wu.
CR Armony M, 2009, OPER RES, V57, P66, DOI 10.1287/opre.1080.0533
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen M, 2015, IEEE NETWORK, V29, P32, DOI 10.1109/MNET.2015.7064900
   Chen M, 2015, IEEE WIREL COMMUN, V22, P20, DOI 10.1109/MWC.2015.7054715
   Du QH, 2015, PERS UBIQUIT COMPUT, V19, P1033, DOI 10.1007/s00779-015-0872-x
   He J, 2014, IEEE T CIRC SYST VID, V24, P669, DOI 10.1109/TCSVT.2013.2283430
   He J, 2013, IEEE T CIRC SYST VID, V23, P1717, DOI 10.1109/TCSVT.2013.2255423
   Hussain M, 2016, MULTIMED TOOLS APPL, V75, P5345, DOI 10.1007/s11042-015-2936-3
   Iosup A, 2011, IEEE T PARALL DISTR, V22, P931, DOI 10.1109/TPDS.2011.66
   Kleinrock L., 1976, Queuing Systems
   Kumar N., 2015, IEEE NETWORK, V29, P134
   Lai CF, 2013, IEEE WIREL COMMUN, V20, P62
   Li BC, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2505805
   LIGGETT TM, 1985, ANN PROBAB, V13, P1279, DOI 10.1214/aop/1176992811
   Maryak JL, 2008, IEEE T AUTOMAT CONTR, V53, P780, DOI 10.1109/TAC.2008.917738
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Wang CN, 2014, IEEE SYST J, V8, P313, DOI 10.1109/JSYST.2013.2253042
   Wang HG, 2014, IEEE COMMUN MAG, V52, P73, DOI 10.1109/MCOM.2014.6766088
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Warneke D, 2011, IEEE T PARALL DISTR, V22, P985, DOI 10.1109/TPDS.2011.65
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wu DL, 2011, IEEE T AUTOMAT CONTR, V56, P2467, DOI 10.1109/TAC.2011.2164034
   Wu DP, 2015, IEEE WIREL COMMUN, V22, P66, DOI 10.1109/MWC.2015.7224729
   Xu CQ, 2015, IEEE T CIRC SYST VID, V25, P1175, DOI 10.1109/TCSVT.2014.2376138
   Xu CQ, 2011, IEEE T BROADCAST, V57, P204, DOI 10.1109/TBC.2011.2119050
   Yang J, 2009, IEEE T SIGNAL PROCES, V57, P3704, DOI 10.1109/TSP.2009.2021452
   Yang JX, 2011, IEEE T SIGNAL PROCES, V59, P667, DOI 10.1109/TSP.2010.2081981
   Zhou L., IEEE T CIRC IN PRESS
   Zhou L, 2015, IEEE T CIRC SYST VID, V25, P1888, DOI 10.1109/TCSVT.2015.2409694
   Zhou L, 2013, IEEE T WIREL COMMUN, V12, P3733, DOI 10.1109/TWC.2013.051413.120597
   Zhou L, 2013, IEEE T MULTIMEDIA, V15, P735, DOI 10.1109/TMM.2013.2241044
NR 31
TC 36
Z9 36
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 905
EP 915
DI 10.1109/TMM.2016.2537782
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200010
DA 2024-07-18
ER

PT J
AU Anarado, I
   Andreopoulos, Y
AF Anarado, Ijeoma
   Andreopoulos, Yiannis
TI Core Failure Mitigation in Integer Sum-of-Product Computations on Cloud
   Computing Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; integer matrix products; multimedia cloud computing
ID INCREMENTAL REFINEMENT; MATRIX MULTIPLICATION; FAULT-TOLERANCE;
   CONVOLUTION; THROUGHPUT; ALGORITHM; DESCRIPTORS; RETRIEVAL
AB The decreasing mean-time-to-failure estimates in cloud computing systems indicate that multimedia applications running on such environments should be able to mitigate an increasing number of core failures at runtime. We propose a new roll-forward failure-mitigation approach for integer sum-of-product computations, with emphasis on generic matrix multiplication (GEMM) and convolution/crosscorrelation (CONV) routines. Our approach is based on the production of redundant results within the numerical representation of the outputs via the use of numerical packing. This differs from all existing roll-forward solutions that require a separate set of checksum (or duplicate) results. Our proposal imposes 37.5% reduction in the maximum output bitwidth supported in comparison to integer sum-of-product realizations performed on 32-bit integer representations which is comparable to the bitwidth requirement of checksum-methods for multiple core failure mitigation. Experiments with state-of-the-art GEMM and CONV routines running on a c4.8xlarge compute-optimized instance of amazon web services elastic compute cloud (AWS EC2) demonstrate that the proposed approach is able to mitigate up to one quadcore failure while achieving processing throughput that is: 1) comparable to that of the conventional, failure-intolerant, integer GEMM and CONV routines, 2) substantially superior to that of the equivalent roll-forward failure-mitigation method based on checksum streams. Furthermore, when used within an image retrieval framework deployed over a cluster of AWS EC2 spot (i.e., low-cost albeit terminatable) instances, our proposal leads to: 1) 16%-23% cost reduction against the equivalent checksum-based method and 2) more than 70% cost reduction against conventional failure-intolerant processing on AWS EC2 on-demand (i.e., higher-cost albeit guaranteed) instances.
C1 [Anarado, Ijeoma; Andreopoulos, Yiannis] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
C3 University of London; University College London
RP Anarado, I; Andreopoulos, Y (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM ijeoma.anarado.12@ucl.ac.uk; i.andreopoulos@ucl.ac.uk
FU Federal Government of Nigeria under PRESSID Scheme; EPSRC
   [EP/M00113X/1]; EPSRC [EP/M00113X/1] Funding Source: UKRI
FX The work of I. Anarado was supported by the Federal Government of
   Nigeria under the PRESSID Scheme. The work of Y. Andreopoulos was
   supported by EPSRC under Grant EP/M00113X/1. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yonggang Wen.
CR Amato G, 2013, LECT NOTES COMPUT SC, V8199, P245, DOI 10.1007/978-3-642-41062-8_25
   Anam MA, 2014, IEEE T CIRC SYST VID, V24, P1860, DOI 10.1109/TCSVT.2014.2321071
   Anam MA, 2012, IEEE T MULTIMEDIA, V14, P797, DOI 10.1109/TMM.2012.2184742
   Anarado I, 2013, IEEE INT ON LINE, P19, DOI 10.1109/IOLTS.2013.6604045
   Anastasia D, 2012, IEEE T SIGNAL PROCES, V60, P2024, DOI 10.1109/TSP.2011.2176337
   Anastasia D, 2010, IEEE T IMAGE PROCESS, V19, P2099, DOI 10.1109/TIP.2010.2045702
   Anastasia D, 2010, IEEE SIGNAL PROC LET, V17, P375, DOI 10.1109/LSP.2010.2041583
   Andreopoulos I, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL III, P37, DOI 10.1109/ISCAS.2000.855990
   Andreopoulos Y, 2003, J VLSI SIG PROC SYST, V34, P209, DOI 10.1023/A:1023244201750
   Andreopoulos Y, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P501, DOI 10.1109/ICDSP.2002.1028137
   Andreopoulos Y, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P330, DOI 10.1109/ICIP.2001.958118
   Andreopoulos Y, 2008, IEEE T IMAGE PROCESS, V17, P1685, DOI 10.1109/TIP.2008.2001051
   Andreopoulos Y, 2007, IEEE T CIRC SYST VID, V17, P751, DOI 10.1109/TCSVT.2007.896662
   Andreopoulos Y, 2013, IEEE T MULTIMEDIA, V15, P291, DOI 10.1109/TMM.2012.2232649
   [Anonymous], 2008, P 10 EUR C COMP VIS
   [Anonymous], 2012, IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
   [Anonymous], INTEL AVX NEW FRONTI
   [Anonymous], 2008, ACM Transactions on Knowledge Discovery from Data (TKDD), DOI DOI 10.1145/1409620.1409621
   [Anonymous], 2014, Intel Math Kernel Library, DOI DOI 10.1007/978-3-319-06486-47
   Arturov K., 2011, RECOMMENDED SETTINGS
   BECKMANN PE, 1993, IEEE T SIGNAL PROCES, V41, P2300, DOI 10.1109/78.224241
   Bertin-Mahieux T., 2011, ISMIR, P591
   Bronevetsky G, 2003, ACM SIGPLAN NOTICES, V38, P84, DOI 10.1145/966049.781513
   Carterette B., 2009, P TEXT RETR C, V9, P1
   Chan C.S., 2014, ACM SIGMETRICS PERFO, V41, P83
   Chao Y., 2010, INTEL MKL 10 X THREA
   CHEN Z., 2009, P C HIGH PERF COMP N, P29
   Chen ZZ, 2008, IEEE T PARALL DISTR, V19, P1628, DOI 10.1109/TPDS.2008.58
   Chen ZZ, 2011, HPDC 11: PROCEEDINGS OF THE 20TH INTERNATIONAL SYMPOSIUM ON HIGH PERFORMANCE DISTRIBUTED COMPUTING, P73
   Daly JT, 2008, IEEE ACM INT SYMP, P795, DOI 10.1109/CCGRID.2008.103
   Di S, 2015, IEEE ACM INT SYMP, P271, DOI 10.1109/CCGrid.2015.17
   Du P, 2012, ACM SIGPLAN NOTICES, V47, P225, DOI 10.1145/2370036.2145845
   Egwutuoha IP, 2013, J SUPERCOMPUT, V65, P1302, DOI 10.1007/s11227-013-0884-0
   Ellis DRW, 2008, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2008.4517545
   Engelmann C., 2009, P IASTED INT C PAR D, V641, P46
   Fagg G. E., 2004, P INT SUP C, P97
   Fiala David., 2012, SC Conference on High Performance Computing Networking, Storage and Analysis, SC '12, Salt Lake City, UT, USA - November 11 - 15, 2012, P78
   Gabriel E, 2004, LECT NOTES COMPUT SC, V3241, P97
   Gong Y., 2014, P INT C HIGH PERF CO
   Goto K, 2008, ACM T MATH SOFTWARE, V34, DOI 10.1145/1356052.1356053
   HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   JOU JY, 1986, P IEEE, V74, P732, DOI 10.1109/PROC.1986.13535
   JOU JY, 1984, P SOC PHOTO-OPT INST, V495, P94, DOI 10.1117/12.944013
   Kadyrov A, 2006, IEEE T PATTERN ANAL, V28, P1882, DOI 10.1109/TPAMI.2006.234
   Kontorinis N, 2009, IEEE T CIRC SYST VID, V19, P1000, DOI 10.1109/TCSVT.2009.2020256
   Luk F. T., 1985, SPIE REAL TIME SIGNA, V564, P631
   Munteanu A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 2, PROCEEDINGS, P61
   Nicolaidis M, 2012, DES AUT TEST EUROPE, P677
   Ohshima S, 2007, LECT NOTES COMPUT SC, V4395, P305
   Page L., 1999, PAGERANK CITATION RA
   REXFORD J, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P649, DOI 10.1109/ISCAS.1992.230168
   Seyerlehner K., 2010, P 13 INT C DIG AUD E, P225
   Stefanidis V. K., 2004, P 2004 INT C NUM AN, P1
   Sundaram S, 2008, IEEE T SIGNAL PROCES, V56, P4244, DOI 10.1109/TSP.2008.924636
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Taylor S., 2004, INTEL INTEGRATED PER
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Treaster M., 2005, ACM COMPUTING RES RE, P1
   Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9
   Verma AK, 2015, PARKINSONS DIS-US, V2015, DOI 10.1155/2015/598028
   Wang C., 2007, Proceedings of the Parallel and Distributed Processing Symposium, P1, DOI [10.1109/IPDPS.2007.370307, DOI 10.1109/IPDPS.2007.370307]
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 68
TC 1
Z9 1
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 789
EP 801
DI 10.1109/TMM.2016.2532603
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Avci, U
   Aran, O
AF Avci, Umut
   Aran, Oya
TI Predicting the Performance in Decision-Making Tasks: From Individual
   Cues to Group Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group performance analysis; multimodal interaction; social computing
ID NONVERBAL BEHAVIOR; TEAM PERFORMANCE; ROLES; METAANALYSIS; PERSONALITY;
   RECOGNITION
AB This paper addresses the problem of predicting the performance of decision-making groups. Towards this goal, we evaluate the predictive power of group attributes and discussion dynamics by using automatically extracted features, such as group members' aural and visual cues, interaction between team members, and influence of each team member, as well as self-reported features such as personality- and perception-related cues, hierarchical structure of the group, and individual- and group-level task performances. We tackle the inference problem from two angles depending on the way that features are extracted: 1) a holistic approach based on the entire meeting, and 2) a sequential approach based on the thin slices of the meeting. In the former, key factors affecting the group performance are identified and the prediction is achieved by support vector machines. As for the latter, we compare and contrast the classification performance of an influence model-based novel classifier with that of hidden Markov model (HMM). Experimental results indicate that the group looking cues and the influence cues are major predictors of group performance and the influence model outperforms the HMM in almost all experimental conditions. We also show that combining classifiers covering unique aspects of data results in improvement in the classification performance.
C1 [Avci, Umut] Izmir Univ Econ, Dept Software Engn, TR-35330 Izmir, Turkey.
   [Aran, Oya] Idiap Res Inst, CH-1920 Martigny, Switzerland.
C3 Izmir Ekonomi Universitesi
RP Avci, U (corresponding author), Izmir Univ Econ, Dept Software Engn, TR-35330 Izmir, Turkey.; Aran, O (corresponding author), Idiap Res Inst, CH-1920 Martigny, Switzerland.
EM umut.avci@ieu.edu.tr; oya.aran@idiap.ch
RI Aran, Oya/ABR-6400-2022
OI Aran, Oya/0000-0002-4679-9335
FU Swiss National Science Foundation (SNSF) Ambizione fellowship under SOBE
   project [PZ00P2_136811]; Swiss National Science Foundation (SNF)
   [PZ00P2_136811] Funding Source: Swiss National Science Foundation (SNF)
FX This work was supported by the Swiss National Science Foundation (SNSF)
   Ambizione fellowship under the SOBE project (PZ00P2_136811). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Qi Tian.
CR [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2001, LEARNING HUMAN INTER
   Avci Umut, 2014, WORKSH UND MOD MULT, P9
   Basu S., 2001, P IEEE CVPR INT WORK, P1
   Bednarik R., 2013, EYE GAZE INTELL USER, V24, P111
   Bell B.G., 2003, Proceedings of the Human Factors and Ergonomics Society Annual Meeting, V47, P1087, DOI [DOI 10.1177/154193120304700909, DOI 10.1037/e576982012-009]
   Bradley JH, 1997, MANAGE DEV, V16, P337, DOI 5
   CARLI LL, 1995, J PERS SOC PSYCHOL, V68, P1030, DOI 10.1037/0022-3514.68.6.1030
   Cristani M, 2011, PATTERN RECOGN, V44, P1785, DOI 10.1016/j.patcog.2011.01.013
   Dong W., 2010, P ICMI MLMI, P40
   Dong W., 2012, CORR
   Dong W., MODELING GROUP UNPUB
   Dong W, 2013, IEEE T MULTIMEDIA, V15, P83, DOI 10.1109/TMM.2012.2225039
   Dong W, 2009, C & C 09: PROCEEDINGS OF THE 2009 ACM SIGCHI CONFERENCE ON CREATIVITY AND COGNITION, P365
   Dong W, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P271
   Eivazi S., 2011, Proc. Workshop on Eye Gaze in Intelligent Human Machine Interaction at IUI, P9
   Escalera S, 2012, SENSORS-BASEL, V12, P1702, DOI 10.3390/s120201702
   Frauendorfer D, 2015, INT J PSYCHOL, V50, P392, DOI 10.1002/ijop.12102
   Hall JA, 2005, PSYCHOL BULL, V131, P898, DOI 10.1037/0033-2909.131.6.898
   Haslam AlexanderS., 2004, Psychology in Organizations: The Social Identity Approach
   Higgs M, 2005, TEAM PERFORM MANAG, V11, P227, DOI 10.1108/13527590510635134
   Hung H, 2010, IEEE T MULTIMEDIA, V12, P563, DOI 10.1109/TMM.2010.2055233
   Jayagopi DB, 2010, IEEE T MULTIMEDIA, V12, P790, DOI 10.1109/TMM.2010.2065218
   Jayagopi D, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433
   Kichuk SL, 1997, J ENG TECHNOL MANAGE, V14, P195, DOI 10.1016/S0923-4748(97)00010-6
   LePine JA, 1997, J APPL PSYCHOL, V82, P803, DOI 10.1037/0021-9010.82.5.803
   Luz S., 2013, P ICMI, P575
   Matsumoto D., 2006, HDB NONVERBAL COMMUN, P219, DOI DOI 10.4135/9781412976152.N12
   McGrath J.E., 1984, GROUPS INTERACTION P
   Neuman GA, 1999, J APPL PSYCHOL, V84, P376, DOI 10.1037/0021-9010.84.3.376
   O'Neill TA, 2011, EUR J PERSONALITY, V25, P31, DOI 10.1002/per.769
   Pan W, 2012, IEEE SIGNAL PROC MAG, V29, P77, DOI 10.1109/MSP.2011.942737
   Peeters MAG, 2006, EUR J PERSONALITY, V20, P377, DOI 10.1002/per.588
   Pentland A., 2014, HONEST SIGNALS THEY
   Raducanu B, 2012, MULTIMED TOOLS APPL, V56, P207, DOI 10.1007/s11042-010-0545-8
   RIDGEWAY CL, 1987, AM SOCIOL REV, V52, P683, DOI 10.2307/2095603
   Roy M.H., 2001, Management Decisions, V39, P323, DOI [10.1108/00251740110391501, DOI 10.1108/00251740110391501]
   Sanchez-Cortes D, 2013, J MULTIMODAL USER IN, V7, P39, DOI 10.1007/s12193-012-0101-0
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sapru A, 2015, IEEE T MULTIMEDIA, V17, P746, DOI 10.1109/TMM.2015.2408437
   Shaw JB, 1998, HUM RELAT, V51, P1307, DOI 10.1177/001872679805101005
   Spreitzer GM, 1999, RES MANAG GRP TEAM, V2, P71
   Stolikj M., 2014, WORLD WIRELESS MOBIL, P1
   SUNDSTROM E, 1990, AM PSYCHOL, V45, P120, DOI 10.1037/0003-066X.45.2.120
   van Dick R, 2009, J MANAGE PSYCHOL, V24, P609, DOI 10.1108/02683940910989011
   Varni G, 2010, IEEE T MULTIMEDIA, V12, P576, DOI 10.1109/TMM.2010.2052592
   Woolley AW, 2010, SCIENCE, V330, P686, DOI 10.1126/science.1193147
   Yang ZJ, 2014, IEEE T MULTIMEDIA, V16, P1766, DOI 10.1109/TMM.2014.2328311
   Zaki M. J., 2014, DATA MINING ANAL FUN, P12
NR 49
TC 24
Z9 26
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 643
EP 658
DI 10.1109/TMM.2016.2521348
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300008
DA 2024-07-18
ER

PT J
AU Fan, HF
   Wang, RG
   Ding, L
   Xie, XD
   Jia, HZ
   Gao, W
AF Fan, Hongfei
   Wang, Ronggang
   Ding, Lin
   Xie, Xiaodong
   Jia, Huizhu
   Gao, Wen
TI Hybrid Zero Block Detection for High Efficiency Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Genuine zero block; high efficiency video coding (HEVC); pseudo zero
   block; rate-distortion estimation; rate-distortion optimization (RDO);
   zero block detection
ID QUANTIZED DCT COEFFICIENTS; ALGORITHM; PREDICTION
AB In this paper we proposean efficient hybrid zero block early detection method for high efficiency video coding (HEVC). Our method detects both genuine zero blocks (GZBs) and pseudo zero blocks (PZBs). For GZB detection, we use a two sum of absolute difference bounds and a one sum of absolute transformed difference threshold to decrease the GZB detection complexity. A fast rate-distortion estimation algorithm for HEVC is proposed to improve the PZB detection rate. Experimental results on the HM platform show that the proposed method saves about 50% of the rate-distortion optimization (RTO) time, with negligible Bjontegaard delta bit rate loss. Our method is faster than other state-of-the-art ZB detection methods for HEVC by 10%-30%.
C1 [Fan, Hongfei; Wang, Ronggang; Ding, Lin] Peking Univ, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Fan, Hongfei; Ding, Lin; Xie, Xiaodong; Jia, Huizhu; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Peking University; Peking University
RP Fan, HF; Wang, RG; Ding, L (corresponding author), Peking Univ, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.; Xie, XD; Jia, HZ; Gao, W (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Inst Digital Media, Beijing 100871, Peoples R China.
EM hffan@pku.edu.cn; rgwang@pkusz.edu.cn; ding.lin@pku.edu.cn;
   donxie@pku.edu.cn; hzjia@pku.edu.cn; wgao@pku.edu.cn
FU National Natural Science Foundation of China [61370115, 61402018]; China
   863 Project [2015AA015905]; Shenzhen Peacock Plan; Fundamental Research
   Project
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61370115 and Grant 61402018, in part by
   the China 863 Project 2015AA015905, in part by the Shenzhen Peacock
   Plan, and in part by the Fundamental Research Project. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shahram Shirani.
CR [Anonymous], 2013, 12 M JCT VC GEN SWIT
   Chiang PT, 2013, IEEE INT SYMP CIRC S, P1640, DOI 10.1109/ISCAS.2013.6572177
   Chiu WY, 2010, TENCON IEEE REGION, P687, DOI 10.1109/TENCON.2010.5686628
   Guoyun Zhong, 2011, Proceedings of the 2011 Second International Conference on Digital Manufacturing and Automation (ICDMA 2011), P535, DOI 10.1109/ICDMA.2011.135
   Lee K, 2013, IEEE J-STSP, V7, P1124, DOI 10.1109/JSTSP.2013.2272772
   Lv Z. G., 2014, P SPIE VIS INFORM PR, V9029, P7
   Moon YH, 2005, IEEE T CIRC SYST VID, V15, P1053, DOI 10.1109/TCSVT.2005.852411
   Su CY, 2007, IEEE INT SYM MULTIM, P72, DOI 10.1109/ISM.2007.14
   Wang HL, 2008, IEEE T CIRC SYST VID, V18, P510, DOI 10.1109/TCSVT.2008.918553
   Wang HL, 2007, IEEE T MULTIMEDIA, V9, P728, DOI 10.1109/TMM.2007.893336
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P547, DOI 10.1109/TCSVT.2006.871390
   Wu D., 2007, P IEEE INT C IM PROC, V5, P329
   Yang JF, 2002, IEEE T CIRC SYST VID, V12, P948, DOI 10.1109/TCSVT.2002.804892
   Yong Zhao, 2010, Proceedings of the 2010 6th International Conference on Digital Content, Multimedia Technology and its Applications (IDC 2010), P183
   Zhang MJ, 2009, IEEE T CIRC SYST VID, V19, P103, DOI 10.1109/TCSVT.2008.2009239
   Zhao X, 2010, IEEE T CIRC SYST VID, V20, P647, DOI 10.1109/TCSVT.2010.2045803
   Zhu C, 2014, IEEE INT SYMP CIRC S, P5, DOI 10.1109/ISCAS.2014.6865051
NR 17
TC 17
Z9 17
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 537
EP 543
DI 10.1109/TMM.2016.2515365
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600018
DA 2024-07-18
ER

PT J
AU Luo, CZ
   Ni, BB
   Yan, SC
   Wang, M
AF Luo, Changzhi
   Ni, Bingbing
   Yan, Shuicheng
   Wang, Meng
TI Image Classification by Selective Regularized Subspace Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coarse-to-fine; feature learning; image classification; selective
   regularized subspace learning (SRSL)
ID SAMPLE-SIZE PROBLEM; FACE RECOGNITION; DISCRIMINANT; MODEL
AB Feature learning is an intensively studied research topic in image classification. Although existing methods like sparse coding, locality-constrained linear coding, fisher vector encoding, etc., have shown their effectiveness in image representation, most of them overlook a phenomenon called thesmall sample size problem, where the number of training samples is relatively smaller than the dimensionality of the features, which may limit the predictive power of the classifier. Subspace learning is a strategy to mitigate this problem by reducing the dimensionality of the features. However, most conventional subspace learning methods attempt to learn a global subspace to discriminate all the classes, which proves to be difficult and ineffective in multi-class classification task. To this end, we propose to learn a local subspace for each sample instead of learning a global subspace for all samples. Our key observation is that, in multi-class image classification, the label of each testing sample is only confused by a few classes which have very similar visual appearance to it. Thus, in this work, we propose a coarse-to-fine strategy, which first picks out such classes, and then conducts a local subspace learning to discriminate them. As the subspace learning method is regularized and conducted within some selected classes, we term it selective regularized subspace learning (SRSL), and we term our classification pipeline selective regularized subspace learning based multi-class image classification (SRSL_MIC). Experimental results on four representative datasets (Caltech-101, Indoor-67, ORL Faces and AR Faces) demonstrate the effectiveness of the proposed method.
C1 [Luo, Changzhi; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
   [Ni, Bingbing] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
C3 Hefei University of Technology; Shanghai Jiao Tong University; National
   University of Singapore
RP Luo, CZ (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Hefei 230009, Peoples R China.
EM changzhiluo@gmail.com; nibingbing1983@gmail.com; eleyans@nus.edu.sg;
   eric.mengwang@gmail.com
RI Yan, Shuicheng/HCI-1431-2022; Wang, Meng/ITR-8699-2023
FU National 973 Program of China [2014CB347600]; National Nature Science
   Foundation of China [61272393, 61322201, 61432019]
FX This work was supported by the National 973 Program of China under Grant
   2014CB347600, and by the National Nature Science Foundation of China
   under Grant 61272393, Grant 61322201, and Grant 61432019.
CR Altintakan UL, 2015, IEEE T MULTIMEDIA, V17, P323, DOI 10.1109/TMM.2014.2388312
   [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   [Anonymous], 2010, P NIPS
   [Anonymous], 2007, PROC IEEE INT C COMP
   [Anonymous], 2009, P ADV NEUR INF PROC
   [Anonymous], 2008, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2008.4587841
   [Anonymous], 2014, ACMMM
   [Anonymous], CORR
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Chatfield K, 2015, INT J MULTIMED INF R, V4, P75, DOI 10.1007/s13735-015-0077-0
   Chen HT, 2005, PROC CVPR IEEE, P846
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chung F. R. K., 1997, AM MATH SOC, V92, DOI DOI 10.1090/CBMS/092
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Domeniconi C., 2001, NIPS, P665
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Fergus R, 2005, PROC CVPR IEEE, P380
   Fu Y., 2005, LOCALLY LINEAR EMBED
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Ginsca Alexandru Lucian, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P318, DOI 10.1007/978-3-319-14445-0_28
   Gumus E, 2010, EXPERT SYST APPL, V37, P6404, DOI 10.1016/j.eswa.2010.02.079
   Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Huang R, 2002, INT C PATT RECOG, P29, DOI 10.1109/ICPR.2002.1047787
   Jaakkola TS, 1999, ADV NEUR IN, V11, P487
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Jing Huang, 1998, Proceedings ACM Multimedia 98, P219
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Kesorn K, 2012, IEEE T MULTIMEDIA, V14, P211, DOI 10.1109/TMM.2011.2170665
   Kokiopoulou E, 2008, IEEE T MULTIMEDIA, V10, P806, DOI 10.1109/TMM.2008.922806
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li Li-Jia., 2012, Trends and Topics in Computer Vision, P57
   Liu XW, 2013, IEEE T CYBERNETICS, V43, P371, DOI 10.1109/TSMCB.2012.2207889
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu YJ, 2009, IEEE T MULTIMEDIA, V11, P1289, DOI 10.1109/TMM.2009.2030632
   Martinez A. M., 1998, 24 AUT U BARC
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Oommen T, 2008, MATH GEOSCI, V40, P409, DOI 10.1007/s11004-008-9156-6
   Pandey M, 2011, IEEE I CONF COMP VIS, P1307, DOI 10.1109/ICCV.2011.6126383
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Reed S E., CoRR
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sadeghi F, 2012, LECT NOTES COMPUT SC, V7576, P228, DOI 10.1007/978-3-642-33715-4_17
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shaban A, 2013, PROC CVPR IEEE, P2794, DOI 10.1109/CVPR.2013.360
   SHAHSHAHANI BM, 1994, IEEE T GEOSCI REMOTE, V32, P1087, DOI 10.1109/36.312897
   Silla CN, 2011, DATA MIN KNOWL DISC, V22, P31, DOI 10.1007/s10618-010-0175-9
   Simonyan K., 2014, 14091556 ARXIV
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Song XN, 2010, APPL SOFT COMPUT, V10, P208, DOI 10.1016/j.asoc.2009.07.002
   Su YC, 2014, IEEE T MULTIMEDIA, V16, P1645, DOI 10.1109/TMM.2014.2322337
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Vincent P, 2002, ADV NEUR IN, V14, P985
   Wah Catherine, 2011, Technical report
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Ye JP, 2004, IEEE T PATTERN ANAL, V26, P982, DOI 10.1109/TPAMI.2004.37
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
NR 75
TC 52
Z9 55
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 40
EP 50
DI 10.1109/TMM.2015.2495248
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700005
DA 2024-07-18
ER

PT J
AU Dong, L
   Fang, YM
   Lin, WS
   Seah, HS
AF Dong, Lu
   Fang, Yuming
   Lin, Weisi
   Seah, Hock Soon
TI Perceptual Quality Assessment for 3D Triangle Mesh Based on Curvature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Curvature; human visual system; quality assessment; saturation effect;
   structure distortion; visual masking
ID IMAGE QUALITY; METRICS; ERROR
AB Triangle meshes are used in representation of 3D geometric models, and they are subject to various visual distortions during geometrical processing and transmission. In this study, we propose a novel objective quality assessment method for 3D meshes based on curvature information; according to characteristics of the human visual system (HVS), two new components including visual masking and saturation effect are designed for the proposed method. Besides, inspired by the fact that the HVS is sensitive to structural information, we compute the structure distortion of 3D meshes. We test the performance of the proposed method on three publicly available databases of 3D mesh quality evaluation. We rotate among these databases for parameter determination to demonstrate the robustness of the proposed scheme. Experimental results demonstrate that the proposed method can predict consistent results in terms of correlation to the subjective scores across the databases.
C1 [Dong, Lu; Lin, Weisi; Seah, Hock Soon] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
C3 Nanyang Technological University; Jiangxi University of Finance &
   Economics
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM s090059@e.ntu.edu.sg; FA0001NG@e.ntu.edu.sg; WSLin@ntu.edu.sg;
   ashsseah@ntu.edu.sg
RI Seah, Hock Soon/AAK-9900-2020; Lin, Weisi/A-8011-2012; Lin,
   Weisi/A-3696-2011
OI Seah, Hock Soon/0000-0003-2699-7147; Lin, Weisi/0000-0001-9866-1947
FU Singapore MoE Tier 1 Project [M4011379 RG141/14]; National NSF of China
   [61571212, 20142BAB217011, 20151BDH80003]; Scientific Research
   Foundation for the Returned Overseas Chinese Scholars, State Education
   Ministry, China
FX This work was supported in part by the Singapore MoE Tier 1 Project
   M4011379 RG141/14, by the National NSF of China under Grant 61571212,
   Grant 20142BAB217011, and Grant 20151BDH80003, and by the Scientific
   Research Foundation for the Returned Overseas Chinese Scholars, State
   Education Ministry, China. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Maria
   Martini. (Corresponding author: Yuming Fang.)
CR Alliez P, 2003, ACM T GRAPHIC, V22, P485, DOI 10.1145/882262.882296
   Bian Z, 2009, J COMPUT SCI TECH-CH, V24, P65, DOI 10.1007/s11390-009-9198-3
   Cheng I, 2007, IEEE T MULTIMEDIA, V9, P386, DOI 10.1109/TMM.2006.886291
   Corsini M, 2013, COMPUT GRAPH FORUM, V32, P101, DOI 10.1111/cgf.12001
   Corsini M, 2007, IEEE T MULTIMEDIA, V9, P247, DOI 10.1109/TMM.2006.886261
   Daly S., 1993, Digital Images and Human Vision
   Desbrun M, 2002, COMPUT GRAPH FORUM, V21, P209, DOI 10.1111/1467-8659.00580
   Dong L, 2014, J VIS COMMUN IMAGE R, V25, P525, DOI 10.1016/j.jvcir.2013.11.009
   Eckert MP, 1998, SIGNAL PROCESS, V70, P177, DOI 10.1016/S0165-1684(98)00124-8
   Hillaire S, 2012, IEEE T VIS COMPUT GR, V18, P356, DOI 10.1109/TVCG.2011.154
   Karni Z, 2000, COMP GRAPH, P279, DOI 10.1145/344779.344924
   Kim SJ, 2002, 10TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P276, DOI 10.1109/PCCGA.2002.1167871
   Lambrecht CJV, 1996, P SOC PHOTO-OPT INS, V2668, P450, DOI 10.1117/12.235440
   Lavoué G, 2006, PROC SPIE, V6312, DOI 10.1117/12.686964
   Lavoué G, 2011, COMPUT GRAPH FORUM, V30, P1427, DOI 10.1111/j.1467-8659.2011.02017.x
   Lavoué G, 2010, IEEE T MULTIMEDIA, V12, P636, DOI 10.1109/TMM.2010.2060475
   Lavoué G, 2009, ACM T APPL PERCEPT, V5, DOI 10.1145/1462048.1462052
   Lin WS, 2012, IEEE J-STSP, V6, P614, DOI 10.1109/JSTSP.2012.2215433
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Qu LJ, 2008, IEEE T VIS COMPUT GR, V14, P1015, DOI 10.1109/TVCG.2008.51
   Sorkine O., 2003, Symposium on Geometry Processing, P42
   Torkhani F., MACH GRAPH VIS UNPUB
   Torkhani F, 2012, LECT NOTES COMPUT SC, V7594, P253, DOI 10.1007/978-3-642-33564-8_31
   Vása L, 2012, COMPUT GRAPH FORUM, V31, P1715, DOI 10.1111/j.1467-8659.2012.03176.x
   Wang K, 2012, COMPUT GRAPH-UK, V36, P808, DOI 10.1016/j.cag.2012.06.004
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Watson AB, 1997, P SOC PHOTO-OPT INS, V3016, P2, DOI 10.1117/12.274501
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Zhao Y, 2011, IEEE T CIRC SYST VID, V21, P1890, DOI 10.1109/TCSVT.2011.2157189
NR 31
TC 30
Z9 33
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2174
EP 2184
DI 10.1109/TMM.2015.2484221
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500006
DA 2024-07-18
ER

PT J
AU Janowski, L
   Pinson, M
AF Janowski, Lucjan
   Pinson, Margaret
TI The Accuracy of Subjects in a Quality Experiment: A Theoretical Subject
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Design of experiments; mean opinion score; quality of experience (QoE);
   subject model; subjective ratings; video quality assessment
AB How accurately are people able to use the absolute category rating (ACR) 5-level scale? Put another way, how repeatable are an individual subject's scores? Several subjective experiments have asked subjects to rate the same sequences a couple of times. Analyses indicate that none of the subjects exactly repeated their prior scores for these sequences. We would like to better understand this imperfection. This paper uses ACR subjective video quality tests to explore the precision of subjective ratings. To make formal measurements possible, we propose a theoretical subject model that is the main contribution of this paper. The proposed subject model indicates three major factors that influence accuracy: subject bias, subject inaccuracy, and stimulus scoring difficulty. These appear to be separate random effects and their existence is a reason why none of the subjects were able to perfectly repeat scores. There are three key consequences. First, subject scoring behavior includes a random component that spans approximately half of the rating scale. Second, the sensitivity and accuracy of most subjective analyses can be improved if the subject scores are normalized by removing subject bias. Third, to some extent, multiple subjects can be replaced with a single subject who rates each sequence multiple times.
C1 [Janowski, Lucjan] AGH Univ Sci & Technol, PL-30059 Krakow, Poland.
   [Pinson, Margaret] Inst Telecommun Sci, Boulder, CO 80305 USA.
C3 AGH University of Krakow
RP Janowski, L (corresponding author), AGH Univ Sci & Technol, Dept Telecommun, PL-30059 Krakow, Poland.
EM janowski@kt.agh.edu.pl; margaret@its.bldrdoc.gov
RI Pinson, Margaret H/A-8342-2013; Janowski, Lucjan/B-2264-2013
FU  [11.11.230.018]
FX The work of L. Janowski was supported by Contract 11.11.230.018. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yonggang Wen.
CR [Anonymous], 2013, PROCEEDING EUROPEAN
   [Anonymous], 1994, CONTRIBUTION TO ANSI
   [Anonymous], 2003, BS1534 ITUR
   Connors L., 2014, HB14501 NAT TEL INF
   Dong Shi, 2010, 2010 2nd International Conference on Industrial Mechatronics and Automation (ICIMA 2010), P229, DOI 10.1109/ICINDMA.2010.5538328
   Gowacz A, 2010, ANN TELECOMMUN, V65, P3, DOI 10.1007/s12243-009-0146-6
   Hoene C., 2013, SUMMARY OPUS LISTENI
   Hossfeld T, 2011, INT WORK QUAL MULTIM, P131, DOI 10.1109/QoMEX.2011.6065690
   Huynh-Thu Q, 2010, SIGNAL PROCESS-IMAGE, V25, P535, DOI 10.1016/j.image.2010.03.006
   Janowski L, 2014, TM14505 NTIA
   Kahneman Daniel, 2011, Thinking, fast and slow (macmillan)
   Kovács PT, 2014, IEEE IMAGE PROC, P768, DOI 10.1109/ICIP.2014.7025154
   Lee JS, 2014, IEEE T MULTIMEDIA, V16, P564, DOI 10.1109/TMM.2013.2292590
   Muralimanohar R. K., 2013, P M AC, V19, P3388
   Ostaszewska A., 2010, WEARABLE AUTONOMOUS, P315
   Pinson M, 2010, REPORT VALIDATION VI
   Pinson M., P 5 INT WORKSH UNPUB
   Pinson MH, 2013, IEEE INT WORKSH MULT, P458, DOI 10.1109/MMSP.2013.6659332
   Pinson MH, 2013, INT WORK QUAL MULTIM, P30, DOI 10.1109/QoMEX.2013.6603199
   Pinson MH, 2012, IEEE J-STSP, V6, P640, DOI 10.1109/JSTSP.2012.2215306
   Pinson MH, 2010, IEEE T BROADCAST, V56, P86, DOI 10.1109/TBC.2009.2034511
   Tavakoli S, 2014, IEEE J SEL AREA COMM, V32, P684, DOI 10.1109/JSAC.2014.140402
   Tominaga Toshiko, 2010, Proceedings of the 2010 Second International Workshop on Quality of Multimedia Experience (QoMEX 2010), P82, DOI 10.1109/QOMEX.2010.5517948
   VANDIJK AM, 1995, P SOC PHOTO-OPT INS, V2451, P90, DOI 10.1117/12.201231
   VORAN SD, 1992, IEE CONF PUBL, V358, P504
   Winkler S, 2012, IEEE J-STSP, V6, P616, DOI 10.1109/JSTSP.2012.2215007
NR 26
TC 49
Z9 52
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2210
EP 2224
DI 10.1109/TMM.2015.2484963
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500009
OA hybrid
DA 2024-07-18
ER

PT J
AU Kim, H
   Lee, S
AF Kim, Haksub
   Lee, Sanghoon
TI Transition of Visual Attention Assessment in Stereoscopic Images With
   Evaluation of Subjective Visual Quality and Discomfort
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Eye-tracker; human visual system; saliency entropy; stereoscopic;
   subjective quality/discomfort assessment; transition of visual attention
   (ToVA)
ID VIDEO QUALITY; SALIENCY; MODEL
AB Through statistical analysis of the behaviors of human visual attention, we discovered that fixation behaviors are highly correlated with the degree to which the viewer experiences visual quality/discomfort with stereoscopic images. To quantify the correlation between visual attention and quality/discomfort with respect to various distortions and disparities, we explore a novel methodology called transition of visual attention (ToVA) that accounts for diverse low-level luminance, chrominance, and depth attributes of 3D images in eye-tracker experiments. Moreover, saliency entropy is defined in order to quantify the distribution of fixations for 3D images. Using saliency entropy, we interpret perceptual factors in accordance with the non-uniform resolution of the human eye and the stereoscopic limits caused by foveation and Panum's fusional area. We then measure ToVA in terms of relative saliency entropy using Kullback-Leibler divergence. To evaluate the effectiveness of ToVA, a successful example application is also provided for which ToVA is applied to obtain subjective measurements of the quality and discomfort experienced when viewing 3D displays, rather than relying on conventional subjective tests using a scoring system.
C1 [Kim, Haksub; Lee, Sanghoon] Yonsei Univ, Dept Elect & Elect Engn, Seoul 120749, South Korea.
C3 Yonsei University
RP Lee, S (corresponding author), Yonsei Univ, Dept Elect & Elect Engn, Seoul 120749, South Korea.
EM khsphillip@yonsei.ac.kr; slee@yonsei.ac.kr
RI Lee, Sanghoon/A-3430-2019
OI Lee, Sanghoon/0000-0001-9895-5347
FU Ministry of Science, ICT, and Future Planning (MSIP), Korea
FX This work was supported by the Ministry of Science, ICT, and Future
   Planning (MSIP), Korea under the ICT R&D Program 2015. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Christian Timmerer. (Corresponding author:
   Sanghoon Lee.)
CR [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2005, ISOIWA32005
   [Anonymous], 2010, P INT WORKSH VID PRO
   Barkowsky Marcus, 2010, 2010 18th International Packet Video Workshop (PV 2010), P193, DOI 10.1109/PV.2010.5706838
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Geisler WS, 1998, P SOC PHOTO-OPT INS, V3299, P294, DOI 10.1117/12.320120
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Huynh-Thu Q, 2011, IEEE T BROADCAST, V57, P421, DOI 10.1109/TBC.2011.2128250
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Itti L, 2009, VISION RES, V49, P1295, DOI 10.1016/j.visres.2008.09.007
   ITU, 2008, Rec. ITU-T P.910
   Kim H, 2014, IEEE T IMAGE PROCESS, V23, P1476, DOI 10.1109/TIP.2014.2303640
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   KULLBACK S, 1987, AM STAT, V41, P340
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Lee K, 2014, IEEE T IMAGE PROCESS, V23, P450, DOI 10.1109/TIP.2013.2290592
   Lee S, 2003, IEEE T CIRC SYST VID, V13, P149, DOI 10.1109/TCSVT.2002.808441
   Lee S, 2002, IEEE T MULTIMEDIA, V4, P129, DOI 10.1109/6046.985561
   Lee S, 2001, IEEE T IMAGE PROCESS, V10, P977, DOI 10.1109/83.931092
   Liu Y, 2010, J VIS, V10, P1, DOI DOI 10.1167/10.12.23.[
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   MULLEN KT, 1985, J PHYSIOL-LONDON, V359, P381, DOI 10.1113/jphysiol.1985.sp015591
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   Park J, 2015, IEEE T IMAGE PROCESS, V24, P1101, DOI 10.1109/TIP.2014.2383327
   Rajashekar U, 2008, IEEE T IMAGE PROCESS, V17, P564, DOI 10.1109/TIP.2008.917218
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Walther D., 2004, WORKSHOP ATTENTION P, P96
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang SN, 2012, OPTOMETRY VISION SCI, V89, P1068, DOI 10.1097/OPX.0b013e31825da430
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang Y, 2010, LECT NOTES COMPUT SC, V5916, P314, DOI 10.1007/978-3-642-11301-7_33
NR 35
TC 36
Z9 38
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2198
EP 2209
DI 10.1109/TMM.2015.2493367
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500008
DA 2024-07-18
ER

PT J
AU You, QZ
   Cao, LL
   Cong, Y
   Zhang, XC
   Luo, JB
AF You, Quanzeng
   Cao, Liangliang
   Cong, Yang
   Zhang, Xianchao
   Luo, Jiebo
TI A Multifaceted Approach to Social Multimedia-Based Prediction of
   Elections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data mining; election; prediction; social multimedia; vector
   auto-regression
ID PRESIDENTIAL-ELECTION
AB Compared with real-world polling, election prediction based on social media can be far more timely and cost-effective due to the immediate availability of fast evolving Web contents. However, information from social media may suffer from noise and sampling bias that are caused by various factors and thus pose one of biggest challenges in social media-based data analytics. This paper presents a new model, named competitive vector auto regression (CVAR), to build a reliable forecasting system for the US presidential elections and US House race. Our CVAR model is designed to analyze the correlation between image-centric social multimedia and real-world phenomena. By introducing the competition mechanism, CVAR compares the popularity among multiple competing candidates. More importantly, CVAR is able to combine visual information with textual information from rich and multifaceted social multimedia, which helps extract reliable signals and mitigate sampling bias. As a result, our proposed system can 1) accurately predict the election outcome, 2) infer the sentiment of the candidate photos shared in the social media communities, and 3) account for the sentiment of viewer comments towards the candidates on the related images. The experiments on the 2012 US presidential election at both national and state levels, as well as the 2014 US House race, have demonstrated the power and promise of the proposed approach.
C1 [You, Quanzeng; Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14623 USA.
   [Cao, Liangliang] Columbia Univ, Dept Elect Engn & Comp Sci, New York, NY 10027 USA.
   [Cao, Liangliang] Yahoo Labs, New York, NY 10036 USA.
   [Cong, Yang] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Zhang, Xianchao] Dalian Univ Technol, Sch Software, Dalian 116600, Peoples R China.
C3 University of Rochester; Columbia University; Yahoo! Inc; Yahoo! Inc
   United States; Chinese Academy of Sciences; Shenyang Institute of
   Automation, CAS; Dalian University of Technology
RP You, QZ (corresponding author), Univ Rochester, Comp Sci, Rochester, NY 14623 USA.
EM qyou@cs.rochester.edu; liangliang.cao@gmail.com; congyang81@gmail.com;
   xczhang@dlut.edu.cn; jluo@cs.rochester.edu
RI Luo, Jiebo/AAI-7549-2020; You, Quanzeng/X-9917-2019
OI You, Quanzeng/0000-0003-3608-0607; Luo, Jiebo/0000-0002-4516-9729
CR [Anonymous], 2012, P 18 ACM SIGKDD INT
   Banbura M, 2010, J APPL ECONOMET, V25, P71, DOI 10.1002/jae.1137
   Blumenthal M, 2014, PS-POLIT SCI POLIT, V47, P297, DOI 10.1017/S1049096514000055
   Campbell JE, 2014, PS-POLIT SCI POLIT, V47, P301, DOI 10.1017/S1049096514000067
   Cao L., 2009, P 17 ACM INT C MULTI, P125
   Dickinson MJ, 2014, PS-POLIT SCI POLIT, V47, P309, DOI 10.1017/S1049096514000080
   Ding X., 2008, P 2008 INT C WEB SEA, P231, DOI [DOI 10.1145/1341531.1341561, 10.1145/1341531.1341561]
   Erikson RS, 2014, PS-POLIT SCI POLIT, V47, P313, DOI 10.1017/S1049096514000092
   Eysenbach Gunther, 2006, AMIA Annu Symp Proc, P244
   Finkel Jenny Rose, 2005, ACL, P363
   Franch F, 2013, J INF TECHNOL POLITI, V10, P57, DOI 10.1080/19331681.2012.705080
   Gayo A. D., 2011, P 5 INT AAAI C WEBL, P490
   Giglietto F., 2012, P ICWSM, P47
   Ginsberg J, 2009, NATURE, V457, P1012, DOI 10.1038/nature07634
   Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7
   Hillygus DS, 2011, PUBLIC OPIN QUART, V75, P962, DOI 10.1093/poq/nfr054
   Ishiguro K, 2012, IEEE DATA MINING, P906, DOI 10.1109/ICDM.2012.37
   Jin Xin, 2010, P 18 ACM INT C MULT, P1235, DOI [10.1145/1873951.1874196, DOI 10.1145/1873951.1874196]
   Johnson HA, 2004, STUD HEALTH TECHNOL, V107, P1202
   Kim Gunhee., 2012, ACM SIGKDD, P1068
   Ltkepohl H., 1993, Introduction to multiple time series analysis, Vsecond
   Ludwig S., 2012, 538S NATE SILVER EXP
   Metaxas P.T., 2011, 2011 IEEE 3 INT C PR, P165, DOI [10.1109/PASSAT/SocialCom.2011.98, DOI 10.1109/PASSAT/SOCIALCOM.2011.98]
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   OConnor B, 2010, ICWSM, P122, DOI DOI 10.1609/ICWSM.V4I1.14031
   Polgreen PM, 2008, CLIN INFECT DIS, V47, P1443, DOI 10.1086/593098
   Sadilek Adam., 2012, AAAI, DOI DOI 10.1609/AAAI.V26I1.8103
   Siersdorfer S., 2010, Proceedings of the 19th International Conference on World Wide Web, WWW'10, P891, DOI DOI 10.1145/1772690.1772781
   Silver N., 2015, SIGNAL NOISE WHY SO
   Tumasjan A, 2010, 4 INT AAAI C WEBL SO, V10, P178, DOI 10.1074/jbc.M501708200
   Wang M, 2012, ACM COMPUT SURV, V44, DOI 10.1145/2333112.2333120
   Zhang Haipeng., 2012, Proceedings of the 21st International Conference on World Wide Web, P749, DOI DOI 10.1145/2187836.2187938
   Zhang XC, 2012, IEEE DATA MINING, P1194, DOI 10.1109/ICDM.2012.28
   Zhu JH, 2013, INT CONF DAT MIN WOR, P421, DOI 10.1109/ICDMW.2013.112
NR 34
TC 27
Z9 28
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2271
EP 2280
DI 10.1109/TMM.2015.2487863
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500014
DA 2024-07-18
ER

PT J
AU Cheung, M
   She, J
   Jie, ZM
AF Cheung, Ming
   She, James
   Jie, Zhanming
TI Connection Discovery Using Big Data of User-Shared Images in Social
   Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Big data; connection; discovery; recommendation; social network
   analysis; user-shared images
ID RECOMMENDATION
AB Billions of user-shared images are generated by individuals in many social networks today, and this particular form of user data is widely accessible to others due to the nature of online social sharing. When user social graphs are only accessible to exclusive parties, these user-shared images are proved to be an easier and effective alternative to discover user connections. This work investigated over 360 000 user shared images from two social networks, Skyrock and 163 Weibo, in which 3 million follower/followee relationships are involved. It is observed that the shared images from users with a follower/followee relationship show relatively higher similarities. A multimedia big data system that utilizes this observed phenomenon is proposed as an alternative to user-generated tags and social graphs for follower/followee recommendation and gender identification. To the best of our knowledge, this is the first attempt in this field to prove and formulate such a phenomenon for mass user-shared images along with more practical prediction methods. These findings are useful for information or services recommendations in any social network with intensive image sharing, as well as for other interesting personalization applications, particularly when there is no access to those exclusive user social graphs.
C1 [Cheung, Ming; She, James; Jie, Zhanming] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Cheung, M (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
EM cpming@ust.hk
OI Cheung, Ming/0000-0003-4646-1980
FU HKUST-NIE Social Media Laboratory, Hong Kong University of Science and
   Technology
FX This work was supported by the HKUST-NIE Social Media Laboratory, Hong
   Kong University of Science and Technology. The guest editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Ramesh Jain.
CR [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2011, INT C WORLD WIDE WEB, DOI DOI 10.1145/1963405.1963481
   [Anonymous], 2010, P ACM WORKSH SURR ME
   Backstrom L., 2011, P 4 ACM INT C WEB SE, P635, DOI DOI 10.1145/1935826.1935914
   Chen JL, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P201, DOI 10.1145/1518701.1518735
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Chow WS, 2008, INFORM MANAGE-AMSTER, V45, P458, DOI 10.1016/j.im.2008.06.007
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Emily J., 2001, PHYS REV E, V4
   Estabrooks A, 2004, COMPUT INTELL-US, V20, P18, DOI 10.1111/j.0824-7935.2004.t01-1-00228.x
   FREEMAN LC, 1979, SOC NETWORKS, V1, P215, DOI 10.1016/0378-8733(78)90021-7
   Gilbert E, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P211
   Golder Scott A., 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P88, DOI 10.1109/SocialCom.2010.22
   Guy I., 2009, P 3 ACM C REC SYST, P53
   Guy I, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P194
   Hsu W., 2006, AAAI SPRING S SERIES
   Jie Z., P IEEE 4 S IN PRESS
   Jurie F, 2005, IEEE I CONF COMP VIS, P604
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Kahanda I., 2009, P INT AAAI C WEB SOC, V3, P74
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Konstas I, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/1571941.1571977
   Krackhardt D, 1999, J PERS SOC PSYCHOL, V76, P770, DOI 10.1037/0022-3514.76.5.770
   Leroy Vincent, 2010, P 16 INT C KNOWL DIS, P393
   Leskovec J., 2010, P 19 INT C WORLD WID, P641
   Li XH, 2008, IEEE T SIGNAL PROCES, V56, P675, DOI 10.1109/TSP.2007.907820
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lü LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027
   McCallum A., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P169, DOI 10.1145/347090.347123
   Meyerson A, 2001, ANN IEEE SYMP FOUND, P426, DOI 10.1109/SFCS.2001.959917
   Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29
   Moxley E, 2009, IEEE INT CON MULTI, P1452, DOI 10.1109/ICME.2009.5202776
   Ottoni R., 2013, Proceedings of the International Conference on Weblogs and Social Media, P457
   Parimi R, 2011, LECT NOTES ARTIF INT, V6635, P75, DOI 10.1007/978-3-642-20847-8_7
   Rae Adam., 2010, Adaptivity, Personalization and Fusion of Heterogeneous Information, P92
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Shepitsen A, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P259
   Walter FE, 2008, AUTON AGENT MULTI-AG, V16, P57, DOI 10.1007/s10458-007-9021-x
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Weng L, 2012, SCI REP-UK, V2, P1, DOI 10.1038/srep00335
   Xing Xie, 2010, Proceedings of the 2010 IEEE/ACM Int'l Conference on Green Computing and Communications (GreenCom) and Int'l Conference on Cyber, Physical and Social Computing (CPSCom), P831, DOI 10.1109/GreenCom-CPSCom.2010.28
   Zhang XM, 2013, SIGNAL PROCESS, V93, P2178, DOI 10.1016/j.sigpro.2012.05.021
   Zhang XM, 2012, J INTELL INF SYST, V39, P87, DOI 10.1007/s10844-011-0184-1
   Zheng Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921596
   Zhou TC, 2010, AAAI CONF ARTIF INTE, P1486
NR 46
TC 34
Z9 37
U1 2
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1417
EP 1428
DI 10.1109/TMM.2015.2460192
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000003
DA 2024-07-18
ER

PT J
AU Yeh, CH
   Tseng, TY
   Lee, CW
   Lin, CY
AF Yeh, Chia-Hung
   Tseng, Tsung-Yi
   Lee, Cheng-Wei
   Lin, Chih-Yang
TI Predictive Texture Synthesis-Based Intra Coding Scheme for Advanced
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; high efficient video coding (HEVC); intra coding; video
   coding; video compression
ID MODE DECISION ALGORITHM
AB This paper aims to improve the intra coding performance on H.264 and high efficient video coding (HEVC). A new intra prediction approach is proposed based on synthesizing two neighboring predictors. These two predictors are selected from different prediction directions and then the predicted block is generated by combining the two predictors using different weights. The weights for the predictors do not need to be saved, so the bit rate can be greatly reduced. The main contributions of this paper are proposing the following: 1) a highly efficient way and high compact compression to select the proper predictors; 2) a weight estimation method for the predictors; and 3) a reversible weight restoration method for the predictors to save the bit rate in the decoding phase. Experimental results show that the proposed method outperforms H.264/AVC and HEVC in intra prediction by 11.79% and 3.6%, respectively, in bitrate reduction.
C1 [Yeh, Chia-Hung] Natl Sun Yat Sen Univ, Dept Elect Engn, Kaohsiung 80424, Taiwan.
   [Yeh, Chia-Hung; Tseng, Tsung-Yi; Lee, Cheng-Wei] Natl Appl Res Lab, Natl Ctr High Performance Comp, Hsinchu 30076, Taiwan.
   [Lin, Chih-Yang] Asia Univ, Dept Comp Sci & Informat Engn, Taichung 41354, Taiwan.
   [Lin, Chih-Yang] China Med Univ, China Med Univ Hosp, Dept Med Res, Taichung 40447, Taiwan.
C3 National Sun Yat Sen University; National Applied Research Laboratories
   - Taiwan; Asia University Taiwan; China Medical University Taiwan; China
   Medical University Hospital - Taiwan
RP Lin, CY (corresponding author), Asia Univ, Dept Comp Sci & Informat Engn, Taichung 41354, Taiwan.
EM yeh@mail.ee.nsysu.edu.tw; M003010092@student.nsysu.edu.tw;
   M013010063@student.nsysu.edu.tw; andrewlin@asia.edu.tw
RI Lin, Chih-Yang/HOF-2583-2023
OI Lin, Chih-Yang/0000-0002-0401-8473
FU Ministry of Science and Technology, Taiwan [NSC102-2221-E-110-032-MY3,
   MOST103-2221-E-110-045-MY3, MOST103-2221-E-468-007-MY2]
FX This work was supported in part by the Ministry of Science and
   Technology, Taiwan, under Grant NSC102-2221-E-110-032-MY3, Grant
   MOST103-2221-E-110-045-MY3, and Grant MOST103-2221-E-468-007-MY2. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Enrico Magli. (Corresponding
   author: Chih-Yang Lin.)
CR [Anonymous], [No title captured]
   [Anonymous], 2014, HEVC Reference Software HM 13.0
   [Anonymous], 2001, ITU T VCEG M AUST TE
   Balle J., 2006, P ICIP, P93
   Hu X. - Y., 2011, P ICME, P158
   Lainema J., 2010, 2010 28th Picture Coding Symposium (PCS 2010), P198, DOI 10.1109/PCS.2010.5702460
   Liu D, 2008, IEEE T IMAGE PROCESS, V17, P1827, DOI 10.1109/TIP.2008.2002835
   Min J. - H., 2010, JCTVCB100 ITUT
   Shiodera T., 2007, VCEGAE14 ITUT
   Suhring K., 2011, H 264 AVC SOFTWARE C
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tan TK, 2006, IEEE IMAGE PROC, P1693, DOI 10.1109/ICIP.2006.312685
   Tan TK, 2007, CONSUM COMM NETWORK, P405, DOI 10.1109/CCNC.2007.86
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang CC, 2010, IEEE T CIRC SYST VID, V20, P1150, DOI 10.1109/TCSVT.2010.2056953
   Ye Y, 2008, IEEE IMAGE PROC, P2116, DOI 10.1109/ICIP.2008.4712205
   Yeh CH, 2015, J VIS COMMUN IMAGE R, V29, P33, DOI 10.1016/j.jvcir.2015.01.010
   Yeh CH, 2014, IEEE T IND INFORM, V10, P594, DOI 10.1109/TII.2013.2273308
   Yeh CH, 2010, IEEE T CIRC SYST VID, V20, P563, DOI 10.1109/TCSVT.2010.2041825
NR 20
TC 22
Z9 22
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1508
EP 1514
DI 10.1109/TMM.2015.2449659
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000011
DA 2024-07-18
ER

PT J
AU Kiliç, V
   Barnard, M
   Wang, WW
   Kittler, J
AF Kilic, Volkan
   Barnard, Mark
   Wang, Wenwu
   Kittler, Josef
TI Audio Assisted Robust Visual Tracking With Adaptive Particle Filtering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive particle filter; audio-visual speaker tracking; particle filter
ID MULTIPLE SPEAKERS; LOCALIZATION; SEGMENTATION; MICROPHONE; FUSION
AB The problem of tracking multiple moving speakers in indoor environments has received much attention. Earlier techniques were based purely on a single modality, e.g., vision. Recently, the fusion of multi-modal information has been shown to be instrumental in improving tracking performance, as well as robustness in the case of challenging situations like occlusions (by the limited field of view of cameras or by other speakers). However, data fusion algorithms often suffer from noise corrupting the sensor measurements which cause non-negligible detection errors. Here, a novel approach to combining audio and visual data is proposed. We employ the direction of arrival angles of the audio sources to reshape the typical Gaussian noise distribution of particles in the propagation step and to weight the observation model in the measurement step. This approach is further improved by solving a typical problem associated with the PF, whose efficiency and accuracy usually depend on the number of particles and noise variance used in state estimation and particle propagation. Both parameters are specified beforehand and kept fixed in the regular PF implementation which makes the tracker unstable in practice. To address these problems, we design an algorithm which adapts both the number of particles and noise variance based on tracking error and the area occupied by the particles in the image. Experiments on the AV16.3 dataset show the advantage of our proposed methods over the baseline PF method and an existing adaptive PF algorithm for tracking occluded speakers with a significantly reduced number of particles.
C1 [Kilic, Volkan; Barnard, Mark; Wang, Wenwu; Kittler, Josef] Univ Surrey, Dept Elect Engn, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
C3 University of Surrey
RP Kiliç, V (corresponding author), Univ Surrey, Dept Elect Engn, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM v.kilic@surrey.ac.uk; mark.barnard@surrey.ac.uk; w.wang@surrey.ac.uk;
   j.kittler@surrey.ac.uk
RI wang, wenwu/HOF-4371-2023; Kılıç, Volkan/JZE-2927-2024
OI Kılıç, Volkan/0000-0002-3164-1981
FU Engineering and Physical Sciences Research Council of the U.K.
   [EP/H050000/1, EP/K014307/1, EP/L000539/1]; EPSRC [EP/L000539/1,
   EP/H050000/1, EP/K014307/1, EP/K014307/2] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Sciences
   Research Council of the U.K. under Grant EP/H050000/1, Grant
   EP/K014307/1, and Grant EP/L000539/1. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Lexing Xie.
CR [Anonymous], 2006, 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, DOI [10.1109/CVPR.2006.312, DOI 10.1109/CVPR.2006.312]
   [Anonymous], 2007, Surveillance performance evaluation initiative (SPEVI) audiovisual people dataset
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Bar-Shalom Y., 1995, MULTITARGET MULTISEN
   BARSHALOM Y, 1998, TRACKING DATA ASS
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Blackrnan Samuel., 1999, Design and analysis of modern tracking systems
   Bluman A.G., 2013, ELEMENTARY STAT
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Chakravorty R, 2009, FUSION: 2009 12TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION, VOLS 1-4, P316
   Checka N, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P881
   Closas P., 2011, 2011 AER C BIG SKY M, P1, DOI DOI 10.1109/AERO.2011.5747439
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Crocco M, 2012, INT CONF ACOUST SPEE, P2597, DOI 10.1109/ICASSP.2012.6288448
   DiBiase J. H., 2000, Ph.D. thesis
   Fallon MF, 2012, IEEE T AUDIO SPEECH, V20, P1409, DOI 10.1109/TASL.2011.2178402
   Fox D, 2003, INT J ROBOT RES, V22, P985, DOI 10.1177/0278364903022012001
   Fritsch J., 2004, P INT C INTELLIGENT, P898
   Gatica-Perez D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P25
   Gatica-Perez D, 2007, IEEE T AUDIO SPEECH, V15, P601, DOI 10.1109/TASL.2006.881678
   Gehrig T, 2005, IEEE WORK APPL SIG, P118, DOI 10.1109/ASPAA.2005.1540183
   Godsill S. J., 1998, Digital Audio Restora- tion
   GORDON NJ, 1993, IEE PROC-F, V140, P107, DOI 10.1049/ip-f-2.1993.0015
   Isard M, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P34, DOI 10.1109/ICCV.2001.937594
   Jaward M, 2006, AEROSP CONF PROC, P2151
   Kilic V., 2013, 21 EUR SIGN PROC C, P1
   Kiliç V, 2013, INT CONF ACOUST SPEE, P3627, DOI 10.1109/ICASSP.2013.6638334
   Kim K, 2006, LECT NOTES COMPUT SC, V3953, P98
   Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Lathoud G, 2005, INT CONF ACOUST SPEE, P265
   Lathoud G., 2006, EURASIP J APPL SIG P, V2006, P169
   Lathoud G., 2006, THESIS EPFL U LAUSAN
   Legg M, 2013, IEEE T IMAGE PROCESS, V22, P4028, DOI 10.1109/TIP.2013.2268974
   Mahler R., 2007, STAT MULTISOURCE MUL
   Mahler R, 2007, IEEE T AERO ELEC SYS, V43, P1523, DOI 10.1109/TAES.2007.4441756
   Mahler RPS, 2004, IEEE AERO EL SYS MAG, V19, P53, DOI 10.1109/MAES.2004.1263231
   McGee L. A., 1985, NASATM86847
   Panta K., P SPIE, V5429, P284
   Pham N.T., 2007, P 15 ACM INT C MULTI, P529
   Pitt MK, 2012, J ECONOMETRICS, V171, P134, DOI 10.1016/j.jeconom.2012.06.004
   Potamitis R, 2004, IEEE T SPEECH AUDI P, V12, P520, DOI 10.1109/TSA.2004.833004
   Quang Nguyen, 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2866, DOI 10.1109/ROBIO.2011.6181740
   Roman N, 2008, IEEE T AUDIO SPEECH, V16, P728, DOI 10.1109/TASL.2008.918978
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Sanin A, 2012, PATTERN RECOGN, V45, P1684, DOI 10.1016/j.patcog.2011.10.001
   Sarkka S., 2004, Seventh International Conference on Information Fusion, P583
   SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830
   Shivappa ST, 2010, IEEE J-STSP, V4, P882, DOI 10.1109/JSTSP.2010.2057890
   Sigal L, 2000, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2000.854764
   Soto A, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1398
   Stiefelhagen R, 2008, LECT NOTES COMPUT SC, V4625, P3
   Strobel N, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.911196
   Talantzis F, 2008, IEEE T SYST MAN CY B, V38, P799, DOI 10.1109/TSMCB.2008.922063
   Talantzis F, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P243, DOI 10.1109/MMSP.2006.285306
   Trivedi MM, 2005, IEEE T SYST MAN CY A, V35, P145, DOI 10.1109/TSMCA.2004.838480
   Verma V., 2003, P INT JOINT C ARTIFI, P976
   Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600
   Vo BN, 2004, INT CONF ACOUST SPEE, P357
   WANG H, 1985, IEEE T ACOUST SPEECH, V33, P823, DOI 10.1109/TASSP.1985.1164667
   Wang YL, 2008, PPAR RES, V2008, DOI 10.1155/2008/209629
   Yuan Li, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2953, DOI 10.1109/CVPRW.2009.5206735
NR 63
TC 57
Z9 60
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2015
VL 17
IS 2
BP 186
EP 200
DI 10.1109/TMM.2014.2377515
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AZ4RN
UT WOS:000348210500004
OA Green Published
DA 2024-07-18
ER

PT J
AU Yuan, ZH
   Ghinea, G
   Muntean, GM
AF Yuan, Zhenhui
   Ghinea, Gheorghita
   Muntean, Gabriel-Miro
TI Beyond Multimedia Adaptation: Quality of Experience-Aware
   Multi-Sensorial Media Delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mulsemedia; quality of experience; subjective testing
ID BANDWIDTH ESTIMATION
AB Multiple sensorial media (mulsemedia) combines multiple media elements which engage three or more of human senses, and as most other media content, requires support for delivery over the existing networks. This paper proposes an adaptive mulsemedia framework (ADAMS) for delivering scalable video and sensorial data to users. Unlike existing two-dimensional joint source-channel adaptation solutions for video streaming, the ADAMS framework includes three joint adaptation dimensions: video source, sensorial source, and network optimization. Using an MPEG-7 description scheme, ADAMS recommends the integration of multiple sensorial effects (i.e., haptic, olfaction, air motion, etc.) as metadata into multimedia streams. ADAMS design includes both coarse-and fine-grained adaptation modules on the server side: mulsemedia flow adaptation and packet priority scheduling. Feedback from subjective quality evaluation and network conditions is used to develop the two modules. Subjective evaluation investigated users' enjoyment levels when exposed to mulsemedia and multimedia sequences, respectively and to study users' preference levels of some sensorial effects in the context of mulsemedia sequences with video components at different quality levels. Results of the subjective study inform guidelines for an adaptive strategy that selects the optimal combination for video segments and sensorial data for a given bandwidth constraint and user requirement. User perceptual tests show how ADAMS outperforms existing multimedia delivery solutions in terms of both user perceived quality and user enjoyment during adaptive streaming of various mulsemedia content. In doing so, it highlights the case for tailored, adaptive mulsemedia delivery over traditional multimedia adaptive transport mechanisms.
C1 [Yuan, Zhenhui; Muntean, Gabriel-Miro] Dublin City Univ, Performance Engn Lab, Sch Elect Engn, Dublin 9, Ireland.
   [Ghinea, Gheorghita] Brunel Univ, Uxbridge UB8 3PH, Middx, England.
C3 Dublin City University; Brunel University
RP Yuan, ZH (corresponding author), Dublin City Univ, Performance Engn Lab, Sch Elect Engn, Dublin 9, Ireland.
EM zhenhui.yuan2@mail.dcu.ie; george.ghinea@brunel.ac.uk;
   gabriel.muntean@dcu.ie
RI Muntean, Gabriel-Miro/U-6783-2019; Ghinea, Gheorghita/AAG-6770-2020
OI Muntean, Gabriel-Miro/0000-0002-9332-4770; Ghinea,
   Gheorghita/0000-0003-2578-5580
FU Enhancing Performance Initiative at Dublin City University, Ireland;
   Enterprise Ireland Innovation Partnership Project with Ericsson
   [IP/2011/0135]; China Scholarship Council; Science Foundation Ireland
   [SFI/13/ISCA/2845]
FX This work was supported by the Enhancing Performance Initiative at
   Dublin City University, Ireland, the Enterprise Ireland Innovation
   Partnership Project with Ericsson under Grant IP/2011/0135, and the
   China Scholarship Council and Science Foundation Ireland under Grant
   SFI/13/ISCA/2845. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Tommaso Melodia.
CR [Anonymous], 2004, 3758 IETF RFC
   [Anonymous], 2008, Subjective video quality assessment methods for multimedia applications. Recommendation P.910
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Bodnar A, 2004, P 6 INT C MULTIMODAL, P183, DOI [10.1145/1027933.1027965, DOI 10.1145/1027933.1027965]
   Boyd-Davis S., 2006, P HUM FACT ERG SOC 2, P25
   Brewster S. A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P653
   Capone A, 2004, IEEE T MOBILE COMPUT, V3, P129, DOI 10.1109/TMC.2004.5
   Ciubotaru B, 2009, IEEE T BROADCAST, V55, P202, DOI 10.1109/TBC.2009.2020448
   Dai J, 2012, IEEE J SEL AREA COMM, V30, P458, DOI 10.1109/JSAC.2012.120226
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Ghinea G., 2011, MULTIPLE SENSORIAL M
   Ghinea G., 2012, ACM T MULTIMEDIA COM, V8
   Ghinea G, 2010, IEEE T SYST MAN CY A, V40, P657, DOI 10.1109/TSMCA.2010.2041224
   Hayward V., 2004, Sensor Review, V24, P16, DOI 10.1108/02602280410515770
   Ishibashi Y., 2004, P 12 ANN ACM INT C M, P604
   Issanchou S., 2002, TESTING ODOR MEMORY, P211
   ITU-T, 1998, Recommendation P.911
   Kahol K, 2006, ACM T MULTIM COMPUT, V2, P219, DOI 10.1145/1152149.1152153
   Kaye J."J."., 2004, INTERACTIONS, V11, P48, DOI DOI 10.1145/962342.964333
   Kaye J.N., 2001, THESIS MIT CAMBRIDGE
   Kennedy M, 2010, C LOCAL COMPUT NETW, P843, DOI 10.1109/LCN.2010.5735821
   Kortum P, 2008, MORG KAUF SER INTER, P1, DOI 10.1016/B978-0-12-374017-5.00001-8
   Kostopoulos K, 2007, J MULTIMODAL USER IN, V1, P13, DOI 10.1007/BF02910055
   Mastronarde N, 2013, IEEE T MULTIMEDIA, V15, P268, DOI 10.1109/TMM.2012.2231668
   Melodia T, 2010, IEEE J SEL AREA COMM, V28, P653, DOI 10.1109/JSAC.2010.100604
   Mingzhe Li, 2008, 2008 33rd IEEE Conference on Local Computer Networks (LCN 2008), P374, DOI 10.1109/LCN.2008.4664193
   Muntean GM, 2004, IEEE T BROADCAST, V50, P1, DOI 10.1109/TBC.2004.824745
   Murray N., 2013, P ACM MULT SYST C OS
   Richard E., 2006, Virtual Reality, V10, P207, DOI DOI 10.1007/S10055-006-0040-8
   Ryu J., 2004, Proc. ACM Symp. Virtual Real. Softw. Technol. VRST 04, P89, DOI DOI 10.1145/1077534.1077551
   Sikora T, 2001, IEEE T CIRC SYST VID, V11, P696, DOI 10.1109/76.927422
   Steinbach E, 2012, P IEEE, V100, P937, DOI 10.1109/JPROC.2011.2182100
   Tijou A, 2006, LECT NOTES COMPUT SC, V3942, P1223, DOI 10.1007/11736639_152
   Yuan Z., 2014, QUALITY EXPERIENCED
   Yuan ZH, 2014, INT WIREL COMMUN, P1142, DOI 10.1109/IWCMC.2014.6906515
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Yuan ZH, 2012, IEEE T VEH TECHNOL, V61, P2158, DOI 10.1109/TVT.2012.2190760
   Zhou L, 2011, IEEE J SEL AREA COMM, V29, P1358, DOI 10.1109/JSAC.2011.110803
NR 38
TC 74
Z9 74
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 104
EP 117
DI 10.1109/TMM.2014.2371240
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400010
OA Green Accepted, Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Petrazzuoli, G
   Maugey, T
   Cagnazzo, M
   Pesquet-Popescu, B
AF Petrazzuoli, Giovanni
   Maugey, Thomas
   Cagnazzo, Marco
   Pesquet-Popescu, Beatrice
TI Depth-Based Multiview Distributed Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3DTV; depth image based rendering (DIBR); depth maps; distributed source
   coding; shape adaptive coding
ID SIDE INFORMATION; 3-D VIDEO; IMAGE; TRANSFORM
AB Multiview distributed video coding (DVC) has gained much attention in the last few years because of its potential in avoiding communication between cameras without decreasing the coding performance. However, the current results are not matching the expectations mainly due to the fact that some theoretical assumptions are not satisfied in the current implementations. For example, in distributed source coding the encoder must know the correlation between the sources, which cannot be achieved in the traditional DVC systems without having a communication between the cameras. In this work, we propose a novel multiview distributed video coding scheme in which the depth maps are used to estimate the way two views are correlated with no exchanges between the cameras. Only their relative positions are known. We design the complete scheme and further propose a rate allocation algorithm to efficiently share the bit budget between the different components of our scheme. Then, a rate allocation algorithm for depth maps is proposed in order to maximize the quality of synthesized virtual views. We show, through detailed experiments, that our scheme significantly outperforms the state-of-the-art DVC system.
C1 [Petrazzuoli, Giovanni; Cagnazzo, Marco; Pesquet-Popescu, Beatrice] Inst Mines Telecom, Dept Image & Signal Proc, F-75634 Paris, France.
   [Petrazzuoli, Giovanni; Cagnazzo, Marco; Pesquet-Popescu, Beatrice] CNRS, Lab Commun & Proc Informat LTCI, Paris, France.
   [Maugey, Thomas] Ecole Polytech Fed Lausanne, Inst Elect Engn, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
C3 IMT - Institut Mines-Telecom; Centre National de la Recherche
   Scientifique (CNRS); Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Petrazzuoli, G (corresponding author), Inst Mines Telecom, Dept Image & Signal Proc, F-75634 Paris, France.
EM petrazzu@telecom-paristech.fr; maugey@epfl.ch;
   cagnazzo@telecom-paristech.fr; pesquet@telecom-paristech.fr
RI Cagnazzo, Marco/AAZ-3881-2020
OI Cagnazzo, Marco/0000-0001-6731-3755
CR Aaron A, 2002, CONF REC ASILOMAR C, P240
   [Anonymous], 2011, P 3DTV C TRUE VIS CA
   [Anonymous], 2009, JTC1SC219WG11MPEC IS
   [Anonymous], 2003, P IIASTED VIIP
   [Anonymous], COMMON TEST CONDITIO
   [Anonymous], P SPIE
   [Anonymous], 2001, VCEG M
   Areia JD, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P453, DOI 10.1109/MMSP.2007.4412914
   Artigas X, 2006, 2006 7TH NORDIC SIGNAL PROCESSING SYMPOSIUM, P250
   Artigas Xavi., 2007, PICTURE CODING S, V17, P1103
   Ascenso J., 2010, P 18 EUSIPCO AUG, P1
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Brites C, 2013, SIGNAL PROCESS-IMAGE, V28, P689, DOI 10.1016/j.image.2013.05.002
   Cagnazzo M, 2004, IEEE IMAGE PROC, P2459
   Cagnazzo M, 2007, IEEE T IMAGE PROCESS, V16, P2916, DOI 10.1109/TIP.2007.909315
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dai W, 2013, IEEE IMAGE PROC, P1787, DOI 10.1109/ICIP.2013.6738368
   Daribo Ismael, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P167, DOI 10.1109/MMSP.2010.5662013
   Daribo I, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P312, DOI 10.1109/MMSP.2007.4412880
   Ferre P., 2007, P IEEE INT C IM PROC, V6, pVI
   Foix S, 2011, IEEE SENS J, V11, P1917, DOI 10.1109/JSEN.2010.2101060
   Ganapathi V, 2010, PROC CVPR IEEE, P755, DOI 10.1109/CVPR.2010.5540141
   Guillemot C, 2007, IEEE SIGNAL PROC MAG, V24, P67, DOI 10.1109/MSP.2007.904808
   Kolb A, 2010, COMPUT GRAPH FORUM, V29, P141, DOI 10.1111/j.1467-8659.2009.01583.x
   Koninckx TP, 2006, IEEE T PATTERN ANAL, V28, P432, DOI 10.1109/TPAMI.2006.62
   Lee S., 2012, SPRINGER BRIEFS COMP
   Li SP, 2000, IEEE T CIRC SYST VID, V10, P725, DOI 10.1109/76.856450
   Louw D. J., 2013, EURASIP J APPL SIG P, V2013, P1
   Martinez JL, 2008, IEEE IMAGE PROC, P1140, DOI 10.1109/ICIP.2008.4711961
   Maugey Thomas, 2009, 2009 17th European Signal Processing Conference (EUSIPCO 2009), P559
   Maugey T, 2013, IEEE T CIRC SYST VID, V23, P2116, DOI 10.1109/TCSVT.2013.2273623
   Maugey T, 2010, INT CONF ACOUST SPEE, P2338, DOI 10.1109/ICASSP.2010.5496065
   Maugey T, 2008, J VIS COMMUN IMAGE R, V19, P589, DOI 10.1016/j.jvcir.2008.09.002
   Mourad O., 2009, EURASIP J IMAGE VIDE
   Muller Karsten, 2008, 2008 IEEE 10th Workshop on Multimedia Signal Processing (MMSP), P34, DOI 10.1109/MMSP.2008.4665045
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Ouaret M., 2006, Proc. ACM Int. Workshop on Video Surveillance and Sensor Networks, P139, DOI DOI 10.1145/1178782.1178803
   Petrazzuoli G., 2013, EURASIP J APPL SIG P, V2013, P17
   Puri R, 2007, IEEE T IMAGE PROCESS, V16, P2436, DOI 10.1109/TIP.2007.904949
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Salmistraro M, 2013, IEEE IMAGE PROC, P1699, DOI 10.1109/ICIP.2013.6738350
   Salmistraro M, 2013, INT CONF ACOUST SPEE, P1685, DOI 10.1109/ICASSP.2013.6637939
   Schwarz H, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P1, DOI 10.1109/PCS.2012.6213271
   Shao F, 2012, IEEE T MULTIMEDIA, V14, P157, DOI 10.1109/TMM.2011.2169045
   Shum HY, 2000, PROC SPIE, V4067, P2, DOI 10.1117/12.386541
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Van Droogenbroeck M, 2005, J MATH IMAGING VIS, V22, P121, DOI 10.1007/s10851-005-4886-2
   Verbist F., 2013, EURASIP J ADV SIG PR, V2013, P156, DOI [10.1186/1687-6180-2013-156, DOI 10.1186/1687-6180-2013-156]
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
   Yea S, 2009, SIGNAL PROCESS-IMAGE, V24, P89, DOI 10.1016/j.image.2008.10.007
   Yoon SU, 2007, IEEE T CIRC SYST VID, V17, P1450, DOI 10.1109/TCSVT.2007.905363
   Yuan H, 2011, IEEE T CIRC SYST VID, V21, P485, DOI 10.1109/TCSVT.2011.2125610
NR 55
TC 14
Z9 14
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1834
EP 1848
DI 10.1109/TMM.2014.2342201
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300004
DA 2024-07-18
ER

PT J
AU Sener, O
   Ugur, K
   Alatan, AA
AF Sener, Ozan
   Ugur, Kemal
   Alatan, A. Aydin
TI Efficient MRF Energy Propagation for Video Segmentation via Bilateral
   Filters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bi-exponential filters; bilateral filters; graph-cut; MRF; video
   segmentation
AB Segmentation of an object from a video is a challenging task in multimedia applications. Depending on the application, automatic or interactive methods are desired; however, regardless of the application type, efficient computation of video object segmentation is crucial for time-critical applications; specifically, mobile and interactive applications require near real-time efficiencies. In this paper, we address the problem of video segmentation from the perspective of efficiency. We initially redefine the problem of video object segmentation as the propagation of MRF energies along the temporal domain. For this purpose, a novel and efficient method is proposed to propagate MRF energies throughout the frames via bilateral filters without using any global texture, color or shape model. Recently presented bi-exponential filter is utilized for efficiency, whereas a novel technique is also developed to dynamically solve graph-cuts for varying, non-lattice graphs in general linear filtering scenario. These improvements are experimented for both automatic and interactive video segmentation scenarios. Moreover, in addition to the efficiency, segmentation quality is also tested both quantitatively and qualitatively. Indeed, for some challenging examples, significant time efficiency is observed without loss of segmentation quality.
C1 [Sener, Ozan] Cornell Univ, Elect & Comp Engn Dept, Ithaca, NY 14853 USA.
   [Sener, Ozan] Middle E Tech Univ, TR-06531 Ankara, Turkey.
   [Ugur, Kemal] Nokia Res Ctr, Tampere, Finland.
   [Alatan, A. Aydin] Middle E Tech Univ, Elect & Elect Engn Dept, TR-06531 Ankara, Turkey.
C3 Cornell University; Middle East Technical University; Nokia Corporation;
   Siemens AG; Nokia Siemens Networks; Nokia Finland; Middle East Technical
   University
RP Sener, O (corresponding author), Cornell Univ, Elect & Comp Engn Dept, Ithaca, NY 14853 USA.
RI Sener, Ozan/ABF-9436-2020; Alatan, A. Aydin/E-3927-2012
CR Achanta R., 2010, SLIC SUPERPIXELS TEC
   Alatan AA, 1998, IEEE T CIRC SYST VID, V8, P802, DOI 10.1109/76.735378
   Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], P IEEE CVPR
   [Anonymous], 1962, FLOWS NETWORKS
   AURICH V, 1995, P DAGM S
   Bai X., P SIGGRAPH 09
   Bai XF, 2007, IEEE IC COMP COM NET, P1
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov Y., P ICCV 2001
   Cigla C., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P696, DOI 10.1109/ICCVW.2011.6130315
   Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9
   Durand F, 2002, ACM T GRAPHIC, V21, P257, DOI 10.1145/566570.566574
   Elgammal A, 2003, IEEE T PATTERN ANAL, V25, P1499, DOI 10.1109/TPAMI.2003.1240123
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Geman S., 1990, STOCHASTIC RELAXATIO, P452
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P469, DOI 10.1109/TPAMI.2006.57
   Kim J, 2011, PROC CVPR IEEE, P1553, DOI 10.1109/CVPR.2011.5995526
   Kohli P, 2007, IEEE T PATTERN ANAL, V29, P2079, DOI 10.1109/TPAMI.2007.1128
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Liu J., 2009, P ACM SIGGRAPH 2009
   Lombaert H., P ICCV 05, P259
   Paris S, 2009, INT J COMPUT VISION, V81, P24, DOI 10.1007/s11263-007-0110-8
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rother Carsten, P SIGGRAPH 2004
   Sener O., 2012, P 2 ACM INT WORKSH I
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Systems A., 2010, CREAT SUIT 5 US GUID
   Thévenaz P, 2012, IEEE T IMAGE PROCESS, V21, P3924, DOI 10.1109/TIP.2012.2200903
   Tsai D., 2010, P BRIT MACH VIS C
   VAZQUEZREINA A, 2010, ECCV 10, V6315, P268, DOI DOI 10.1007/978-3-642-15555-020
   Wang J, 2005, ACM T GRAPHIC, V24, P585, DOI 10.1145/1073204.1073233
   Yatziv L., J COMPUTAT PHYS, V212
NR 36
TC 13
Z9 14
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1292
EP 1302
DI 10.1109/TMM.2014.2314069
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, WG
   Yang, M
   Li, HQ
   Wang, XY
   Lin, YQ
   Tian, Q
AF Zhou, Wengang
   Yang, Ming
   Li, Houqiang
   Wang, Xiaoyu
   Lin, Yuanqing
   Tian, Qi
TI Towards Codebook-Free: Scalable Cascaded Hashing for Mobile Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary signature; cascaded hashing; matching verification; mobile image
   search
ID NEIGHBOR; SCALE
AB State-of-the-art image retrieval algorithms using local invariant features mostly rely on a large visual codebook to accelerate the feature quantization and matching. This codebook typically contains millions of visual words, which not only demands for considerable resources to train offline but also consumes large amount of memory at the online retrieval stage. This is hardly affordable in resource limited scenarios such as mobile image search applications. To address this issue, we propose a codebook-free algorithm for large scale mobile image search. In our method, we first employ a novel scalable cascaded hashing scheme to ensure the recall rate of local feature matching. Afterwards, we enhance the matching precision by an efficient verification with the binary signatures of these local features. Consequently, our method achieves fast and accurate feature matching free of a huge visual codebook. Moreover, the quantization and binarizing functions in the proposed scheme are independent of small collections of training images and generalize well for diverse image datasets. Evaluated on two public datasets with a million distractor images, the proposed algorithm demonstrates competitive retrieval accuracy and scalability against four recent retrieval methods in literature.
C1 [Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Elect Engn & Informat Sci Dept, Hefei 230027, Peoples R China.
   [Yang, Ming; Wang, Xiaoyu; Lin, Yuanqing] NEC Labs Amer Inc, Cupertino, CA 95014 USA.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; NEC Corporation; University of Texas System; University of
   Texas at San Antonio (UTSA)
RP Zhou, WG (corresponding author), Univ Sci & Technol China, Elect Engn & Informat Sci Dept, Hefei 230027, Peoples R China.
EM zhwgeeis@gmail.com; myang@nec-labs.com; lihq@ustc.edu.cn;
   xwang@nec-labs.com; ylin@nec-labs.com; qitian@cs.utsa.edu
RI Yang, Ming-Hsuan/AAE-7350-2019; wang, xiaoyu/HJP-6901-2023; Li, Houqiang
   Li/B-6259-2013; Yang, Ming-Hsuan/T-9533-2019
OI Yang, Ming-Hsuan/0000-0003-4848-2304; Yang, Ming/0000-0003-1691-6817;
   Wang, Xiaoyu/0000-0002-6431-8822
FU Fundamental Research Funds for the Central Universities [WK2100060014];
   University of Science and Technology of China [KY2100000036]; NSFC
   [61325009, 61390514, 61272316]; ARO [W911NF-12-1-0057]; NEC Laboratories
   of America; Google; FXPAL; UTSA START-R Research Award; National Science
   Foundation of China (NSFC) [61128007]
FX This work was supported in part to Dr. Zhou by the Fundamental Research
   Funds for the Central Universities under contract No. WK2100060014 and
   the start-up funding from the University of Science and Technology of
   China under contract No. KY2100000036, in part to Dr. Li by NSFC under
   contract No. 61325009, No. 61390514, and No. 61272316, and in part to
   Dr. Tian by ARO grant W911NF-12-1-0057, Faculty Research Awards by NEC
   Laboratories of America, Google, and FXPAL, and 2012 UTSA START-R
   Research Award, respectively. This work was supported in part by
   National Science Foundation of China (NSFC) under contract No. 61128007.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yu-Gang Jiang.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], P BRIT MACH VIS C
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   ARYA S, ANN LIB APPROXIMATE
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   BENTLEY JL, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P187, DOI 10.1145/98524.98564
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2011, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2011.5995601
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He JF, 2012, PROC CVPR IEEE, P3005, DOI 10.1109/CVPR.2012.6248030
   Jain M., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1441
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kuo Yin-Hsi., 2009, ACM Multimedia, P65
   Liu Z., 2012, P ACM INT C MULTIMED, P199
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Nister David, 2006, CVPR
   Philbin J., 2008, P CVPR, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tian Q, 2011, MULTIMED TOOLS APPL, V51, P441, DOI 10.1007/s11042-010-0636-6
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang XY, 2011, IEEE I CONF COMP VIS, P209, DOI 10.1109/ICCV.2011.6126244
   Weiss Y., 2008, P ADV NEUR INF PROC, P3424
   Zhang SL, 2013, IEEE T IMAGE PROCESS, V22, P2889, DOI 10.1109/TIP.2013.2251650
   Zhang SG, 2010, PROCEEDINGS OF THE ASME JOINT RAIL CONFERENCE, VOL 2, P501, DOI 10.1145/1873951.1874018
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhou W., 2011, Proceedings of ACM Multimedia, P1349, DOI DOI 10.1145/2072298.2072012
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
   Zhou W., 2013, MULTIMEDIA SYST, P1
   Zhou WG, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2422956.2422960
   Zhou WG, 2012, IEEE T IMAGE PROCESS, V21, P4269, DOI 10.1109/TIP.2012.2199506
   Zhou WG, 2010, INT CONF ACOUST SPEE, P2394, DOI 10.1109/ICASSP.2010.5496205
NR 38
TC 57
Z9 59
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 601
EP 611
DI 10.1109/TMM.2014.2301979
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jian, M
   Jung, CL
   Zheng, YG
AF Jian, Meng
   Jung, Cheolkon
   Zheng, Yaoguo
TI Discriminative Structure Learning for Semantic Concept Detection With
   Graph Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data warping; discriminative structure; graph embedding; label
   propagation; semantic concept detection; semi-supervised learning
ID KERNEL; CLASSIFICATION; ANNOTATION
AB Semantic concept detection is a very promising way to manage huge amounts of personal contents. In this paper, we propose discriminative structure learning for semantic concept detection with graph embedding. We focus on the task of whole-image categorization and employ graphical model inference based semi-supervised learning (SSL) to detect the semantic category of an image. To effectively extract global features from images, we utilize the spatial pyramid image representation. Then, we perform data warping over the histogram intersection kernel-based graph to learn discriminative features and make image distributions more discriminative for both labeled and unlabeled images. By data warping, each cluster of images is mapped into a relatively compact cluster as well as clusters become well-separated. Moreover, we adopt low-rank representation (LRR) in the embedded space to capture the global discriminative structure from the learned features for label propagation due to its good ability of capturing the global structure of data distributions and robustness against noise and outliers. Finally, we design a smooth nonlinear detector on the captured global discriminative structure to effectively propagate the concepts of labeled images to unlabeled images. Extensive experiments are conducted on four publicly available databases to verify the superiority of the proposed method compared to the state-of-the-art methods.
C1 [Jian, Meng; Jung, Cheolkon; Zheng, Yaoguo] Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
C3 Xidian University
RP Jian, M (corresponding author), Xidian Univ, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM jianmeng648@163.com; zhengzk@xidian.edu.cn; zheng_yaoguo@163.com
FU National Basic Research Program (973 Program) of China [2013CB329402];
   National Natural Science Foundation of China [61271298, 61050110144];
   Fund for Foreign Scholars in University Research and Teaching Programs
   (the 111 Project) [B07048]; Program for Cheung Kong Scholars and
   Innovative Research Team in University [IRT1170]
FX This work was supported by the National Basic Research Program (973
   Program) of China (No. 2013CB329402), the National Natural Science
   Foundation of China (Nos. 61271298 and 61050110144), the Fund for
   Foreign Scholars in University Research and Teaching Programs (the 111
   Project) (No. B07048), and the Program for Cheung Kong Scholars and
   Innovative Research Team in University (No. IRT1170). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Selcuk Candan.
CR [Anonymous], 2008, COMPUT SCI
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], 2001, PATTERN CLASSIFICAT
   [Anonymous], 2010, INT C MACHINE LEARNI
   [Anonymous], 2007, P 15 ACM INT C MULT, DOI DOI 10.1145/1291233.1291430
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2008, MIR 08
   Belkin M, 2004, LECT NOTES COMPUT SC, V3120, P624, DOI 10.1007/978-3-540-27819-1_43
   Berg A., 2005, P IEEE C COMP VIS PA
   Bosch A, 2008, IEEE T PATTERN ANAL, V30, P712, DOI 10.1109/TPAMI.2007.70716
   Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Chapelle O., 2002, P 15 INT C NEURAL IN, P601
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   GRIFFIN G, 2006, CALTECH 256 IMAGE DA
   Hadjidemetriou E, 2004, IEEE T PATTERN ANAL, V26, P831, DOI 10.1109/TPAMI.2004.32
   He J., 2010, P ICML, P1
   Ho J., 2003, P IEEE INT C COMP VI
   Holub AD, 2005, IEEE I CONF COMP VIS, P136
   Hotta K, 2009, IEEE IMAGE PROC, P2053, DOI 10.1109/ICIP.2009.5414055
   Huang TS, 2012, IMAGE VISION COMPUT, V30, P463, DOI 10.1016/j.imavis.2011.10.001
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li Fei-Fei, 2004, P IEEE CVPR WORKSH G
   Li Z., 2007, Computer Vision, P1
   Liu N., 2012, NEUROCOMPUTING
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Serre T., 2005, P IEEE INT C COMP VI
   Szummer M., 2002, ADV NEURAL INF PROCE
   Tang JH, 2008, IEEE T MULTIMEDIA, V10, P620, DOI 10.1109/TMM.2008.921853
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Verbeek J, 2010, P ACM INT C MULT INF
   Wallraven C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P257
   Wang G., 2006, P IEEE INT C COMP VI
   Wang M, 2009, COMPUT VIS IMAGE UND, V113, P384, DOI 10.1016/j.cviu.2008.08.003
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Zhang L, 2012, IEEE T SIGNAL PROCES, V60, P1684, DOI 10.1109/TSP.2011.2179539
   Zhou DY, 2004, ADV NEUR IN, V16, P321
NR 44
TC 8
Z9 9
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 413
EP 426
DI 10.1109/TMM.2013.2291657
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800011
DA 2024-07-18
ER

PT J
AU Aran, O
   Biel, JI
   Gatica-Perez, D
AF Aran, Oya
   Biel, Joan-Isaac
   Gatica-Perez, Daniel
TI Broadcasting Oneself: Visual Discovery of Vlogging Styles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video blogs; vlogs; YouTube; nonverbal behavior; weighted motion energy
   images; pattern clustering; crowdsourcing; personality impressions
ID VIDEO; RECOGNITION; CLASSIFICATION; YOUTUBE
AB We present a data-driven approach to discover different styles that people use to present themselves in online video blogging (vlogging). By vlogging style, we denote the combination of conscious and unconscious choices that the vlogger made during the production of the vlog, affecting the video quality, appearance, and structure. A compact set of vlogging styles is discovered using clustering methods based on a fast and robust spatio-temporal descriptor to characterize the visual activity in a vlog. On 2268 YouTube vlogs, our results show that the vlogging styles are differentiated with respect to the vloggers' level of editing and conversational activity in the video. Furthermore, we show that these automatically discovered styles relate to vloggers with different personality trait impressions and to vlogs that receive different levels of social attention.
C1 [Aran, Oya; Biel, Joan-Isaac] Idiap Res Inst, Martigny, Switzerland.
   [Gatica-Perez, Daniel] Idiap Res Inst, Social Comp Grp, Martigny, Switzerland.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Aran, O (corresponding author), Idiap Res Inst, Martigny, Switzerland.
EM oya.aran@idiap.ch; jibiel@idiap.ch; gatica@idiap.ch
RI Aran, Oya/ABR-6400-2022
OI Aran, Oya/0000-0002-4679-9335
FU Swiss National Science Foundation (SNSF) Ambizione fellowship under the
   project "Multimodal Computational Modeling of Nonverbal Social Behavior
   in Face to Face Interaction" (SOBE); SNSF National Center of Competence
   in Research on Interactive Multimodal Information Management (IM2)
FX This work was supported by the Swiss National Science Foundation (SNSF)
   Ambizione fellowship under the project "Multimodal Computational
   Modeling of Nonverbal Social Behavior in Face to Face Interaction"
   (SOBE) and the SNSF National Center of Competence in Research on
   Interactive Multimodal Information Management (IM2). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   [Anonymous], 2008, P CVPR
   [Anonymous], 2000, HDB PARAMETRIC NONPA
   Barry B, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P197
   Biel J.-I., 2011, P ICWSM
   Biel J.-I., 2012, P ICWSM
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Biel JI, 2011, ACM T MULTIM COMPUT, V7, DOI 10.1145/2037676.2037690
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Brezeale D, 2008, IEEE T SYST MAN CY C, V38, P416, DOI 10.1109/TSMCC.2008.919173
   Counts S., 2009, P AAAI INT C WEBL SO
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Ding C. H. Q., 2004, P ICML
   Gao W, 2010, ACM COMPUT SURV, V42, DOI 10.1145/1749603.1749606
   Gatica-Perez D, 2003, IEEE T CIRC SYST VID, V13, P539, DOI 10.1109/TCSVT.2003.813428
   Godwin-Jones R, 2007, LANG LEARN TECHNOL, V11, P16
   Gosling S. D., 2007, P AAAI INT C WEBL SO
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Griffith M., 2007, P ANN M ASS ED JOURN
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Iyengar G, 1997, P SOC PHOTO-OPT INS, V3312, P216, DOI 10.1117/12.298454
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Knapp M.L., 2009, Nonverbal communication in human interaction, V7th
   Landry B.M., 2008, ART CIRCUS CHARACTER
   Lange P. G., 2007, J COMPUT MEDIATED CO, V1
   Lange P. G., 2007, P SOC APPL ANTHR C M
   Molyneaux H., 2008, AM COMMUN J, V10
   Ren W, 2009, PATTERN RECOGN, V42, P267, DOI 10.1016/j.patcog.2008.08.033
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2006, ACM T MULTIM COMPUT, V2, P91, DOI 10.1145/1142020.1142021
   Sundaram H, 2002, IEEE T MULTIMEDIA, V4, P482, DOI 10.1109/TMM.2002.802017
   Wang L., 2006, P 18 INT C PATT REC
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Wesch Michael., 2009, Explorations in Media Ecology, V8, P19, DOI DOI 10.1386/EME.8.2.99_1
   Wu J, 2012, IEEE T MULTIMEDIA, V14, P291, DOI 10.1109/TMM.2011.2174969
   YANG L., 2007, MIR, P265
   Yu CC, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/975291
NR 39
TC 26
Z9 30
U1 1
U2 126
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 201
EP 215
DI 10.1109/TMM.2013.2284893
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Feng, YL
   Cheung, G
   Tan, WT
   Le Callet, P
   Ji, YS
AF Feng, Yunlong
   Cheung, Gene
   Tan, Wai-tian
   Le Callet, Patrick
   Ji, Yusheng
TI Low-Cost Eye Gaze Prediction System for Interactive Networked Video
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Eye-gaze prediction; network streaming; Hidden Markov Model.
ID SALIENCY; ATTENTION; MODEL
AB Eye gaze is now used as a content adaptation trigger in interactive media applications, such as customized advertisement in video, and bit allocation in streaming video based on region-of-interest (ROI). The reaction time of a gaze-based networked system, however, is lower-bounded by the network round trip time (RTT). Furthermore, only low-sampling-rate gaze data is available when commonly available webcam is employed for gaze tracking. To realize responsive adaptation of media content even under non-negligible RTT and using common low-cost webcams, we propose a Hidden Markov Model (HMM) based gaze-prediction system that utilizes the visual saliency of the content being viewed. Specifically, our HMM has two states corresponding to two of human's intrinsic gaze behavioral movements, and its model parameters are derived offline via analysis of each video's visual saliency maps. Due to the strong prior of likely gaze locations offered by saliency information, accurate runtime gaze prediction is possible even under large RTT and using common webcam. We demonstrate the applicability of our low-cost gaze prediction system by focusing on ROI-based bit allocation for networked video streaming. To reduce transmission rate of a video stream without degrading viewer's perceived visual quality, we allocate more bits to encode the viewer's current spatial ROI, while devoting fewer bits in other spatial regions. The challenge lies in overcoming the delay between the time a viewer's ROI is detected by gaze tracking, to the time the effected video is encoded, delivered and displayed at the viewer's terminal. To this end, we use our proposed low-cost gaze prediction system to predict future eye gaze locations, so that optimized bit allocation can be performed for future frames. Through extensive subjective testing, we show that bit-rate can be reduced by up to 29% without noticeable visual quality degradation when RTT is as high as 200 ms.
C1 [Feng, Yunlong] Grad Univ Adv Studies SOKENDAI, Tokyo, Japan.
   [Feng, Yunlong; Cheung, Gene; Ji, Yusheng] Natl Inst Informat, Tokyo 1018430, Japan.
   [Tan, Wai-tian] Hewlett Packard Labs, Palo Alto, CA 94304 USA.
   [Le Callet, Patrick] Univ Nantes, Polytech Nantes, F-44306 Nantes 3, France.
C3 Graduate University for Advanced Studies - Japan; Research Organization
   of Information & Systems (ROIS); National Institute of Informatics (NII)
   - Japan; Hewlett-Packard; Nantes Universite
RP Feng, YL (corresponding author), Grad Univ Adv Studies SOKENDAI, Tokyo, Japan.
EM fengyl@nii.ac.jp; cheung@nii.ac.jp; wai-tian.tan@hp.com;
   patrick.lecallet@univ-nantes.fr; kei@nii.ac.jp
RI Cheung, Gene/AAB-9284-2020; Ji, Yusheng/AAF-1537-2020; Le Callet,
   Patrick/F-5772-2010
OI Ji, Yusheng/0000-0003-4364-8491; Cheung, Gene/0000-0002-5571-4137
FU Grants-in-Aid for Scientific Research [23700136] Funding Source: KAKEN
CR [Anonymous], 1998, H263 ITUT
   [Anonymous], 2007, NEURAL INF PROCESS S
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Bruce N., 2009, P IEEE INT C IM PROC
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Bur A, 2007, LECT NOTES COMPUT SC, V4528, P109
   Chen J., 2011, P IEEE INT C COMP VI
   Chen Z., 2009, P IEEE INT C IM PROC
   Davies S., 2008, P IEEE INT C IM PROC
   Doulamis N, 1998, IEEE T CIRC SYST VID, V8, P928, DOI 10.1109/76.736718
   Duchowski A. T., 2017, EYE TRACKING METHODO, DOI [10.1007/978-3-319-57883-5, DOI 10.1007/978-3-319-57883-5]
   Duchowski AT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314309
   Faragher R, 2012, IEEE SIGNAL PROC MAG, V29, P128, DOI 10.1109/MSP.2012.2203621
   Feng Y., 2012, P IS T SPIE VIS INF
   Feng YL, 2011, IEEE INT CON MULTI
   Floyd S., 1999, IEEE ACM T NETW
   Floyd S., 2000, P ACM SIGCOMM STOCKH
   Gao D, 2008, J VISION, V8, DOI 10.1167/8.7.13
   Geisler W., 1998, SPIE P, V3299
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L., 2006, ADV NEURAL INFORM PR, P802
   ITU-R, 1998, REC BT 500 8 METH SU
   JAYANT N, 1993, P IEEE, V81, P1385, DOI 10.1109/5.241504
   Komogortsev O., 2007, P ACM MULT COMP NETW
   Komogortsev O., 2008, P EYE TRACK RES APPL
   Komogortsev O. V., 2009, J CONTROL THEORY APP, V7
   LC Technologies Inc., EYEG SYST
   Le Meur O, 2007, VISION RES, V47, P2483, DOI 10.1016/j.visres.2007.06.015
   Leigh RJ., 2006, NEUROLOGY EYE MOVEME, V4th
   Li XP, 2010, IEEE INT C INT ROBOT
   Liu Y, 2008, IEEE T CIRC SYST VID, V18, P134, DOI 10.1109/TCSVT.2007.913754
   Loschky LC, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1314303.1314310
   Meur O. L., 2009, P IEEE INT C IM PROC
   Meur O L, 2006, IEEE T PATTERN ANAL, V28, P802
   Oliva A., 2003, P IEEE INT C IM PROC
   Peters R., 2006, P S EYE TRACK RES AP
   Reale M., 2011, P IEEE INT C COMP VI
   Reale M., 2010, P IEEE INT C MULT EX
   Sheskin DJ., 2003, HDB PARAMETRIC NONPA, DOI [10.1201/9781420036268, DOI 10.1201/9781420036268]
   Sippl A, 2010, LECT NOTES COMPUT SC, V6439, P167, DOI 10.1007/978-3-642-16917-5_17
   Sugano Y., 2010, P IEEE INT C COMP VI
   Sugano Y, 2008, LECT NOTES COMPUT SC, V5304, P656, DOI 10.1007/978-3-540-88690-7_49
   TAYLOR MM, 1967, J ACOUST SOC AM, V41, P782, DOI 10.1121/1.1910407
   Tobii Technology AB, EYE TACK EYE CONTR R
   Valenti R, 2012, INT J COMPUT VISION, V98, P324, DOI 10.1007/s11263-011-0511-6
   Venkatesh M. V., 2010, P IEEE INT C IM PROC
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
NR 49
TC 17
Z9 19
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1865
EP 1879
DI 10.1109/TMM.2013.2272918
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900012
DA 2024-07-18
ER

PT J
AU Wang, XW
   Zhang, T
   Tretter, DR
   Lin, Q
AF Wang, Xianwang
   Zhang, Tong
   Tretter, Daniel R.
   Lin, Qian
TI Personal Clothing Retrieval on Photo Collections by Color and Attributes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute learning; clothing retrieval; color matching; reranking.
AB Automatic personal clothing retrieval on photo collections, i.e., searching the same clothes worn by the same person, is not a trivial problem as photos are usually taken under completely uncontrolled realistic imaging conditions. Typically, the captured clothing images have large variations due to geometric deformation, occlusion, cluttered background, and photometric variability from illumination and viewpoint, which pose significant challenges to text-based or reranking-based visual search methods. In this paper, a novel framework is presented to tackle these issues by leveraging low-level features (e. g., color) and high-level features (attributes) of clothing. First, a content-based image retrieval (CBIR) approach based on the bag-of-visual-words (BOW) model is developed as our baseline system, in which a codebook is constructed from extracted dominant color patches. A reranking approach is then proposed to improve search quality by exploiting clothing attributes, including the type of clothing, sleeves, patterns, etc. Compared to low-level features, the attributes have better robustness to clothing variations, and carry semantic meanings as high-level image representations. Different visual attribute detectors are learned from large amounts of training data to extract the corresponding attributes. The construction of codebook and building of attribute classifiers are conducted offline, which leads to fast online search performance. Extensive experiments on photo collections show that the reranking algorithm based on attribute learning significantly improves retrieval performance in combination with the proposed baseline. Even our color-based baseline alone outperforms the previous CBIR-based search approaches. The experiments also demonstrate that our approach is robust to large variations of images taken in unconstrained environment.
C1 [Wang, Xianwang; Zhang, Tong; Tretter, Daniel R.; Lin, Qian] Hewlett Packard Corp, 3500 Deer Creek Rd, Palo Alto, CA 94304 USA.
C3 Hewlett-Packard
RP Wang, XW (corresponding author), Hewlett Packard Corp, 3500 Deer Creek Rd, Palo Alto, CA 94304 USA.
EM xianwang.wang@hp.com; Tong.Zhang@hp.com; dan.tretterm@hp.com;
   qian.lin@hp.com
CR Ames M, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P971
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2010, P ACM INT C MULT
   [Anonymous], 2006, P CVPR, DOI 10.1109/CVPR.2006.81
   Chen Y., 2010, P ACM INT C MULT, P221
   Chin-Hsien Tseng, 2009, WSEAS Transactions on Computers, V8, P1195
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772
   Ferrari V., 2007, P NIPS
   Gallagher AC, 2008, PROC CVPR IEEE, P1073
   Hsu W. H., 2006, MULTIMEDIA '06, P35
   Hsu WinstonH., 2007, ACM MM
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Jones M., 2003, P CVPR
   Kennedy Lyndon., 2007, Proceedings of the 15th International Conference on Multimedia, P631, DOI DOI 10.1145/1291233.1291384
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Liu Jingjing., 2007, MULTIMEDIA 07, P208
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu Y, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P297, DOI 10.1109/ICME.2008.4607430
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Olga R., 2010, P ECCV
   Rohrbach M., 2010, P CVPR
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Schroff F., 2007, P ICCV
   Song Y, 2006, LECT NOTES COMPUT SC, V3953, P382, DOI 10.1007/11744078_30
   Sugiyama M, 2010, IEICE T INF SYST, VE93D, P2690, DOI 10.1587/transinf.E93.D.2690
   Tian X., 2008, ACM INT C MULTIMEDIA, P131, DOI DOI 10.1145/1459359.1459378.ISBN
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   van de Weijer J, 2007, PROC CVPR IEEE, P1898
   van de Weijer J, 2006, LECT NOTES COMPUT SC, V3952, P334
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang L., 2009, P ACM INT C MULT, P725
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   Yanai K., 2010, P ACM INT C MULT, P419
   Yang YH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P285, DOI [10.1109/ICNNSP.2008.4590357, 10.1109/ICME.2008.4607427]
   Yogarajah P, 2010, IEEE IMAGE PROC, P2225, DOI 10.1109/ICIP.2010.5652798
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zhou W., 2010, PROC INT CONF IMAGE, P205
   Zitouni H., 2008, 19th International Conference on Pattern Recognition, P1
NR 42
TC 41
Z9 46
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2035
EP 2045
DI 10.1109/TMM.2013.2279658
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900025
DA 2024-07-18
ER

PT J
AU Li, NX
   Jain, JJ
   Busso, C
AF Li, Nanxiang
   Jain, Jinesh J.
   Busso, Carlos
TI Modeling of Driver Behavior in Real World Scenarios Using Multiple
   Noninvasive Sensors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Driver behavior; Gaussian mixture models; multimodal feature analysis;
   subjective evaluation of distraction
ID COGNITIVE DISTRACTION; SYSTEM
AB With the development of new in-vehicle technology, drivers are exposed to more sources of distraction, which can lead to an unintentional accident. Monitoring the driver attention level has become a relevant research problem. This is the precise aim of this study. A database containing 20 drivers was collected in real-driving scenarios. The drivers were asked to perform common secondary tasks such as operating the radio, phone and a navigation system. The collected database comprises of various noninvasive sensors including the controller area network-bus (CAN-Bus), video cameras and microphone arrays. The study analyzes the effects in driver behaviors induced by secondary tasks. The corpus is analyzed to identify multimodal features that can be used to discriminate between normal and task driving conditions. Separate binary classifiers are trained to distinguish between normal and each of the secondary tasks, achieving an average accuracy of 77.2%. When a joint, multi-class classifier is trained, the system achieved accuracies of 40.8%, which is significantly higher than chances (12.5%). We observed that the classifiers' accuracy varies across secondary tasks, suggesting that certain tasks are more distracting than others. Motivated by these results, the study builds statistical models in the form of Gaussian Mixture Models (GMMs) to quantify the actual deviations in driver behaviors from the expected normal driving patterns. The study includes task independent and task dependent models. Building upon these results, a regression model is proposed to obtain a metric that characterizes the attention level of the driver. This metric can be used to signal alarms, preventing collision and improving the overall driving experience.
C1 [Li, Nanxiang] Erik Jonsson Sch Engn & Comp Sci, Dept Elect Engn, Dallas, TX 75252 USA.
   [Busso, Carlos] Univ Texas Dallas, Dept Elect Engn, Richardson, TX 75080 USA.
   [Jain, Jinesh J.] Belkin Int Inc, Playa Vista, CA 90094 USA.
   [Jain, Jinesh J.] Belkin Int Inc, Culver City, CA 90230 USA.
C3 University of Texas System; University of Texas Dallas
RP Li, NX (corresponding author), Erik Jonsson Sch Engn & Comp Sci, Dept Elect Engn, Dallas, TX 75252 USA.
EM nxl056000@utdallas.edu; jinesh25@gmail.com; busso@utdallas.edu
OI Busso, Carlos/0000-0002-4075-4072
CR Abut H, 2009, IN-VEHICLE CORPUS AND SIGNAL PROCESSING FOR DRIVER BEHAVIOR, P23, DOI 10.1007/978-0-387-79582-9_3
   Ahlstrom C., 2010, P INT C METH TECHN B, P2
   Angkititrakul P, 2007, 2007 IEEE INTELLIGENT VEHICLES SYMPOSIUM, VOLS 1-3, P954
   [Anonymous], 2006, IMPACT DRIVER INATTE
   [Anonymous], 2007, INTERSPEECH 2007
   [Anonymous], P INTERSPEECH FLOR I
   [Anonymous], 2005, PROC INT TECH C ENHA
   [Anonymous], P IEEE INT C AUT FAC
   [Anonymous], 2006, P 28 INT C SOFTW ENG
   [Anonymous], P 23 BRIT HCI GROUP
   Azman A., 2010, P INT C ADV COMP THE, V3
   Bartlett M. S., 2006, Journal of Multimedia, V1, DOI 10.4304/jmm.1.6.22-35
   Bergasa LM, 2006, IEEE T INTELL TRANSP, V7, P63, DOI 10.1109/TITS.2006.869598
   Berka C, 2007, AVIAT SPACE ENVIR MD, V78, pB231
   Busso C, 2012, DIGITAL SIGNAL PROCESSING FOR IN-VEHICLE SYSTEMS AND SAFETY, P253, DOI 10.1007/978-1-4419-9607-7_18
   Caffier PP, 2003, EUR J APPL PHYSIOL, V89, P319, DOI 10.1007/s00421-003-0807-5
   Tran C, 2012, COMPUT VIS IMAGE UND, V116, P435, DOI 10.1016/j.cviu.2011.09.008
   Damousis LG, 2008, IEEE T INTELL TRANSP, V9, P491, DOI 10.1109/TITS.2008.928241
   Dong YC, 2009, IEEE INT VEH SYM, P875, DOI 10.1109/IVS.2009.5164395
   Engström J, 2005, TRANSPORT RES F-TRAF, V8, P97, DOI 10.1016/j.trf.2005.04.012
   Ersal T, 2010, IEEE T INTELL TRANSP, V11, P692, DOI 10.1109/TITS.2010.2049741
   Giusti A, 2009, IEEE T INTELL TRANSP, V10, P127, DOI 10.1109/TITS.2008.2011707
   Green P., 1999, P INT TRANSP SOC ITS
   Harbluk JL, 2007, ACCIDENT ANAL PREV, V39, P372, DOI 10.1016/j.aap.2006.08.013
   Jain J., 2011, P 5 BIENN WORKSH DSP
   Jain J., 2011, P IEEE INT C MULT EX
   Kutila MH, 2007, P I MECH ENG D-J AUT, V221, P1027, DOI 10.1243/09544070JAUTO332
   Kutila M, 2007, IEEE IMAGE PROC, P2997
   Liang YL, 2007, IEEE T INTELL TRANSP, V8, P340, DOI 10.1109/TITS.2007.895298
   Lin CT, 2005, IEEE T CIRCUITS-I, V52, P2726, DOI 10.1109/TCSI.2005.857555
   McCall JC, 2007, P IEEE, V95, P374, DOI [10.1109/JPROC.2006.888388, 10.1109/JPROC.2007.888388]
   Mendenhall William., 2006, STAT ENG SCI, V5th
   Murphy-Chutorian E, 2010, IEEE T INTELL TRANSP, V11, P300, DOI 10.1109/TITS.2010.2044241
   Pérez A, 2010, IEEE T INTELL TRANSP, V11, P463, DOI 10.1109/TITS.2010.2046323
   Putze F., 2010, P INT C PATT REC ICP
   Ranney T. A., 2001, NHTSA DRIVER DISTRAC
   Ranney TA, 2008, 810787 DOT HS NAT HI
   Sathyanarayana A., 2010, P IEEE INT VEH S IV
   Sathyanarayana A., 2008, P IEEE INT C VEH EL
   Su MC, 2006, IEEE SYS MAN CYBERN, P429, DOI 10.1109/ICSMC.2006.384420
   Takeda K, 2011, IEEE T INTELL TRANSP, V12, P1609, DOI 10.1109/TITS.2011.2167680
   Tango Fabio, 2009, Advances in Data Mining, Applications and Theoretical Aspects. Proceedings 9th Industrial Confer, ICDM 2009, P176, DOI 10.1007/978-3-642-03067-3_15
   Trezise I., 2006, ROAD SAF COMM INQ DR
   Trivedi MM, 2007, IEEE T INTELL TRANSP, V8, P108, DOI 10.1109/TITS.2006.889442
   Victor T., 2008, Driver Distraction, P135, DOI DOI 10.1201/9781420007497CH10
   Vural Esra, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3874, DOI 10.1109/ICPR.2010.943
   Vurall E, 2007, LECT NOTES COMPUT SC, V4796, P6
   Wu Q., 2009, P IEEE 10 INT C COMP
   Yang J., 2010, P ANN C IEEE IND EL
   Zhu ZW, 2004, ITSC 2004: 7TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, PROCEEDINGS, P657, DOI 10.1109/ITSC.2004.1398979
NR 50
TC 63
Z9 68
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1213
EP 1225
DI 10.1109/TMM.2013.2241416
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600022
DA 2024-07-18
ER

PT J
AU Ni, BB
   Xu, MD
   Cheng, B
   Wang, M
   Yan, SC
   Tian, Q
AF Ni, Bingbing
   Xu, Mengdi
   Cheng, Bin
   Wang, Meng
   Yan, Shuicheng
   Tian, Qi
TI Learning to Photograph: A Compositional Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative model; maximum a posteriori; photo composition; spatial
   context; view recommendation
ID IMAGE; HISTOGRAMS
AB In this paper, we present an intelligent photography system which can recommend the most user-favored view rectangle for arbitrary camera input, from a photographic compositional perspective. Automating this process is difficult, due to the subjectivity of human's aesthetics judgement and large variations of image contents, where heuristic compositional rules lack generality. Motivated by the recent prevalence of photo-sharing websites, e. g., Flickr.com, we develop a learning-based framework which discovers the underlying aesthetic photographic compositional structures from a large set of user-favored online sharing photographs and utilizes the implicitly shared knowledge among the professional photographers for aesthetically optimal view recommendation. In particular, we propose an Omni-Range Context method which explicitly encodes the spatial and geometric distributions of various visual elements in the photograph as well as cooccurrence characteristics of visual element pairs by using generative mixture models. Searching the optimal view rectangle is then formulated as maximum a posterior by imposing the trained prior distributions along with additional photographic constraints. The proposed system has the potential to operate in near real-time. Comprehensive user studies well demonstrate the effectiveness of the proposed framework for aesthetically optimal view recommendation.
C1 [Ni, Bingbing] Adv Digital Sci Ctr, Singapore 138632, Singapore.
   [Xu, Mengdi; Cheng, Bin; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Wang, Meng] Hefei Univ Technol, Hefei, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 National University of Singapore; Hefei University of Technology;
   University of Texas System; University of Texas at San Antonio (UTSA)
RP Ni, BB (corresponding author), Adv Digital Sci Ctr, Singapore 138632, Singapore.
EM bingbing.ni@adsc.com.sg; g0900224@nus.edu.sg; g0800415@nus.edu.sg;
   eric.mengwang@gmail.com; eleyans@nus.edu.sg; qitian@cs.utsa.edu
RI Wang, Meng/ITR-8699-2023; Yan, Shuicheng/HCI-1431-2022; Xu,
   Meng/HJY-7139-2023
FU Advanced Digital Sciences Center from Singapore's Agency for Science,
   Technology and Research (A*STAR); National Basic Research Program of
   China (973 Program) [2013CB329604]; National Natural Science Foundation
   of China [61272393]; ARO [W911BF-12-1-0057]; National Science Foundation
   [NSF IIS 1052851]; Google; FXPAL; NEC Laboratories of America
FX This work was supported in part by a research grant for the Human Sixth
   Sense Programme at the Advanced Digital Sciences Center from Singapore's
   Agency for Science, Technology and Research (A*STAR), the National Basic
   Research Program of China (973 Program) under Grant 2013CB329604, and
   the National Natural Science Foundation of China under Grant 61272393.
   The work of Q. Tian was supported in party by the ARO under Grant
   W911BF-12-1-0057, the National Science Foundation under Grnat NSF IIS
   1052851, and Faculty Research Awards by Google, FXPAL, and NEC
   Laboratories of America, respectively. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Weisi Lin.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2009, P INT C COMP VIS
   [Anonymous], 2004, P ACM C INT US INT
   [Anonymous], 2009, P 17 ACM INT C MULT
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Birchfield ST, 2005, PROC CVPR IEEE, P1158
   Byers Z., 2003, Proceedings of the Fifteenth Innovative Applications of Artificial Intelligence Conference, P65
   Chang Y, 2007, IEEE T IMAGE PROCESS, V16, P329, DOI 10.1109/TIP.2006.888347
   Cheng B., 2009, P ACM INT C MULT, P291
   Cho T. S., 2008, P INT C COMP VIS PAT
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Feng J., 2011, P INT C COMP VIS PAT
   Gilks, 1996, MARKOV CHAIN MONTE C, V1, P19, DOI DOI 10.1201/B14835
   Gooch B, 2001, SPRING EUROGRAP, P83
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Hao Q., 2010, P 19 INT C WORLD WID, P401, DOI [DOI 10.1145/1772690.1772732, 10.1145/1772690.1772732]
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hou X, 2007, P INT C COMP VIS PAT
   Ji Rongrong., 2009, P 17 ASS COMP MACH I, P105, DOI DOI 10.1145/1631272.1631289
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   King BruceM., 2003, STAT REASONING PSYCH, VFourth
   Kowalski MichaelA., 2001, Proceedings of the 2001 symposium on Interactive 3D graphics, P99
   Li J., 2008, P INT C COMP VIS PAT
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Luo W., 2011, P INT C COMP VIS
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Marchesotti L., 2011, P INT C COMP VIS
   Martinez B., 1998, VISUAL FORCES INTRO
   Ni B., 2009, P INT C COMP VIS PAT
   Ni B., 2009, Proceedings of the 17th ACM international conference on Multimedia, P85
   Ni BB, 2011, IEEE T MULTIMEDIA, V13, P1217, DOI 10.1109/TMM.2011.2167317
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   Setlur Vidya., 2005, MUM, V154, P59, DOI DOI 10.1145/1149488.1149499
   Shapiro L.G., 2003, Computer Vision, Vsecond
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P2117, DOI 10.1109/TIP.2005.859389
   Simakov D., 2008, P INT C COMP VIS PAT
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang Y., 2007, P INT C COMP VIS PAT, P1968
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L., 2007, P INT C COMP VIS
   Xiaoyan Zhang, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P414, DOI 10.1109/PSIVT.2010.76
   Zhang M., 2005, P INT C MULT EXP
   Zheng Y., 2008, P INT C COMP VIS PAT
   ZHENG YT, 2009, P INT C COMP VIS PAT
NR 53
TC 42
Z9 45
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1138
EP 1151
DI 10.1109/TMM.2013.2241042
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600016
DA 2024-07-18
ER

PT J
AU Wu, CY
   Su, PC
AF Wu, Ching-Yu
   Su, Po-Chyi
TI Fast Intra-Coding for H.264/AVC by Using Projection-Based Predicted
   Block Residuals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; intra prediction; mode decision
ID MODE DECISION ALGORITHM
AB An efficient intra-prediction mode decision mechanism for H.264/AVC is presented in this research. A projection-based approach, which employs the reconstructed surrounding pixels and block content to compute the predicted block residuals (PBR), can effectively eliminate less probable modes from the computation of Rate Distortion Optimization. Both the projected vectors and corresponding predictors can be formed by shifting/adding the related data so the proposed scheme is suitable for hardware implementation. According to the PBR and coding information acquired during the encoding process, some prediction modes can also be skipped to further accelerate the intra coding. The experimental results show that the proposed scheme can effectively reduce the encoding time with slight video quality degradation and bit-rate increment.
C1 [Wu, Ching-Yu; Su, Po-Chyi] Natl Cent Univ, Dept Comp Sci & Informat Engn, Jhongli 32001, Taiwan.
C3 National Central University
RP Wu, CY (corresponding author), Natl Cent Univ, Dept Comp Sci & Informat Engn, Jhongli 32001, Taiwan.
EM 985402014@cc.ncu.edu.tw; pochyisu@csie.ncu.edu.tw
RI SU, PO-CHYI/GWC-9682-2022
OI Su, Po-Chyi/0000-0002-7457-8409
FU National Science Council in Taiwan [NSC100-2221-E-008-120]
FX This work was supported by the National Science Council in Taiwan, under
   Grant NSC100-2221-E-008-120. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Beatrice Pesquet-Popescu.
CR [Anonymous], P SPIE C APPL DIG IM
   Bharanitharan K, 2008, IEEE T MULTIMEDIA, V10, P1250, DOI 10.1109/TMM.2008.2004904
   Bjontegaard G., 2001, ITU T VCEG M MAR
   Huang YH, 2010, IEEE T CIRC SYST VID, V20, P1122, DOI 10.1109/TCSVT.2010.2057018
   International Telecommunications Union, 2010, 1449610 ISOIEC INT T
   Joint Video Team (JVT) of ISO/IEC MPEG & ITU-T VCEG, H 264 AVC REF SOFTW
   Lee YM, 2010, IEEE T CIRC SYST VID, V20, P463, DOI 10.1109/TCSVT.2009.2035853
   Milani S, 2011, IEEE T IMAGE PROCESS, V20, P121, DOI 10.1109/TIP.2010.2055572
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Tan T.-K., 2007, ITU T VID COD EXP GR
   Tsai AC, 2008, IEEE T CIRC SYST VID, V18, P694, DOI 10.1109/TCSVT.2008.919113
   Tseng CH, 2006, IEEE T CIRC SYST VID, V16, P1027, DOI 10.1109/TCSVT.2006.878146
   Wang JC, 2007, IEEE T CIRC SYST VID, V17, P1414, DOI 10.1109/TCSVT.2007.903786
   Wei ZY, 2008, SIGNAL PROCESS-IMAGE, V23, P699, DOI 10.1016/j.image.2008.08.002
   Zeng HQ, 2010, IEEE T CIRC SYST VID, V20, P907, DOI 10.1109/TCSVT.2010.2045802
NR 15
TC 9
Z9 10
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1083
EP 1093
DI 10.1109/TMM.2013.2247033
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600012
DA 2024-07-18
ER

PT J
AU Huang, Z
   Liu, JJ
   Cui, B
   Du, XY
AF Huang, Zi
   Liu, Jiajun
   Cui, Bin
   Du, Xiaoyong
TI A Gram-Based String Paradigm for Efficient Video Subsequence Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High-dimensional indexing; sequence indexing; similarity search; video
   subsequence search
ID NEAREST-NEIGHBOR; RETRIEVAL
AB The unprecedented increase in the generation and dissemination of video data has created an urgent demand for the large-scale video content management system to quickly retrieve videos of users' interests. Traditionally, video sequence data are managed by high-dimensional indexing structures, most of which suffer from the well-known "curse of dimensionality" and lack of support of subsequence retrieval. Inspired by the high efficiency of string indexing methods, in this paper, we present a string paradigm called VideoGram for large-scale video sequence indexing to achieve fast similarity search. In VideoGram, the feature space is modeled as a set of visual words. Each database video sequence is mapped into a string. A gram-based indexing structure is then built to tackle the effect of the "curse of dimensionality" and support video subsequence matching. Given a high-dimensional query video sequence, retrieval is performed by transforming the query into a string and then searching the matched strings from the index structure. By doing so, expensive high-dimensional similarity computations can be completely avoided. An efficient sequence search algorithm with upper bound pruning power is also presented. We conduct an extensive performance study on real-life video collections to validate the novelties of our proposal.
C1 [Huang, Zi; Liu, Jiajun] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
   [Cui, Bin] Peking Univ, Dept Comp Sci & Technol, Beijing 100871, Peoples R China.
   [Du, Xiaoyong] Renmin Univ China, Sch Informat, Beijing 100871, Peoples R China.
C3 University of Queensland; Peking University; Renmin University of China
RP Huang, Z (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM bincui@gmail.com; duyong@ruc.edu.cn
RI liu, jia/JAC-7852-2023; liu, jia/HKE-9796-2023; Yu, Kun/IAP-9807-2023;
   Cui, Bin/A-4554-2012; liu, jiajia/IUN-0901-2023; liu,
   jiajia/ISS-0316-2023; liu, jiayu/JCP-0511-2023; li,
   jiawei/HOA-5023-2023; Liu, Jiayu/JCO-5073-2023; Li, Jiawei/GXM-4151-2022
OI HUANG, ZI/0000-0002-9738-4949; CUI, Bin/0000-0003-1681-4677
FU Natural Science Foundation of China [61073019, 60933004]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61073019 and 60933004. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Francesco G. B. De Natale.
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], P ACM SIGMOD INT C M
   [Anonymous], 2007, P 23 INT C DATA ENG
   [Anonymous], 2000, P 26 INT C VER LARG
   [Anonymous], 2008, P 7 ACM INT C IMAGE
   [Anonymous], 2008, P 16 ACM INT C MULT, DOI DOI 10.1145/1459359.1459410
   Ballan L, 2009, LECT NOTES COMPUT SC, V5716, P170, DOI 10.1007/978-3-642-04146-4_20
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Chen L., 2005, P 2005 ACM SIGMOD IN, P491
   Cheung SCS, 2003, IEEE T CIRC SYST VID, V13, P59, DOI 10.1109/TCSVT.2002.808080
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Gibbon D.C., 2008, INTRO VIDEO SEARCH E
   Houle ME, 2005, PROC INT CONF DATA, P619
   Huang Z, 2009, ACM T INFORM SYST, V27, DOI 10.1145/1508850.1508855
   Jagadish HV, 2005, ACM T DATABASE SYST, V30, P364, DOI 10.1145/1071610.1071612
   Jensen ChristianS., 2004, Proceedings of Very Large Databases (VLDB), P768, DOI DOI 10.1016/B978-012088469-8/50068-1
   Jiang YG, 2009, COMPUT VIS IMAGE UND, V113, P405, DOI 10.1016/j.cviu.2008.10.002
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Li C., 2007, VLDB, P303
   Lin J, 2007, DATA MIN KNOWL DISC, V15, P107, DOI 10.1007/s10618-007-0064-z
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   Rasetic Slobodan., 2005, Proc. of the 31st Intl. Conf. on Very Large Data Bases, P934
   Shen H. T., 2007, P ACM MULT C, P164
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Shen HT, 2007, VLDB J, V16, P219, DOI 10.1007/s00778-005-0167-3
   Shen HT, 2009, IEEE T KNOWL DATA EN, V21, P321, DOI 10.1109/TKDE.2008.168
   Shieh J., 2008, P 14 ACM SIGKDD INT, P623, DOI [10.1145/1401890.1401966, DOI 10.1145/1401890.1401966]
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Valle Eduardo., 2008, Proceeding of the 17th ACM conference on Information and knowledge management, P739, DOI DOI 10.1145/1458082.1458181
   Wang F., 2008, ACM MULTIMEDIA
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Wu A. G., 2007, P ACM MM, P218
   Xiaoyong Liu, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186
   Xu SH, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P83
   Yan Y, 2008, PROC INT CONF DATA, P853, DOI 10.1109/ICDE.2008.4497494
   Yeh MC, 2011, IEEE T MULTIMEDIA, V13, P320, DOI 10.1109/TMM.2010.2094999
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhu XQ, 2004, MULTIMEDIA SYST, V10, P98, DOI 10.1007/s00530-004-0142-7
NR 39
TC 1
Z9 2
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 608
EP 620
DI 10.1109/TMM.2012.2236307
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900012
DA 2024-07-18
ER

PT J
AU Valero, X
   Alías, F
AF Valero, Xavier
   Alias, Francesc
TI Gammatone Cepstral Coefficients: Biologically Inspired Features for
   Non-Speech Audio Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio classification; audio scene recognition; environmental sound;
   feature extraction; Gammatone cepstral coefficients
ID ENVIRONMENTAL SOUND RECOGNITION; FREQUENCY
AB In the context of non-speech audio recognition and classification for multimedia applications, it becomes essential to have a set of features able to accurately represent and discriminate among audio signals. Mel frequency cepstral coefficients (MFCC) have become a de facto standard for audio parameterization. Taking as a basis the MFCC computation scheme, the Gammatone cepstral coefficients (GTCCs) are a biologically inspired modification employing Gammatone filters with equivalent rectangular bandwidth bands. In this letter, the GTCCs, which have been previously employed in the field of speech research, are adapted for non-speech audio classification purposes. Their performance is evaluated on two audio corpora of 4 h each (general sounds and audio scenes), following two cross-validation schemes and four machine learning methods. According to the results, classification accuracies are significantly higher when employing GTCC rather than other state-of-the-art audio features. As a detailed analysis shows, with a similar computational cost, the GTCC are more effective than MFCC in representing the spectral characteristics of non-speech audio signals, especially at low frequencies.
C1 [Valero, Xavier; Alias, Francesc] La Salle Univ Ramon Llull, GTM Grp Recerca Tecnol Media, Barcelona 08022, Spain.
C3 Universitat Ramon Llull
RP Valero, X (corresponding author), La Salle Univ Ramon Llull, GTM Grp Recerca Tecnol Media, Barcelona 08022, Spain.
EM xvalero@salle.url.edu; falias@salle.url.edu
RI Alías-Pujol, Francesc/L-1088-2014
OI Alías-Pujol, Francesc/0000-0002-1921-2375
CR Abdulla W.H., 2002, Advances in Communications and Software Technologies, P231
   [Anonymous], 2007, P ICASSP
   [Anonymous], 2341 APU
   Besacier L, 2000, SIGNAL PROCESS, V80, P1245, DOI 10.1016/S0165-1684(00)00033-5
   Cai R, 2008, IEEE T MULTIMEDIA, V10, P596, DOI 10.1109/TMM.2008.921739
   CARNEY LH, 1988, J NEUROPHYSIOL, V60, P1653, DOI 10.1152/jn.1988.60.5.1653
   Cheng O., 2005, P ISSPA
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   Cristani M, 2007, IEEE T MULTIMEDIA, V9, P257, DOI 10.1109/TMM.2006.886263
   Eronen AJ, 2006, IEEE T AUDIO SPEECH, V14, P321, DOI 10.1109/TSA.2005.854103
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T
   GREENWOOD DD, 1990, J ACOUST SOC AM, V87, P2592, DOI 10.1121/1.399052
   Gygi B, 2001, Ph.D. thesis
   Kim HG, 2005, MPEG-7 AUDIO AND BEYOND: AUDIO CONTENT INDEXING AND RETRIEVAL, P1, DOI 10.1002/0470093366
   Patterson R. D., 1996, ADV SPEECH HEARING B, V3, P554
   Rabaoui A, 2007, P 4 INT MULT SYST SI
   Rabaoui A., 2008, IEEE T INF FORENSICS, V3
   Shao Y., 2008, P ICASSP
   Shao Y., 2009, P ICASSP
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   Slaney M., 1993, 35 APPL COMP LIB
   Slaney M., 1988, 13 APPL COMP CORP LI
   Umapathy K, 2005, IEEE T MULTIMEDIA, V7, P308, DOI 10.1109/TMM.2005.843363
   Valero X., 2010, P 1 EUR C, P1
NR 26
TC 171
Z9 184
U1 3
U2 49
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2012
VL 14
IS 6
BP 1684
EP 1689
DI 10.1109/TMM.2012.2199972
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046YE
UT WOS:000311800400017
DA 2024-07-18
ER

PT J
AU Shen, CC
   Wu, SP
   Sane, N
   Wu, HH
   Plishker, W
   Bhattacharyya, SS
AF Shen, Chung-Ching
   Wu, Shenpei
   Sane, Nimish
   Wu, Hsiang-Huang
   Plishker, William
   Bhattacharyya, Shuvra S.
TI Design and Synthesis for Multimedia Systems Using the Targeted Dataflow
   Interchange Format
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Dataflow graphs; design tools; embedded signal processing; scheduling;
   software synthesis
ID MODEL
AB Development of multimedia systems that can be targeted to different platforms is challenging due to the need for rigorous integration between high-level abstract modeling, and low-level synthesis and optimization. In this paper, a new dataflow-based design tool called the targeted dataflow interchange format is introduced for retargetable design, analysis, and implementation of embedded software for multimedia systems. Our approach provides novel capabilities, based on principles of task-level dataflow analysis, for exploring and optimizing interactions across design components; object-oriented data structures for encapsulating contextual information for components; a novel model for representing parameterized schedules that are derived from repetitive graph structures; and automated code generation for programming interfaces and low-level customizations that are geared toward high-performance embedded-processing architectures. We demonstrate our design tool for cross-platform application design, parameterized schedule representation, and associated dataflow graph-code generation using a case study centered around an image registration application.
C1 [Shen, Chung-Ching; Wu, Hsiang-Huang; Plishker, William; Bhattacharyya, Shuvra S.] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Wu, Shenpei] SAIC, Rockville, MD 20852 USA.
   [Sane, Nimish] New Jersey Inst Technol, Newark, NJ 07102 USA.
C3 University System of Maryland; University of Maryland College Park;
   Science Applications International Corporation (SAIC); New Jersey
   Institute of Technology
RP Shen, CC (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM ccshen@umd.edu; shenpei.wu@gmail.com; nimish.sane@njit.edu;
   hhwu@umd.edu; plishker@umd.edu; ssb@umd.edu
OI Bhattacharyya, Shuvra/0000-0001-7719-1106
CR [Anonymous], 2007, NVIDIA CUDA COMPUTE
   [Anonymous], THESIS U MARYLAND CO
   [Anonymous], 2003, M0348 UCBERL
   [Anonymous], 1974, PROC IFIP C 74
   Bhattacharya B, 2001, IEEE T SIGNAL PROCES, V49, P2408, DOI 10.1109/78.950795
   Bhattacharyya S.S., Handbook of signal processing systems
   Bilsen G, 1996, IEEE T SIGNAL PROCES, V44, P397, DOI 10.1109/78.485935
   Boehm Barry., 2000, COCOMO 2 MODEL DEFIN
   Broadcom, 2007, HIGH DEF 720P MOB MU
   BUCK JT, 1994, CONF REC ASILOMAR C, P508, DOI 10.1109/ACSSC.1994.471505
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Fischman L., 2005, CrossTalk: The Journal of Defense Software Engineering, V18, P26
   Geilen Marc, 2004, P 4 ACM INT C EMB SO, P137
   Hsu C.-J., 2005, P 2005 WORKSHOP SOFT, P37, DOI DOI 10.1145/1140389.1140394
   Johnson Gary., 1997, LABVIEW GRAPHICAL PR
   Ko MY, 2008, J SIGNAL PROCESS SYS, V50, P163, DOI 10.1007/s11265-007-0114-1
   Ko MY, 2007, IEEE T SIGNAL PROCES, V55, P3126, DOI 10.1109/TSP.2007.893964
   KWON S, 2004, P INT SOC DES C OCT, P9
   LEE EA, 1989, IEEE T ACOUST SPEECH, V37, P1751, DOI 10.1109/29.46557
   LEE EA, 1987, P IEEE, V75, P1235, DOI 10.1109/PROC.1987.13876
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nikolov H, 2005, ANN IEEE SYM FIELD P, P255
   NVIDIA Corp, 2010, WHIT BEN MULT CPU CO
   Oh H, 2004, J VLSI SIG PROC SYST, V37, P41, DOI 10.1023/B:VLSI.0000017002.91721.0e
   Pino JL, 1998, CONF REC ASILOMAR C, P1710, DOI 10.1109/ACSSC.1998.751617
   Plishker William, 2011, Transactions on High-Performance Embedded Architectures and Compilers IV, P391, DOI 10.1007/978-3-642-24568-8_20
   Plishker W, 2008, P IEEE RAP SYST PROT, P17, DOI 10.1109/RSP.2008.32
   Putnam LawrenceH., 2003, Five core metrics : the intelligence behind successful software management
   Ritz S., 1993, Proceedings. International Conference on Application-Specific Array Processors (Cat. No.93TH0572-8), P285, DOI 10.1109/ASAP.1993.397152
   Ritz S., 1992, Proceedings of the International Conference on Application Specific Array Processors (Cat. No.92TH0453-1), P679, DOI 10.1109/ASAP.1992.218536
   Sane N., 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P13, DOI 10.1109/SIPS.2010.5624821
   Shen CC, 2011, IEEE INT CON MULTI
   Stefanov T, 2004, DESIGN, AUTOMATION AND TEST IN EUROPE CONFERENCE AND EXHIBITION, VOLS 1 AND 2, PROCEEDINGS, P340
   Teixeira L.F., 2008, P 16 ACM INT C MULTI, P873
   Texas Instruments, 2002, MIL MULT VID PROC MV
   Theelen BD, 2006, FOURTH ACM & IEEE INTERNATIONAL CONFERENCE ON FORMAL METHODS AND MODELS FOR CO-DESIGN, PROCEEDINGS, P185, DOI 10.1109/MEMCOD.2006.1695924
   Thies W, 2002, LECT NOTES COMPUT SC, V2304, P179
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 38
TC 5
Z9 5
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 630
EP 640
DI 10.1109/TMM.2012.2191397
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wong, BY
   Shih, KT
   Liang, CK
   Chen, HH
AF Wong, Bing-Yi
   Shih, Kuang-Tsu
   Liang, Chia-Kai
   Chen, Homer H.
TI Single Image Realism Assessment and Recoloring by Color Compatibility
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color compatibility; color similarity; color tendency; image
   composition; image realism; image recoloring
AB In this paper, we investigate the assessment of image realism by focusing our attention on the color compatibility between an inserted object and the background in an image composite. We propose a technique based on two color compatibility properties to achieve realistic image composition. The first property is related to the color similarity, and the second one to the consistence of color tendency between image regions. We further propose algorithms based on these two properties for image realism assessment and recoloring. These algorithms only require information from the image to be tested, making them suitable for practical applications where real images are unavailable. Effectiveness of the algorithms is demonstrated through various images and verified by ground truth.
C1 [Wong, Bing-Yi; Shih, Kuang-Tsu] Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Liang, Chia-Kai] Lytro Inc, Mountain View, CA 94041 USA.
   [Chen, Homer H.] Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   [Chen, Homer H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Wong, BY (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
EM homer@cc.ee.ntu.edu.tw
RI ; Liang, Chia-Kai/L-9973-2019
OI Chen, Homer/0000-0002-8795-1911; Liang, Chia-Kai/0000-0003-3649-2505
FU National Science Council of Taiwan [NSC100-2221-E-002-197-MY3]; National
   Taiwan University [10R70609-2]
FX This work was supported in part by the National Science Council of
   Taiwan under the contract NSC100-2221-E-002-197-MY3 and National Taiwan
   University under Contract 10R70609-2. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Nicu Sebe.
CR [Anonymous], 2004, Analyzing microarray gene expression data
   [Anonymous], INT IND TRAIN SIM ED
   Bradley AP, 1997, PATTERN RECOGN, V30, P1145, DOI 10.1016/S0031-3203(96)00142-2
   Cavanagh P, 2005, NATURE, V434, P301, DOI 10.1038/434301a
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   CHANG YOUNGHA., 2004, Proceedings of the 1st Symposium on Applied Perception in Graphics and Visualization, P91, DOI [10.1145/1012551.1012567, DOI 10.1145/1012551.1012567]
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Cutzu F, 2003, PROC CVPR IEEE, P305
   Fattal R, 2002, ACM T GRAPHIC, V21, P249
   Hays J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239455
   Jia JY, 2006, ACM T GRAPHIC, V25, P631, DOI 10.1145/1141911.1141934
   Lalonde JF, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276381, 10.1145/1239451.1239454]
   Lalonde Jean-Franfois., 2007, Proc. IEEE Int. Conf. Computer Vision (ICCV), P14
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Luan Q., 2007, P 18 EUR C CREND TEC, P309
   Mileva Y, 2007, LECT NOTES COMPUT SC, V4713, P152
   Omer I, 2004, PROC CVPR IEEE, P946
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Rademacher P, 2001, SPRING EUROGRAP, P235
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   RUSSELL BC, 2005, AIM2005025 MIT AI LA
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Tai YW, 2005, PROC CVPR IEEE, P747
   Tokumaru M, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2, P378, DOI 10.1109/FUZZ.2002.1005020
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Wong BY, 2009, IEEE IMAGE PROC, P4385, DOI 10.1109/ICIP.2009.5413553
NR 29
TC 10
Z9 11
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 760
EP 769
DI 10.1109/TMM.2012.2188997
PN 2
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700009
DA 2024-07-18
ER

PT J
AU Lin, JC
   Wu, CH
   Wei, WL
AF Lin, Jen-Chun
   Wu, Chung-Hsien
   Wei, Wen-Li
TI Error Weighted Semi-Coupled Hidden Markov Model for Audio-Visual Emotion
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual bimodal fusion; emotion recognition; semi-coupled hidden
   Markov model (SC-HMM)
ID FACIAL EXPRESSION; AGREEMENT; STATES; AUDIO; FACE; HMM
AB This paper presents an approach to the automatic recognition of human emotions from audio-visual bimodal signals using an error weighted semi-coupled hidden Markov model (EWSC-HMM). The proposed approach combines an SC-HMM with a state-based bimodal alignment strategy and a Bayesian classifier weighting scheme to obtain the optimal emotion recognition result based on audio-visual bimodal fusion. The state-based bimodal alignment strategy in SC-HMM is proposed to align the temporal relation between audio and visual streams. The Bayesian classifier weighting scheme is then adopted to explore the contributions of the SC-HMM-based classifiers for different audio-visual feature pairs in order to obtain the emotion recognition output. For performance evaluation, two databases are considered: the MHMC posed database and the SEMAINE naturalistic database. Experimental results show that the proposed approach not only outperforms other fusion-based bimodal emotion recognition methods for posed expressions but also provides satisfactory results for naturalistic expressions.
C1 [Lin, Jen-Chun; Wu, Chung-Hsien; Wei, Wen-Li] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Inst Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Cheng Kung University
RP Lin, JC (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Inst Comp Sci & Informat Engn, Tainan 70101, Taiwan.
EM pa-paman01@yahoo.com.tw; chunghsienwu@gmail.com; lilijinjin@gmail.com
RI Lin, Jen-Chun/AAQ-3701-2021; Wu, Chung-Hsien/E-7970-2013; Wei,
   Wen-Li/AAQ-3848-2021
OI Lin, Jen-Chun/0000-0002-9237-4119; Wu, Chung-Hsien/0000-0002-3947-2123;
   Wei, Wen-Li/0000-0002-6753-2824
CR AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   Ananthakrishnan S, 2005, INT CONF ACOUST SPEE, P269
   [Anonymous], 2011, ICDECOM
   Bailly Gerard., 1992, Talking Machines - Theories, Models, and Designs
   Boersma P., 2007, PRAAT: doing phonetics by computer
   Bradley MM, 2001, EMOTION, V1, P276, DOI 10.1037//1528-3542.1.3.276
   Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450
   Brand M., 1997, Coupled hidden markov models for modeling interacting processes
   Bulut M., 2004, P 6 INT C MULT INT, P205
   Carletta J, 1996, COMPUT LINGUIST, V22, P249
   Chang Y, 2006, IMAGE VISION COMPUT, V24, P605, DOI 10.1016/j.imavis.2005.08.006
   Chen CW, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3133, DOI 10.1109/IROS.2008.4650788
   Choi HC, 2006, IEEE SYS MAN CYBERN, P1559, DOI 10.1109/ICSMC.2006.384939
   Cohen I, 2003, COMPUT VIS IMAGE UND, V91, P160, DOI 10.1016/S1077-3142(03)00081-X
   COOTES TF, 1992, IMAGE VISION COMPUT, V10, P289, DOI 10.1016/0262-8856(92)90044-4
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cowie R, 2003, SPEECH COMMUN, V40, P5, DOI 10.1016/S0167-6393(02)00071-7
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Douglas-Cowie E., 2008, P PROGR WORKSH CORP
   Douglas-Cowie E, 2007, LECT NOTES COMPUT SC, V4738, P488
   Ekman P, 1978, FACIAL ACTION CODING
   FLEISS JL, 1971, PSYCHOL BULL, V76, P378, DOI 10.1037/h0031619
   Fragopanagos N, 2005, NEURAL NETWORKS, V18, P389, DOI 10.1016/j.neunet.2005.03.006
   Hoch S., 2005, Proc. Acoustics, Speech, P1085, DOI [10.1109/ICASSP.2005.1415597., DOI 10.1109/ICASSP.2005.1415597]
   International Organization for Standardization, 1998, N2502 ISOIET JTC1SC2
   Ioannou SV, 2005, NEURAL NETWORKS, V18, P423, DOI 10.1016/j.neunet.2005.03.004
   Ivanov Y., 2005, 258 CBCL MIT
   Kapoor A, 2004, INT C PATT RECOG, P969, DOI 10.1109/ICPR.2004.1334690
   Karpouzis K, 2007, LECT NOTES COMPUT SC, V4451, P91
   Kim S, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P48, DOI 10.1109/MMSP.2007.4412815
   Kwon O.W., 8 EUROPEAN C SPEECH
   LANDIS JR, 1977, BIOMETRICS, V33, P159, DOI 10.2307/2529310
   LANG PJ, 1993, PSYCHOPHYSIOLOGY, V30, P261, DOI 10.1111/j.1469-8986.1993.tb03352.x
   LANITIS A, 1994, ELECTRON LETT, V30, P1587, DOI 10.1049/el:19941110
   Lee CM, 2005, IEEE T SPEECH AUDI P, V13, P293, DOI 10.1109/TSA.2004.838534
   Levenson RW, 2003, SER AFFECTIVE SCI, P212
   Li X, 2007, INT CONF ACOUST SPEE, P1081, DOI 10.1109/ICMLC.2007.4370304
   Lin YL, 2005, PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9, P4898
   Lucey Simon., 2007, FACE RECOGNITION DEL, P275
   Luengo I., 2005, INTERSPEECH 2005, P493
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   MEHRABIAN A, 1968, PSYCHOL TODAY, V2, P53
   Metallinou A, 2010, INT CONF ACOUST SPEE, P2462, DOI 10.1109/ICASSP.2010.5494890
   Metallinou A, 2008, IEEE INT SYM MULTIM, P250, DOI 10.1109/ISM.2008.40
   Mikels JA, 2005, BEHAV RES METHODS, V37, P626, DOI 10.3758/BF03192732
   Morrison D, 2007, SPEECH COMMUN, V49, P98, DOI 10.1016/j.specom.2006.11.004
   Nefian AV, 2002, INT CONF ACOUST SPEE, P2013
   Pernkopf F, 2004, INT C PATT RECOG, P223, DOI 10.1109/ICPR.2004.1334508
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Raouzaiou A, 2002, EURASIP J APPL SIG P, V2002, P1021, DOI 10.1155/S1110865702206149
   Schuller B, 2003, INT CONF ACOUST SPEE, P1
   Schuller B, 2007, ICMI'07: PROCEEDINGS OF THE NINTH INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, P30
   Sebe N, 2006, INT C PATT RECOG, P1136
   Song ML, 2008, NEUROCOMPUTING, V71, P1913, DOI 10.1016/j.neucom.2007.07.041
   Stevenson RA, 2007, BEHAV RES METHODS, V39, P1020, DOI 10.3758/BF03192999
   Tang FQ, 2007, ICNC 2007: THIRD INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 2, PROCEEDINGS, P632
   Tekalp AM, 2000, SIGNAL PROCESS-IMAGE, V15, P387, DOI 10.1016/S0923-5965(99)00055-7
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Ververidis D, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P593
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wagner J., IEEE T AFFE IN PRESS
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   WEIZENBAUM J, 1966, COMMUN ACM, V9, P36, DOI 10.1145/357980.357991
   Wu C.H., 2006, ACM Trans. Asian Lang. Inf. Process. (TALIP), V5, P165, DOI DOI 10.1145/1165255.1165259
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Wu CH, 2009, AFFECTIVE INFORMATION PROCESSING, P93, DOI 10.1007/978-1-84800-306-4_6
   Xie L, 2007, PATTERN RECOGN, V40, P2325, DOI 10.1016/j.patcog.2006.12.001
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   Zeng ZH, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P828
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
NR 72
TC 86
Z9 92
U1 1
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 142
EP 156
DI 10.1109/TMM.2011.2171334
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100014
DA 2024-07-18
ER

PT J
AU Tang, S
   Zheng, YT
   Wang, Y
   Chua, TS
AF Tang, Sheng
   Zheng, Yan-Tao
   Wang, Yu
   Chua, Tat-Seng
TI Sparse Ensemble Learning for Concept Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept detection; ensemble learning; non-negative matrix factorization;
   pattern recognition; sparse coding
ID NONNEGATIVE MATRIX FACTORIZATION; FACE RECOGNITION; ALGORITHMS; ONLINE;
   VIDEO
AB This work presents a novel sparse ensemble learning scheme for concept detection in videos. The proposed ensemble first exploits a sparse non-negative matrix factorization (NMF) process to represent data instances in parts and partition the data space into localities, and then coordinates the individual classifiers in each locality for final classification. In the sparse NMF, data exemplars are projected to a set of locality bases, in which the non-negative superposition of basis images reconstructs the original exemplars. This additive combination ensures that each locality captures the characteristics of data exemplars in part, thus enabling the local classifiers to hold reasonable diversity in their own regions of expertise. More importantly, the sparse NMF ensures that an exemplar is projected to only a few bases (localities) with non-zero coefficients. The resultant ensemble model is, therefore, sparse, in the way that only a small number of efficient classifiers in the ensemble will fire on a testing sample. Extensive tests on the TRECVid 08 and 09 datasets show that the proposed ensemble learning achieves promising results and outperforms existing approaches. The proposed scheme is feature-independent, and can be applied in many other large scale pattern recognition problems besides visual concept detection.
C1 [Tang, Sheng; Wang, Yu] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Zheng, Yan-Tao] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
   [Chua, Tat-Seng] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); National University of Singapore
RP Tang, S (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
EM ts@ict.ac.cn; yzheng@i2r.a-star.edu.sg; wangyu@ict.ac.cn;
   chuats@comp.nus.edu.sg
RI Wang, Xiaogang/B-2439-2013
OI Wang, Xiaogang/0000-0002-7929-5889
FU National Basic Research Program of China (973 Program) [2007CB311105];
   National Nature Science Foundation of China [60873165, 61173054]
FX This work was supported in part by National Basic Research Program of
   China (973 Program, 2007CB311105) and in part by the National Nature
   Science Foundation of China (60873165 and 61173054). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Alan F. Smeaton.
CR Adler A, 2010, INT CONF ACOUST SPEE, P782, DOI 10.1109/ICASSP.2010.5494973
   Amir A., 2003, P TREC VID RETR EV W
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2019, Ensemble Learning: Pattern Classification Using Ensemble Methods, DOI DOI 10.1142/9789814271073_0003
   [Anonymous], P IEEE INT WORKSH CO
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P NIST TRECVID WORKS
   Ayache G. Q. S., 2007, P EUR C INF RETR
   Ayache S, 2008, LECT NOTES COMPUT SC, V4956, P187
   Blei D. M., 2003, J MACH LEARN RES
   Bordes A, 2005, J MACH LEARN RES, V6, P1579
   Borth D., 2010, ADAPTING WEB BASED V
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Cai N., 2007, P 2 INT C PERV COMP
   Cao J., 2006, P NIST TRECVID WORKS
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067
   Efron B., 1993, INTRO BOOTSTRAP, VVolume 914, DOI DOI 10.1007/978-1-4899-4541-9
   Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1
   Farsiu S, 2006, IEEE T IMAGE PROCESS, V15, P141, DOI 10.1109/TIP.2005.860336
   Févotte C, 2009, NEURAL COMPUT, V21, P793, DOI 10.1162/neco.2008.04-08-771
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131
   Graepel Thore., 2000, Proceedings of the thirteenth annual conference on computational learning theory, P298
   Hauptmann Alexander., 2007, CIVR 07, P627
   Hoyer PO, 2004, J MACH LEARN RES, V5, P1457
   Jiang W., 2008, P INT C IM PROC
   Jiang Y.-G., 2006, P NIST TRECVID WORKS
   Jiang Y.-G., 2009, THESIS CITY U HONG K
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Kuncheva L. I., 2004, COMBINING PATTERN CL, V390, P413
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Martínez-Muñoz G, 2009, IEEE T PATTERN ANAL, V31, P245, DOI 10.1109/TPAMI.2008.78
   MATAS J, 2002, P BRIT MACH VIS C 20
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Ngo C.-W., 2009, P NIST TRECVID WORKS
   NIST, NIST TRECV WORKSH PA
   Opitz D., 1999, J ARTIF INTELL RES, V11, P169, DOI DOI 10.1613/JAIR.614
   Opitz D. W., 1996, Connection Science, V8, P337, DOI 10.1080/095400996116802
   Opitz DW, 1996, ADV NEUR IN, V8, P535
   Over P., 2008, P NIST TRECVID WORKS
   Over P., 2009, P NIST TRECVID WORKS
   Pytlik D. K. B., 2005, P NIST TRECVID WORKS
   Qi G.-J., 2007, P ACM INT C MULT
   Schapire, 1996, MACH LEARN P 13 INT, V96, P148, DOI DOI 10.1023/A:1022646704993
   Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Snoek C. G., 2010, P NIST TRECVID WORKS
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Tang S., 2007, P NIST TRECVID WORKS
   Tang S., 2008, P NIST TRECVID WORKS
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Tipping M., 2000, ADV NEURAL INF PROCE
   Tsoumakas G, 2005, INTELL DATA ANAL, V9, P511, DOI 10.3233/IDA-2005-9602
   Vailaya A, 1998, PATTERN RECOGN, V31, P1921, DOI 10.1016/S0031-3203(98)00079-X
   Vapnik V., 1999, NATURE STAT LEARNING
   Wagner A, 2009, PROC CVPR IEEE, P597, DOI 10.1109/CVPRW.2009.5206654
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Weng M.-F., 2008, P ACM INT C MULT
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu TT, 2008, ANN APPL STAT, V2, P224, DOI 10.1214/07-AOAS147
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yilmaz Emine, 2006, Proceedings of the 2006 ACM CIKM International Conference on Information and Knowledge Management, Arlington, Virginia, USA, November 6-11, 2006, P102, DOI [10.1145/1183614.1183633 (cit. on p. 34, DOI 10.1145/1183614.1183633(CIT.ONP.34]
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
   Zhang L, 2011, PATTERN RECOGN, V44, P97, DOI 10.1016/j.patcog.2010.07.021
   Zheng Y.-T., 2008, ACM INT C CONTENT BA, P161
   Zouari H, 2005, PATTERN RECOGN, V38, P2195, DOI 10.1016/j.patcog.2005.02.012
NR 79
TC 42
Z9 45
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 43
EP 54
DI 10.1109/TMM.2011.2168198
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100005
DA 2024-07-18
ER

PT J
AU Xie, HT
   Gao, K
   Zhang, YD
   Tang, S
   Li, JT
   Liu, YZ
AF Xie, Hongtao
   Gao, Ke
   Zhang, Yongdong
   Tang, Sheng
   Li, Jintao
   Liu, Yizhi
TI Efficient Feature Detection and Effective Post-Verification for Large
   Scale Near-Duplicate Image Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geometric consistency constraints; graphics processing units; local
   feature; near-duplicate image search; spatial coherent verification
ID VIDEOS
AB State-of-the-art near-duplicate image search systems mostly build on the bag-of-local features (BOF) representation. While favorable for simplicity and scalability, these systems have three shortcomings: 1) high time complexity of the local feature detection; 2) discriminability reduction of local descriptors due to BOF quantization; and 3) neglect of the geometric relationships among local features after BOF representation. To overcome these shortcomings, we propose a novel framework by using graphics processing units (GPU). The main contributions of our method are: 1) a new fast local feature detector coined Harris-Hessian (H-H) is designed according to the characteristics of GPU to accelerate the local feature detection; 2) the spatial information around each local feature is incorporated to improve its discriminability, supplying semi-local spatial coherent verification (LSC); and 3) a new pairwise weak geometric consistency constraint (P-WGC) algorithm is proposed to refine the search result. Additionally, part of the system is implemented on GPU to improve efficiency. Experiments conducted on reference datasets and a dataset of one million images demonstrate the effectiveness and efficiency of H-H, LSC, and P-WGC.
C1 [Xie, Hongtao; Gao, Ke; Zhang, Yongdong; Tang, Sheng; Li, Jintao; Liu, Yizhi] Chinese Acad Sci, Dept Inst Comp Technol, Beijing 100190, Peoples R China.
   [Xie, Hongtao] Chinese Acad Sci, Dept Grad Univ, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Chinese Academy of Sciences
RP Xie, HT (corresponding author), Chinese Acad Sci, Dept Inst Comp Technol, Beijing 100190, Peoples R China.
EM xiehongtao@ict.ac.cn; kegao@ict.ac.cn; zhyd@ict.ac.cn; ts@ict.ac.cn;
   jtli@ict.ac.cn; liuyizhi@ict.ac.cn
FU National Basic Research Program of China (973 Program) [2007CB311100];
   National Nature Science Foundation of China [61003163, 60873165];
   Beijing New Star Project on Science Technology [2007B071]; National High
   Technology and Research Development Program of China (863 Program)
   [2009AA01A403]; Beijing Municipal Education Commission
FX Manuscript received January 25, 2011; revised May 13, 2011 and August
   18, 2011; accepted August 22, 2011. Date of publication September 06,
   2011; date of current version November 18, 2011. This work is supported
   in part by the National Basic Research Program of China (973 Program,
   2007CB311100); National Nature Science Foundation of China (61003163,
   60873165); Beijing New Star Project on Science & Technology (2007B071);
   National High Technology and Research Development Program of China (863
   Program, 2009AA01A403); and Co-building Program of Beijing Municipal
   Education Commission. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Zhu Liu.
CR [Anonymous], P EUR C COMP VIS
   [Anonymous], 2010, NVIDIA CUDA PROGR GU
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], TRECVID
   [Anonymous], P EUR C COM IN PRESS
   [Anonymous], BINARIES AFFINE COVA
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 2007, P IEEE C COMP VIS PA
   [Anonymous], P ACM INT C MULT RET
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bhat DN, 1998, IEEE T PATTERN ANAL, V20, P415, DOI 10.1109/34.677275
   Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P2089, DOI 10.1109/TPAMI.2007.1126
   Catanzaro B., 2008, P 25 INT C MACHINE L, P104, DOI DOI 10.1145/1390156.1390170
   Chandrasekhar Vijay, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2504, DOI 10.1109/CVPRW.2009.5206733
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Cornelis N., 2008, PROC IEEE C COMPUTER, P1
   Fischer B, 2003, IEEE T PATTERN ANAL, V25, P513, DOI 10.1109/TPAMI.2003.1190577
   Harris C., 1988, ALVEY VISION C, P147151
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Lin KI, 2001, SEVENTH INTERNATIONAL CONFERENCE ON DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS, P174, DOI 10.1109/DASFAA.2001.916376
   LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051
   Lindeberg T, 2003, LECT NOTES COMPUT SC, V2695, P148
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Mortensen EN, 2005, PROC CVPR IEEE, P184
   Nister David, 2006, CVPR
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Philbin J., 2008, P CVPR, P1
   Ruijters Daniel, 2008, Journal of Graphics Tools, V13, P61
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sivic J, 2009, IEEE T PATTERN ANAL, V31, P591, DOI 10.1109/TPAMI.2008.111
   Tsai SS, 2010, IEEE IMAGE PROC, P1029, DOI 10.1109/ICIP.2010.5648942
   Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017
   Wu C., 2007, Siftgpu: A gpu implementation of david lowe's scale invariant feature transform (sift)
   Wu X, 2009, IEEE T MULTIMEDIA, V11, P196, DOI 10.1109/TMM.2008.2009673
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Xie HT, 2010, INT CONF ACOUST SPEE, P2494, DOI 10.1109/ICASSP.2010.5494898
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
NR 50
TC 39
Z9 45
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2011
VL 13
IS 6
BP 1319
EP 1332
DI 10.1109/TMM.2011.2167224
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 852GD
UT WOS:000297343400012
DA 2024-07-18
ER

PT J
AU Gómez, AM
   Carmona, JL
   Gónzález, JA
   Sánchez, V
AF Gomez, Angel M.
   Carmona, Jose L.
   Gonzalez, Jose A.
   Sanchez, Victoria
TI One-Pulse FEC Coding for Robust CELP-Coded Speech Transmission Over
   Erasure Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive codebook; code-excited linear prediction (CELP); error
   correction coding; error propagation; frame erasure; speech coding
AB In this paper, we present an improved quantization scheme for the redundancy data of a forward error correction (FEC) technique proposed for the transmission of code-excited linear prediction (CELP)-coded speech over erasure channels. The use of a FEC-based error protection scheme is motivated by the well-known fact that, after a frame erasure, the previous excitation is not available and a desynchronization between the encoder and the decoder long-term prediction (LTP) filters appears, causing an additional distortion which is propagated to subsequent frames. LTP synchronization can be recovered by means of a single-pulse representation of the previous excitation. No additional delay is introduced by this technique which only requires a small transmission bandwidth increase. In this paper, we focus on the efficient encoding of this pulse. Thus, an optimization procedure, which takes into account the overall synthesis error, is proposed in order to provide better pulse-position and pulse-amplitude quantization codebooks. Moreover, by extending the previous procedure, an efficient joint position-amplitude quantization can be obtained. Objective quality tests applied to our proposal show that, by means of the proposed codebooks, the number of bits required to represent the resynchronization pulse is effectively reduced. In addition, a discontinuous transmission mechanism is derived from the cost functional used during joint position-amplitude quantization, further reducing the bit-rate.
C1 [Gomez, Angel M.; Carmona, Jose L.; Gonzalez, Jose A.; Sanchez, Victoria] Univ Granada, Fac Ciencias, Dept Signal Theory Networking & Commun, E-18071 Granada, Spain.
C3 University of Granada
RP Gómez, AM (corresponding author), Univ Granada, Fac Ciencias, Dept Signal Theory Networking & Commun, Campus Fuentenueva S-N, E-18071 Granada, Spain.
EM amgg@ugr.es
RI Gonzalez, Jose A./AAA-8130-2019; García, Angel Manuel Gómez/C-6856-2012
OI Gonzalez, Jose A./0000-0002-5531-8994; García, Angel Manuel
   Gómez/0000-0002-9995-3068; Sanchez Calle, Victoria
   Eugenia/0000-0003-1546-9728
FU Spanish MEC [TEC2010-18009, CEB09-0010, JC2010-0194]
FX Manuscript received August 09, 2010; revised February 24, 2011; accepted
   May 09, 2011. Date of publication May 19, 2011; date of current version
   September 16, 2011. This work was supported by the Spanish MEC through
   projects TEC2010-18009, CEB09-0010, and grant JC2010-0194. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Jin Li.
CR AGIOMYRGIANNAKI.Y, 2005, P IEEE INT C AC SPEE, V1, P141
   Andersen SV, 2002, 2002 IEEE SPEECH CODING WORKSHOP PROCEEDINGS, P23, DOI 10.1109/SCW.2002.1215711
   [Anonymous], 2001, REC ITU T P 862
   Atal B. S., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P614
   CARMONA J, 2008, P IEEE INT C AC SPEE, V4, P4805
   Chibani M, 2007, IEEE T AUDIO SPEECH, V15, P2485, DOI 10.1109/TASL.2007.907332
   Ehara H, 2008, IEEE T MULTIMEDIA, V10, P549, DOI 10.1109/TMM.2008.917411
   EKSLER V, 2008, P IEEE ICASSP LAS VE, P4001
   Eksler V, 2010, IEEE T AUDIO SPEECH, V18, P1208, DOI 10.1109/TASL.2009.2031699
   *ETSI, 1998, 301703 ETSI EN
   ETSI, 2000, 300961 ETSI EN
   Garofolo J. S., STRUCTURE FORMAT DAR
   Gómez AM, 2010, IEEE T AUDIO SPEECH, V18, P1258, DOI 10.1109/TASL.2009.2031798
   *ITU T, 1993, G729 ITUT
   JELINEK M, 2004, P IEEE INT C AC SPEE, V1, P281
   Lamel L.F., 1986, Proceedings of the DARPA Speech Recognition Workshop, P100
   LEFEBVRE R, 2004, P IEEE INT C AC SPEE, V1, P265
   LINDBLOM J, 2000, P IEEE WORKSH SPEECH
   MARTIN R, 2001, P IEEE INT C AC SPEE
   PRAESTHOLM S, 2004, P IEEE INT C MULT EX, V3, P1667
   SERIZAWA M, 2002, P IEEE INT C AC SPEE, V1, P169
   SINGHAL S, 1989, IEEE T ACOUST SPEECH, V37, P317, DOI 10.1109/29.21700
   Vaillancourt T, 2007, INT CONF ACOUST SPEE, P1113
   WANG J, 2001, P IEEE INT C AC SPEE, V2, P745
   XYDEAS C, 2003, P IEEE INT C AC SPEE, V1, P112
NR 25
TC 9
Z9 10
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 894
EP 904
DI 10.1109/TMM.2011.2156773
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300006
DA 2024-07-18
ER

PT J
AU Yang, J
   Hu, H
   Xi, HS
   Hanzo, L
AF Yang, Jian
   Hu, Han
   Xi, Hongsheng
   Hanzo, Lajos
TI Online Buffer Fullness Estimation Aided Adaptive Media Playout for Video
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive media playout; large deviation principle; media buffering;
   variable bit rate channel; variable bit rate encoded video; video
   streaming
ID DESIGN; AUDIO; STATE
AB Adaptive media playout (AMP) control is proposed in order to compensate for the bit-rate fluctuation of networks, which may result in playout interruptions in video streaming application. Most AMP algorithms found in the literature trigger playout-rate adjustments based on the buffer fullness or its variation. However, the challenge of the threshold based methods is to select the appropriate threshold for triggering a playout-rate adjustment owing to the unknown fluctuation of the channel quality and the video bitrate. We conceive an adaptive media playout regime based on underflow probability estimation, which requires no significant statistical knowledge of the previous tele-traffic load. To achieve this, we present an underflow probability estimation model based on large deviation theory relying on the buffer fullness and on its variation. We will then directly use the underflow probability to trigger the actions of playout control, instead of using indirect methods based on a buffer fullness threshold or buffer fullness variation threshold. Experiments based on MPEG-4 Variable Bit-Rate encoded video and VBR channels associated with Adaptive Modulation and Coding are conducted in order to investigate the achievable performance of the proposed algorithm. Our simulation results demonstrate an improved performance in comparison to other recent AMP algorithms.
C1 [Yang, Jian; Hu, Han; Xi, Hongsheng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
   [Yang, Jian; Hu, Han; Xi, Hongsheng] Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
   [Hanzo, Lajos] Univ Southampton, Sch Elect & Comp Sci ECS, Southampton SO17 1BJ, Hants, England.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; University of Southampton
RP Yang, J (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Anhui, Peoples R China.
EM jianyang@ustc.edu.cn; huhan@mail.ustc.edu.cn; xihs@ustc.edu.cn;
   lh@ecs.soton.ac.uk
RI Hanzo, Lajos/ITV-5242-2023; Hanzo, Lajos/S-4875-2016
OI Hanzo, Lajos/0000-0002-2636-5214; Hanzo, Lajos/0000-0002-2636-5214
FU RC-UK under the IU-ATC; China-UK Science Bridge in 4 G wireless
   communications; European Union [FP7/2007-2013, 214625]; Program for New
   Century Excellent Talents in University [NCET-08-0522]; EPSRC
   [EP/G05178X/1] Funding Source: UKRI
FX Manuscript received October 19, 2010; revised February 10, 2011 and June
   07, 2011; accepted June 13, 2011. Date of publication June 20, 2011;
   date of current version September 16, 2011. This work was supported by
   the RC-UK under the auspices of the IU-ATC and the China-UK Science
   Bridge in 4 G wireless communications, the European Union's Seventh
   Framework Programme ([FP7/2007-2013]) under grant agreement no [214625],
   as well as the Program for New Century Excellent Talents in University
   (NCET-08-0522). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yen-Kuang Chen.
CR *3GPP, TS25331 3GPP
   *3GPP, TS25214 3GPP
   Ahmad K, 2009, IEEE COMMUN MAG, V47, P68, DOI 10.1109/MCOM.2009.5350371
   [Anonymous], 2007, Large deviations for Gaussian queues
   [Anonymous], VIDEO TRACE LIB
   [Anonymous], 2007, VIDEO COMPRESSION CO
   Chakareski J, 2007, IEEE COMMUN MAG, V45, P77, DOI 10.1109/MCOM.2007.284541
   Chen MH, 2005, IEEE WIREL COMMUN, V12, P32, DOI 10.1109/MWC.2005.1497856
   Cherriman P, 2002, IEEE T CIRC SYST VID, V12, P342, DOI 10.1109/TCSVT.2002.1003473
   Cherriman P, 1999, IEEE T CIRC SYST VID, V9, P701, DOI 10.1109/76.780360
   Cherriman P, 1999, IEEE T VEH TECHNOL, V48, P1726, DOI 10.1109/25.790554
   Cherriman P, 2000, IEEE T CIRC SYST VID, V10, P1355, DOI 10.1109/76.889019
   Cherriman PJ, 2002, IEEE T CIRC SYST VID, V12, P829, DOI 10.1109/TCSVT.2002.804887
   Chuang HC, 2007, IEEE T MULTIMEDIA, V9, P1273, DOI 10.1109/TMM.2007.902884
   GARDNER ES, 1985, J FORECASTING, V4, P1, DOI 10.1002/for.3980040103
   Hanzo L, 2000, P IEEE, V88, P1388, DOI 10.1109/5.883314
   HANZO L, 2008, 3 G HSPA FDD VERSUS
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kalman M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P869, DOI 10.1109/ICME.2002.1035920
   KATO M, 1997, P IEEE PIMRC 97, V3, P1049
   Liang YJ, 2001, INT CONF ACOUST SPEE, P1445, DOI 10.1109/ICASSP.2001.941202
   Markopoulou AP, 2003, IEEE ACM T NETWORK, V11, P747, DOI 10.1109/TNET.2003.818179
   Mate S, 2009, IEEE COMMUN MAG, V47, P116, DOI 10.1109/MCOM.2009.5350378
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   *MOT, 2002, R1020675 3GPP RAN WG
   Narbutt M, 2005, IEEE INTERNET COMPUT, V9, P28, DOI 10.1109/MIC.2005.72
   Ng SX, 2006, IEEE T CIRC SYST VID, V16, P363, DOI 10.1109/TCSVT.2006.869968
   Park S, 2008, J VIS COMMUN IMAGE R, V19, P106, DOI 10.1016/j.jvcir.2007.09.002
   Proakis J. G., 1995, DIGITAL COMMUNICATIO
   Roccetti M, 2001, MULTIMED TOOLS APPL, V14, P23, DOI 10.1023/A:1011303506685
   ROTHERMAL K, 1995, LNCS, V1018, P189
   Roychoudhuri L, 2006, COMPUT COMMUN, V29, P1578, DOI 10.1016/j.comcom.2006.04.004
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   STEINBACH E, 2007, MULTIMEDIA IP WIRELE, P527
   Stockhammer T, 2005, IEEE WIREL COMMUN, V12, P6, DOI 10.1109/MWC.2005.1497853
   Su YF, 2009, IEEE T MULTIMEDIA, V11, P1331, DOI 10.1109/TMM.2009.2030543
   Yuang MC, 1997, IEEE J SEL AREA COMM, V15, P136, DOI 10.1109/49.552064
   Yuang MC, 1998, MULTIMED TOOLS APPL, V6, P47, DOI 10.1023/A:1009638628952
NR 38
TC 34
Z9 38
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1141
EP 1153
DI 10.1109/TMM.2011.2160158
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300025
DA 2024-07-18
ER

PT J
AU Lu, YJ
   Zhang, L
   Liu, JM
   Tian, Q
AF Lu, Yijuan
   Zhang, Lei
   Liu, Jiemin
   Tian, Qi
TI Constructing Concept Lexica With Small Semantic Gaps
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; large-scale; lexica; semantic gap
ID DATABASE; IMAGES
AB In recent years, constructing mathematical models for visual concepts by using content features, i.e., color, texture, shape, or local features, has led to the fast development of concept-based multimedia retrieval. In concept-based multimedia retrieval, defining a good lexicon of high-level concepts is the first and important step. However, which concepts should be used for data collection and model construction is still an open question. People agree that concepts that can be easily described by low-level visual features can construct a good lexicon. These concepts are called concepts with small semantic gaps. Unfortunately, there is very little research found on semantic gap analysis and on automatically choosing multimedia concepts with small semantic gaps, even though differences of semantic gaps among concepts are well worth investigating.
   In this paper, we propose a method to quantitatively analyze semantic gaps and develop a novel framework to identify high-level concepts with small semantic gaps from a large-scale web image dataset. Images with small semantic gaps are selected and clustered first by defining a confidence score and a content-context similarity matrix in visual space and textual space. Then, from the surrounding descriptions (titles, categories, and comments) of these images, concepts with small semantic gaps are automatically mined. In addition, considering that semantic gap analysis depends on both features and content-contextual consistency, we construct a lexicon family of high-level concepts with small semantic gaps (LCSS) based on different low-level features and different consistency measurements. This set of lexica is both independent to each other and mutually complimentary. LCSS is very helpful for data collection, feature selection, annotation, and modeling for large-scale image retrieval. It also shows a promising application potential for image annotation refinement and rejection. The experimental results demonstrate the validity of the developed concept lexica.
C1 [Lu, Yijuan] SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
   [Zhang, Lei] Microsoft Res Asia, Beijing 100190, Peoples R China.
   [Liu, Jiemin] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Texas State University System; Texas State University San Marcos;
   Microsoft; Microsoft Research Asia; Shanghai Jiao Tong University;
   University of Texas System; University of Texas at San Antonio (UTSA)
RP Lu, YJ (corresponding author), SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
EM yl12@txstate.edu; leizhang@microsoft.com; liujiemin8715@gmail.com;
   qitian@cs.utsa.edu
RI LU, YIJUAN/GNM-8769-2022
OI LU, YIJUAN/0000-0002-9855-8365
FU Texas State University; Army Research Office (ARO) [W911NF-05-1-0404];
   Department of Homeland Security (DHS)
FX This work was supported in part by the Research Enhancement Program
   (REP) and start-up funding from the Texas State University, the Army
   Research Office (ARO) grant under W911NF-05-1-0404, and the Department
   of Homeland Security (DHS). The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Ajay
   Divakaran.
CR Agarwal S, 2004, IEEE T PATTERN ANAL, V26, P1475, DOI 10.1109/TPAMI.2004.108
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2007, Technical Report
   Carneiro G, 2007, IEEE T PATTERN ANAL, V29, P394, DOI 10.1109/TPAMI.2007.61
   Fei-Fei L, 2004, P IEEE C COMP VIS PA, P178
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hauptmann Alexander., 2007, CIVR 07, P627
   Li J, 2008, IEEE T PATTERN ANAL, V30, P985, DOI 10.1109/TPAMI.2007.70847
   Lu Y., 2008, IEEE PES GEN M, P1
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Ponce J, 2006, LECT NOTES COMPUT SC, V4170, P29
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   von Ahn Luis., 2004, CHI, DOI DOI 10.1145/985692.985733
   Wang CH, 2008, MULTIMEDIA SYST, V14, P205, DOI 10.1007/s00530-008-0128-y
   Wang XJ, 2008, IEEE T PATTERN ANAL, V30, P1919, DOI 10.1109/TPAMI.2008.127
   Zhang L., 2004, MULTIMEDIA 04, P716
NR 18
TC 28
Z9 31
U1 1
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 288
EP 299
DI 10.1109/TMM.2010.2046292
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100006
DA 2024-07-18
ER

PT J
AU Ding, Y
   Fan, GL
AF Ding, Yi
   Fan, Guoliang
TI Sports Video Mining via Multichannel Segmental Hidden Markov Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hidden Markov models; semantic structures; sports video analysis; video
   mining
AB We study sports video mining as a machine learning and statistical inference problem. We focus on mid-level semantic structures that can serve as building blocks for high-level semantic analysis. Particularly, we are interested in how to infer multiple coexistent structures jointly. We present a new multichannel segmental hidden Markov model (MCSHMM) that is a unique probabilistic graphical model with two advantages. One is the integration of both hierarchical and parallel dynamic structures that offers more flexibility and capacity of capturing the interaction between multiple Markov chains. The other is the incorporation of the segmental HMM (SHMM) to deal with variable-length observations. In addition, we develop a maximum a posteriori (MAP) estimator to optimize the model structure and parameters simultaneously. The proposed MCSHMM is used for American football video analysis. The experiment result shows that the MCSHMM outperforms existing HMMs and has potential to be extended for other video mining tasks.
C1 [Ding, Yi; Fan, Guoliang] Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74078 USA.
C3 Oklahoma State University System; Oklahoma State University - Stillwater
RP Ding, Y (corresponding author), Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74078 USA.
EM yi.ding@ok-state.edu; guoliang.fan@okstate.edu
RI Fan, Guoliang/G-2893-2011
OI Fan, Guoliang/0000-0002-8584-9040
FU National Science Foundation ( NSF) [IS-0347613]; Oklahoma NASA EPSCoR
   Research Initiation Grant
FX Manuscript received January 19, 2009; July 12, 2009. First published
   August 21, 2009; current version published October 16, 2009. This paper
   was presented in part in the 2008 ACM Multimedia Conference, Vancouver,
   BC, Canada, Oct. 2008. This work was supported in part by the National
   Science Foundation ( NSF) under Grant IIS-0347613 and in part by the
   2009 Oklahoma NASA EPSCoR Research Initiation Grant. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. James Z. Wang.
CR [Anonymous], 1995, P IEEE INT C MULT CO
   ASSFALG J, 2002, P IEEE INT C MULT EX
   ASSFALG J, 2003, P COMP VIS IM UND
   Bouchard G, 2006, IEEE T PATTERN ANAL, V28, P544, DOI 10.1109/TPAMI.2006.82
   Brand M, 1999, NEURAL COMPUT, V11, P1155, DOI 10.1162/089976699300016395
   Brand M., 1997, Coupled hidden markov models for modeling interacting processes
   Chang SF, 2002, IEEE MULTIMEDIA, V9, P6, DOI 10.1109/93.998041
   DING Y, 2007, P IEEE INT C COMP VI
   DING Y, 2007, P IEEE INT C MULT EX
   Ding Y, 2006, IEEE INT SYM MULTIM, P317
   DUAN LY, 2003, P IEEE INT C MULT EX
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   Fine S, 1998, MACH LEARN, V32, P41, DOI 10.1023/A:1007469218079
   Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095
   GALES M, 1993, 133 CUEDFINFENGTR
   Ghahramani Z., 2002, HDB BRAIN THEORY NEU, V2nd
   GHAHRAMANI Z, 1996, P C ADV NEUR INF PRO
   HUA W, 2002, P IEEE INT C MULT EX
   Kokaram A, 2006, IEEE SIGNAL PROC MAG, V23, P47, DOI 10.1109/MSP.2006.1621448
   Lafferty J., 2001, ICML 01 P 18 INT C M
   Mei T, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P107
   Murphy Kevin Patrick, 2002, Dynamic bayesian networks: representation, inference and learning
   Ngo CW, 2005, IEEE T CIRC SYST VID, V15, P296, DOI 10.1109/TCSVT.2004.841694
   PETROVIC N, 2006, P IEEE INT C COMP VI, V1, P17
   RUI Y, 1998, P ACM MULT C
   Sadlier DA, 2005, IEEE T CIRC SYST VID, V15, P1225, DOI 10.1109/TCSVT.2005.854237
   Srinivasan MV, 1997, PATTERN RECOGN, V30, P593, DOI 10.1016/S0031-3203(96)00106-9
   Worring M, 2007, IEEE T MULTIMEDIA, V9, P909, DOI 10.1109/TMM.2007.898913
   Xie L., 2003, VIDEO MINING
   XIE L, 2002, P INT C AC SPEECH SI
   Xiong ZY, 2006, IEEE SIGNAL PROC MAG, V23, P18
   ZHONG D, 2001, P IEEE C MULT EXP TO
NR 33
TC 7
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1301
EP 1309
DI 10.1109/TMM.2009.2030828
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300008
DA 2024-07-18
ER

PT J
AU Liu, ZY
   Shen, YM
   Ross, KW
   Panwar, SS
   Wang, Y
AF Liu, Zhengye
   Shen, Yanming
   Ross, Keith W.
   Panwar, Shivendra S.
   Wang, Yao
TI LayerP2P: Using Layered Video Chunks in P2P Live Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layered video; peer-to-peer; streaming
ID SYSTEM
AB Although there are several successful commercial deployments of live P2P streaming systems, the current designs 1) lack incentives for users to contribute bandwidth resources, 2) lack adaptation to aggregate bandwidth availability, and 3) exhibit poor video quality when bandwidth availability falls below bandwidth supply. In this paper, we propose, prototype, deploy, and validate LayerP2P, a P2P live streaming system that addresses all three of these problems. LayerP2P combines layered video, mesh P2P distribution, and a tit-for-tat-like algorithm, in a manner such that a peer contributing more upload bandwidth receives more layers and consequently better video quality. We implement LayerP2P (including seeds, clients, trackers, and layered codecs), deploy the prototype in PlanetLab, and perform extensive experiments. We also examine a wide range of scenarios using trace-driven simulations. The results show that LayerP2P has high efficiency, provides differentiated service, adapts to bandwidth deficient scenarios, and provides protection against free-riders.
C1 [Liu, Zhengye; Shen, Yanming; Panwar, Shivendra S.; Wang, Yao] Polytech Inst New York Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
   [Ross, Keith W.] Polytech Inst New York Univ, Dept Comp Sci & Engn, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering;
   New York University; New York University Tandon School of Engineering
RP Liu, ZY (corresponding author), Polytech Inst New York Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
EM zhengye@vision.poly.edu; shen@dlut.edu.cn; ross@poly.edu;
   panwar@poly.edu; yao@poly.edu
RI Panwar, Shivendra S/A-6884-2016; Panwar, Shivendra/K-6473-2019
OI Panwar, Shivendra S/0000-0002-9822-6838; Wang, Yao/0000-0003-3199-3802
FU National Science Foundation [CNS0435228]; New York State Center for
   Advanced Technology in Telecommunications (CATT) at Polytechnic
   Institute of New York University, Brooklyn, NY
FX Manuscript received January 20, 2009; revised June 28, 2009. First
   published August 18, 2009; current version published October 16, 2009.
   This work was supported in part by the National Science Foundation under
   Grant No. CNS0435228 and in part by the New York State Center for
   Advanced Technology in Telecommunications (CATT) at Polytechnic
   Institute of New York University, Brooklyn, NY. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhihai ( Henry) He.
CR AGARWAL S, 2006, P IEEE ICDCS JUL
   [Anonymous], 2007, IEEE T CIRCUITS SYST
   [Anonymous], 2003, P WORKSH EC PEER TO
   BONALD T, 2008, P ACM SIGMETRICS JUN
   CASTRO M, 2003, P IPTPS FEB
   Chu Y.-H., 2000, P ACM SIGMETRICS JUN
   CUI Y, 2003, P ACM NOSSDAV JUN
   Dischinger M., 2007, P ACM IMC OCT
   FELDMAN M, 2004, P EC
   FITZEK FH, 2004, P WPMC SEP
   Huang C., 2007, P ACM SIGCOMM AUG
   Huang Y., 2008, P ACM SIGCOMM AUG
   LIOGKAS N, 2006, P IPTPS FEB
   LIU Z, 2007, P ACM SIGCOMM P2P TV
   LIU Z, 2008, P IEEE ICNP OCT
   Mol JD, 2007, IEEE T MULTIMEDIA, V9, P1593, DOI 10.1109/TMM.2007.907450
   PADMANABHAN VN, 2003, P ACM NOSSDAV MAY
   Pianese F, 2007, IEEE T MULTIMEDIA, V9, P1645, DOI 10.1109/TMM.2007.907466
   Scalable Video Coding (SVC), 2008, JTC1SC29WG11N9560 IS
   Tourapis A.M., 2005, JVTQ042
   VENKATARAMAN V, 2006, P IEEE ICNP NOV
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1227, DOI 10.1109/TCSVT.2007.905519
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   ZHANG X, 2005, P IEEE INFOCOM MAR
   ZHAO BQ, 2008, P NETECON
NR 25
TC 84
Z9 93
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1340
EP 1352
DI 10.1109/TMM.2009.2030656
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, Y
   Wang, BB
   Liu, KJR
AF Chen, Yan
   Wang, Beibei
   Liu, K. J. Ray
TI Multiuser Rate Allocation Games for Multimedia Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cheat-proof; game theory; multimedia; Nash equilibrium; proportional
   fairness; rate allocation
ID VIDEO; FRAMEWORK; NETWORKS
AB How to efficiently and fairly allocate data rate among different users is a key problem in the field of multiuser multimedia communication. However, most of the existing optimization-based methods, such as minimizing the weighted sum of the distortions or maximizing the weighted sum of the peak signal-to-noise ratios (PSNRs), have their weights heuristically determined. Moreover, those approaches mainly focus on the efficiency issue while there is no notion of fairness. In this paper, we address this problem by proposing a game-theoretic framework, in which the utility/payoff function of each user/player is jointly determined by the characteristics of the transmitted video sequence and the allocated bit-rate. We show that a unique Nash equilibrium (NE), which is proportionally fair in terms of both utility and PSNR, can be obtained, according to which the controller can efficiently and fairly allocate the available network bandwidth to the users. Moreover, we propose a distributed cheat-proof rate allocation scheme for the users to converge to the optimal NE using alternative ascending clock auction. We also show that the traditional optimization-based approach that maximizes the weighted sum of the PSNRs is a special case of the game-theoretic framework with the utility function defined as an exponential function of PSNR. Finally, we show several experimental results on real video data to demonstrate the efficiency and effectiveness of the proposed method.
C1 [Chen, Yan; Wang, Beibei; Liu, K. J. Ray] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Chen, Y (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM yan@umd.edu; bebewang@umd.edu; kjrliu@umd.edu
RI Chen, Yan/P-5344-2019; Chen, Yan/H-5483-2012; Chen, Yan/C-6466-2014;
   Liu, K.J. Ray/C-2798-2009; Chen, Yan/P-8901-2017
OI Chen, Yan/0000-0002-3227-4562; Chen, Yan/0000-0002-3227-4562
CR Ausubel LM, 2004, AM ECON REV, V94, P1452, DOI 10.1257/0002828043052330
   Boyd S., 2004, CONVEX OPTIMIZATION
   CHEN Y, 2008, P IEEE INT C AC SPEE
   Chiang TH, 1997, IEEE T CIRC SYST VID, V7, P246, DOI 10.1109/76.554439
   DAI M, 2004, P IEEE INT C IM PROC
   Ding W, 1996, IEEE T CIRC SYST VID, V6, P12, DOI 10.1109/76.486416
   Han Z, 2005, IEEE T COMMUN, V53, P1366, DOI 10.1109/TCOMM.2005.852826
   Han Z, 2008, IEEE T WIREL COMMUN, V7, P1889, DOI 10.1109/TWC.2008.061014
   Hang HM, 1997, IEEE T CIRC SYST VID, V7, P287
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   LI Z, 2004, P IEEE INT C IM PROC
   Ortega A., 1998, IEEE Signal Process. Mag, V15, P25
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   RHEE W, 2000, P IEEE VEH TECHN C
   Ribas-Corbera J, 1999, IEEE T CIRC SYST VID, V9, P172, DOI 10.1109/76.744284
   Saraydar CU, 2002, IEEE T COMMUN, V50, P291, DOI 10.1109/26.983324
   Shen C, 2008, IEEE T WIREL COMMUN, V7, P3546, DOI 10.1109/TWC.2008.070337
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   WANG M, 2005, P IEEE INT C AC SPEE
   ZHANG X, 2002, P SPIE C VIS COMM IM
   ZHAO XJ, 2002, P PACK VID PV
   H 264 AVC SOFTWARE
NR 24
TC 37
Z9 41
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1170
EP 1181
DI 10.1109/TMM.2009.2026101
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700013
DA 2024-07-18
ER

PT J
AU Hsieh, CK
   Lai, SH
   Chen, YC
AF Hsieh, Chao-Kuei
   Lai, Shang-Hong
   Chen, Yung-Chang
TI Expression-Invariant Face Recognition With Constrained Optical Flow
   Warping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Constrained optical flow; expression invariance; expression
   normalization; face recognition
ID ILLUMINATION; IMAGES
AB Face recognition is one of the most intensively studied topics in computer vision and pattern recognition, but few are focused on how to robustly recognize expressional faces with one single training sample per class. In this paper, we modify the regularization-based optical flow algorithm by imposing constraints on some given point correspondences to compute precise pixel displacements and intensity variations. By using the optical How computed for the input expression variant face with respect to a reference neutral face image, we remove the expression from the face image by elastic image warping to recognize the subject with facial expression. Experimental validation is given to show that the proposed expression normalization algorithm significantly improves the accuracy of face recognition on expression variant faces.
C1 [Hsieh, Chao-Kuei; Chen, Yung-Chang] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lai, Shang-Hong] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
C3 National Tsing Hua University; National Tsing Hua University
RP Hsieh, CK (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
EM d903915@oz.nthu.edu.tw; lai@cs.nthu.edu.tw; ycchen@ee.nthu.edu.tw
RI Lai, Shang-Hong/AAS-4002-2020
OI Lai, Shang-Hong/0000-0002-5092-993X
CR [Anonymous], 1991, P 1991 IEEE COMP SOC, DOI DOI 10.1109/CVPR.1991.139758
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Cevikalp H, 2005, IEEE T PATTERN ANAL, V27, P4, DOI 10.1109/TPAMI.2005.9
   CHAI X, P INT C AUT FAC GEST
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   ESSA IA, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P76, DOI 10.1109/CVPR.1994.323813
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Gorodnichy DO, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P330, DOI 10.1109/CRV.2005.87
   He YH, 2005, PROCEEDINGS OF 2005 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-9, P4605
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   HSIEH CK, 2007, P INT C MULT EXP BEI
   Kim TK, 2005, IEEE T PATTERN ANAL, V27, P318, DOI 10.1109/TPAMI.2005.58
   Kim YH, 2005, IMAGE VISION COMPUT, V23, P365, DOI 10.1016/j.imavis.2004.05.010
   Li S.Z., 2005, Handbook of Face Recognition
   LI X, 2006, P 3 CAN C COMP ROB V
   LIN SC, 2003, P INT C COMP VIS
   Martinez A.M., 2003, P IEEE C COMP VIS PA
   Martínez AM, 2002, IEEE T PATTERN ANAL, V24, P748, DOI 10.1109/TPAMI.2002.1008382
   Martínez AM, 2003, VISION RES, V43, P1047, DOI 10.1016/S0042-6989(03)00079-8
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   MILBORROW S, 2008, P EUR C COMP VIS MAR
   Negahdaripour S, 1998, IEEE T PATTERN ANAL, V20, P961, DOI 10.1109/34.713362
   Park U, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P322
   RAMA A, 2005, P INT C IM PROC GEN
   RAMA A, 2006, P INT C MULT EXP TOR
   RAMACHANDRAN M, 2005, P ICASSP MAR, P18
   Ramamoorthi R, 2001, J OPT SOC AM A, V18, P2448, DOI 10.1364/JOSAA.18.002448
   Shashua A, 2001, IEEE T PATTERN ANAL, V23, P129, DOI 10.1109/34.908964
   Teng CH, 2005, COMPUT VIS IMAGE UND, V97, P315, DOI 10.1016/j.cviu.2004.08.002
   Tsai PH, 2005, IEEE SYS MAN CYBERN, P1712
   Yacoob Y, 1996, IEEE T PATTERN ANAL, V18, P636, DOI 10.1109/34.506414
   Yang MH, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P215
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhang YB, 2006, IMAGE VISION COMPUT, V24, P626, DOI 10.1016/j.imavis.2005.08.004
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 37
TC 28
Z9 29
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 600
EP 610
DI 10.1109/TMM.2009.2017606
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900003
DA 2024-07-18
ER

PT J
AU He, YF
   Lee, I
   Guan, L
AF He, Yifen
   Lee, Ivan
   Guan, Ling
TI Distributed Throughput Maximization in P2P VoD Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convex optimization; distributed algorithm; hybrid-forwarding
   architecture; peer-to-peer (P2P) video-on-demand (VoD); throughput
   maximization
ID DECOMPOSITION
AB In peer-to-peer (P2P) video-on-demand (VoD) systems, a scalable source coding is a promising solution to provide heterogeneous peers with different video quality. In this paper, we present a systematic study on the throughput maximization problem in P2P VoD applications. We apply network coding to scalable P2P systems to eliminate the delivery redundancy. Since each peer receives distinct packets, a peer with a higher throughput can reconstruct the video at a higher quality. We maximize the throughput in the existing buffer-forwarding P2P VoD systems using a fully distributed algorithm. We demonstrate in the simulations that the proposed distributed algorithm achieves a higher throughput compared to the proportional allocation scheme or the equal allocation scheme. The existing buffer-forwarding architecture has a limitation in total upload capacity. Therefore we propose a hybrid-forwarding P2P VoD architecture to improve the throughput by combining the buffer-forwarding approach with the storage-forwarding approach. The throughput maximization problem in the hybrid-forwarding architecture is also solved using a fully distributed algorithm. We demonstrate that the proposed hybrid-forwarding architecture greatly improves the throughput compared to the existing buffer-forwarding architecture. In addition, by adjusting the priority weight at each peer, we can implement the differentiated throughput among different users within a video session in the buffer-forwarding architecture, and the differentiated throughput among different video sessions in the hybrid-forwarding architecture.
C1 [He, Yifen; Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   [Lee, Ivan] Univ S Australia, Sch Comp & Informat Sci, Adelaide, SA 5095, Australia.
C3 Toronto Metropolitan University; University of South Australia
RP He, YF (corresponding author), Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
EM yhe@ee.ryerson.ca; lguan@ee.ryerson.ca; Ivan.Lee@unisa.edu.au
RI Lee, Ivan/F-4131-2013
OI Lee, Ivan/0000-0002-2826-6367
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   ANNAPUREDDY S, 2006, P WWW MAY
   [Anonymous], 2003, P ANN ALL C COMM CON
   [Anonymous], P IEEE ICC
   [Anonymous], 1998, J. Oper. Res. Soc.
   [Anonymous], P ICME JUL
   [Anonymous], P ACM NOSSDAV MONT C
   [Anonymous], P IPTPS FEB
   [Anonymous], 2001, Linear Programming: Foundations and extensions, Department of Operations Research and Financial Engineering
   Bertsekas D. P., 2003, CONVEX ANAL OPTIMIZA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Byers JW, 2004, IEEE ACM T NETWORK, V12, P767, DOI 10.1109/TNET.2004.836103
   CHEN B, 2007, P IPTPS FEB
   Chi H., 2006, IEEE J SEL AREA COMM, V3, P1467
   Chiang M, 2007, P IEEE, V95, P255, DOI 10.1109/JPROC.2006.887322
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   Heffeeda M., 2003, P ACM MULTIMEDIA, P45
   Ho T, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P442
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Lee I, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P513
   Li J, 2005, Proceedings of the 3rd Annual Communication Networks and Services Research Conference, P197
   LIANG J, 2006, P SPIE MMCN JAN, V6071
   Narayanan, 2006, P 35 IEEE APPL IMAGE, P2, DOI DOI 10.1109/AIPR.2006.21
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Puri R., 1999, Proceedings of the 33rd Asilomar Confe. Signals, Systems, V1, P342
   Schwarz H, 2004, IEEE IMAGE PROC, P3113
   WU C, 2005, P 13 ANN ACM INT C M, P307
   Wu C., 2005, PROC ACM INT C MULTI, P69
   Wu C, 2007, IEEE J SEL AREA COMM, V25, P222, DOI 10.1109/JSAC.2007.070122
   Wu YN, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P143
   Xu DY, 2002, INT CON DISTR COMP S, P363, DOI 10.1109/ICDCS.2002.1022274
   Xu XF, 2004, IEEE IMAGE PROC, P1759
   YIU WP, 2006, P IEEE ICC JUN, V1, P55
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 34
TC 25
Z9 25
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 509
EP 522
DI 10.1109/TMM.2009.2012921
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300016
DA 2024-07-18
ER

PT J
AU Li, D
   Wu, JP
   Cui, Y
AF Li, Dan
   Wu, Jianping
   Cui, Yong
TI Defending Against Buffer Map Cheating in DONet-Like P2P Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Buffer map cheating; P2P streaming; trustworthy incentive mechanism
ID MULTICAST
AB Data-driven Overlay Network (DONet)-like P2P system is especially suitable to support live stream applications, since its data structure can tolerate node dynamics quite well. However, optimal streaming demands the cooperation of individual nodes. If selfish nodes cheat about their buffer maps to reduce the forwarding burden, the overall streaming quality would be negatively affected. To defend against this kind of cheating, we design a trustworthy service-differentiation based incentive mechanism with low complexity in this paper. The mechanism is composed of the service-differentiation algorithm and the contribution-evaluation algorithm. Compared with other studies in this area, the primary characteristic of our mechanism lies in two aspects. Firstly, the contribution of each node is evaluated considering the features of live streaming, not just by the transferring bytes. Secondly, the potential cheating behavior of overlay nodes during the fulfillment of incentive algorithms can be avoided, which is usually not considered by other similar studies. Extensive simulations suggest that the algorithms are indeed effective for defending against buffer map cheating in DONet-like P2P streaming.
C1 [Li, Dan] Microsoft Res Asia, Beijing 100190, Peoples R China.
   [Wu, Jianping; Cui, Yong] Tsinghua Univ, Dept Comp Sci, Beijing 100080, Peoples R China.
C3 Microsoft; Microsoft Research Asia; Tsinghua University
RP Li, D (corresponding author), Microsoft Res Asia, Beijing 100190, Peoples R China.
EM li.dan@microsoft.com; jianping@cernet.edu.cn;
   cy@csnet1.cs.tsinghua.edu.cn
FU National Natural Science Foundation of China [60873252, 60873253];
   National High Technology Development Program of China [2006AA01Z205];
   National Major Basic Research Program of China [2009CB320501,
   2009CB320503, 2007CB307105]; International S&T Cooperation Program of
   China [2008DFA11630]
FX Manuscript received August 28, 2007: revised October 31, 2008. First
   published March 06, 2009: current version published March 18, 2009. This
   work was supported in part by the National Natural Science Foundation of
   China (60873252, 60873253), in part by the National High Technology
   Development Program of China (2006AA01Z205), in part by the National
   Major Basic Research Program of China (2009CB320501, 2009CB320503,
   2007CB307105). and in part by the International S&T Cooperation Program
   of China (2008DFA11630). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Ishfaq Ahmad.
CR [Anonymous], P ACM SIGCOMM 88 STA
   BANERJEE S, 2003, P ACM SIGMETRICS 03
   BANERJEE S, 2002, P ACM SIGCOMM 02 PIT
   BURAGOHAIN C, 2003, P P2P 03 LINK SWED S
   CHU YH, 2000, P ACM SIGM 00 SANT C
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   Eugster PT, 2004, COMPUTER, V37, P60, DOI 10.1109/MC.2004.1297243
   GOLLE P, 2001, P ACM EC 01 TAMP FL
   HABIB A, 2004, P IWQOS 04 MONTR QC
   JIN S, 2002, P NGC 02 BOST MA OCT
   LI D, 2005, P IEEE GLOBECOM 05 S
   LI D, 2006, P IEEE ICC 06 IST TU
   Liu JC, 2003, IEEE MULTIMEDIA, V10, P22, DOI 10.1109/MMUL.2003.1167919
   MA R, 2004, P ACM SIGMETRICS 04
   MATHY L, 2004, P IEEE INFOCOM 04 HO
   Nowak MA, 1998, NATURE, V393, P573, DOI 10.1038/31225
   SHNEIDMAN J, 2003, P IPTPS 03 BERK CA F
   Sun Y, 2006, IEEE T MULTIMEDIA, V8, P1, DOI 10.1109/TMM.2005.861296
   TRAN D, 2003, P IEEE INFOCOM 03 SA
   Vishnumurthy V., 2003, P WORKSH EC PEER TO
   WANG W, 2005, P IEEE INFOCOM 05 MI
   YUEN S, 2005, P IEEE INFOCOM 05 MI
   ZEGURA EW, 1996, P IEEE INFOCOM 96 SA
   ZHANG X, 2005, P IEEE INFOCOM 05 MI
NR 24
TC 5
Z9 5
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 535
EP 542
DI 10.1109/TMM.2009.2012941
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300018
DA 2024-07-18
ER

PT J
AU Chan, CW
   Wee, S
   Apostolopoulos, J
AF Chan, Carri W.
   Wee, Susie
   Apostolopoulos, John
TI Multiple Distortion Measures for Packetized Scalable Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Embedded packet schedules; H.264/MPEG-4 SVC; JPEG2000; multiple
   distortion measures; rate-distortion optimization; scalable streaming
ID VIDEO; ARCHITECTURES
AB As the diversity in end-user devices and networks grows, it becomes important to be able to efficiently and adaptively serve media content to different types of users. A key question surrounding adaptive media is how to do Rate-Distortion optimized scheduling. Typically, distortion is measured with a single distortion measure, such as the Mean-Squared Error compared to the original high resolution image or video sequence. Due to the growing diversity of users with varying capabilities such as different display sizes and resolutions, we introduce Multiple Distortion Measures (MDM) to account for a diverse range of users and target devices. MDM gives a clear framework with which to evaluate the performance of media systems which serve a variety of users. Scalable coders, such as JPEG2000 and H.264/MPEG-4 SVC, allow for adaptation to be performed with relatively low computational cost. We show that accounting for MDM can significantly improve system performance; furthermore, by combining this with scalable coding, this can be done efficiently. Given these MDM, we propose an algorithm to generate embedded schedules, which enables low-complexity, adaptive streaming of scalable media packets to minimize distortion across multiple users. We show that using MDM achieves up to 4 dB gains for spatial scalability applied to images and 12 dB gains for emporal scalability applied to video.
C1 [Chan, Carri W.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
   [Wee, Susie] Hewlett Packard Corp, Cupertino, CA 95014 USA.
   [Apostolopoulos, John] Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Stanford University; Hewlett-Packard; Hewlett-Packard
RP Chan, CW (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.
EM cwchan@stanford.edu; susie.wee@hp.com; john_apostolopoulos@hp.com
RI Chan, Carri W/J-3237-2016
CR *AHG, 2005, JVTQ007 AHG ISOIEC
   Apostolopoulos JG, 2004, IEEE IMAGE PROC, P1763
   BJORK N, 1998, P IEEE INT C AC SPEE, V5, P2813
   BOZINOVIC N, 2006, P IEEE ICASSP MAY, P29
   Chan C, 2007, INT CONF ACOUST SPEE, P705
   Chan C, 2006, IEEE IMAGE PROC, P785, DOI 10.1109/ICIP.2006.312519
   Chen JJ, 1997, IEEE T CIRC SYST VID, V7, P299
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   ISHTIAQ F, 1999, P IEEE INT C IM PROC, V4, P280
   KALMAN M, 2005, ST J RES, V2, P45
   Kellerer H., 2004, Knapsack Problems. Springer Nature Book Archives Millennium, P317
   LEE JW, 1994, IEEE T IMAGE PROCESS, V3, P513, DOI 10.1109/83.334989
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   PODOLSKY M, 1998, UCBCSD981024
   Reed EC, 2002, IEEE T IMAGE PROCESS, V11, P873, DOI 10.1109/TIP.2002.801122
   Shanableh T, 2000, IEEE T MULTIMEDIA, V2, P101, DOI 10.1109/6046.845014
   Song H, 1999, SIGNAL PROCESS-IMAGE, V15, P127, DOI 10.1016/S0923-5965(99)00027-2
   Song H, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P375, DOI 10.1109/ICIP.1998.723384
   Sun HF, 1996, IEEE T CIRC SYST VID, V6, P191, DOI 10.1109/76.488826
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang Y, 2005, IEEE T CIRC SYST VID, V15, P1270, DOI 10.1109/TCSVT.2005.854224
   WEE SJ, 2003, IEEE INT IMAGE PROCE, V1, P205
   WEE SJ, 1999, P IEEE INT C IM PROC
   Wu XL, 2005, IEEE T IMAGE PROCESS, V14, P2012, DOI 10.1109/TIP.2005.857267
   15444 JPEG 2000 IM 1
NR 25
TC 6
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1671
EP 1686
DI 10.1109/TMM.2008.2007285
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sun, HM
   Chen, CM
   Shieh, CZ
AF Sun, Hung-Min
   Chen, Chien-Ming
   Shieh, Cheng-Zong
TI Flexible-Pay-Per-Channel: A New Model for Content Access Control in
   Pay-TV Broadcasting Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Conditional access system (CAS); pay-TV; access control; channel
   protection; hierarchical key management
ID KEY; MANAGEMENT; EFFICIENT; SCHEME
AB In the past, two models, pay-per-channel (PPC) and pay-per-view (PPV), for content access control in Pay-TV broadcasting systems have been proposed. In PPC, a subscriber subscribes to one or multiple group(s) of channels, prearranged by the service provider, for a fixed period of time. Thus, it is unfair for the subscriber because the subscriber can not subscribe an arbitrary combination of channels according to his/her preference, and can not dynamically unsubscribe/change his/her subscription during the subscription period. On the other hand, PPV specifies that a subscriber can pay for each single program; thus, it provides a fair service. However, compared with PPC, PPV makes the subscriber inconvenient because of the high subscription frequency and low flexibility of channel selection. Moreover, all the existing PPV systems are only suitable for a small-scale Pay-TV system with limited users because of the overhead for transmission and storage. In this paper, we define a new model, Flexible-PPC (F-PPC), which combines the properties and the advantages of PPC and PPV. F-PPC is an improved model of PPC for its fairness. It allows a subscriber to subscribe freely to his/her favorite channels, and unsubscribe/change his/her subscription at any time. Furthermore, it eliminates the inconvenience in PPV, and is suitable for a large-scale Pay-TV system with numerous subscribers and channels. In order to fulfill the requirements of F-PPC, we propose a new conditional access system (CAS) with a four-level key hierarchy. The proposed CAS utilizes a structure of a binary tree to update keys rapidly; therefore, the service provider can promptly update necessary keys when the subscribers change their subscriptions or leave the system. Compared with the existing CAS systems, our CAS is more efficient and flexible to support F-PPC.
C1 [Sun, Hung-Min; Chen, Chien-Ming; Shieh, Cheng-Zong] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
C3 National Tsing Hua University
RP Sun, HM (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM hmsun@cs.nthu.edu.tw; kkyy@is.cs.nthu.edu.tw; barrlak@is.cs.nthu.edu.tw
RI Chen, Chien-Ming/W-2133-2019
OI Chen, Chien-Ming/0000-0002-6502-472X
CR Bürklin H, 2007, ACM SIGCOMM COMP COM, V37, P65, DOI 10.1145/1198255.1198266
   Chen H.-Y, 2004, IEEE T KNOWL DATA EN, V16, P1301, DOI [10.1109/TKDE.2004.59, DOI 10.1109/TKDE.2004.59]
   Chor B, 2000, IEEE T INFORM THEORY, V46, P893, DOI 10.1109/18.841169
   Crinon RJ, 2006, P IEEE, V94, P102, DOI 10.1109/JPROC.2005.861020
   CRUSELLES E, 1993, GLOBECOM '93 COMMUNICATIONS FOR A CHANGING WORLD, CONFERENCE RECORD, P188, DOI 10.1109/GLOCOM.1993.318121
   Di Pietro R, 2003, INT CONF PARA PROC, P397, DOI 10.1109/ICPPW.2003.1240395
   Do TT, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1467, DOI 10.1109/ICC.2004.1312755
   *DVB, 2004, FRAM STRUCT CHANN CO
   Fei ZM, 2005, IEEE T MULTIMEDIA, V7, P942, DOI 10.1109/TMM.2005.854403
   Fiat A., 1993, LECT NOTES COMPUTER, P480, DOI DOI 10.1007/3-540-48329-2
   HOU TW, 2007, P IEEE REG 10 C TENC, P1
   HU A, 2001, P IEEE INFOCOM, V1, P508
   Huang YL, 2004, IEEE T MULTIMEDIA, V6, P760, DOI 10.1109/TMM.2004.834861
   Huang YL, 2000, P INT COMP SOFTW APP, V24, P569, DOI 10.1109/CMPSAC.2000.884782
   *ITU, 1992, 810 ITU
   JANAKIRAMAN RW, 2002, P ANN JOINT C IEEE C, V2, P920
   Jiang TP, 2004, IEEE T CONSUM ELECTR, V50, P225, DOI 10.1109/TCE.2004.1277866
   Kim HW, 2004, MAT SCI SEMICON PROC, V7, P1, DOI 10.1016/j.mssp.2003.12.001
   Kim Y, 2004, IEEE T COMPUT, V53, P905, DOI 10.1109/TC.2004.31
   LEE JW, 1996, P INT C CRYPT INF SE, P82
   Liu BF, 2004, IEEE T CONSUM ELECTR, V50, P632, DOI 10.1109/TCE.2004.1309442
   MACQ BM, 1995, P IEEE, V83, P944, DOI 10.1109/5.387094
   Naor D., 2001, Advances in Cryptology - CRTPTO 2001. 21st Annual International Cryptology Conference, Proceedings (Lecture Notes in Computer Science Vol.2139), P41
   Purandare D, 2007, IEEE T MULTIMEDIA, V9, P1633, DOI 10.1109/TMM.2007.907453
   To V.D., 2003, DRM 03 P 3 ACM WORKS, P67
   Tu FK, 1999, IEEE T CONSUM ELECTR, V45, P151, DOI 10.1109/30.754430
   Tzeng WG, 2002, IEEE T KNOWL DATA EN, V14, P182, DOI 10.1109/69.979981
   Waldvogel M, 1999, IEEE J SEL AREA COMM, V17, P1614, DOI 10.1109/49.790485
   Wang DS, 2006, CHEM-ASIAN J, V1, P91, DOI 10.1002/asia.200600078
   Wong CK, 2000, IEEE ACM T NETWORK, V8, P16, DOI 10.1109/90.836475
NR 30
TC 38
Z9 38
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1109
EP 1120
DI 10.1109/TMM.2008.2001381
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600014
DA 2024-07-18
ER

PT J
AU Wang, JQ
   Duan, LY
   Liu, QS
   Lu, HQ
   Jin, JS
AF Wang, Jinqiao
   Duan, Lingyu
   Liu, Qingshan
   Lu, Hanqing
   Jin, Jesse S.
TI A multimodal scheme for program segmentation and representation in
   broadcast video streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE broadcast video; latent semantic analysis; multimodal fusion; TV program
   segmentation
ID RETRIEVAL
AB With the advance of digital video recording and playback systems, the request for efficiently managing recorded TV video programs is evident so that users can readily locate and browse their favorite programs. In this paper, we propose a multimodal scheme to segment and represent TV video streams. The scheme aims to recover the temporal and structural characteristics of TV programs with visual, auditory, and textual information. In terms of visual cues, we develop a novel concept named program-oriented informative images (POIM) to identify the candidate points correlated with the boundaries of individual programs. For audio cues, a multiscale Kullback-Leibler (K-L) distance is proposed to locate audio scene changes (ASC), and accordingly ASC is aligned with video scene changes to represent candidate boundaries of programs. In addition, latent semantic analysis (LSA) is adopted to calculate the textual content similarity (TCS) between shots to model the inter-program similarity and intra-program dissimilarity in terms of speech content. Finally, we fuse the multimodal features of POIM, ASC, and TCS to detect the boundaries of programs including individual commercials (spots). Towards effective program guide and attracting content browsing, we propose a multimodal representation of individual programs by using POIM images, key frames, and textual keywords in a summarization manner. Extensive experiments are carried out over an open benchmarking dataset TRECVID 2005 corpus and promising results have been achieved. Compared with the electronic program guide (EPG), our solution provides a more generic approach to determine the exact boundaries of diverse TV programs even including dramatic spots.
C1 [Wang, Jinqiao; Liu, Qingshan; Lu, Hanqing] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100080, Peoples R China.
   [Duan, Lingyu] Inst Infocomm Res, Singapore 119613, Singapore.
   [Jin, Jesse S.] Univ Newcastle, Sch Design Commun & Informat Technol, Callaghan, NSW 2308, Australia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Agency for
   Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm
   Research (I2R); University of Newcastle
RP Wang, JQ (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100080, Peoples R China.
EM jqwang@nlpr.ia.ac.cn; lingyu@i2r.a-star.edu.sg; qsliu@nlpr.ia.ac.cn;
   luhq@nlpr.ia.ac.cn; jesse.jin@newcastle.edu.au
FU National Natural Science Foundation [60475010, 60121302, 60675003,
   2006AA01Z315]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 60475010, 60121302, and 60675003, and by the 863
   Program 2006AA01Z315. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Jie Yang.
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   AGNIHOTRI GWL, 2000, P ICME 00, V3, P1345
   Allan J., 1998, P DARPA BROADC NEWS
   [Anonymous], P ACM MM 06
   [Anonymous], P FUT TV AD INSTR YO
   [Anonymous], TRECVID
   [Anonymous], 1986, COMPUTATIONAL APPROA, DOI DOI 10.1109/TPAMI.1986.4767851
   Ardissono L, 2004, HUM-COMPUT INT-SPRIN, P3
   Campbell JP, 1997, P IEEE, V85, P1437, DOI 10.1109/5.628714
   Chua T.-S., 2004, PROC ACMMM, P656, DOI [10.1145/1027527.1027679, DOI 10.1145/1027527.1027679]
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   FISCHER S, 1995, P ACM MM 95
   FRANZ M, 2000, P TDT 3 WORKSH 00
   Gauvain JL, 2002, SPEECH COMMUN, V37, P89, DOI 10.1016/S0167-6393(01)00061-9
   GENTLE JE, 1998, NUMER LINEAR ALGEBRA
   HANJALIC A, 1999, P 3 INT C VISUAL 99
   HAUPTMANN AG, 1999, P ADV DIG LIBR C SAN
   Hawking D., 2004, P ACM SIGIR 04 JUN, P25
   HSU W, 2004, P IS T SPIE S EL IM
   HSU W, 2004, P ICASSP 04 MAY
   Huang JC, 2005, IEEE T MULTIMEDIA, V7, P538, DOI 10.1109/TMM.2005.843346
   Jasinschi RS, 2001, EUROMICRO CONF PROC, P370, DOI 10.1109/EURMIC.2001.952477
   KIM TH, 2002, P ITC CSCC 02 JUL, P1527
   LIU JHZ, 1998, P ICIP 98 OCT, V3, P526
   Lu Lie., 2001, Proceedings of the ninth ACM international conference on Multimedia, P203
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ngo CW, 2002, IEEE T MULTIMEDIA, V4, P446, DOI 10.1109/TMM.2002.802022
   Okamoto H, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P53, DOI 10.1109/ICME.2002.1035716
   Rasheed Z, 2005, IEEE T MULTIMEDIA, V7, P1097, DOI 10.1109/TMM.2005.858392
   Rennert P., 2003, P TRECVID WORKSH
   REYNAR JC, 1994, P 32 ANN M ASS COMP
   ROACH M, 2001, P ICCASP 01 SALT LAK, V3, P7
   SNOEK CG, 2006, P ACM MM 06
   SNOEK CG, 2004, P TRECVID WORKSH 04
   SUNDARAM H, 2002, THESIS COLUMBIA U NE
   Truong BT, 2000, INT C PATT RECOG, P230, DOI 10.1109/ICPR.2000.902901
   Vapnik V., 1999, NATURE STAT LEARNING
   VOGT CC, 1999, INF RETRIEV      OCT
   WANG J, 2006, P PCM 06, P279
   WANG J, 2006, P ICASSP 06 MAY
   WU Y, 2004, P ACM MM 04
   Yeung M., 1996, P 13 INT C PATT REC, V3, P375
   ZHAI Y, 2005, P ICCV05 BEIJ CHIN O, P15
   Zhang DQ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P117
   OPENCREDIT
   CLOSECREDIT
NR 47
TC 17
Z9 21
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 393
EP 408
DI 10.1109/TMM.2008.917362
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100009
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, JJ
   Hu, CJ
   Su, CR
AF Chen, Jiann-Jone
   Hu, Chia-Jung
   Su, Chun-Rong
TI Scalable retrieval and mining with optimal peer-to-peer configuration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based information retrieval and mining; image database;
   multi-instance query; optimal system configuration; peer-to-peer (P2P)
   networks; scalable retrieval and mining
AB We proposed to utilize the scalable peer-to-peer network to perform the content-based image retrieval and mining, Le, P2P-CBIRM. The decentralized unstructured P2P model with certain overheads, i.e., peer clustering and update procedures, is adopted to compromise with the structured one while still reserving flexible routing control when peers join/leave or network fails. The peer CBIRM engine is designed to utilize multi-instance query with multi-feature types to effectively reduce network traffic while maintaining high retrieval accuracy. It helps to enhance the knowledge discovery and image data mining capability. The proposed P2P-CBIRM system provides the scalable retrieval and mining function that the query scope and retrieval accuracy can be adaptively and progressively controlled. To improve the query efficiency (recall-rate/query-scope), it effectively utilizes both: 1) forwarding query message (forward phase) to reduce the query scope and 2) transmitting retrieval results (backward phase) such that activated peers keep filtering high similarity images on the link-path toward the query peer. Experiments show that the query efficiency of the scalable retrieval approach is better than previous methods, i.e., firework query model and breadth-first search. It provides a scalable knowledge discovery platform for efficient image data mining applications. We also proposed to optimally configure the P2P-CBIRM system such that, under a certain number of online users, it would yield the highest recall rate. Simulations demonstrate that, with the optimal configuration, recall rates can be improved to 2.5 to 3 times larger while the network traffic of each peer is reduced to 30% of the original, under the same number of on-line users.
C1 [Chen, Jiann-Jone; Hu, Chia-Jung; Su, Chun-Rong] Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology
RP Chen, JJ (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM jjchen@mail.ntust.edu.tw; m9307304@mail.ntust.edu.tw;
   d9607304@mail.ntust.edu.tw
RI Chen, Jiann-Jone/IVU-6945-2023; Hu, Chia Jung/JXN-1406-2024
OI Hu, Chia Jung/0000-0002-1625-6831
CR ARDIZZONE E, 2006, P IEEE INT S MULT SA
   Bober M, 2001, IEEE T CIRC SYST VID, V11, P716, DOI 10.1109/76.927426
   CHANG FC, 2006, IEICE T INFO SYS E D, V89
   CHEN JJ, 2001, P IEEE PAC RIM C MUL, P285
   Clarke Ian, 2001, Freenet: A Distributed Anonymous Information Storage and Retrieval System, P46, DOI [DOI 10.1007/3-540-44702-4, DOI 10.1007/3-540-44702-4_4, 10.1007/3-540-44702-4_4]
   EISENHARDT M, 2006, P IEEE INT S MULT
   FANNING S, 2007, NAPSTER
   Inaba T, 2006, INTERNATIONAL SYMPOSIUM ON APPLICATIONS AND THE INTERNET , PROCEEDINGS, P45
   *ISO IEC, 2002, JTCISC29WG11N4674 IS
   Jeong S., 1999, P IEEE TENCON, P982
   King I, 2004, ACM T INFORM SYST, V22, P477, DOI 10.1145/1010614.1010619
   Li XQ, 2005, PROC INT CONF PARAL, P277
   Lin TN, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CLUSTER COMPUTING AND THE GRID - CCGRID 2004, P346
   LIU B, 2006, P IEEE C DISTR COMP, P155
   MARAGOS P, 1987, IEEE T ACOUST SPEECH, V35, P1153, DOI 10.1109/TASSP.1987.1165259
   Ratnasamy S, 2002, LECT NOTES COMPUT SC, V2429, P45
   Yang J, 2003, 2003 INTERNATIONAL CONFERENCE ON COMPUTER NETWORKS AND MOBILE COMPUTING, PROCEEDINGS, P247
   YANG KH, 2006, IEICE T COMMUN E B, V89
   Zeinalipour-Yazti D, 2004, COMPUT SCI ENG, V6, P20, DOI 10.1109/MCSE.2004.12
   ZHANG H, 2004, P 3 INT JOINT C AUT, P456
   Zhu Y., 2005, P INT PAR DISTR PROC, p56a
   2007, MORPHEUS V5 4
   2004, GNUTELLA PROTOCOL SP
NR 23
TC 7
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 209
EP 220
DI 10.1109/TMM.2007.911821
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700005
DA 2024-07-18
ER

PT J
AU Lu, MT
   Wu, JC
   Peng, KJ
   Huang, P
   Yao, JJ
   Chen, HH
AF Lu, Meng-Ting
   Wu, Jui-Chieh
   Peng, Kuan-Jen
   Huang, Polly
   Yao, Jason J.
   Chen, Homer H.
TI Design and evaluation of a P2PIPTV system for heterogeneous networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error recovery; IPTV; multiple description; coding; overlay network;
   peer-to-peer (P2P); video broadcast; video streaming
AB NTUStreaming is an overlay P2P-based IPTV system that integrates innovations in both overlay networking and video coding for optimal user experience. The system consists of three key components: partnership formation, robust video coding, and video segment request scheduling. For partnership formation, a graph construction mechanism TYPHOON based on epidemic algorithms is developed to reduce disconnect time and isolated peers. For robust video coding, a multiple description coding (MDC) scheme with spatial-temporal hybrid interpolation (STHI) is proposed to adjust streaming traffic according to the bandwidth and device capability of each peer. For request scheduling, an optimization algorithm is developed by taking the available bandwidth and the video segment type into account. Experimental results show that NTUStreaming is able to deliver optimal video quality in lossy and dynamic networking environments.
C1 Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   Natl Taiwan Univ, Grad Inst Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   Natl Taiwan Univ, Dept Elect Engn, Grad Inst Commun Engn, Taipei 10617, Taiwan.
   Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University; National Taiwan University
RP Huang, P (corresponding author), Natl Taiwan Univ, Grad Inst Commun Engn, Taipei 10617, Taiwan.
EM phuang@cc.ee.ntu.edu.tw; homer@cc.ee.ntu.edu.tw
OI Chen, Homer/0000-0002-8795-1911
CR [Anonymous], P IEEE INT C DEP SYS
   Apostolopoulos JG, 2001, PROC SPIE, V4310, P392
   APOSTOLOPOULOS JG, 1999, P INT WORKSH VER LOW, P168
   Breslau L, 2000, COMPUTER, V33, P59, DOI 10.1109/2.841785
   Cai H, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/45412
   CASTRO M, 2003, P ACM SOSPO3 NEW YOR
   Chen LJ, 2006, COMPUT COMMUN, V29, P1605, DOI 10.1016/j.comcom.2005.07.010
   CUI Y, 2003, P INT WORKSH NETW OP
   Demers Alan, 1987, P 6 ANN ACM S PRINCI, P1, DOI [DOI 10.1145/41840.41841, 10.1145/41840.41841]
   Deng D., 2006, P IEEE INFOCOM BARC
   Estrin D., 1998, PROTOCOL INDEPENDENT
   Floyd S, 1997, IEEE ACM T NETWORK, V5, P784, DOI 10.1109/90.650139
   Ganesh AJ, 2003, IEEE T COMPUT, V52, P139, DOI 10.1109/TC.2003.1176982
   HUANG C, 2007, P ACM SIGCOMM KYOT J
   *ISO IEC, 2000, 144962FPDAM4 ISOIEC
   Khan S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P503, DOI 10.1109/ICME.2004.1394239
   LU MT, 2006, P IEEE PACK VID WORK
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   PADMANABHAN VN, 2003, P IEEE INT C NETW PR, P240
   PURI R, 1999, P ASIL C SIGN SYST C
   RATNASAMY S, 2001, P 3 INT WORKSH NETW, P14
   REICHEL J, 2006, JOINT VIDEO TEAM ITU
   REJAIE R, 2003, P INT WORKSH NETW OP
   SHEN Y, 2005, P IEEE INT C MULT ME
   TAN DA, 2004, IEEE J SEL AREA COMM, P121
   WAITZMAN D, 1988, DIST VECTOR MULTICAS
   WU JC, 2006, P IPTV WORKSH CONJ 1
   XU X, 2005, P IEEE INT C IM PROC, P1759
   Zhang GJ, 2004, IEEE IMAGE PROC, P829
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zink M, 2004, EUROMICRO CONF PROC, P240, DOI 10.1109/EURMIC.2004.1333377
NR 31
TC 25
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1568
EP 1579
DI 10.1109/TMM.2007.907456
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900004
OA Green Published
DA 2024-07-18
ER

PT J
AU Pianese, F
   Perino, D
   Keller, J
   Biersack, EW
AF Pianese, Fabio
   Perino, Diego
   Keller, Joaquin
   Biersack, Ernst W.
TI PULSE: An adaptive, incentive-b ased, unstructured P2P live streaming
   system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE incentives; live streaming; mesh-based; peer-topeer; unstructured
AB Large-scale live media streaming is a challenge for traditional server-based approaches. To appropriately support big audiences, broadcasters must be able to allocate huge bandwidth and computational resources. The costs involved with such an infrastructure exclude all but the established content producers from exploiting the Internet as a distribution medium. Publishers of not-yet-popular content, unless they manage to properly predict their maximum audience size, will likely fail to dimension correctly their broadcast infrastructure. Peer-to-peer systems for live streaming allow the users to support content distribution by contributing their unused resources: this increases the scalability of the content distribution while reducing at the same time the economical burden on the streaming provider. This paper presents and evaluates PULSE, an unstructured mesh-based peer-to-peer system designed to support live streaming to large audiences under the arbitrary resource availability as is typically the case for the Internet. PULSE is a highly dynamic system: it constantly optimizes its mesh of data connections using a feedback-driven peer selection strategy that is based on pairwise incentives. We evaluate the behavior of PULSE under realistic scenarios via simulation and emulation, and present the advantages of our approach, namely a best-effort response to, system-wide resource scarcity, high resilience to node churn, and good hop-count properties of the average data distribution paths.
C1 France Telecom, R&D, Orange Labs, F-9274 Issy Les Moulineaux, France.
   Inst Eurocom, Corporate Communicat Dept, F-06904 Sophia Antipolis, France.
C3 Orange SA
RP Pianese, F (corresponding author), France Telecom, R&D, Orange Labs, 38-40 Rue Gen Leclerc, F-9274 Issy Les Moulineaux, France.
EM fabio.pianese@orange-ftgroup.com; diego.perino@orange-ftgroup.com;
   joaquin.keller@orange-ftgroup.com; erbi@eurecom.fr
CR [Anonymous], P IEEE INFOCOM 05 MA
   BANERJEE S, P ACM SIGCOMM 02
   BHARAMBE A, 2005, P 4 INT WORKSH PEER
   CASTRO M, 2003, P ACM SOSP 03 OCT
   Castro M., 2004, MSRTR200473
   CHU Y, 2004, P USENIX 04 JUN
   CHU YH, 2004, P 14 IEEE NOSSDAV JU
   COHEN B, 2003, P WORKSH EC P2P SYST
   Ganesh AJ, 2003, IEEE T COMPUT, V52, P139, DOI 10.1109/TC.2003.1176982
   GOYAL VK, 2001, IEEE SIGNAL PROCESS
   GROTHOFF C, 2003, WIRTSCHAFTSINFORMATI
   KOSTIC D, 2003, ACM SOSP 03 OCT
   LEGOUT A, 2005, INRIA00000156
   LEGOUT A, 2006, P IMC 06 OCT
   NGUYEN T, 2002, P PACK VID WORKSH AP
   Pai V., 2005, P 4 INT WORKSH PEER
   SAROIU S, 2002, P MULT COMP NETW
   SRIPANDIKULCHAI K, P ACM SIGCOMM 04
   TRAN DA, 2004, IEEE J SEL AREAS COM, V22
   VENKATARAMAN V, 2006, P 14 IEEE ICNP NOV
   WHITE B, 2002, P OSDI 02 DEC
   GRID 5000 GRID PLATF
NR 22
TC 58
Z9 69
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1645
EP 1660
DI 10.1109/TMM.2007.907466
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900010
DA 2024-07-18
ER

PT J
AU Wang, N
   Pavlou, G
AF Wang, Ning
   Pavlou, George
TI Traffic engineered multicast content delivery without MPLS overlay
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE link weight optimization; multicast routing; multimedia content
   delivery; Steiner tree; traffic engineering
ID ALGORITHM
AB Multicast traffic engineering (TE) has recently attracted significant attention given the emergence of point-to-multipoint multimedia content delivery over the Internet. Existing multicast resource provisioning solutions tend to use explicit-routing based TE with multiprotocol label switching (MPLS) tunnels. In this paper, we shift away from this overlay approach and address native IP multicast traffic engineering based on link state routing protocols. The objective is that, through plain Protocol Independent Multicast-Sparse Mode (PIM-SM) shortest path routing with optimized multitopology IGP (MT-IGP) link weights, the resulting multicast trees are geared towards minimal consumption of bandwidth resources. We apply genetic algorithms (GA) to the calculation of optimized MT-IGP link weights that specifically cater for engineered PIM-SM routing with statistical bandwidth guarantees in multimedia content delivery. Our evaluation results show that GA-based multicast traffic engineering consumes significantly less bandwidth in comparison to conventional IP approaches while also exhibiting higher service availability.
C1 Univ Surrey, Ctr Commun Syst Res, Guildford BU2 7XH, Surrey, England.
C3 University of Surrey
RP Wang, N (corresponding author), Univ Surrey, Ctr Commun Syst Res, Guildford BU2 7XH, Surrey, England.
EM n.wang@surrey.ac.uk; g.pavlou@surrey.ac.uk
CR [Anonymous], GT ITM
   Awduche D., 2002, OVERVIEW PRINCIPLES
   CHEUNG S, 1996, P IEEE INFOCOM SAN F
   CUI J, 2002, P IEEE GLOBECOM
   DEERING S, 1989, 1112 RFC
   Ericsson M, 2002, J COMB OPTIM, V6, P299, DOI 10.1023/A:1014852026591
   Fenner B., 2006, Protocol Independent Multicast - Sparse Mode (PIM-SM): Protocol Specification (Revised)
   FORTZ B, 2000, P IEEE INFOCOM TEL A
   Kodialam M, 2003, IEEE ACM T NETWORK, V11, P676, DOI 10.1109/TNET.2003.815302
   Low CP, 2000, COMPUT COMMUN, V23, P1740, DOI 10.1016/S0140-3664(00)00222-X
   OOMS D, 2002, 3353 RFC
   PRZYGIENDA T, 2005, M ISIS MULTI TOPOLOG
   PSENAK P, 2006, UNPUB MT OSPF MULTI
   RETVARI G, 2004, P IFIP NETW
   TAKAHASHI H, MATH JAPONICA, V6, P533
   WANG N, 2004, P IEEE IFIP MAN MULT
   WANG Y, 2001, P IEEE INFOCOM ANCH
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   YANG B, COMPUT COMMUN, V27, P162
   YASUKAWA S, 2004, UNPUB REQUIREMENTS P
NR 20
TC 17
Z9 18
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 619
EP 628
DI 10.1109/TMM.2006.888016
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100016
DA 2024-07-18
ER

PT J
AU Yang, HJ
   Kot, AC
AF Yang, Huijuan
   Kot, Alex C.
TI Pattern-based data hiding for binary image authentication by
   connectivity-preserving
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing
CY MAR 19-23, 2005
CL Philadelphia, PA
SP IEEE
DE authentication; binary images; connectivity-preserving pattern; data
   hiding
ID WATERMARKING
AB In this paper, a novel blind data hiding method for binary images authentication aims at preserving the connectivity of pixels in a local neighborhood is proposed. The "flippability" of a pixel is determined by imposing three transition criteria in a 3 x 3 moving window centered at the pixel. The "embeddability" of a block is invariant in the watermark embedding process, hence the watermark can be extracted without referring to the original image. The "uneven embeddability" of the host image is handled by embedding the watermark only in those "embeddable" blocks. The locations are chosen in such a way that the visual quality of the watermarked image is guaranteed. Different types of blocks are studied and their abilities to increase the capacity are compared. The problem of how to locate the "embeddable" pixels in a block for different block schemes is addressed which facilitates the incorporation of the cryptographic signature as the hard authenticator watermark to ensure integrity and authenticity of the image. Discussions on the security considerations, visual quality against capacity, counter measures against steganalysis and analysis of the computational load are provided. Comparisons with prior methods show superiority of the proposed scheme.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Yang, HJ (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM ehjyang@ntu.edu.sg; eackot@ntu.edu.sg
RI Yang, Huijuan/HQZ-2610-2023
OI Yang, Huijuan/0000-0002-5433-778X
CR Amano T., 1999, Proceedings of the Fifth International Conference on Document Analysis and Recognition. ICDAR '99 (Cat. No.PR00318), P91, DOI 10.1109/ICDAR.1999.791732
   BRASSIL JT, 1995, IEEE J SEL AREA COMM, V13, P1495, DOI 10.1109/49.464718
   Brassil JT, 1999, P IEEE, V87, P1181, DOI 10.1109/5.771071
   BREY BB, INTEL MICROPROCESSOR
   Chen M, 2001, PROC SPIE, V4518, P166, DOI 10.1117/12.448201
   Chotikakamthorn N, 1998, APCCAS '98 - IEEE ASIA-PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P419, DOI 10.1109/APCCAS.1998.743799
   Chotikakamthorn N., 1999, P 1999 INT C, V2, P250
   Cox I., 2001, Digital Watermarking
   Fu MS, 2002, IEEE T IMAGE PROCESS, V11, P477, DOI 10.1109/TIP.2002.999680
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   Huang D, 2001, IEEE T CIRC SYST VID, V11, P1237, DOI 10.1109/76.974678
   Koch E., 1995, IEEE WORKSHOP NONLIN, P1
   Kronsjo Lydia, 1987, ALGORITHMS THEIR COM
   Liu TH, 2004, INT C PATT RECOG, P831
   Lu H., 2003, P 2003 INT S CIRC SY, V3, P806
   Lu HP, 2004, IEEE SIGNAL PROC LET, V11, P228, DOI 10.1109/LSP.2003.821748
   Lu HP, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P300
   Mei Q, 2001, PROC SPIE, V4314, P369, DOI 10.1117/12.435420
   PAN G, 2002, P IEEE INT C AC SPEE, V4, P3469
   Pan HK, 2000, IEEE SYMP COMP COMMU, P750, DOI 10.1109/ISCC.2000.860731
   Tseng YC, 2002, IEEE T COMPUT, V51, P873, DOI 10.1109/TC.2002.1017706
   TSENG YC, 2001, P 20 ANN JOINT C IEE, V2, P887
   Tzeng CH, 2003, IEEE COMMUN LETT, V7, P443, DOI 10.1109/LCOMM.2003.815656
   Wong PW, 2001, IEEE T IMAGE PROCESS, V10, P1593, DOI 10.1109/83.951543
   Wu M, 2004, IEEE T MULTIMEDIA, V6, P528, DOI 10.1109/tmm.2004.830814
   Wu M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P393, DOI 10.1109/ICME.2000.869623
   WU M, 2001, THESIS PRINCETON U P
   YANG H, 2005, P IEEE INT C AC SPEE, V2, P505
   YANG H, 2004, P IEEE INT S CIRC SY, V5, P692
NR 29
TC 91
Z9 107
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 475
EP 486
DI 10.1109/TMM.2006.887990
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100004
DA 2024-07-18
ER

PT J
AU Zhang, C
   Chen, TH
AF Zhang, Cha
   Chen, Tsuhan
TI Active rearranged capturing of image-based rendering scenes - Theory and
   practice
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE active rearranged capturing; active sampling; camera array; image-based
   rendering; self-reconfigurable
AB In this paper, we propose to capture image-based rendering scenes using a novel approach called active rearranged capturing (ARC). Given the total number of available cameras, ARC moves them strategically on the camera plane in order to minimize the sum of squared rendering errors for a given set of light rays to be rendered. Assuming the scene changes slowly so that the optimized camera locations are valid in the next time instance, we formulate the problem as a recursive weighted vector quantization problem, which can be solved efficiently. The ARC approach is verified on both synthetic and real-world scenes. In particular, a large self-reconfigurable camera array is built to demonstrate ARC's performance on real-world scenes. The system renders virtual views at 5-10 frames/s depending on the scene complexity on a moderately equipped computer. Given the virtual view point, the cameras move on a set of rails to perform ARC and improve the rendering quality on the fly.
C1 Microsoft Res, Redmond, WA 98052 USA.
   Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.
C3 Microsoft; Carnegie Mellon University
RP Zhang, C (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM chazhang@microsoft.com; tsuhan@cmu.edu
OI Chen, Tsuhan/0000-0003-3951-7931
CR [Anonymous], POV RAY
   [Anonymous], 1987, Art gallery theorems and algorithms
   Bouguet J., 1999, CAMERA CALIBRATION T
   Buehler C, 2001, COMP GRAPH, P425, DOI 10.1145/383259.383309
   Chai JX, 2000, COMP GRAPH, P307, DOI 10.1145/344779.344932
   FLEISHMAN S, 2000, COMPUT GRAPH FORUM
   KANADE SVT, 1998, CMURITR9834
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   LIN ZC, 2000, P CVPR
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Matusik W, 2000, COMP GRAPH, P369, DOI 10.1145/344779.344951
   More J. J., 1978, Proceedings of the Biennial Conference on numerical analysis, P105
   Naemura T, 2002, IEEE COMPUT GRAPH, V22, P66, DOI 10.1109/38.988748
   NAMBOORI R, 2004, P COMP GRAPH INT
   SCHIRMACHER H, 1999, P EUROGRAPHICS
   *SCOTT EDW EL INC, MINISSC 2
   Shum HY, 2003, IEEE T CIRC SYST VID, V13, P1020, DOI 10.1109/TCSVT.2003.817360
   Shum HY, 1999, COMP GRAPH, P299, DOI 10.1145/311535.311573
   SLABAUGH GG, 2002, HPL200228
   VEDULA S, 2002, P 13 ACM EUR WORKSH
   WERNER T, 1996, P ICPR
   WILBURN B, 2005, P SIGGRAPH
   WILBURN B, 2002, P MED PROC
   YANG JC, 2002, EUR WORKSH REND
   YANG R, 2002, P PAC GRAPH
   Zhang C, 2004, SIGNAL PROCESS-IMAGE, V19, P1, DOI 10.1016/j.image.2003.07.001
   Zhang C, 2003, IEEE T CIRC SYST VID, V13, P1038, DOI 10.1109/TCSVT.2003.817350
   ZHANG C, 2001, AMP0106
   ZHANG C, 2003, P ICME
   ZHANG C, 2003, AMP0302
   ZHANG C, 2004, THESIS CARNEGIE MELL
   ZHANG C, 2003, P VCIP
   ZHANG Z, MSRTR9871
NR 33
TC 11
Z9 11
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 520
EP 531
DI 10.1109/TMM.2006.888010
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100008
DA 2024-07-18
ER

PT J
AU Zhang, Y
   Gao, W
   Lu, Y
   Huang, QM
   Zhao, DB
AF Zhang, Yuan
   Gao, Wen
   Lu, Yan
   Huang, Qingming
   Zhao, Debin
TI Joint source-channel rate-distortion optimization for H.264 video coding
   over error-prone networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error resilience; H.264/MPEG-4 AVC; rate distortion optimization; video
   coding
ID OPTIMAL-MODE SELECTION; COMMUNICATION; TRANSMISSION
AB For a typical video distribution system, the video contents are first compressed and then stored in the local storage or transmitted to the end users through networks. While the compressed videos are transmitted through error-prone networks, error robustness becomes an important issue. In the past years, a number of rate-distortion (R-D) optimized coding mode selection schemes have been proposed for error-resilient video coding, including a recursive optimal per-pixel estimate (ROPE) method. However, the ROPE-related approaches assume integer-pixel motion-compensated prediction rather than subpixel prediction, whose extension to H.264 is not straightforward. Alternatively, an error-robust R-D optimization (ER-RDO) method has been included in H.264 test model, in which the estimate of pixel distortion is derived by simulating decoding process multiple times in the encoder. Obviously, the computing complexity is very high. To address this problem, we propose a new end-to-end distortion model for R-D optimized coding mode selection, in which the overall distortion is taken as the sum of several separable distortion items. Thus, it can suppress the approximation errors caused by pixel averaging operations such as subpixel prediction. Based on the proposed end-to-end distortion model, a new Lagrange multiplier is derived for R-D optimized coding mode selection in packet-loss environment by taking into account of the network conditions. The rate control and complexity issues are also discussed in this paper.
C1 Chinese Acad Sci, Grad Sch, Beijing 100080, Peoples R China.
   Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100080, Peoples R China.
   Microsoft Res Asia, Beijing 100080, Peoples R China.
   Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peking University; Microsoft; Microsoft Research Asia; Harbin
   Institute of Technology
RP Zhang, Y (corresponding author), Chinese Acad Sci, Grad Sch, Beijing 100080, Peoples R China.
EM yzhang@jdl.ac.cn; wgao@jdl.ac.cn; yanlu@microsoft.com;
   qmhuang@jdl.ac.cn; dbzhao@jdl.ac.cn
RI Zhao, Debin/JEP-0204-2023; Huang, Qingming/GLR-3473-2022; Zhang,
   Shiwei/JIY-4344-2023
OI Huang, Qingming/0000-0002-3025-7099; 
CR [Anonymous], H 264 MPEG 4 AVC REF
   [Anonymous], P SPIE VIS COMM IM P
   Côté G, 1999, SIGNAL PROCESS-IMAGE, V15, P25, DOI 10.1016/S0923-5965(99)00022-3
   Côté G, 2000, IEEE J SEL AREA COMM, V18, P952, DOI 10.1109/49.848249
   HASKELL P, 1992, P ICASSP, V3, P545
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P511, DOI 10.1109/TCSVT.2002.800313
   HINDS RO, 1999, THESIS MIT CAMBRIDGE
   Leontaris A, 2003, IEEE DATA COMPR CONF, P63
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   *MICR RES AS, 2003, WIR WIR WID AR NETW
   *MPEG VID GROUP, 2003, 1449652004PDAM6 MPEG
   Stockhammer T, 2002, P PACK VID WORKSH PI
   STOCKHAMMER T, 2002, P IWDC 2002 CAPR IT
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   SU L, 2004, P ICME 2004 TAIB TAI
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wiegand T, 2000, IEEE J SEL AREA COMM, V18, P1050, DOI 10.1109/49.848255
   WIEGAND T, 2001, P ICIP 2001 THESS GR
   Wu DP, 2000, IEEE J SEL AREA COMM, V18, P977, DOI 10.1109/49.848251
   Yang H, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P469
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 23
TC 105
Z9 139
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 445
EP 454
DI 10.1109/TMM.2006.887989
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100001
DA 2024-07-18
ER

PT J
AU Shen, JL
   Shepherd, J
   Ngu, AHH
AF Shen, Jialie
   Shepherd, John
   Ngu, Anne H. H.
TI Towards effective content-based music retrieval with multiple acoustic
   feature combination
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE classification; multimedia database; music retrieval
ID CLASSIFICATION; NETWORKS
AB In this paper, we present a new approach to constructing music descriptors to support efficient content-based music retrieval and classification. The system applies multiple musical properties combined with a hybrid architecture based on principal component analysis (PCA) and a multilayer perceptron neural network. This architecture enables straightforward incorporation of multiple musical feature vectors, based on properties such as timbral texture, pitch, and rhythm structure, into a single low-dimensioned vector that is more effective for classification than the larger individual feature vectors. The use of supervised training enables incorporation of human musical perception that further enhances the classification process. We compare our approach with state of the art techniques and demonstrate its effectiveness on content-based music retrieval. In addition, extensive experimental study illustrates its effectiveness and robustness against various kinds of audio alteration.
C1 Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   SW Texas State Univ, Dept Comp Sci, San Marcos, TX 78666 USA.
C3 University of New South Wales Sydney; Texas State University System;
   Texas State University San Marcos
RP Shen, JL (corresponding author), Univ New S Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM jls@cse.unsw.edu.au; jas@cse.unsw.edu.au; angu@txstate.edu
RI SHEN, Jialie/E-8573-2012; Shen, Jialie/AAX-6851-2020; Shepherd,
   John/A-5941-2016
OI Shepherd, John/0000-0003-1241-4182
CR [Anonymous], P ACM INT C MAN DAT
   [Anonymous], P 5 INT C FDN DAT OR
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   Byrd D, 2002, INFORM PROCESS MANAG, V38, P249, DOI 10.1016/S0306-4573(01)00033-4
   CHAKRABARTI K, 2000, P VLDB C
   CHAKRABARTI SMK, 1999, ICDE, P440
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Clynes Manfred., 1982, Music, Mind, and the Brain: The Neuropsychology of Music
   DOWNIE S, 2000, P 23 ANN INT ACM SIG, P73
   Duda R., 1973, Pattern Classification and Scene Analysis
   Haykin S., 1994, NEURAL NETWORKS, V2
   HECHTNIELSEN R, 1995, SCIENCE, V269, P1860, DOI 10.1126/science.269.5232.1860
   HURON D, 2004, HUMDRUM TOOLKIT REFE
   KARHUNEN J, 1994, NEURAL NETWORKS, V7, P113, DOI 10.1016/0893-6080(94)90060-4
   Kruskal J., 1977, Multidimensional Scaling
   LEE D, 1993, P SPIE STORAGE RETR, V3, P24
   Li GH, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P885, DOI 10.1109/ICME.2000.871501
   LI T, 2003, P 26 ANN INT ACM SIG, P282
   Logan Beth, 2000, ISMIR, V270, P1
   Lu L, 2003, MULTIMEDIA SYST, V8, P482, DOI 10.1007/s00530-002-0065-0
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   NAM U, 2001, P INT S MUS INF RETR
   Pierce JohnR., 1992, SCI MUSICAL SOUND
   Quinlan J.R., 1993, C4 5 PROGRAMS MACHIN
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   SALTON G, 1983, INTRO MODERN INFORMA
   Selfridge-Field E., 1997, MIDI HDB MUSICAL COD
   SHEN J, 2006, P ACM SIGMOD 06 CHIC
   SIROVICH L, 1987, J OPT SOC AM A, V4, P519, DOI 10.1364/JOSAA.4.000519
   Tolonen T, 2000, IEEE T SPEECH AUDI P, V8, P708, DOI 10.1109/89.876309
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Vapnik V. N., 1998, STAT LEARNING THEORY
   Wiles J, 1996, NEURAL COMPUT, V8, P1179, DOI 10.1162/neco.1996.8.6.1179
NR 33
TC 36
Z9 36
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1179
EP 1189
DI 10.1109/TMM.2006.884618
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700008
OA Green Published
DA 2024-07-18
ER

PT J
AU Lu, CS
   Sun, SW
   Hsu, CY
   Chang, PC
AF Lu, Chun-Shien
   Sun, Shih-Wei
   Hsu, Chao-Yong
   Chang, Pao-Chi
TI Media hash-dependent image watermarking resilient against both geometric
   attacks and estimation attacks based on false positive-oriented
   detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE attack; embedding; false positive detection; media hash; mesh;
   robustness; watermark
ID SCHEME; SCALE
AB The major disadvantage of existing watermarking methods is their limited resistance to extensive geometric attacks. In addition, we have found that the weakness of multiple watermark embedding methods that were initially designed to resist geometric attacks is their inability to withstand the watermark-estimation attacks (WEAs), leading to reduce resistance to geometric attacks. In view of these facts, this paper proposes a robust image watermarking scheme that can withstand geometric distortions and WEAs simultaneously. Our scheme is mainly composed of three components: 1) robust mesh generation and mesh-based watermarking to resist geometric distortions; 2) construction of media hash-based content-dependent watermark to resist WEAs; and 3) a mechanism of false positive-oriented watermark detection, which can be used to determine the existence of a watermark so as to achieve a tradeoff between correct detection and false detection. Furthermore, extensive experimental results obtained using the standard benchmark (i.e., Stirmark) and WEAs, and comparisons with relevant watermarking methods confirm the excellent performance of our method in improving robustness. To our knowledge, such a thorough evaluation has not been reported in the literature before.
C1 Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   Natl Cent Univ, Dept Elect Engn, Chungli 320, Taiwan.
C3 Academia Sinica - Taiwan; National Central University
RP Lu, CS (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
EM lcs@iis.sinica.edu.tw
OI Sun, Shih-Wei/0000-0003-2761-7484
CR Alghoniemy M, 2004, IEEE T IMAGE PROCESS, V13, P145, DOI 10.1109/TIP.2004.823831
   Bas P, 2002, IEEE T IMAGE PROCESS, V11, P1014, DOI 10.1109/TIP.2002.801587
   Craver S, 1998, IEEE J SEL AREA COMM, V16, P573, DOI 10.1109/49.668979
   DURSTENFELD R, 1964, COMMUN ACM, V7, P420, DOI 10.1145/364520.364540
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   Hayter A., 1995, PROBABILITY STAT ENG
   Hernández JR, 1999, P IEEE, V87, P1142, DOI 10.1109/5.771069
   HERRIGEL A, 2001, P SPIE SEC WAT MULT, V4314
   Kim HS, 2003, IEEE T CIRC SYST VID, V13, P766, DOI 10.1109/TCSVT.2003.815955
   Knuth D. E., ART COMPUTER PROGRAM, V2
   Kutter M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P320, DOI 10.1109/ICIP.1999.821622
   KUTTER M, 1998, P SPIE INT S VOIC VI
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lu CS, 2005, MULTIMEDIA SYST, V11, P159, DOI 10.1007/s00530-005-0199-y
   Lu CS, 2005, SIGNAL PROCESS-IMAGE, V20, P129, DOI 10.1016/j.image.2004.10.002
   Lu CS, 2004, LECT NOTES COMPUT SC, V2939, P61
   LU CS, 2005, P IEEE INT C MULT EX
   LU CS, 2005, P SPIE SEC STEG WAT, V7, P147
   Manuel A. R., 2002, SIGNAL PROCESS-IMAGE, V17, P611
   Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9
   Nikolaidis A, 2001, IEEE T IMAGE PROCESS, V10, P1726, DOI 10.1109/83.967400
   O'Ruanaidh J., 1998, Signal Processing, V66, P303, DOI DOI 10.1016/S0165-1684(98)00012-7
   Pereira S, 2000, IEEE T IMAGE PROCESS, V9, P1123, DOI 10.1109/83.846253
   Pereira S, 2000, PATTERN RECOGN, V33, P173, DOI 10.1016/S0031-3203(99)00106-5
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   PETITCOLAS FAP, LNCS, V1525, P219
   RAMKUMAR M, 1998, P SPIE MULT SYST APP, V3528, P474
   Seo JS, 2004, PATTERN RECOGN, V37, P1365, DOI 10.1016/j.patcog.2003.12.013
   Simitopoulos D, 2003, IEEE T CIRC SYST VID, V13, P732, DOI 10.1109/TCSVT.2003.815947
   Solachidis V, 2001, IEEE T IMAGE PROCESS, V10, P1741, DOI 10.1109/83.967401
   Stankovic S, 2001, IEEE T IMAGE PROCESS, V10, P650, DOI 10.1109/83.913599
   Tang CW, 2003, IEEE T SIGNAL PROCES, V51, P950, DOI 10.1109/TSP.2003.809367
   Voloshynovskiy S, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P999, DOI 10.1109/ICIP.2001.958294
   Voloshynovskiy S, 2001, SIGNAL PROCESS, V81, P1177, DOI 10.1016/S0165-1684(01)00039-1
   Voloshynovskiy Sviatoslav., 1999, Third International Workshop on Information Hiding, P211
   Zheng D, 2003, IEEE T CIRC SYST VID, V13, P753, DOI 10.1109/TCSVT.2003.815959
   ZHU PF, 1995, IEEE T PATTERN ANAL, V17, P737, DOI 10.1109/34.400564
NR 38
TC 45
Z9 49
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 668
EP 685
DI 10.1109/TMM.2006.876300
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300003
DA 2024-07-18
ER

PT J
AU Li, Y
   Shum, HY
AF Li, Yan
   Shum, Heung-Yeung
TI Learning dynamic audio-visual mapping with input-output hidden Markov
   models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE animation; audio-visual mapping; HMM; IOHMM; learning
AB In this paper, we formulate the problem of synthesizing facial animation from an input audio sequence as a dynamic audio-visual mapping. We propose that audio-visual mapping should be modeled with an input-output hidden Markov model, or IOHMM. An IOHMM is an HMM for which the output and transition probabilities are conditional on the input sequence. We train IOHMMs using the expectation-maximization (EM) algorithm with a novel architecture to explicitly model the relationship between transition probabilities and the input using neural networks. Given an input sequence, the output sequence is synthesized by the maximum likelihood estimation. Experimental results demonstrate that IOHMMs can generate natural and good-quality facial animation sequences from the input audio.
C1 Carnegie Mellon Univ, Dept Comp Engn, Pittsburgh, PA 15213 USA.
   Microsoft Res Asia, Beijing, Peoples R China.
C3 Carnegie Mellon University; Microsoft Research Asia; Microsoft
RP Li, Y (corresponding author), Carnegie Mellon Univ, Dept Comp Engn, Pittsburgh, PA 15213 USA.
EM yanli@andrew.cmu.edu; hshum@microsoft.com
CR BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Bengio Y, 1996, IEEE T NEURAL NETWOR, V7, P1231, DOI 10.1109/72.536317
   Bengio Y., 1999, Neural Computing Surveys, V2
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Brand M, 1999, COMP GRAPH, P21, DOI 10.1145/311535.311537
   Bregler C., 1997, P 24 ANN C COMP GRAP, V31, P353, DOI DOI 10.1145/258734.258880
   Chen T, 1998, P IEEE, V86, P837, DOI 10.1109/5.664274
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fitzgibbon AW, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P662, DOI 10.1109/ICCV.2001.937584
   Guenin BM, 1998, P IEEE SEMICOND THER, P55, DOI 10.1109/STHERM.1998.660387
   Kyoung Ho Choi, 1999, 1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451), P175, DOI 10.1109/MMSP.1999.793816
   LEVINSON SE, 1983, AT&T TECH J, V62, P1035, DOI 10.1002/j.1538-7305.1983.tb03114.x
   Li Y, 2002, ACM T GRAPHIC, V21, P465
   North B, 2000, IEEE T PATTERN ANAL, V22, P1016, DOI 10.1109/34.877523
   PAVLOVIC V, 1999, IEEE INT C COMP VIS
   PAVLOVIC V, 2000, IEEE INT C COMP VIS
   Pearl J., 1988, PROBABILISTIC REASON
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Soatto S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P439, DOI 10.1109/ICCV.2001.937658
   Wolberg G, 1994, Digital image warping
NR 20
TC 24
Z9 26
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 542
EP 549
DI 10.1109/TMM.2006.870732
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000011
DA 2024-07-18
ER

PT J
AU Cheng, H
   Zhang, XM
   Shi, YQ
   Vetro, A
   Sun, HF
AF Cheng, H
   Zhang, XM
   Shi, YQ
   Vetro, A
   Sun, HF
TI Constant quality rate allocation for FGS coding using composite R-D
   analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID VIDEO
AB In this correspondence, we propose a constant quality rate allocation algorithm for fine granularity scaiability (FGS) coded videos. The rate allocation problem is formulated as a constrained minimization of quality fluctuation. The minimization is solved using a composite rate distortion (R-D) analysis. For a set of video frames, a composite R-D curve is first computed and then used for computing the optimal rate allocation. This algorithm is efficient because it is neither iterative nor recursive. After the composite R-D curve is computed, it can he used for optimal rate allocation of any rate budget. Moreover, the composite R-D curve can be updated efficiently over sliding windows. Experimental results have shown both the effectiveness and the efficiency of the proposed algorithm.
C1 Sarnoff Corp, Princeton, NJ 08540 USA.
   SONY Elect Inc, US Adv Technol Ctr, San Jose, CA 95134 USA.
   New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
   Mitsubishi Elect Res Lab, Cambridge, MA 02139 USA.
C3 Sarnoff Corporation; New Jersey Institute of Technology
RP Cheng, H (corresponding author), Sarnoff Corp, Princeton, NJ 08540 USA.
EM hcheng@sarnoff.com; ximin.zhang@am.sony.com; shi@njit.edu;
   avetro@merl.com; hsun@merl.com
RI Shi, Yun/JWP-3360-2024
CR Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   LI W, 2001, IEEE T CIRCUITS SYST, V11, P302
   VANDERSCHAAR M, 2001, P IEEE ICIP 01, V2, P1037
   VANDERSCHAAR M, 2001, IEEE T CIRCUITS SYST, V11, P302
   VIERON J, 2002, P IEEE INT S CIRC SY, V1, P453
   WANG Q, 2001, P IEEE INT S CIRC SY, V2, P397
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
NR 7
TC 2
Z9 2
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 405
EP 407
DI 10.1109/TMM.2005.864351
PG 3
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300019
DA 2024-07-18
ER

PT J
AU Hanjalic, A
AF Hanjalic, A
TI Adaptive extraction of highlights from a sport video based on excitement
   modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE affective video content analysis; video abstraction; video content
   modeling; video content pruning; video highlights extraction
AB This paper addresses the challenge of automatically extracting the highlights from sports TV broadcasts. In particular, we are interested in finding a generic method of highlights extraction, which does not require the development of models for the events that are thought to be interpreted by the users as highlights. Instead, we search for highlights in those video segments that are expected to excite the users most. It is namely realistic to assume that a highlighting event induces a steady increase in a user's excitement, as compared to other, less interesting events. We mimic the expected variations in a user's excitement by observing the temporal behavior of selected audiovisual low-level features and the editing scheme of a video. Relations between this noncontent information and the evoked excitement are drawn partly from psychophysiological research and partly from analyzing the live-video directing practice. The expected variations in a user's excitement are represented by the excitement time curve, which is, subsequently, filtered in an adaptive way to extract the highlights in the prespecified total length and in view of the preferences regarding the highlights strength: extraction can namely be performed with variable sensitivity to capture few "strong" highlights or more "less strong" ones. We evaluate and discuss the performance of our method on the case study of soccer TV broadcasts.
C1 Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Delft Univ Technol, Dept Mediamat, NL-2628 CD Delft, Netherlands.
EM A.Hanjalic@ewi.tudelft.nl
OI Hanjalic, Alan/0000-0002-5771-2549
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   [Anonymous], 1995, P IEEE INT C MULT CO
   BABAGUCHI N, 2003, P IEEE ICIP 2003 BAR
   Chang YL, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P306, DOI 10.1109/MMCS.1996.534992
   Dagtas S, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P91, DOI 10.1109/MMSP.2001.962717
   Detenber B. H., 1997, J BROADCAST ELECTRON, V21, P112
   *DURL RES LTD, 2000, DIG LOC STOR PVR HOM
   EKIN A, 2003, P IEEE ICIP 2003 BAR
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hua W, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P821, DOI 10.1109/ICME.2002.1035908
   JAIMES A, 2002, P IEEE ICIP 2002 ROC, V1
   Kawashima T, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P871, DOI 10.1109/ICIP.1998.723657
   LEONARDI R, 2003, P IEEE ICIP 2003 BAR
   LI B, 2003, P IEEE ICIP 2003 BAR
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   Nitta N, 2000, INT C PATT RECOG, P718
   PAN H, 2001, P IEEE ICASSP SALT L
   Petkovic M, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P817, DOI 10.1109/ICME.2002.1035907
   Picard R. W., 1997, AFFECTIVE COMPUTING
   RUI Y, 2000, P ACM MULT 2000 LOS
   SNOCK CGM, 2003, P IEEE INT C MULT EX, V3, P481
   Sudhir G, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P81, DOI 10.1109/CAIVD.1998.646036
   Utsumi O, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P45, DOI 10.1109/ICME.2002.1035714
   XIE L, 2002, P INT C AC SPEECH SI, V4, P4096
   Xiong ZY, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P632
   XU G, 2003, P IEEE ICIP 2003 BAR
NR 26
TC 92
Z9 101
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1114
EP 1122
DI 10.1109/TMM.2005.858397
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200012
OA Green Published
DA 2024-07-18
ER

PT J
AU De Keukelaere, F
   De Zutter, S
   Van de Walle, R
AF De Keukelaere, F
   De Zutter, S
   Van de Walle, R
TI MPEG-21 Digital Item Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital item; digital item methods; digital item processing; MPEG-21
AB This paper introduces the concept of MPEG-21 Digital Item Processing (DIP), which enables the incorporation of interoperable descriptions of programmability related to the multimedia experience. In the first part of the paper, a motivation for defining DIP is outlined and the different terms and definitions used within DIP are defined and discussed. The second part of the paper gives an overview of the current development status of DIP and discusses the Application Programming Interfaces provided to the Digital Item Method author. This illustrates how DIP can be used to interact between the different parts of MPEG-21. A walkthrough of the DIP process is given enabling the creation of a truly interoperable MPEG-21 DIP peer. Finally, the paper ends with some conclusions and gives some insight on what can be expected for DIP in the future.
C1 Univ Ghent, IBBT, Multimedia Lab, B-9000 Ghent, Belgium.
C3 Ghent University
RP De Keukelaere, F (corresponding author), Univ Ghent, IBBT, Multimedia Lab, B-9000 Ghent, Belgium.
EM frederik.dekeukelaere@ugent.be; saar.dezutter@ugent.be;
   rik.vandewalle@ugent.be
CR [Anonymous], 159385 ISO IEC
   BARLAS C, 2005, IEEE T MULTIMEDIA, V7
   Burnett I, 2003, IEEE MULTIMEDIA, V10, P60, DOI 10.1109/MMUL.2003.1237551
   Burnett IS, 2005, IEEE T MULTIMEDIA, V7, P400, DOI 10.1109/TMM.2005.846789
   *ECMA, 1999, STAND ECMA 262 ECMA
   *IETF RFC, 1996, 2045 IETF RFC
   *ISO IEC, 2004, 2100052004 ISO IEC
   *ISO IEC, 2004, 2100072003 ISO IEC
   *ISO IEC, 2003, 2100010 ISO IEC
   *ISO IEC, 2002, JTC1SC29WG11 ISO IEC
   MANJUNATH B, 2002, INTRO MPEG 7 MULT CO
   PEREIRA F, 2002, MPEG 4 BOOK, P103
   VETRO A, 2005, IEEE T MULTIMEDIA, V7
   Wang X, 2005, IEEE T MULTIMEDIA, V7, P408, DOI 10.1109/TMM.2005.846788
   *WWW CONS, 2002, XHTML 1 0 EXT HYPERT
   *WWW CONS, 2004, DOC OBJ MOD DOM LEV
   *WWW CONS EBR, 2001, SYNCHR MULT INT LANG
   2003, 2100022003 ISO IEC
   2003, 2100010 ISO IEC
   2003, 210006 ISO IEC FDIS
NR 20
TC 8
Z9 8
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 427
EP 434
DI 10.1109/TMM.2005.846792
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200005
DA 2024-07-18
ER

PT J
AU Devillers, S
   Timmerer, C
   Heuer, J
   Hellwagner, H
AF Devillers, S
   Timmerer, C
   Heuer, J
   Hellwagner, H
TI Bitstream syntax description-based adaptation in streaming and
   constrained environments
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE BSD; MPEG-21; MPEG-7 BiM; multimedia content adaptation; SAX; STX;
   transcoding; Universal Multimedia Access; XML; XSLT
AB The seamless access to rich multimedia content on any device and over any network, usually known as Universal Multimedia Access, requires interoperable description tools and adaptation techniques to be developed. To address the latter issue, MPEG-21 Digital Item Adaptation (DIA) introduces the Bitstream Syntax Description (BSD) framework, which provides tools for adapting multimedia content in a generic (i.e., coding format independent) way. The basic idea is to use the eXtensible Markup Language (XML) to describe the high-level structure of a binary media bitstream, to transform its description [e.g., by means of eXtensible Stylesheet Language Transformations (XSLT)], and to construct the adapted media bitstream from the transformed description. This paper presents how this basic BSD framework, initially developed for nonstreamed content and suffering from inherent limitations and high memory consumption of XML-related technologies such as XSLT, can be advanced and efficiently implemented in a streaming environment and on resource-constrained devices. Two different attempts to solve the inherent problems are described. The first approach proposes an architecture based on the streamed processing of Simple Application Programming Interface for XML (SAX) events and adopts Streaming Transformations for XML (STX) as an alternative to XSLT, whereas the second approach breaks a BSD up into well-formed fragments called process units that can be processed individually by a standard XSLT processor. The current status of our work, as well as directions for future research, are given.
C1 France Telecom, R&D Div, Issy Les Moulineaux, France.
   Univ Klagenfurt, Klagenfurt, Austria.
   Siemens Corp Technol, Munich, Germany.
C3 Orange SA; University of Klagenfurt; Siemens AG; Siemens Germany
RP France Telecom, R&D Div, Issy Les Moulineaux, France.
EM Sylvain.Devillers@free.fr; christian.timmerer@itec.uni-klu.ac.at;
   Joerg.Heuer@siemens.com; hermann.hellwagner@itec.uni-klu.ac.at
OI Hellwagner, Hermann/0000-0003-1114-2584
CR AMIELH M, 2002, P 11 INT WWW C WWW20
   BECKER O, 2003, XML EUROPE 2003
   DEVILLERS S, 2003, P 9 INT C PAR DISTR, V2790
   Eleftheriadis A, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P1
   HEUER J, 2002, INTRO MPEG 7 MULTIME, P61
   HONG D, 2002, P IEEE INT C MULT EX
   *ISO IEC, 210007 ISO IEC
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   Panis G, 2003, SIGNAL PROCESS-IMAGE, V18, P721, DOI 10.1016/S0923-5965(03)00061-4
   Scheu S, 2001, BASIC APPL ECOL, V2, P3, DOI 10.1078/1439-1791-00031
   Taubman D, 2003, PROC SPIE, V5150, P791, DOI 10.1117/12.502889
   TIMMERER C, 2003, P SPIE INT S ITCOM 2, V5242
   Vetro A, 2005, IEEE T MULTIMEDIA, V7, P418, DOI 10.1109/TMM.2005.846795
   VETRO A, 2005, IN PRESS MPEG 21 BOO
   *XSLT, 1999, W3C REC
   2000, 154441 ISO IEC
   159381 ISO IEC
NR 17
TC 36
Z9 44
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 463
EP 470
DI 10.1109/TMM.2005.846794
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200009
DA 2024-07-18
ER

PT J
AU Ito, Y
   Tasaka, S
AF Ito, Y
   Tasaka, S
TI Quantitative assessment of user-level QoS and its mapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multiple regression analysis; principal component analysis; psychometric
   methods; quality of service (QoS) mapping; user-level QoS
ID QUALITY
AB This paper proposes a scheme for quantitative assessment of user-level (or perceptual) quality of service (QoS) for audio-video transmission by means of two psychometric methods: the method of paired comparisons and the law of comparative judgment. Moreover, we discuss QoS mapping from application-level QoS to user-level QoS by principal component analysis and multiple regression analysis. In the assessment, we simulate the transmission of an audio-video stream over a loaded network. In order to investigate the effect of the contents on QoS mapping, we treat two types of audio-video streams. By experiment, we demonstrate that our scheme can construct an interval scale as the user-level QoS parameter for each stream and represent it as a function of two application-level QoS parameters with high accuracy. We notice that the multiple regression line depends on the contents. We also propose the concept of control gain by media synchronization, which indicates how much media synchronization control subjectively lightens the average network load.
C1 Nagoya Inst Technol, Grad Sch Engn, Dept Comp Sci & Engn, Nagoya, Aichi 4668555, Japan.
C3 Nagoya Institute of Technology
RP Nagoya Inst Technol, Grad Sch Engn, Dept Comp Sci & Engn, Nagoya, Aichi 4668555, Japan.
EM yoshi@nitech.ac.jp; tasaka@nitech.ac.jp
CR Blakowski G, 1996, IEEE J SEL AREA COMM, V14, P5, DOI 10.1109/49.481691
   Breiman L, 1998, BIOMETRICS, DOI [10.1201/9781315139470, DOI 10.2307/2530946]
   DaSilva LA, 2000, IEEE ICC, P713, DOI 10.1109/ICC.2000.853592
   DIXON WJ, 1984, INTRO STAT ANAL
   Francis-Cobley P, 1998, ICAATM'98: 1998 1ST IEEE INTERNATIONAL CONFERENCE ON ATM, P529, DOI 10.1109/ICATM.1998.688223
   FUKUDA K, 1997, P 5 IFIP INT WORKSH, P291
   Ghinea G., 1998, Proceedings ACM Multimedia 98, P49, DOI 10.1145/290747.290754
   Ghinea G, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P473, DOI 10.1145/319463.319689
   Guilford J.P., 1954, Journal of Educational Psychology, Vsecond
   GULLIKSEN H, 1956, PSYCHOMETRIKA, V21, P125, DOI 10.1007/BF02289093
   Huard J.-F., 1997, P IEEE IFIP 5 INT WO, P303
   ISHIBASHI Y, 1995, IEEE INFOCOM SER, P1010, DOI 10.1109/INFCOM.1995.515977
   JONES BL, 1986, SMPTE J, V95, P1166, DOI 10.5594/J04083
   KUWANO S, 1985, PSYCHOL RES-PSYCH FO, V47, P27, DOI 10.1007/BF00309216
   MOSTELLER F, 1951, PSYCHOMETRIKA, V16, P207
   Mosteller F., 1951, Psychometrika, V16, P3
   Shin J, 2001, IEEE T MULTIMEDIA, V3, P219, DOI 10.1109/6046.923821
   Tasaka S, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, CONFERENCE PROCEEDINGS, P1105, DOI 10.1109/ICC.2002.997023
   Tasaka S, 1997, GLOB TELECOMM CONF, P138, DOI 10.1109/GLOCOM.1997.632527
   Tasaka S, 2000, IEEE ICC, P1535, DOI 10.1109/ICC.2000.853753
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Thurstone LL, 1932, J EXP PSYCHOL, V15, P284, DOI 10.1037/h0075454
   Watson A., 1998, Proceedings ACM Multimedia 98, P55, DOI 10.1145/290747.290755
NR 23
TC 28
Z9 32
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 572
EP 584
DI 10.1109/TMM.2005.846785
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200019
DA 2024-07-18
ER

PT J
AU van der Schaar, M
   Andreopoulos, Y
AF van der Schaar, M
   Andreopoulos, Y
TI Rate-distortion-complexity modeling for network and receiver aware
   adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE complexity modeling; MPEG-21 digital item adaptation; rate-distortion
   optimization; video streaming
AB Existing research on Universal Multimedia Access has mainly focused on adapting multimedia to the network characteristics while overlooking the receiver capabilities. Alternatively, part 7 of the MPEG-21 standard entitled Digital Item Adaptation (DIA) defines description tools to guide the multimedia adaptation process based on both the network conditions and the available receiver resources. In this paper, we propose a new and generic rate-distortion-complexity model that can generate such DIA descriptions for image and video decoding algorithms running on various hardware architectures. The novelty of our approach is in virtualizing complexity, i.e., we explicitly model the complexity involved in decoding a bitstream by a generic receiver. This generic complexity is translated dynamically into "real" complexity, which is architecture-specific. The receivers can then negotiate with the media server/proxy the transmission of a bitstream having a desired complexity level based on their resource constraints. Hence, unlike in previous streaming systems, multimedia transmission can be optimized in an integrated rate-distortion-complexity setting by minimizing the incurred distortion under joint rate-complexity constraints.
C1 Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
EM mvanderschaar@ece.ucdavis.edu; yandreop@etro.vub.ac.be
RI Andreopoulos, Ioannis/C-8377-2009
CR Albonesi DH, 2003, COMPUTER, V36, P49, DOI 10.1109/MC.2003.1250883
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Argenti F, 2002, IEE P-VIS IMAGE SIGN, V149, P152, DOI 10.1049/ip-vis:20020385
   Barbarien J, 2005, SIGNAL PROCESS-IMAGE, V20, P315, DOI 10.1016/j.image.2004.12.006
   CHEN R, 2003, P IEEE INT C CONS EL, P132
   He ZH, 2001, IEEE T CIRC SYST VID, V11, P1221, DOI 10.1109/76.974677
   Hennessy J., 1995, COMPUTER ARCHITECTUR
   Horowitz M, 2003, IEEE T CIRC SYST VID, V13, P704, DOI 10.1109/TCSVT.2003.814967
   LANDGE G, 2005, IN PRESS P IEEE INT
   LI Q, 2002, ISO IEC JTC 1SC 29WG
   McCanne S, 1997, IEEE J SEL AREA COMM, V15, P983, DOI 10.1109/49.611154
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   Narayanan D., 2003, P INT C MOB SYST APP
   Pan W., 2000, Proceedings DCC 2000. Data Compression Conference, P263, DOI 10.1109/DCC.2000.838166
   RAVASI M, 2003, WORKSH POW TIM MOD O, P440
   Reichel J, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P318, DOI 10.1109/ICIP.2001.958115
   Saponara S, 2004, EURASIP J APPL SIG P, V2004, P220, DOI 10.1155/S111086570431019X
   SINGER D, 2001, 1449612002 ISO IEC
   Sow DM, 2003, IEEE T INFORM THEORY, V49, P604, DOI 10.1109/TIT.2002.808135
   SUN M, 2001, COMPRESSED VIDEO OVE
   Valentim J, 2002, IEEE T CIRC SYST VID, V12, P1034, DOI 10.1109/TCSVT.2002.805497
   van der Schaar M, 2000, IEEE T CONSUM ELECTR, V46, P923, DOI 10.1109/30.920442
   van der Schaar M, 2004, GLOB TELECOMM CONF, P639
   VANDERSCHAAR M, 2002, TARGETED SCALABLE MU
   VANDERSCHAAR M, IEEE INT C IM PROC I
   VERDICCHIO F, 2004, P IEEE INT C IM PROC
   Vetro A, 2005, IEEE T MULTIMEDIA, V7, P418, DOI 10.1109/TMM.2005.846795
   WIEGAND T, 2002, JOINT FINAL COMMITTE
   2003, 210007 ISO IEC
   1999, INFORMATION TECHNOLG
NR 31
TC 40
Z9 46
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 471
EP 479
DI 10.1109/TMM.2005.846790
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200010
DA 2024-07-18
ER

PT J
AU Bartsch, MA
   Wakefield, GH
AF Bartsch, MA
   Wakefield, GH
TI Audio thumbnailing of popular music using chroma-based representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio summarization; chroma; feature extraction; musical structure;
   popular music
AB With the growing prevalence of large databases of multimedia content, methods for facilitating rapid browsing of such databases or the results of a database search are becoming increasingly important. However, these methods are necessarily media dependent. We present a system for producing short, representative samples (or "audio thumbnails") of selections of popular music. The system searches for structural redundancy within a given song with the aim of identifying something like a chorus or refrain. To isolate a useful class of features for performing such structure-based pattern recognition, we present a development of the chromagram, a variation on traditional time-frequency distributions that seeks to represent the cyclic attribute of pitch perception, known as chroma. The pattern recognition system itself employs a quantized chromagram that represents the spectral energy at each of the 12 pitch classes. We evaluate the system on a database of popular music and score its performance against a set of "ideal" thumbnail locations. Overall performance is found to be quite good, with the majority of errors resulting from songs that do not meet our structural assumptions.
C1 ATK Mission Res, Beavercreek, OH 45430 USA.
   Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 University of Michigan System; University of Michigan
RP ATK Mission Res, Beavercreek, OH 45430 USA.
EM mbartsch@umich.edu; ghw@umich.edu
CR DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dixon S., 2000, PRICAI 2000. Topics in Artificial Intelligence. 6th Pacific Rim International Conference on Artificial Intelligence. Proceedings (Lecture Notes in Artificial Intelligence Vol.1886), P778
   Foote J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P77, DOI 10.1145/319463.319472
   FOOTE J, 2000, P IEEE INT C MULT EX, V1, P452
   FOOTE J, 1997, AM ASS ARTIFICIAL IN, P1
   GERHARD DB, 1997, 9713 CMPT TR S FRAS
   Hirschberg J., 1999, Proc. of the ESCA/ETRW Workshop on Accessing Information in Spoken Audio, P117
   Kimber D., 1996, P INT C INT FDN N AM, P295
   Logan B, 2000, INT CONF ACOUST SPEE, P749
   MARTIN KD, 1998, ACM MULT WORKSH CONT
   NELSON D, 1995, P IEEE INT C AC SPEE, V2, P1101
   Patterson R.D., 1986, PSYCHOL MUSIC, V14, P44, DOI [10.1177/0305735686141004, DOI 10.1177/0305735686141004]
   Pielemeier WJ, 1996, J ACOUST SOC AM, V99, P2382, DOI 10.1121/1.415426
   Scheirer E, 1997, INT CONF ACOUST SPEE, P1331, DOI 10.1109/ICASSP.1997.596192
   SHEPARD RN, 1964, J ACOUST SOC AM, V36, P2346, DOI 10.1121/1.1919362
   Tzanetakis G., 1999, Proceedings 25th EUROMICRO Conference. Informatics: Theory and Practice for the New Millennium, P61, DOI 10.1109/EURMIC.1999.794763
   Tzanetakis G., 1999, Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA'99 (Cat. No.99TH8452), P103, DOI 10.1109/ASPAA.1999.810860
   Wakefield GH, 1999, P SOC PHOTO-OPT INS, V3807, P637, DOI 10.1117/12.367679
   WELSH M, 1999, UCBCSD001096
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
NR 20
TC 142
Z9 170
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 96
EP 104
DI 10.1109/TMM.2004.840597
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Erol, B
   Kossentini, F
AF Erol, B
   Kossentini, F
TI Shape-based retrieval of video objects
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compressed-domain retrieval; MPEG-4; object-based retrieval; video
   databases; video retrieval
ID IMAGE
AB The increasing availability of object-based video content requires new technologies for automatically extracting and matching of the low level features of arbitrarily shaped video. This paper proposes methods for shape retrieval of arbitrarily shaped video objects. Our methods take into account not only the still shape features but also the shape deformations that may occur in an object's lifespan. In this paper, we compute the shape similarity of video objects by comparing the similarity of their representative temporal instances. We also describe motion of a video object via describing the deformations in an object's shape. Experimental results show that our proposed methods offer very good retrieval performance and match closely with the human ranking.
C1 Ricoh Calif Res Ctr, Menlo Pk, CA 94025 USA.
   Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 Ricoh Company, Ltd.; University of British Columbia
RP Ricoh Calif Res Ctr, Menlo Pk, CA 94025 USA.
EM berna_erol@rii.ricoh.com; faouzi@ece.ubc.ca
CR BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   CHANG R, 2000, P IEEE INT C IM PROC, V2, P546
   Chang SF, 1998, IEEE T CIRC SYST VID, V8, P602, DOI 10.1109/76.718507
   Dagtas S, 2000, IEEE T IMAGE PROCESS, V9, P88, DOI 10.1109/83.817601
   Deng YN, 1998, IEEE T CIRC SYST VID, V8, P616, DOI 10.1109/76.718508
   Erol B, 2000, IEEE T MULTIMEDIA, V2, P129, DOI 10.1109/6046.845016
   Hoey J, 2000, PROC CVPR IEEE, P752, DOI 10.1109/CVPR.2000.855896
   *ISO IEC, 2001, JTC1SC29WG11 ISOIEC
   Kobla V, 1998, J ELECTRON IMAGING, V7, P294, DOI 10.1117/1.482645
   *MPEG 4 VID GROUP, 2000, 144962 MPEG4 ISOIEC
   NDJIKINYA P, 2000, P ISO WG11 MPEG M GE
   Nguyen HT, 2000, IEEE T IMAGE PROCESS, V9, P137, DOI 10.1109/83.817605
   Safar M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P141, DOI 10.1109/ICME.2000.869564
   Salembier P, 1999, IEEE T CIRC SYST VID, V9, P1147, DOI 10.1109/76.809153
   Smolic A, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P271, DOI 10.1109/ICIP.2000.899352
   Stalidis G, 1998, COMPUT CARDIOL, V25, P733, DOI 10.1109/CIC.1998.731978
   *THREEDV SYST, DEPTH SENS
NR 17
TC 13
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 179
EP 182
DI 10.1109/TMM.2004.840607
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300017
DA 2024-07-18
ER

PT J
AU Wah, BW
   Lin, D
AF Wah, BW
   Lin, D
TI LSP-based multiple-description coding for real-time low bit-rate voice
   over IP
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Internet; Line Spectral Pairs; loss concealment; low bit-rate speech
   coding; multiple-description coding; single-description coding; UDP;
   voice over IP
ID SPEECH CODERS; RECOVERY; QUALITY
AB A fundamental issue in real-time interactive voice transmissions over unreliable IP networks is the loss or late arrival of packets for playback. This problem is especially serious when transmitting low bit rate-coded speech with pervasive dependencies introduced. In this case, the loss or late arrival of a single packet will lead to the loss of subsequent dependent frames. In this paper, we study end-to-end loss-concealment schemes for ensuring high quality in playback. We propose a novel multiple description-coding method for concealing packet losses in transmitting low bit rate-coded speech. Based on high correlations observed in linear predictor parameters-in the form of Line Spectral Pairs (LSPs)-of adjacent frames, we generate multiple descriptions in senders by interleaving LSPs, and reconstruct lost LSPs in receivers by linear interpolations. As excitation codewords have low correlations, we further enlarge the segment size for excitation generation and replicate excitation codewords in all the descriptions in order to maintain the same transmission bandwidth. Our proposed scheme can be extended easily to more than two descriptions and can adapt its number of descriptions dynamically to network-loss conditions. Experimental results on FS-1016 CELP, ITU G.723.1, and FS MELP coders show good performance of our scheme.
C1 Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
   Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of Illinois System; University of Illinois Urbana-Champaign
RP Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM wah@manip.crhc.uiuc.edu; dong.lin@oracle.com
CR Anandakumar AK, 2000, INT CONF ACOUST SPEE, P3682, DOI 10.1109/ICASSP.2000.860201
   [Anonymous], P IEEE INFOCOM SAN F
   ATUNGSIRI SA, 1993, IEE PROC-I, V140, P97, DOI 10.1049/ip-i-2.1993.0013
   ATUNGSIRI SA, 1993, P 1993 IEE COLL LOW
   Bolot J.-C., 1993, Journal of High Speed Networks, V2, P305
   Bolot JC, 1996, IEEE INFOCOM SER, P232, DOI 10.1109/INFCOM.1996.497898
   CAMPBELL JH, 1991, BASIC RES CARDIOL, V86, P3
   Chen YL, 1997, IEEE T SPEECH AUDI P, V5, P220, DOI 10.1109/89.568729
   CHOI A, 1989, P 2 IEE NAT C TEL YO, P380
   Cluver K, 1996, PROCEEDINGS OF THE IEEE-SP INTERNATIONAL SYMPOSIUM ON TIME-FREQUENCY AND TIME-SCALE ANALYSIS, P277, DOI 10.1109/TFSA.1996.547467
   Cox RV, 1996, IEEE COMMUN MAG, V34, P34, DOI 10.1109/35.556484
   COX RV, 1989, P INT C AC SPEECH SI, V2, P739
   DASILVA L, 1989, P 8 ANN JOINT C IEEE, V3, P1098
   Erdöl N, 1993, IEEE T SPEECH AUDI P, V1, P295, DOI 10.1109/89.232613
   Goldberg R., 2000, A practical handbook of speech coders
   Hardman V., 1995, P INET 95, P171
   *INT TEL UN, 1996, ITUTG7231
   JAYANT NS, 1981, IEEE T COMMUN, V29, P101, DOI 10.1109/TCOM.1981.1094975
   JIANG W, 2000, P IEEE INT C MULT EX, V1, P444
   Kostas TJ, 1998, IEEE NETWORK, V12, P18, DOI 10.1109/65.660003
   LIN D, 1998, THESIS U ILLINOIS UR
   LIN D, 2002, THESIS U ILLINOIS UR
   MONTMINY C, 2000, INT C MULT EXP 2000, V1, P433
   Rabiner Lawrence, 2010, Digital Processing of Speech Signals
   Sanneck H, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P140, DOI 10.1109/MMCS.1998.693633
   SPANIAS AS, 1994, P IEEE, V82, P1541, DOI 10.1109/5.326413
   SUONG FK, 1984, P IEEE INT C AC SPEE
   Supplee LM, 1997, INT CONF ACOUST SPEE, P1591, DOI 10.1109/ICASSP.1997.596257
   SUZUKI J, 1989, IEEE J SEL AREA COMM, V7, P707, DOI 10.1109/49.32334
   TUCKER RCF, 1985, P IEEE C DIG PROC SI, P227
   Wah BW, 1999, IEEE T MULTIMEDIA, V1, P342, DOI 10.1109/6046.807954
   WAH BW, 2004, Patent No. 6754203
   WANG J, 2001, P IEEE INT C AC SPEE, V2, P745
   Wang JF, 2001, IEEE T MULTIMEDIA, V3, P98, DOI 10.1109/6046.909597
   WANG SH, 1992, IEEE J SEL AREA COMM, V10, P819, DOI 10.1109/49.138987
   WASEM OJ, 1988, IEEE T ACOUST SPEECH, V36, P342, DOI 10.1109/29.1530
   YUITO M, 1989, P 1989 IEEE INT C AC, V1, P381
NR 37
TC 18
Z9 20
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2005
VL 7
IS 1
BP 167
EP 178
DI 10.1109/TMM.2004.840593
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 893CQ
UT WOS:000226697300016
DA 2024-07-18
ER

PT J
AU Jing, X
   Chau, LP
AF Jing, X
   Chau, LP
TI An efficient three-step search algorithm for block motion estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE block matching; motion estimation; video coding
AB The three-step search algorithm has been widely used in block matching motion estimation due to its simplicity and effectiveness. The sparsely distributed checking points pattern in the first step is very suitable for searching large motion. However, for stationary or quasistationary blocks it will easily lead the search to be trapped into a local minimum. In this paper we propose a modification on the three-step search algorithm which employs a small diamond pattern in the first step, and the unrestricted search step is used to search the center area. Experimental results show that the new efficient three-step search performs better than new three-step search in terms of MSE and requires less computation by up to 15% on average.
C1 Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM PG01772596@ntu.edu.sg; elpchau@ntu.edu.sg
RI Chau, Lap-Pui/A-5149-2011
OI Chau, Lap-Pui/0000-0003-4932-0593
CR [Anonymous], 1981, P NAT TEL C NEW ORL
   JAIN JR, 1981, IEEE T COMMUN, V29, P1799, DOI 10.1109/TCOM.1981.1094950
   LI RX, 1994, IEEE T CIRC SYST VID, V4, P438, DOI 10.1109/76.313138
   Liu LK, 1996, IEEE T CIRC SYST VID, V6, P419, DOI 10.1109/76.510936
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Tham JY, 1998, IEEE T CIRC SYST VID, V8, P369, DOI 10.1109/76.709403
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
NR 7
TC 120
Z9 138
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 435
EP 438
DI 10.1109/TMM.2004.827517
PG 4
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200006
DA 2024-07-18
ER

PT J
AU Chim, J
   Lau, RWH
   Leong, HV
   Si, A
AF Chim, J
   Lau, RWH
   Leong, HV
   Si, A
TI CyberWalk: A web-based distributed virtual walkthrough environment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed virtual environments; model prefetching; multiresolution
   caching; multiresolution modeling; virtual walkthrough
AB A distributed virtual walkthrough environment allows users connected to the geometry server to walk through a specific place of interest, without having to travel physically. This place of interest may be a virtual museum, virtual library or virtual university. There are two basic approaches to distribute the virtual environment from the geometry server to the clients, complete replication and on-demand transmission. Although the on-demand transmission approach saves waiting time and optimizes network usage, many technical issues need to be addressed in order for the system to be interactive.
   CyberWalk is a web-based distributed virtual walkthrough system developed based on the on-demand transmission approach. It achieves the necessary performance with a multiresolution caching mechanism. First, it reduces the model transmission and rendering times by employing a progressive multiresolution modeling technique. Second, it reduces the Internet response time by providing a caching and prefetching mechanism. Third, it allows a client to continue to operate, at least partially, when the Internet is disconnected. The caching mechanism of CyberWalk tries to maintain at least a minimum resolution of the object models in order to provide at least a coarse view of the objects to the viewer. All these features allow CyberWalk to provide sufficient interactivity to the user for virtual walkthrough over the Internet environment.
   In this paper, we demonstrate the design and implementation of CyberWalk. We investigate the effectiveness of the multiresolution caching mechanism of CyberWalk in supporting virtual walkthrough applications in the Internet environment through numerous experiments, both on the simulation system and on the prototype system.
C1 Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
   City Univ Hong Kong, Dept Comp Sci & Comp Engn & Informat Technol, Kowloon, Hong Kong, Peoples R China.
   Oracle Corp, Redwood Shores, CA 94065 USA.
C3 Hong Kong Polytechnic University; City University of Hong Kong; Oracle
RP Sch Visual Arts, New York, NY 10010 USA.
EM rynson@cs.cityu.edu.hk
OI LEONG, Hong-va/0000-0001-7682-9032; LAU, Rynson W H/0000-0002-8957-8129
CR [Anonymous], 1997, PERS TECHNOL
   [Anonymous], 1987, Concurrency Control and Recovery in Database Systems
   [Anonymous], P EUR
   CALVIN J, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P450, DOI 10.1109/VRAIS.1993.380745
   CAREY M, 1991, P ACM SIGMOD INT C M, P357
   CARLSSON C, 1993, IEEE VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM, P394, DOI 10.1109/VRAIS.1993.380753
   Chan BYL, 1999, DISTRIB PARALLEL DAT, V7, P343, DOI 10.1023/A:1008738928499
   CHIM J, 1998, P ACM MULTIMEDIA SEP
   Chim JHP, 1998, P IEEE INT FORUM RES, P66, DOI 10.1109/ADL.1998.670381
   EFFELSBERG W, 1984, ACM T DATABASE SYST, V9, P560, DOI 10.1145/1994.2022
   FALBY JS, 1993, COMPUT GRAPH, V17, P65, DOI 10.1016/0097-8493(93)90052-B
   FRANKLIN MJ, 1992, PROC INT CONF VERY L, P596
   GREENHALGH C, 1995, INT CON DISTR COMP S, P27, DOI 10.1109/ICDCS.1995.499999
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   ICSLER V, 1996, P ACM VRST JUL, P11
   Kraiss A, 1998, VLDB J, V7, P141, DOI 10.1007/s007780050060
   Lau RWH, 1997, P IEEE VIRT REAL ANN, P20, DOI 10.1109/VRAIS.1997.583040
   Lau RWH, 1998, PRESENCE-TELEOP VIRT, V7, P22, DOI 10.1162/105474698565505
   Macedonia M. R., 1995, Proceedings. Virtual Reality Annual International Symposium '95 (Cat. No.95CH35761), P2, DOI 10.1109/VRAIS.1995.512473
   Mannoni B, 1997, COMMUN ACM, V40, P61, DOI 10.1145/260750.260772
   Ohshima T, 1996, P IEEE VIRT REAL ANN, P103, DOI 10.1109/VRAIS.1996.490517
   SCHMALSTIEG D, 1996, P EUR 96, P421
   Serrin J., 1994, Topol. Methods Nonlinear Anal., V3, P1, DOI [10.12775/TMNA.1994.001, DOI 10.12775/TMNA.1994.001]
   Silberschatz A., 1996, Database System Concepts
   Singhal S., 1999, Networked Virtual Environments
   WATSON B, 1996, P ACM CHI 96 APR, P227
   WONG WMR, 2000, P ACM SIGMET RICS JU
NR 27
TC 55
Z9 71
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 503
EP 515
DI 10.1109/TMM.2003.819094
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700002
OA Green Accepted, Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Chen, H
   Wei, XS
   Xiao, L
AF Chen, Hao
   Wei, Xiu-Shen
   Xiao, Liang
TI Prototype Learning for Automatic Check-Out
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic check-out; prototype learning; classifier learning; prototype
   alignment; discriminative re-ranking
ID OPTIMIZATION; MULTICLASS; NETWORKS; SHAPE
AB The basic goal of Automatic Check-Out (ACO) task is to accurately predict the categories and quantities of products selected by customers in the check-out images. However, there is a significant domain gap between the single-product exemplars as training data and the check-out images as testing data. To mitigate the domain gap, we propose a novel method termed as Prototype Learning for Automatic Check-Out (PLACO). In PLACO, prototype learning is designed to reach the goal in two ways. Specifically, in the prototype-based classifier learning module, to fully exploit the invariance of category prototypes, the prototypes obtained from the single-product exemplars are employed to generate classifiers for classifying the proposals of check-out image. On the other side, in prototype alignment module, prototypes for both the single-product exemplar and check-out image domains are entered simultaneously to ensure intra-category compactness and inter-category sparsity. Moreover, to further improve the performance of PLACO, we develop a discriminative re-ranking module to both adjust the predicted scores of product proposals for bringing more discriminative ability in classifier learning and provide a reasonable sorting possibility by considering the fine-grained nature. Experiments are conducted on the large-scale RPC dataset for evaluations. Our PLACO obtains the optimal results in both traditional ACO task setting and incremental task setting.
C1 [Chen, Hao; Wei, Xiu-Shen; Xiao, Liang] Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.
   [Chen, Hao; Wei, Xiu-Shen] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Nanjing University of Science & Technology; Xidian University
RP Wei, XS; Xiao, L (corresponding author), Nanjing Univ Sci & Technol, Nanjing 210094, Peoples R China.
EM chenh@njust.edu.cn; weixs.gm@gmail.com; xiaoliang@mail.njust.edu.cn
RI xiao, liang/G-2968-2010
OI xiao, liang/0000-0003-0178-9384; Wei, Xiu-Shen/0000-0002-8200-1845
FU National Key Ramp;D Program of China
FX No Statement Available
CR Aishan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P395, DOI 10.1007/978-3-030-58601-0_24
   Arik SO, 2020, J MACH LEARN RES, V21
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Chen CF, 2019, ADV NEUR IN, V32
   Chen CQ, 2021, PROC CVPR IEEE, P12571, DOI 10.1109/CVPR46437.2021.01239
   Chen H, 2022, LECT NOTES COMPUT SC, V13685, P277, DOI 10.1007/978-3-031-19806-9_16
   Chen H, 2022, IEEE T IMAGE PROCESS, V31, P3004, DOI 10.1109/TIP.2022.3163527
   Chen K, 2019, Arxiv, DOI arXiv:1906.07155
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P1558, DOI 10.1109/TMM.2021.3067439
   Follmann P, 2018, LECT NOTES COMPUT SC, V11214, P581, DOI 10.1007/978-3-030-01249-6_35
   Frontoni E, 2013, LECT NOTES COMPUT SC, V8158, P509, DOI 10.1007/978-3-642-41190-8_55
   Fu ZR, 2022, IEEE T CIRC SYST VID, V32, P4736, DOI 10.1109/TCSVT.2021.3124908
   Gao TY, 2019, AAAI CONF ARTIF INTE, P6407
   George M, 2014, LECT NOTES COMPUT SC, V8690, P440, DOI 10.1007/978-3-319-10605-2_29
   Georgiadis K., 2021, P 14 PERVASIVE TECHN, P1
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jund P, 2016, Arxiv, DOI arXiv:1611.05799
   Kozerawski J, 2018, PROC CVPR IEEE, P3446, DOI 10.1109/CVPR.2018.00363
   Lapin M, 2018, IEEE T PATTERN ANAL, V40, P1533, DOI 10.1109/TPAMI.2017.2751607
   Lee H, 2022, IEEE WINT CONF APPL, P1101, DOI 10.1109/WACV51458.2022.00117
   Li CC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2152, DOI 10.1145/3343031.3350989
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu AA, 2022, IEEE T CYBERNETICS, V52, P13862, DOI 10.1109/TCYB.2021.3139927
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Merler M, 2007, PROC CVPR IEEE, P3634
   Nie WZ, 2022, IEEE T CIRC SYST VID, V32, P992, DOI 10.1109/TCSVT.2021.3070969
   Paolanti M, 2018, J INTELL ROBOT SYST, V91, P165, DOI 10.1007/s10846-017-0674-7
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sciucca Laura, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12662), P534, DOI 10.1007/978-3-030-68790-8_42
   Shermin T, 2021, IEEE T MULTIMEDIA, V23, P2732, DOI 10.1109/TMM.2020.3016126
   Snell J, 2017, ADV NEUR IN, V30
   Tychsen-Smith L, 2018, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR.2018.00719
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Viéville T, 2004, J COMPUT NEUROSCI, V17, P271, DOI 10.1023/B:JCNS.0000044873.20850.9c
   Wang C., 2021, NeurIPS, V34, P2936
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37
   Wei XS, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-022-3513-y
   Wei XS, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3489-1
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Wei XS, 2019, IEEE T IMAGE PROCESS, V28, P6116, DOI 10.1109/TIP.2019.2924811
   Wei Xiu-Shen, 2021, Advances in Neural Information Processing Systems, V34, P5720
   Wu A., 2021, P IEEE CVF INT C COM, P9567
   Xu W., 2020, NEURIPS, V33, P21969
   Yang HM, 2018, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2018.00366
   Yang YD, 2021, IEEE WINT CONF APPL, P626, DOI 10.1109/WACV48630.2021.00067
   Yeh MC, 2020, IEEE T PATTERN ANAL, V42, P1530, DOI 10.1109/TPAMI.2019.2911065
   Yue Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10183, DOI 10.1109/CVPR42600.2020.01020
   Zhan XL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11762, DOI 10.1109/ICCV48922.2021.01157
   Zhang J, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108469
   Zhang LB, 2021, IEEE T MULTIMEDIA, V23, P4158, DOI 10.1109/TMM.2020.3037502
   Zhao LY, 2019, IEEE IMAGE PROC, P3891, DOI [10.1109/ICIP.2019.8803536, 10.1109/icip.2019.8803536]
   Zhao-Min Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P633, DOI 10.1007/978-3-030-58589-1_38
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu P., 2019, P INT C MACH LEARN, P7643
NR 67
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9147
EP 9160
DI 10.1109/TMM.2023.3247219
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200006
DA 2024-07-18
ER

PT J
AU Du, YH
   Zhao, ZC
   Song, Y
   Zhao, YY
   Su, F
   Gong, T
   Meng, HY
AF Du, Yunhao
   Zhao, Zhicheng
   Song, Yang
   Zhao, Yanyun
   Su, Fei
   Gong, Tao
   Meng, Hongying
TI StrongSORT: Make DeepSORT Great Again
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-object tracking; baseline; AFLink; GSI
ID MULTIPLE-OBJECT TRACKING; VISION
AB Recently, Multi-Object Tracking (MOT) has attracted rising attention, and accordingly, remarkable progresses have been achieved. However, the existing methods tend to use various basic models (e.g, detector and embedding model), and different training or inference tricks, etc. As a result, the construction of a good baseline for a fair comparison is essential. In this paper, a classic tracker, i.e., DeepSORT, is first revisited, and then is significantly improved from multiple perspectives such as object detection, feature embedding, and trajectory association. The proposed tracker, named StrongSORT, contributes a strong and fair baseline for the MOT community. Moreover, two lightweight and plug-and-play algorithms are proposed to address two inherent "missing" problems of MOT: missing association and missing detection. Specifically, unlike most methods, which associate short tracklets into complete trajectories at high computation complexity, we propose an appearance-free link model (AFLink) to perform global association without appearance information, and achieve a good balance between speed and accuracy. Furthermore, we propose a Gaussian-smoothed interpolation (GSI) based on Gaussian process regression to relieve the missing detection. AFLink and GSI can be easily plugged into various trackers with a negligible extra computational cost (1.7 ms and 7.1 ms per image, respectively, on MOT17). Finally, by fusing StrongSORT with AFLink and GSI, the final tracker (StrongSORT++) achieves state-of-the-art results on multiple public benchmarks, i.e., MOT17, MOT20, DanceTrack and KITTI. Codes are available at https://github.com/dyhBUPT/StrongSORT and https://github.com/open-mmlab/mmtracking.
C1 [Du, Yunhao; Zhao, Zhicheng; Song, Yang; Zhao, Yanyun; Su, Fei] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst & Network Culture, Beijing 100088, Peoples R China.
   [Gong, Tao] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230052, Peoples R China.
   [Gong, Tao] Shanghai AI Lab, Shanghai, Peoples R China.
   [Meng, Hongying] Brunel Univ London, Coll Engn Design & Phys Sci, Uxbridge UB8 3PH, England.
C3 Beijing University of Posts & Telecommunications; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Shanghai
   Artificial Intelligence Laboratory; Brunel University
RP Zhao, ZC (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing Key Lab Network Syst & Network Culture, Beijing 100088, Peoples R China.
EM dyh_bupt@bupt.edu.cn; zhaozc@bupt.edu.cn; sy12138@bupt.edu.cn;
   zyy@bupt.edu.cn; sufei@bupt.edu.cn; gongtao@pjlab.org.cn;
   hongying.meng@brunel.ac.uk
RI Meng, Hongying/O-5192-2014
OI Meng, Hongying/0000-0002-8836-1382; Zhao, Zhicheng/0000-0001-6506-7298
FU National Natural Science Foundation of China
FX No Statement Available
CR Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Bochinski Erik, 2017, 2017 14th IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), DOI 10.1109/AVSS.2017.8078516
   Bochinski E, 2018, 2018 15TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), P435
   Brasó G, 2020, PROC CVPR IEEE, P6246, DOI 10.1109/CVPR42600.2020.00628
   Cao J, 2022, ARXIV
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Dendorfer P., 2020, arXiv
   Du Y, 2022, IEEE INT CONF MULTI, P1, DOI DOI 10.1109/ICMEW56448.2022.9859481
   Du YH, 2021, IEEE INT CONF COMP V, P2809, DOI 10.1109/ICCVW54120.2021.00315
   Ess A, 2008, PROC CVPR IEEE, P1857
   Evangelidis GD, 2008, IEEE T PATTERN ANAL, V30, P1858, DOI 10.1109/TPAMI.2008.113
   Llorca DF, 2021, IET INTELL TRANSP SY, V15, P987, DOI 10.1049/itr2.12079
   Fu ZY, 2019, IEEE T MULTIMEDIA, V21, P2277, DOI 10.1109/TMM.2019.2902480
   Gao J., 2018, arXiv
   Gao TZ, 2022, IEEE T MULTIMEDIA, V24, P995, DOI 10.1109/TMM.2021.3062489
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Han SD, 2022, NEUROCOMPUTING, V476, P75, DOI 10.1016/j.neucom.2021.12.104
   He JW, 2021, PROC CVPR IEEE, P5295, DOI 10.1109/CVPR46437.2021.00526
   Hofmann M, 2013, IEEE INT W PERFORM, P22, DOI 10.1109/PETS.2013.6523791
   Hu HN, 2023, IEEE T PATTERN ANAL, V45, P1992, DOI 10.1109/TPAMI.2022.3168781
   Kalman R E., 1960, J BASIC ENG, V82, P35, DOI DOI 10.1115/1.3662552
   Khurana T., 2021, IEEE INT C COMP VIS, P3174
   Kingma D. P., 2014, arXiv
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Naiel MA, 2017, COMPUT VIS IMAGE UND, V154, P94, DOI 10.1016/j.cviu.2016.07.003
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Peng JL, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107480
   Perera AGA, 2006, IEEE COMP SOC C COMP, P666
   Possegger H, 2014, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR.2014.170
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Shao S, 2018, Arxiv, DOI [arXiv:1805.00123, DOI 10.48550/ARXIV.1805.00123]
   Stadler D., 2021, 2021 17 IEEE INT C A, P1, DOI DOI 10.1109/AVSS52988.2021.9663836
   Stadler D, 2022, IEEE WINT CONF APPL, P133, DOI 10.1109/WACVW54805.2022.00019
   Sun PZ, 2022, PROC CVPR IEEE, P20961, DOI 10.1109/CVPR52688.2022.02032
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Tokmakov P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10840, DOI 10.1109/ICCV48922.2021.01068
   Wang B, 2017, IEEE T PATTERN ANAL, V39, P589, DOI 10.1109/TPAMI.2016.2551245
   Wang GA, 2023, IEEE T MULTIMEDIA, V25, P1256, DOI 10.1109/TMM.2022.3140919
   Wang GA, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9856, DOI 10.1109/ICCV48922.2021.00973
   Wang GA, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P482, DOI 10.1145/3343031.3350853
   Wang Q, 2021, PROC CVPR IEEE, P3875, DOI 10.1109/CVPR46437.2021.00387
   Weng XS, 2020, IEEE INT C INT ROBOT, P10359, DOI 10.1109/IROS45743.2020.9341164
   Williams CKI, 1996, ADV NEUR IN, V8, P514
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Xu YH, 2020, PROC CVPR IEEE, P6786, DOI 10.1109/CVPR42600.2020.00682
   Yang F, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104091
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhu YD, 2019, MULTIMED TOOLS APPL, V78, P817, DOI 10.1007/s11042-018-6163-6
NR 67
TC 64
Z9 66
U1 47
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8725
EP 8737
DI 10.1109/TMM.2023.3240881
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000069
OA Green Submitted, Green Published
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Ghose, S
   Prevost, JJ
AF Ghose, Sanchita
   Prevost, John J.
TI FoleyGAN: Visually Guided Generative Adversarial Network-Based
   Synchronous Sound Generation in Silent Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep neural network; foley generation; generative adversarial network;
   multi-modal learning; sound synthesis; video class prediction; visual
   guidance; visual-to-sound
ID AUDIO GENERATION; SEARCH
AB Deep learning based visual-to-sound generation systems have been developed that identify and create audio features from video signals. However, these techniques often fail to consider the time-synchronicity of the visual and audio features. In this paper we introduce a novel method for guiding a class-conditioned GAN to synthesize representative audio with temporally-extracted visual information. We accomplish this visual-to-sound generation task by adapting the synchronicity traits between the audio-visual modalities. Our proposed FoleyGAN model is capable of conditioning action sequences of visual events leading to the generation of visually aligned realistic soundtracks. We expanded our previously proposed Automatic Foley data set. We evaluated FoleyGAN's synthesized sound output through human surveys that show noteworthy (on average 81%) audio-visual synchronicity performance. Our approach outperforms other baseline models and audio-visual data sets in statistical and ablation experiments achieving improved IS, FID and NDB scores. In ablation analysis we showed the significance of our visual and temporal feature extraction method as well as augmented performance of our generation network. Overall, our FoleyGAN model showed sound retrieval accuracy of 76.08% surpassing existing visual-to-audio synthesis deep neural networks.
C1 [Ghose, Sanchita; Prevost, John J.] Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Prevost, JJ (corresponding author), Univ Texas San Antonio, Dept Elect & Comp Engn, San Antonio, TX 78249 USA.
EM sanchita.ghose@my.utsa.edu; jeff.prevost@utsa.edu
OI Prevost, John/0000-0002-2303-599X; Ghose, Sanchita/0000-0003-0883-5718
FU Open Cloud Institute at UTSA
FX This work was supported by the Open Cloud Institute at UTSA. The
   Associate Editor coordinating the review of this manuscript and
   approving itfor publication was Prof. Susanto Rahardja
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   ALLEN JB, 1977, IEEE T ACOUST SPEECH, V25, P235, DOI 10.1109/TASSP.1977.1162950
   Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Aytar Y, 2016, ADV NEUR IN, V29
   Bolia RS, 1999, HUM FACTORS, V41, P664, DOI 10.1518/001872099779656789
   Brock A., 2019, INT C LEARN REPR
   Chen K., 2018, P EUR C COMP VIS WOR
   Chen LL, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P349, DOI 10.1145/3126686.3126723
   Chen PH, 2020, IEEE T IMAGE PROCESS, V29, P8292, DOI 10.1109/TIP.2020.3009820
   CROCHIERE RE, 1980, IEEE T ACOUST SPEECH, V28, P99, DOI 10.1109/TASSP.1980.1163353
   Donahue C., 2019, Adversarial audio synthesis
   Donahue C., 2018, Adversarial audio synthesis
   Donahue J, 2019, Advances in Neural Information Processing Systems
   Donahue J., 2017, ICLR, P1, DOI DOI 10.48550/ARXIV.1605.09782
   Donahue J., 2016, arXiv
   Dumoulin V, 2017, 5 INT C LEARN REPR I
   Engel J., 2019, P INT C LEARN REPR, P1
   Gao RH, 2019, PROC CVPR IEEE, P324, DOI 10.1109/CVPR.2019.00041
   GAVER WW, 1993, ECOL PSYCHOL, V5, P1, DOI 10.1207/s15326969eco0501_1
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghose S, 2021, IEEE T MULTIMEDIA, V23, P1895, DOI 10.1109/TMM.2020.3005033
   Ghose S, 2020, 2020 IEEE 15TH INTERNATIONAL CONFERENCE OF SYSTEM OF SYSTEMS ENGINEERING (SOSE 2020), P563, DOI [10.1109/SoSE50414.2020.9130483, 10.1109/sose50414.2020.9130483]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haque KN, 2020, IEEE ACCESS, V8, P223509, DOI 10.1109/ACCESS.2020.3040797
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Iashin V., 2021, P BRIT MACH VIS C
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kingma D. P., 2014, arXiv
   Kong J., 2020, ADV NEURAL INFORM PR, V33, P17022
   Kumar K, 2019, ADV NEUR IN, V32
   Li D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201391
   Liu SG, 2022, IEEE T CIRC SYST VID, V32, P1299, DOI 10.1109/TCSVT.2021.3079897
   Liu SG, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323045
   Majdak P, 2010, ATTEN PERCEPT PSYCHO, V72, P454, DOI 10.3758/APP.72.2.454
   Marafioti A., 2019, P MACHINE LEARNING R, P4352
   Morgado P, 2018, ADV NEUR IN, V31
   Owens A, 2018, INT J COMPUT VISION, V126, P1120, DOI 10.1007/s11263-018-1083-5
   Owens A, 2016, PROC CVPR IEEE, P2405, DOI 10.1109/CVPR.2016.264
   Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48
   Perrott DR, 1996, HUM FACTORS, V38, P702, DOI 10.1518/001872096778827260
   Prusa Z, 2014, LECT NOTES COMPUT SC, V8905, P419, DOI 10.1007/978-3-319-12976-1_25
   Richardson E, 2018, ADV NEUR IN, V31
   Salimans T, 2016, ADV NEUR IN, V29
   Saxe A. M., 2014, ICLR
   SHELTON BR, 1980, PERCEPT PSYCHOPHYS, V28, P589, DOI 10.3758/BF03198830
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   van den Doel K, 2001, COMP GRAPH, P537, DOI 10.1145/383259.383322
   Vasquez S, 2019, Arxiv, DOI arXiv:1906.01083
   Wang FZ, 2017, IEEE T MULTIMEDIA, V19, P418, DOI 10.1109/TMM.2016.2613641
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang LC, 2017, Arxiv, DOI arXiv:1703.10847
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zhang Chuanxi, VISUALLY INDICATED S
   Zhang H, 2019, 36 INT C MACHINE LEA, V97
   Zhang Zechen, 2019, ACM Trans. Graph., V38, P1
   Zheng J., 2020, ACM Transactions on Graphics, V39, P1
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou H, 2019, IEEE I CONF COMP VIS, P283, DOI 10.1109/ICCV.2019.00037
   Zhou YP, 2018, PROC CVPR IEEE, P3550, DOI 10.1109/CVPR.2018.00374
NR 63
TC 1
Z9 1
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4508
EP 4519
DI 10.1109/TMM.2022.3177894
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200031
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Guo, ZC
   Zhao, JX
   Jiao, LC
   Liu, X
   Liu, F
AF Guo, Zhicheng
   Zhao, Jiaxuan
   Jiao, Licheng
   Liu, Xu
   Liu, Fang
TI A Universal Quaternion Hypergraph Network for Multimodal Video Question
   Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quaternions; Task analysis; Cognition; Visualization; Knowledge
   discovery; Feature extraction; Convolution; Video question answering;
   multimodal features; quaternion operations; hypergraph convolution
AB Fusion and interaction of multimodal features are essential for video question answering. Structural information composed of the relationships between different objects in videos is very complex, which restricts understanding and reasoning. In this paper, we propose a quaternion hypergraph network (QHGN) for multimodal video question answering, to simultaneously involve multimodal features and structural information. Since quaternion operations are suitable for multimodal interactions, four components of the quaternion vectors are applied to represent the multimodal features. Furthermore, we construct a hypergraph based on the visual objects detected in the video. Most importantly, the quaternion hypergraph convolution operator is theoretically derived to realize multimodal and relational reasoning. Question and candidate answers are embedded in quaternion space, and a Q & A reasoning module is creatively designed for selecting the answer accurately. Moreover, the unified framework can be extended to other video-text tasks with different quaternion decoders. Experimental evaluations on the TVQA dataset and DramaQA dataset show that our method achieves state-of-the-art performance.
C1 [Guo, Zhicheng; Zhao, Jiaxuan; Jiao, Licheng; Liu, Xu; Liu, Fang] Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Sch Artificial Intelligence, Key Lab Intelligent Percept & Image Understanding,, Xian 710071, Peoples R China.
C3 Xidian University
RP Jiao, LC (corresponding author), Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Sch Artificial Intelligence, Key Lab Intelligent Percept & Image Understanding,, Xian 710071, Peoples R China.
EM zchguo@stu.xidian.edu.cn; jiaxuanzhao@stu.xidian.edu.cn;
   lchjiao@mail.xidian.edu.cn; xuliu361@163.com; f63liu@163.com
RI Guo, Zhicheng/AHE-6978-2022; Jiao, Licheng/JOZ-0842-2023
OI Guo, Zhicheng/0000-0002-5846-7194; Jiao, Licheng/0000-0003-3354-9617;
   Zhao, Jiaxuan/0000-0002-2827-0681
FU Key Scientific Technological Innovation Research Project by Ministry of
   Education; State Key Program and the Foundation for Innovative Research
   Groups of the National Natural Science Foundation of China [61836009,
   61621005]; Key Research and Development Program in Shaanxi Province of
   China [2019ZDLGY03-06]; Major Research Plan of the National Natural
   Science Foundation of China [91438201, 91438103, 61801124]; National
   Natural Science Foundation of China [U1701267, 62006177, 61871310,
   61902298, 61573267, 91838303, 61906150]; Fund for Foreign Scholars in
   University Research and Teaching Program's 111 Project [B07048]; Program
   for Cheung Kong Scholars and Innovative Research Team in University [IRT
   15R53]; ST Innovation Project from the Chinese Ministry of Education;
   National Science Basic Research Plan in Shaanxi Province of China
   [2019JQ-659]; Scientific Research Project of Education Department in
   Shaanxi Province of China [20JY023]; fundamental research funds for the
   central universities [XJS201901, XJS201903, JBF201905, JB211908];
   CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by the Key Scientific Technological
   Innovation Research Project by Ministry of Edu-cation, in part by the
   State Key Program and the Foundation for Innovative Research Groups of
   the National Natural Science Foundation of China under Grants 61836009
   and 61621005, in part by Key Research and Development Program in Shaanxi
   Province of China under Grant 2019ZDLGY03-06, in part the Major Research
   Plan of the National Natural Science Foundation of China under Grants
   91438201, 91438103, and 61801124, in part by the National Natural
   Science Foundation of China under Grants U1701267, 62006177, 61871310,
   61902298, 61573267, 91838303 and 61906150, in part by the Fund for
   Foreign Scholars in University Research and Teaching Program's 111
   Project under Grant B07048, in part by the Program for Cheung Kong
   Scholars and Innovative Research Team in University under Grant IRT
   15R53, in part by the ST Innovation Project from the Chinese Ministry of
   Education, in part by the National Science Basic Re-search Plan in
   Shaanxi Province of China under Grant 2019JQ-659, in part by the
   Scientific Research Project of Education Department in Shaanxi Province
   of China under Grant 20JY023, in part by the fundamental research funds
   for the central universities under Grants XJS201901, XJS201903,
   JBF201905, and JB211908, and in part by the CAAI-Huawei MindSpore Open
   Fund.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Bai S, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107637
   Bebensee B., 2020, PROC IEEE INT C ACOU, P4005
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chadha A., 2021, PROC IEEECVF WINTER, P1
   Chan THH, 2020, THEOR COMPUT SCI, V806, P416, DOI 10.1016/j.tcs.2019.07.024
   Chitra U., 2019, PR MACH LEARN RES, P1172
   Choi S, 2021, AAAI CONF ARTIF INTE, V35, P1166, DOI 10.5626/KTCP.2021.27.1.7
   Deng J., 2009, IEEE C COMP VIS PATT
   Devlin J., 2018, BERT PRE TRAINING DE
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ell TA, 2007, IEEE T IMAGE PROCESS, V16, P22, DOI 10.1109/TIP.2006.884955
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Gao Y, 2022, IEEE T PATTERN ANAL, V44, P2548, DOI 10.1109/TPAMI.2020.3039374
   Garcia N, 2020, AAAI CONF ARTIF INTE, V34, P10826
   Gaudet A. S., 2018, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2018.8489651
   Geng SJ, 2020, Arxiv, DOI arXiv:2005.08646
   Hamilton W R., 1848, London, V33, P58, DOI [10.1080/14786444808646046, DOI 10.1080/14786444808646046]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Jang Y, 2017, PROC CVPR IEEE, P1359, DOI 10.1109/CVPR.2017.149
   Jiang P., 2020, PROC AAAI C ARTIF IN, V34, p11 109
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Junyeong Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10103, DOI 10.1109/CVPR42600.2020.01012
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim H., 2020, P 58 ANN M ACL, P4812
   Kim J, 2019, IEEE IJCNN, DOI [10.1109/ijcnn.2019.8852087, 10.1007/s00779-019-01299-w]
   Kim J, 2019, PROC CVPR IEEE, P8329, DOI 10.1109/CVPR.2019.00853
   Kim KM, 2018, LECT NOTES COMPUT SC, V11219, P698, DOI 10.1007/978-3-030-01267-0_41
   Kim KM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2016
   Kipf TN, 2017, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lei J, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1369
   Lei Jie, 2020, P 58 ANN M ASS COMPU, P8211
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Liu F, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4253, DOI 10.1145/3394171.3413649
   Liu Y., 2020, P ICLR, P1
   Mason J. C., 2002, CHEBYSHEV POLYNOMIAL, P261
   Na S, 2017, IEEE I CONF COMP VIS, P677, DOI 10.1109/ICCV.2017.80
   Parcollet T., 2019, INT C LEARN REPR ICL, P1
   Parcollet T, 2018, INTERSPEECH, P22
   Pavllo Dario, 2018, BRIT MACH VIS C, DOI [10.1109/HUMANOIDS.2018.8624922, DOI 10.1109/HUMANOIDS.2018.8624922]
   Pei SC, 2001, IEEE T SIGNAL PROCES, V49, P2783, DOI 10.1109/78.960426
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38
   Shang F, 2014, IEEE T GEOSCI REMOTE, V52, P5693, DOI 10.1109/TGRS.2013.2291940
   Shi L., 2020, INT CONF ACOUST SPEE
   Sun G, 2021, MOBILE NETW APPL, P1
   Szeliski R, 2011, TEXTS COMPUT SCI, P181, DOI 10.1007/978-1-84882-935-0_4
   Tapaswi M, 2016, PROC CVPR IEEE, P4631, DOI 10.1109/CVPR.2016.501
   Tay Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1494
   Le TM, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P818
   Thao Minh Le, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9969, DOI 10.1109/CVPR42600.2020.00999
   Wang B, 2018, AAAI CONF ARTIF INTE, P7380
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yadati N., 2019, ADV NEUR IN, V32, P1511
   Yadati N, 2019, Arxiv, DOI [arXiv:1809.02589, 10.48550/arXiv.1809.02589]
   Yang C., 2020, COMPUT RES REPOSITOR, P1
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yang ZK, 2020, IEEE WINT CONF APPL, P1545, DOI [10.1109/wacv45572.2020.9093596, 10.1109/WACV45572.2020.9093596]
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yuan ZQ, 2021, IEEE T MULTIMEDIA, V23, P1744, DOI 10.1109/TMM.2020.3002667
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang X, 2020, IEEE ACCESS, V8, P6087, DOI 10.1109/ACCESS.2019.2963449
   Zhou Dengyong, 2006, 19 INT C NEURAL INFO, V19, P1601
NR 67
TC 8
Z9 8
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 38
EP 49
DI 10.1109/TMM.2021.3120544
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 8C6OL
UT WOS:000917725300001
DA 2024-07-18
ER

PT J
AU Hu, N
   Huang, XD
   Li, WH
   Li, XY
   Liu, AA
AF Hu, Nian
   Huang, Xiangdong
   Li, Wenhui
   Li, Xuanya
   Liu, An-An
TI Cross-Domain Image-Object Retrieval Based on Weighted Optimal Transport
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D Object Retrieval; Cross-Domain Feature Learning; Multi-View Learning
ID NEURAL-NETWORKS; 3D
AB Given a 2D image query and a pool of 3D objects, the goal of image-object retrieval is to rank the 3D objects according to how well their content fits the query. Previous methods usually project 2D images and 3D objects into a joint embedding space and minimize the distance metric to complete the retrieval task. Since 2D images and 3D objects come from two different domains with large discrepancy, even when 3D objects and 2D images are mapped to a shared space, the gap in feature distribution remains significant, which always leads to domain misalignment. In this work, we propose a novel image-object retrieval method by leveraging optimal transport theory. Specifically, to tackle the dimensionality gap between 2D images and 3D objects, we first represent a 3D object via a sequence of its 2D projections. We then design a Cross-Domain View Attention module (CDVA) to automatically compute the optimal combination of 3D object projections given a 2D query image. Next, we exploit Weighted Optimal Transport (WOT)-based distance to depict the discrepancy between 2D images and 3D objects, and reduce the discrepancy to achieve instance-level alignment. Through this scheme, the transported 2D images and 3D objects with the same label are enforced to follow similar distributions. Finally, we design an explicit Category Centroid Alignment module (CCA) to achieve class-level alignment to improve the retrieval performance. Extensive experiments show that our method can achieve competitive performance on the MI3DOR and MI3DOR-2 benchmarks.
C1 [Huang, Xiangdong; Li, Wenhui; Li, Xuanya; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Hu, Nian; Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
   [Li, Xuanya] Baidu Inc, Beijing 100085, Peoples R China.
C3 Tianjin University; Baidu
RP Li, XY (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM hunian_nian@163.com; xdhuang@tju.edu.cn; liwenhui@tju.edu.cn;
   lixuanya@baidu.com; anan0422@gmail.com
OI Hu, Nian/0000-0002-3174-6919
FU National Natural Science Foundation of China
FX No Statement Available
CR Akgül CB, 2009, IEEE T PATTERN ANAL, V31, P1117, DOI 10.1109/TPAMI.2009.25
   Blondel M, 2018, PR MACH LEARN RES, V84
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cicek S, 2019, IEEE I CONF COMP VIS, P1416, DOI 10.1109/ICCV.2019.00150
   Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921
   Damodaran BB, 2018, LECT NOTES COMPUT SC, V11208, P467, DOI 10.1007/978-3-030-01225-0_28
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   El Hamri M, 2022, MACH LEARN, V111, P4159, DOI 10.1007/s10994-022-06231-7
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P3181, DOI 10.1109/TPAMI.2022.3182052
   Gao Y, 2022, IEEE T PATTERN ANAL, V44, P2548, DOI 10.1109/TPAMI.2020.3039374
   Genevay A, 2016, ADV NEUR IN, V29
   Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Grabner A, 2019, INT CONF 3D VISION, P583, DOI 10.1109/3DV.2019.00070
   Hu NA, 2022, IEEE T CIRC SYST VID, V32, P8010, DOI 10.1109/TCSVT.2022.3182533
   Hu N, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103426
   Kantorovitch L, 1942, CR ACAD SCI URSS, V37, P199
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuo WC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12569, DOI 10.1109/ICCV48922.2021.01236
   Lee S, 2019, IEEE I CONF COMP VIS, P91, DOI 10.1109/ICCV.2019.00018
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071
   Li YZ, 2018, ADV NEUR IN, V31
   Lin MX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11385, DOI 10.1109/ICCV48922.2021.01121
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu AA, 2022, IEEE T CYBERNETICS, V52, P13862, DOI 10.1109/TCYB.2021.3139927
   Liu ZG, 2023, IEEE T PATTERN ANAL, V45, P681, DOI 10.1109/TPAMI.2021.3139918
   Löffler J, 2000, IEEE INFOR VIS, P82, DOI 10.1109/IV.2000.859741
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Nie W.-Z., 2020, IEEE Trans. Multimedia, V23, P1962
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Nie WZ, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3344684
   Nong LP, 2021, NEUROCOMPUTING, V463, P580, DOI 10.1016/j.neucom.2021.08.006
   Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934
   PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Savva M., 2017, P 10 EUR WORKSH 3D O, P89
   Seguy Vivien, 2018, P INT C LEARN REPR
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Shen J, 2018, AAAI CONF ARTIF INTE, P4058
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Su YT, 2021, IEEE T MULTIMEDIA, V23, P2127, DOI 10.1109/TMM.2020.3008056
   Su YT, 2020, IEEE T CIRC SYST VID, V30, P3765, DOI 10.1109/TCSVT.2019.2942688
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5
   Wang D, 2019, IEEE T MULTIMEDIA, V21, P2071, DOI 10.1109/TMM.2019.2892004
   Wang D, 2017, NEUROCOMPUTING, V252, P58, DOI 10.1016/j.neucom.2016.06.095
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang Ximei, 2019, PROC NEURIPS, V32, P1951
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu RJ, 2020, PROC CVPR IEEE, P4393, DOI 10.1109/CVPR42600.2020.00445
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Yu T, 2018, PROC CVPR IEEE, P186, DOI 10.1109/CVPR.2018.00027
   Zellinger W, 2019, Arxiv, DOI [arXiv:1702.08811, DOI 10.48550/ARXIV.1702.08811]
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang Z, 2023, IEEE T KNOWL DATA EN, V35, P5091, DOI 10.1109/TKDE.2022.3144352
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zhao W, 2020, IEEE T KNOWL DATA EN, V32, P1092, DOI 10.1109/TKDE.2019.2897932
   Zhou HY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P925, DOI 10.1145/3394171.3413631
   Zhou HY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P839
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
   Zhou YQ, 2022, IEEE T CIRC SYST VID, V32, P7147, DOI 10.1109/TCSVT.2022.3168967
   Zhou YQ, 2021, J VIS COMMUN IMAGE R, V79, DOI 10.1016/j.jvcir.2021.103197
NR 78
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9557
EP 9571
DI 10.1109/TMM.2023.3254889
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200013
DA 2024-07-18
ER

PT J
AU Jing, PG
   Cui, K
   Guan, WL
   Nie, LQ
   Su, YT
AF Jing, Peiguang
   Cui, Kai
   Guan, Weili
   Nie, Liqiang
   Su, Yuting
TI Category-Aware Multimodal Attention Network for Fashion Compatibility
   Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic graph convolutional network; fashion compatibility modeling;
   multimodal representation
AB Fashion compatibility modeling, which is used to estimate the matching degree of a given set of fashion items, has received increasing attention in recent years. However, existing studies often fail to fully leverage multimodal information or ignore the semantic guidance of clothing categories in elevating the reliability of multimodal information. In this paper, we propose a fashion compatibility modeling approach with a category-aware multimodal attention network, termed as FCM-CMAN. In FCM-CMAN, we focus on enriching and aggregating multimodal representations of fashion items by means of the dynamic representations of categories and a contextual attention mechanism simultaneously. Specifically, considering that category correlations are always dynamic and varied for different fashion items, we design a categorical dynamic graph convolutional network to adaptively learn the semantic correlations between categories. When combined with the multi-layered visual outputs of a convolutional neural network and the surrounding contextual information, multiple content-aware category representations and context-aware attention weights are obtained to better characterize fashion items from different aspects. On this basis, two pieces of aware information are integrated by a multimodal factorized bilinear pooling strategy to generate visual-semantic embeddings, which are further improved by a multi-head self-attention mechanism to capture significant elements related to fashion compatibility. Extensive experiments conducted on the FashionVC and ExpFashion datasets demonstrate the superiority of FCM-CMAN over state-of-the-art methods.
C1 [Jing, Peiguang; Cui, Kai; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Jing, Peiguang] Guangxi Normal Univ, Guangxi Key Labof Multisource Informat Min & Secu, Guilin 541004, Peoples R China.
   [Guan, Weili] Monash Univ, Fac Informat Technol, Clayton Campus, Clayton, Vic 3800, Australia.
   [Nie, Liqiang] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 Tianjin University; Guangxi Normal University; Monash University; Harbin
   Institute of Technology
RP Su, YT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM pgjing@tju.edu.cn; cuikai2020@tju.edu.cn; weili.guan@monash.edu;
   nieliqiang@gmail.com; ytsu@tju.edu.cn
OI Jing, Peiguang/0000-0003-2648-7358; Cui, Kai/0000-0002-7424-7204
FU Guangxi Key Lab of Multi-source Information Mining amp; Security
FX No Statement Available
CR [Anonymous], 2013, P INT ACM C MULTIMED, DOI DOI 10.1145/2502081.2502093
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Chen L, 2018, AAAI CONF ARTIF INTE, P2103
   Chen W, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2662, DOI 10.1145/3292500.3330652
   Chen X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P765, DOI 10.1145/3331184.3331254
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Dong X, 2024, IEEE T NEUR NET LEAR, V35, P246, DOI 10.1109/TNNLS.2022.3173295
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Guan WL, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2299, DOI 10.1145/3474085.3475392
   Guo B, 2019, NEUROCOMPUTING, V363, P366, DOI 10.1016/j.neucom.2019.07.052
   Han XJ, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P785, DOI 10.1145/3331184.3331245
   Han XJ, 2020, IEEE T IMAGE PROCESS, V29, P871, DOI 10.1109/TIP.2019.2936742
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang FR, 2021, IEEE T CYBERNETICS, V51, P1506, DOI 10.1109/TCYB.2019.2896100
   Hui Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12652, DOI 10.1109/CVPR42600.2020.01267
   Jing PG, 2022, IEEE T MULTIMEDIA, V24, P1277, DOI 10.1109/TMM.2021.3062736
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Kaicheng P., 2021, PROC IEEECVF C COMPU, P3894
   Kingma D. P., 2014, arXiv
   Kuang ZH, 2019, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2019.00316
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Lin M, 2014, Arxiv, DOI [arXiv:1312.4400, DOI 10.48550/ARXIV.1312.4400]
   Lin YL, 2020, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR42600.2020.00337
   Lin YJ, 2020, IEEE T KNOWL DATA EN, V32, P1502, DOI 10.1109/TKDE.2019.2906190
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P2894, DOI 10.1109/TMM.2020.3018021
   Lu Z, 2021, PROC CVPR IEEE, P12717, DOI 10.1109/CVPR46437.2021.01253
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P5585, DOI 10.1109/TIP.2018.2852503
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Ramachandram D, 2017, IEEE SIGNAL PROC MAG, V34, P96, DOI 10.1109/MSP.2017.2738401
   Rendle S., 2009, P 25 C UNC ART INT, P452, DOI DOI 10.5555/1795114.1795167
   Severyn A, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P959, DOI 10.1145/2766462.2767830
   Song XM, 2023, IEEE T MULTIMEDIA, V25, P856, DOI 10.1109/TMM.2021.3134164
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Song XM, 2018, ACM/SIGIR PROCEEDINGS 2018, P5, DOI 10.1145/3209978.3209996
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P329, DOI 10.1145/3343031.3350909
   Wu ZW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1038, DOI 10.1145/3394171.3413650
   Yang XT, 2017, PROC CVPR IEEE, P5066, DOI 10.1109/CVPR.2017.538
   Yang XW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2636, DOI 10.1145/3394171.3413936
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
NR 51
TC 4
Z9 4
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9120
EP 9131
DI 10.1109/TMM.2023.3246796
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200030
DA 2024-07-18
ER

PT J
AU Khezri, HR
   Kim, S
   Hefeeda, M
AF khezri, Hamed Rahmani
   Kim, Suhong
   Hefeeda, Mohamed
TI Unsupervised Single-Image Reflection Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image reflection; unsupervised learning
ID SEPARATION
AB Reflections often degrade the quality of images by obstructing the background scenes. This is not desirable for everyday users, and it negatively impacts the performance of multimedia applications that process images with reflections. Most current methods for removing reflections utilize supervised learning models. These models require an extensive number of image pairs of the same scenes with and without reflections to perform well. However, collecting such image pairs is challenging and costly. Thus, most current supervised models are trained on small datasets that cannot cover the numerous possibilities of real-life images with reflections. In this paper, we propose an unsupervised method for single-image reflection removal. Instead of learning from a large dataset, we optimize the parameters of two cross-coupled deep convolutional neural networks on a target image to generate two exclusive background and reflection layers. In particular, we design a network model that embeds semantic features extracted from the input image and utilizes these features in the separation of the background layer from the reflection layer. We show through objective and subjective studies on benchmark datasets that the proposed method substantially outperforms current methods in the literature. The proposed method does not require large datasets for training, removes reflections from single images, and does not impose impractical constraints on the input images.
C1 [khezri, Hamed Rahmani; Kim, Suhong; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM hamed_rahmani@sfu.ca; suhongk@sfu.ca; mhefeeda@sfu.ca
OI Hefeeda, Mohamed/0000-0003-3261-4376
FU Natural Sciences and Engineering Research Council of Canada
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council of Canada.
CR Abiko R, 2019, IEEE ACCESS, V7, P148790, DOI 10.1109/ACCESS.2019.2947266
   Alayrac JB, 2019, PROC CVPR IEEE, P2452, DOI 10.1109/CVPR.2019.00256
   Chandramouli P, 2019, IEEE INT CONF COMP V, P3315, DOI 10.1109/ICCVW.2019.00413
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   kaggle, Single-image-reflection-removal-dataset
   Kingma D. P., 2014, arXiv
   Lee D., 2018, ARXIV180104102
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Ma DQ, 2019, IEEE I CONF COMP VIS, P2444, DOI 10.1109/ICCV.2019.00253
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Nandoriya A, 2017, IEEE I CONF COMP VIS, P2430, DOI 10.1109/ICCV.2017.264
   Niklaus S, 2021, IEEE WINT CONF APPL, P3712, DOI 10.1109/WACV48630.2021.00376
   Pang Y., 2020, P ACM SIGGRAPH POSTE, P1, DOI [10.1145/3388770.3407419, DOI 10.1145/3388770.3407419]
   Prasad BHP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2370, DOI 10.1109/ICCV48922.2021.00239
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun C, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P466, DOI 10.1145/2964284.2967264
   Sungatullina D, 2018, LECT NOTES COMPUT SC, V11210, P587, DOI 10.1007/978-3-030-01231-1_36
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wan RJ, 2021, INT J COMPUT VISION, V129, P385, DOI 10.1007/s11263-020-01372-5
   Wan RJ, 2020, PROC CVPR IEEE, P2395, DOI 10.1109/CVPR42600.2020.00247
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Yang J, 2018, LECT NOTES COMPUT SC, V11207, P675, DOI 10.1007/978-3-030-01219-9_40
   Yingda Y., 2020, arXiv
   Zeng Y., 2021, arXiv
   Zhang HD, 2020, IEEE T MULTIMEDIA, V22, P2012, DOI 10.1109/TMM.2019.2951461
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng Q, 2021, PROC CVPR IEEE, P13390, DOI 10.1109/CVPR46437.2021.01319
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401
NR 39
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4958
EP 4971
DI 10.1109/TMM.2022.3185929
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300026
DA 2024-07-18
ER

PT J
AU Kim, G
   Kim, H
   Kong, K
   Song, JW
   Kang, SJ
AF Kim, Ginam
   Kim, Hyunsung
   Kong, Kyeongbo
   Song, Jou-Won
   Kang, Suk-Ju
TI Human Body-Aware Feature Extractor Using Attachable Feature Corrector
   for Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human pose estimation; vision transformer; deep learning; neural
   networks
AB Top-down pose estimation generally employs a person detector and estimates the keypoints of the detected person. This method assumes that only a single person exists within the bounding box cropped by detection. However, this assumption leads to some challenges in practice. First, a loose-fitted bounding box may include certain body parts of a non-target person. Second, spatial interference between several people exists owing to occlusion, so more than a single person can exist in the cropped image. In such scenarios, the pose estimation may falsely predict the keypoints of two or more persons as those of a single person. To tackle these issues, this paper proposes the human body-aware feature extractor based on the global- and local-reasoning features. The global-reasoning feature considers the entire body using transformer's non-local computation property and the local-reasoning feature concentrates on the individual body parts using convolutional neural networks. With those two features, we extract corrected features by filtering unnecessary features and supplementing necessary features using our proposed novel architecture. Hence, the proposed method can focus on the target person's keypoints, thereby mitigating the aforementioned concerns. Our method achieves noticeable improvement when applied to state-of-the-art top-down pose estimation networks.
C1 [Kim, Ginam] LG Elect, Seoul 06772, South Korea.
   [Kim, Hyunsung; Song, Jou-Won; Kang, Suk-Ju] Sogang Univ, Vis & Display Syst Lab Elect Engn, Seoul 04017, South Korea.
   [Kong, Kyeongbo] Pukyong Natl Univ, Media Commun, Busan 48547, South Korea.
C3 LG Electronics; Sogang University; Pukyong National University
RP Kang, SJ (corresponding author), Sogang Univ, Vis & Display Syst Lab Elect Engn, Seoul 04017, South Korea.
EM ginam11.kim@lge.com; ghkskxlr@sogang.ac.kr; kkb4723@gmail.com;
   wn5649@sogang.ac.kr; sjkang@sogang.ac.kr
FU Ministry of Culture, Sports and Tourism; Korea Creative Content Agency
   [R2020040058]; Convergent Technology R&D Program for Human Augmentation
   through the National Research Foundation of Korea(NRF) - Ministry of
   Science and ICT [2020M3C1B8081320]; National R&DProgram through
   theNational Research Foundation ofKorea (NRF) - Ministry of Science and
   ICT [2021M3H2A1038042]
FX This work was supported in part by the Ministry of Culture, Sports and
   Tourism and Korea Creative Content Agency under Grant R2020040058, in
   part by the Convergent Technology R&D Program for Human Augmentation
   through the National Research Foundation of Korea(NRF) funded by
   Ministry of Science and ICT under Grant 2020M3C1B8081320, and in part by
   the National R&DProgram through theNational Research Foundation ofKorea
   (NRF) funded by Ministry of Science and ICT under Grant
   2021M3H2A1038042.
CR Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Carion N, 2020, P EUR C COMP VIS, P1330
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fathi A, 2016, 2 IMAGENETCOCOVIS RE
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SL, 2017, IEEE I CONF COMP VIS, P3047, DOI 10.1109/ICCV.2017.329
   Kiliboz NÇ, 2015, J VIS COMMUN IMAGE R, V28, P97, DOI 10.1016/j.jvcir.2015.01.015
   Kingma D.P., 2014, ARXIV14126980
   Kocabas M, 2018, LECT NOTES COMPUT SC, V11215, P437, DOI 10.1007/978-3-030-01252-6_26
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li K, 2021, PROC CVPR IEEE, P1944, DOI 10.1109/CVPR46437.2021.00198
   Li WB, 2019, Arxiv, DOI arXiv:1901.00148
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Liang ZJ, 2014, IEEE INT CON MULTI
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu ZG, 2021, PROC CVPR IEEE, P525, DOI 10.1109/CVPR46437.2021.00059
   Lu Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P396, DOI 10.1007/978-3-030-58565-5_24
   Luong T., 2015, P 2015 C EMP METH NA, DOI [DOI 10.18653/V1/D15-1166, 10.18653/v1/D15-1166]
   Moon G, 2019, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2019.00796
   Newell A, 2017, Arxiv, DOI arXiv:1611.05424
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Qiu L, 2020, P EUR C COMP VIS, P1330
   Ronchi M. R., 2017, PROC IEEE INT C COMP, P1330
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang CY, 2013, PROC CVPR IEEE, P915, DOI 10.1109/CVPR.2013.123
   Wang J., 2020, P EUR C COMP VIS, P1330
   Wang K, 2018, IEEE INT CONF AUTOMA, P789, DOI 10.1109/FG.2018.00126
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei F., 2020, P EUR C COMP VIS, P1330
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie EZ, 2021, Arxiv, DOI arXiv:2101.08461
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
NR 48
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5789
EP 5799
DI 10.1109/TMM.2022.3199098
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500011
DA 2024-07-18
ER

PT J
AU Li, HF
   Li, YD
   Cao, YZH
   Han, YS
   Jin, Y
   Wei, YC
AF Li, Huifang
   Li, Yidong
   Cao, Yuanzhouhan
   Han, Yushan
   Jin, Yi
   Wei, Yunchao
TI Weakly Supervised Object Detection With Class Prototypical Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-attention; prototypical network; weakly supervised object
   detection
ID LOCALIZATION
AB In this paper, we aim to devise a new framework to compel the network to be equipped with the capability of detecting objects using image-level class labels as supervision. The challenge of such a weakly supervised setting mainly lies in how to make the network accurately understand both semantics and objectness of a given proposal without bounding box annotations. To this end, we contribute a concise framework, named Class Prototypical Network (CPNet). Concretely, our CPNet defines a set of learnable class prototypes to help classify object proposals. To endow the prototypes be not only discriminative for classes but also sensitive for proposals' objectness, we conduct both class-aware cross-attention and location-aware cross-attention between the feature embeddings of the learnable prototypes and the proposals. The learned attention scores are then used to form the proposal-level category information into the image-level one, making the entire framework be trained without any bounding box annotations. Besides, by applying these two kinds of attention mechanisms, the knowledge from both proposals' location and its class information can be successfully transferred into the corresponding prototypes. With the help of prototypes, our CPNet detects true positive object proposals. In addition, the CPNet further introduces a multi-head detection head to perform complementary training, preventing the model from falling into local discriminative parts and improving the model's performance on challenging non-rigid categories. We examine our CPNet on popular benchmarks, i.e., PASCAL VOC 2007, 2012 and MS COCO 2014. Extensive experiments show our CPNet is a simple and effective framework.
C1 [Li, Huifang; Li, Yidong; Cao, Yuanzhouhan; Han, Yushan; Jin, Yi; Wei, Yunchao] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Li, YD (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
EM 17112084@bjtu.edu.cn; ydli@bjtu.edu.cn; yzhcao@bjtu.edu.cn;
   21112034@bjtu.edu.cn; yjin@bjtu.edu.cn; yunchao.wei@bjtu.edu.cn
RI Li, Hui-Fang/KDM-4732-2024
OI Li, Hui-Fang/0000-0003-3744-3254; Han, Yushan/0000-0002-1774-4300; Jin,
   Yi/0000-0001-8408-3816
FU National Key R&D Program of China [2019YFB2204200]; National Natural
   Science Foundation of China [U1934220]; Didi Scientific Research
   Co-operation [K21L01100]; Fundamental Research Funds for the Central
   Universities [K22RC00010]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFB2204200, in part by the National Natural Science
   Foundation of China under Grant U1934220, in part by the Didi Scientific
   Research Co-operation under Project K21L01100, and in part by the
   Fundamental Research Funds for the Central Universities under Grant
   K22RC00010.
CR [Anonymous], 2014, Advances in Neural Information Processing Systems
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bilen H, 2015, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2015.7298711
   Boyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P763, DOI 10.1007/978-3-030-58598-3_45
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Choe J, 2019, PROC CVPR IEEE, P2214, DOI 10.1109/CVPR.2019.00232
   Cinbis RG, 2017, IEEE T PATTERN ANAL, V39, P189, DOI 10.1109/TPAMI.2016.2535231
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2012, INT J COMPUT VISION, V100, P275, DOI 10.1007/s11263-012-0538-3
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Dosovitskiy A, 2020, PROC INT C LEARN REP
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan Q, 2020, PROC CVPR IEEE, P4012, DOI 10.1109/CVPR42600.2020.00407
   Gao TY, 2019, AAAI CONF ARTIF INTE, P6407
   Gao Y, 2019, IEEE I CONF COMP VIS, P9833, DOI 10.1109/ICCV.2019.00993
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Huang Z., 2020, P 34 INT C NEUR INF, V33, P16797
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Ke Yang, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8371, DOI 10.1109/ICCV.2019.00846
   Li XY, 2019, IEEE I CONF COMP VIS, P9734, DOI 10.1109/ICCV.2019.00983
   Lin CH, 2020, AAAI CONF ARTIF INTE, V34, P11482
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu BB, 2019, POLYMERS-BASEL, V11, DOI 10.3390/polym11050770
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Paszke A, 2019, ADV NEUR IN, V32
   Redmon J., 2018, IEEE C COMPUTER VISI
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Shen YH, 2019, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2019.00079
   Shen YH, 2020, IEEE T IMAGE PROCESS, V29, P843, DOI 10.1109/TIP.2019.2933735
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun PZ, 2021, PROC CVPR IEEE, P14449, DOI 10.1109/CVPR46437.2021.01422
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wang C, 2014, LECT NOTES COMPUT SC, V8694, P431, DOI 10.1007/978-3-319-10599-4_28
   Wei Y., 2018, P EUR C COMP VIS, P434
   Xie E., 2021, PROC 30 INT JOINT C, P1194
   Xu Wenjia, 2020, Attribute Prototype Network for Zero-Shot Learning
   Xu YQ, 2021, IEEE T IMAGE PROCESS, V30, P3029, DOI 10.1109/TIP.2021.3056887
   Yang HM, 2022, IEEE T PATTERN ANAL, V44, P2358, DOI 10.1109/TPAMI.2020.3045079
   Yang HM, 2018, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2018.00366
   Yin YF, 2021, AAAI CONF ARTIF INTE, V35, P3190
   Ze Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12992, DOI 10.1109/CVPR42600.2020.01301
   Zeng ZY, 2019, IEEE I CONF COMP VIS, P8291, DOI 10.1109/ICCV.2019.00838
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhang XL, 2018, PROC CVPR IEEE, P1325, DOI 10.1109/CVPR.2018.00144
   Zhang YX, 2021, PROC CVPR IEEE, P12420, DOI 10.1109/CVPR46437.2021.01224
   Zhang Y, 2022, IEEE T KNOWL DATA EN, V34, P5586, DOI 10.1109/TKDE.2021.3070203
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
NR 56
TC 1
Z9 1
U1 4
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1868
EP 1878
DI 10.1109/TMM.2022.3187257
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100022
DA 2024-07-18
ER

PT J
AU Li, XK
   Zhang, ZF
   Gan, CQ
   Xiang, Y
AF Li, Xiaoke
   Zhang, Zufan
   Gan, Chenquan
   Xiang, Yong
TI Multi-Label Speech Emotion Recognition via Inter-Class Difference Loss
   Under Response Residual Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion response; inter-class difference loss; multi-label; residual
   network (ResNet); speech emotion recognition
ID NEURAL-NETWORKS; CLASSIFICATION; FEATURES; FRAMEWORK; RECURRENT; MODEL
AB Speech emotion recognition has always been a challenging task due to the difference in emotion expression and perception. Currently, in the supervised speech emotion recognition systems, the soft label overcomes the disadvantage of the hard label losing annotations variability and emotion perception subjectivity, but it only considers the emotion perceptions of a few annotators and thus still brings high statistical error. For this issue, this paper redefines the target and designs a novel loss function (denoted as inter-class difference loss), which enables the network to adaptively learn an emotion distribution in all utterances. This not only restricts the negative class probability less than the positive class probability, but also limits the negative class probability close to zero. To make the speech emotion recognition system more efficient, this paper proposes an end-to-end network, called response residual network (R-ResNet), which incorporates the ResNet for features extraction, together with the emotion response module for data augmentation and variable-length data processing. Finally, the experimental results not only demonstrate the advanced performance of our work, but also confirm that the ambiguous utterances contain emotional characteristics. In addition, another interesting finding is that, on the unbalanced dataset, the batch normalization (BN) after addition performs better than BN before addition.
C1 [Li, Xiaoke; Zhang, Zufan; Gan, Chenquan] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Li, Xiaoke; Zhang, Zufan; Gan, Chenquan] Key Lab Mobile Commun Technol, Chongqing 400065, Peoples R China.
   [Li, Xiaoke; Zhang, Zufan; Gan, Chenquan] Minist Educ, Engn Res Ctr Mobile Commun, Chongqing 400065, Peoples R China.
   [Xiang, Yong] Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia.
C3 Chongqing University of Posts & Telecommunications; Deakin University
RP Xiang, Y (corresponding author), Deakin Univ, Sch Informat Technol, Burwood, Vic 3125, Australia.
EM lxkever@foxmail.com; zhangzf@cqupt.edu.cn; gcq2010cqu@163.com;
   yxiang@deakin.edu.cn
OI Gan, Chenquan/0000-0002-0453-5630; zhang, zufan/0000-0001-5315-2065;
   Xiang, Yong/0000-0003-3545-7863
FU Chongqing University of Posts and Telecommunications Ph.D. Innovative
   Talents Project [BYJS202106]; Major Project of Science and Technology
   Research Program of Chongqing Education Commission of China
   [KJZD-M201900601]; China Postdoctoral Science Foundation [2021MD703932];
   Chongqing Research Program of Basic Research and Frontier Technology
   [cstc2021jcyj-msxmX0761]; Chongqing Key Laboratory of Mobile
   Communications Technology [cqupt-mct-202002]; Engineering Research
   Center of Mobile Communications, Ministry of Education, China
   [cqupt-mct-202006]
FX This work was supported in part by the Chongqing University of Posts and
   Telecommunications Ph.D. Innovative Talents Project under Grant
   BYJS202106, in part by the Major Project of Science and Technology
   Research Program of Chongqing Education Commission of China under Grant
   KJZD-M201900601, in part by the China Postdoctoral Science Foundation
   under Grant 2021MD703932, in part by the Chongqing Research Program of
   Basic Research and Frontier Technology under Grant
   cstc2021jcyj-msxmX0761, in part by the Chongqing Key Laboratory of
   Mobile Communications Technology under Grant cqupt-mct-202002, and in
   part by the Engineering Research Center of Mobile Communications,
   Ministry of Education, China under Grant cqupt-mct-202006. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Preeti Rao.
CR Alam M, 2018, IEEE T NEUR NET LEAR, V29, P4905, DOI 10.1109/TNNLS.2017.2776248
   Ando A, 2019, INTERSPEECH, P2818, DOI 10.21437/Interspeech.2019-2524
   Ando A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4964, DOI 10.1109/ICASSP.2018.8461299
   [Anonymous], 2009, 2009 3 INT C AFF COM, DOI DOI 10.1109/ACII.2009.5349500
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chen M, 2020, IEEE T NEUR NET LEAR, V31, P2430, DOI 10.1109/TNNLS.2019.2929071
   Chen MY, 2018, IEEE SIGNAL PROC LET, V25, P1440, DOI 10.1109/LSP.2018.2860246
   Chiu PS, 2020, IEEE ACCESS, V8, P62032, DOI 10.1109/ACCESS.2020.2984383
   Devillers L, 2005, NEURAL NETWORKS, V18, P407, DOI 10.1016/j.neunet.2005.03.007
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Eyben F., 2009, 3 INT C AFF COMP INT, P1, DOI DOI 10.1109/ACII.2009.5349350
   Fayek HM, 2016, IEEE IJCNN, P566, DOI 10.1109/IJCNN.2016.7727250
   Fayek HM, 2017, NEURAL NETWORKS, V92, P60, DOI 10.1016/j.neunet.2017.02.013
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang Y., 2019, IEEE ACCESS, V8, P28395
   Kim Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5104, DOI 10.1109/ICASSP.2018.8462011
   Lotfian R, 2017, INT CONF AFFECT, P415, DOI 10.1109/ACII.2017.8273633
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Mower E, 2011, IEEE T AUDIO SPEECH, V19, P1057, DOI 10.1109/TASL.2010.2076804
   Noroozi F, 2019, IEEE T AFFECT COMPUT, V10, P60, DOI 10.1109/TAFFC.2017.2713783
   Ramakrishnan S, 2013, TELECOMMUN SYST, V52, P1467, DOI 10.1007/s11235-011-9624-z
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schuller B, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P552, DOI 10.1109/ASRU.2009.5372886
   Shivappa ST, 2010, P IEEE, V98, P1692, DOI 10.1109/JPROC.2010.2057231
   Steidl S, 2005, INT CONF ACOUST SPEE, P317
   Takahashi N, 2018, IEEE T MULTIMEDIA, V20, P513, DOI 10.1109/TMM.2017.2751969
   Tao F, 2021, IEEE T MULTIMEDIA, V23, P1, DOI 10.1109/TMM.2020.2975922
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xia R, 2017, IEEE T AFFECT COMPUT, V8, P3, DOI 10.1109/TAFFC.2015.2512598
   Xie Y, 2019, IEEE-ACM T AUDIO SPE, V27, P1675, DOI 10.1109/TASLP.2019.2925934
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhao JF, 2019, BIOMED SIGNAL PROCES, V47, P312, DOI 10.1016/j.bspc.2018.08.035
   Zhao ZP, 2019, IEEE ACCESS, V7, P97515, DOI 10.1109/ACCESS.2019.2928625
NR 38
TC 4
Z9 4
U1 9
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3230
EP 3244
DI 10.1109/TMM.2022.3157485
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200022
DA 2024-07-18
ER

PT J
AU Li, XF
   Sun, QG
   Jiao, LC
   Liu, F
   Liu, X
   Li, LL
   Chen, PH
   Zuo, Y
AF Li, Xiufang
   Sun, Qigong
   Jiao, Licheng
   Liu, Fang
   Liu, Xu
   Li, Lingling
   Chen, Puhua
   Zuo, Yi
TI <i>D</i><SUP>3</SUP><i>K</i>: Dynastic Data-Free Knowledge Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Data models; Knowledge engineering; Training data; Neural
   networks; Training; Task analysis; Dhash; dynastic network; image
   classification; generated data diversity; knowledge distillation
ID GENERATIVE ADVERSARIAL NETWORKS
AB Data-free knowledge distillation further broadens the applications of the distillation model. Nevertheless, the problem of providing diverse data with rich expression patterns needs to be further explored. In this paper, a novel dynastic data-free knowledge distillation ((DK)-K-3) model is proposed to alleviate this problem. In this model, a dynastic supernet generator (D-SG) with a flexible network structure is proposed to generate diverse data. The D-SG can adaptively alter architectural configurations and activate different subnet generators in different sequential iteration spaces. The variable network structure increases the complexity and capacity of the generator, and strengthens its ability to generate diversified data. In addition, a novel additive constraint based on the differentiable dhash (D-Dhash) is designed to guide the structure parameter selection of the D-SG. This constraint forces the D-SG to constantly jump out of the fixed generation mode and generate diverse data in semantics and instance. The effectiveness of the proposed model is verified on the experimental benchmark datasets (MNIST, CIFAR-10, CIFAR-100, and SVHN).
C1 [Li, Xiufang; Jiao, Licheng; Liu, Fang; Liu, Xu; Li, Lingling; Chen, Puhua; Zuo, Yi] Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Shaanxi, Peoples R China.
   [Sun, Qigong] SenseTime Res, Shanghai 200000, Peoples R China.
   [Sun, Qigong] Shanghai AI Lab, Shanghai 200000, Peoples R China.
C3 Xidian University; Shanghai Artificial Intelligence Laboratory
RP Jiao, LC (corresponding author), Xidian Univ, Int Res Ctr Intelligent Percept & Computat, Key Lab Intelligent Percept & Image Understanding, Minist Educ, Xian 710071, Shaanxi, Peoples R China.
EM xfl_xidian@163.com; xd_qigongsun@163.com; lchjiao@mail.xidian.edu.cn;
   f63liu@163.com; xuliu361@163.com; llli@xidian.edu.cn;
   phchen@xidian.edu.can; yzuo_1@stu.xidian.edu.cn
OI xiufang, Li/0000-0002-6908-0892
FU Key Scientific Technological Innovation Research Project by Ministry of
   Education
FX No Statement Available
CR Ahmad F, 2022, IEEE T KNOWL DATA EN, V34, P2681, DOI 10.1109/TKDE.2020.3017786
   Almahairi A., 2016, INT C MACH LEARN PML, P2549
   Bang D, 2021, IEEE INT CONF COMP V, P2347, DOI 10.1109/ICCVW54120.2021.00266
   Binici K., 2022, P IEEE CVF WINT C AP, P663
   Binici K, 2022, AAAI CONF ARTIF INTE, P6089
   BOLUKBASI T, 2017, PR MACH LEARN RES, V70
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Cai H., 2019, P INT C LEARN REPR S, P1
   Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361
   Chen ZZ, 2022, IEEE T MULTIMEDIA, V24, P609, DOI 10.1109/TMM.2021.3056896
   Chougule Abhijeet, 2020, Information Systems Security. 16th International Conference, ICISS 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12553), P97, DOI 10.1007/978-3-030-65610-2_7
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Deng CF, 2022, IEEE T MULTIMEDIA, V24, P1968, DOI 10.1109/TMM.2021.3074273
   Ermaliuc M., 2021, P INT C ENG APPL NEU, P13
   Fang G., 2021, P INT JOINT C ART IN, P2374
   Fang GF, 2022, AAAI CONF ARTIF INTE, P6597
   Fang GF, 2020, Arxiv, DOI arXiv:1912.11006
   Fu ZY, 2011, IEEE T MULTIMEDIA, V13, P303, DOI 10.1109/TMM.2010.2098858
   Ghosh A, 2018, PROC CVPR IEEE, P8513, DOI 10.1109/CVPR.2018.00888
   Lopes RG, 2017, Arxiv, DOI arXiv:1710.07535
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Han Sun, 2020, 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), P1112, DOI 10.1109/HPCC-SmartCity-DSS50907.2020.00146
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2014, P NEAR INF PROC SYST, P1
   Hoang TT, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207181
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang PL, 2022, IEEE-CAA J AUTOMATIC, V9, P339, DOI 10.1109/JAS.2021.1004210
   Huang QB, 2022, IEEE T MULTIMEDIA, V24, P2004, DOI 10.1109/TMM.2021.3074803
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiao LC, 2022, IEEE T NEUR NET LEAR, V33, P3195, DOI 10.1109/TNNLS.2021.3053249
   Kim Y, 2022, AAAI CONF ARTIF INTE, P1201
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li D, 2019, LECT NOTES COMPUT SC, V11730, P703, DOI 10.1007/978-3-030-30490-4_56
   Li HX, 2015, PROC CVPR IEEE, P5325, DOI 10.1109/CVPR.2015.7299170
   Lin J, 2017, ADV NEUR IN, V30
   Liu F, 2019, IEEE T NEUR NET LEAR, V30, P2707, DOI 10.1109/TNNLS.2018.2885799
   Liu KL, 2019, IEEE I CONF COMP VIS, P6391, DOI 10.1109/ICCV.2019.00648
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Luo L., 2020, arXiv
   Malinin A., 2019, P INT CONJ LEARN REP, P1
   Micaelli P, 2019, ADV NEUR IN, V32
   Nayak GK, 2019, PR MACH LEARN RES, V97
   Netzer Y., 2018, Technical Report
   Phuong M, 2019, IEEE I CONF COMP VIS, P1355, DOI 10.1109/ICCV.2019.00144
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Sabour S, 2017, ADV NEUR IN, V30
   Shahroudnejad A, 2021, Arxiv, DOI arXiv:2102.01792
   Srinivas S., 2015, P BRIT MACH VIS C 20, DOI DOI 10.5244/C.29.31
   Srivastava A, 2017, ADV NEUR IN, V30
   Sun QG, 2019, AAAI CONF ARTIF INTE, P5024
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Tedjopurnomo DA, 2022, IEEE T KNOWL DATA EN, V34, P1544, DOI 10.1109/TKDE.2020.3001195
   Trenta F., 2019, P IEEE INT C AUT FAC, P1
   Valanarasu JMJ, 2021, LECT NOTES COMPUT SC, V12901, P36, DOI 10.1007/978-3-030-87193-2_4
   Wang Z, 2021, PROC CVPR IEEE, P14908, DOI 10.1109/CVPR46437.2021.01467
   Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919
   Xia XJ, 2019, IEEE T MULTIMEDIA, V21, P1359, DOI 10.1109/TMM.2018.2879750
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yoojin Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3047, DOI 10.1109/CVPRW50498.2020.00363
   Yu Y, 2017, LECT NOTES COMPUT SC, V10667, P97, DOI 10.1007/978-3-319-71589-6_9
   Yu ZW, 2022, IEEE T KNOWL DATA EN, V34, P3267, DOI 10.1109/TKDE.2020.3025301
   Zhang DW, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107562
   Zhang XK, 2022, IEEE T MULTIMEDIA, V24, P1990, DOI 10.1109/TMM.2021.3074807
   Zhang ZL, 2022, IEEE T KNOWL DATA EN, V34, P2335, DOI 10.1109/TKDE.2020.3005952
   Zhang ZY, 2022, IEEE T MULTIMEDIA, V24, P677, DOI 10.1109/TMM.2021.3057989
   Zhao HR, 2022, IEEE T CYBERNETICS, V52, P2070, DOI 10.1109/TCYB.2020.3007506
   Zhao WB, 2021, PROC CVPR IEEE, P16821, DOI 10.1109/CVPR46437.2021.01655
   Zhou YR, 2018, AAAI CONF ARTIF INTE, P4596
   Zoph B., 2017, P INT C LEARN REPR A, P1
NR 75
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8358
EP 8371
DI 10.1109/TMM.2023.3236212
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000050
DA 2024-07-18
ER

PT J
AU Li, YX
   Chen, H
   Cao, WC
   Huang, QS
   He, QH
AF Li, Yanxiong
   Chen, Hao
   Cao, Wenchang
   Huang, Qisheng
   He, Qianhua
TI Few-Shot Speaker Identification Using Lightweight Prototypical Network
   With Feature Grouping and Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature grouping; feature interaction; few-shot learning; prototypical
   network; speaker identification
ID RECOGNITION; VERIFICATION; ATTENTION
AB Existing methods for few-shot speaker identification (FSSI) obtain high accuracy, but their computational complexities and model sizes need to be reduced for lightweight applications. In this work, we propose a FSSI method using a lightweight prototypical network with the final goal to implement the FSSI on intelligent terminals with limited resources, such as smart watches and smart speakers. In the proposed prototypical network, an embedding module is designed to perform feature grouping for reducing the memory requirement and computational complexity, and feature interaction for enhancing the representational ability of the learned speaker embedding. In the proposed embedding module, audio feature of each speech sample is split into several low-dimensional feature subsets that are transformed by a recurrent convolutional block in parallel. Then, the operations of averaging, addition, concatenation, element-wise summation and statistics pooling are sequentially executed to learn a speaker embedding for each speech sample. The recurrent convolutional block consists of a block of bidirectional long short-term memory, and a block of de-redundancy convolution in which feature grouping and interaction are conducted too. Our method is compared to baseline methods on three datasets that are selected from three public speech corpora (VoxCeleb1, VoxCeleb2, and LibriSpeech). The results show that our method obtains higher accuracy under several conditions, and has advantages over all baseline methods in computational complexity and model size.
C1 [Li, Yanxiong; Chen, Hao; Cao, Wenchang; Huang, Qisheng; He, Qianhua] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
C3 South China University of Technology
RP Li, YX (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510640, Peoples R China.
EM eeyxli@scut.edu.cn; 865924195@qq.com; wenchangcao98@163.com;
   839508665@qq.com; eeqhhe@scut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Aljasem M, 2021, IEEE T INF FOREN SEC, V16, P3524, DOI 10.1109/TIFS.2021.3082303
   AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   Anand P, 2019, Arxiv, DOI [arXiv:1904.08775, DOI 10.48550/ARXIV.1904.08775]
   Bai ZX, 2021, NEURAL NETWORKS, V140, P65, DOI 10.1016/j.neunet.2021.03.004
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Campbell JP, 2009, IEEE SIGNAL PROC MAG, V26, P95, DOI 10.1109/MSP.2008.931100
   Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086
   Nunes JAC, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207519
   Chowdhury A, 2020, IEEE T INF FOREN SEC, V15, P1616, DOI 10.1109/TIFS.2019.2941773
   Chuanqi Tan, 2018, Artificial Neural Networks and Machine Learning - ICANN 2018. 27th International Conference on Artificial Neural Networks. Proceedings: Lecture Notes in Computer Science (LNCS 11141), P270, DOI 10.1007/978-3-030-01424-7_27
   Chung JS, 2018, INTERSPEECH, P1086
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Finn C, 2017, PR MACH LEARN RES, V70
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Greenberg CS, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101032
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jadon S, 2020, Arxiv, DOI arXiv:2008.06365
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kanervisto A, 2022, IEEE-ACM T AUDIO SPE, V30, P477, DOI 10.1109/TASLP.2021.3138681
   Kenny P, 2005, IEEE T SPEECH AUDI P, V13, P345, DOI 10.1109/TSA.2004.840940
   Khan U, 2020, INTERSPEECH, P3002, DOI 10.21437/Interspeech.2020-1882
   Kishan KC, 2022, INT CONF ACOUST SPEE, P7062, DOI 10.1109/ICASSP43922.2022.9747613
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kye SM, 2020, INTERSPEECH, P2982, DOI 10.21437/Interspeech.2020-1283
   Lee J, 2022, IEEE SIGNAL PROC LET, V29, P224, DOI 10.1109/LSP.2021.3129695
   Li C, 2017, Arxiv, DOI arXiv:1705.02304
   Li QJ, 2022, INT CONF ACOUST SPEE, P7067, DOI 10.1109/ICASSP43922.2022.9746247
   Li RR, 2020, INT CONF ACOUST SPEE, P3522, DOI [10.1109/icassp40776.2020.9054111, 10.1109/ICASSP40776.2020.9054111]
   Li RR, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P340, DOI 10.1145/3336191.3371802
   Li Y, 2022, SPEAKER LANGUAGE REC, P221, DOI [DOI 10.21437/ODYSSEY.2022-31, 10.21437/Odyssey.2022-31]
   Li YX, 2022, APPL SOFT COMPUT, V126, DOI 10.1016/j.asoc.2022.109291
   Li YX, 2021, IEEE T MULTIMEDIA, V23, P3377, DOI 10.1109/TMM.2020.3024667
   Li YX, 2020, INT CONF ACOUST SPEE, P286, DOI [10.1109/ICASSP40776.2020.9054433, 10.1109/icassp40776.2020.9054433]
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1385, DOI 10.1109/TMM.2019.2947199
   Mary NJMS, 2022, IEEE-ACM T AUDIO SPE, V30, P404, DOI 10.1109/TASLP.2021.3134566
   Mishra P., 2020, arXiv, DOI [10.48550/arXiv.2008.11088, DOI 10.48550/ARXIV.2008.11088]
   Nagrani A, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101027
   Narayanaswamy B, 2005, INT CONF ACOUST SPEE, P621
   Nassif AB, 2021, APPL SOFT COMPUT, V103, DOI 10.1016/j.asoc.2021.107141
   Ng RWM, 2018, IEEE W SP LANG TECH, P1044, DOI 10.1109/SLT.2018.8639564
   Nguyen PC, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P617
   Oneata D., 2021, P IEEE 28 EUR SIGN P, P1
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Powell A., 2019, P FAIR, P196
   Koluguri NR, 2020, Arxiv, DOI [arXiv:2010.12653, 10.48550/ARXIV.2010.12653]
   Ravanelli M, 2018, IEEE W SP LANG TECH, P1021, DOI 10.1109/SLT.2018.8639585
   Sabour S, 2017, ADV NEUR IN, V30
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shahin I, 2006, SPEECH COMMUN, V48, P1047, DOI 10.1016/j.specom.2006.01.005
   Snell J, 2017, ADV NEUR IN, V30
   Snyder D, 2019, INT CONF ACOUST SPEE, P5796, DOI 10.1109/ICASSP.2019.8683760
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Sun LF, 2015, INT CONF ACOUST SPEE, P4869, DOI 10.1109/ICASSP.2015.7178896
   Tan K, 2019, IEEE-ACM T AUDIO SPE, V27, P189, DOI 10.1109/TASLP.2018.2876171
   Tkachenko M, 2018, LECT NOTES ARTIF INT, V11096, P687, DOI 10.1007/978-3-319-99579-3_70
   Todisco M., 2016, P SPEAK LANG REC WOR, V25, P249, DOI DOI 10.21437/ODYSSEY.2016-41
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Variani Ehsan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4052, DOI 10.1109/ICASSP.2014.6854363
   Velez I., 2021, Appl. Soft Comput., V95, P1
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wan L, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4879, DOI 10.1109/ICASSP.2018.8462665
   Wang JX, 2019, INT CONF ACOUST SPEE, P3652, DOI 10.1109/ICASSP.2019.8683393
   Wang R, 2022, IEEE-ACM T AUDIO SPE, V30, P2267, DOI 10.1109/TASLP.2022.3182856
   Wang XM, 2022, NEUROCOMPUTING, V490, P283, DOI 10.1016/j.neucom.2021.11.092
   Xu CL, 2021, IEEE-ACM T AUDIO SPE, V29, P2696, DOI 10.1109/TASLP.2021.3100682
   Zeljkovic I, 2008, INT CONF ACOUST SPEE, P4129, DOI 10.1109/ICASSP.2008.4518563
   Zhang CL, 2017, INTERSPEECH, P1487, DOI 10.21437/Interspeech.2017-1608
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zoph B., 2017, ICLR, P1, DOI DOI 10.1109/ICAIIC48513.2020.9065031
NR 72
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9241
EP 9253
DI 10.1109/TMM.2023.3253301
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, X
   Yi, JH
   Cheung, YM
   Xu, X
   Cui, Z
AF Liu, Xin
   Yi, Jinhan
   Cheung, Yiu-ming
   Xu, Xing
   Cui, Zhen
TI OMGH: Online Manifold-Guided Hashing for Flexible Cross-Modal Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Semantics; Hash functions; Manifolds; Correlation; Streaming
   media; Training; Cross-modal hashing; streaming data; Anchor-based
   manifold structure; online discrete optimization
AB Cross-modal hashing hasrecently gained an increasing attention for its efficiency and fast retrieval speed in indexing the multimedia data across different modalities. Nevertheless, the multimedia data points often emerge in a streaming manner, and existing online methods often lack of learning capacity to handle both labeled and unlabeled data.To alleviate these concerns, this paper proposes an Online Manifold-Guided Hashing (OMGH) framework, which can incrementally learn the compact hash code of streaming data while adaptively optimizing the hash function in a streaming manner. To be specific, OMGH first exploits a matrix tri-factorization framework to learn the discriminative hash codes for streaming multi-modal data. Then, an online anchor-based manifold structure is designed to sparsely represent the old data and adaptively guide the hash code learning process, which can wellreduce the complexity in preserving the semantic correlation between the old data and streaming data. Meanwhile, such anchor-based manifold embedding is adaptive to the unsupervised and supervised learning strategies in a flexible way. Besides, an online discrete optimization method is efficiently addressed to incrementally update the hash functions and optimize the hash codes on streaming data points. As a result, the derived hash codes are more semantically meaningful for various online cross-modal retrieval tasks. Extensive experiments verify the advantages of the proposed OMGH model, by achieving and improving the state-of-the-art cross-modal retrieval performances on three benchmark datasets.
C1 [Liu, Xin] Huaqiao Univ, Dept Comp Sci, Xiamen 361021, Fujian, Peoples R China.
   [Liu, Xin; Cui, Zhen] Nanjing Univ Sci & Technol, Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, Nanjing 210094, Jiangsu, Peoples R China.
   [Yi, Jinhan] Huaqiao Univ, Xiamen Key Lab Comp Vision & Pattern Recognit, Xiamen 361021, Fujian, Peoples R China.
   [Yi, Jinhan] Huaqiao Univ, Fujian Key Lab Big Data Intelligence & Secur, Xiamen 361021, Fujian, Peoples R China.
   [Cheung, Yiu-ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong, Peoples R China.
   [Xu, Xing] Univ Elect Sci & Technol China, Ctr Future Multimedia, Chengdu 611731, Sichuan, Peoples R China.
   [Xu, Xing] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
C3 Huaqiao University; Nanjing University of Science & Technology; Huaqiao
   University; Huaqiao University; Hong Kong Baptist University; University
   of Electronic Science & Technology of China; University of Electronic
   Science & Technology of China
RP Liu, X (corresponding author), Huaqiao Univ, Dept Comp Sci, Xiamen 361021, Fujian, Peoples R China.
EM xliu@hqu.edu.cn; jhyi@stu.hqu.edu.cn; ymc@comp.hkbu.edu.hk;
   xing.xu@uestc.edu.cn; zhen.cui@njust.edu.cn
RI Cheung, Yiu-ming/E-2050-2015
OI Cheung, Yiu-ming/0000-0001-7629-4648
FU Open Project of Zhejiang Lab [2021KH0AB01]; National Science Foundation
   of China [61673185, 61672444, 61976049]; NSFC/RGC Joint Research Scheme
   [N_HKBU214/21]; RGC General Research Fund [RGC/HKBU/12201321]; Hong Kong
   Baptist University [RC-FNRA-IG/18-19/SCI/03, RC-IRCMs/1819/SCI/01];
   Innovation and Technology Fund of Innovation and Technology Commission
   of the Government of the Hong Kong SAR [ITS/339/18]; National Science
   Foundation of Fujian Province [2020J01084]; Natural Science Foundation
   of Shandong Province [ZR2020LZH008]
FX This work was supported in part by the Open Project of Zhejiang Lab
   under Grant 2021KH0AB01, in part by the National Science Foundation of
   China under Grants 61673185, 61672444, and 61976049, in part by NSFC/RGC
   Joint Research Scheme under Grant N_HKBU214/21, in part by RGC General
   Research Fund under Grant RGC/HKBU/12201321, in part by Hong Kong
   Baptist University under Grants RC-FNRA-IG/18-19/SCI/03 and
   RC-IRCMs/1819/SCI/01, in part by the Innovation and Technology Fund of
   Innovation and Technology Commission of the Government of the Hong Kong
   SAR under Grant ITS/339/18, in part by theNational Science Foundation of
   Fujian Province under Grant 2020J01084, and in part by the Natural
   Science Foundation of Shandong Province under Grant ZR2020LZH008.
CR Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Chen XX, 2021, IEEE T KNOWL DATA EN, V33, P1089, DOI 10.1109/TKDE.2019.2934687
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2018, PATTERN RECOGN, V79, P114, DOI 10.1016/j.patcog.2018.02.002
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   LIU X, IEEE T NEURAL NETW L
   Liu X, 2021, IEEE T PATTERN ANAL, V43, P964, DOI 10.1109/TPAMI.2019.2940446
   Liu Y, 2021, PROC CVPR IEEE, P14949, DOI 10.1109/CVPR46437.2021.01471
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Ma XH, 2020, IEEE T MULTIMEDIA, V22, P3101, DOI 10.1109/TMM.2020.2969792
   Mandal D, 2019, IEEE T IMAGE PROCESS, V28, P102, DOI 10.1109/TIP.2018.2863040
   Peng YX, 2018, IEEE T CIRC SYST VID, V28, P2372, DOI 10.1109/TCSVT.2017.2705068
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Su RQ, 2020, IEEE ACCESS, V8, P206360, DOI 10.1109/ACCESS.2020.3037968
   Sun GD, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3458928
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Wang D, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1409, DOI 10.1145/3397271.3401132
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang L, 2019, IEEE INT CON MULTI, P37, DOI 10.1109/ICME.2019.00015
   Wang YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P871, DOI 10.1145/3394171.3413971
   Wei YC, 2017, IEEE T CYBERNETICS, V47, P449, DOI 10.1109/TCYB.2016.2519449
   Weng ZY, 2019, IEEE ACCESS, V7, P88369, DOI 10.1109/ACCESS.2019.2926303
   Xie L, 2016, AAAI CONF ARTIF INTE, P294
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012
   Zeng ZX, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1125, DOI 10.1145/3404835.3462867
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang PF, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1517, DOI 10.1145/3474085.3475286
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
NR 41
TC 13
Z9 13
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3811
EP 3824
DI 10.1109/TMM.2022.3166668
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500021
DA 2024-07-18
ER

PT J
AU Liu, YX
   Yang, JX
   Gu, X
   Chen, YJ
   Guo, Y
   Yang, GZ
AF Liu, Yuxuan
   Yang, Jianxin
   Gu, Xiao
   Chen, Yijun
   Guo, Yao
   Yang, Guang-Zhong
TI EgoFish3D: Egocentric 3D Pose Estimation From a Fisheye Camera via
   Self-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Cameras; Pose estimation; Training;
   Self-supervised learning; Synthetic data; Annotations; Egocentric
   vision; 3D human pose estimation; Multi-view constraints
AB Egocentric vision has gained increasing popularity recently, opening new avenues for human-centric applications. However, the use of the egocentric fisheye cameras allows wide angle coverage but image distortion is introduced along with strong human body self-occlusion imposing significant challenges in data processing and model reconstruction. Unlike previous work only leveraging synthetic data for model training, this paper presents a new real-world EgoCentric Human Pose (ECHP) dataset. To tackle the difficulty of collecting 3D ground truth using motion capture systems, we simultaneously collect images from a head-mounted egocentric fisheye camera as well as from two third-person-view cameras, circumventing the environmental restrictions. By using self-supervised learning under multi-view constraints, we propose a simple yet effective framework, namely EgoFish3D, for egocentric 3D pose estimation from a single image in different real-world scenarios. The proposed EgoFish3D incorporates three main modules. 1) The third-person-view module takes two exocentric images as input and estimates the 3D pose represented in the third-person camera frame; 2) the egocentric module predicts the 3D pose in the egocentric camera frame; and 3) the interactive module estimates the rotation matrix between the third-person and the egocentric views. Experimental results on our ECHP dataset and existing benchmark datasets demonstrate the effectiveness of the proposed EgoFish3D, which can achieve superior performance to existing methods.
C1 [Liu, Yuxuan; Yang, Jianxin; Guo, Yao; Yang, Guang-Zhong] Shanghai Jiao Tong Univ, Sch Biomed Engn, Inst Med Robot, Shanghai 200240, Peoples R China.
   [Gu, Xiao] Imperial Coll London, Hamlyn Ctr Robot Surg, London SW7 2AZ, England.
   [Chen, Yijun] Shanghai Tech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
C3 Shanghai Jiao Tong University; Imperial College London; ShanghaiTech
   University
RP Guo, Y; Yang, GZ (corresponding author), Shanghai Jiao Tong Univ, Sch Biomed Engn, Inst Med Robot, Shanghai 200240, Peoples R China.
EM 20000905lyx@sjtu.edu.cn; jianxinyang@sjtu.edu.cn;
   xiao.gu17@imperial.ac.uk; chengyj2@shanghaitech.edu.cn;
   yao.guo@sjtu.edu.cn; gzyang@sjtu.edu.cn
RI Yang, Guangzhong/ABB-7316-2021; Yang, Jianxin/AAH-7846-2020
OI Yang, Guangzhong/0000-0002-7289-5806; Yang, Jianxin/0000-0002-6737-6913;
   Liu, Yuxuan/0000-0002-2431-2653
FU National Natural Science Foundation of China
FX No Statement Available
CR Alletto S, 2015, PATTERN RECOGN, V48, P4082, DOI 10.1016/j.patcog.2015.06.006
   Ardeshir S, 2019, IEEE T PATTERN ANAL, V41, P1353, DOI 10.1109/TPAMI.2018.2832121
   Bandini A., 2020, IEEE Trans. Pattern Anal. Mach. Intell., DOI [10.1109/TPAM2020.2986648, DOI 10.1109/TPAM2020.2986648]
   Bouazizi A, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667074
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chang I, 2021, 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE IN INFORMATION AND COMMUNICATION (IEEE ICAIIC 2021), P255, DOI 10.1109/ICAIIC51459.2021.9415244
   Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Corke PI, 2005, IEEE ROBOT AUTOM MAG, V12, P16, DOI 10.1109/MRA.2005.1577021
   Garrido-Jurado S, 2014, PATTERN RECOGN, V47, P2280, DOI 10.1016/j.patcog.2014.01.005
   Gu X, 2021, Arxiv, DOI arXiv:2109.01397
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P2273, DOI 10.1109/TMM.2021.3078882
   Iqbal U, 2020, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR42600.2020.00529
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Jiang H, 2017, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2017.373
   Kocabas M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11107, DOI 10.1109/ICCV48922.2021.01094
   Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117
   Li PK, 2022, IEEE T PATTERN ANAL, V44, P3260, DOI 10.1109/TPAMI.2020.3048039
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Lu ML, 2019, IEEE T IMAGE PROCESS, V28, P3703, DOI 10.1109/TIP.2019.2901707
   Luol ZY, 2021, ADV NEUR IN, V34
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Ragusa F, 2020, PATTERN RECOGN LETT, V131, P150, DOI 10.1016/j.patrec.2019.12.016
   Rhodin H, 2018, LECT NOTES COMPUT SC, V11214, P765, DOI 10.1007/978-3-030-01249-6_46
   Rhodin H, 2018, PROC CVPR IEEE, P8437, DOI 10.1109/CVPR.2018.00880
   Rhodin H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980235
   Tome D, 2023, IEEE T PATTERN ANAL, V45, P6794, DOI 10.1109/TPAMI.2020.3029700
   Wandt Bastian, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P13289, DOI 10.1109/CVPR46437.2021.01309
   Xu WP, 2019, IEEE T VIS COMPUT GR, V25, P2093, DOI 10.1109/TVCG.2019.2898650
   Yang Jianxin, 2022, 2022 International Conference on Robotics and Automation (ICRA), P1297, DOI 10.1109/ICRA46639.2022.9812051
   Yuan Y, 2019, IEEE I CONF COMP VIS, P10081, DOI 10.1109/ICCV.2019.01018
   Zhang QC, 2023, IEEE T MULTIMEDIA, V25, P572, DOI 10.1109/TMM.2021.3129056
   Zhang YH, 2021, IEEE WINT CONF APPL, P1771, DOI 10.1109/WACV48630.2021.00181
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
NR 36
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8880
EP 8891
DI 10.1109/TMM.2023.3242551
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000038
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lv, P
   Fan, JQ
   Nie, XX
   Dong, WM
   Jiang, XH
   Zhou, B
   Xu, ML
   Xu, CS
AF Lv, Pei
   Fan, Jianqi
   Nie, Xixi
   Dong, Weiming
   Jiang, Xiaoheng
   Zhou, Bing
   Xu, Mingliang
   Xu, Changsheng
TI User-Guided Personalized Image Aesthetic Assessment Based on Deep
   Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image enhancement; Task analysis; Reinforcement learning; Feature
   extraction; Visualization; Training; Neural networks; Deep reinforcement
   learning; image aesthetic assessment; personalized aesthetic
   distribution; personalized image enhancement; user interaction
ID QUALITY ASSESSMENT; NEURAL-NETWORKS; PHOTO; CLASSIFICATION; ENHANCEMENT
AB Personalized image aesthetic assessment (PIAA) has recently become a hot topic due to its wide applications, such as photography, film, television, e-commerce, fashion design, and so on. This task is more seriously affected by subjective factors and samples provided by users. In order to acquire precise personalized aesthetic distribution by small amount of samples, we propose a novel user-guided personalized image aesthetic assessment framework. This framework leverages user interactions to retouch and rank images for aesthetic assessment based on deep reinforcement learning (DRL), and generates personalized aesthetic distribution that is more in line with the aesthetic preferences of different users. It mainly consists of two stages. In the first stage, personalized aesthetic ranking is generated by interactive image enhancement and manual ranking, meanwhile, two policy networks will be trained. These two networks will be trained iteratively and alternatively to facilitate the final personalized aesthetic assessment. In the second stage, these modified images are labeled with aesthetic attributes by one style-specific classifier, and then the personalized aesthetic distribution is generated based on the multiple aesthetic attributes of these images, which conforms to the aesthetic preference of users better. Compared with other existing methods, our approach has achieved new state-of-the-art in the task of personalized image aesthetic assessment on the public AVA and FLICKR-AES datasets.
C1 [Lv, Pei; Fan, Jianqi; Nie, Xixi; Jiang, Xiaoheng; Zhou, Bing; Xu, Mingliang] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China.
   [Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, NLPR, Inst Automat, Beijing 100864, Peoples R China.
   [Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Zhengzhou University; Chinese Academy of Sciences; Institute of
   Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Xu, ML (corresponding author), Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China.
EM ielvpei@zzu.edu.cn; zzu_fjq@163.com; sunny_nxx@126.com;
   weiming.dong@ia.ac.cn; jiangxiaoheng@zzu.edu.cn; iebzhou@zzu.edu.cn;
   iexumingliang@zzu.edu.cn; csxu@nlpr.ia.ac.cn
RI DONG, Weiming/AAG-7678-2020; Zhou, heng/JCN-6493-2023; xu,
   cj/HJZ-3488-2023
OI DONG, Weiming/0000-0001-6502-145X; xu, chang sheng/0000-0001-8343-9665;
   ZHOU, BING/0000-0003-3446-3903; , Pei/0000-0002-2654-0561
FU National Natural Sci-ence Foundation of China [61772474, 61872324,
   62072136, 62172371]; Science and Technology Innovation Talents through
   the Universities of Henan Province [20HASTIT021]
FX This work was supported in part by the National Natural Sci-ence
   Foundation of China under Grants 61772474, 61872324, 62072136, and
   62172371, and in part by the Program for Science and Technology
   Innovation Talents through the Universities of Henan Province under
   Grant 20HASTIT021.
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Bhattacharya S., 2010, P 18 ACM INT C MULTI, P271
   Bianco S, 2020, IEEE T IMAGE PROCESS, V29, P6223, DOI 10.1109/TIP.2020.2989584
   Cao G, 2018, COMPUT ELECTR ENG, V66, P569, DOI 10.1016/j.compeleceng.2017.09.012
   Chen YL, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P37, DOI 10.1145/3123266.3123274
   Cui C., 2020, ACM T MULTIM COMPUT, V16, P1
   Cui CR, 2020, INFORM SCIENCES, V512, P780, DOI 10.1016/j.ins.2019.10.011
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng YB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P870, DOI 10.1145/3240508.3240531
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Du XY, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P349, DOI 10.1145/3323873.3325055
   Fang H, 2017, Arxiv, DOI arXiv:1707.03491
   Gao F, 2020, NEUROCOMPUTING, V395, P247, DOI 10.1016/j.neucom.2018.06.099
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo GJ, 2018, IEEE T MULTIMEDIA, V20, P2073, DOI 10.1109/TMM.2018.2794262
   Han-Ul Kim, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P374, DOI 10.1007/978-3-030-58577-8_23
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Ignatov A, 2018, IEEE COMPUT SOC CONF, P804, DOI 10.1109/CVPRW.2018.00112
   Ignatov A, 2017, IEEE I CONF COMP VIS, P3297, DOI 10.1109/ICCV.2017.355
   Jin B, 2016, IEEE IMAGE PROC, P2291, DOI 10.1109/ICIP.2016.7532767
   Jin X, 2018, AAAI CONF ARTIF INTE, P77
   Jin X, 2019, IET COMPUT VIS, V13, P206, DOI 10.1049/iet-cvi.2018.5249
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kao YY, 2016, SIGNAL PROCESS-IMAGE, V47, P500, DOI 10.1016/j.image.2016.05.004
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Kucer M, 2018, IEEE T IMAGE PROCESS, V27, P5100, DOI 10.1109/TIP.2018.2845100
   Li CC, 2009, IEEE J-STSP, V3, P236, DOI 10.1109/JSTSP.2009.2015077
   Li DB, 2018, PROC CVPR IEEE, P8193, DOI 10.1109/CVPR.2018.00855
   Li LD, 2020, IEEE T IMAGE PROCESS, V29, P3898, DOI 10.1109/TIP.2020.2968285
   Liao J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073683
   Lo KY, 2012, INT C PATT RECOG, P2186
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Mavridaki E, 2015, IEEE IMAGE PROC, P887, DOI 10.1109/ICIP.2015.7350927
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   O'Donovan P., 2014, Proceedings of the Workshop on Computational Aesthetics, P33
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Park K, 2017, IEEE WINT CONF APPL, P1206, DOI 10.1109/WACV.2017.139
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Schwarz K, 2018, IEEE WINT CONF APPL, P2048, DOI 10.1109/WACV.2018.00226
   Sheng KK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P879, DOI 10.1145/3240508.3240554
   Sheng-yu J., 2005, J GUANGXI TEACHERS C, V26, P4
   Son H, 2019, COMPUT GRAPH FORUM, V38, P277, DOI 10.1111/cgf.13836
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tong HG, 2004, LECT NOTES COMPUT SC, V3331, P198
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Vu T, 2019, LECT NOTES COMPUT SC, V11133, P243, DOI 10.1007/978-3-030-11021-5_16
   Wang LJ, 2019, IEEE COMPUT SOC CONF, P1833, DOI 10.1109/CVPRW.2019.00234
   Wang WN, 2019, IEEE IMAGE PROC, P1875, DOI [10.1109/icip.2019.8803119, 10.1109/ICIP.2019.8803119]
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Wang ZY, 2017, IEEE IJCNN, P941, DOI 10.1109/IJCNN.2017.7965953
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wei ZJ, 2018, PROC CVPR IEEE, P5437, DOI 10.1109/CVPR.2018.00570
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Xu Y, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102804
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Yeh C.-H., 2010, Proceedings of the international conference on Multimedia - MM'10, page, P211
   Yeh CH, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2584105
   Ying ZQ, 2017, LECT NOTES COMPUT SC, V10425, P36, DOI 10.1007/978-3-319-64698-5_4
   Zeng H, 2019, PROC CVPR IEEE, P5942, DOI 10.1109/CVPR.2019.00610
   Zhang C, 2018, SIGNAL PROCESS-IMAGE, V67, P12, DOI 10.1016/j.image.2018.05.006
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhao L, 2020, COMPUT VIS IMAGE UND, V199, DOI 10.1016/j.cviu.2020.103024
   Zhao MQ, 2016, SIGNAL PROCESS-IMAGE, V47, P511, DOI 10.1016/j.image.2016.05.009
   Zhou J, 2021, COMPUT VIS MEDIA, V7, P241, DOI 10.1007/s41095-021-0207-y
   Zhu H., 2020, IEEE T CYBERN, P1
NR 80
TC 12
Z9 12
U1 7
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 736
EP 749
DI 10.1109/TMM.2021.3130752
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Meng, DP
   Yu, CQ
   Deng, JJ
   Qian, DH
   Li, HQ
   Ren, DC
AF Meng, Depu
   Yu, Changqian
   Deng, Jiajun
   Qian, Deheng
   Li, Houqiang
   Ren, Dongchun
TI Hybrid Motion Representation Learning for Prediction From Raw Sensor
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Trajectory; Object detection; Decoding; Transformers; Feature
   extraction; Point cloud compression; Task analysis; Motion prediction;
   joint perception and prediction; autonomous driving
ID OBJECT DETECTION; R-CNN; MULTIVIEW
AB Motion prediction from raw LiDAR sensor data has drawn increasing attention and led to a surge of studies following two main paradigms. One paradigm is global motion paradigm, which simultaneously detects objects from point clouds and predicts the trajectories of each object in the future. The other paradigm is local motion paradigm, which directly performs dense motion prediction pointwisely. We observe that global motion prediction can benefit from local motion representation, since it contains rich local displacement contexts that are not explicitly exploited in global motion prediction. Correspondingly, local motion prediction can benefit from global motion representation, since it provides object contexts to improve prediction consistency inside an object. However, the complement of these two motion representations has not fully explored in the literature. To this end, we propose Hybrid Motion Representation Learning (HyMo), a unified framework to address the problem of motion prediction by making the best of both global and local motion cues. We have conducted extensive experiments on nuScenes dataset. The experimental results demonstrate that the learned hybrid motion representation achieves state-of-the-art performance on both global and local motion prediction tasks.
C1 [Meng, Depu] Univ Sci & Technol China, Dept Automat, Hefei 230027, Peoples R China.
   [Yu, Changqian; Qian, Deheng; Ren, Dongchun] Meituan, Beijing 100102, Peoples R China.
   [Deng, Jiajun; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Yu, CQ (corresponding author), Meituan, Beijing 100102, Peoples R China.
EM mdp@mail.ustc.edu.cn; yuchangqian@meituan.com; djiajun1206@gmail.com;
   835853273@qq.com; lihq@ustc.edu.cn; dongchun.ren@gmail.com
RI Deng, Jiajun/KIK-3592-2024; 钱, 德恒/KLD-3678-2024; Meng,
   Depu/GPF-6192-2022
OI Meng, Depu/0000-0003-4355-0451; Ren, Dongchun/0000-0002-0909-5419
FU National Natural Science Foundation of China
FX No Statement Available
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Bansal M, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV
   Caesar Holger, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11618, DOI 10.1109/CVPR42600.2020.01164
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Casas Sergio, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P624, DOI 10.1007/978-3-030-58592-1_37
   Casas S, 2021, PROC CVPR IEEE, P14398, DOI 10.1109/CVPR46437.2021.01417
   Casas Sergio, 2018, P C ROB LEARN PMLR, P947
   Chai Y., 2019, ARXIV191005449
   Chen XK, 2021, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR46437.2021.00264
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Chu H, 2019, IEEE I CONF COMP VIS, P4521, DOI 10.1109/ICCV.2019.00462
   Cui HG, 2019, IEEE INT CONF ROBOT, P2090, DOI [10.1109/ICRA.2019.8793868, 10.1109/icra.2019.8793868]
   Dai J., 2021, ICLR
   Dai ZG, 2021, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR46437.2021.00165
   Deng JJ, 2021, IEEE T CIRC SYST VID, V31, P4722, DOI 10.1109/TCSVT.2021.3100848
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Djuric N, 2020, IEEE WINT CONF APPL, P2084, DOI [10.1109/wacv45572.2020.9093332, 10.1109/WACV45572.2020.9093332]
   Gao P, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3601, DOI 10.1109/ICCV48922.2021.00360
   Gilles T, 2021, IEEE INT C INTELL TR, P500, DOI 10.1109/ITSC48978.2021.9564944
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gu JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15283, DOI 10.1109/ICCV48922.2021.01502
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   He H, 2020, IEEE INT C INT ROBOT, P5962, DOI 10.1109/IROS45743.2020.9340943
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Homayounfar N, 2019, IEEE I CONF COMP VIS, P2911, DOI 10.1109/ICCV.2019.00300
   Homayounfar N, 2018, PROC CVPR IEEE, P3417, DOI 10.1109/CVPR.2018.00360
   Hong J, 2019, PROC CVPR IEEE, P8446, DOI 10.1109/CVPR.2019.00865
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Huang T. S., 2016, P 24 ACM INT C MULT, P516, DOI 10.1145/2964284.2967274
   Jiyang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11522, DOI 10.1109/CVPR42600.2020.01154
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kong T, 2020, IEEE T IMAGE PROCESS, V29, P7389, DOI 10.1109/TIP.2020.3002345
   Ku J, 2018, IEEE INT C INT ROBOT, P5750, DOI 10.1109/IROS.2018.8594049
   Law H., 2020, P BRIT MACH VIS C
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li LL, 2020, IEEE INT C INT ROBOT, P5784, DOI 10.1109/IROS45743.2020.9341392
   Li YH, 2019, IEEE I CONF COMP VIS, P6053, DOI 10.1109/ICCV.2019.00615
   Liang J, 2019, PROC CVPR IEEE, P9504, DOI 10.1109/CVPR.2019.00974
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YF, 2022, LECT NOTES COMPUT SC, V13687, P531, DOI 10.1007/978-3-031-19812-0_31
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2929, DOI 10.1109/ICCV48922.2021.00294
   Lu X, 2019, PROC CVPR IEEE, P7355, DOI 10.1109/CVPR.2019.00754
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Luo WJ, 2018, PROC CVPR IEEE, P3569, DOI 10.1109/CVPR.2018.00376
   Mangalam Karttikeya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P759, DOI 10.1007/978-3-030-58536-5_45
   Máttyus G, 2015, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2015.197
   Meng DP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3631, DOI 10.1109/ICCV48922.2021.00363
   Meyer GP, 2019, PROC CVPR IEEE, P12669, DOI 10.1109/CVPR.2019.01296
   Ming Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P541, DOI 10.1007/978-3-030-58536-5_32
   Ming Liang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11550, DOI 10.1109/CVPR42600.2020.01157
   Misra I, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2886, DOI 10.1109/ICCV48922.2021.00290
   Paszke A, 2019, ADV NEUR IN, V32
   Pengxiang Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11382, DOI 10.1109/CVPR42600.2020.01140
   Phillips J, 2021, PROC CVPR IEEE, P4677, DOI 10.1109/CVPR46437.2021.00465
   Qi CR, 2019, IEEE I CONF COMP VIS, P9276, DOI 10.1109/ICCV.2019.00937
   Qiu Y, 2023, IEEE T MULTIMEDIA, V25, P1991, DOI 10.1109/TMM.2022.3141933
   Redmon J., 2016, P IEEE C COMP VIS PA, P7709
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rehder E, 2018, IEEE INT CONF ROBOT, P5903
   Rehder E, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P139, DOI 10.1109/ICCVW.2015.28
   Rhinehart N, 2019, IEEE I CONF COMP VIS, P2821, DOI 10.1109/ICCV.2019.00291
   Robicquet A, 2016, LECT NOTES COMPUT SC, V9912, P549, DOI 10.1007/978-3-319-46484-8_33
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi S., 2021, Int. J. Comput. Vis.., V131, P1
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Sun C., 2019, P INT C LEARN REPR
   Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tung Phan-Minh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14062, DOI 10.1109/CVPR42600.2020.01408
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Y., 2022, P C ROB LEARN, P180, DOI [10.48550/arXiv.2110.06922, DOI 10.48550/ARXIV.2110.06922]
   Yang B, 2018, PROC CVPR IEEE, P7652, DOI 10.1109/CVPR.2018.00798
   Yang Bin, 2018, C ROBOT LEARNING, P146
   Yang JR, 2022, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR52688.2022.00531
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Ye MS, 2021, PROC CVPR IEEE, P11313, DOI 10.1109/CVPR46437.2021.01116
   Zeng WY, 2021, IEEE INT C INT ROBOT, P532, DOI 10.1109/IROS51168.2021.9636035
   Zhang DL, 2020, IEEE T CYBERNETICS, V50, P3033, DOI 10.1109/TCYB.2019.2905157
   Zhang GJ, 2022, PROC CVPR IEEE, P939, DOI 10.1109/CVPR52688.2022.00102
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhao H., 2020, TNT TARGET DRIVEN TR
   Zhou X., 2019, arXiv
   Zhu BJ, 2019, Arxiv, DOI arXiv:1908.09492
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
NR 88
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8868
EP 8879
DI 10.1109/TMM.2023.3242581
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000014
DA 2024-07-18
ER

PT J
AU Nie, JH
   He, ZW
   Yang, YX
   Gao, MY
   Dong, ZK
AF Nie, Jiahao
   He, Zhiwei
   Yang, Yuxiang
   Gao, Mingyu
   Dong, Zhekang
TI Learning Localization-Aware Target Confidence for Siamese Visual
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Task analysis; Feature extraction; Training; Location
   awareness; Visualization; Smoothing methods; Localization-aware
   components; Siamese tracking paradigm; task misalignment
ID OBJECT TRACKING
AB Siamese tracking paradigm has achieved great success, providing effective appearance discrimination and size estimation by classification and regression. While such a paradigm typically optimizes the classification and regression independently, leading to task misalignment (accurate prediction boxes have no high target confidence scores). In this paper, to alleviate this misalignment, we propose a novel tracking paradigm, called SiamLA. Within this paradigm, a series of simple, yet effective localization-aware components are introduced to generate localization-aware target confidence scores. Specifically, with the proposed localization-aware dynamic label (LADL) loss and localization-aware label smoothing (LALS) strategy, collaborative optimization between the classification and regression is achieved, enabling classification scores to be aware of location state, not just appearance similarity. Besides, we propose a separate localization-aware quality prediction (LAQP) branch to produce location quality scores to further modify the classification scores. To guide a more reliable modification, a novel localization-aware feature aggregation (LAFA) module is designed and embedded into this branch. Consequently, the resulting target confidence scores are more discriminative for the location state, allowing accurate prediction boxes tend to be predicted as high scores. Extensive experiments are conducted on six challenging benchmarks, including GOT10 k, TrackingNet, LaSOT, TNL2K, OTB100 and VOT2018. Our SiamLA achieves competitive performance in terms of both accuracy and efficiency. Furthermore, a stability analysis reveals that our tracking paradigm is relatively stable, implying that the paradigm is potential for real-world applications.
C1 [Nie, Jiahao; He, Zhiwei; Gao, Mingyu] Hangzhou Dianzi Univ, Sch Elect Informat, Hangzhou 310018, Peoples R China.
   [Yang, Yuxiang] Univ Sci & Technol China, Sch Control Sci & Engn, Hefei 230052, Peoples R China.
   [Dong, Zhekang] Zhejiang Univ, Sch Elect Engn, Hangzhou 310058, Peoples R China.
C3 Hangzhou Dianzi University; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Zhejiang University
RP He, ZW (corresponding author), Hangzhou Dianzi Univ, Sch Elect Informat, Hangzhou 310018, Peoples R China.
EM jhnie@hdu.edu.cn; zwhe@hdu.edu.cn; yyx@hdu.edu.cn; mackgao@hdu.edu.cn;
   englishp@126.com
RI Nie, Jiahao/HNP-4027-2023
OI Nie, Jiahao/0000-0002-1474-1817; He, Zhiwei/0000-0001-7264-2019
FU National Natural Science Foundation of China [61571394, 62001149]; Key
   Research and Development Program in Zhejiang Province of China
   [2020C03098]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61571394 and 62001149 and in part by
   the Key Research and Development Program in Zhejiang Province of China
   under Grant 2020C03098.
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bingyan Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P429, DOI 10.1007/978-3-030-58542-6_26
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan NN, 2021, NEURAL NETWORKS, V140, P344, DOI 10.1016/j.neunet.2021.04.004
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Han G, 2023, IEEE T MULTIMEDIA, V25, P430, DOI 10.1109/TMM.2021.3127357
   Han WC, 2021, PROC CVPR IEEE, P16565, DOI 10.1109/CVPR46437.2021.01630
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kristan M., 2020, COMPUTER VISION ECCV, P547
   Kristan M, 2021, IEEE INT CONF COMP V, P2711, DOI 10.1109/ICCVW54120.2021.00305
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kriznar A, 2012, ACTA ARTIS ACADEMICA 2012: KNOWLEDGE AND EXPERIENCE IN THE FINE ART, P25
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li MH, 2021, IEEE T MULTIMEDIA, V23, P105, DOI 10.1109/TMM.2020.2978623
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P2114, DOI 10.1109/TMM.2020.3008028
   Liu Q, 2023, IEEE T MULTIMEDIA, V25, P1269, DOI 10.1109/TMM.2022.3140929
   Lukezic A, 2020, PROC CVPR IEEE, P7131, DOI 10.1109/CVPR42600.2020.00716
   Marvasti-Zadeh SM, 2022, IEEE T INTELL TRANSP, V23, P3943, DOI 10.1109/TITS.2020.3046478
   Mayer C, 2022, Arxiv, DOI arXiv:2203.11192
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nie JH, 2022, IEEE T CIRC SYST VID, V32, P6186, DOI 10.1109/TCSVT.2022.3162599
   Peng Jinlong, 2021, arXiv
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sun ZH, 2021, IEEE T CIRC SYST VID, V31, P1819, DOI 10.1109/TCSVT.2020.3009717
   Tang F, 2021, IEEE T IMAGE PROCESS, V30, P8785, DOI 10.1109/TIP.2021.3120305
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2019, PROC CVPR IEEE, P3638, DOI 10.1109/CVPR.2019.00376
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wang X, 2021, PROC CVPR IEEE, P13758, DOI 10.1109/CVPR46437.2021.01355
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie F., 2022, arXiv
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Xu YF, 2021, 35 C NEURAL INFORM P
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yang K, 2022, IEEE T MULTIMEDIA, V24, P1956, DOI 10.1109/TMM.2021.3074239
   Yang TY, 2020, PROC CVPR IEEE, P6717, DOI 10.1109/CVPR42600.2020.00675
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zha YF, 2020, IEEE T MULTIMEDIA, V22, P96, DOI 10.1109/TMM.2019.2922125
   Zhang J, 2021, IEEE INTERNET THINGS, V8, P7789, DOI 10.1109/JIOT.2020.3039359
   Zhang QM, 2022, Arxiv, DOI [arXiv:2202.10108, 10.48550/arXiv.2202.10108]
   Zhang SQ, 2021, IEEE T MULTIMEDIA, V23, P859, DOI 10.1109/TMM.2020.2990089
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 63
TC 11
Z9 11
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6194
EP 6206
DI 10.1109/TMM.2022.3206668
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500039
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Qin, BS
   Hu, HJ
   Zhuang, YT
AF Qin, Bosheng
   Hu, Haoji
   Zhuang, Yueting
TI Deep Residual Weight-Sharing Attention Network With Low-Rank Attention
   for Visual Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visual question answering; low-rank attention; weight-sharing; residual
   learning
ID INFORMATION FUSION
AB The attention-based networks have become prevailing recently in visual question answering (VQA) due to their high performances. However, the extensive memory consumption of attention-based models poses excessive-high demand for the implementation equipment, raising concerns about their future application scenarios. Therefore, designing an efficient and lightweight VQA model is central to expanding possible application areas. Our work presents a novel lightweight attention-based VQA model, namely residual weight-sharing attention network (RWSAN), consisting of residual weight-sharing attention (RWSA) layers cascaded in depth. Each RWSA layer models the textual representation with self residual weight-sharing attention (SRWSA) and captures question features and question-image interactions with self-guided residual weight-sharing attention (SGRWSA). Inside each RWSA layer, the proposed low-rank attention (LRA) units perform residual learning with learned connection patterns and shared parameters, and every stacked RWSA layer also uses the same parameters. Extensive ablation experiments with quantitative and qualitative analysis are conducted to illustrate the effectiveness and generality of RWSA. Experiments on VQA-v2, GQA, and CLEVR datasets show that the RWSAN achieves competitive performance with much fewer parameters over the state-of-the-art methods. We release our code at https://github.com/BrightQin/RWSAN.
C1 [Qin, Bosheng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Qin, Bosheng; Hu, Haoji] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Hu, Haoji] Zhejiang Prov Key Lab Informat Proc Commun & Netwo, Hangzhou 310027, Peoples R China.
   [Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University
RP Hu, HJ (corresponding author), Zhejiang Prov Key Lab Informat Proc Commun & Netwo, Hangzhou 310027, Peoples R China.
EM bsqin@zju.edu.cn; haoji_hu@zju.edu.cn; yzhuang@zju.edu.cn
RI Qin, Bosheng/AAD-6679-2021
OI Qin, Bosheng/0000-0003-1978-9999; Hu, Haoji/0000-0001-6048-6549
FU National Natural Science Foundation of China [U21B2004]; Zhejiang
   Provincial key RD Program, China [2021C01119]
FX This work was supported in by the National Natural Science Foundation of
   China under Grant U21B2004, and in part by the Zhejiang Provincial key
   RD Program, China, under Grant 2021C01119.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Arriaga RI, 2006, MACH LEARN, V63, P161, DOI 10.1007/s10994-006-6265-7
   Bai Shaojie, 2019, Advances in Neural Information Processing Systems, P690
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Chen K, 2016, Arxiv, DOI [arXiv:1511.05960, DOI 10.48550/ARXIV.1511.05960,ARXIV]
   Dabre R, 2019, AAAI CONF ARTIF INTE, P6292
   Dehghani M., 2018, ARXIV180703819
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Fan A., 2020, P INT C LEARN REPR
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gouthaman KV, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107812
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hao X, 2019, INT CONF ACOUST SPEE, P6895, DOI 10.1109/ICASSP.2019.8683169
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hsu Y.-T., 2020, P SUSTAINLP WORKSH S, P48
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215
   Johnson W.B., 1984, CONTEMP MATH-SINGAP, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005
   Katharopoulos A, 2020, PR MACH LEARN RES, V119
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kim JH, 2018, ADV NEUR IN, V31
   Kim Yoon, 2017, arXiv preprint, arXiv170200887, P2
   Kingma D. P., 2014, arXiv
   Kitaev Nikita, 2020, INT C LEARN REPR
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Gupta AK, 2017, Arxiv, DOI arXiv:1705.03865
   Lan Z., 2019, ARXIV190911942, DOI DOI 10.48550/ARXIV.1909.11942
   Lee D, 2021, AAAI CONF ARTIF INTE, V35, P1845
   Lei Ba J., 2016, arXiv
   Li X., 2020, P EUR C COMP VIS, DOI DOI 10.1007/978-3-030-58577-8_8
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Li YY, 2021, AAAI CONF ARTIF INTE, V35, P13315
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Zhouhan, 2017, A structured self-attentive sentence embedding
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Ma X, 2021, ADV NEURAL INFORM PR, V34
   Manmadhan S, 2020, ARTIF INTELL REV, V53, P5705, DOI 10.1007/s10462-020-09832-7
   Miao HR, 2020, INT CONF ACOUST SPEE, P6084, DOI [10.1109/icassp40776.2020.9053165, 10.1109/ICASSP40776.2020.9053165]
   Michel P, 2019, ADV NEUR IN, V32
   Paszke A., 2017, ADV NEURAL INF PROCE, V9, P1, DOI DOI 10.1017/CB09781107707221.009
   Peng H., 2021, P INT C LEARN REPR
   Peng L, 2022, IEEE T PATTERN ANAL, V44, P318, DOI 10.1109/TPAMI.2020.3004830
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Rahman T, 2021, IEEE COMPUT SOC CONF, P1653, DOI 10.1109/CVPRW53098.2021.00181
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy A, 2021, T ASSOC COMPUT LING, V9, P53, DOI 10.1162/tacl_a_00353
   Shi L, 2020, INT CONF ACOUST SPEE, P4412, DOI [10.1109/ICASSP40776.2020.9053595, 10.1109/icassp40776.2020.9053595]
   Smith Noah A., 2021, P INT C LEARN REPR
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stefanini M, 2021, INT C PATT RECOG, P1212, DOI 10.1109/ICPR48806.2021.9413269
   Suhr A, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6418
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   Vaswani A, 2017, ADV NEUR IN, V30
   Voita E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5797
   Wang SN, 2020, Arxiv, DOI arXiv:2006.04768
   Xiao T, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5292
   XIONG P., 2022, arXiv
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Yu Zhou, 2019, Openvqa
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang DX, 2019, INFORM FUSION, V52, P268, DOI 10.1016/j.inffus.2019.03.005
   Zhang PC, 2021, PROC CVPR IEEE, P5575, DOI 10.1109/CVPR46437.2021.00553
   Zhou LW, 2020, AAAI CONF ARTIF INTE, V34, P13041
   Zhou YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2054, DOI 10.1109/ICCV48922.2021.00208
NR 81
TC 8
Z9 8
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4282
EP 4295
DI 10.1109/TMM.2022.3173131
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200015
DA 2024-07-18
ER

PT J
AU Shao, ZR
   Pu, YF
   Zhou, JL
   Wen, BH
   Zhang, Y
AF Shao, Zerui
   Pu, Yifei
   Zhou, Jiliu
   Wen, Bihan
   Zhang, Yi
TI Hyper RPCA: Joint Maximum Correntropy Criterion and Laplacian Scale
   Mixture Modeling on-the-Fly for Moving Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data models; Adaptation models; Laplace equations; Streaming media;
   Kernel; Surveillance; Spatiotemporal phenomena; Moving object detection;
   background subtraction; maximum correntropy criterion; laplacian scale
   mixture model
ID ROBUST-PCA; IMAGE; APPROXIMATION; SIGNAL
AB Moving object detection is critical for automated video analysis in many vision-related tasks, such as surveillance tracking, video compression coding, etc. Robust Principal Component Analysis (RPCA), as one of the most popular moving object modelling methods, aims to separate the temporally-varying (i.e., moving) foreground objects from the static background in video, assuming the background frames to be low-rank while the foreground to be spatially sparse. Classic RPCA imposes sparsity of the foreground component using L-1-norm, and minimizes the modeling error via (2)-norm pound. We show that such assumptions can be too restrictive in practice, which limits the effectiveness of the classic RPCA, especially when processing videos with dynamic background, camera jitter, camouflaged moving object, etc. In this paper, we propose a novel RPCA-based model, called Hyper RPCA, to detect moving objects on the fly. Different from classic RPCA, the proposed Hyper RPCA jointly applies the maximum correntropy criterion (MCC) for the modeling error, and Laplacian scale mixture (LSM) model for foreground objects. Extensive experiments have been conducted, and the results demonstrate that the proposed Hyper RPCA has competitive performance for foreground detection to the state-of-the-art algorithms on several well-known benchmark datasets.
C1 [Shao, Zerui; Pu, Yifei; Zhou, Jiliu; Zhang, Yi] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Wen, Bihan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Sichuan University; Nanyang Technological University
RP Zhang, Y (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.; Wen, BH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zeruishao@outlook.com; puyifei@scu.edu.cn; zhoujl@scu.edu.cn;
   bihan.wen@ntu.edu.sg; yzhang@scu.edu.cn
RI Wen, Bihan/B-3123-2017
OI Wen, Bihan/0000-0002-6874-6453; shao, zerui/0000-0002-4936-031X
FU National Key R&D Program of China [2018YFC0830300]; Advanced Deployed
   Discipline of Sichuan University LAIW (AI in LAW), China; Sichuan
   Science and Technology Program [2021JDJQ0024]
FX This work was supported by the National Key R&D Program of China under
   Grant 2018YFC0830300, by the Advanced Deployed Discipline of Sichuan
   University LAIW (AI in LAW), China, and by Sichuan Science and
   Technology Program under Grant 2021JDJQ0024.
CR Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bertsekas D. P., 1999, Nonlinear Program, V2nd
   Bouwmans T, 2017, COMPUT SCI REV, V23, P1, DOI 10.1016/j.cosrev.2016.11.001
   Bouwmans T, 2018, P IEEE, V106, P1427, DOI 10.1109/JPROC.2018.2853589
   Box G.E., 2011, Bayesian inference in statistical analysis
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cao XC, 2016, IEEE T CYBERNETICS, V46, P1014, DOI 10.1109/TCYB.2015.2419737
   Chen BH, 2019, IEEE T CIRC SYST VID, V29, P982, DOI 10.1109/TCSVT.2018.2828606
   Chen BH, 2014, IEEE T MULTIMEDIA, V16, P837, DOI 10.1109/TMM.2014.2298377
   Chen C., 2012, P 20 ACM INT C MULT, P713, DOI DOI 10.1145/2393347.2396294
   Chen DT, 2008, IEEE T MULTIMEDIA, V10, P268, DOI 10.1109/TMM.2007.911835
   Chen H, 2018, IEEE T MED IMAGING, V37, P1333, DOI 10.1109/TMI.2018.2805692
   Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743
   Cullen D, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P7, DOI 10.1109/AVSS.2012.35
   Dong WS, 2018, IEEE J-STSP, V12, P1435, DOI 10.1109/JSTSP.2018.2873047
   Du B, 2017, IEEE T IMAGE PROCESS, V26, P1694, DOI 10.1109/TIP.2017.2651372
   Ebadi SE, 2018, IEEE T PATTERN ANAL, V40, P2273, DOI 10.1109/TPAMI.2017.2745573
   Feng Jiashi, 2013, Advances in Neural Information Processing Systems, P404
   Gao Z, 2014, IEEE T PATTERN ANAL, V36, P1975, DOI 10.1109/TPAMI.2014.2314663
   Garrigues P., 2010, Advances in neural information processing systems, V23, P676
   Gemignani G, 2016, IEEE T IMAGE PROCESS, V25, P5239, DOI 10.1109/TIP.2016.2605004
   Guo KL, 2018, IEEE T NEUR NET LEAR, V29, P2323, DOI 10.1109/TNNLS.2016.2643286
   He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848
   He R, 2011, IEEE T IMAGE PROCESS, V20, P1485, DOI 10.1109/TIP.2010.2103949
   He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220
   He YC, 2020, IEEE T SIGNAL PROCES, V68, P181, DOI 10.1109/TSP.2019.2952057
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Huang T, 2017, IEEE T IMAGE PROCESS, V26, P3171, DOI 10.1109/TIP.2017.2676466
   Javed S, 2019, IEEE T IMAGE PROCESS, V28, P1007, DOI 10.1109/TIP.2018.2874289
   Javed S, 2018, IEEE T CIRC SYST VID, V28, P1315, DOI 10.1109/TCSVT.2016.2632302
   Javed S, 2017, IEEE T IMAGE PROCESS, V26, P5840, DOI 10.1109/TIP.2017.2746268
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   Jodoin PM, 2012, IEEE T IMAGE PROCESS, V21, P4244, DOI 10.1109/TIP.2012.2199326
   Kang Z, 2015, IEEE DATA MINING, P211, DOI 10.1109/ICDM.2015.15
   Li LY, 2004, IEEE T IMAGE PROCESS, V13, P1459, DOI 10.1109/TIP.2004.836169
   Liu WF, 2007, IEEE T SIGNAL PROCES, V55, P5286, DOI 10.1109/TSP.2007.896065
   Lu CY, 2013, IEEE I CONF COMP VIS, P1801, DOI 10.1109/ICCV.2013.226
   Maddalena L., 2012, 2012 IEEE COMP SOC C, P21, DOI [10.1109/CVPRW.2012.6238922, DOI 10.1109/CVPRW.2012.6238922]
   Nikolova M, 2005, SIAM J SCI COMPUT, V27, P937, DOI 10.1137/030600862
   Ning Q, 2020, AAAI CONF ARTIF INTE, V34, P11791
   Oreifej O, 2013, IEEE T PATTERN ANAL, V35, P450, DOI 10.1109/TPAMI.2012.97
   Pan ZL, 2007, IEEE T MULTIMEDIA, V9, P268, DOI 10.1109/TMM.2006.887992
   Paul M, 2010, INT CONF ACOUST SPEE, P734, DOI 10.1109/ICASSP.2010.5495033
   Rodriguez P, 2016, J MATH IMAGING VIS, V55, P1, DOI 10.1007/s10851-015-0610-z
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Shakeri M, 2016, COMPUT VIS IMAGE UND, V146, P27, DOI 10.1016/j.cviu.2016.02.009
   Shi G, 2018, IEEE T IMAGE PROCESS, V27, P4810, DOI 10.1109/TIP.2018.2845123
   Shijila B, 2019, FUTURE GENER COMP SY, V90, P198, DOI 10.1016/j.future.2018.07.065
   St-Charles PL, 2016, IEEE T IMAGE PROCESS, V25, P4768, DOI 10.1109/TIP.2016.2598691
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauder J, 1999, IEEE T MULTIMEDIA, V1, P65, DOI 10.1109/6046.748172
   Tianyi Zhou, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1286, DOI 10.1109/ISIT.2012.6283064
   Tom AJ, 2021, IEEE T CYBERNETICS, V51, P1004, DOI 10.1109/TCYB.2019.2921827
   Toyama K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P255, DOI 10.1109/ICCV.1999.791228
   Vaswani N, 2018, IEEE SIGNAL PROC MAG, V35, P32, DOI 10.1109/MSP.2018.2826566
   Wang NY, 2012, LECT NOTES COMPUT SC, V7578, P126, DOI 10.1007/978-3-642-33786-4_10
   Wang Y, 2014, IEEE COMPUT SOC CONF, P393, DOI 10.1109/CVPRW.2014.126
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xin B, 2015, PROC CVPR IEEE, P4676, DOI 10.1109/CVPR.2015.7299099
   Xu J, 2013, IEEE I CONF COMP VIS, P3376, DOI 10.1109/ICCV.2013.419
   Ye XC, 2015, IEEE T CIRC SYST VID, V25, P1721, DOI 10.1109/TCSVT.2015.2392491
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yong HW, 2018, IEEE T PATTERN ANAL, V40, P1726, DOI 10.1109/TPAMI.2017.2732350
   Zhang JP, 2020, IEEE T GEOSCI REMOTE, V58, P6420, DOI 10.1109/TGRS.2020.2976855
   Zhao Q, 2014, PR MACH LEARN RES, V32, P55
   Zheng JY, 2006, TRANSPORT RES REC, P82
   Zheng YQ, 2012, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2012.6247828
   Zhou TB, 2011, INT J NEPHROL, V2011, DOI 10.4061/2011/360357
   Zhou XW, 2013, IEEE T PATTERN ANAL, V35, P597, DOI 10.1109/TPAMI.2012.132
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 70
TC 4
Z9 4
U1 4
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 112
EP 125
DI 10.1109/TMM.2021.3121571
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 8C6OL
UT WOS:000917725300002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, LY
   Shang, XQ
   Yang, C
   Sun, MX
AF Song, Lingyun
   Shang, Xuequn
   Yang, Chen
   Sun, Mingxuan
TI Attribute-Guided Multiple Instance Hashing Network for Cross-Modal
   Zero-Shot Hashing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attribute; cross-modal hashing (CMH); deep learning; information
   retrieval; zero-shot learning (ZSL)
AB Cross-Modal Zero-Shot Hashing (CMZSH) is an important image retrieval technique, e.g., Text Based Image Retrieval. Most of existing CMZSH methods mainly use semantic attributes as guidance to generate hash codes for both the images and texts of seen and unseen categories. However, existing CMZSH methods only focus on learning global attribute vectors and hash codes for images, which mixes up information of complex semantics and background clutters, and thus impedes the retrieval performance. To solve this issue, we propose an Attribute-Guided Multiple Instance Hashing (AG-MIH) network for CMZSH, where each instance represents one image region. Instead of generating global image hash codes, AG-MIH can effectively learn instance-level hash codes based on instance attributes. To improve the attribute learning for instances, AG-MIH can exploia novel 2-D Category-Attribute Relation (CAR) layer, which uses different matching templates to model the relationships between each instance and the attributes for different categories. Under the guidance of semantic attributes, AG-MIH can effectively learn hash codes for each visual instance and texts by a Multi-stream Instance Hashing Refinement (MIHR) procedure. In the MIHR, the pseudo supervisions for the instance-level attributes and hash codes in each stream are from its proceeding stream. Empirical studies on benchmark datasets show that AG-MIH achieves state-of-the-art performance on both cross-modal and single-modal zero-shot image retrieval tasks.
C1 [Song, Lingyun; Shang, Xuequn; Yang, Chen] Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Shaanxi, Peoples R China.
   [Song, Lingyun; Shang, Xuequn; Yang, Chen] Northwestern Polytech Univ, Key Lab Big Data Storage & Management, Minist Ind & Informat Technol, Xian 710129, Shaanxi, Peoples R China.
   [Sun, Mingxuan] Louisiana State Univ, Sch Elect Engn & Comp Sci, Div Comp Sci & Engn, Baton Rouge 70803, LA USA.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University; Louisiana State University System; Louisiana State
   University
RP Song, LY; Shang, XQ (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710129, Shaanxi, Peoples R China.
EM lysong@nwpu.edu.cn; shang@nwpu.edu.cn; yangchen803@mail.nwpu.edu.cn;
   msun@csc.lsu.edu
RI Gao, Bowen/JSK-7810-2023
FU National Key Research and Development Program of China [2020AAA0108500];
   National Nature Science Foundation of China [62102321, 61772426];
   Fundamental Research Funds for the Central Universities [D5000200146]
FX The work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0108500, in part by the
   National Nature Science Foundation of China under Grants 62102321 and
   61772426, in part by the Fundamental Research Funds for the Central
   Universities under Grant D5000200146.
CR Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Bucher M, 2016, LECT NOTES COMPUT SC, V9909, P730, DOI 10.1007/978-3-319-46454-1_44
   Cao Y, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P197, DOI 10.1145/2911996.2912000
   Huynh D, 2020, PROC CVPR IEEE, P4482, DOI 10.1109/CVPR42600.2020.00454
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding GG, 2016, IEEE T IMAGE PROCESS, V25, P5427, DOI 10.1109/TIP.2016.2607421
   Guo YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1767
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Jayaraman D, 2014, ADV NEUR IN, V27
   Ji Z, 2020, IEEE T NEUR NET LEAR, V31, P321, DOI 10.1109/TNNLS.2019.2904991
   Jiang Q.-Y., 2019, arXiv
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Lai HJ, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P196, DOI 10.1145/3206025.3206026
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li X., 2020, P INT JOINT C NEUR N
   Li X, 2021, INT C PATT RECOG, P1836, DOI 10.1109/ICPR48806.2021.9412798
   Li Y, 2023, IEEE T MULTIMEDIA, V25, P1600, DOI 10.1109/TMM.2021.3139211
   Li ZH, 2019, PATTERN RECOGN, V88, P595, DOI 10.1016/j.patcog.2018.12.010
   Liu H, 2017, PROC CVPR IEEE, P6345, DOI 10.1109/CVPR.2017.672
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu XW, 2019, IEEE DATA MINING, P449, DOI 10.1109/ICDM.2019.00055
   Liu XW, 2019, AAAI CONF ARTIF INTE, P4400
   Luo MN, 2018, IEEE T CYBERNETICS, V48, P648, DOI 10.1109/TCYB.2017.2647904
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998
   Qi G., 2011, Proc. ACM International Conference on World Wide Web, P297
   Qi GJ, 2017, IEEE T PATTERN ANAL, V39, P1360, DOI 10.1109/TPAMI.2016.2587643
   Shen FM, 2019, IEEE T IMAGE PROCESS, V28, P3662, DOI 10.1109/TIP.2019.2899987
   Shen FM, 2015, IEEE T IMAGE PROCESS, V24, P1839, DOI 10.1109/TIP.2015.2405340
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Song LY, 2019, AAAI CONF ARTIF INTE, P8885
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Taherkhani Fariborz, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P279, DOI 10.1109/TBIOM.2020.2983467
   Tu RC, 2022, IEEE T KNOWL DATA EN, V34, P560, DOI 10.1109/TKDE.2020.2987312
   van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P1217, DOI 10.1109/TMM.2022.3140656
   Wang R., 2021, arXiv
   Wu BT, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3946
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu YH, 2017, IEEE INT CON MULTI, P133, DOI 10.1109/ICME.2017.8019425
   Yang EK, 2017, AAAI CONF ARTIF INTE, P1618
   YANG Y, 2016, P 24 ACM INT C MULT, P1286, DOI DOI 10.1145/2964284.2964319
   Yao HT, 2022, IEEE T MULTIMEDIA, V24, P1933, DOI 10.1109/TMM.2021.3074252
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Zeng HT, 2020, IEEE T MULTIMEDIA, V22, P1519, DOI 10.1109/TMM.2019.2944241
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang J, 2020, IEEE T MULTIMEDIA, V22, P174, DOI 10.1109/TMM.2019.2922128
NR 50
TC 7
Z9 7
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5305
EP 5318
DI 10.1109/TMM.2022.3190222
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300049
DA 2024-07-18
ER

PT J
AU Sun, ZR
   Yao, YZ
   Wei, XS
   Shen, FM
   Zhang, J
   Hua, XS
AF Sun, Zeren
   Yao, Yazhou
   Wei, Xiu-Shen
   Shen, Fumin
   Zhang, Jian
   Hua, Xian-Sheng
TI Boosting Robust Learning Via Leveraging Reusable Samples in Noisy Web
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; fine-grained classification; label noise; robust
   learning; sample selection
ID IMAGES; PARTS
AB Webly-supervised fine-grained visual classification (FGVC) has attracted increasing attention in recent years because of the unaffordable cost of obtaining correctly-labeled large-scale fine-grained datasets. However, due to the existence of label noise in web images and the high memorization capacity of deep neural networks, training deep fine-grained (FG) models directly through web images tends to have an inferior recognition ability. In the literature, to alleviate this issue, loss correction methods try to estimate the noise transition matrix, but the inevitable false correction would cause accumulated errors. Sample selection methods identify clean ("easy") samples based on the fact that small losses can alleviate the accumulated errors. However, "hard" and mislabeled examples that can both boost the robustness of FG models are also dropped. To this end, we propose a certainty-based reusable sample selection and correction approach, termed as CRSSC, for coping with label noise in training deep FG models with web images. Our key idea is to additionally identify and correct reusable samples, and then leverage them together with clean examples to update the network. Furthermore, in order to endow our model with the capability to capture richer and more discriminative feature representations, we propose a cross-layer attention-based feature refinement (CLAR) block. We demonstrate the superiority of the proposed approach from both theoretical and experimental perspectives.
C1 [Sun, Zeren; Yao, Yazhou; Wei, Xiu-Shen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
   [Zhang, Jian] Univ Technol Sydney, Fac Engn & IT, Ultimo, NSW 2007, Australia.
   [Hua, Xian-Sheng] Alibaba Grp, Hangzhou 311121, Peoples R China.
C3 Nanjing University of Science & Technology; University of Electronic
   Science & Technology of China; University of Technology Sydney; Alibaba
   Group
RP Yao, YZ (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zerens@njust.edu.cn; yazhou.yao@njust.edu.cn; weixiushen@megvii.com;
   fumin.shen@gmail.com; xiansheng.hxs@alibaba-inc.com
RI Shen, Fumin/R-2121-2016
OI Yao, Yazhou/0000-0002-0337-9410; Sun, Zeren/0000-0001-6262-5338; Hua,
   Xian-Sheng/0000-0002-8232-5049; Zhang, Jian/0000-0002-7240-3541
FU National Natural Science Foundation of China [62102182, 61976116,
   61905114]; Natural Science Foundation of Jiangsu Province [BK20210327];
   Fundamental Research Funds [30920021135]; National Key Ramp;D Program of
   China [2021YFF0602101]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62102182, 61976116, and 61905114, in
   part by the Natural Science Foundation of Jiangsu Province under Grant
   BK20210327, in part by the Fundamental Research Funds for the Central
   Universities under Grant 30920021135, and in part by the National Key R
   & D Program of China under Grant 2021YFF0602101. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Erkut Erdem.& nbsp;
CR [Anonymous], 2015, P INT C LEARN REPR
   Arpit D, 2017, PR MACH LEARN RES, V70
   Branson S., 2014, BIRD SPECIES CATEGOR, V1, P7
   Chang HS, 2017, ADV NEUR IN, V30
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Dubey A, 2018, ADV NEUR IN, V31
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gan C, 2017, AAAI CONF ARTIF INTE, P4032
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gan C, 2016, LECT NOTES COMPUT SC, V9907, P849, DOI 10.1007/978-3-319-46487-9_52
   Ganapathiraju A., 2000, INTERSPEECH, P210
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Gold JR, 2017, PLAN HIST ENVIRON SE, P1
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jiang Lu, 2017, PROC INT C MACH LEAR, P2309
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li J, 2021, IEEE T PATTERN ANAL, V43, P1808, DOI 10.1109/TPAMI.2019.2961910
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Liang S, 2017, AEBMR ADV ECON, V34, P1
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2018, IEEE T PATTERN ANAL, V40, P1309, DOI 10.1109/TPAMI.2017.2723400
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P546, DOI 10.1109/TMM.2021.3055024
   Maji, 2017, BMVC
   Malach E, 2017, ADV NEUR IN, V30
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Pawan Kumar M., 2010, NIPS
   Ramachandran P, 2019, ADV NEUR IN, V32
   Reed S, 2014, Training deep neural networks on noisy labels with bootstrapping
   Ren J, 2019, 33 C NEURAL INFORM P, V32
   Ren MY, 2018, PR MACH LEARN RES, V80
   Rosales R., 2009, Proc. Snowbird Mach. Learn. Workshop, P1
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song H, 2019, PR MACH LEARN RES, V97
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P92, DOI 10.1145/3394171.3413978
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedaldi A., 2013, Technical report
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290
   Yang JF, 2018, IEEE T IMAGE PROCESS, V27, P5303, DOI 10.1109/TIP.2018.2855449
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yu Xingrui, 2019, PROC INT C MACH LEAR, P7164
   Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zheng XT, 2021, IEEE T MULTIMEDIA, V23, P1187, DOI 10.1109/TMM.2020.2993960
NR 78
TC 2
Z9 2
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3284
EP 3295
DI 10.1109/TMM.2022.3158001
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200025
DA 2024-07-18
ER

PT J
AU Tian, PZ
   Xie, SR
AF Tian, Pinzhuo
   Xie, Shaorong
TI An Adversarial Meta-Training Framework for Cross-Domain Few-Shot
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Meta-learning; adversarial training; cross-domain few-shot Learning
ID NETWORKS
AB Meta-learning provides a promising way for deep learning models to efficiently learn in few-shot learning. With this capacity, many deep learning systems can be applied in many real applications. However, many existing meta-learning based few-shot learning systems suffer from vulnerable generalization when new tasks are from unseen domains (a.k.a, cross-domain few-shot learning). In this work, we consider this problem from the perspective of designing a model-agnostic meta-training framework to improve the generalization of existing meta-learning methods in cross-domain few-shot learning. In this way, compared with focusing on elaborately designing modules for a specific meta-learning model, our method is endowed with the ability to be compatible with different meta-learning models in various few-shot problems. To achieve this goal, a novel adversarial meta-training framework is proposed. The proposed framework utilizes max-min episodic iteration. In the episode of maximization, our framework focuses on how to dynamically generate appropriate pseudo tasks which benefit learning cross-domain knowledge. In the episode of minimization, our method aims to solve how to help meta-learning model learn cross-task and robust meta-knowledge. To comprehensively evaluate our framework, experiments are conducted on two few-shot learning settings, three meta-learning models, and eight datasets. These results demonstrate that our method is applicable to various meta-learning models in different few-shot learning problems. The superiority of our method is verified compared with existing state-of-the-art methods.
C1 [Tian, Pinzhuo; Xie, Shaorong] Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
C3 Shanghai University
RP Xie, SR (corresponding author), Shanghai Univ, Sch Comp Engn & Sci, Shanghai 200444, Peoples R China.
EM pinzhuo@shu.edu.cn; srxie@shu.edu.cn
OI Yu, Hang/0000-0003-3444-9992
FU National Natural Science Foundation of China [62206166, 61991410]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62206166 and 61991410.
CR Al Chanti D, 2021, IEEE T MED IMAGING, V40, P2615, DOI 10.1109/TMI.2021.3058303
   Antoniou A., 2019, P INT C LEARN REPR
   Bateni Peyman, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14481, DOI 10.1109/CVPR42600.2020.01450
   Bertinetto Luca, 2018, P INT C LEARN REPR
   Chen ZT, 2019, PROC CVPR IEEE, P8672, DOI 10.1109/CVPR.2019.00888
   Codella N, 2019, Arxiv, DOI arXiv:1902.03368
   Colson B, 2007, ANN OPER RES, V153, P235, DOI 10.1007/s10479-007-0176-2
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Finn C., 2018, P INT C LEARN REPR
   Finn C, 2017, PR MACH LEARN RES, V70
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Hong Y., 2020, PROC INT C MACH LEAR, P1
   Hong Y, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2535, DOI 10.1145/3394171.3413561
   Hospedales T, 2022, IEEE T PATTERN ANAL, V44, P5149, DOI 10.1109/TPAMI.2021.3079209
   Houben S, 2013, IEEE IJCNN
   Jing YC, 2020, IEEE T VIS COMPUT GR, V26, P3365, DOI 10.1109/TVCG.2019.2921336
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9506, DOI 10.1109/ICCV48922.2021.00939
   Liang MJ, 2022, PATTERN RECOGN, V128, DOI 10.1016/j.patcog.2022.108662
   Liu L, 2021, POSTGRAD MED, V133, P265, DOI 10.1080/00325481.2020.1803666
   Liu YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8433, DOI 10.1109/ICCV48922.2021.00834
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mishra N, 2018, Arxiv, DOI arXiv:1707.03141
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Munkhdalai T, 2017, PR MACH LEARN RES, V70
   Ni R., 2021, P 38 INT C MACH LEAR, P8152
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Oh J., 2021, P INT C LEARN REPR, P1
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Phoo C. P., 2020, arXiv
   Qian K, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2639
   Raghu A., 2020, INT C LEARNING REPRE
   Rakelly K, 2019, PR MACH LEARN RES, V97
   Ravi S., 2017, C TRACK P, P1
   Santoro A, 2016, PR MACH LEARN RES, V48
   Sinha A., 2017, arXiv:1710.10571, V2
   Snell J, 2017, ADV NEUR IN, V30
   Sun JM, 2020, Arxiv, DOI arXiv:2007.08790
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Triantafillou E., 2019, METADATASET DATASET, P1
   Tseng H. Y., 2020, ICLR
   Tseng HY, 2021, PROC CVPR IEEE, P7917, DOI 10.1109/CVPR46437.2021.00783
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Volpi R, 2018, ADV NEUR IN, V31
   Von Oswald J., 2021, Adv. Neural Inf. Proces. Syst., V34, P5250, DOI [DOI 10.48550/ARXIV.2110.14402, 10.48550/arXiv.2110.14402]
   Wang H, 2021, P 30 INT JOINT C ART, P1075
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Welinder P., 2010, Tech. Rep. CNS-TR-201, P5250
   Yao HX, 2021, PR MACH LEARN RES, V139
   Yunhui Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P124, DOI 10.1007/978-3-030-58583-9_8
   Zhang BQ, 2021, PATTERN RECOGN, V117, DOI 10.1016/j.patcog.2021.107946
   Zhang HY, 2018, Arxiv, DOI [arXiv:1710.09412, DOI 10.48550/ARXIV.1710.09412]
   Zhang TY, 2023, IEEE T MULTIMEDIA, V25, P3773, DOI 10.1109/TMM.2022.3165715
   Zhao L., 2020, Advances in Neural Information Processing Systems
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhu W, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107797
NR 62
TC 4
Z9 4
U1 21
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6881
EP 6891
DI 10.1109/TMM.2022.3215310
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000013
DA 2024-07-18
ER

PT J
AU Wang, LS
   Li, CL
   Dai, WR
   Li, SH
   Zou, JN
   Xiong, HK
AF Wang, Lisha
   Li, Chenglin
   Dai, Wenrui
   Li, Shaohui
   Zou, Junni
   Xiong, Hongkai
TI QoE-Driven Adaptive Streaming for Point Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point clouds; rate adaptation; perspective projection; submodular
   function maximization; MPEG G-PCC
ID KNAPSACK-PROBLEM; MPEG
AB With increasing popularity of virtual reality and augmented reality, application of point clouds is in critical demand as it enables users to freely navigate in an immersive scene with six degrees of freedom. However, point clouds usually comprise large amounts of data, and are thus difficult to stream in bandwidth-constrained networks. It is therefore important, yet challenging, to efficiently stream the resource-intensive point clouds, such that the user's quality of experience (QoE) is guaranteed on a high-level but with a low bandwidth consumption. To this end, we propose a QoE-driven adaptive streaming approach for the tile-based point cloud transmission, to maximize the user's QoE while reducing the transmission redundancy. By exploiting the perspective projection, we specifically model the QoE of a 3D tile as a function of the bitrate of its representation, user's view frustum and spatial position, occlusion between tiles, and the resolution of rendering device. Based on this QoE model, we then formulate the QoE-optimized rate adaptation problem as a multiple-choice knapsack problem, which allocates bitrates for different tiles under a given transmission capacity. It is equivalently converted to a submodular function maximization problem subject to knapsack constraints, and solved by a practical greedy-based algorithm with a theoretical worst-case performance guarantee. The proposed algorithm is able to achieve a near-optimal performance, but with a very low computational complexity. Experimental results further demonstrate superiority of the proposed rate adaptation algorithm over existing schemes, in terms of both user's visual quality and transmission efficiency.
C1 [Wang, Lisha; Li, Chenglin; Dai, Wenrui; Li, Shaohui; Zou, Junni; Xiong, Hongkai] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Li, CL (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM wang_lisha@sjtu.edu.cn; lcl1985@sjtu.edu.cn; daiwenrui@sjtu.edu.cn;
   lishaohui@sjtu.edu.cn; zoujunni@sjtu.edu.cn; xionghongkai@sjtu.edu.cn
RI LI, CHENGLIN/JUF-8254-2023
OI Xiong, Hongkai/0000-0003-4552-0029; Li, Shaohui/0000-0002-9650-8874
FU National Natural Science Foundation of China [61931023, 61831018,
   61871267, 61972256, 62125109, T2122024, 6211001027]; Shanghai
   Rising-Star Program [20QA1404600]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61931023, 61831018, 61871267,
   61972256,62125109, T2122024, and 6211001027 and in part by the Shanghai
   Rising-Star Program under Grant 20QA1404600.
CR Akbar MM, 2006, COMPUT OPER RES, V33, P1259, DOI 10.1016/j.cor.2004.09.016
   Alexiou E., 2019, APSIPA Trans. Signal Inf. Process, V8, P1
   Alface PR, 2012, BELL LABS TECH J, V16, P135, DOI 10.1002/bltj.20538
   [Anonymous], 2001, P ITU T VID COD EXP
   [Anonymous], ILOG CPLEX OPTIMIZAT
   [Anonymous], 2017, ISO/IEC JTC1/SC29/WG11 MPEG2017/N16763
   [Anonymous], 2020, JTC1SC29WG11 ISOIEC
   Baker M., 1997, COMPUTER GRAPHICS C
   dEon E., 2017, Standard ISO/IEC JTC1/SC29 Joint WG11/WG1 (MPEG/JPEG)input document WG11M40059/WG1M74006
   DUDZINSKI K, 1987, EUR J OPER RES, V28, P3, DOI 10.1016/0377-2217(87)90165-2
   Foley J. D., 1994, Introduction to Computer Graphics", V55
   github, Mpeg-Pcc-Tmc13
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   Hifi M, 2004, J OPER RES SOC, V55, P1323, DOI 10.1057/palgrave.jors.2601796
   Hosseini M, 2019, Arxiv, DOI arXiv:1911.00812
   Hosseini M, 2018, PROCEEDINGS OF THE 23TH ACM WORKSHOP ON PACKET VIDEO (PV'18), P25, DOI 10.1145/3210424.3210429
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Javaheri A, 2021, IEEE T MULTIMEDIA, V23, P4049, DOI 10.1109/TMM.2020.3037481
   Kellerer H., 2004, MULTIPLE CHOICE KNAP
   Krause A, 2014, TRACTABILITY, P71
   Krivokuca M., 2018, JTC2SC29WG11 ISOIEC
   Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420
   Li J., 2020, PROC IEEE INT C COMM, P1
   lindo, LINGO 18
   Liu H, 2020, IEEE T BROADCAST, V66, P701, DOI 10.1109/TBC.2019.2957652
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu X, 2017, HOTNETS-XVI: PROCEEDINGS OF THE 16TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS, P50, DOI 10.1145/3152434.3152443
   mpeg, MPEG 3D GRAPH
   Park J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P447, DOI 10.1145/3343031.3351021
   Park J, 2019, IEEE J EM SEL TOP C, V9, P149, DOI 10.1109/JETCAS.2019.2898622
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Subramanyam S, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3669, DOI 10.1145/3394171.3413535
   Sun LY, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P162, DOI 10.1145/3204949.3204978
   van der Hooft J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2405, DOI 10.1145/3343031.3350917
   Wang LS, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P1930, DOI 10.1109/ICASSP39728.2021.9414121
   Wang ZX, 2019, Arxiv, DOI arXiv:1903.01864
   Yu Matt., 2015, P 3 INT WORKSHOP IMM, P1
   Zou JN, 2020, IEEE J-STSP, V14, P161, DOI 10.1109/JSTSP.2019.2956716
NR 39
TC 12
Z9 12
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2543
EP 2558
DI 10.1109/TMM.2022.3148585
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600007
DA 2024-07-18
ER

PT J
AU Wei, LW
   Hu, D
   Zhou, W
   Hu, SL
AF Wei, Lingwei
   Hu, Dou
   Zhou, Wei
   Hu, Songlin
TI Modeling Both Intra- and Inter-Modality Uncertainty for Multimodal Fake
   News Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fake news detection; uncertainty learning; multimodal fusion; vartional
   autoencoder; attention; social network analysis
AB Multimodal fake news detection has obtained increasing attention recently. Existing works generally encode multimodal contents into a deterministic point in semantic subspaces, and then fuse multimodal features by simple concatenation or attention mechanisms. However, most methods suffer from adapting to noisy multimodal contents since they neglect the robustness of modality-specific features. Besides, as different modalities usually have varying confidence levels, previous attention-based fusion models that learn modality-independent weights based on the input data feature, would limit the optimal integration of multimodal contents. To alleviate the above issues, we propose novel Multimodal Uncertainty Learning Network (MM-ULN) to enhance multimodal fake news detection by modeling both intra- and inter-modality uncertainty. Specifically, we incorporate a novel intra-modality uncertainty learning (EUL) module to better understand noisy multimodal contents. EULs provide feature regularization in a variational way, successfully alleviating the effects of data uncertainty within modalities. We design a new variational attention fusion (VAF) module to adaptively fuse multimodal contents with modality-dependent weights. The VAF module consider the relative confidence between modalities and enables to explore complementary properties for detection. Extensive experiments on two benchmark datasets demonstrate the effectiveness and superiority of MM-ULN on multimodal fake news detection.
C1 [Wei, Lingwei; Hu, Dou; Zhou, Wei; Hu, Songlin] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Wei, Lingwei; Hu, Dou; Hu, Songlin] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Zhou, W (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
EM weilingwei@iie.ac.cn; hudou@iie.ac.cn; zhouwei@iie.ac.cn;
   husonglin@iie.ac.cn
RI Hu, Dou/KIC-6747-2024
OI Hu, Dou/0000-0001-7790-8568; Hu, Songlin/0000-0002-7170-3809; Wei,
   Lingwei/0000-0002-7058-2662
FU National Natural Science Foundation of China [6210071416]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 6210071416.
CR Alemi A. A., 2016, INT C LEARNING REPRE
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bian T, 2020, AAAI CONF ARTIF INTE, V34, P549
   Boididou C., 2016, PROC MULTIMEDIA RETR, P1
   Cao J., 2020, Disinformation, Misinformation, and Fake News in Social Media (Lecture Notes in Social Networks), P141, DOI 10.1007/978-3-030-42699-6_8
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen HS, 2021, PROC CVPR IEEE, P10374, DOI 10.1109/CVPR46437.2021.01024
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   French Robert M., 1993, P 6 INT C NEURAL INF, V6, P1176
   Fung YR, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P1683
   Gao J, 2020, NEURAL COMPUT, V32, P829, DOI 10.1162/neco_a_01273
   Garrido-Merchán EC, 2020, LECT NOTES ARTIF INT, V12344, P13, DOI 10.1007/978-3-030-61705-9_2
   Ha T., 2021, PROC PACIFIC RIM INT, V13032, P100
   Hu D., 2022, P 16 INT WORKSH SEM, P335
   [胡斗 Hu Dou], 2021, [计算机研究与发展, Journal of Computer Research and Development], V58, P1395
   Jin ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P795, DOI 10.1145/3123266.3123454
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Jin ZW, 2014, IEEE DATA MINING, P230, DOI 10.1109/ICDM.2014.91
   Kai Shu, 2017, ACM SIGKDD Explorations Newsletter, V19, P22, DOI 10.1145/3137597.3137600
   Kendall A, 2017, 31 ANN C NEURAL INFO, V30
   Khattar D, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2915, DOI 10.1145/3308558.3313552
   Kim Y., 2014, P 2014 C EMP METH NA, P1746, DOI [DOI 10.3115/V1/D14-1181, 10.3115/v1/D14-1181]
   Kingma D. P., 2014, arXiv
   Kingma DP, 2013, ARXIV
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Li MH, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2021, P274
   Liu Y, 2018, AAAI CONF ARTIF INTE, P354
   Ma J., 2015, P 24 ACM INT C INF K, P1751, DOI DOI 10.1145/2806416.2806607
   Ma J., 2016, P 25 INT JOINT C ART, P3818
   Ma J, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P708, DOI 10.18653/v1/P17-1066
   Morris Meredith Ringel, 2012, P ACM 2012 C COMP SU, P441, DOI DOI 10.1145/2145204.2145274
   Nan Q, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3343, DOI 10.1145/3459637.3482139
   Qazvinian V., 2011, RUMOR HAS IT IDENTIF
   Qian SS, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P153, DOI 10.1145/3404835.3462871
   She JH, 2021, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR46437.2021.00618
   Silva A, 2021, AAAI CONF ARTIF INTE, V35, P557
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singhal S, 2020, AAAI CONF ARTIF INTE, V34, P13915
   Singhal S, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P39, DOI [10.1109/BigMM.2019.00-44, 10.1109/BigMM.2019.00018]
   Song CG, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2020.102437
   Sun C, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P380
   Teja SP, 2021, PROC CVPR IEEE, P9608, DOI 10.1109/CVPR46437.2021.00949
   Tian D.P., 2013, International Journal of Multimedia and Ubiquitous Engineering, V8, P385
   Tredici M. D., 2020, P 28 INT C COMPUTATI, P5467
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang YQ, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3708, DOI 10.1145/3447548.3467153
   Wang YQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P849, DOI 10.1145/3219819.3219903
   Wangetal Y., 2021, P INT AAAI C WEB SOC, P776, DOI 10.1609/icwsm.v15i1.18102
   Wei LW, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P3845
   Wei LW, 2021, LECT NOTES ARTIF INT, V12459, P633, DOI 10.1007/978-3-030-67664-3_38
   Wu MH, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P7291
   Wu Y, 2021, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, ACL-IJCNLP 2021, P2560
   Xue JX, 2021, INFORM PROCESS MANAG, V58, DOI 10.1016/j.ipm.2021.102610
   Yang JC, 2020, AAAI CONF ARTIF INTE, V34, P9378
   Yu F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3901
   Zhang DY, 2018, IEEE INT CONF BIG DA, P891, DOI 10.1109/BigData.2018.8622344
   Zhang HW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1942, DOI 10.1145/3343031.3350850
   Zhang T, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206973
   Zhou LY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2964, DOI 10.1145/3394171.3413515
   Zhou XY, 2020, LECT NOTES ARTIF INT, V12085, P354, DOI 10.1007/978-3-030-47436-2_27
NR 65
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7906
EP 7916
DI 10.1109/TMM.2022.3229966
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400023
DA 2024-07-18
ER

PT J
AU Wu, CH
   Hsu, CF
   Hung, TK
   Griwodz, C
   Ooi, WT
   Hsu, CH
AF Wu, Cheng-Hao
   Hsu, Chih-Fan
   Hung, Tzu-Kuan
   Griwodz, Carsten
   Ooi, Wei Tsang
   Hsu, Cheng-Hsin
TI Quantitative Comparison of Point Cloud Compression Algorithms With PCC
   Arena
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Benchmark; compression; quality assessment; subject metrics; objective
   metrics; Quality-of-Experience; Quality-of-Service
ID GEOMETRY
AB With the growth of Extended Reality (XR) and capturing devices, point cloud representation has become attractive to academics and industry. Point Cloud Compression (PCC) algorithms further promote numerous XR applications that may change our daily life. However, in the literature, PCC algorithms are often evaluated with heterogeneous datasets, metrics, and parameters, making the results hard to interpret. In this article, we propose an open-source benchmark platform called PCC Arena. Our platform is modularized in three aspects: PCC algorithms, point cloud datasets, and performance metrics. Users can easily extend PCC Arena in each aspect to fulfill the requirements of their experiments. To show the effectiveness of PCC Arena, we integrate seven PCC algorithms into PCC Arena along with six point cloud datasets. We then compare the algorithms on ten carefully selected metrics to evaluate the quality of the output point clouds. We further conduct a user study to quantify the user-perceived quality of rendered images that are produced by different PCC algorithms. Several novel insights are revealed in our comparison: (i) Signal Processing (SP)-based PCC algorithms are stable for different usage scenarios, but the trade-offs between coding efficiency and quality should be carefully addressed, (ii) Neural Network (NN)-based PCC algorithms have the potential to consume lower bitrates yet provide similar results to SP-based algorithms, (iii) NN-based PCC algorithms may generate artifacts and suffer from long running time, and (iv) NN-based PCC algorithms are worth more in-depth studies as the recently proposed NN-based PCC algorithms improve the quality and running time. We believe that PCC Arena can play an essential role in allowing engineers and researchers to better interpret and compare the performance of future PCC algorithms.
C1 [Wu, Cheng-Hao; Hung, Tzu-Kuan; Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Hsu, Chih-Fan] Inventec Corp, Digital Ctr, Taipei 111059, Taiwan.
   [Griwodz, Carsten] Univ Oslo, Dept Informat, N-0315 Oslo, Norway.
   [Ooi, Wei Tsang] Natl Univ Singapore, Dept Comp Sci, Singapore 117590, Singapore.
C3 National Tsing Hua University; Inventec; University of Oslo; National
   University of Singapore
RP Hsu, CH (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
EM chenghao.nthu@gmail.com; hsuchihfan@gmail.com; hungtzukuan@gmail.com;
   griff@ifi.uio.no; ooiwt@comp.nus.edu.sg; chsu@cs.nthu.edu.tw
RI Wu, Cheng Hao/AAG-2521-2020
OI Wu, Cheng Hao/0000-0002-6515-0988; Hsu, Cheng-Hsin/0000-0002-8116-2591
FU Ministry of Science and Technology of Taiwan [110-2221-E-007-102];
   Singapore Ministry of Education Academic Research Fund Tier 1 [T1
   251RES2038]; NOVATEK Fellowship
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grant 110-2221-E-007-102, in part by the
   Singapore Ministry of Education Academic Research Fund Tier 1 under
   Grant T1 251RES2038, and in part by a NOVATEK Fellowship.
CR Akhtar A, 2022, IEEE T MULTIMEDIA, V24, P2866, DOI 10.1109/TMM.2021.3090148
   Alexiou E., 2019, APSIPA Trans. Signal Inf. Process, V8, P1
   Alexiou E, 2020, PROC SPIE, V11510, DOI 10.1117/12.2569115
   [Anonymous], 2021, Qualtrics XM
   Balle J., 2018, INT C LEARN REPR ICL, P1
   Cao C, 2019, PROCEEDINGS WEB3D 2019: THE 24TH INTERNATIONAL ACM CONFERENCE ON 3D WEB TECHNOLOGY, DOI 10.1145/3329714.3338130
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Cloud Compare.org, 2021, CLOUDCOMPARE OPEN SO
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   dEon E., 2017, Standard ISO/IEC JTC1/SC29/WG1, input document M74006 ISO/IEC JTC1/SC29/WG11 input document m40059
   Fu ZQ, 2021, IEEE T MULTIMEDIA, V23, P3022, DOI 10.1109/TMM.2021.3068606
   Graziosi D, 2020, APSIPA TRANS SIGNAL, V9, DOI 10.1017/ATSIP.2020.12
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   ITU, 2015, AND BEYOND
   Jia W, 2022, IEEE T MULTIMEDIA, V24, P2352, DOI 10.1109/TMM.2021.3079698
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Kulkarni TD, 2015, ADV NEUR IN, V28
   Lim T., 2016, Proc. ICLR
   Liu H, 2021, IEEE T MULTIMEDIA, V23, P2045, DOI 10.1109/TMM.2020.3007331
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Mekuria R., 2016, INT ORG STANDARDIZAT
   Meynet G, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123147
   MPEG 3DGC, 2020, JTC1SC29WG7 ISOIEC
   MPEG 3DGC, 2020, INT ORG STANDARDIZAT
   Papadakis P., 2014, EUROGRAPHICS WORKSHO, P33
   Park SB, 2009, IEEE T MULTIMEDIA, V11, P177, DOI 10.1109/TMM.2008.2008868
   Perry S, 2020, IEEE IMAGE PROC, P3428, DOI 10.1109/ICIP40778.2020.9191308
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Quach M, 2020, IEEE IMAGE PROC, P3309, DOI 10.1109/ICIP40778.2020.9191180
   Quach M, 2020, IEEE INT WORKSH MULT
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/ICIP.2019.8803413, 10.1109/icip.2019.8803413]
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Richardson J, SAGE ENCY ED RES MEA, P937
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sheng XH, 2022, IEEE T MULTIMEDIA, V24, P2617, DOI 10.1109/TMM.2021.3086711
   Sim JY, 2008, IEEE T MULTIMEDIA, V10, P305, DOI 10.1109/TMM.2008.917349
   Sim JY, 2005, IEEE T MULTIMEDIA, V7, P1191, DOI 10.1109/TMM.2005.858410
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   The Draco authors, 2020, DRACO 3D GRAPHICS CO
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Torlig EM, 2018, PROC SPIE, V10752, DOI 10.1117/12.2322741
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Turner H, 2020, CATALOGUING CULTURE, P1
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   van der Hooft J, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123081
   Viola I, 2020, INT WORK QUAL MULTIM, DOI 10.1109/qomex48832.2020.9123089
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   Wang JQ, 2021, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC50243.2021.00015
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu C.-H., 2020, P 12 ACM INT WORKSH, P1
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhou QY, 2018, Arxiv, DOI arXiv:1801.09847
NR 63
TC 5
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3073
EP 3088
DI 10.1109/TMM.2022.3154927
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200010
DA 2024-07-18
ER

PT J
AU Wu, ZH
   Lin, XC
   Lin, ZH
   Chen, ZL
   Bai, Y
   Wang, SP
AF Wu, Zhihao
   Lin, Xincan
   Lin, Zhenghong
   Chen, Zhaoliang
   Bai, Yang
   Wang, Shiping
TI Interpretable Graph Convolutional Network for Multi-View Semi-Supervised
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; Semisupervised learning; Laplace
   equations; Matrix decomposition; Neural networks; Deep learning; Task
   analysis; Graph convolutional network; interpretable deep learning;
   multi-view semi-supervised classification; orthogonal normalization
ID INFORMATION
AB As real-world data become increasingly heterogeneous, multi-view semi-supervised learning has garnered widespread attention. Although existing studies have made efforts towards this and achieved decent performance, they are restricted to shallow models and how to mine deeper information from multiple views remains to be investigated. As a recently emerged neural network, Graph Convolutional Network (GCN) exploits graph structure to propagate label signals and has achieved encouraging performance, and it has been widely employed in various fields. Nonetheless, research on solving multi-view learning problems via GCN is limited and lacks interpretability. To address this gap, in this paper we propose a framework termed Interpretable Multi-view Graph Convolutional Network (IMvGCN). We first combine the reconstruction error and Laplacian embedding to formulate a multi-view learning problem that explores the original space from feature and topology perspectives. In light of a series of derivations, we establish a potential connection between GCN and multi-view learning, which holds significance for both domains. Furthermore, we propose an orthogonal normalization method to guarantee the mathematical connection, which solves the intractable problem of orthogonal constraints in deep learning. In addition, the proposed framework is applied to the multi-view semi-supervised learning task. Comprehensive experiments demonstrate the superiority of our proposed method over other state-of-the-art methods.
C1 [Wu, Zhihao; Lin, Xincan; Lin, Zhenghong; Chen, Zhaoliang; Wang, Shiping] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.
   [Wu, Zhihao; Lin, Xincan; Lin, Zhenghong; Chen, Zhaoliang; Wang, Shiping] Fuzhou Univ, Fujian Prov Key Lab Network Comp & Intelligent Inf, Fuzhou 350116, Peoples R China.
   [Bai, Yang] Chengdu Univ Informat Technol, Sch Cyberspace Secur, Chengdu 610054, Peoples R China.
C3 Fuzhou University; Fuzhou University; Chengdu University of Information
   Technology
RP Wang, SP (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350116, Peoples R China.; Wang, SP (corresponding author), Fuzhou Univ, Fujian Prov Key Lab Network Comp & Intelligent Inf, Fuzhou 350116, Peoples R China.
EM zhihaowu1999@gmail.com; xincanlinms@gmail.com;
   hongzhenglin970323@gmail.com; chenzl23@outlook.com; alicepub@163.com;
   shipingwangphd@163.com
RI Chen, Zhaoliang/KGL-0282-2024
OI Chen, Zhaoliang/0000-0002-7832-908X; Wu, Zhihao/0000-0001-5835-9903;
   Lin, Zhenghong/0000-0002-7495-3846; , Yang/0000-0002-2475-4232
FU National Natural Science Foundation of China
FX No Statement Available
CR Bo DY, 2021, AAAI CONF ARTIF INTE, V35, P3950
   Chen C, 2019, IEEE T NEUR NET LEAR, V30, P269, DOI 10.1109/TNNLS.2018.2837166
   Chen YY, 2021, IEEE T IMAGE PROCESS, V30, P4022, DOI 10.1109/TIP.2021.3068646
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Chen ZL, 2023, IEEE T MULTIMEDIA, V25, P228, DOI 10.1109/TMM.2021.3124087
   Cheng JF, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2973
   Chiang WL, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P257, DOI 10.1145/3292500.3330925
   Du SD, 2021, IEEE T SIGNAL PROCES, V69, P4623, DOI 10.1109/TSP.2021.3101979
   Fu LL, 2023, IEEE T MULTIMEDIA, V25, P4972, DOI 10.1109/TMM.2022.3185886
   Gao HY, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1416, DOI 10.1145/3219819.3219947
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P3181, DOI 10.1109/TPAMI.2022.3182052
   Huang JJ, 2022, IEEE T MULTIMEDIA, V24, P188, DOI 10.1109/TMM.2020.3047762
   Huang L, 2018, AAAI CONF ARTIF INTE, P3271
   Huang S, 2023, IEEE T MULTIMEDIA, V25, P3259, DOI 10.1109/TMM.2022.3157997
   Huang ZY, 2021, IEEE T IMAGE PROCESS, V30, P5352, DOI 10.1109/TIP.2021.3083072
   Jia XD, 2021, IEEE T PATTERN ANAL, V43, P2496, DOI 10.1109/TPAMI.2020.2973634
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Li S, 2020, AAAI CONF ARTIF INTE, V34, P4691
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P330, DOI 10.1109/TPAMI.2020.3011148
   Liu JY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2726, DOI 10.1145/3474085.3475379
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Nie F., 2016, IJCAI, P1881
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Sánchez-Martin P, 2022, AAAI CONF ARTIF INTE, P8159
   Shu XB, 2021, IEEE T NEUR NET LEAR, V32, P663, DOI 10.1109/TNNLS.2020.2978942
   Smith S. P., 1995, Journal of Computational and Graphical Statistics, V4, P134, DOI DOI 10.2307/1390762
   Sun JK, 2022, IEEE T KNOWL DATA EN, V34, P2348, DOI 10.1109/TKDE.2020.3008774
   Sun K, 2020, AAAI CONF ARTIF INTE, V34, P5892
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P5924
   Tang C, 2022, IEEE T PATTERN ANAL, V44, P955, DOI 10.1109/TPAMI.2020.3014629
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P2837, DOI 10.1109/TMM.2019.2909860
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tao H, 2017, IEEE T IMAGE PROCESS, V26, P4283, DOI 10.1109/TIP.2017.2717191
   Wan ZB, 2021, AAAI CONF ARTIF INTE, V35, P10085
   Wang S., 2023, arXiv
   Wang SP, 2022, IEEE T PATTERN ANAL, V44, P5042, DOI 10.1109/TPAMI.2021.3082632
   Wang X, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1243, DOI 10.1145/3394486.3403177
   Wu F, 2019, PR MACH LEARN RES, V97
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xu JL, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3202
   Yang ML, 2019, PATTERN RECOGN, V88, P236, DOI 10.1016/j.patcog.2018.11.015
   Zhang ZY, 2023, IEEE T NEUR NET LEAR, V34, P4296, DOI 10.1109/TNNLS.2021.3116936
   Zhu MQ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P1215, DOI 10.1145/3442381.3449953
NR 46
TC 17
Z9 17
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8593
EP 8606
DI 10.1109/TMM.2023.3260649
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000039
DA 2024-07-18
ER

PT J
AU Xie, JY
   Zhu, YC
   Chen, ZZ
AF Xie, Jiayi
   Zhu, Yaochen
   Chen, Zhenzhong
TI Micro-Video Popularity Prediction Via Multimodal Variational Information
   Bottleneck
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social networking (online); Feature extraction; Visualization; Hidden
   Markov models; Uncertainty; Fuses; Task analysis; Micro-video analysis;
   multimodal learning; popularity prediction; deep variational information
   bottleneck; product-of-experts system
AB In this paper, we propose a Hierarchical Multimodal Variational Encoder-Decoder (HMMVED) to predict the popularity of micro-videos by comprehensively leveraging the user information and the micro-video content in a hierarchical fashion. In particular, the multimodal variational encoder-decoder framework encodes the input modalities to a lower dimensional stochastic embedding, from which the popularity of micro-videos can be decoded. Considering the leading role of the user's social influence in social media for information dissemination, a user encoder-decoder is designed to learn the prior Gaussian embedding of the micro-video from the user information, which is informative about the coarse-grained popularity. In order to incorporate the fluctuation around the coarse-grained popularity caused by the diverse multimodal content, in the micro-video encoder-decoder, the refined posterior distribution of the micro-video embedding is encoded from the content features while encouraged to be close to the learned prior distribution. The fine-grained popularity of each micro-video is decoded from the posterior embedding of the micro-video. Based on the multimodal extension of variational information bottleneck theory, we show that the learned latent embeddings of micro-videos are maximally expressive about the popularity whilst maximally compressing the information from input modalities. Extensive experiments conducted on two real-world datasets demonstrate the effectiveness of the proposed method.
C1 [Xie, Jiayi; Zhu, Yaochen; Chen, Zhenzhong] Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
C3 Wuhan University
RP Chen, ZZ (corresponding author), Wuhan Univ, Sch Remote Sensing & Informat Engn, Wuhan 430079, Hubei, Peoples R China.
EM xjyxie@whu.edu.cn; yaochenzhu@whu.edu.cn; zzchen@ieee.org
RI Chen, Zhenzhong/C-2529-2015; Zhu, Yaochen/AAO-2023-2020
OI Zhu, Yaochen/0000-0001-6266-2788
FU National Natural Science Foundation of China [62036005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62036005.
CR Alemi A., 2017, PROC INT C LEARN REP
   [Anonymous], 2013, P ACL
   [Anonymous], 2014, P INT C MULTIMEDIA R, DOI [DOI 10.1145/2578726.2578776, 10.1145/2578726.2578776, 10.1145]
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bielski A, 2018, IEEE ACCESS, V6, P74277, DOI 10.1109/ACCESS.2018.2884831
   Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773
   Cao Q, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P70, DOI 10.1145/3336191.3371834
   Chen C, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P225, DOI 10.1145/3331184.3331192
   Chen GD, 2019, NEUROCOMPUTING, V333, P221, DOI 10.1016/j.neucom.2018.12.039
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Gal Y., 2016, UNCERTAINTY DEEP LEA
   Gao XF, 2021, IEEE T KNOWL DATA EN, V33, P2165, DOI 10.1109/TKDE.2019.2952856
   Gelli F, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P907, DOI 10.1145/2733373.2806361
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   Ghosh S, 2016, Arxiv, DOI arXiv:1602.06291
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jing PG, 2018, IEEE T KNOWL DATA EN, V30, P1519, DOI 10.1109/TKDE.2017.2785784
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Kingma D. P., 2014, arXiv
   Klushyn N., 2019, ADV NEURAL INF PROCE, P2866
   Kong QC, 2020, IEEE T SYST MAN CY-S, V50, P3817, DOI 10.1109/TSMC.2018.2855806
   Li HT, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P169, DOI 10.1145/2505515.2505523
   Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077
   Liao DL, 2019, AAAI CONF ARTIF INTE, P200
   Ma ZY, 2013, J AM SOC INF SCI TEC, V64, P1399, DOI 10.1002/asi.22844
   Mishra S, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1069, DOI 10.1145/2983323.2983812
   Mithun NC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1856, DOI 10.1145/3240508.3240712
   Naseri M, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1053, DOI 10.1145/3331184.3331301
   Nie L., 2019, Multimodal learning toward micro-video understanding
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sang L, 2021, IEEE T MULTIMEDIA, V23, P2019, DOI 10.1109/TMM.2020.3007330
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Su YT, 2020, MULTIMEDIA SYST, V26, P519, DOI 10.1007/s00530-020-00660-x
   Tishby Naftali, 1999, ANN ALL C COMM CONTR
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Wang SF, 2021, IEEE T AFFECT COMPUT, V12, P1002, DOI 10.1109/TAFFC.2019.2912377
   Wu MK, 2018, ADV NEUR IN, V31
   Xie JY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2542, DOI 10.1145/3366423.3380004
   Zaremba W, 2016, PROC INT C LEARN REP
   Zayats Victoria, 2018, TACL, V6, P121, DOI 10.1162/tacl_a_00009
   Zeng M, 2018, IEEE WIREL COMMUN, V25, P36, DOI 10.1109/MWC.2018.1700330
   Zhang W, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1277, DOI 10.1145/3178876.3186026
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhu YC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P130, DOI 10.1145/3343031.3350997
NR 53
TC 8
Z9 8
U1 5
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 24
EP 37
DI 10.1109/TMM.2021.3120537
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, KH
   Wu, Y
   Dong, ML
   Liu, B
   Liu, D
   Liu, QS
AF Zhang, Kaihua
   Wu, Yang
   Dong, Mingliang
   Liu, Bo
   Liu, Dong
   Liu, Qingshan
TI Deep Object Co-Segmentation and Co-Saliency Detection via High-Order
   Spatial-Semantic Network Modulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-saliency detection; co-segmentation; graph aggregation; network
   modulation
ID GRAPH; DISCOVERY; MODEL
AB Object co-segmentation (CSG) is to segment the common objects of the same category in multiple relevant images while the co-saliency detection (CSD) aims to discover the salient and common foreground objects in a group of images. To process both tasks simultaneously, this paper presents an adaptive spatially and high-order semantically modulated deep network framework. A backbone network is first adopted to extract multi-resolution image features. With the multi-resolution features of the relevant images as input, we design an adaptive spatial modulator to learn a spatial representation that can highlight the co-object regions for each image. The adaptive spatial modulator fully captures the rich correlations of all image feature descriptors via unsupervised clustering and a graph aggregation strategy. The learned representation can well localize the common foreground object while effectively suppressing the background signals. For the high-order semantic modulator, we model it as a supervised image classification task. We propose a hierarchical high-order pooling module to learn the rich semantic features for classification use. The outputs of the two modulators manipulate the multi-resolution features by a shift-and-scale operation so that the features focus on segmenting common object regions. The proposed model is trained end-to-end without any intricate post-processing. Extensive experiments on three CSG benchmark datasets (MSRC, i-Coseg, and PASCAL-VOC) and three CSD datasets (Cosal2015, CoCA, and CoSOD3k) demonstrate the superior accuracy of the proposed method compared to state-of-the-art methods on both tasks.
C1 [Zhang, Kaihua; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210000, Peoples R China.
   [Wu, Yang; Dong, Mingliang] Nanjing Univ Informat Sci & Technol, Sch Automat, Nanjing 210000, Peoples R China.
   [Liu, Bo] JD Finance Amer Corp, Mountain View, CA 94089 USA.
   [Liu, Dong] Netflix Inc, Los Gatos, CA 95032 USA.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; Netflix, Inc.
RP Zhang, KH (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210000, Peoples R China.
EM zhkhua@gmail.com; wuy98419@163.com; 895825191@qq.com; kfliubo@gmail.com;
   dongliu.hit@gmail.com; qsliu@nuist.edu.cn
RI Zhang, Kaihua/AAD-7882-2022; Zhang, Kaihua/E-1026-2013
OI Zhang, Kaihua/0000-0003-3734-3624; Zhang, Kaihua/0000-0002-1613-3401
FU National Key Research and Development Program of China [2018AAA0100400];
   Natural Science Foundation of China [61876088, 61825601, U20B2065]; 333
   High-level Talents Cultivation Project of Jiangsu Province [BRA2020291]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100400, in part by the
   Natural Science Foundation of China under Grants 61876088, 61825601, and
   U20B2065, and in part by the 333 High-level Talents Cultivation Project
   of Jiangsu Province under Grant BRA2020291. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Joao M Ascenso. (Kaihua Zhang and Yang Wu
   contributed equally to this work.) (Corresponding author: Kaihua Zhang.)
CR Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Brockschmidt M, 2019, 25TH AMERICAS CONFERENCE ON INFORMATION SYSTEMS (AMCIS 2019)
   Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Casanova Arantxa, 2018, INT C LEARNING REPRE
   Chang HS, 2015, COMPUT VIS IMAGE UND, V141, P18, DOI 10.1016/j.cviu.2015.06.004
   Chang KY, 2011, PROC CVPR IEEE
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen H, 2019, LECT NOTES COMPUT SC, V11364, P435, DOI 10.1007/978-3-030-20870-7_27
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen YL, 2014, INT C PATT RECOG, P2305, DOI 10.1109/ICPR.2014.400
   Chen YC, 2021, IEEE T PATTERN ANAL, V43, P3632, DOI 10.1109/TPAMI.2020.2985395
   Collins MD, 2012, PROC CVPR IEEE, P1656, DOI 10.1109/CVPR.2012.6247859
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025
   Dai JF, 2013, IEEE I CONF COMP VIS, P1305, DOI 10.1109/ICCV.2013.165
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   de Vries H, 2017, ADV NEUR IN, V30
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding C., 2004, P 21 INT C MACH LEAR, P29, DOI DOI 10.1145/1015330.1015408
   Doi T., 2020, P AS C COMP VIS, P581
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Fan DP, 2022, IEEE T PATTERN ANAL, V44, P4339, DOI 10.1109/TPAMI.2021.3060412
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Flores CF, 2019, PATTERN RECOGN, V94, P62, DOI 10.1016/j.patcog.2019.05.002
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Gao ZL, 2019, PROC CVPR IEEE, P3019, DOI 10.1109/CVPR.2019.00314
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   Hati A, 2016, LECT NOTES COMPUT SC, V9910, P736, DOI 10.1007/978-3-319-46466-4_44
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hsu KJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P748
   Hsu KJ, 2019, PROC CVPR IEEE, P8838, DOI 10.1109/CVPR.2019.00905
   Huang R, 2015, IEEE INT CON MULTI
   Huang T. S., 2016, P 24 ACM INT C MULT, P516, DOI 10.1145/2964284.2967274
   Jerripothula KR, 2021, IEEE T IMAGE PROCESS, V30, P2784, DOI 10.1109/TIP.2021.3054464
   Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jerripothula KR, 2014, IEEE IMAGE PROC, P3277, DOI 10.1109/ICIP.2014.7025663
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P3193, DOI 10.1109/TMM.2020.3021251
   Jin W-D., 2020, Adv. Neural Inf. Process. Syst., P18749
   Kingma D. P., 2014, arXiv
   Kosub S, 2019, PATTERN RECOGN LETT, V120, P36, DOI 10.1016/j.patrec.2018.12.007
   Lee C, 2015, PROC CVPR IEEE, P3837, DOI 10.1109/CVPR.2015.7299008
   Li B, 2019, IEEE I CONF COMP VIS, P8518, DOI 10.1109/ICCV.2019.00861
   Li TP, 2022, IEEE T MULTIMEDIA, V24, P492, DOI 10.1109/TMM.2021.3054526
   Li Weihao, 2018, ASIAN C COMPUT VIS, P638
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Li Y, 2016, NEUROCOMPUTING, V172, P225, DOI 10.1016/j.neucom.2014.12.110
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Lu ZM, 2019, IEEE ACCESS, V7, P62875, DOI 10.1109/ACCESS.2019.2917152
   Mukherjee P, 2018, Arxiv, DOI arXiv:1803.02555
   Mustafa A, 2017, PROC CVPR IEEE, P5583, DOI 10.1109/CVPR.2017.592
   Perez E, 2018, AAAI CONF ARTIF INTE, P3942
   Prol Hugo, 2018, arXiv
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Quan R, 2016, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2016.81
   Rubinstein M, 2013, PROC CVPR IEEE, P1939, DOI 10.1109/CVPR.2013.253
   Shi XJ, 2015, ADV NEUR IN, V28
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tan ZY, 2013, INT CONF ACOUST SPEE, P2114, DOI 10.1109/ICASSP.2013.6638027
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tsai CC, 2020, IEEE T MULTIMEDIA, V22, P1016, DOI 10.1109/TMM.2019.2936803
   Tsai CC, 2017, IEEE INT CON MULTI, P523, DOI 10.1109/ICME.2017.8019413
   Ulutan Oytun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13614, DOI 10.1109/CVPR42600.2020.01363
   Vicente S, 2011, PROC CVPR IEEE
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Wang C, 2019, AAAI CONF ARTIF INTE, P8917
   Wang C, 2017, IEEE T IMAGE PROCESS, V26, P5825, DOI 10.1109/TIP.2017.2750410
   Wang F, 2013, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2013.110
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Wei XS, 2019, PATTERN RECOGN, V88, P113, DOI 10.1016/j.patcog.2018.10.022
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wu Z, 2019, IEEE I CONF COMP VIS, P7263, DOI 10.1109/ICCV.2019.00736
   Xu HB, 2021, AAAI CONF ARTIF INTE, V35, P3030
   Yang LJ, 2018, PROC CVPR IEEE, P6499, DOI 10.1109/CVPR.2018.00680
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Yuan ZH, 2014, LECT NOTES COMPUT SC, V8689, P695, DOI 10.1007/978-3-319-10590-1_45
   Zhang C, 2021, IEEE T IMAGE PROCESS, V30, P5652, DOI 10.1109/TIP.2021.3087401
   Zhang DW, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3158674
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang K., 2020, P IEEE CVF C COMP VI, P9050
   Zhang KH, 2021, PROC CVPR IEEE, P13698, DOI 10.1109/CVPR46437.2021.01349
   Zhang KH, 2020, AAAI CONF ARTIF INTE, V34, P12813
   Zhang KH, 2019, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2019.00321
   Zhang N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4147, DOI 10.1109/ICCV48922.2021.00413
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P455, DOI 10.1007/978-3-030-58610-2_27
   Zhu HY, 2016, J VIS COMMUN IMAGE R, V34, P12, DOI 10.1016/j.jvcir.2015.10.012
NR 98
TC 4
Z9 5
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5733
EP 5746
DI 10.1109/TMM.2022.3198848
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500007
DA 2024-07-18
ER

PT J
AU Zhang, QC
   Jiang, Y
   Zhou, Q
   Zhao, YR
   Liu, Y
   Lu, HT
   Hua, XS
AF Zhang, Qinchuan
   Jiang, Yi
   Zhou, Qin
   Zhao, Yiru
   Liu, Yao
   Lu, Hongtao
   Hua, Xian-Sheng
TI Single Person Dense Pose Estimation via Geometric Equivariance
   Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose estimation; Three-dimensional displays; Training; Biological system
   modeling; Annotations; Task analysis; Solid modeling; Dense human pose
   estimation; representation learning
AB We study the task of single person dense pose estimation. Specifically, given a human-centric image, we learn to map all human pixels onto a 3D, surface-based human body model. Existing methods approach this problem by fitting deep convolutional networks on sparse annotated points where the regression on both surface coordinate components for each body part is uncorrelated and optimized separately. In this work, we devise a novel, unified loss function that explicitly characterizes the correlation for surface coordinates regression, achieving significant improvements in both accuracy and efficiency. Furthermore, based on an observation that the image-to-surface correspondence is intrinsically invariant to geometric transformations from input images, we propose to enforce a geometric equivariance consistency on the target mapping, thereby allowing us to enable reliable supervision on large amounts of unlabeled pixels. We conduct comprehensive studies on the effectiveness of our approach using a quite simple network. Extensive experiments on the DensePose-COCO dataset show that our model achieves superior performance against previous state-of-the-art methods with much less computation complexity. We hope that our work would serve as a solid baseline for future study in the field. The code will be available at https://github.com/Johnqczhang/densepose.pytorch.
C1 [Zhang, Qinchuan] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Jiang, Yi] ByteDance AI Lab, Beijing, Peoples R China.
   [Zhou, Qin] Shanghai Jiao Tong Univ, Med Robot Inst, Shanghai, Peoples R China.
   [Zhao, Yiru] Alibaba Grp, Hangzhou, Peoples R China.
   [Liu, Yao] Alibaba Grp, Hangzhou, Peoples R China.
   [Lu, Hongtao] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Hua, Xian-Sheng] Alibaba Grp, Hangzhou, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Alibaba
   Group; Alibaba Group; Shanghai Jiao Tong University; Alibaba Group
RP Lu, HT (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
EM qinchuan.zhang@sjtu.edu.cn; jiangyi0425@gmail.com;
   sunnyzq1990@gmail.com; yiru.zyr@alibaba-inc.com; xuanyao0111@gmail.com;
   htlu@sjtu.edu.cn; xiansheng.hxs@alibaba-inc.com
RI Zhao, Yiru/JVO-8384-2024
OI Hua, Xian-Sheng/0000-0002-8232-5049; Lu, Hongtao/0000-0003-2300-3039
FU National Natural Science Foundation of China [61772330, 62176155];
   Science and Technology Commission of Shanghai Municipality
   [2021SHZDZX0102]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61772330 and 62176155, and in part by
   the Science and Technology Commission of Shanghai Municipality under
   Grant 2021SHZDZX0102. The Associate Editor coordinating the review of
   thismanuscript and approving it for publication was Dr. Jiebo Luo
CR Aila T, 2017, PROC 5 INT C LEARN R
   [Anonymous], 2005, 1530 U WISC DEP COMP
   Bachman P, 2014, ADV NEUR IN, V27
   BOOKSTEIN FL, 1989, IEEE T PATTERN ANAL, V11, P567, DOI 10.1109/34.24792
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chan C, 2019, IEEE I CONF COMP VIS, P5932, DOI 10.1109/ICCV.2019.00603
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Clark K, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1914
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Güler RA, 2019, PROC CVPR IEEE, P10876, DOI 10.1109/CVPR.2019.01114
   Güler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762
   Guo YY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P356, DOI 10.1145/3343031.3350856
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Huang HJ, 2020, IEEE T IMAGE PROCESS, V29, P7468, DOI 10.1109/TIP.2020.3003442
   Hung WC, 2019, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2019.00096
   Ioffe S., 2015, P INT C MACH LEARN, VVolume 1, P448, DOI DOI 10.48550/ARXIV.1502.03167
   Kamel A, 2021, IEEE T MULTIMEDIA, V23, P1330, DOI 10.1109/TMM.2020.2999181
   Kingma Diederik P., 2015, P 3 INT C LEARN
   Kirillov A, 2019, PROC CVPR IEEE, P6392, DOI 10.1109/CVPR.2019.00656
   Kolotouros N, 2019, PROC CVPR IEEE, P4496, DOI 10.1109/CVPR.2019.00463
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni N, 2019, IEEE I CONF COMP VIS, P2202, DOI 10.1109/ICCV.2019.00229
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Neverova N., 2018, PROC EUR C COMPUT VI, P123
   Neverova N., 2020, NeurIPS
   Neverova N, 2019, Advances in Neural Information Processing Systems (NeurIPS), V32, P920
   Neverova N, 2019, PROC CVPR IEEE, P10907, DOI 10.1109/CVPR.2019.01117
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Oliver A, 2018, ADV NEUR IN, V31
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Paszke A, 2019, ADV NEUR IN, V32
   Rong Y, 2019, IEEE I CONF COMP VIS, P5339, DOI 10.1109/ICCV.2019.00544
   Sanakoyeu A, 2020, PROC CVPR IEEE, P5232, DOI 10.1109/CVPR42600.2020.00528
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Thewlis J., 2017, Advances in Neural Information Processing Systems, P844
   Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang T., 2018, ARXIV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3780, DOI 10.1145/3394171.3414014
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiao C., 2019, PROC IEEECVF INT C C, P1725
   Xie Q., 2020, P ADV NEUR INF PROC, P6252
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Zhang HK, 2019, IEEE I CONF COMP VIS, P1725, DOI 10.1109/ICCV.2019.00181
   Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zheng HT, 2021, IEEE T IMAGE PROCESS, V30, P1898, DOI 10.1109/TIP.2020.3031108
   Zhou ZH, 2018, NATL SCI REV, V5, P44, DOI 10.1093/nsr/nwx106
   Zhu Tyler, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P225, DOI 10.1007/978-3-030-58526-6_14
NR 60
TC 2
Z9 2
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 572
EP 583
DI 10.1109/TMM.2021.3129056
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800018
DA 2024-07-18
ER

PT J
AU Zhou, DL
   Zhang, HJ
   Li, Q
   Ma, JH
   Xu, XF
AF Zhou, Dongliang
   Zhang, Haijun
   Li, Qun
   Ma, Jianghong
   Xu, Xiaofei
TI COutfitGAN: Learning to Synthesize Compatible Outfits Supervised by
   Silhouette Masks and Fashion Styles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compatibility learning; fashion analysis; generative adversarial
   network; image-to-image translation; fashion synthesis
ID IMAGE SYNTHESIS; DESIGN
AB How to recommend outfits has gained considerable attention in both academia and industry in recent years. Many studies have been carried out regarding fashion compatibility learning, to determine whether the fashion items in an outfit are compatible or not. These methods mainly focus on evaluating the compatibility of existing outfits and rarely consider applying such knowledge to 'design' new fashion items. We propose the new task of generating complementary and compatible fashion items based on an arbitrary number of given fashion items. In particular, given some fashion items that can make up an outfit, the aim of this paper is to synthesize photo-realistic images of other, complementary, fashion items that are compatible with the given ones. To achieve this, we propose an outfit generation framework, referred to as COutfitGAN, which includes a pyramid style extractor, an outfit generator, a UNet-based real/fake discriminator, and a collocation discriminator. To train and evaluate this framework, we collected a large-scale fashion outfit dataset with over 200 K outfits and 800 K fashion items from the Internet. Extensive experiments show that COutfitGAN outperforms other baselines in terms of similarity, authenticity, and compatibility measurements.
C1 [Zhou, Dongliang; Zhang, Haijun; Li, Qun; Ma, Jianghong; Xu, Xiaofei] Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology
RP Zhang, HJ (corresponding author), Harbin Inst Technol, Dept Comp Sci, Shenzhen 518055, Peoples R China.
EM zhou_dongliang@stu.hit.edu.cn; hjzhang@hit.edu.cn;
   19b951028@stu.hit.edu.cn; majianghong@hit.edu.cn; xiaofei@hit.edu.cn
RI Zhang, Haijun/N-8470-2015; Zhou, Dongliang/AAY-4577-2021; Xu,
   Xiaofei/IQS-7571-2023
OI Zhou, Dongliang/0000-0003-0361-8597; Ma, Jianghong/0000-0002-0524-3584
FU National Natural Science Foundation of China [61972112, 61832004];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515020088];
   Shenzhen Science and Technology Program [JCYJ20210324131203009];
   HITSZ-J&A Joint Laboratory of Digital Design and Intelligent Fabrication
   [HITSZ-JA-2021A01]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61972112 and 61832004, in part by the
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2021B1515020088, in part by the Shenzhen Science and Technology Program
   under Grant JCYJ20210324131203009, and in part by the HITSZ-J&A Joint
   Laboratory of Digital Design and Intelligent Fabrication under Grant
   HITSZ-J&A-2021A01.
CR Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Bau D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323023
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Ding YJ, 2022, IEEE T MULTIMEDIA, V24, P2687, DOI 10.1109/TMM.2021.3088281
   Feng ZH, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3338841
   Gatys L., 2015, NIPS
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648
   Hensel M, 2017, ADV NEUR IN, V30
   Hsiao WL, 2019, IEEE I CONF COMP VIS, P5046, DOI 10.1109/ICCV.2019.00515
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras T, 2018, P INT C LEARN REPR I
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim D, 2021, IEEE INT CONF COMP V, P1057, DOI 10.1109/ICCVW54120.2021.00123
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu LL, 2020, IEEE T NEUR NET LEAR, V31, P3540, DOI 10.1109/TNNLS.2019.2944979
   Liu LL, 2019, NEUROCOMPUTING, V341, P156, DOI 10.1016/j.neucom.2019.03.011
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Or-El Roy, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P739, DOI 10.1007/978-3-030-58539-6_44
   Park Taesung, 2020, EUR C COMP VIS, P319, DOI [DOI 10.1007/978-3-030-58545-719, DOI 10.1007/978-3-030-58545-7_19]
   Paszke A., 2017, ADV NEURAL INF PROCE, V9, P1, DOI DOI 10.1017/CB09781107707221.009
   Le Q, 2014, PR MACH LEARN RES, V32, P1188
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Schonfeld E., 2020, 2020 IEEECVF C COMPU, P8204, DOI 10.1109/CVPR42600.2020.00823
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XM, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P320, DOI 10.1145/3343031.3350956
   Song XM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P753, DOI 10.1145/3123266.3123314
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P329, DOI 10.1145/3343031.3350909
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Yan HZ, 2024, IEEE T COMPUT SOC SY, V11, P3079, DOI [10.1109/TCSS.2022.3161996, 10.1109/TMM.2022.3146010]
   Yang X., 2021, ACM Trans. Multimedia Comput., Commun., Appl., V17, P1
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 58
TC 10
Z9 10
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4986
EP 5001
DI 10.1109/TMM.2022.3185894
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300028
DA 2024-07-18
ER

PT J
AU Chen, T
   Wang, SH
   Wang, Q
   Zhang, Z
   Xie, GS
   Tang, ZN
AF Chen, Tao
   Wang, Shui-Hua
   Wang, Qiong
   Zhang, Zheng
   Xie, Guo-Sen
   Tang, Zhenmin
TI Enhanced Feature Alignment for Unsupervised Domain Adaptation of
   Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial learning; Domain adaptation; pseudo label; semantic
   segmentation
ID NETWORK
AB Unsupervised domain adaptation for semantic segmentation aims to transfer knowledge from a labeled source domain to another unlabeled target domain. However, due to the label noise and domain mismatch, learning directly from source domain data tends to have poor performance. Though adversarial learning methods strive to reduce domain discrepancies by aligning feature distributions, traditional methods suffer from the training imbalance and feature distortion problems. Besides, due to the absence of target domain labels, the classifier is blind to features from the target domain during training. Consequently, the final classifier overfits the source domain features and usually fails to predict the structured outputs of the target domain. To alleviate these problems, we focus on enhancing the adversarial learning based feature alignment from three perspectives. First, a classification constrained discriminator is proposed to balance the adversarial training and alleviate the feature distortion problem. Next, to alleviate the classifier overfitting problem, self-training is collaboratively used to learn a domain robust classifier with target domain pseudo labels. Moreover, an efficient class centroid calculation module is proposed and the domain discrepancy is further reduced by aligning the feature centroids of the same class from different domains. Experimental evaluations on GTA5 -> Cityscapes and SYNTHIA -> Cityscapes demonstrate state-of-the-art results compared to other counterpart methods. The source code and models have been made available at.(1)
C1 [Chen, Tao; Wang, Qiong; Xie, Guo-Sen; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Wang, Shui-Hua] Univ Leicester, Sch Math & Actuarial Sci, Leicester LE1 7RH, Leics, England.
   [Zhang, Zheng] Harbin Inst Technol, ShenzhenKey Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Zheng] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Nanjing University of Science & Technology; University of Leicester;
   Harbin Institute of Technology; Peng Cheng Laboratory
RP Xie, GS (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.; Zhang, Z (corresponding author), Harbin Inst Technol, ShenzhenKey Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
EM taochen@njust.edu.cn; sw546@le.ac.uk; wangq@njust.edu.cn;
   zhengzhang@hit.edu.cn; gsxiehm@gmail.com; Tzm.cs@njust.edu.cn
RI Zhang, Zheng/M-6325-2014; Zhang, Zhang/JAX-2097-2023; Wang,
   Shuihua/G-7326-2016; Chen, Tao/ABB-5983-2022; Xie, Guo-Sen/AAL-6674-2020
OI Zhang, Zheng/0000-0003-1470-6998; Xie, Guo-Sen/0000-0002-5487-9845;
   Chen, Tao/0000-0001-8239-1698; Wang, Qiong/0000-0003-4193-0960
FU National Natural Science Foundation of China [61976116, 61702163];
   Fundamental Research Funds for the Central Universities [30920021135];
   Shenzhen Fundamental Research Fund
   [GXWD20201230155427003-20200824103320001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976116 and 61702163, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   30920021135, and in part by Shenzhen Fundamental Research Fund under
   Grant GXWD20201230155427003-20200824103320001. The lead editor
   coordinating the review of this manuscript and approving it for
   publication was J. Zhang.
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen CQ, 2019, PROC CVPR IEEE, P627, DOI 10.1109/CVPR.2019.00072
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen M., 2011, ADV NEURAL INFORM PR, P2456
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Chen YH, 2017, IEEE I CONF COMP VIS, P2011, DOI 10.1109/ICCV.2017.220
   Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du L, 2019, IEEE I CONF COMP VIS, P982, DOI 10.1109/ICCV.2019.00107
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang J., 2020, P EUR C COMP VIS, P705
   Hung W.-C., 2018, PROC BRIT MACH VIS C
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jinyu Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P480, DOI 10.1007/978-3-030-58583-9_29
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kang Y. Wei, 2020, P ADV NEUR INF PROC, V33, P3569
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee WY, 2018, IEEE T MULTIMEDIA, V20, P142, DOI 10.1109/TMM.2017.2726184
   Li YS, 2019, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR.2019.00710
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Liu H, 2019, PR MACH LEARN RES, V97
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luo YW, 2019, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2019.00261
   Lv FM, 2020, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR42600.2020.00439
   Maas A. L., 2013, PROC INT C MACH LEAR
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pan F, 2020, PROC CVPR IEEE, P3763, DOI 10.1109/CVPR42600.2020.00382
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Ros G, 2016, PROC CVPR IEEE, P3234, DOI 10.1109/CVPR.2016.352
   Simonyan K, 2015, IEEE INT C ICLR
   Subhani M. N., 2020, P EUR C COMP VIS, P290
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   Tzeng E., 2014, ARXIV14123474
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang Q, 2019, IEEE T IMAGE PROCESS, V28, P4376, DOI 10.1109/TIP.2019.2910667
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xie S., 2018, PR MACH LEARN RES, P5423
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yang YC, 2020, PROC CVPR IEEE, P4084, DOI 10.1109/CVPR42600.2020.00414
   Yao Yazhou, 2021, CVPR
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang WC, 2018, PROC CVPR IEEE, P3801, DOI 10.1109/CVPR.2018.00400
   Zhang Y, 2020, IEEE T PATTERN ANAL, V42, P1823, DOI 10.1109/TPAMI.2019.2903401
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SC, 2022, IEEE T NEUR NET LEAR, V33, P473, DOI 10.1109/TNNLS.2020.3028503
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 66
TC 19
Z9 19
U1 6
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1042
EP 1054
DI 10.1109/TMM.2021.3106095
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800003
DA 2024-07-18
ER

PT J
AU Liu, ZM
   Jia, W
   Yang, M
   Luo, PY
   Guo, Y
   Tan, MK
AF Liu, Zhuoman
   Jia, Wei
   Yang, Ming
   Luo, Peiyao
   Guo, Yong
   Tan, Mingkui
TI Deep View Synthesis via Self-Consistent Generative Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Task analysis; Image reconstruction; Generative adversarial
   networks; Training; Solid modeling; Feature extraction; Generative
   model; large baseline; self-consistency; view synthesis
ID QUALITY INDEX; IMAGES
AB View synthesis aims to produce unseen views from a set of views captured by two or more cameras at different positions. This task is non-trivial since it is hard to conduct pixel-level matching among different views. To address this issue, most existing methods seek to exploit the geometric information to match pixels. However, when the distinct cameras have a large baseline (i. e., far away from each other), severe geometry distortion issues would occur and the geometric information may fail to provide useful guidance, resulting in very blurry synthesized images. To address the above issues, in this paper, we propose a novel deep generative model, called Self-Consistent Generative Network (SCGN), which synthesizes novel views from the given input views without explicitly exploiting the geometric information. The proposed SCGN model consists of two main components, i. e., a View Synthesis Network (VSN) and a View Decomposition Network (VDN), both employing an Encoder-Decoder structure. Here, the VDN seeks to reconstruct input views from the synthesized novel view to preserve the consistency of view synthesis. Thanks to VDN, SCGN is able to synthesize novel views without using any geometric rectification before encoding, making it easier for both training and applications. Finally, adversarial loss is introduced to improve the photo-realism of novel views. Both qualitative and quantitative comparisons against several state-of-the-art methods on two benchmark tasks demonstrated the superiority of our approach.
C1 [Liu, Zhuoman; Luo, Peiyao; Guo, Yong; Tan, Mingkui] South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
   [Liu, Zhuoman; Jia, Wei; Yang, Ming] CVTE Res, Guangzhou 510335, Peoples R China.
   [Liu, Zhuoman] Pazhou Lab, Guangzhou 510335, Peoples R China.
   [Tan, Mingkui] Minist Educ, Key Lab Big Data & Intelligent Robot, Beijing, Peoples R China.
C3 South China University of Technology; Pazhou Lab
RP Tan, MK (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
EM liuzhuoman@cvte.com; jiawei@cvte.com; yangming@cvte.com;
   is.luopeiyao@gmail.com; guoyongcs@gmail.com; mingkuitan@scut.edu.cn
OI Liu, Zhuoman/0000-0002-2991-5242; Guo, Yong/0000-0002-3444-4588
FU Science and Technology Program of Guangzhou, China [202007030007];
   Key-Area Research and Development Program of Guangdong Province
   [2018B010107001]; National Natural Science Foundation of China (NSFC)
   [61836003]; Guangdong Project [2017ZT07X183]; Fundamental Research Funds
   for the Central Universities [D2191240]
FX This work was supported in part by the Science and Technology Program of
   Guangzhou, China, under Grant 202007030007, in part by the Key-Area
   Research and Development Program of Guangdong Province (2018B010107001),
   in part by National Natural Science Foundation of China (NSFC) 61836003
   (key project), Guangdong Project 2017ZT07X183, Fundamental Research
   Funds for the Central Universities D2191240. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto. (Zhuoman Liu and Wei Jia
   contributed equally to this work.)
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Atzpadin N, 2004, IEEE T CIRC SYST VID, V14, P321, DOI 10.1109/TCSVT.2004.823391
   Cao JZ, 2022, IEEE T PATTERN ANAL, V44, P211, DOI 10.1109/TPAMI.2020.3012096
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Choi I, 2019, IEEE I CONF COMP VIS, P7780, DOI 10.1109/ICCV.2019.00787
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Flynn J, 2019, PROC CVPR IEEE, P2362, DOI 10.1109/CVPR.2019.00247
   Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo Y, 2020, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR42600.2020.00545
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   Habtegebrial T, 2019, PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP), VOL 5, P792, DOI 10.5220/0007360107920799
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Huang YP, 2020, IEEE SIGNAL PROC LET, V27, P685, DOI 10.1109/LSP.2020.2988830
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Ji DH, 2017, PROC CVPR IEEE, P7092, DOI 10.1109/CVPR.2017.750
   Jin X, 2018, IEEE T MULTIMEDIA, V20, P2891, DOI 10.1109/TMM.2018.2827781
   Karras T., 2018, INT CONFLEARN REPRES
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Li JC, 2019, IEEE T BIO-MED ENG, V66, P3499, DOI 10.1109/TBME.2019.2906667
   Li LD, 2021, IEEE T MULTIMEDIA, V23, P320, DOI 10.1109/TMM.2020.2980185
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Liu MM, 2018, PROC CVPR IEEE, P4616, DOI 10.1109/CVPR.2018.00485
   Novotny D., 2019, PROC INT C NEURAL IN, P7599
   Oh KJ, 2009, PCS: 2009 PICTURE CODING SYMPOSIUM, P233
   Park E, 2017, PROC CVPR IEEE, P702, DOI 10.1109/CVPR.2017.82
   Paszke A, 2019, ADV NEUR IN, V32
   Radford A., 2015, ARXIV
   Sagonas C, 2015, IEEE I CONF COMP VIS, P3871, DOI 10.1109/ICCV.2015.441
   Salimans T, 2016, ADV NEUR IN, V29
   Scharstein D, 1996, PROC CVPR IEEE, P852, DOI 10.1109/CVPR.1996.517171
   Sun SH, 2018, LECT NOTES COMPUT SC, V11207, P162, DOI 10.1007/978-3-030-01219-9_10
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Xu XG, 2019, IEEE I CONF COMP VIS, P7790, DOI 10.1109/ICCV.2019.00788
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P863, DOI 10.1109/TMM.2018.2870540
   Yu F., 2015, ARXIV
   Zhang Y, 2019, PROC CVPR IEEE, P5853, DOI 10.1109/CVPR.2019.00601
   Zhou TH, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201323
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhou Y, 2019, IEEE T IMAGE PROCESS, V28, P4566, DOI 10.1109/TIP.2019.2912463
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 45
TC 5
Z9 5
U1 4
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 451
EP 465
DI 10.1109/TMM.2021.3053401
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300035
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Mun, H
   Yoon, GJ
   Song, J
   Yoon, SM
AF Mun, Hwanbok
   Yoon, Gang-Joon
   Song, Jinjoo
   Yoon, Sang Min
TI Texture Preserving Photo Style Transfer Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Image decomposition; Deep learning; Distortion;
   Semantics; Image edge detection; Smoothing methods; Photo style
   transfer; image decomposition; texture preserving style transfer
AB Photo style transfer aims to change the style of a given photo to a reference style image with the constraint by retaining the broad and faithful conservation of the content of the input image. Most previous algorithms still have challenging issues on how to exactly extract and represent the style of the image to avoid the interruption of human visual perception. In this paper, we present a texture preserving photo style transfer algorithm by separating the input image into texture and structure and then applying the deep structure style transfer network to effectively change the extracted style characteristics of the structure. The texture preserving photo style transfer overcomes the main drawback of the previous approaches like distortion and saturation of the boundary of the objects. The quantitative and qualitative experimental results including user study prove that the proposed photo style transfer is universally applicable comparing to remarkable previous approaches.
C1 [Mun, Hwanbok] ActionPower Corp, 1838 Nambusunhwan Ro,11F, Seoul 08788, South Korea.
   [Yoon, Gang-Joon] Natl Inst Math Sci, Yuseong Daero 1689 Beon Gil, Daejeon 34047, South Korea.
   [Song, Jinjoo; Yoon, Sang Min] Kookmin Univ, Coll Comp Sci, 77 Jeungneung Ro, Seoul 02707, South Korea.
C3 National Institute for Mathematical Sciences (NIMS), Republic of Korea;
   Kookmin University
RP Yoon, SM (corresponding author), Kookmin Univ, Coll Comp Sci, 77 Jeungneung Ro, Seoul 02707, South Korea.
EM helloiwsy@gmail.com; gangjoon@gmail.com; decpear1@kookmin.ac.kr;
   smyoon@kookmin.ac.kr
RI Yoon, Gangjoon/GZM-8532-2022
OI Yoon, Gang-Joon/0000-0002-0654-491X; Song, Jinjoo/0000-0003-3335-5644
FU National Institute for Mathematical Sciences, Korean government
   [NIMS-B21810000]; National Research Foundation of Korea
   [NRF-2021R1A2C1008555, 2021R1F1A1059202]; Institute of Information
   Communications Technology Planning and Evaluation (IITP) [2020-0-01826]
FX This work was supported in part by the National Institute for
   Mathematical Sciences, Korean government under Grant NIMS-B21810000, in
   part by the National Research Foundation of Korea under Grant
   2021R1F1A1059202, in part by the Institute of Information Communications
   Technology Planning and Evaluation (IITP) under Grant 2020-0-01826, and
   in part by the National Research Foundation of Korea under Grant
   NRF2021R1A2C1008555.
CR An J, 2020, AAAI CONF ARTIF INTE, V34, P10443
   [Anonymous], 2018, COLOUR VISUAL COMPUT
   [Anonymous], 2005, P ACM S SOL PHYS MOD
   Araslanov N., 2021, IEEECVF C COMPUTER V, P15384
   Ashikhmin M, 2003, IEEE COMPUT GRAPH, V23, P38, DOI 10.1109/MCG.2003.1210863
   Bénard P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461929
   Chang YH, 2003, COMPUTER GRAPHICS INTERNATIONAL, PROCEEDINGS, P176, DOI 10.1109/CGI.2003.1214463
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen QF, 2017, IEEE I CONF COMP VIS, P2516, DOI 10.1109/ICCV.2017.273
   Chen Y.-L, 2016, PROC BRIT MACH VIS C, P8
   Cheng L, 2008, PROC CVPR IEEE, P179
   Chengtao C, 2015, CHIN CONT DECIS CONF, P3964, DOI 10.1109/CCDC.2015.7162616
   Cho H, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601188
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhir R., 2020, REV COMPUTER ENG RES, V7, P86
   Efros AA, 2001, COMP GRAPH, P341, DOI 10.1145/383259.383296
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Fan QN, 2021, IEEE T PATTERN ANAL, V43, P33, DOI 10.1109/TPAMI.2019.2925793
   Fan QN, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275081
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Frigo O, 2016, PROC CVPR IEEE, P553, DOI 10.1109/CVPR.2016.66
   Frigo O, 2015, LECT NOTES COMPUT SC, V9005, P655, DOI 10.1007/978-3-319-16811-1_43
   Gatys LA, 2015, ADV NEUR IN, V28
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Jheng-Wei Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7965, DOI 10.1109/CVPR42600.2020.00799
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Li YH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2230
   Li YJ, 2018, LECT NOTES COMPUT SC, V11207, P468, DOI 10.1007/978-3-030-01219-9_28
   Li YJ, 2017, ADV NEUR IN, V30
   Lin K, 2019, PICT COD SYMP, P1
   Liu SF, 2016, LECT NOTES COMPUT SC, V9908, P560, DOI 10.1007/978-3-319-46493-0_34
   Liu ZC, 2004, IEEE COMPUT GRAPH, V24, P30, DOI 10.1109/MCG.2004.1297008
   Lu KY, 2018, LECT NOTES COMPUT SC, V11208, P229, DOI 10.1007/978-3-030-01225-0_14
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Park JH, 2019, IMAGE VISION COMPUT, V87, P13, DOI 10.1016/j.imavis.2019.04.001
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Ruder M, 2016, LECT NOTES COMPUT SC, V9796, P26, DOI 10.1007/978-3-319-45886-1_3
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J, 2019, NEUROCOMPUTING, V366, P264, DOI 10.1016/j.neucom.2019.08.017
   Song J, 2018, IEEE T CIRC SYST VID, V28, P2164, DOI 10.1109/TCSVT.2017.2717542
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wang N, 2021, IEEE T IMAGE PROCESS, V30, P1784, DOI 10.1109/TIP.2020.3048629
   Wang X, 2017, PROC CVPR IEEE, P7178, DOI 10.1109/CVPR.2017.759
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Xu L, 2015, PR MACH LEARN RES, V37, P1669
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Yao X, 2019, IEEE IMAGE PROC, P2314, DOI [10.1109/icip.2019.8803312, 10.1109/ICIP.2019.8803312]
   Yoo J, 2019, IEEE I CONF COMP VIS, P9035, DOI 10.1109/ICCV.2019.00913
   Zhang Q, 2014, LECT NOTES COMPUT SC, V8691, P815, DOI 10.1007/978-3-319-10578-9_53
   Zhang R, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073703
   Zhang SD, 2020, VISUAL COMPUT, V36, P1797, DOI 10.1007/s00371-019-01774-8
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
   Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0
NR 61
TC 7
Z9 7
U1 4
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3823
EP 3834
DI 10.1109/TMM.2021.3108401
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400012
DA 2024-07-18
ER

PT J
AU Tajrobehkar, M
   Tang, KH
   Zhang, HW
   Lim, JH
AF Tajrobehkar, Mitra
   Tang, Kaihua
   Zhang, Hanwang
   Lim, Joo-Hwee
TI Align R-CNN: A Pairwise Head Network for Visual Relationship Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Visualization; Task analysis; Predictive models;
   Semantics; Context modeling; Data mining; Pairwise feature alignment;
   scene graph generation; visual attention
AB Scene graphs connect individual objects with visual relationships. They serve as a comprehensive scene representation for downstream multimodal tasks. However, by exploring recent progress in Scene Graph Generation (SGG), we find that the performance of recent works is highly limited by the pairwise relationship modeling by naive feature concatenation. Such pairwise features lack sufficient object interaction due to the mis-aligned object parts, resulting in non-discriminative pairwise features for visual relationship prediction. For example, naive concatenated pairwise feature usually make the model fail to discriminate between riding and feeding for object pair person and horse. To this end, we design a meta-architecture- learning-to-align - for dynamic object feature concatenation. We call our model: Align R-CNN. Specifically, we introduce a novel attention-based multiple region alignment module that can be jointly optimized with SGG. Experiments on the large-scale SGG benchmark Visual Genome show that the proposed Align R-CNN can replace the naive feature concatenation and thus boost all the existing SGG methods.
C1 [Tajrobehkar, Mitra; Tang, Kaihua; Zhang, Hanwang; Lim, Joo-Hwee] Nanyang Technol Univ, SCSE, Singapore 67904777, Singapore.
   [Tajrobehkar, Mitra; Lim, Joo-Hwee] ASTAR, Inst Infocomm Res, Singapore 119613, Singapore.
C3 Nanyang Technological University; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Tajrobehkar, M (corresponding author), Nanyang Technol Univ, SCSE, Singapore 67904777, Singapore.
EM tajr0001@e.ntu.edu.sg; kaihua001@e.ntu.edu.sg; hanwangzhang@ntu.edu.sg;
   joohwee@i2r.a-star.edu.sg
OI Zhang, Hanwang/0000-0001-7374-8739; Tang, Kaihua/0000-0002-1008-7053;
   LIM, Joo Hwee/0000-0002-4103-3824
CR Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gkanatsios N, 2019, IEEE INT CONF COMP V, P1754, DOI 10.1109/ICCVW.2019.00218
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   He K., 2018, IEEE T PATTERN ANAL, V42, P386, DOI DOI 10.1109/TPAMI.2018.2844175
   Hwang SJ, 2018, PROC CVPR IEEE, P1014, DOI 10.1109/CVPR.2018.00112
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Krishna R, 2018, PROC CVPR IEEE, P6867, DOI 10.1109/CVPR.2018.00718
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lazaridou Angeliki, 2015, P 2015 C N AM CHAPT, P153, DOI 10.3115/v1/N15-1016
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Newell A., 2017, ADV NEUR IN, P2171
   Peyre J, 2017, IEEE I CONF COMP VIS, P5189, DOI 10.1109/ICCV.2017.554
   Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408
   Qi MS, 2019, PROC CVPR IEEE, P5232, DOI 10.1109/CVPR.2019.00538
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schroeder B, 2019, IEEE INT CONF COMP V, P1783, DOI 10.1109/ICCVW.2019.00221
   Schuster S., 2015, P 4 WORKSHOP VISION, P70
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo S., 2018, PROC 32 INT C NEURAL, P558
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang X, 2018, LECT NOTES COMPUT SC, V11216, P38, DOI 10.1007/978-3-030-01258-8_3
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yin GJ, 2018, LECT NOTES COMPUT SC, V11207, P330, DOI 10.1007/978-3-030-01219-9_20
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zenkel Thomas, 2019, ARXIV190111359
   Zhang C., 2019, BMVC 2019, P288
   Zhang CR, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1641, DOI 10.1145/3343031.3351000
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang HW, 2017, IEEE I CONF COMP VIS, P4243, DOI 10.1109/ICCV.2017.454
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang Y., 2018, 6 INT C LEARN REPR I
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
NR 59
TC 5
Z9 5
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1266
EP 1276
DI 10.1109/TMM.2021.3062543
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200002
DA 2024-07-18
ER

PT J
AU Wang, B
   Xu, MW
   Ren, FY
   Wu, JP
AF Wang, Bo
   Xu, Mingwei
   Ren, Fengyuan
   Wu, Jianping
TI Improving Robustness of DASH Against Unpredictable Network Variations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit rate; Quality of experience; Throughput; Prediction algorithms;
   Streaming media; Heuristic algorithms; Robustness; Adaptive control;
   DASH; lightweight; robustness
ID VIDEO
AB Most video players use adaptive bitrate (ABR) algorithms to provide good quality-of-experience (QoE) in dynamic network conditions. To deal with the adaptation challenges, many ABR algorithms select bitrate by optimizing a defined QoE function. Within the framework, various algorithms mainly differ in how the optimization problem is solved, including prediction-based approaches and learn-based approaches. However, these algorithms suffer from limited performance in the current popular mobile streaming which has limited resources and rapidly changing link rates. Existing machine-learning approaches face deployment difficulties on mobile devices, and prediction-based approaches that rely on throughput prediction experience large buffer occupancy variations in cellular networks, resulting in rebuffering frequently. To provide a robust and lightweight ABR algorithm for mobile streaming, this work improves the robustness of prediction-based scheme against unpredictable network variations and develops RBC (Robust Bitrate Controller) algorithm. Rather than optimizing QoE over the entire buffer capacity, RBC creates buffer margins to absorb the impact of throughput jitters and solves QoE maximization on the narrowed buffer range. The amount of buffer margin is dynamically adjusted based on the real-time throughput fluctuation to ensure sufficient de-jitter space. For online lightweight deployment, RBC provides a closed-form solution of the desired bitrate with small computation complexity by using adaptive control approach. Trace-driven experiments and real-world tests show that RBC effectively reduces the playback freezing and gains an improvement in overall QoE.
C1 [Wang, Bo; Xu, Mingwei; Wu, Jianping] Tsinghua Univ, Inst Network Sci & Cyberspace, Beijing 100084, Peoples R China.
   [Ren, Fengyuan] Tsinghua Univ, Dept Comp Sci & Techonol, Beijing 100084, Peoples R China.
   [Ren, Fengyuan] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University
RP Wang, B (corresponding author), Tsinghua Univ, Inst Network Sci & Cyberspace, Beijing 100084, Peoples R China.
EM wangbo2019@tsinghua.edu.cn; xumw@tsinghua.edu.cn; renfy@tsinghua.edu.cn;
   jianping@cernet.edu.cn
RI Xu, Mingwei/I-3712-2015
OI Xu, Mingwei/0000-0002-4847-4585
FU National Natural Science Foundation of China [62002196]; China
   Postdoctoral Science Foundation [2020M680573]; National Key Research and
   Development Plan of China [2017YFB0801701]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62002196, in part by the China
   Postdoctoral Science Foundation under Grant 2020M680573, and in part by
   the National Key Research and Development Plan of China under Grant
   2017YFB0801701. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Ali C. Begen.
CR Aaron A, 2015, IEEE IMAGE PROC, P1732, DOI 10.1109/ICIP.2015.7351097
   Aguayo M, 2018, IEEE T MULTIMEDIA, V20, P1224, DOI 10.1109/TMM.2017.2764325
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2018, RAW DATA MEASURING B
   [Anonymous], 2020, REC UPL ENC SETT
   Astrom K. J., 2013, ADAPTIVE CONTROL
   Balakrishnan H., 1997, Performance Evaluation Review, V25, P2, DOI 10.1145/258623.258631
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Google, 2016, ExoPlayer
   He Q, 2005, ACM SIGCOMM COMP COM, V35, P145, DOI 10.1145/1090191.1080110
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2020, IEEE J SEL AREA COMM, V38, P2324, DOI 10.1109/JSAC.2020.3000363
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Jiang ZB, 2019, IEEE T MULTIMEDIA, V21, P1577, DOI 10.1109/TMM.2018.2881095
   Khan K, 2019, IEEE T MULTIMEDIA, V21, P2012, DOI 10.1109/TMM.2019.2892304
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Qin Y., 2017, INT GEOL REV, P1, DOI DOI 10.1155/2017/3627251
   Qin YY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P189, DOI 10.1145/3304109.3306231
   Qin YY, 2018, CONEXT'18: PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P366, DOI 10.1145/3281411.3281439
   Ravi Netravali, 2015, 2015 USENIX ANN TECH, P417
   Sengupta S, 2018, I C NETWORK PROTOCOL, P165, DOI 10.1109/ICNP.2018.00026
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Wang B, 2021, IEEE T MOBILE COMPUT, V20, P174, DOI 10.1109/TMC.2019.2939124
   Wang B, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1122, DOI 10.1145/3123266.3123284
   Xiao KF, 2020, IEEE T MULTIMEDIA, V22, P474, DOI 10.1109/TMM.2019.2929929
   Xie XF, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P413, DOI 10.1145/2789168.2790118
   Yan F. Y., 2020, NSDI, P495
   Yeo H, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P645
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Yue CQ, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P153, DOI 10.1145/3339825.3391867
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 37
TC 4
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 323
EP 337
DI 10.1109/TMM.2021.3050086
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300025
DA 2024-07-18
ER

PT J
AU Zhang, B
   Chen, T
   Wang, B
   Li, RY
AF Zhang, Bo
   Chen, Tao
   Wang, Bin
   Li, Ruoyao
TI Joint Distribution Alignment via Adversarial Learning for Domain
   Adaptive Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Joint adaptive detection framework; domain adaptation for object
   detection; adversarial learning; class-wise transferability
ID DEEP; NETWORKS
AB Unsupervised domain adaptive object detection aims to adapt a well-trained detector from its original source domain with rich labeled data to a new target domain with unlabeled data. Recently, mainstream approaches perform this task through adversarial learning, yet still suffer from two limitations. First, they mainly align marginal distribution by unsupervised cross-domain feature matching, and ignore each feature's categorical and positional information that can be exploited for conditional alignment; Second, they treat all classes as equally important for transferring cross-domain knowledge and ignore that different classes usually have different transferability. In this article, we propose a joint adaptive detection framework (JADF) to address the above challenges. First, an end-to-end joint adversarial adaptation framework for object detection is proposed, which aligns both marginal and conditional distributions between domains without introducing any extra hyper-parameter. Next, to consider the transferability of each object class, a metric for class-wise transferability assessment is proposed, which is incorporated into the JADF objective for domain adaptation. Further, an extended study from unsupervised domain adaptation (UDA) to unsupervised few-shot domain adaptation (UFDA) is conducted, where only a few unlabeled training images are available in unlabeled target domain. Extensive experiments validate that JADF is effective in both the UDA and UFDA settings, achieving significant performance gains over existing state-of-the-art cross-domain detection methods.
C1 [Zhang, Bo; Chen, Tao; Wang, Bin; Li, Ruoyao] Fudan Univ, Key Lab Informat Sci Electromagnet Waves MoE, Shanghai 200433, Peoples R China.
   [Zhang, Bo; Chen, Tao; Wang, Bin; Li, Ruoyao] Fudan Univ, Res Ctr Smart Networks & Syst, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
C3 Fudan University; Fudan University
RP Wang, B (corresponding author), Fudan Univ, Key Lab Informat Sci Electromagnet Waves MoE, Shanghai 200433, Peoples R China.; Wang, B (corresponding author), Fudan Univ, Res Ctr Smart Networks & Syst, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
EM 18110720048@fudan.edu.cn; eetchen@fudan.edu.cn; wangbin@fudan.edu.cn;
   18210720036@fudan.edu.cn
RI BIN, WANG/K-3869-2019; Zhang, Bo/ABF-8476-2021; Chen, Tao/IQV-1588-2023
OI Zhang, Bo/0000-0001-8052-782X; Chen, Tao/0000-0003-4565-5548; Chen,
   Tao/0000-0002-0779-9818
FU National Natural Science Foundation of China [61731021, 62071127,
   U1909207]; Shanghai Municipal Science and Technology Major Project
   [2021SHZDZX0100]
FX Thiswork was supported in part by the National Natural Science
   Foundation ofChina under Grants 61731021, 62071127, and U1909207, and in
   part by the Shanghai Municipal Science and Technology Major Project
   under Grant 2021SHZDZX0100.
CR Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542
   Chang-Dong Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11721, DOI 10.1109/CVPR42600.2020.01174
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, ADV NEUR IN, V29
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   He ZW, 2019, IEEE I CONF COMP VIS, P6667, DOI 10.1109/ICCV.2019.00677
   Hsu HK, 2020, IEEE WINT CONF APPL, P738, DOI [10.1109/WACV45572.2020.9093358, 10.1109/wacv45572.2020.9093358]
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Kim S, 2019, IEEE I CONF COMP VIS, P6091, DOI 10.1109/ICCV.2019.00619
   Kim T, 2019, PROC CVPR IEEE, P12448, DOI 10.1109/CVPR.2019.01274
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long M., 2016, Advances in neural information processing systems, V29
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Redmon J, 2018, Arxiv, DOI [arXiv:1804.02767, DOI 10.1109/CVPR.2017.690, DOI 10.48550/ARXIV.1804.02767]
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang T, 2019, PROC CVPR IEEE, P7166, DOI 10.1109/CVPR.2019.00734
   Wang Ximei, 2019, P ADV NEUR INF PROC, P1953
   Wang XD, 2019, PROC CVPR IEEE, P7281, DOI 10.1109/CVPR.2019.00746
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang YB, 2019, PROC CVPR IEEE, P5026, DOI 10.1109/CVPR.2019.00517
   Zhang ZS, 2018, PROC CVPR IEEE, P5813, DOI 10.1109/CVPR.2018.00609
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zhu XG, 2019, PROC CVPR IEEE, P687, DOI 10.1109/CVPR.2019.00078
NR 53
TC 7
Z9 7
U1 9
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4102
EP 4112
DI 10.1109/TMM.2021.3114550
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400031
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, FF
   Xu, ML
   Xu, CS
AF Zhang, Feifei
   Xu, Mingliang
   Xu, Changsheng
TI Weakly-Supervised Facial Expression Recognition in the Wild With Noisy
   Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Face recognition; Data models; Task analysis;
   Training data; Training; Annotations; Facial expression recognition;
   noisy labeled data; clean labels; end-to-end; pose modeling; noise
   modeling
ID MULTIVIEW; POSE
AB Facial expression recognition (FER) has attracted much attention in recent years due to its wide applications. While some progress has been achieved thanks to the emergence of deep learning, the challenge occasioned by pose variations remains. Therefore, most conventional approaches mainly perform FER under laboratory-controlled environment, and the FER in-the-wild has received relatively less attention. To implement the FER in-the-wild, the pose-invariant expression recognition model would be a possible solution but for a paucity of training data. Sufficient training data with reliable expression labels on FER tasks typically are unavailable. This paper devotes to addressing the problem of how to model pose variations in facial images, and how to leverage noisy data in the web to boost the FER performance. The proposed model is implemented in an end-to-end weakly supervised manner and enjoys several merits. First, the proposed model utilizes massive noisy labeled data to boost the performance of the FER classifier trained on a small set of clean labels. Second, we offer a novel pose modeling network to adaptively capture the discrepancy in the deep representation space of facial images under different head poses, and consequently, the pose-invariant expression representations can be learned in our model. Last, to exploit the reliable information in the noisy data, we formulate a noise modeling network, which is capable of learning the mapping from feature space to the residuals between clean labels and noisy labels. We validate the proposed approach on four public FER benchmarks: AffectNet, RAF-DB, SFEW, and BU-3DFE. Extensive experiments show that the proposed method performs favorably against state-of-the-art methods.
C1 [Zhang, Feifei; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450000, Peoples R China.
   [Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Zhengzhou
   University; Chinese Academy of Sciences; University of Chinese Academy
   of Sciences, CAS; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM feifei.zhang@ia.ac.cn; iexumingliang@zzu.edu.cn; csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022; zhang, fei/KHU-5230-2024;
   Zhang, Feifei/A-3199-2015
OI xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2017YFB1002804];
   National Natural Science Foundation of China [61720106006, 62002355,
   61721004, 61832002, 61532009, U1705262, U1836220, 61751211, 62072455];
   Key Research Program of Frontier Sciences, CAS [QYZDJ-SSW-JSC039];
   National Postdoctoral Program for Innovative Talents [BX20190367];
   Jiangsu Province key research and development plan [BE2020036]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002804, in part by the
   National Natural Science Foundation of China under Grants 61720106006,
   62002355, 61721004, 61832002, 61532009, U1705262, U1836220, 61751211,
   and 62072455, in part by Key Research Program of Frontier Sciences, CAS,
   under Grant QYZDJ-SSW-JSC039, in part by the National Postdoctoral
   Program for Innovative Talents under Grant BX20190367, and in part by
   Jiangsu Province key research and development plan under Grant
   BE2020036.
CR Al Futaisi ND, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P205, DOI 10.1145/3340555.3353751
   [Anonymous], 2013, OXID MED CELL LONGEV, DOI DOI 10.1155/2013/146860
   [Anonymous], 2016, P INT C LEARN REPR
   Ben Tanfous A, 2020, IEEE T PATTERN ANAL, V42, P2594, DOI 10.1109/TPAMI.2019.2932979
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Ding CX, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2845089
   Ekman P., 1976, PICTURES FACIAL AFFE, P153
   Eleftheriadis S, 2015, IEEE T IMAGE PROCESS, V24, P189, DOI 10.1109/TIP.2014.2375634
   Gold JR, 2017, PLAN HIST ENVIRON SE, P1
   Han JF, 2019, IEEE I CONF COMP VIS, P5137, DOI 10.1109/ICCV.2019.00524
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hesse N, 2012, INT C PATT RECOG, P3533
   Hu MY, 2019, PROC CVPR IEEE, P11509, DOI 10.1109/CVPR.2019.01178
   Hu W, 2019, PROC CVPR IEEE, P11879, DOI 10.1109/CVPR.2019.01216
   Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiabei Zeng, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11217), P227, DOI 10.1007/978-3-030-01261-8_14
   Jiang D, 2007, IEEE INT CONF MOB, P1001
   Jiang YG, 2018, IEEE T MULTIMEDIA, V20, P3137, DOI 10.1109/TMM.2018.2823900
   Jourabloo A, 2017, IEEE I CONF COMP VIS, P3219, DOI 10.1109/ICCV.2017.347
   Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341
   Kim Y, 2019, IEEE I CONF COMP VIS, P101, DOI 10.1109/ICCV.2019.00019
   King DB, 2015, ACS SYM SER, V1214, P1
   Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571
   Lenc K, 2015, PROC CVPR IEEE, P991, DOI 10.1109/CVPR.2015.7298701
   Li JN, 2019, PROC CVPR IEEE, P5046, DOI 10.1109/CVPR.2019.00519
   Li S, 2019, IEEE T IMAGE PROCESS, V28, P356, DOI 10.1109/TIP.2018.2868382
   Li S, 2017, PROC CVPR IEEE, P2584, DOI 10.1109/CVPR.2017.277
   Li S, 2018, 2018 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
   Li W, 2017, IEEE INT CONF AUTOMA, P103, DOI 10.1109/FG.2017.136
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P2439, DOI 10.1109/TIP.2018.2886767
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   LUCEY S, 2007, FACE RECOGNITION BOO, P395
   Luo CW, 2019, IEEE T MULTIMEDIA, V21, P2473, DOI 10.1109/TMM.2019.2903724
   Martinez Brais, 2019, IEEE Transactions on Affective Computing, V10, P325, DOI 10.1109/TAFFC.2017.2731763
   Masi I, 2019, INT J COMPUT VISION, V127, P642, DOI 10.1007/s11263-019-01178-0
   Mauthner T., 2015, P COMP VIS WINT WORK, P91
   Mejjati YA, 2018, PROC CVPR IEEE, P3465, DOI 10.1109/CVPR.2018.00365
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   Moore S, 2011, COMPUT VIS IMAGE UND, V115, P541, DOI 10.1016/j.cviu.2010.12.001
   Munjal B, 2019, PROC CVPR IEEE, P811, DOI 10.1109/CVPR.2019.00090
   Niu XS, 2019, PROC CVPR IEEE, P11909, DOI 10.1109/CVPR.2019.01219
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Qiurui Wang, 2019, IEEE Transactions on Multimedia, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Senechal T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P860, DOI 10.1109/FG.2011.5771363
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sukhbaatar S., 2015, Training convolutional networks with noisy labels, P1
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tang H, 2010, IEEE INT CON MULTI, P1202, DOI 10.1109/ICME.2010.5582576
   Tariq U, 2014, PATTERN RECOGN LETT, V46, P89, DOI 10.1016/j.patrec.2014.05.011
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang K, 2020, IEEE T IMAGE PROCESS, V29, P4057, DOI 10.1109/TIP.2019.2956143
   Wang SF, 2020, IEEE T PATTERN ANAL, V42, P2082, DOI 10.1109/TPAMI.2019.2911937
   Wang YS, 2018, PROC CVPR IEEE, P8688, DOI 10.1109/CVPR.2018.00906
   Whitehill J, 2009, IEEE T PATTERN ANAL, V31, P2106, DOI 10.1109/TPAMI.2009.42
   Wu BY, 2018, INT J COMPUT VISION, V126, P875, DOI 10.1007/s11263-018-1085-3
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xie SY, 2019, IEEE T MULTIMEDIA, V21, P211, DOI 10.1109/TMM.2018.2844085
   Xu Y, 2020, SOFT COMPUT, V24, P5971, DOI 10.1007/s00500-019-04530-1
   Yang HY, 2018, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2018.00231
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Zhan C, 2019, IEEE I CONF COMP VIS, P1151, DOI 10.1109/ICCV.2019.00124
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P6574, DOI 10.1109/TIP.2020.2991549
   Zhang FF, 2020, IEEE T IMAGE PROCESS, V29, P4445, DOI 10.1109/TIP.2020.2972114
   Zhang FF, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P126, DOI 10.1145/3240508.3240574
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zhao YR, 2019, PROC CVPR IEEE, P4908, DOI 10.1109/CVPR.2019.00505
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2009, IEEE I CONF COMP VIS, P1901, DOI 10.1109/ICCV.2009.5459421
   Zhong JX, 2019, PROC CVPR IEEE, P1237, DOI 10.1109/CVPR.2019.00133
   Zhong L, 2012, PROC CVPR IEEE, P2562, DOI 10.1109/CVPR.2012.6247974
   Zhou Y, 2019, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2019.00218
   Zong Y, 2018, IEEE T MULTIMEDIA, V20, P3160, DOI 10.1109/TMM.2018.2820321
NR 87
TC 28
Z9 28
U1 6
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1800
EP 1814
DI 10.1109/TMM.2021.3072786
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200005
DA 2024-07-18
ER

PT J
AU Kim, DH
   Song, BC
AF Kim, Dae Ha
   Song, Byung Cheol
TI Deep Metric Learning With Manifold Class Variability Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Manifolds; Task analysis; Tools; Probability distribution;
   Neural networks; Extraterrestrial measurements; Deep metric learning;
   image retrieval; linear discriminant analysis
ID EXTENSIONS
AB In deep metric learning (DML) techniques, understanding both the local and global characteristics of embedding space is essential. However, conventional DML techniques have two limitations as follows: First, Euclidean distance-based metrics never imply global information such as class variability because they only depend on the physical distance of samples. Second, they assume that the embedding space is simply a vector space which cannot represent complex data features. Therefore, we propose a novel loss function which can fully utilize characteristics of embedding space by using discriminant analysis and nonlinear mapping. With theoretical analysis, the superior performance of the proposed method is verified for the fine-grained retrieval datasets such as Cars196, CUB200-2011, Stanford online products, and In-shop clothes. Source code is available at https://github.com/kdhht2334/MCVA.
C1 [Kim, Dae Ha] Inha Univ, Elect Engn, Incheon 22212, South Korea.
   [Song, Byung Cheol] Inha Univ, Dept Elect Engn, Incheon 22212, South Korea.
C3 Inha University; Inha University
RP Song, BC (corresponding author), Inha Univ, Dept Elect Engn, Incheon 22212, South Korea.
EM kdhht5022@gmail.com; bcsong@inha.ac.kr
RI Kim, Daeha/HJH-1957-2023
OI Song, Byung Cheol/0000-0001-8742-3433; Kim, Dae Ha/0000-0003-3838-126X
FU Institute of Information and Communications Technology Planning
   Evaluation (IITP) - Korea government (MSIT, Ministry of Science and ICT)
   [2020-0-01389]; Industrial Technology Innovation Program through the
   Ministry of Trade, Industry, and Energy (MI, Korea) [10073154, 20006483]
FX This work was supported by Institute of Information and Communications
   Technology Planning Evaluation (IITP) grant funded by the Korea
   government (MSIT, Ministry of Science and ICT) [2020-0-01389, Artificial
   Intelligence Convergence Research Center (Inha University)] and
   Industrial Technology Innovation Program through the Ministry of Trade,
   Industry, and Energy (MI, Korea) [10073154, Development of
   Human-Friendly Human-Robot Interaction Technologies using Human Internal
   Emotional States] and [20006483, Development on Deep Learning based
   4K30P Edge Computing enabled IP Camera System].
CR Alipanahi B., 2008, PROC 23 NAT C ARTIF, P598
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Aziere N, 2019, PROC CVPR IEEE, P7291, DOI 10.1109/CVPR.2019.00747
   Ba D. P. K. J., 2015, PROC INT C LEARN REP
   Bengio Y, 2004, ADV NEUR IN, V16, P177
   Bishop Christopher M., 2006, Pattern Recognition and Machine Learning, V4
   Cai D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P714
   Carmo M. P. D, 2013, RIEMANNIAN GEOMETRY
   Chu Xu, 2020, P 37 INT C MACH LEAR, P1962
   Davis J. V., 2007, ICML, P209
   Ding SY, 2015, PATTERN RECOGN, V48, P2993, DOI 10.1016/j.patcog.2015.04.005
   Fletcher PT, 2004, IEEE T MED IMAGING, V23, P995, DOI 10.1109/TMI.2004.831793
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Goldberger J., 2004, P 17 INT C NEUR INF, P513
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Han YH, 2013, SIGNAL PROCESS, V93, P2169, DOI 10.1016/j.sigpro.2012.05.036
   Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XF, 2004, ADV NEUR IN, V16, P153
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jacob P, 2019, IEEE I CONF COMP VIS, P6538, DOI 10.1109/ICCV.2019.00664
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kemertas Mete, 2020, P IEEE CVF C COMP VI, P14362
   Kim H, 2018, IEEE T MULTIMEDIA, V20, P2415, DOI 10.1109/TMM.2018.2806224
   Kim S, 2020, PROC CVPR IEEE, P3235, DOI 10.1109/CVPR42600.2020.00330
   Kingma DP, 2013, ARXIV
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li DW, 2018, NEURAL NETWORKS, V105, P447, DOI 10.1016/j.neunet.2018.06.003
   Li YH, 2020, IEEE T MULTIMEDIA, V22, P1285, DOI 10.1109/TMM.2019.2939711
   Li YJ, 2019, PR MACH LEARN RES, V97
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin XD, 2018, LECT NOTES COMPUT SC, V11219, P714, DOI 10.1007/978-3-030-01267-0_42
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Nutter F, 2014, PR MACH LEARN RES, V32
   Opitz M, 2020, IEEE T PATTERN ANAL, V42, P276, DOI 10.1109/TPAMI.2018.2848925
   Podolskiy A, 2021, AAAI CONF ARTIF INTE, V35, P13675
   Qian Q, 2019, IEEE I CONF COMP VIS, P6459, DOI 10.1109/ICCV.2019.00655
   Ridgeway K, 2018, ADV NEUR IN, V31
   Roth K, 2020, PROC CVPR IEEE, P6567, DOI 10.1109/CVPR42600.2020.00660
   Roy SK, 2019, IEEE I CONF COMP VIS, P3046, DOI 10.1109/ICCV.2019.00314
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Schutze H., 2008, Introduction to information retrieval, V39, P234
   Snell J, 2017, ADV NEUR IN, V30
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sugiyama M, 2007, J MACH LEARN RES, V8, P1027
   Tyagi H, 2013, INF INFERENCE, V2, P69, DOI 10.1093/imaiai/iat003
   Ustinova E, 2016, ADV NEUR IN, V29
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vinyals O., 2016, P ADV NEUR INF PROC, V29, P3637
   Wah Catherine, 2011, Technical report
   Wang RP, 2009, PROC CVPR IEEE, P429, DOI 10.1109/CVPRW.2009.5206850
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Wu L, 2017, PATTERN RECOGN, V65, P238, DOI 10.1016/j.patcog.2016.12.022
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang M., 2020, Advances in Neural Information Processing Systems, V33, P17792
   Yin Z, 2018, ADV NEUR IN, V31
   Yuan TT, 2019, PROC CVPR IEEE, P4810, DOI 10.1109/CVPR.2019.00495
   Zhang M., 2013, ADV NEURAL INF PROCE, V26, P1178
   Zhou Y, 2016, NEURAL PROCESS LETT, V43, P727, DOI 10.1007/s11063-015-9443-4
NR 63
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG 4
PY 2021
VL 24
BP 3533
EP 3544
DI 10.1109/TMM.2021.3101944
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NB
UT WOS:000824706000001
OA hybrid
DA 2024-07-18
ER

PT J
AU Chen, L
   Zheng, HC
   Yan, ZW
   Li, Y
AF Chen, Lvran
   Zheng, Huicheng
   Yan, Zhiwei
   Li, Ye
TI Discriminative Region Mining for Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discriminative region mining; fine-grained representation; object
   detection
AB In generic object detection, detectors are often susceptible to foreground objects and background regions that share similar appearances. In this paper, we propose a novel discriminative region mining (DRM) module for object detection, which enables discriminative region localization and representation for accurate object identification. The DRM module is collaboratively optimized by an extra intramodule classification loss in addition to the usual detection loss, which ensures its adequate discriminative capability. Specifically, two derivatives of the DRM module, namely a local DRM module and a contextual DRM module are proposed to excavate local and contextual discriminative regions, respectively. Furthermore, we extend the local DRM module to capture multiple local discriminative regions with a diversity constraint. To explore informative local features, an image upsampling branch is introduced to generate fine-grained representation for the local DRM module. Extensive experiments on the PASCAL VOC and MS COCO datasets demonstrate the effectiveness of the proposed method. Simple baseline detectors with the built-in DRM can achieve state-of-the-art detection performance. For example, the proposed detector achieves a mean average precision of 81.0% on PASCAL VOC 2007 with an input size of 300 x 300 using a ResNet-18 backbone, which runs at 24.2 fps on an Nvidia Titan X GPU.
C1 [Chen, Lvran; Zheng, Huicheng; Yan, Zhiwei; Li, Ye] Sun Yat Sen Univ, Sch Comp Sci & Engineer Ing, Guangzhou 510006, Peoples R China.
   [Chen, Lvran; Zheng, Huicheng; Yan, Zhiwei; Li, Ye] Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou 510006, Peoples R China.
   [Chen, Lvran; Zheng, Huicheng; Yan, Zhiwei; Li, Ye] Guangdong Key Lab Informat Secur Technol, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University
RP Zheng, HC (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engineer Ing, Guangzhou 510006, Peoples R China.; Zheng, HC (corresponding author), Minist Educ, Key Lab Machine Intelligence & Adv Comp, Guangzhou 510006, Peoples R China.
EM chenlr3@mail2.sysu.edu.cn; zhenghch@mail.sysu.edu.cn;
   yanzhw5@mail2.sysu.edu.cn; liye27@mail2.sysu.edu.cn
FU National Natural Science Foundation of China [61976231, U1611461,
   61573387, 61172141]; Guangdong Basic and Applied Basic Research
   Foundation [2019A1515011869]; Science andTechnology Program of Guangzhou
   [201803030029]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976231, U1611461, 61573387, and
   61172141, in part by the Guangdong Basic and Applied Basic Research
   Foundation underGrant 2019A1515011869, and in part by the Science
   andTechnology Program of Guangzhou underGrant 201803030029.
CR Akilan T, 2017, IEEE SYS MAN CYBERN, P566, DOI 10.1109/SMC.2017.8122666
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Bell S, 2016, PROC CVPR IEEE, P2874, DOI 10.1109/CVPR.2016.314
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Chen XL, 2017, IEEE I CONF COMP VIS, P4106, DOI 10.1109/ICCV.2017.440
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gkioxari G, 2015, IEEE I CONF COMP VIS, P1080, DOI 10.1109/ICCV.2015.129
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoiem D, 2012, LECT NOTES COMPUT SC, V7574, P340, DOI 10.1007/978-3-642-33712-3_25
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jaderberg M, 2015, ADV NEUR IN, V28
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Kim B, 2021, INT J COMPUT VISION, V129, P579, DOI 10.1007/s11263-020-01386-z
   Kim SW, 2018, LECT NOTES COMPUT SC, V11209, P239, DOI 10.1007/978-3-030-01228-1_15
   Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557
   Kong T, 2016, PROC CVPR IEEE, P845, DOI 10.1109/CVPR.2016.98
   Li HY, 2019, INT J COMPUT VISION, V127, P225, DOI 10.1007/s11263-018-1101-7
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li S, 2019, IEEE I CONF COMP VIS, P6608, DOI 10.1109/ICCV.2019.00671
   Li YC, 2017, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2017.199
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Nie J, 2019, IEEE I CONF COMP VIS, P9536, DOI 10.1109/ICCV.2019.00963
   Ouyang WL, 2017, IEEE I CONF COMP VIS, P1956, DOI 10.1109/ICCV.2017.214
   Ouyang WL, 2017, IEEE T PATTERN ANAL, V39, P1320, DOI 10.1109/TPAMI.2016.2587642
   Pang YW, 2019, PROC CVPR IEEE, P7328, DOI 10.1109/CVPR.2019.00751
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212
   Shrivastava A., 2016, ARXIV PREPRINT ARXIV
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Singh K.K., 2020, P IEEE CVF C COMP VI, P11070
   Song KY, 2019, IEEE T CIRC SYST VID, V29, P2972, DOI 10.1109/TCSVT.2018.2875449
   Sun FC, 2019, IEEE T IMAGE PROCESS, V28, P5041, DOI 10.1109/TIP.2019.2917781
   Wang TC, 2019, IEEE I CONF COMP VIS, P1971, DOI 10.1109/ICCV.2019.00206
   Wang Wenguan, 2018, IEEE Trans Image Process, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062
   Zhu CC, 2019, PROC CVPR IEEE, P840, DOI 10.1109/CVPR.2019.00093
   Zhu YS, 2017, IEEE I CONF COMP VIS, P4146, DOI 10.1109/ICCV.2017.444
NR 65
TC 11
Z9 11
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4297
EP 4310
DI 10.1109/TMM.2020.3040539
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900030
DA 2024-07-18
ER

PT J
AU Li, JQ
   Li, JC
   Fang, FM
   Li, F
   Zhang, GX
AF Li, Jiaqian
   Li, Juncheng
   Fang, Faming
   Li, Fang
   Zhang, Guixu
TI Luminance-Aware Pyramid Network for Low-Light Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image enhancement; Lighting; Task analysis; Computer
   architecture; Image color analysis; Computational modeling; Low-light
   image enhancement; luminance-aware guidance; multi-scale contrast
   feature; pyramid structure
ID HISTOGRAM EQUALIZATION; FUSION NETWORK; SUPERRESOLUTION; RETINEX
AB Low-light image enhancement based on deep convolutional neural networks (CNNs) has revealed prominent performance in recent years. However, it is still a challenging task since the underexposed regions and details are always imperceptible. Moreover, deep learning models are always accompanied by complex structures and enormous computational burden, which hinders their deployment on mobile devices. To remedy these issues, in this paper, we present a lightweight and efficient Luminance-aware Pyramid Network (LPNet) to reconstruct normal-light images in a coarse-to-fine strategy. The architecture is comprised of two coarse feature extraction branches and a luminance-aware refinement branch with an auxiliary subnet learning the luminance map of the input and target images. Besides, we propose a multi-scale contrast feature block (MSCFB) that involves channel split, channel shuffle strategies, and contrast attention mechanism. MSCFB is the essential component of our network, which achieves an excellent balance between image quality and model size. In this way, our method can not only brighten up low-light images with rich details and high contrast but also significantly ameliorate the execution speed. Extensive experiments demonstrate that our LPNet outperforms state-of-the-art methods both qualitatively and quantitatively.
C1 [Li, Jiaqian; Li, Juncheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.
   [Li, Jiaqian; Li, Juncheng; Fang, Faming; Zhang, Guixu] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
   [Li, Fang] East China Normal Univ, Shanghai Key Lab PMMP, Shanghai 200241, Peoples R China.
   [Li, Fang] East China Normal Univ, Sch Math Sci, Shanghai 200241, Peoples R China.
C3 East China Normal University; East China Normal University; East China
   Normal University; East China Normal University
RP Fang, FM (corresponding author), East China Normal Univ, Shanghai Key Lab Multidimens Informat Proc, Shanghai 200062, Peoples R China.
EM 51184506021@stu.ecnu.edu.cn; cvjunchengli@gmail.com;
   fmfang@cs.ecnu.edu.cn; fli@math.ecnu.edu.cn; gxzhang@cs.ecnu.edu.cn
RI Li, Juncheng/AHA-3971-2022
OI Li, Fang/0000-0001-6804-2651; Li, Juncheng/0000-0001-7314-6754
FU Key Project of the National Natural Science Foundation of China
   [61731009]; National Natural Science Foundation of China [61871185,
   11671002]; "Chenguang Program" - Shanghai Education Development
   Foundation [17CG25]; Shanghai Municipal Education Commission [17CG25];
   Fundamental Research Funds for the Central Universities; Science and
   Technology Commission of Shanghai Municipality [19 JC1420102,
   18dz2271000]
FX This work was supported in part by the Key Project of the National
   Natural Science Foundation of China under Grant 61731009, in part by the
   National Natural Science Foundation of China under Grant 61871185 and
   11671002, in part by the "Chenguang Program" supported by the Shanghai
   Education Development Foundation and Shanghai Municipal Education
   Commission under Grant 17CG25, and in part by the Fundamental Research
   Funds for the Central Universities, and Science and Technology
   Commission of ShanghaiMunicipality (No. 19 JC1420102, No. 18dz2271000).
CR Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen C, 2019, IEEE I CONF COMP VIS, P3184, DOI 10.1109/ICCV.2019.00328
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Denton Emily, 2015, Advances in Neural Information Processing Systems
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu XY, 2020, IEEE T NEUR NET LEAR, V31, P1794, DOI 10.1109/TNNLS.2019.2926481
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2013, IEEE GLOB CONF SIG, P1085, DOI 10.1109/GlobalSIP.2013.6737082
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Hu J., 2018, P IEEE C COMP VIS PA, P7132
   Hui Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2024, DOI 10.1145/3343031.3351084
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jin Z, 2020, IEEE T MULTIMEDIA, V22, P1055, DOI 10.1109/TMM.2019.2938340
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P451, DOI 10.1109/83.557356
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li JC, 2018, LECT NOTES COMPUT SC, V11212, P527, DOI 10.1007/978-3-030-01237-3_32
   Li MD, 2018, IEEE T IMAGE PROCESS, V27, P2828, DOI 10.1109/TIP.2018.2810539
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   Miao H, 2019, IEEE IJCNN
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   PIZER SM, 1987, COMPUT VISION GRAPH, V39, P355, DOI 10.1016/S0734-189X(87)80186-X
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Tao X, 2018, PROC CVPR IEEE, P8174, DOI 10.1109/CVPR.2018.00853
   Vonikakis V, 2008, IET IMAGE PROCESS, V2, P19, DOI 10.1049/iet-ipr:20070012
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang W, 2013, SIAM J IMAGING SCI, V6, P1823, DOI 10.1137/130909196
   Wang WJ, 2018, IEEE INT CONF AUTOMA, P751, DOI 10.1109/FG.2018.00118
   Wang Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2015, DOI 10.1145/3343031.3350983
   Wei C., 2018, P BRIT MACH VIS C 20
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Yu F., 2015, ARXIV
   Yuanming Hu, 2018, ACM Transactions on Graphics, V37, DOI 10.1145/3181974
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
NR 55
TC 64
Z9 68
U1 6
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3153
EP 3165
DI 10.1109/TMM.2020.3021243
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000016
DA 2024-07-18
ER

PT J
AU Lu, X
   Liu, L
   Nie, LQ
   Chang, XJ
   Zhang, HX
AF Lu, Xu
   Liu, Li
   Nie, Liqiang
   Chang, Xiaojun
   Zhang, Huaxiang
TI Semantic-Driven Interpretable Deep Multi-Modal Hashing for Large-Scale
   Multimedia Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Task analysis; Data models; Feature extraction; Redundancy;
   Fuses; Optimization; Multi-modal hashing; large-scale multimedia;
   retrieval; interpretable hashing
AB Multi-modal hashing focuses on fusing different modalities and exploring the complementarity of heterogeneous multi-modal data for compact hash learning. However, existing multi-modal hashing methods still suffer from several problems, including: 1) Almost all existing methods generate unexplainable hash codes. They roughly assume that the contribution of each hash code bit to the retrieval results is the same, ignoring the discriminative information embedded in hash learning and semantic similarity in hash retrieval. Moreover, the length of hash code is empirically set, which will cause bit redundancy and affect retrieval accuracy. 2) Most existing methods exploit shallow models which fail to fully capture higher-level correlation of multi-modal data. 3) Most existing methods adopt online hashing strategy based on immutable direct projection, which generates query codes for new samples without considering the differences of semantic categories. In this paper, we propose a Semantic-driven Interpretable Deep Multi-modal Hashing (SIDMH) method to generate interpretable hash codes driven by semantic categories within a deep hashing architecture, which can solve all these three problems in an integrated model. The main contributions are: 1) A novel deep multi-modal hashing network is developed to progressively extract hidden representations of heterogeneous modality features and deeply exploit the complementarity of multi-modal data. 2) Learning interpretable hash codes, with discriminant information of different categories distinctively embedded into hash codes and their different impacts on hash retrieval intuitively explained. Besides, the code length depends on the number of categories in the dataset, which can reduce the bit redundancy and improve the retrieval accuracy. 3) The semantic-driven online hashing strategy encodes the significant branches and discards the negligible branches of each query sample according to the semantics contained in it, therefore it could capture different semantics in dynamic queries. Finally, we consider both the nearest neighbor similarity and semantic similarity of hash codes. Experiments on several public multimedia retrieval datasets validate the superiority of the proposed method.
C1 [Lu, Xu; Liu, Li; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Chang, Xiaojun] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.
C3 Shandong Normal University; Shandong University; Monash University
RP Zhang, HX (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM lxuu306@hotmail.com; liuli_790209@163.com; nieliqiang@gmail.com;
   cxj273@gmail.com; huaxzhang@hotmail.com
RI Chang, Xiaojun/A-2055-2015
OI Chang, Xiaojun/0000-0002-7778-8807; zhang, hua
   xiang/0000-0001-6259-7533; Lu, Xu/0000-0002-8459-3186
FU National Natural Science Foundation of China [61772322, U1836216,
   62076153]; Major Fundamental Research Project of Shandong, China
   [ZR2019ZD03]; Taishan Scholar Project of Shandong, China [ts20190924]
FX The work was supported in part by the National Natural Science
   Foundation of China under Grants 61772322, U1836216, and 62076153, in
   part by the Major Fundamental Research Project of Shandong, China under
   Grant ZR2019ZD03, and in part by the Taishan Scholar Project of
   Shandong, China under Grant ts20190924.
CR Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chen ZX, 2016, LECT NOTES ELECTR EN, V360, P125, DOI 10.1007/978-3-662-48365-7_13
   Chen ZH, 2018, 2018 IEEE 2ND ELECTRON DEVICES TECHNOLOGY AND MANUFACTURING CONFERENCE (EDTM 2018), P274, DOI 10.1109/EDTM.2018.8421500
   Cheng ZY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3291060
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Guo J, 2020, IEEE T IMAGE PROCESS, V29, P1344, DOI 10.1109/TIP.2019.2941858
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Huiskes M., 2010, Proceedings of the international conference on Multimedia information retrieval
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Kang Y, 2012, IEEE DATA MINING, P930, DOI 10.1109/ICDM.2012.24
   Kim S, 2013, INT CONF ACOUST SPEE, P3123, DOI 10.1109/ICASSP.2013.6638233
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2020, IEEE T PATTERN ANAL, V42, P580, DOI 10.1109/TPAMI.2018.2882816
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu X., 2012, PROC ACM MULTIMEDIA, P881
   Lu X, 2020, IEEE T MULTIMEDIA, V22, P2048, DOI 10.1109/TMM.2019.2947358
   Lu X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1129, DOI 10.1145/3343031.3350999
   Lu X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P715, DOI 10.1145/3331184.3331217
   Luo X, 2019, IEEE T IMAGE PROCESS, V28, P2962, DOI 10.1109/TIP.2019.2892703
   Murty G. Katta, 2007, Technometrics, V49, P105
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Do TT, 2020, IEEE T MULTIMEDIA, V22, P992, DOI 10.1109/TMM.2019.2935680
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Yang R, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P180, DOI 10.1145/3078971.3078981
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang CH, 2017, IEEE T IMAGE PROCESS, V26, P2604, DOI 10.1109/TIP.2017.2675205
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zheng CQ, 2020, IEEE SIGNAL PROC LET, V27, P1270, DOI 10.1109/LSP.2020.3008335
   Zheng CQ, 2020, IEEE T KNOWL DATA EN, V32, P2171, DOI 10.1109/TKDE.2019.2913388
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 39
TC 21
Z9 23
U1 2
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4541
EP 4554
DI 10.1109/TMM.2020.3044473
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800012
DA 2024-07-18
ER

PT J
AU Pei, EC
   Oveneke, MC
   Zhao, Y
   Jiang, DM
   Sahli, H
AF Pei, Ercheng
   Oveneke, Meshia Cedric
   Zhao, Yong
   Jiang, Dongmei
   Sahli, Hichem
TI Monocular 3D Facial Expression Features for Continuous Affect
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Three-dimensional displays; Feature extraction; Solid
   modeling; Emotion recognition; Optical imaging; Strain; 3D morphable
   model; 3D scene flow; continuous affect recognition
ID FACE RECOGNITION; MOTION; IMAGE; MODEL
AB Automated facial expression analysis from image sequences for continuous emotion recognition is a very challenging task due to the loss of the three-dimensional information during the image formation process. State-of-the-art relied on estimating dynamic textures features and convolutional neural network features to derive spatio-temporal features. Despite their great success, such features are insensitive to micro facial muscle deformations and are affected by identity, face pose, illumination variation, and self-occlusion. In this work, we argue that retrieving, from image sequences, 3D facial spatio-temporal information, which describes the natural facial muscle deformation, provides a semantical and efficient way of representation and is useful for emotion recognition. In this paper, we propose a framework for extracting three-dimensional facial spatio-temporal features from monocular image sequences using an extended 3D Morphable Model (3DMM) which disentangles the identity factor from the facial expressions of a specific person. An LSTM model is used to evaluate the effectiveness of the proposed spatio-temporal features on video-based facial expression recognition task and continuous affect recognition task. Experimental results, on the AFEW6.0 datasets for facial expression recognition, and the RECOLA and SEMAINE datasets for continuous emotion prediction, illustrate the potential of the proposed 3D spatio-temporal features for facial expressions analysis and continuous affect recognition, as well as their efficiency compared to recent state-of-the-art features.
C1 [Pei, Ercheng; Zhao, Yong; Jiang, Dongmei] Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big, Shaanxi Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab,Sch Comp Sci, Youyi Xilu 127, Xian 710072, Peoples R China.
   [Pei, Ercheng; Zhao, Yong] Vrije Univ Brussel VUB, Dept ETRO, Pl Laan 2, B-1050 Brussels, Belgium.
   [Oveneke, Meshia Cedric; Sahli, Hichem] Vrije Univ Brussel, Dept ETRO, Pl Laan 2, B-1050 Brussels, Belgium.
   [Jiang, Dongmei] Peng ChengLab, Shenzhen 518055, Guangdong, Peoples R China.
   [Sahli, Hichem] Interuniv Microelect Ctr IMEC, Kapeldreef 75, B-3001 Heverlee, Belgium.
C3 Northwestern Polytechnical University; Vrije Universiteit Brussel; Vrije
   Universiteit Brussel; IMEC
RP Jiang, DM (corresponding author), Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big, Shaanxi Key Lab Speech & Image Informat Proc, NPU VUB Joint AVSP Res Lab,Sch Comp Sci, Youyi Xilu 127, Xian 710072, Peoples R China.
EM peiercheng@mail.nwpu.edu.cn; mcovenek@etrovub.be; yzhao@etrovub.be;
   jiangdm@nwpu.edu.cn; hsahli@etrovub.be
OI Jiang, Dongmei/0000-0002-6238-8499; Sahli, Hichem/0000-0002-1774-2970;
   ZHAO, YONG/0000-0003-2644-952X
FU Chinese Scholarship Council [201706290115]; Shaanxi Provincial
   International Science and Technology Collaboration Project
   [2017KW-ZD-14]
FX This work was supported in part by the Chinese Scholarship Council under
   Grant 201706290115 and in part by the Shaanxi Provincial International
   Science and Technology Collaboration Project under Grant 2017KW-ZD-14.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sen-Ching Samson Cheung.
CR Ahonen T, 2004, LECT NOTES COMPUT SC, V3021, P469
   Almaev TR, 2013, INT CONF AFFECT, P356, DOI 10.1109/ACII.2013.65
   An ZF, 2018, IEEE INT CONF AUTOMA, P416, DOI 10.1109/FG.2018.00067
   [Anonymous], 2016, IJCAI
   [Anonymous], 2016, ARXIV161109232
   Baltrusaitis T, 2018, IEEE INT CONF AUTOMA, P59, DOI 10.1109/FG.2018.00019
   Barros P, 2015, NEURAL NETWORKS, V72, P140, DOI 10.1016/j.neunet.2015.09.009
   Bejaoui H, 2017, LECT NOTES COMPUT SC, V10617, P39, DOI 10.1007/978-3-319-70353-4_4
   Berretti S, 2019, IEEE T NEUR NET LEAR
   Blanz V, 2003, IEEE T PATTERN ANAL, V25, P1063, DOI 10.1109/TPAMI.2003.1227983
   Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Brady K, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P97, DOI 10.1145/2988257.2988264
   Bruckstein AM, 1999, INT J COMPUT VISION, V35, P223, DOI 10.1023/A:1008156210387
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chang FJ, 2019, INT J COMPUT VISION, V127, P930, DOI 10.1007/s11263-019-01151-x
   Chao L, 2015, P 5 INT WORKSH AUD V, P65, DOI [DOI 10.1145/2808196.2811634, 10.1145/2808196.2811634]
   Chen H, 2015, PROC CVPR IEEE, P1836, DOI 10.1109/CVPR.2015.7298793
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19
   Chu B, 2014, PROC CVPR IEEE, P1907, DOI 10.1109/CVPR.2014.245
   Deshmukh S, 2016, IET BIOMETRICS, V5, P155, DOI 10.1049/iet-bmt.2014.0104
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Ding H, 2017, IEEE INT CONF AUTOMA, P118, DOI 10.1109/FG.2017.23
   Ding W, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P506, DOI 10.1145/2993148.2997637
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Ghazi MM, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P514, DOI 10.1145/2993148.2997634
   Happy SL, 2019, IEEE T AFFECT COMPUT, V10, P394, DOI 10.1109/TAFFC.2017.2723386
   Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282
   He L, 2019, IEEE T MULTIMEDIA, V21, P1476, DOI 10.1109/TMM.2018.2877129
   He Lang, 2015, P 5 INT WORKSH AUD V
   Hou YS, 2011, SIGNAL PROCESS-IMAGE, V26, P550, DOI 10.1016/j.image.2011.05.003
   Hu M, 2019, J VIS COMMUN IMAGE R, V59, P176, DOI 10.1016/j.jvcir.2018.12.039
   Huang D, 2011, IEEE T SYST MAN CY C, V41, P765, DOI 10.1109/TSMCC.2011.2118750
   Huang J, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P6837, DOI 10.1109/ICASSP.2018.8461963
   Huang XH, 2016, NEUROCOMPUTING, V175, P564, DOI 10.1016/j.neucom.2015.10.096
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   K_achele M., 2015, AVEC 2015-Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge, co-Located with MM 2015, P9, DOI [DOI 10.1145/2808196.2811637, 10.1145/2808196.2811637]
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li Shan, 2018, arXiv:1804.08348
   Liu L, 2016, IEEE T IMAGE PROCESS, V25, P1368, DOI 10.1109/TIP.2016.2522378
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   Nicolle J, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P501
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Oveneke M. C, 2019, IEEE T AFFECTIVE COM
   Oveneke MC, 2015, INT CONF AFFECT, P623, DOI 10.1109/ACII.2015.7344634
   Patil G, 2017, PROCEEDINGS OF THE 2017 INTERNATIONAL CONFERENCE ON SMART TECHNOLOGIES FOR SMART NATION (SMARTTECHCON), P825, DOI 10.1109/SmartTechCon.2017.8358488
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   PEI E, 2019, MULTIMED TOOLS APPL, P1
   Pei EC, 2015, INT CONF AFFECT, P208, DOI 10.1109/ACII.2015.7344573
   Prenter P., 2008, Splines and variational methods
   Kassim SRA, 2006, IEEE IMAGE PROC, P661
   Ravyse I, 2008, INT CONF ACOUST SPEE, P1089, DOI 10.1109/ICASSP.2008.4517803
   Rifai S., 2011, CONTRACTIVE AUTOENCO, DOI DOI 10.5555/3104482.3104587
   Ringeval F, 2013, IEEE INT CONF AUTOMA
   Ringeval F, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1335, DOI 10.1145/2733373.2806408
   Ringeval F, 2015, PATTERN RECOGN LETT, V66, P22, DOI 10.1016/j.patrec.2014.11.007
   Rouast PV, 2021, IEEE T AFFECT COMPUT, V12, P524, DOI 10.1109/TAFFC.2018.2890471
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Schuller B, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P449
   Song SY, 2019, IEEE INT CONF COMP V, P1608, DOI 10.1109/ICCVW.2019.00200
   Sun B, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P83, DOI 10.1145/2988257.2988270
   Tautkute I, 2018, IEEE COMPUT SOC CONF, P1959, DOI 10.1109/CVPRW.2018.00246
   Tran Long Q., 2022, International Journal of Electrical Engineering Education, V59, P158, DOI 10.1177/0020720919852784
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Vedula S, 2005, IEEE T PATTERN ANAL, V27, P475, DOI 10.1109/TPAMI.2005.63
   WANG SF, 2008, LNCS, P589
   Wen LY, 2015, IEEE T INF FOREN SEC, V10, P1432, DOI 10.1109/TIFS.2015.2414392
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Yan JW, 2018, NEUROCOMPUTING, V309, P27, DOI 10.1016/j.neucom.2018.03.068
   Zadeh A, 2017, IEEE INT CONF COMP V, P2519, DOI 10.1109/ICCVW.2017.296
   Zeiler M. D., 2012, CoRR
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
   Zhang Y, 2019, IEEE T MULTIMEDIA, V21, P617, DOI 10.1109/TMM.2018.2882744
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhao JF, 2018, VISUAL COMPUT, V34, P1461, DOI 10.1007/s00371-018-1477-y
   Zhao XY, 2016, LECT NOTES COMPUT SC, V9906, P425, DOI 10.1007/978-3-319-46475-6_27
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhu GY, 2007, IEEE T MULTIMEDIA, V9, P1167, DOI 10.1109/TMM.2007.902847
   Zhu XY, 2015, PROC CVPR IEEE, P787, DOI 10.1109/CVPR.2015.7298679
NR 85
TC 5
Z9 5
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3540
EP 3550
DI 10.1109/TMM.2020.3026894
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100009
DA 2024-07-18
ER

PT J
AU Sung, J
   Lee, D
AF Sung, Jihoon
   Lee, Dujeong
TI Efficient Design and Control for Network-Assisted Device-to-Device
   Content Delivery Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Device-to-device communication; Servers; Mobile handsets; Routing;
   Lyapunov methods; Optimization; Scheduling; Network-assisted content
   delivery network; device-to-device communication; design and control;
   cross-layer
ID WIRELESS; PLACEMENT; CACHE
AB Proliferation of mobile devices over the past few years has explosively increased demand for multimedia content in various service scenarios such as video and game streaming. This growth in demand for mobile content has brought a significant challenge of how to efficiently deliver such content to users. Content delivery networks (CDNs) have grown in popularity as the most feasible solution to overcome this challenge. Many mobile network operators have tried to develop customized CDN solutions suited to their network systems in practice. In this paper, we focus on an intriguing paradigm where mobile devices are used as cache servers and localized content sharing is activated via device-to-device (D2D) communication. As the most feasible network system, we consider a network-assisted D2D CDN where a mobile network assists and controls D2D communication processes. We first investigate inherent characteristics of the network system, resulting in the following findings: 1) D2D communication utilization and 2) network assisted control. As a new approach to devise efficient CDN design and control algorithms that simultaneously consider the inherent characteristics, we adopt the Lyapunov stability theory. As a result, we can derive efficient algorithms including content placement, content request routing, and content scheduling as a total solution including CDN design as well as control based on a cross-layer design approach. Through packet simulations, it is demonstrated that the performance yielded by the proposed strategies is superior to that delivered by other strategies in terms of delay to directly represent content delivery performance.
C1 [Sung, Jihoon] Samsung Elect, Mobile Commun Div, Suwon 16677, South Korea.
   [Lee, Dujeong] Samsung Elect, Syst LSI Div, Hwaseong 18448, South Korea.
C3 Samsung Electronics; Samsung; Samsung Electronics; Samsung
RP Lee, D (corresponding author), Samsung Elect, Syst LSI Div, Hwaseong 18448, South Korea.
EM sung.jh@kaist.ac.kr; dj.lee@kaist.ac.kr
OI Sung, Jihoon/0000-0003-0041-3238
CR Akhtar N, 2015, IEEE T VEH TECHNOL, V64, P248, DOI 10.1109/TVT.2014.2319107
   Amble MM, 2011, IEEE INFOCOM SER, P2858, DOI 10.1109/INFCOM.2011.5935123
   [Anonymous], 2006, RESOURCE ALLOCATION
   Ao WC, 2018, IEEE T MOBILE COMPUT, V17, P1048, DOI 10.1109/TMC.2017.2750143
   Ceselli A, 2017, IEEE ACM T NETWORK, V25, P1818, DOI 10.1109/TNET.2017.2652850
   CVNI Cisco, 2019, CISC VIS NETW IND GL
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Golrezaei N., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2781, DOI 10.1109/ISIT.2012.6284029
   Golrezaei N, 2013, IEEE COMMUN MAG, V51, P142, DOI 10.1109/MCOM.2013.6495773
   GSMA, 2019, The 5G guide: A reference for operators
   Gupta P, 2000, IEEE T INFORM THEORY, V46, P388, DOI 10.1109/18.825799
   Güven C, 2017, 2017 IEEE 28TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC.2017.8292555
   He ZF, 2016, IEEE T WIREL COMMUN, V15, P728, DOI 10.1109/TWC.2015.2477509
   Hou TT, 2018, INT J COMMUN SYST, V31, DOI 10.1002/dac.3706
   Ji MY, 2016, IEEE J SEL AREA COMM, V34, P176, DOI 10.1109/JSAC.2015.2452672
   Lee MC, 2018, IEEE T WIREL COMMUN, V17, P7500, DOI 10.1109/TWC.2018.2867596
   Li LY, 2018, IEEE COMMUN SURV TUT, V20, P1710, DOI 10.1109/COMST.2018.2820021
   Luo Yuchong, 2019, 2019 IEEE 21st International Conference on High Performance Computing and Communications; IEEE 17th International Conference on Smart City; IEEE 5th International Conference on Data Science and Systems (HPCC/SmartCity/DSS). Proceedings, P2379, DOI 10.1109/HPCC/SmartCity/DSS.2019.00331
   Seet B.C., 2018, RECENT ADV CELLULAR RECENT ADV CELLULAR
   Sengupta A, 2017, IEEE T COMMUN, V65, P1940, DOI 10.1109/TCOMM.2017.2664815
   SK Telecom, 2014, SK Telecom 5G White Paper
   Sung J, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P547, DOI [10.1109/ICMLA.2016.0096, 10.1109/ICMLA.2016.85]
   Sung J, 2016, IEEE T MULTIMEDIA, V18, P1163, DOI 10.1109/TMM.2016.2543658
   Tang GM, 2019, IEEE ACM T NETWORK, V27, P98, DOI 10.1109/TNET.2018.2881169
   Vu TX, 2018, IEEE T WIREL COMMUN, V17, P2827, DOI 10.1109/TWC.2018.2803816
   Wang X., 2012, CISC VIS NETW IND GL
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Williams H.P., 2013, Model Building in Mathematical Programming
   Williamson D. P., 2011, DESIGN APPROXIMATION, DOI DOI 10.1017/CBO9780511921735
   Xu YL, 2017, IEEE T MULTIMEDIA, V19, P2597, DOI 10.1109/TMM.2017.2700208
   Yang CX, 2014, ADV INTEL SYS RES, V59, P1
   Zhang L, 2016, IEEE T COMMUN, V64, P2438, DOI 10.1109/TCOMM.2016.2552164
   Zhu H, 2019, IEEE MULTIMEDIA, V26, P10, DOI 10.1109/MMUL.2018.2879589
NR 33
TC 2
Z9 2
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2442
EP 2456
DI 10.1109/TMM.2020.3011330
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800022
DA 2024-07-18
ER

PT J
AU Wu, J
   Chen, TS
   Wu, HF
   Yang, Z
   Luo, GC
   Lin, L
AF Wu, Jie
   Chen, Tianshui
   Wu, Hefeng
   Yang, Zhi
   Luo, Guangchun
   Lin, Liang
TI Fine-Grained Image Captioning With Global-Local Discriminative Objective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Visualization; Task analysis; Semantics; Reinforcement
   learning; Pipelines; Maximum likelihood estimation; Image captioning;
   Fine-grained captions; Global discriminative constraint; Local
   discriminative constraint; Self-retrieval
ID TEXT
AB Significant progress has been made in recent years in image captioning, an active topic in the fields of vision and language. However, existing methods tend to yield overly general captions and consist of some of the most frequent words/phrases, resulting in inaccurate and indistinguishable descriptions (see Fig. 1). This is primarily due to (i) the conservative characteristic of traditional training objectives that drives the model to generate correct but hardly discriminative captions for similar images and (ii) the uneven word distribution of the ground-truth captions, which encourages generating highly frequent words/phrases while suppressing the less frequent but more concrete ones. In this work, we propose a novel global-local discriminative objective that is formulated on top of a reference model to facilitate generating fine-grained descriptive captions. Specifically, from a global perspective, we design a novel global discriminative constraint that pulls the generated sentence to better discern the corresponding image from all others in the entire dataset. From the local perspective, a local discriminative constraint is proposed to increase attention such that it emphasizes the less frequent but more concrete words/phrases, thus facilitating the generation of captions that better describe the visual details of the given images. We evaluate the proposed method on the widely used MS-COCO dataset, where it outperforms the baseline methods by a sizable margin and achieves competitive performance over existing leading approaches. We also conduct self-retrieval experiments to demonstrate the discriminability of the proposed method.
C1 [Wu, Jie] Sun Yat Sen Univ, Sch Elect & Informat Engn, Guangzhou 515000, Peoples R China.
   [Wu, Hefeng] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 515000, Peoples R China.
   [Yang, Zhi] Sun Yat Sen Univ, Guangzhou 515000, Peoples R China.
   [Chen, Tianshui; Lin, Liang] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
   [Chen, Tianshui; Lin, Liang] Dark Matter Res, Guangzhou 510006, Peoples R China.
   [Luo, Guangchun] Univ Elect Sci & Technol China, Chengdu 610051, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University;
   Sun Yat Sen University; University of Electronic Science & Technology of
   China
RP Chen, TS (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.; Chen, TS (corresponding author), Dark Matter Res, Guangzhou 510006, Peoples R China.
EM wujie23@mail2.sysu.edu.cn; tianshuichen@gmail.com; wuhefeng@gmail.com;
   issyz@mail.sysu.edu.cn; gcluo@uestc.edu.cn; linliang@ieee.org
RI l, j/JVZ-8480-2024; Li, Jiaxi/HTS-3430-2023; Lin, L/HKO-8213-2023; Li,
   Li/IAQ-0885-2023; zhang, cl/JDW-6549-2023; LU, LU/JEZ-4760-2023; l,
   j/HNC-5728-2023; Lin, Liang/IQR-8601-2023; L, J/JEF-9564-2023
OI Li, Jiaxi/0000-0002-8197-8590; Lin, Liang/0000-0003-2248-3755; Chen,
   Tianshui/0000-0002-5848-5624; Wu, Hefeng/0000-0002-2132-6515
FU National Key Research and Development Program of China [2018YFC0830103];
   National Natural Science Foundation of China (NSFC) [61876045,
   U1811463]; National High Level Talents Special Support Plan (Ten
   Thousand Talents Program); Natural Science Foundation of Guangdong
   Province [2017A030312006]; Zhujiang Science and Technology New Star
   Project of Guangzhou [201906010057]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFC0830103, in part by
   National Natural Science Foundation of China (NSFC) under Grants
   61876045 and U1811463, in part by National High Level Talents Special
   Support Plan (Ten Thousand Talents Program), in part by the Natural
   Science Foundation of Guangdong Province under Grant 2017A030312006, and
   in part by Zhujiang Science and Technology New Star Project of Guangzhou
   under Grant 201906010057. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. YingLi Tian.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2017, P IEEE INT C COMP VI
   [Anonymous], 2017, ARXIV170705612
   [Anonymous], 2015, arXiv
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Bengio Y., 2014, TECHNICAL REPORT
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen SZ, 2016, IEEE T IMAGE PROCESS, V25, P2353, DOI 10.1109/TIP.2016.2545929
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chen TS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P2023, DOI 10.1145/3240508.3240523
   Chen TS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P627
   Chen TS, 2018, AAAI CONF ARTIF INTE, P6730
   Chen TS, 2016, IEEE T NEUR NET LEAR, V27, P1135, DOI 10.1109/TNNLS.2015.2506664
   Chen X, 2015, Microsoft coco captions: Data collection and evaluation server
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Dai B, 2017, ADV NEUR IN, V30
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Diba A., 2017, ARXIV171108174
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Gao J., 2019, P IEEE C COMP VIS PA
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li Y., 2018, EUR C COMP VIS ECCV, P684
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu X, 2018, ARXIV180308314
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mao Junhua, 2014, CoRR
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Robertson S, 2004, J DOC, V60, P503, DOI 10.1108/00220410410560582
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saquil Y., 2018, P BRIT MACH VIS C
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vedantam Ramakrishna, 2017, CVPR
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xiong W, 2018, PROC CVPR IEEE, P2364, DOI 10.1109/CVPR.2018.00251
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xue HY, 2018, IEEE T IMAGE PROCESS, V27, P5563, DOI 10.1109/TIP.2018.2859820
   Xue HY, 2017, IEEE T IMAGE PROCESS, V26, P5656, DOI 10.1109/TIP.2017.2746267
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2360, DOI 10.1109/TMM.2018.2807588
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang L, 2017, ARXIV PREPRINT ARXIV
   Zhang MX, 2019, IEEE T IMAGE PROCESS, V28, P32, DOI 10.1109/TIP.2018.2855415
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhu YS, 2019, IEEE T IMAGE PROCESS, V28, P113, DOI 10.1109/TIP.2018.2865280
NR 74
TC 41
Z9 42
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2413
EP 2427
DI 10.1109/TMM.2020.3011317
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, SQ
   Zhao, X
   Fang, LJ
AF Zhang, Shiquan
   Zhao, Xu
   Fang, Liangji
TI CAT: Corner Aided Tracking With Deep Regression Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Shape; Estimation; Cats; Strain; Reliability; Training;
   Corner aided tracker; deep regression tracking; visual object tracking
ID OBJECT TRACKING
AB Single object tracking in visual media is an important yet challenging task. Various challenges, especially target scale variation, shape deformation and occlusion, can have large effects on the performances of trackers. Current deep regression based trackers only pay close attention to regression on the center key point of the tracking target, meanwhile employ the image pyramid based multi-scale testing method to deal with scale estimation. Such procedure can not properly handle the three challenges. We address these challenges in a principled way by the aid of auxiliary regressions on the four bounding box corners of the tracking target. In this work, we propose the novel Corner Aided Tracker with deep regression network, abbreviated as CAT. Different from RPN-based trackers, in CAT, four corners along with the center key point of the bounding box for tracking target are simultaneously obtained by five corresponding response maps. Furthermore, to robustly and accurately generate tight bounding boxes for the tracking target and collect reliable samples for online training of the network, we propose an adaptive key point selection method to select the subset of reliable key points and drop the unreliable ones, based on the qualities of their corresponding response maps as well as the constraints from shape, scale and location. We demonstrate that the regressed corners can help naturally locate the tracking target with tight bounding boxes. The challenges of scale variation, shape deformation and occlusion can be handled explicitly. The commonly used time-consuming image pyramid based multi-scale testing method can also be discarded. Extensive experiments on OTB2013, OTB2015, UAV123, LaSOT, VOT2016 and VOT2018 datasets are conducted to report new state-of-the-art performances and demonstrate the effectiveness of CAT.
C1 [Zhang, Shiquan; Zhao, Xu; Fang, Liangji] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
   [Zhao, Xu] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Zhao, X (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
EM sq.zhang@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; fangliangji@sjtu.edu.cn
FU NSFC [61673269, U1764264, 61273285]; Institute of Medical Robotics at
   Shanghai Jiao Tong University
FX This work was supported in part NSFC under Grants 61673269, U1764264,
   and 61273285 and in part by the project funding of Institute of Medical
   Robotics at Shanghai Jiao Tong University.
CR Andriluka M, 2018, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR.2018.00542
   [Anonymous], 2018, CoRR
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Choi J, 2016, PROC CVPR IEEE, P4321, DOI 10.1109/CVPR.2016.468
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fan H., 2018, ARXIV181206148
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao JY, 2019, IEEE T IMAGE PROCESS, V28, P3923, DOI 10.1109/TIP.2019.2904434
   Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Han B, 2017, PROC CVPR IEEE, P521, DOI 10.1109/CVPR.2017.63
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Kingma D. P., 2014, arXiv
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li HX, 2016, IEEE T IMAGE PROCESS, V25, P1834, DOI 10.1109/TIP.2015.2510583
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu C, 2018, IEEE T MULTIMEDIA, V20, P889, DOI 10.1109/TMM.2017.2760633
   Liu WX, 2019, IEEE T IMAGE PROCESS, V28, P3766, DOI 10.1109/TIP.2019.2902784
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35
   Pu S., 2018, SYSTEMS, P1935
   Real E, 2017, PROC CVPR IEEE, P7464, DOI 10.1109/CVPR.2017.789
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Sun X, 2017, IEEE I CONF COMP VIS, P5496, DOI 10.1109/ICCV.2017.586
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang Q., 2018, ARXIV181205050
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Yang W, 2018, PROC CVPR IEEE, P5255, DOI 10.1109/CVPR.2018.00551
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang P., 2019, IEEE T MULTIMEDIA, P1
   Zhipeng Z., 2019, ARXIV190101660
   Zhu Z., 2018, 2018 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2018.8508710
   Zhu Z, 2018, PROC CVPR IEEE, P548, DOI 10.1109/CVPR.2018.00064
   Zhu Z, 2017, IEEE INT CONF COMP V, P1973, DOI 10.1109/ICCVW.2017.231
NR 80
TC 6
Z9 6
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 859
EP 870
DI 10.1109/TMM.2020.2990089
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300003
DA 2024-07-18
ER

PT J
AU Eltobgy, O
   Arafa, O
   Hefeeda, M
AF Eltobgy, Omar
   Arafa, Omar
   Hefeeda, Mohamed
TI Mobile Streaming of Live 360-Degree Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Long Term Evolution; Wireless communication; Logic gates;
   Streaming media; Base stations; Cellular networks; Mobile multimedia;
   360-degree video; adaptive streaming; multicast
AB Live streaming of immersive multimedia content, e.g., 360-degree videos, is getting popular due to the recent availability of commercial devices that support interacting with such content such as smartphones/tablets and head-mounted displays. Streaming live content to mobile users using individual connections (i.e., unicast) consumes substantial network resources and does not scale to large number of users. Multicast, on the other hand, offers a scalable solution but it introduces multiple challenges, including handling user interactivity, ensuring smooth quality, conserving the energy of mobile receivers, and achieving fairness among users. We propose a new solution for the problem of live multicast streaming of 360-degree videos to mobile users, which addresses the aforementioned challenges. The proposed solution, referred to as VRCast, is designed for cellular networks that support multicast, such as LTE. We show through trace-driven simulations that VRCast outperforms the closest algorithms in the literature by wide margins across several performance metrics. For example, compared to the state-of-the-art, VRCast improves the viewport quality by up to 2.5 dB. We have implemented VRCast in an LTE testbed to show its practicality. Our experimental results show that VRCast ensures smooth video quality and saves energy for mobile devices.
C1 [Eltobgy, Omar; Arafa, Omar; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hefeeda, M (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
EM oeltobgy@sfu.ca; oarafa@sfu.ca; mhefeeda@cs.sfu.ca
OI Hefeeda, Mohamed/0000-0003-3261-4376
FU Natural Sciences and Engineering Research Council (NSERC) of Canada
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   J. Wu.
CR 3GPP, 2017, 36331 3GPP TS EUTRA
   3GPP, 2017, 36211 3GPP TS EUTRA
   3GPP, 2017, ENH TEL SERV 3GPP EM
   3GPP, 2017, 36104 3GPP TS EUTRA
   3GPP, 2017, 26346 3GPP TS
   Ahmadi H, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P170, DOI 10.1145/3126686.3126743
   Almowuena S, 2016, IEEE T MULTIMEDIA, V18, P102, DOI 10.1109/TMM.2015.2502067
   Amarisoft, 2018, AM FULL LTE SOFTW SU
   [Anonymous], 2017, CISC VIS NETW IND GL
   [Anonymous], 2017, P 27 WORKSHOP NETWOR
   [Anonymous], 2012, VISUAL COMMUNICATION
   [Anonymous], 2016, P INT C MULT SYST KL
   Bao YN, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1161, DOI 10.1109/BigData.2016.7840720
   Chen H, 2020, IEEE T MULTIMEDIA, V22, P459, DOI 10.1109/TMM.2019.2928497
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Ericsson, 2017, GIG NETW LAUNCH CHIN
   Expway, 2017, EXPW BROADC MULT SER
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Graf M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P261, DOI 10.1145/3083187.3084016
   He J, 2018, MOBISYS'18: PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P482, DOI 10.1145/3210240.3210323
   Jiasi Chen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1266, DOI 10.1109/INFOCOM.2015.7218502
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Lecompte D, 2012, IEEE COMMUN MAG, V50, P68, DOI 10.1109/MCOM.2012.6353684
   Lee K, 2009, IEEE INFOCOM SER, P855, DOI 10.1109/INFCOM.2009.5061995
   Lentisco CM, 2017, IEEE T MULTIMEDIA, V19, P173, DOI 10.1109/TMM.2016.2620605
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Nguyen ND, 2013, PROCEEDINGS OF THE 2013 38TH ANNUAL IEEE CONFERENCE ON LOCAL COMPUTER NETWORKS WORKSHOPS (LCN WORKSHOPS), P70, DOI 10.1109/LCNW.2013.6758500
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Sharangi S, 2011, IEEE T MULTIMEDIA, V13, P102, DOI 10.1109/TMM.2010.2076799
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   Tofallis Chris, 2014, INFORMS Transactions on Education, V14, P109, DOI 10.1287/ited.2013.0124
   Verizon, 2014, VER DEL LTE MULT COM
   Viitanen M, 2015, IEEE INT SYMP CIRC S, P1662, DOI 10.1109/ISCAS.2015.7168970
   Walsh R., 2012, 6726 IETF RFC
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xie XF, 2017, MOBISYS'17: PROCEEDINGS OF THE 15TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P427, DOI 10.1145/3081333.3081367
   Xie XF, 2015, MOBICOM '15: PROCEEDINGS OF THE 21ST ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P413, DOI 10.1145/2789168.2790118
   Yu YJ, 2012, IEEE T MOBILE COMPUT, V11, P1508, DOI 10.1109/TMC.2011.186
   Zare A., 2016, P 24 ACM INT C MULT, P170
   Zhou C, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P27, DOI 10.1145/3083187.3083190
NR 44
TC 20
Z9 20
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3139
EP 3152
DI 10.1109/TMM.2020.2973855
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700010
DA 2024-07-18
ER

PT J
AU Liu, HT
   Xu, YH
   Zhang, JL
   Zhu, JK
   Li, Y
   Hoi, SCH
AF Liu, Hantang
   Xu, Yinghao
   Zhang, Jialiang
   Zhu, Jianke
   Li, Yang
   Hoi, Steven C. H.
TI DeepFacade: A Deep Learning Approach to Facade Parsing With Symmetric
   Loss
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Windows; Microsoft Windows; Grammar; Deep learning; Semantics;
   Facade parsing; deep learning; semantic segmentation
ID SEGMENTATION
AB Parsing building facades into procedural grammars plays an important role for 3D building model generation tasks, which have been long desired in computer vision. Deep learning is a promising approach to facade parsing, however, a straightforward solution by directly applying standard deep learning approaches cannot always yield the optimal results. This is primarily due to two reasons: 1) it is nontrivial to train existing semantic segmentation networks for facade parsing, e.g., Fully-Convolutional Neural Networks (FCN) which are usually weak at predicting fine-grained shapes (J. Long et al., 2015); and 2) building facades are man-made architectures with highly regularized shape priors, and the prior knowledge plays an important role in facade parsing, for which how to integrate the prior knowledge into deep neural networks remains an open problem. In this paper, we present a novel symmetric loss function that can be used in deep neural networks for end-to-end training. This novel loss is based on the assumption that most of windows and doors have a highly symmetric rectangle shape, and it penalizes all window predictions that are non-rectangles. This prior knowledge is smoothly integrated into the end-to-end training process. Quantitative evaluation demonstrates that our method has outperformed previous state-of-art methods significantly on five popular facade parsing datasets. Qualitative results have shown that our method effectively aids deep convolutional neural networks to predict more accurate, visually pleasing, and symmetric shapes. To the best of our knowledge, we are the first to incorporate symmetry constraint into end-to-end training in deep neural networks for facade parsing.
C1 [Liu, Hantang; Xu, Yinghao; Zhang, Jialiang; Zhu, Jianke; Li, Yang] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Zhu, Jianke] Alibaba Zhejiang Univ Joint Res Inst Frontier Tec, Hangzhou 310027, Peoples R China.
   [Hoi, Steven C. H.] Singapore Management Univ, Singapore 178902, Singapore.
C3 Zhejiang University; Singapore Management University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM liuhantang@zju.edu.cn; 3150103924@zju.edu.cn; zjialiang@zju.edu.cn;
   jkzhu@zju.edu.cn; liyang89@zju.edu.cn; chhoi@smu.edu.sg
RI HOI, Steven C. H./A-3736-2011
OI Li, Yang/0000-0001-9427-7665; Zhang, Jialiang/0000-0001-5085-3771; Zhu,
   Jianke/0000-0003-1831-0106
FU National Natural Science Foundation of China [61831015]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61831015. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr Jianfei Cai.
CR Abdulnabi AH, 2018, IEEE T MULTIMEDIA, V20, P1656, DOI 10.1109/TMM.2017.2774007
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen T, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508378
   Cohen A, 2017, INT CONF 3D VISION, P393, DOI 10.1109/3DV.2017.00052
   Cohen A, 2014, PROC CVPR IEEE, P3206, DOI 10.1109/CVPR.2014.410
   Debevec P. E., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P11, DOI 10.1145/237170.237191
   Femiani J., 2018, CORR
   Gadde Raghudeep, 2018, IEEE Trans Pattern Anal Mach Intell, V40, P1273, DOI 10.1109/TPAMI.2017.2696526
   Gadde R, 2016, INT J COMPUT VISION, V117, P290, DOI 10.1007/s11263-016-0887-4
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han F, 2009, IEEE T PATTERN ANAL, V31, P59, DOI [10.1109/TPAMI.2008.65, 10.1109/TPAMI.2008.55]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jiang HY, 2016, GRAPH MODELS, V85, P11, DOI 10.1016/j.gmod.2016.01.003
   Jiang NJ, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618459
   Korc F., 2009, etrims image database for interpreting images of man-made scenes
   Koutsourakis P, 2009, IEEE I CONF COMP VIS, P1795, DOI 10.1109/ICCV.2009.5459400
   Kozinski M, 2015, PROC CVPR IEEE, P2820, DOI 10.1109/CVPR.2015.7298899
   Kozinski M, 2015, LECT NOTES COMPUT SC, V9006, P79, DOI 10.1007/978-3-319-16817-3_6
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu HT, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2301
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mathias M, 2011, INT ARCH PHOTOGRAMM, V38-5, P171
   Mathias M, 2016, INT J COMPUT VISION, V118, P22, DOI 10.1007/s11263-015-0868-z
   Müller P, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276484, 10.1145/1239451.1239536]
   Nishida G, 2018, COMPUT GRAPH FORUM, V37, P415, DOI 10.1111/cgf.13372
   Oh BM, 2001, COMP GRAPH, P433
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Recky M., 2011, 2011 International Conference on 3D Imaging, Modeling, Processing, Visualization and Transmission (3DIMPVT), P358, DOI 10.1109/3DIMPVT.2011.52
   Ren S., 2017, IEEETransactionsonPatternAnalysisandMachineIntelligence, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Riemenschneider H, 2012, PROC CVPR IEEE, P1640, DOI 10.1109/CVPR.2012.6247857
   Ripperda N, 2006, LECT NOTES COMPUT SC, V4174, P750
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schmitz M, 2016, INT ARCH PHOTOGRAMM, V41, P709, DOI 10.5194/isprsarchives-XLI-B3-709-2016
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Stiny G. N., 1975, PICTORIAL FORMAL ASP, DOI [10.1007/978-3-0348-6879-2, DOI 10.1007/978-3-0348-6879-2]
   Teboul O., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2273, DOI 10.1109/CVPR.2011.5995319
   Teboul O., 2010, Ecole centrale paris facades database
   Teboul O, 2010, PROC CVPR IEEE, P3105, DOI 10.1109/CVPR.2010.5540068
   Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39
   Wendel A, 2010, LECT NOTES COMPUT SC, V6376, P51
   Yang MY, 2011, LECT NOTES COMPUT SC, V6952, P209, DOI 10.1007/978-3-642-24393-6_18
   Zhang H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461923
   Zhang JC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1514, DOI 10.18653/v1/P17-1139
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao P, 2010, PROC CVPR IEEE, P342, DOI 10.1109/CVPR.2010.5540192
   Zheng YY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185595
NR 52
TC 32
Z9 34
U1 5
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3153
EP 3165
DI 10.1109/TMM.2020.2971431
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700011
DA 2024-07-18
ER

PT J
AU Huang, B
   Xu, TF
   Jiang, SW
   Chen, YW
   Bai, Y
AF Huang, Bo
   Xu, Tingfa
   Jiang, Shenwang
   Chen, Yiwen
   Bai, Yu
TI Robust Visual Tracking via Constrained Multi-Kernel Correlation Filters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Correlation; Target tracking; Adaptation models; Training;
   Frequency-domain analysis; Feature extraction; Discriminative
   Correlation Filter; spatial constraints; constrained optimization;
   adaptive updating
ID OBJECT TRACKING; SIAMESE NETWORKS
AB Discriminative Correlation Filter (DCF) based trackers are quite efficient in tracking objects by exploiting the circulant structure. The kernel trick further improves the performance of such trackers. The unwanted boundary effects, however, are difficult to solve in the kernelized correlation models. In this paper, we propose a novel Constrained Multi-Kernel Correlation tracking Filter (CMKCF), which applies spatial constraints to address this drawback. We build the multi-kernel models for multi-channel features with three different attributes, and then employ a spatial cropping operator on the semi-kernel matrix to address the boundary effects. For the constrained optimization solution, we develop an Alternating Direction Method of Multipliers (ADMM) based algorithm to learn our multi-kernel filters efficiently in the frequency domain. In particular, we suggest an adaptive updating mechanism by exploiting the feedback from high-confidence tracking results to avoid corruption in the model. Extensive experimental results demonstrate that the proposed method performs favorably on OTB-2013, OTB-2015, VOT-2016 and VOT-2018 dataset against several state-of-the-art methods.
C1 [Huang, Bo; Jiang, Shenwang; Chen, Yiwen; Bai, Yu] Beijing Inst Technol, Sch Opt & Photon, Image Engn & Video Technol Lab, Beijing 1000811, Peoples R China.
   [Xu, Tingfa] Minist Educ China, Key Lab Photoelect Imaging Technol & Syst, Beijing 100081, Peoples R China.
   [Xu, Tingfa] Beijing Inst Technol, Chongqing Innovat Ctr, Chongqing 401135, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Xu, TF (corresponding author), Minist Educ China, Key Lab Photoelect Imaging Technol & Syst, Beijing 100081, Peoples R China.; Xu, TF (corresponding author), Beijing Inst Technol, Chongqing Innovat Ctr, Chongqing 401135, Peoples R China.
EM a1039377853@163.com; ciom_xtf1@bit.edu.cn; jiangwenj02@gmail.com;
   cyw951025@163.com; baiyu_bit@163.com
RI Chen, Yiwen/IUP-2419-2023
OI Chen, Yiwen/0000-0002-1734-2141; Huang, Bo/0000-0001-6734-5247; xu,
   tingfa/0000-0001-5452-2662; Jiang, Shenwang/0000-0002-0914-4954
FU Major Science Instrument Program of the National Natural Science
   Foundation of China [61527802]; General Program of National Nature
   Science Foundation of China [61371132, 61471043]; International S&T
   Cooperation Program of China [2014DFR10960]
FX This work was supported in part by the Major Science Instrument Program
   of the National Natural Science Foundation of China under Grant
   61527802, in part by the General Program of National Nature Science
   Foundation of China under Grants 61371132 and 61471043, and in part by
   the International S&T Cooperation Program of China under Grant
   2014DFR10960.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Danelljan Martin, 2014, P BRIT MACH VIS C 20
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Galoogahi HK, 2015, PROC CVPR IEEE, P4630, DOI 10.1109/CVPR.2015.7299094
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Kristan M, 2016, The Visual Object Tracking VOT2016 challenge results, P777
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2017, IEEE INT CONF COMP V, P2001, DOI 10.1109/ICCVW.2017.234
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang NX, 2018, IEEE T MULTIMEDIA, V20, P2289, DOI 10.1109/TMM.2018.2803518
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P664, DOI 10.1109/TMM.2018.2863604
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2015, PROC CVPR IEEE, P5388, DOI 10.1109/CVPR.2015.7299177
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Tang M, 2018, PROC CVPR IEEE, P4874, DOI 10.1109/CVPR.2018.00512
   Tang M, 2015, IEEE I CONF COMP VIS, P3038, DOI 10.1109/ICCV.2015.348
   Wang LJ, 2015, IEEE I CONF COMP VIS, P3119, DOI 10.1109/ICCV.2015.357
   Wang MM, 2017, PROC CVPR IEEE, P4800, DOI 10.1109/CVPR.2017.510
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P930, DOI 10.1109/TMM.2018.2869277
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
NR 46
TC 39
Z9 42
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2820
EP 2832
DI 10.1109/TMM.2020.2965482
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900006
OA hybrid
DA 2024-07-18
ER

PT J
AU Mandal, D
   Rao, PM
   Biswas, S
AF Mandal, Devraj
   Rao, Pramod
   Biswas, Soma
TI Semi-Supervised Cross-Modal Retrieval With Label Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training data; Annotations; Training; Noise measurement; Entropy; Task
   analysis; Labeling; Semi-Supervised Learning; Multi-Label Prediction;
   Binary Cross Entropy Loss; Cross-Modal Retrieval
ID REPRESENTATION
AB Cross-modal retrieval tasks with image-text, audio-image, etc. are gaining increasing importance due to an abundance of data from multiple modalities. In general, supervised approaches give significant improvement over their unsupervised counterparts at the additional cost of labeling or annotation of the training data. Recently, semi-supervised methods are becoming popular as they provide an elegant framework to balance the conflicting requirement of labeling cost and accuracy. In this work, we propose a novel deep semi-supervised framework, which can seamlessly handle both labeled as well as unlabeled data. The network has two important components: (a) first, the labels for the unlabeled portion of the training data are predicted using the label prediction component, and then (b) a common representation for both the modalities is learned for performing cross-modal retrieval. The two parts of the network are trained sequentially one after the other. Extensive experiments on three benchmark datasets, Wiki, Pascal VOC, and NUS-WIDE demonstrate that the proposed framework outperforms the state-of-the-art for both supervised and semi-supervised settings.
C1 [Mandal, Devraj; Biswas, Soma] Indian Inst Sci, Dept Elect Engn, Bengaluru 560012, India.
   [Rao, Pramod] Saarland Univ, Dept Comp Sci, Saarbrucken, Germany.
C3 Indian Institute of Science (IISC) - Bangalore; Saarland University
RP Biswas, S (corresponding author), Indian Inst Sci, Dept Elect Engn, Bengaluru 560012, India.
EM devrajm@iisc.ac.in; s8prraoo@stud.uni-saarland.de; somabiswas@iisc.ac.in
CR Andrew G., 2013, P INT C MACH LEARN, P1247
   [Anonymous], 2016, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-46478-7_5
   [Anonymous], 2015, IEEE T CYBERNETICS, DOI DOI 10.1109/TCYB.2014.2356136
   [Anonymous], 2018, IEEE LAT AMER SYMP
   Beigman E, 2009, P JOINT C 47 ANN M A, V1, P280
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Chua T.-S., 2009, P ACM INT C IM VID R
   Das N, 2017, IEEE T IMAGE PROCESS, V26, P3995, DOI 10.1109/TIP.2017.2707858
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fergus R., 2009, P ADV NEUR INF PROC, P522
   Frénay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang X., 2018, ARXIV181208342V2CSLG
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jian MW, 2018, MULTIMED TOOLS APPL, V77, P29099, DOI 10.1007/s11042-018-6122-2
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   King DB, 2015, ACS SYM SER, V1214, P1
   Mandal D, 2016, IEEE T IMAGE PROCESS, V25, P3826, DOI 10.1109/TIP.2016.2577885
   Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460
   Mikolov T., 2013, ADV NEURAL INFORM PR, P3111
   Ngiam J., 2011, P INT C MACH LEARN, P689
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rasiwasia N., 2010, P ACM INT C MULT, P251
   Rasiwasia N, 2014, JMLR WORKSH CONF PRO, V33, P823
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang M, 2016, IEEE T KNOWL DATA EN, V28, P1864, DOI 10.1109/TKDE.2016.2535367
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang WR, 2015, INT CONF ACOUST SPEE, P4590, DOI 10.1109/ICASSP.2015.7178840
   Wu XZ, 2017, PR MACH LEARN RES, V70
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang L, 2018, IEEE T MULTIMEDIA, V20, P128, DOI 10.1109/TMM.2017.2723841
   Zhu X., 2006, COMPUTER SCI, V2, P3
NR 40
TC 20
Z9 22
U1 2
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2345
EP 2353
DI 10.1109/TMM.2019.2954741
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ding, YJ
   Wong, WK
   Lai, ZH
   Chen, YD
AF Ding, Yujuan
   Wong, Wai Keung
   Lai, Zhihui
   Chen, Yudong
TI Study on 2D Feature-Based Hash Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Two dimensional displays; Feature extraction; Binary codes; Image
   retrieval; Matrix converters; Sparse matrices; learning to hash; 2D
   image features; bilinear projection
ID DISCRIMINANT-ANALYSIS; FACE REPRESENTATION; 2-DIMENSIONAL PCA;
   BINARY-CODES; DIMENSIONALITY
AB Hashing is an important topic in image processing, as it can help save a considerable amount of storage and computational cost. Recently, inspired by 2D strategies employed in other areas of image processing, such as feature extraction, some 2D-based hashing methods were proposed. Related papers have shown that these methods may have better image retrieval performance in terms of both effectiveness and efficiency. However, the difference in the retrieval performances of hashing methods resulting from different forms of input (1D or 2D) has not been previously studied. Whether the widely used bilinear strategy in 2D-based hashing can truly help improve the retrieval precision has not been investigated in existing research. In this paper, we conduct a comparison study on 1D and 2D feature-based hashing methods and attempt to theoretically and experimentally analyse the differences in using 1D and 2D features in hashing. Furthermore, two new hashing methods are proposed for conducting the comparison experiments. Through a comprehensive study, we obtain three main conclusions in this paper: 1) Linear projection on 1D features and bilinear projection on 2D features are essentially the same. 2) 2D-based hashing methods are obviously more efficient than 1D-based methods for analysing high-dimensional input features. 3) 2D-based hashing methods show generally better performance for solving small sample size problems.
C1 [Ding, Yujuan; Wong, Wai Keung; Lai, Zhihui] Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Peoples R China.
   [Lai, Zhihui; Chen, Yudong] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Hong Kong Polytechnic University; Shenzhen University
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Inst Text & Clothing, Hong Kong, Peoples R China.
EM dingyujuan385@gmail.com; calvin.wong@polyu.edu.hk; lai_zhi_hui@163.com;
   ydchen7@foxmail.com
RI Lai, Zhihui/R-1000-2019
OI Lai, Zhihui/0000-0002-4388-3080; Wong, Wai Keung/0000-0002-5214-7114
FU Hong Kong Polytechnic University
FX This work was supported by The Hong Kong Polytechnic University (project
   code: RHQK).
CR [Anonymous], 2009, NEURIPS
   [Anonymous], 2016, P ASIAN C COMPUTER V
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Ding Y.-H., IEEE T CIRCUITS SYST
   Ding Y, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P185, DOI 10.1145/3340555.3353759
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu D, 2019, IEEE T MULTIMEDIA, V21, P973, DOI 10.1109/TMM.2018.2866771
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kim S, 2015, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2015.7298739
   Kong H, 2005, IEEE IJCNN, P108
   Kong H, 2005, PROC CVPR IEEE, P1083
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai ZH, 2018, IEEE T IMAGE PROCESS, V27, P6147, DOI 10.1109/TIP.2018.2867956
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H., 2009, P 26 ANN INT C MACHI, V382, P609, DOI 10.1145/1553374.1553453
   Lee SH, 2007, IEEE SIGNAL PROC LET, V14, P735, DOI 10.1109/LSP.2007.896438
   Li Q., 2005, ADV NEURAL INFORM PR, P1569, DOI DOI 10.5555/2976040.2976237
   Liu HD, 2014, PATTERN RECOGN, V47, P1835, DOI 10.1016/j.patcog.2013.11.007
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   McSherry F, 2008, LECT NOTES COMPUT SC, V4956, P414
   Mu Y, 2017, AAAI CONF ARTIF INTE, P2380
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ren CX, 2010, PATTERN RECOGN, V43, P3742, DOI 10.1016/j.patcog.2010.04.029
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Yang X, 2019, IEEE T NEUR NET LEAR, V30, P2987, DOI [10.1109/TNNLS.2018.2861991, 10.1109/TNNLS.2018.2790479]
   Yang X, 2018, IEEE T IMAGE PROCESS, V27, P791, DOI 10.1109/TIP.2017.2765836
   Yang X, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3089249
   Zhang DQ, 2005, NEUROCOMPUTING, V69, P224, DOI 10.1016/j.neucom.2005.06.004
   Zhang HF, 2019, NEUROCOMPUTING, V329, P12, DOI 10.1016/j.neucom.2018.10.043
   Zhang HF, 2019, PATTERN RECOGN LETT, V117, P201, DOI 10.1016/j.patrec.2018.04.011
   Zhang HF, 2018, IEEE T IMAGE PROCESS, V27, P1626, DOI 10.1109/TIP.2017.2781422
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zheng WS, 2008, PATTERN RECOGN, V41, P2156, DOI 10.1016/j.patcog.2007.11.025
NR 47
TC 4
Z9 4
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1298
EP 1309
DI 10.1109/TMM.2019.2940875
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200015
DA 2024-07-18
ER

PT J
AU Liu, Y
   Kiliç, V
   Guan, J
   Wang, WW
AF Liu, Yang
   Kilic, Volkan
   Guan, Jian
   Wang, Wenwu
TI Audio-Visual Particle Flow SMC-PHD Filtering for Multi-Speaker Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Atmospheric measurements; Particle measurements; Noise measurement;
   Frequency modulation; Visualization; Monte Carlo methods; Weight
   measurement; Audio-Visual Tracking; Sequential Monte Carlo; PHD filter;
   Particle Flow; Optimal Proposal Distribution
ID MONTE-CARLO METHODS; AUDIO; IMPLEMENTATION; ALGORITHM; TARGET; CORPUS
AB Sequential Monte Carlo probability hypothesis density (SMC-PHD) filtering is a popular method used recently for audio-visual (AV) multi-speaker tracking. However, due to the weight degeneracy problem, the posterior distribution can be represented poorly by the estimated probability, when only a few particles are present around the peak of the likelihood density function. To address this issue, we propose a new framework where particle flow (PF) is used to migrate particles smoothly from the prior to the posterior probability density. We consider both zero and non-zero diffusion particle flows (ZPF/NPF), and developed two new algorithms, AV-ZPF-SMC-PHD and AV-NPF-SMC-PHD, where the speaker states from the previous frames are also considered for particle relocation. The proposed algorithms are compared systematically with several baseline tracking methods using the AV16.3, AVDIAR and CLEAR datasets, and are shown to offer improved tracking accuracy and average effective sample size (ESS).
C1 [Liu, Yang; Wang, Wenwu] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Kilic, Volkan] Izmir Katip Celebi Univ, Dept Elect & Elect Engn, TR-35620 Cigli Izmir, Turkey.
   [Guan, Jian] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 University of Surrey; Izmir Katip Celebi University; Harbin Engineering
   University
RP Liu, Y (corresponding author), Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
EM yangliu@surrey.ac.uk; volkan.kilis@ikc.edu.tr; j.guan@hrbeu.edu.cn;
   w.wang@surrey.ac.uk
RI wang, wenwu/HOF-4371-2023; Kılıç, Volkan/JZE-2927-2024
OI Kılıç, Volkan/0000-0002-3164-1981; Wang, Wenwu/0000-0002-8393-5703
FU EPSRC Programme Grant S3A: Future Spatial Audio for an Immersive
   Listener Experience at Home [EP/L000539/1]; EPSRC Making Sense of Sounds
   project [EP/N014111/1]; BBC as part of the BBC Audio Research
   Partnership; China Scholarship Council (CSC); EPSRC [EP/K014307/1]; MOD
   University Defence Research Collaboration in Signal Processing; EPSRC
   [EP/L000539/1, EP/K014307/1, EP/P022529/1] Funding Source: UKRI
FX This work was supported in part by the EPSRC Programme Grant S3A: Future
   Spatial Audio for an Immersive Listener Experience at Home
   (EP/L000539/1), in part by the EPSRC Making Sense of Sounds project
   (EP/N014111/1), in part by the BBC as part of the BBC Audio Research
   Partnership, in part by the China Scholarship Council (CSC), and in part
   by the EPSRC Grant EP/K014307/1 and the MOD University Defence Research
   Collaboration in Signal Processing.
CR [Anonymous], [No title captured]
   [Anonymous], 2014, P INT C INF FUS
   [Anonymous], 2018, THESIS
   [Anonymous], 2000, EUR J BIOCHEM, DOI DOI 10.1111/J.1432-1033.1993.TB18143.X
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2015, ARXIV150908787
   [Anonymous], [No title captured]
   [Anonymous], 2007, Surveillance performance evaluation initiative (SPEVI) audiovisual people dataset
   [Anonymous], [No title captured]
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Berzuini C, 1997, J AM STAT ASSOC, V92, P1403, DOI 10.2307/2965410
   Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   Bunch P, 2016, J AM STAT ASSOC, V111, P748, DOI 10.1080/01621459.2015.1038387
   Carletta J, 2005, LECT NOTES COMPUT SC, V3869, P28
   Chen LJ, 2010, PROC SPIE, V7697, DOI 10.1117/12.853001
   Chlebek C, 2016, 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P2043
   D'Arca E, 2016, SIGNAL PROCESS, V129, P137, DOI 10.1016/j.sigpro.2016.04.014
   Daum F., 2012, 2012 15th International Conference on Information Fusion (FUSION 2012), P135
   Daum F, 2003, AEROSP CONF PROC, P1979
   Daum F., 2017, P SOC PHOTO-OPT INS, V10200
   Daum F, 2008, PROC SPIE, V6969, DOI 10.1117/12.764909
   Daum F, 2013, PROC SPIE, V8857, DOI 10.1117/12.2021370
   Daum F, 2013, PROC SPIE, V8745, DOI 10.1117/12.2009364
   Daum F, 2011, PROC SPIE, V8137, DOI 10.1117/12.887514
   Daum F, 2010, PROC SPIE, V7697, DOI 10.1117/12.839590
   de Melo Flavio Eler, 2015, ARXIV151101448
   Deleforge A, 2015, IEEE-ACM T AUDIO SPE, V23, P718, DOI 10.1109/TASLP.2015.2405475
   Ding T, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P257, DOI 10.1109/SSP.2012.6319675
   Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038
   Gebru I. D., 2015, P IEEE INT C COMP VI, P15
   Gebru ID, 2018, IEEE T PATTERN ANAL, V40, P1086, DOI 10.1109/TPAMI.2017.2648793
   Gehrig T, 2005, IEEE WORK APPL SIG, P118, DOI 10.1109/ASPAA.2005.1540183
   Gilks WR, 2001, J ROY STAT SOC B, V63, P127, DOI 10.1111/1467-9868.00280
   Hampapur I, 2005, IEEE SIGNAL PROC MAG, V22, P38
   Hoel P. G., 1960, Elementary Statistics
   Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166
   KAILATH T, 1967, IEEE T COMMUN TECHN, VCO15, P52, DOI 10.1109/TCOM.1967.1089532
   Khan MA, 2016, 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P2229
   Khan MA, 2015, 2015 18TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P74
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kim DY, 2017, INT CONF CONTR AUTO, P38, DOI 10.1109/ICCAIS.2017.8217590
   Kitagawa G., 1996, J COMPUT GRAPH STAT, V5, P1, DOI DOI 10.2307/1390750
   Lathoud G, 2005, LECT NOTES COMPUT SC, V3361, P182
   Li TC, 2016, SIGNAL PROCESS, V119, P115, DOI 10.1016/j.sigpro.2015.07.013
   Li YP, 2017, IEEE T SIGNAL PROCES, V65, P4102, DOI 10.1109/TSP.2017.2703684
   Li YP, 2016, INT CONF ACOUST SPEE, P3979, DOI 10.1109/ICASSP.2016.7472424
   Liu JS, 1998, J AM STAT ASSOC, V93, P1032, DOI 10.2307/2669847
   Liu Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4304, DOI 10.1109/ICASSP.2018.8461791
   Liu Y, 2017, INT CONF ACOUST SPEE, P4371, DOI 10.1109/ICASSP.2017.7952982
   Liu Y, 2017, LECT NOTES COMPUT SC, V10169, P344, DOI 10.1007/978-3-319-53547-0_33
   Mahler R, 2007, IEEE T AERO ELEC SYS, V43, P1523, DOI 10.1109/TAES.2007.4441756
   Mahler R, 2010, PROC SPIE, V7698, DOI 10.1117/12.849470
   Mahler RPS, 2011, IEEE T SIGNAL PROCES, V59, P3497, DOI 10.1109/TSP.2011.2128316
   Mahler RPS, 2003, IEEE T AERO ELEC SYS, V39, P1152, DOI 10.1109/TAES.2003.1261119
   Ménélas B, 2010, IEEE SYMPOSIUM ON 3D USER INTERFACES (3DUI 2010), P51, DOI 10.1109/3DUI.2010.5444722
   Minotto VP, 2015, IEEE T MULTIMEDIA, V17, P1694, DOI 10.1109/TMM.2015.2463722
   Okuma K, 2004, LECT NOTES COMPUT SC, V3021, P28, DOI 10.1007/978-3-540-24670-1_3
   Ooi MH, 2007, J CLIN MICROBIOL, V45, P1858, DOI 10.1128/JCM.01394-06
   Pitt MK, 1999, J AM STAT ASSOC, V94, P590, DOI 10.2307/2670179
   Qian XY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3071, DOI 10.1109/ICASSP.2018.8461323
   Ristic B., 2010, Thirteenth Conference on Information Fusion, P1
   Ristic B, 2011, IEEE T SIGNAL PROCES, V59, P3452, DOI 10.1109/TSP.2011.2140111
   RUBIN DB, 1987, J AM STAT ASSOC, V82, P543, DOI 10.2307/2289460
   SHAN TJ, 1985, IEEE T ACOUST SPEECH, V33, P806, DOI 10.1109/TASSP.1985.1164649
   SHIH FY, 1995, PATTERN RECOGN, V28, P331, DOI 10.1016/0031-3203(94)00104-T
   SMITH AFM, 1992, AM STAT, V46, P84, DOI 10.2307/2684170
   van der Merwe R, 2001, ADV NEUR IN, V13, P584
   Vermaak J, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P741, DOI 10.1109/ICCV.2001.937600
   Vo BN, 2006, IEEE T SIGNAL PROCES, V54, P4091, DOI 10.1109/TSP.2006.881190
   Vo BN, 2005, IEEE T AERO ELEC SYS, V41, P1224, DOI 10.1109/TAES.2005.1561884
   Welch G., 1995, INTRO KALMAN FILTER
   Whiteley N, 2010, IEEE T AERO ELEC SYS, V46, P1437, DOI 10.1109/TAES.2010.5545199
   Yeo HS, 2015, MULTIMED TOOLS APPL, V74, P2687, DOI 10.1007/s11042-013-1501-1
   Zhao LL, 2016, PROC SPIE, V9842, DOI 10.1117/12.2228326
NR 77
TC 11
Z9 11
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 934
EP 948
DI 10.1109/TMM.2019.2937185
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400009
DA 2024-07-18
ER

PT J
AU Subudhi, BN
   Thangaraj, V
   Esakkirajan, S
   Ghosh, A
AF Subudhi, Badri Narayan
   Thangaraj, Veerakumar
   Esakkirajan, S.
   Ghosh, Ashish
TI Kernelized Fuzzy Modal Variation for Local Change Detection From Video
   Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background subtraction; object detection; temporal analysis; fuzzy
   logic; modal variation
ID MOVING OBJECT SEGMENTATION; BACKGROUND SUBTRACTION; ALGORITHM
AB Background subtraction (BGS) is a popular scheme epitomized in the state-of-the-art literature on video processing. In this context, a novel online kernelized fuzzy modal variation based background subtraction scheme for detecting local changes from the sequences of image frames is proposed. In the proposed scheme, the time varying background at different instances of time are modeled using fuzzy set theory. The proposed background subtraction scheme, utilizes the fuzzy modal variation as the cost function for fitting the pixel values of the image frames. The use of kernel based modal variation helps in projecting the pixel values in a higher dimensional space, linearly separating them into object and background classes. The results of the proposed technique is verified on different challenging sequences including dynamic background, camera jitter, noise, blurred scene, etc. The proposed technique is successfully tested over several test sequences with two major databases (all sequences) and it provides better results compared to the twenty one existing state-of-the-art techniques.
C1 [Subudhi, Badri Narayan] Indian Inst Technol Jammu, Dept Elect Engn, Jammu 181121, India.
   [Thangaraj, Veerakumar] Natl Inst Technol, Dept Elect & Commun Engn, Ponda 403401, Goa, India.
   [Esakkirajan, S.] PSG Coll Technol, Instrumentat & Control Engn, Coimbatore 641004, Tamil Nadu, India.
   [Ghosh, Ashish] Indian Stat Inst, Ctr Soft Comp Res, Kolkata 700108, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) Jammu; National Institute of Technology (NIT System);
   National Institute of Technology Goa; PSG College Technology; Indian
   Statistical Institute; Indian Statistical Institute Kolkata
RP Subudhi, BN (corresponding author), Indian Inst Technol Jammu, Dept Elect Engn, Jammu 181121, India.
EM subudhi.badri@gmail.com; tveerakumar@yahoo.co.in; rajanesakki@yahoo.com;
   ash@isical.ac.in
RI - PSGCT, Dr Esakkirajan S/GZM-8929-2022; SUBUDHI, BADRI N/B-6830-2013;
   Thangaraj, Veerakumar/AAS-8625-2020
OI SUBUDHI, BADRI N/0000-0002-4378-0065; Thangaraj,
   Veerakumar/0000-0001-9084-1847; GHOSH, ASHISH/0000-0003-1548-5576
FU Science and Engineering Research Board, India [EMR/2016/002552]; DSTICPS
FX This work was supported in part by Science and Engineering Research
   Board, India, under Grant EMR/2016/002552 and in part by DSTICPS
   support: DST/ICPS/CLUSTER/Data Science/2018 dt. 07.01.2019.
CR Babaee M, 2018, PATTERN RECOGN, V76, P635, DOI 10.1016/j.patcog.2017.09.040
   Balcilar M, 2013, LECT NOTES COMPUT SC, V7824, P287, DOI 10.1007/978-3-642-37213-1_30
   Barnich O, 2011, IEEE T IMAGE PROCESS, V20, P1709, DOI 10.1109/TIP.2010.2101613
   Bouwmans T., 2009, MASAUM Journal of of Basic and Applied Sciences, V1, P265
   Cao WF, 2016, IEEE T IMAGE PROCESS, V25, P4075, DOI 10.1109/TIP.2016.2579262
   Chan AB, 2011, MACH VISION APPL, V22, P751, DOI 10.1007/s00138-010-0262-3
   Chen ML, 2018, IEEE T PATTERN ANAL, V40, P1518, DOI 10.1109/TPAMI.2017.2717828
   Chen ML, 2014, LECT NOTES COMPUT SC, V8695, P521, DOI 10.1007/978-3-319-10584-0_34
   De Gregorio M, 2017, PATTERN RECOGN LETT, V96, P55, DOI 10.1016/j.patrec.2017.05.029
   Durucan E, 2001, P IEEE, V89, P1368, DOI 10.1109/5.959336
   El Baf F, 2008, IEEE INT CONF FUZZY, P1731
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Ghosh A, 2012, IEEE T CIRC SYST VID, V22, P1127, DOI 10.1109/TCSVT.2012.2190476
   Gonzalez R.C., 2018, DIGITAL IMAGE PROCES, V4th
   Graves D, 2010, FUZZY SET SYST, V161, P522, DOI 10.1016/j.fss.2009.10.021
   Guyon Charles., 2012, Principal component analysis
   Haines TSF, 2014, IEEE T PATTERN ANAL, V36, P670, DOI 10.1109/TPAMI.2013.239
   Heikkilä M, 2006, IEEE T PATTERN ANAL, V28, P657, DOI 10.1109/TPAMI.2006.68
   Hongxun Z, 2006, LECT NOTES COMPUT SC, V4223, P887
   Jiang SQ, 2018, IEEE T CIRC SYST VID, V28, P2105, DOI 10.1109/TCSVT.2017.2711659
   Jodoin PM, 2007, IEEE T CIRC SYST VID, V17, P1758, DOI 10.1109/TCSVT.2007.906935
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Kim W, 2012, IEEE SIGNAL PROC LET, V19, P127, DOI 10.1109/LSP.2011.2182648
   Lee H, 2016, IEEE T MULTIMEDIA, V18, P2093, DOI 10.1109/TMM.2016.2595262
   Li J, 2004, PROCEEDINGS OF THE 2004 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3928
   Liu X, 2018, IEEE T CIRC SYST VID, V28, P1737, DOI 10.1109/TCSVT.2017.2697972
   Liu X, 2015, IEEE T IMAGE PROCESS, V24, P2502, DOI 10.1109/TIP.2015.2419084
   Maddalena L, 2008, IEEE T IMAGE PROCESS, V17, P1168, DOI 10.1109/TIP.2008.924285
   Maddalena L, 2010, NEURAL COMPUT APPL, V19, P179, DOI 10.1007/s00521-009-0285-8
   Mandellos NA, 2011, EXPERT SYST APPL, V38, P1619, DOI 10.1016/j.eswa.2010.07.083
   Minaee S, 2019, IEEE T IMAGE PROCESS, V28, P3192, DOI 10.1109/TIP.2019.2894966
   Mondal A., 2015, P INT COMP COMM DEV, V1, P571
   Parks Donovan H., 2008, 2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance, P192, DOI 10.1109/AVSS.2008.19
   Rout DK, 2018, EXPERT SYST APPL, V97, P117, DOI 10.1016/j.eswa.2017.12.009
   Sajid H, 2017, IEEE T IMAGE PROCESS, V26, P3249, DOI 10.1109/TIP.2017.2695882
   Seidel F, 2014, MACH VISION APPL, V25, P1227, DOI 10.1007/s00138-013-0555-4
   Seo JW, 2014, IEEE T MULTIMEDIA, V16, P2333, DOI 10.1109/TMM.2014.2353772
   Sigari MH, 2008, INT J COMPUT SCI NET, V8, P138
   Spagnolo P, 2006, IMAGE VISION COMPUT, V24, P411, DOI 10.1016/j.imavis.2006.01.001
   St-Charles PL, 2015, IEEE T IMAGE PROCESS, V24, P359, DOI 10.1109/TIP.2014.2378053
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Subudhi BN, 2016, INFORM SCIENCES, V366, P31, DOI 10.1016/j.ins.2016.04.049
   Subudhi BN, 2015, SOFT COMPUT, V19, P2769, DOI 10.1007/s00500-014-1440-4
   Subudhi BN, 2013, MACH VISION APPL, V24, P795, DOI 10.1007/s00138-012-0475-8
   Subudhi BN, 2011, IEEE T CIRC SYST VID, V21, P993, DOI 10.1109/TCSVT.2011.2133870
   Tekalp A.M., 1995, DIGITAL VIDEO PROCES
   Wang Y, 2017, PATTERN RECOGN LETT, V96, P66, DOI 10.1016/j.patrec.2016.09.014
   Yong HW, 2018, IEEE T PATTERN ANAL, V40, P1726, DOI 10.1109/TPAMI.2017.2732350
   Zhang X, 2017, IEEE T MULTIMEDIA, V19, P2425, DOI 10.1109/TMM.2017.2701645
NR 49
TC 8
Z9 8
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 912
EP 920
DI 10.1109/TMM.2019.2938342
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400007
DA 2024-07-18
ER

PT J
AU Yan, B
   Niu, XJ
   Bare, B
   Tan, WM
AF Yan, Bo
   Niu, Xuejing
   Bare, Bahetiyaer
   Tan, Weimin
TI Semantic Segmentation Guided Pixel Fusion for Image Retargeting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image segmentation; Distortion; Saliency detection;
   Visualization; Strain; Image resolution; Image retargeting; semantic
   segmentation; pixel fusion; scaling factor
ID NETWORK
AB Image retargeting aims to obtain high visual quality of target images for human vision. Through semantic segmentation and understanding of input images, we can better preserve the important semantic regions, so as to effectively improve the performance of image retargeting. Benefit from the successful application of deep neural network in the field of semantic segmentation, in this paper, we propose a novel image retargeting approach using semantic segmentation and pixel fusion. Compared with existing image retargeting methods, our approach can effectively reduce geometric distortion during image retargeting by finely reallocating scaling factors for each region based on the semantic segmentation results. Experimental results demonstrate that the proposed approach can well preserve important semantic regions while leaving less unnatural geometric distortion. Our approach also shows the important role of semantic segmentation and understanding of scenes in image retargeting in detail.
C1 [Yan, Bo; Niu, Xuejing; Bare, Bahetiyaer; Tan, Weimin] Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Yan, B (corresponding author), Fudan Univ, Shanghai Key Lab Intelligent Informat Proc, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM byan@fudan.edu.cn
RI Yan, Bo/AFQ-7025-2022
OI Yan, Bo/0000-0002-7775-1270
FU NSFC [61772137]
FX This work was supported in part by NSFC uner Grant 61772137. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. C.-S. Kim.
CR [Anonymous], 2018, CIKM18 P 27 ACM INT, DOI DOI 10.1145/3269206.3271759
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen L, 2015, RSC SMART MATER, P1
   Dong WM, 2016, IEEE T VIS COMPUT GR, V22, P1088, DOI 10.1109/TVCG.2015.2440255
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Krähenbühl P, 2009, ACM T GRAPHIC, V28, DOI [10.1145/1616452.1618472, 10.1145/1618452.1618472]
   Lau CP, 2018, IEEE T IMAGE PROCESS, V27, P5787, DOI 10.1109/TIP.2018.2858146
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu X, 2006, CONFERENCE RECORD OF THE 2006 IEEE INTERNATIONAL SYMPOSIUM ON ELECTRICAL INSULATION, P241, DOI 10.1109/ELINSL.2006.1665302
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Panozzo D, 2012, COMPUT GRAPH FORUM, V31, P229, DOI 10.1111/j.1467-8659.2012.03001.x
   Qu Z, 2013, IEEE T MULTIMEDIA, V15, P1677, DOI 10.1109/TMM.2013.2267727
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P1453, DOI 10.1109/TCYB.2013.2273270
   Sun KR, 2012, IEEE IMAGE PROC, P2105, DOI 10.1109/ICIP.2012.6467307
   Tan WM, 2016, IEEE T MULTIMEDIA, V18, P128, DOI 10.1109/TMM.2015.2500727
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Yan B, 2015, IEEE T CIRC SYST VID, V25, P15, DOI 10.1109/TCSVT.2014.2329374
   Yan B, 2014, IEEE T MULTIMEDIA, V16, P272, DOI 10.1109/TMM.2013.2286112
   Yen TC, 2011, IEEE T IMAGE PROCESS, V20, P2339, DOI 10.1109/TIP.2011.2114357
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang LM, 2015, IEEE T MULTIMEDIA, V17, P1538, DOI 10.1109/TMM.2015.2451954
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 35
TC 20
Z9 21
U1 5
U2 62
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 676
EP 687
DI 10.1109/TMM.2019.2932566
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700009
DA 2024-07-18
ER

PT J
AU Zhu, LW
   Kwong, S
   Zhang, Y
   Wang, SQ
   Wang, X
AF Zhu, Linwei
   Kwong, Sam
   Zhang, Yun
   Wang, Shiqi
   Wang, Xu
TI Generative Adversarial Network-Based Intra Prediction for Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Video coding; Generative adversarial networks; Gallium
   nitride; Transforms; Predictive models; Generative adversarial network;
   intra prediction; inpainting; high efficiency video coding; versatile
   video coding
AB In this paper, a novel intra prediction method is proposed to improve the video coding performance, in which the generative adversarial network (GAN) is adopted to intelligently remove the spatial redundancy with the inference process. The proposed GAN-based method improves the prediction by exploiting more information and generating more flexible prediction patterns. In particular, the intra prediction is modeled as an inpainting task, which is accomplished with the GAN model to fill in the missing part by conditioning on the available reconstructed pixels. As such, the learned GAN model is incorporated into both video encoder and decoder, and the rate-distortion optimization is performed for the competition between GAN-based intra prediction and traditional angular-based intra prediction to achieve better coding performance. The proposed scheme is implemented into the high-efficiency video coding test model (HM 16.17) and the versatile video coding test model (VTM 1.1). The experimental results show that the proposed algorithm can achieve 6.6%, 7.5%, and 7.5% under HM 16.17 and 6.75%, 7.63%, and 7.65% under VTM 1.1 bit rate savings on average for luma and chroma components in the intra coding scenario.
C1 [Zhu, Linwei; Zhang, Yun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Zhu, Linwei; Kwong, Sam; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Kwong, Sam; Wang, Shiqi] City Univ Hong Kong, Hong Kong Shenzhen Inst, Shenzhen 518057, Peoples R China.
   [Wang, Xu] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; City University of Hong Kong; City University of Hong Kong;
   Shenzhen University
RP Kwong, S (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM lwzhu2-c@my.cityu.edu.hk; cssamk@cityu.edu.hk; yun.zhang@siat.ac.cn;
   shiqwang@cityu.edu.hk; wangxu@szu.edu.cn
RI Zhang, Yun/V-7261-2019; Kwong, Sam/C-9319-2012
OI Zhang, Yun/0000-0001-9457-7801; Kwong, Sam/0000-0001-7484-7261; ,
   linwei/0000-0002-9385-9054
FU Natural Science Foundation of China [61672443, 61871312, 61871270];
   China Postdoctoral Science Foundation [2019M653127]; Hong Kong RGC
   General Research Fund [9042322 (CityU 11200116), 9042489 (CityU
   11206317)]; Hong Kong RGC Early Career Scheme [9048122 (CityU
   21211018)]; City University of Hong Kong [7200539/CS]; Guangdong Natural
   Science Funds for Distinguished Young Scholar [2016A030306022]; Shenzhen
   Science and Technology Development Project [JCYJ20170811160212033];
   Shenzhen International Collaborative Research Project
   [GJHZ20170314155404913]; Shenzhen Science and Technology Plan Project
   [JCYJ20180507183823045]; Guangdong Provincial Science and Technology
   Development [2017B010110014]; Free Application Fund of Natural Science
   Foundation of Guangdong Province [2018A0303130126]; Guangdong
   International Science and Technology Cooperative Research Project
   [2018A050506063]; Membership of Youth Innovation Promotion Association,
   Chinese Academy of Sciences [2018392]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61672443, Grant 61871312, and Grant 61871270, in part
   by China Postdoctoral Science Foundation under Grant 2019M653127, in
   part by Hong Kong RGC General Research Fund 9042322 (CityU 11200116) and
   9042489 (CityU 11206317), in part by Hong Kong RGC Early Career Scheme
   9048122 (CityU 21211018), in part by City University of Hong Kong under
   Grant 7200539/CS, in part by Guangdong Natural Science Funds for
   Distinguished Young Scholar under Grant 2016A030306022, in part by
   Shenzhen Science and Technology Development Project under Grant
   JCYJ20170811160212033, in part by Shenzhen International Collaborative
   Research Project under Grant GJHZ20170314155404913, in part by Shenzhen
   Science and Technology Plan Project under Grant JCYJ20180507183823045,
   in part by Guangdong Provincial Science and Technology Development under
   Grant 2017B010110014, in part by Free Application Fund of Natural
   Science Foundation of Guangdong Province under Grant 2018A0303130126, in
   part by Guangdong International Science and Technology Cooperative
   Research Project under Grant 2018A050506063 and in part by Membership of
   Youth Innovation Promotion Association, Chinese Academy of Sciences
   under Grant 2018392. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Mrak Marta.
CR [Anonymous], 2018, JVETK1001V7
   [Anonymous], ARXIV180308943V2
   [Anonymous], 2001, M33 ITUT VID COD EXP
   [Anonymous], 2017, JCTVCAB1002
   Blasi SG, 2015, IEEE T CIRC SYST VID, V25, P798, DOI 10.1109/TCSVT.2014.2359097
   Bossen F., 2012, JCTVCJ1100
   Chen C, 2018, IEEE T CIRC SYST VID, V28, P1906, DOI 10.1109/TCSVT.2017.2694966
   Chen C, 2017, IEEE T CIRC SYST VID, V27, P1727, DOI 10.1109/TCSVT.2016.2556478
   Chen CC, 2017, IEEE T CIRC SYST VID, V27, P1568, DOI 10.1109/TCSVT.2016.2543098
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Demir U., 2018, Patch-based image inpainting with generative adversarial networks
   Ghorai M, 2018, IEEE T IMAGE PROCESS, V27, P556, DOI 10.1109/TIP.2017.2768180
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jin DR, 2019, IEEE T CIRC SYST VID, V29, P1310, DOI 10.1109/TCSVT.2018.2839351
   Kamisli F, 2019, IEEE T CIRC SYST VID, V29, P502, DOI 10.1109/TCSVT.2017.2787638
   Kamisli F, 2013, IEEE T IMAGE PROCESS, V22, P3916, DOI 10.1109/TIP.2013.2264679
   Kim IK, 2012, IEEE T CIRC SYST VID, V22, P1697, DOI 10.1109/TCSVT.2012.2223011
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Li JH, 2018, IEEE T IMAGE PROCESS, V27, P3236, DOI 10.1109/TIP.2018.2817044
   Li JH, 2018, IEEE T CIRC SYST VID, V28, P947, DOI 10.1109/TCSVT.2016.2633377
   Li ZD, 2015, IEEE T IMAGE PROCESS, V24, P1138, DOI 10.1109/TIP.2014.2383322
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Pfaff J, 2018, PROC SPIE, V10752, DOI 10.1117/12.2321273
   Qi XL, 2012, INT CONF ACOUST SPEE, P1217, DOI 10.1109/ICASSP.2012.6288107
   Saxena A, 2013, IEEE T IMAGE PROCESS, V22, P3974, DOI 10.1109/TIP.2013.2265882
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yeh CH, 2015, IEEE T MULTIMEDIA, V17, P1508, DOI 10.1109/TMM.2015.2449659
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P3983, DOI 10.1109/TIP.2018.2830640
   Zhang T, 2018, IEEE T MULTIMEDIA, V20, P1622, DOI 10.1109/TMM.2017.2775223
   Zhang T, 2017, IEEE T MULTIMEDIA, V19, P2404, DOI 10.1109/TMM.2017.2703114
   Zheng AM, 2016, IEEE T CIRC SYST VID, V26, P2152, DOI 10.1109/TCSVT.2015.2501738
NR 35
TC 36
Z9 38
U1 5
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 45
EP 58
DI 10.1109/TMM.2019.2924591
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000006
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Liu, YX
   Dong, WS
   Shi, GM
   Lin, WS
AF Wu, Jinjian
   Liu, Yongxu
   Dong, Weisheng
   Shi, Guangming
   Lin, Weisi
TI Quality Assessment for Video With Degradation Along Salient Trajectories
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Trajectory; Degradation; Motion measurement; Distortion measurement;
   Optical distortion; Distortion; Quality assessment; Video-quality
   assessment; motion trajectory; optical flow; spatial-temporal quality
   degradation
ID IMAGE; MOTION; SIMILARITY; ATTENTION; EFFICIENT; MODEL; DISTORTIONS;
   INFORMATION; DEVIATION; INDEX
AB With the rapid growth of digital video through the Internet, a reliable objective video-quality assessment (VQA) algorithm is in great demand for video management. Motion information plays a dominant role for video perception, and the human visual system (HVS) is able to track moving objects effectively with eye movement. Moreover, the middle temporal area of the brain is selective for moving objects with particular velocities. In other words, visual contents that are along the motion trajectories will automatically attract our attention for dedicated processing. Inspired by the motion-related process in the HVS, we suggest analyzing the degradation along attended motion trajectories for VQA. The characteristic of motion velocity along each trajectory is analyzed for temporal quality measurement. Meanwhile, visual information along each trajectory is extracted for joint spatial-temporal quality measurement. Finally, considering the spatial-quality degradation from each frame, a novel full-reference assessor along salient trajectories (FAST) for VQA (which combines the spatial, temporal, and joint spatial-temporal quality degradations) is introduced. Experimental results on five publicly available VQA databases demonstrate that the proposed FAST VQA model performs consistently with the subjective perception. The source code of the proposed method is available at http://web.xidian.edu.cn/wjj/paper.html.
C1 [Wu, Jinjian; Dong, Weisheng] Xidian Univ, Sch Artificial Intelligence, State Key Lab Integrated Serv Networks, Xian 710126, Shaanxi, Peoples R China.
   [Liu, Yongxu; Shi, Guangming] Xidian Univ, Sch Artificial Intelligence, Key Lab Intelligent Percept, Image Understanding Minist Educ, Xian 710126, Shaanxi, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Xidian University; Xidian University; Nanyang Technological University
RP Wu, JJ (corresponding author), Xidian Univ, Sch Artificial Intelligence, State Key Lab Integrated Serv Networks, Xian 710126, Shaanxi, Peoples R China.
EM jinjian.wu@mail.xidian.edu.cn; yongxu.liu@stu.xidian.edu.cn;
   wsdong@mail.xidian.edu.cn; gmshi@xidian.edu.cn; wslin@ntu.edu.sg
RI Wu, Jinjian/GQH-0222-2022; Lin, Weisi/A-8011-2012; Liu,
   Yongxu/JVE-2493-2024; Lin, Weisi/A-3696-2011
OI Liu, Yongxu/0009-0008-5719-1107; Lin, Weisi/0000-0001-9866-1947
FU National Natural Science Foundation of China [61772388, 61632019,
   61621005, 61472301, 2018KJXX-030]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772388, Grant 61632019, Grant
   61621005, andGrant 61472301 and in part by theYoung Star Science and
   Technology Project 2018KJXX-030 in Shaanxi province. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Hantao Liu.
CR [Anonymous], 2011, IVP Subjective Quality Video Database
   [Anonymous], 2016, NETFLIX TECH BLOG
   Bampis CG, 2017, IEEE SIGNAL PROC LET, V24, P1333, DOI 10.1109/LSP.2017.2726542
   Born RT, 2005, ANNU REV NEUROSCI, V28, P157, DOI 10.1146/annurev.neuro.26.041002.131052
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   He LH, 2017, NEUROCOMPUTING, V269, P108, DOI 10.1016/j.neucom.2016.08.143
   Lee C, 2003, OPT ENG, V42, P265, DOI 10.1117/1.1523420
   Li LD, 2018, IEEE T MULTIMEDIA, V20, P914, DOI 10.1109/TMM.2017.2760062
   Lin JY, 2015, J VIS COMMUN IMAGE R, V30, P1, DOI 10.1016/j.jvcir.2015.02.012
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Manasa K, 2016, IEEE T IMAGE PROCESS, V25, P2480, DOI 10.1109/TIP.2016.2548247
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Nafchi HZ, 2016, IEEE ACCESS, V4, P5579, DOI 10.1109/ACCESS.2016.2604042
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Ninassi A, 2009, IEEE J-STSP, V3, P253, DOI 10.1109/JSTSP.2009.2014806
   Peng P, 2017, PATTERN RECOGN, V70, P1, DOI 10.1016/j.patcog.2017.04.031
   Pitrey Y, 2012, PROC SPIE, V8291, DOI 10.1117/12.912180
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   Treue S, 1999, NATURE, V399, P575, DOI 10.1038/21176
   Treue S, 1996, NATURE, V382, P539, DOI 10.1038/382539a0
   Treue S, 1999, J NEUROSCI, V19, P7591, DOI 10.1523/JNEUROSCI.19-17-07591.1999
   Vu PV, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.1.013016
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2007, J OPT SOC AM A, V24, pB61, DOI 10.1364/JOSAA.24.000B61
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wolf S., 2011, Tech. Memo TM-11-482
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Xu L, 2016, IEEE T MULTIMEDIA, V18, P590, DOI 10.1109/TMM.2016.2525004
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   You JY, 2014, IEEE T IMAGE PROCESS, V23, P200, DOI 10.1109/TIP.2013.2287611
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang W, 2017, IEEE T IMAGE PROCESS, V26, P1275, DOI 10.1109/TIP.2017.2651410
NR 43
TC 41
Z9 44
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2738
EP 2749
DI 10.1109/TMM.2019.2908377
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000004
DA 2024-07-18
ER

PT J
AU Qian, XY
   Brutti, A
   Lanz, O
   Omologo, M
   Cavallaro, A
AF Qian, Xinyuan
   Brutti, Alessio
   Lanz, Oswald
   Omologo, Maurizio
   Cavallaro, Andrea
TI Multi-Speaker Tracking From an Audio-Visual Sensing Device
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual fusion; 3-D target tracking; co-located sensors;
   likelihood; particle filter
ID ROBUST VISUAL TRACKING; SPEAKER TRACKING; INFORMATION FUSION; MULTIPLE;
   AUDIO; CORPUS; FILTER
AB Compact multi-sensor platforms are portable and thus desirable for robotics and personal-assistance tasks. However, compared to physically distributed sensors, the size of these platforms makes person tracking more difficult. To address this challenge, we propose a novel 3-D audio-visual people tracker that exploits visual observations (object detections) to guide the acoustic processing by constraining the acoustic likelihood on the horizontal plane defined by the predicted height of a speaker. This solution allows the tracker to estimate, with a small microphone array, the distance of a sound. Moreover, we apply a color-based visual likelihood on the image plane to compensate for misdetections. Finally, we use a 3-D particle filter and greedy data association to combine visual observations, color-based, and acoustic likelihoods to track the position of multiple simultaneous speakers. We compare the proposed multimodal 3-D tracker against two state-of-the-art methods on the AV16.3 dataset and on a newly collected dataset with co-located sensors, which we make available to the research community. Experimental results show that our multimodal approach outperforms the other methods both in 3-D and on the image plane.
C1 [Qian, Xinyuan; Cavallaro, Andrea] Queen Mary Univ London, Ctr Intelligent Sensing, London E1 4NS, England.
   [Brutti, Alessio; Lanz, Oswald; Omologo, Maurizio] Fdn Bruno Kessler, Ctr Informat & Commun Technol, I-38122 Trento, Italy.
C3 University of London; Queen Mary University London; Fondazione Bruno
   Kessler
RP Qian, XY (corresponding author), Queen Mary Univ London, Ctr Intelligent Sensing, London E1 4NS, England.
EM x.qian@qmul.ac.uk; brutti@fbk.eu; lanz@fbk.eu; omologo@fbk.eu;
   a.cavallaro@qmul.ac.uk
RI qian, xinyuan/ADN-5425-2022; Lanz, Oswald/AAW-7865-2021
OI Brutti, Alessio/0000-0003-4146-3071; Qian, Xinyuan/0000-0002-9511-6713;
   Omologo, Maurizio/0000-0003-0879-0548; Lanz, Oswald/0000-0003-4793-4276
CR Alameda-Pineda X, 2013, J MULTIMODAL USER IN, V7, P79, DOI 10.1007/s12193-012-0111-y
   [Anonymous], 2006, ELRA Newslett
   [Anonymous], MACHINE LEARNING MUL
   [Anonymous], 2001, Sequential Monte Carlo methods in practice
   [Anonymous], 2008, P 2008 INT C INF FUS
   [Anonymous], 2007, Surveillance performance evaluation initiative (SPEVI) audiovisual people dataset
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], EURASIP J AUDIO SPEE
   [Anonymous], 2011, Video Tracking: Theory and Practice
   [Anonymous], 1998, SPOKEN DIALOGUE COMP
   [Anonymous], 2008, P 10 INT C MULTIMODA
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Bae SH, 2014, PROC CVPR IEEE, P1218, DOI 10.1109/CVPR.2014.159
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Ban YT, 2017, IEEE INT CONF COMP V, P446, DOI 10.1109/ICCVW.2017.60
   Barnard M, 2014, IEEE T MULTIMEDIA, V16, P864, DOI 10.1109/TMM.2014.2301977
   Beal MJ, 2003, IEEE T PATTERN ANAL, V25, P828, DOI 10.1109/TPAMI.2003.1206512
   Birchfield S, 1998, PROC CVPR IEEE, P232, DOI 10.1109/CVPR.1998.698614
   Birchfield ST, 2005, PROC CVPR IEEE, P1158
   Brandstein M., 2001, MICROPHONE ARRAYS SI
   Brandstein M.S., 1995, A Framework for Speech Source Localization Using Sensor Arrays
   Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232
   Brunelli R., 2006, Multimodal Technologies for Perception of Humans. First International Evaluation Workshop on Classification of Events, Activities and Relationships, CLEAR 2006. Revised Selected Papers (Lecture Notes in Computer Science Vol.4122), P55
   Brutti A, 2010, EUR SIGNAL PR CONF, P974
   Cai YZ, 2006, LECT NOTES COMPUT SC, V3954, P107
   Cevher V, 2007, IEEE T MULTIMEDIA, V9, P715, DOI 10.1109/TMM.2007.893340
   Chen JD, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/26503
   Coleman P, 2018, IEEE T MULTIMEDIA, V20, P1919, DOI 10.1109/TMM.2018.2794780
   Conaire CO, 2007, INT CONF ACOUST SPEE, P1069
   D'Arca E., 2012, Data Fusion Target Tracking Conference (DF TT 2012): Algorithms Applications, 9th IET, P1
   Deleforge A, 2015, IEEE-ACM T AUDIO SPE, V23, P718, DOI 10.1109/TASLP.2015.2405475
   FORTMANN TE, 1983, IEEE J OCEANIC ENG, V8, P173, DOI 10.1109/JOE.1983.1145560
   Fritsch J., 2004, P INT C INTELLIGENT, P898
   FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330
   Gatica-Perez D, 2007, IEEE T AUDIO SPEECH, V15, P601, DOI 10.1109/TASL.2006.881678
   Gebru ID, 2018, IEEE T PATTERN ANAL, V40, P1086, DOI 10.1109/TPAMI.2017.2648793
   Gebru ID, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P702, DOI 10.1109/ICCVW.2015.96
   Gebru ID, 2015, LECT NOTES COMPUT SC, V9237, P143, DOI 10.1007/978-3-319-22482-4_17
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   Heuer Michael, 2011, 2011 International Conference on Multimedia Technology, P6450
   Huiyu Zhou, 2007, 2007 First ACM/IEEE International Conference on Distributed Smart Cameras, P170
   Katsaggelos AK, 2015, P IEEE, V103, P1635, DOI 10.1109/JPROC.2015.2459017
   Kilic V, 2016, IEEE T MULTIMEDIA, V18, P2417, DOI 10.1109/TMM.2016.2599150
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   Kirchmaier U, 2011, INFORM FUSION, V12, P275, DOI 10.1016/j.inffus.2010.06.005
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177
   Lathoud G, 2005, INT CONF ACOUST SPEE, P265
   Liu QJ, 2018, IEEE T MULTIMEDIA, V20, P1767, DOI 10.1109/TMM.2017.2777671
   Liu Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4304, DOI 10.1109/ICASSP.2018.8461791
   Lourakis MIA, 2009, ACM T MATH SOFTWARE, V36, DOI 10.1145/1486525.1486527
   Mostefa D, 2007, LANG RESOUR EVAL, V41, P389, DOI 10.1007/s10579-007-9054-4
   Nickel K., 2005, Proceedings of the ACM International Conference on Multimodal Interfaces, P61
   Omologo M, 1997, IEEE T SPEECH AUDI P, V5, P288, DOI 10.1109/89.568735
   Qian XY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3071, DOI 10.1109/ICASSP.2018.8461323
   Qian XY, 2017, INT CONF ACOUST SPEE, P2896, DOI 10.1109/ICASSP.2017.7952686
   REID DB, 1979, IEEE T AUTOMAT CONTR, V24, P843, DOI 10.1109/TAC.1979.1102177
   Shivappa ST, 2010, P IEEE, V98, P1692, DOI 10.1109/JPROC.2010.2057231
   Shivappa ST, 2010, IEEE J-STSP, V4, P882, DOI 10.1109/JSTSP.2010.2057890
   Taj M, 2009, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2009.4960384
   Talantzis F, 2008, IEEE T SYST MAN CY B, V38, P799, DOI 10.1109/TSMCB.2008.922063
   Tian Y, 2015, APPL ACOUST, V89, P71, DOI 10.1016/j.apacoust.2014.09.004
   Wu B, 2007, INT J COMPUT VISION, V75, P247, DOI 10.1007/s11263-006-0027-7
   Wu X, 2018, IEEE T INF FOREN SEC, V13, P2884, DOI 10.1109/TIFS.2018.2833032
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
   Zhou HY, 2008, IEEE J-STSP, V2, P503, DOI 10.1109/JSTSP.2008.2001429
   Zotkin D, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P20, DOI 10.1109/EVENT.2001.938862
   Zotkin DN, 2002, EURASIP J APPL SIG P, V2002, P1154, DOI 10.1155/S1110865702206058
NR 69
TC 35
Z9 36
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2576
EP 2588
DI 10.1109/TMM.2019.2902489
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400012
DA 2024-07-18
ER

PT J
AU Cui, XR
   Wang, D
   Wang, ZJ
AF Cui, Xinrui
   Wang, Dan
   Wang, Z. Jane
TI Multi-Scale Interpretation Model for Convolutional Neural Networks:
   Building Trust Based on Hierarchical Interpretation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Model interpretability; multi-scale interpretation; convolutional neural
   networks; model-agnostic
ID RELEVANCE FEEDBACK
AB With the rapid development of deep learning models, their performances in various tasks have improved; meanwhile, their increasingly intricate architectures make them difficult to interpret. To tackle this challenge, model interpretability is essential and has been investigated in a wide range of applications. For end users, model interpretability can he used to build trust in the deployed machine learning models. For practitioners, interpretability plays a critical role in model explanation, model validation, and model improvement to develop a faithful model. In this paper, we propose a novel Multi-scale Interpretation (MINT) model for convolutional neural networks using both the perturbation-based and the gradient-based interpretation approaches. It learns the class-discriminative interpretable knowledge from the multi-scale perturbation of feature information in different layers of deep networks. The proposed MINT model provides the coarse-scale and the fine-scale interpretations for the attention in the deep layer and specific features in the shallow layer, respectively. Experimental results show that the MINT model presents the class-discriminative interpretation of the network decision and explains the significance of the hierarchical network structure.
C1 [Cui, Xinrui; Wang, Dan; Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia
RP Wang, D (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM xinruic@ece.ubc.ca; danw@ece.ubc.ca; zjanew@ece.ubc.ca
RI Cui, Xinrui/KPB-0947-2024
OI Cui, Xinrui/0000-0001-9641-7801; Wang, Dan/0000-0001-6374-0418
FU Canadian Natural Sciences and Engineering Research Council; Four-Year
   Doctoral Fellowship; International Doctoral Fellowship at the University
   of British Columbia
FX This work was supported in part by the Canadian Natural Sciences and
   Engineering Research Council and in part by the Four-Year Doctoral
   Fellowship and the International Doctoral Fellowship at the University
   of British Columbia. The guest editor coordinating the review of this
   manuscript and approving it for publication was Prof. Qing Fang.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2014, P INT C LEARN REPR W
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2015, P INT C LEARN REPR W
   [Anonymous], ARXIV190202497
   Doulamis N, 2006, SIGNAL PROCESS-IMAGE, V21, P334, DOI 10.1016/j.image.2005.11.006
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Jackson P., 1986, Introduction to Expert Systems
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Malinowski M, 2017, INT J COMPUT VISION, V125, P110, DOI 10.1007/s11263-017-1038-2
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 21
TC 7
Z9 8
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2263
EP 2276
DI 10.1109/TMM.2019.2902099
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200009
DA 2024-07-18
ER

PT J
AU Yu, LY
   Yu, J
   Ling, Q
AF Yu, Lingyun
   Yu, Jun
   Ling, Qiang
TI BLTRCNN-Based 3-D Articulatory Movement Prediction: Learning
   Articulatory Synchronicity From Both Text and Audio Inputs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; long short-term memory; bottleneck
   network; skip connection; articulatory movement prediction
ID NEURAL-NETWORKS; VIRTUAL HEAD; SPEECH; FEATURES; TRACKING; SYSTEM
AB Predicting articulatory movements from audio or text has diverse applications, such as speech visualization. Various approaches have been proposed to solve the acoustic-articulatory mapping problem. However, their precision is not high enough with only acoustic features available. Recently, deep neural network (DNN) has brought tremendous success in various fields, like speech recognition and image processing. To increase the accuracy, we propose a new network architecture for articulatory movement prediction with both text and audio inputs, called a bottleneck long-term recurrent convolutional neural network (BLTRCNN). To the best of our knowledge, it is the first time to predict articulatory movements based on DNN by fusing text and audio inputs. Our BLTRCNN consists of two networks. The first is the bottleneck network, generating a compact bottleneck features of text information for each frame independently. The second, including convolutional neural network, long short-term memory and skip connection, is called the long-term recurrent convolutional neural network (LTRCNN). LTRCNN is used for articulatory movement prediction when bottleneck features, acoustic features, and text features are integrated as inputs together. Experiments show that the proposed BLTRCNN achieves the state-of-the-art root-mean-square error (RMSE) 0.528 mm and the correlation coefficient 0.961. Moreover, we also demonstrate how text information complements acoustic features in this prediction task.
C1 [Yu, Lingyun; Yu, Jun; Ling, Qiang] Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Yu, J; Ling, Q (corresponding author), Univ Sci & Technol China, Dept Automat, Hefei 230027, Anhui, Peoples R China.
EM yuly@mail.ustc.edu.cn; harryjun@ustc.edu.cn; qling@ustc.edu.cn
OI Ling, Qiang/0000-0001-5688-4130
FU National Natural Science Foundation of China [U1736123, 61572450,
   61303150]; Anhui Provincial Natural Science Foundation [1708085QF138];
   Fundamental Research Funds for the Central Universities [WK2350000002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U1736123, Grant 61572450, and Grant
   61303150, in part by the Anhui Provincial Natural Science Foundation
   under Grant 1708085QF138, and in part by the Fundamental Research Funds
   for the Central Universities under Grant WK2350000002.
CR Abbas S, 2014, NEUROCOMPUTING, V142, P326, DOI 10.1016/j.neucom.2014.04.028
   Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Akgul YS, 1998, PROC CVPR IEEE, P298, DOI 10.1109/CVPR.1998.698623
   [Anonymous], HDB BRAIN THEORY NEU
   [Anonymous], 2014, P NIPS DEEP LEARN WO
   [Anonymous], 2010, THESIS
   [Anonymous], 2016, Residual Networks Behave Like Ensembles of Relatively Shallow Networks
   BAER T, 1987, Magnetic Resonance Imaging, V5, P1, DOI 10.1016/0730-725X(87)90477-2
   Benesty J., 2009, Noise Reduction in Speech Processing
   Bian JR, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION ENGINEERING (ICRAE), P431, DOI 10.1109/ICRAE.2017.8291424
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Deena S, 2013, IEEE T MULTIMEDIA, V15, P1755, DOI 10.1109/TMM.2013.2279659
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fanelli G, 2010, IEEE T MULTIMEDIA, V12, P591, DOI 10.1109/TMM.2010.2052239
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Goldberg Y., 2014, ARXIV
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Katz W, 2014, INTERSPEECH, P1174
   Ling Z.-H., 2010, P 11 ANN C INT SPEEC, P834
   Ling ZH, 2010, SPEECH COMMUN, V52, P834, DOI 10.1016/j.specom.2010.06.006
   Ling ZH, 2009, IEEE T AUDIO SPEECH, V17, P1171, DOI 10.1109/TASL.2009.2014796
   Liu P, 2015, INT CONF ACOUST SPEE, P4450, DOI 10.1109/ICASSP.2015.7178812
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Marcos S, 2010, INTERACT COMPUT, V22, P176, DOI 10.1016/j.intcom.2009.12.002
   Richmond K., 2009, P INTERSPEECH, P2835
   Richmond K, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P1516
   SCHONLE PW, 1987, BRAIN LANG, V31, P26, DOI 10.1016/0093-934X(87)90058-7
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shlizerman E, 2018, PROC CVPR IEEE, P7574, DOI 10.1109/CVPR.2018.00790
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   [唐郅 Tang Zhi], 2016, [自动化学报, Acta Automatica Sinica], V42, P923
   Toda T, 2008, SPEECH COMMUN, V50, P215, DOI 10.1016/j.specom.2007.09.001
   Uria B., 2011, P NIPS WORKSH DEEP L
   Uria B, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P866
   Wei Z., 2016, AS PAC SIGN INF PROC, P1
   Wu Z., 2016, SSW, P202, DOI DOI 10.21437/SSW.2016-33
   Wu ZZ, 2015, INT CONF ACOUST SPEE, P4460, DOI 10.1109/ICASSP.2015.7178814
   Xie L, 2007, IEEE T MULTIMEDIA, V9, P500, DOI 10.1109/TMM.2006.888009
   Yu D, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P244
   Yu J., 2013, OR COCOSDA HELD JOIN, P1
   Yu J, 2018, IEEE T CIRC SYST VID, V28, P920, DOI 10.1109/TCSVT.2016.2643504
   Yu J, 2015, IEEE T CYBERNETICS, V45, P977, DOI 10.1109/TCYB.2014.2341737
   Yu LY, 2017, MULTIMED TOOLS APPL, V76, P19241, DOI 10.1007/s11042-017-4578-0
   Zhao GY, 2009, IEEE T MULTIMEDIA, V11, P1254, DOI 10.1109/TMM.2009.2030637
   Zhu PC, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2192
NR 49
TC 11
Z9 11
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1621
EP 1632
DI 10.1109/TMM.2018.2887027
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700001
DA 2024-07-18
ER

PT J
AU Pang, SM
   Ma, J
   Xue, JR
   Zhu, JH
   Ordonez, V
AF Pang, Shanmin
   Ma, Jin
   Xue, Jianru
   Zhu, Jihua
   Ordonez, Vicente
TI Deep Feature Aggregation and Image Re-Ranking With Heat Diffusion for
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Heat equation; deep feature aggregation; re-ranking; image retrieval
ID DESCRIPTORS
AB Image retrieval based on deep convolutional features has demonstrated state-of-the-art performance in popular benchmarks. In this paper, we present a unified solution to address deep convolutional feature aggregation and image re-ranking by simulating the dynamics of heat diffusion. A distinctive problem in image retrieval is that repetitive or bursty features tend to dominate final image representations, resulting in representations less distinguishable. We show that by considering each deep feature as a heat source, our unsupervised aggregation method is able to avoid over-representation of bursty features. We additionally provide a practical solution for the proposed aggregation method and further show the efficiency of our method in experimental evaluation. Inspired by the aforementioned deep feature aggregation method, we also propose a method to re-rank a number of top ranked images for a given query image by considering the query as the heat source. Finally, we extensively evaluate the proposed approach with pre-trained and fine-tuned deep networks on common public benchmarks and show superior performance compared to previous work.
C1 [Pang, Shanmin; Ma, Jin; Zhu, Jihua] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Shaanxi, Peoples R China.
   [Xue, Jianru] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
   [Ordonez, Vicente] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; University of
   Virginia
RP Xue, JR (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Shaanxi, Peoples R China.
EM pangsm@xjtu.edu.cn; m799133891@stu.xjtu.edu.cn; jrxue@xjtu.edu.cn;
   zhujh@xjtu.edu.cn; vicente@virginia.edu
RI Pang, Shanmin/KBQ-6978-2024
OI Xue, Jianru/0000-0002-4994-9343
FU National Key Research and Development Plan [2016YFB1001004]; National
   Natural Science Foundation of China [61603289]; China Postdoctoral
   Science Foundation [2016M602823]; Fundamental Research Funds for the
   Central Universities [xjj2017118]
FX This work was supported in part by the National Key Research and
   Development Plan 2016YFB1001004, in part by the National Natural Science
   Foundation of China under Grant 61603289, in part by the China
   Postdoctoral Science Foundation under Grant 2016M602823, and in part by
   the Fundamental Research Funds for the Central Universities xjj2017118.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Zhu Liu.
CR [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, ARXIV170200338
   [Anonymous], 2013, P BRIT MACH VIS C
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Babenko A, 2015, IEEE I CONF COMP VIS, P1269, DOI 10.1109/ICCV.2015.150
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Cao JW, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P456, DOI 10.1145/2964284.2967262
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chen SH, 2016, PATTERN RECOGN, V60, P2, DOI 10.1016/j.patcog.2016.05.016
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Do TT, 2018, IEEE T PATTERN ANAL, V40, P626, DOI 10.1109/TPAMI.2017.2686861
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   Egozi A, 2010, IEEE T IMAGE PROCESS, V19, P1319, DOI 10.1109/TIP.2010.2040448
   Furuya T, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P171, DOI 10.1145/2671188.2749380
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Iscen A, 2018, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR.2018.00796
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Karpushin M, 2016, IEEE T MULTIMEDIA, V18, P1762, DOI 10.1109/TMM.2016.2590305
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Lu S, 2014, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2014.357
   Mikulik A, 2013, INT J COMPUT VISION, V103, P163, DOI 10.1007/s11263-012-0600-1
   Murray N, 2017, IEEE T PATTERN ANAL, V39, P1797, DOI 10.1109/TPAMI.2016.2615621
   Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374
   Pang SM, 2018, PATTERN RECOGN, V83, P150, DOI 10.1016/j.patcog.2018.05.010
   Pang SM, 2018, SIGNAL PROCESS-IMAGE, V63, P1, DOI 10.1016/j.image.2018.01.004
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Philbin J., 2008, PROC 21TH IEEE C COM, P1
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Simonyan K., 2014, 14091556 ARXIV
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tolias G., 2016, Conference Track Proceedings,
   Tolias G, 2014, PATTERN RECOGN, V47, P3466, DOI 10.1016/j.patcog.2014.04.007
   Tolias G, 2013, IEEE I CONF COMP VIS, P1401, DOI 10.1109/ICCV.2013.177
   Hoang T, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1600, DOI 10.1145/3123266.3123417
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Weickert J., 1998, ANISOTROPIC DIFFUSIO, V1
   Xu J., 2018, 2018 IEEE 19th Workshop on Control and Modeling for Power Electronics (COMPEL), P1, DOI [10.1109/COMPEL.2018.8459937, DOI 10.1109/COMPEL.2018.8459937, 10.1109/COMPEL.2018.8460140]
   Yang XW, 2009, PROC CVPR IEEE, P357, DOI 10.1109/CVPRW.2009.5206844
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang JY, 2010, PROC CVPR IEEE, P2125, DOI 10.1109/CVPR.2010.5539891
NR 54
TC 28
Z9 28
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1513
EP 1523
DI 10.1109/TMM.2018.2876833
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400014
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, HT
   Mao, ZD
   Zhang, YD
   Deng, H
   Yan, CG
   Chen, ZN
AF Xie, Hongtao
   Mao, Zhendong
   Zhang, Yongdong
   Deng, Han
   Yan, Chenggang
   Chen, Zhineng
TI Double-Bit Quantization and Index Hashing for Nearest Neighbor Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Nearest neighbor search; double-bit quantization; double-bit index
   hashing; weighted distance measurement; binary embedding
ID ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH; BINARY-CODES; SPACE
AB Asbinary code is storage efficient and fast to compute, it has become a trend to compact real-valued data to binary codes for the nearest neighbors (NN) search in a large-scale database. However, the use of binary code for the NN search leads to low retrieval accuracy. To increase the discriminability of the binary codes of existing hash functions, in this paper, we propose a framework of double-bit quantization and index hashing for an effective NN search. The main contributions of our framework are: first, a novel double-bit quantization (DBQ) is designed to assign more bits to each dimension for higher retrieval accuracy; second, a double-bit index hashing (DBIH) is presented to efficiently index binary codes generated by DBQ; and third, a weighted distance measurement for DBQ binary codes is put forward to re-rank the search results from DBIH. The empirical results on three benchmark databases demonstrate the superiority of our framework over existing approaches in terms of both retrieval accuracy and query efficiency. Specifically, we observe an absolute improvement on precision of 10%-25% in most cases and the query speed increases over 30 times compared to traditional binary embedding methods and linear scan, respectively.
C1 [Xie, Hongtao; Mao, Zhendong; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
   [Deng, Han] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Inst Informat & Control, Hangzhou 310000, Zhejiang, Peoples R China.
   [Chen, Zhineng] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; Institute of Information
   Engineering, CAS; Hangzhou Dianzi University; Chinese Academy of
   Sciences; Institute of Automation, CAS
RP Mao, ZD (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Anhui, Peoples R China.
EM htxie@ustc.edu.cn; maozhendong2008@gmail.com; zhyd73@ustc.edu.cn;
   denghan@iie.ac.cn; cgyan@hdu.edu.cn; zhineng.chen@ia.ac.cn
RI chen, zhineng/AAD-6723-2020
FU National Key Research and Development Program of China [2017YFC0820600];
   National Defense Science and Technology Fund for Distinguished Young
   Scholars [2017JCJQ-ZQ-022]; National Nature Science Foundation of China
   [61525206, 61771468, 61772526, 61502477]; Youth Innovation Promotion
   Association Chinese Academy of Sciences [2017209]
FX This work was supported in part by the National Key Research and
   Development Program of China (2017YFC0820600); in part by the National
   Defense Science and Technology Fund for Distinguished Young Scholars
   under Grant 2017JCJQ-ZQ-022; in part by the National Nature Science
   Foundation of China under Grants 61525206, 61771468, 61772526, and
   61502477; and in part by the Youth Innovation Promotion Association
   Chinese Academy of Sciences (2017209). The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Benoit Huet.
CR Cruz-Roa AA, 2013, LECT NOTES COMPUT SC, V8150, P403, DOI 10.1007/978-3-642-40763-5_50
   [Anonymous], AAAI C ART INT
   [Anonymous], 2002, INFORM FUSION
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], EUR C COMP VIS
   [Anonymous], 2009, NEURIPS
   Bawa M, 2005, Proceedings of the 14th International Conference on World Wide Web-WWW'05, P651, DOI [DOI 10.1145/1060745.1060840, 10.1145/1060745.1060840]
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Erhan D., 2016, P IEEE C COMP VIS PA, P2155
   Fei-Fei L., 2004, CVPR WORKSH GEN MOD, V106, P178, DOI DOI 10.1109/CVPR.2004.383
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gordo A, 2014, IEEE T PATTERN ANAL, V36, P33, DOI 10.1109/TPAMI.2013.101
   Guttman A., 1984, SIGMOD Record, V14, P47, DOI 10.1145/971697.602266
   Herranz L, 2016, PROC CVPR IEEE, P571, DOI 10.1109/CVPR.2016.68
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Jégou H, 2011, INT CONF ACOUST SPEE, P861
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jiang YG, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3184745
   Jiang YG, 2016, IEEE T MULTIMEDIA, V18, P2161, DOI 10.1109/TMM.2016.2614233
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Katayama N., 1997, SIGMOD Record, V26, P369, DOI 10.1145/253262.253347
   Kim S, 2015, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2015.7298739
   KONG S, 2012, PROC ACM SIGIR CONF, P45
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Megrhi S., 2013, P ADV MULT INF PROC, P375
   Moran S., 2013, P 51 ANN M ASS COMP, P753
   Moran S., 2013, P ACM SIGIR C RES DE, P1009
   Muja M., 2012, 2012 Canadian Conference on Computer and Robot Vision, P404, DOI 10.1109/CRV.2012.60
   Murillo A. C., 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2196, DOI 10.1109/ICCVW.2009.5457552
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Tuytelaars T, 2007, IEEE I CONF COMP VIS, P754
   Wang HB, 2016, IEEE T MULTIMEDIA, V18, P1579, DOI 10.1109/TMM.2016.2569412
   Wang Z, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2298
   Weiss Y., 2008, P NIPS, V282, P1753
   Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25
   Xie H., 2011, 2011 INT C ELECT MAC, P1
   Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031
   Xie HT, 2011, IEEE T MULTIMEDIA, V13, P1319, DOI 10.1109/TMM.2011.2167224
   Xiong C., 2014, P 2014 SIAM INT CONF, P172
   Yahiaoui I, 2006, LECT NOTES COMPUT SC, V4261, P357
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yan CC, 2015, FRONT COMPUT SCI-CHI, V9, P741, DOI 10.1007/s11704-015-4192-0
   Yang J, 2004, IEEE T PATTERN ANAL, V26, P131, DOI 10.1109/TPAMI.2004.1261097
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Zezula P., 2006, Similarity search
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, P2212, DOI 10.1109/TIP.2015.2419074
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhu H., 2014, IEEE 2014 P 3 INT C, P1
NR 54
TC 21
Z9 22
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1248
EP 1260
DI 10.1109/TMM.2018.2872898
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600014
DA 2024-07-18
ER

PT J
AU Kim, S
   Kim, C
AF Kim, Seohyang
   Kim, Chongkwon
TI XMAS: An Efficient Mobile Adaptive Streaming Scheme Based on Traffic
   Shaping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE ABR; adaptive streaming; bandwidth estimation; DASH; QoE; TCP flow
   control; traffic shaping
ID RATE ALLOCATION; ADAPTATION; DELIVERY
AB Adaptive video streaming can provide adequate Quality of Experience by dynamically adjusting video rates in responding to fluctuating service conditions. However, adaptive video streaming shows dismal performances in wireless networks where several players share a wireless bottleneck link. This paper proposes a novel video rate selection scheme called XMAS for efficient video streaming in wireless networks. XMAS consists of two components: an available bandwidth estimation part and a video rate selection part. To redress the problem of inaccurate bandwidth estimation due to peculiar ON-OFF transmission patterns of mobile video streaming, we devised a novel client-based traffic shaping scheme that effectively throttles server's packet transmission. Equipped with accurate bandwidth estimates, XMAS determines target transmission rates considering playback buffer levels. We implemented XMAS on Linux and performed rigorous experiments to analyze its behavior and performance. Our performance results showed that XMAS achieves up to 20% increase in average video rates while reducing rebuffer rates significantly.
C1 [Kim, Seohyang; Kim, Chongkwon] Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151742, South Korea.
   [Kim, Chongkwon] Seoul Natl Univ, Ind Engn, Seoul 151742, South Korea.
C3 Seoul National University (SNU); Seoul National University (SNU)
RP Kim, C (corresponding author), Seoul Natl Univ, Sch Comp Sci & Engn, Seoul 151742, South Korea.; Kim, C (corresponding author), Seoul Natl Univ, Ind Engn, Seoul 151742, South Korea.
EM shkim@popeye.snu.ac.kr; ckim@snu.ac.kr
OI Kim, Chong-Kwon/0000-0002-9492-6546
FU National Research Foundation of Korea (NRF) - Korean Government (MSIP)
   [2016R1A5A1012966]; MSIP support program [IITP-2018-2015-0-00378]; IITP
   grant - Korea government (MSIT) [2015-0-00557]; Institute for Industrial
   Systems Innovation of Seoul National University
FX This work was supported in part by the National Research Foundation of
   Korea (NRF) Grant funded by the Korean Government (MSIP) (No.
   2016R1A5A1012966), in part by MSIP support program
   (IITP-2018-2015-0-00378) supervised by the IITP, in part by IITP grant
   funded by the Korea government (MSIT) (No. 2015-0-00557), and in part by
   the Institute for Industrial Systems Innovation of Seoul National
   University. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Liang Zhou.
CR Aguayo M, 2018, IEEE T MULTIMEDIA, V20, P1224, DOI 10.1109/TMM.2017.2764325
   Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   Akhshabi Saamer, 2013, Proceeding of the 23rd ACM Workshop on Network and Operating Systems Support for Digital Audio and Video-NOSSDAV'13
   Allman M, 2013, ACM SIGCOMM COMP COM, V43, P31
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], ITEC VIDEO DATASET
   [Anonymous], 2012, Proceedings of the 22nd international workshop on Network and Operating System Support for Digital Audio and Video
   [Anonymous], C3 INTERNET SCALE CO
   [Anonymous], 2013, P IEEE 20 INT PACK V
   [Anonymous], 2011, ACM IMC 11
   [Anonymous], WHIT PAP CISC VNI FO
   [Anonymous], GETMOBILE MOBILE COM
   [Anonymous], GLOB INT PHEN REP
   [Anonymous], PENSIEVE DRL BASED A
   [Anonymous], P ACM MULT MAY
   Argyriou A, 2015, IEEE T MULTIMEDIA, V17, P736, DOI 10.1109/TMM.2015.2408254
   Baik Eilwoo, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P1, DOI 10.1109/INFOCOM.2015.7218361
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Cangialosi F, 2015, IMC'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON INTERNET MEASUREMENT CONFERENCE, P289, DOI 10.1145/2815675.2815701
   Chan A, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P221
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Cho Y, 2008, IEEE T MULTIMEDIA, V10, P1419, DOI 10.1109/TMM.2008.2004901
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Das R, 2017, CONEXT'17: PROCEEDINGS OF THE 2017 THE 13TH INTERNATIONAL CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES, P376, DOI 10.1145/3143361.3143390
   Dobrian F, 2011, ACM SIGCOMM COMP COM, V41, P362, DOI 10.1145/2043164.2018478
   Flach T, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P468, DOI 10.1145/2934872.2934873
   Ge C, 2017, IEEE T MULTIMEDIA, V19, P2222, DOI 10.1109/TMM.2017.2735301
   Gettys J, 2011, IEEE INTERNET COMPUT, V15, P95, DOI 10.1109/MIC.2011.56
   Go Y, 2015, IEEE T MULTIMEDIA, V17, P1646, DOI 10.1109/TMM.2015.2451951
   Huang T.Y., 2012, P 2012 ACM C INT MEA, P225, DOI 10.1145/2398776.2398800
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Jiang H., 2005, Performance Evaluation Review, V33, P241, DOI 10.1145/1071690.1064240
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Joseph V, 2014, IEEE INFOCOM SER, P406, DOI 10.1109/INFOCOM.2014.6847963
   Konda V, 2009, IEEE INFOCOM SER, P1, DOI 10.1109/INFCOM.2009.5061900
   Kuschnig R., 2010, MMSYS, P157
   Lederer S., 2012, P 3 MULT SYST C, P89
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P965, DOI 10.1109/TMM.2017.2757761
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Manish Jain, 2005, Performance Evaluation Review, V33, P265, DOI 10.1145/1071690.1064242
   Mansy A., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, P214
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Mittal R, 2015, ACM SIGCOMM COMP COM, V45, P537, DOI 10.1145/2829988.2787510
   Nam H., 2016, IEEE INFOCOM 2016-The 35th Annual IEEE International Conference on Computer Communications, P1
   Narwaria M, 2012, IEEE T MULTIMEDIA, V14, P525, DOI 10.1109/TMM.2012.2190589
   Paul AK, 2016, IEEE T NETW SERV MAN, V13, P768, DOI 10.1109/TNSM.2016.2572212
   Qin Y., 2017, PROC IEEE C COMPUT C, P1
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song W, 2014, IEEE T MULTIMEDIA, V16, P738, DOI 10.1109/TMM.2014.2298217
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Toni Laura., 2014, Proceedings of_the_5th_ACM_Multimedia_Systems_Conference, P271
   Wang Z, 2015, IEEE T MULTIMEDIA, V17, P867, DOI 10.1109/TMM.2015.2425216
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zaki Y, 2015, ACM SIGCOMM COMP COM, V45, P509, DOI 10.1145/2829988.2787498
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhou L, 2017, IEEE T CIRC SYST VID, V27, P84, DOI 10.1109/TCSVT.2016.2539698
NR 65
TC 18
Z9 20
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 442
EP 456
DI 10.1109/TMM.2018.2856626
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400014
DA 2024-07-18
ER

PT J
AU Kim, HR
   Kim, YS
   Kim, SJ
   Lee, IK
AF Kim, Hye-Rin
   Kim, Yeong-Seok
   Kim, Seon Joo
   Lee, In-Kwon
TI Building Emotional Machines: Recognizing Image Emotions Through Deep
   Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion prediction; image emotion; deep network
ID EXPRESSION RECOGNITION; FEATURES; AROUSAL; VALENCE; COLOR
AB An image is a very effective tool for conveying emotions. Many researchers have investigated emotions in images by using various features extracted from images. In this paper, we focus on two high-level features, the object and the background, and assume that the semantic information in images is a good cue for predicting emotions. An object is one of the most important elements that define an image, and we discover through experiments that there is a high correlation between the objects and emotions in images in most cases. Even with the same object, there may be slight differences in emotion due to different backgrounds, and we use the semantic information of the background to improve the prediction performance. By combining the different levels of features, we build an emotion-based feedforward deep neural network that produces the emotion values of a given image. The output emotion values in our framework are continuous values in two-dimensional space (valence and arousal), which are more effective than using a small number of emotion categories to describe emotions. Experiments confirm the effectiveness of our network in predicting the emotions of images.
C1 [Kim, Hye-Rin; Kim, Yeong-Seok; Kim, Seon Joo; Lee, In-Kwon] Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
C3 Yonsei University
RP Lee, IK (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul 03722, South Korea.
EM rinee86@gmail.com; kys71015@gmail.com; seonjookim@yonsei.ac.kr;
   iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022
OI Lee, In-Kwon/0000-0002-1534-1882
FU Samsung Research Funding Center of Samsung Electronics [SRFC-IT1601-04]
FX This work was supported by the Samsung Research Funding Center of
   Samsung Electronics under Project SRFC-IT1601-04.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Aftanas L I, 2004, Neurosci Behav Physiol, V34, P859, DOI 10.1023/B:NEAB.0000038139.39812.eb
   ALAMEDAPINEDA X, 2016, PROC CVPR IEEE, P5240, DOI DOI 10.1109/CVPR.2016.566
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], IMAGE VIS COMPUT
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   [Anonymous], P 18 ACM CONF MULT
   [Anonymous], 2014, Comput. Sci.
   [Anonymous], 1820, THEORIE ANAL PROBABI
   [Anonymous], IMAGE VIDEO PROCESSI
   [Anonymous], 1980, A circumplex model of affect
   Borth D., 2013, P 21 ACM INT C MULTI, P223, DOI 10.1145/2502081.2502282
   BRADLEY MM, 1994, J BEHAV THER EXP PSY, V25, P49, DOI 10.1016/0005-7916(94)90063-9
   Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980
   Chanel G, 2006, LECT NOTES COMPUT SC, V4105, P530
   Cheng ZZ, 2015, IEEE I CONF COMP VIS, P415, DOI 10.1109/ICCV.2015.55
   Csurka G., 2010, P 7 IND C COMP VIS G, P298
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dibeklioglu H, 2015, IEEE T MULTIMEDIA, V17, P279, DOI 10.1109/TMM.2015.2394777
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Drucker H, 1997, ADV NEUR IN, V9, P155
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Fanelli G, 2010, IEEE T MULTIMEDIA, V12, P591, DOI 10.1109/TMM.2010.2052239
   Han K, 2014, INTERSPEECH, P223
   Hanjalic A, 2006, IEEE SIGNAL PROC MAG, V23, P90, DOI 10.1109/MSP.2006.1621452
   Haykin S., 1994, NEURAL NETWORKS, V2
   Islam J, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P124, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.29
   Kim HR, 2016, COMPUT GRAPH FORUM, V35, P209, DOI 10.1111/cgf.13018
   Kolcz A., 2004, ACM SIGKDD EXPLORATI, V6, P1, DOI [10.2973/odp.proc.ir.207.2004, DOI 10.1145/1007730.1007733]
   Lang P. J., 2005, A6 U FLOR CTR RES PS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu NN, 2011, LECT NOTES COMPUT SC, V6974, P195, DOI 10.1007/978-3-642-24600-5_23
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Longadge R, 2013, INT J COMPUTER SCI N, P1707, DOI [DOI 10.48550/ARXIV.1305.1707, 10.48550/arxiv.1305.1707]
   Lu Xin, 2012, Proc ACM Int Conf Multimed, V2012, P229, DOI 10.1145/2393347.2393384
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   OSGOOD CE, 1952, PSYCHOL BULL, V49, P197, DOI 10.1037/h0055737
   Peng KC, 2015, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2015.7298687
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Sebe N, 2006, INT C PATT RECOG, P1136
   Shan CF, 2009, IMAGE VISION COMPUT, V27, P803, DOI 10.1016/j.imavis.2008.08.005
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Tawari A, 2010, IEEE T MULTIMEDIA, V12, P502, DOI 10.1109/TMM.2010.2058095
   VALDEZ P, 1994, J EXP PSYCHOL GEN, V123, P394, DOI 10.1037/0096-3445.123.4.394
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Warriner AB, 2013, BEHAV RES METHODS, V45, P1191, DOI 10.3758/s13428-012-0314-x
   Wu Zifeng, 2016, CoRR
   Xu C., 2014, Visual sentiment prediction with deep convolutional neural networks
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105
   Zeng ZH, 2008, IEEE T MULTIMEDIA, V10, P570, DOI 10.1109/TMM.2008.921737
   Zhang H, 2011, LECT NOTES COMPUT SC, V7014, P413, DOI 10.1007/978-3-642-24800-9_38
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P47, DOI 10.1145/2647868.2654930
NR 60
TC 45
Z9 50
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2980
EP 2992
DI 10.1109/TMM.2018.2827782
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Matsui, Y
   Yamasaki, T
   Aizawa, K
AF Matsui, Yusuke
   Yamasaki, Toshihiko
   Aizawa, Kiyoharu
TI PQTable: Nonexhaustive Fast Search for Product-Quantized Codes Using
   Hash Tables
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Product quantization; approximate nearest neighbor search; hash table
AB In this paper, we propose a product quantization table (PQTable)-a fast search method for product-quantized codes via hash tables. An identifier of each database vector is associated with the slot of a hash table by using its PQ-code as a key. For querying, an input vector is PQ-encoded and hashed, and the items associated with that code are then retrieved. The proposed PQTable produces the same results as a linear PQ scan, and is 10(2) -10(5) times faster. Although the state-of-the-art performance can be achieved by previous inverted-indexing-based approaches, such methods require manually designed parameter setting and significant training; our PQTable is free of these limitations, and therefore offers a practical and effective solution for real-world problems. Specifically, when the vectors are highly compressed, our PQTable achieves one of the fastest search performances on a single CPU to date with significantly efficient memory usage (0.059-ms per query over 10(9) data points with just 5.5-GB memory consumption). Finally, we show that our proposed PQTable can naturally handle the codes of an optimized product quantization (OPQTable).
C1 [Matsui, Yusuke] Natl Inst Informat, Tokyo 1000003, Japan.
   [Yamasaki, Toshihiko; Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138654, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; University of Tokyo
RP Matsui, Y (corresponding author), Natl Inst Informat, Tokyo 1000003, Japan.
EM matsui@nii.ac.jp; yamasaki@hal.t.u-tokyo.ac.jp;
   aizawa@hal.t.u-tokyo.ac.jp
FU Japan Society for the Promotion of Science KAKENHI [16H07411, 15K12025];
   JST ACT-I [JPMJPR16UO]; Grants-in-Aid for Scientific Research [16H07411,
   15K12025] Funding Source: KAKEN
FX This work was supported in part by the Japan Society for the Promotion
   of Science KAKENHI Grant 16H07411 and Grant 15K12025 and in part by the
   JST ACT-I Grant JPMJPR16UO. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Zhu Li.
CR André F, 2015, PROC VLDB ENDOW, V9, P288
   [Anonymous], 2017, BILLION SCALE SIMILA
   [Anonymous], 2016, ARXIV160600185
   [Anonymous], ARXIV14041831
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226
   Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052
   Babenko A, 2015, IEEE T PATTERN ANAL, V37, P1247, DOI 10.1109/TPAMI.2014.2361319
   Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   Cheng J, 2014, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2014.8
   Douze M, 2016, LECT NOTES COMPUT SC, V9906, P785, DOI 10.1007/978-3-319-46475-6_48
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Greene D., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P722, DOI 10.1109/SFCS.1994.365720
   He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378
   Heo JP, 2016, PROC CVPR IEEE, P2009, DOI 10.1109/CVPR.2016.221
   Heo JP, 2014, PROC CVPR IEEE, P2139, DOI 10.1109/CVPR.2014.274
   Iwamura M, 2013, IEEE I CONF COMP VIS, P3535, DOI 10.1109/ICCV.2013.439
   Jain H, 2016, LECT NOTES COMPUT SC, V9911, P681, DOI 10.1007/978-3-319-46478-7_42
   Jégou H, 2011, INT CONF ACOUST SPEE, P861
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9
   Matsui Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1725, DOI 10.1145/3123266.3123430
   Matsui Y, 2015, IEEE I CONF COMP VIS, P1940, DOI 10.1109/ICCV.2015.225
   Ning QQ, 2017, IEEE T MULTIMEDIA, V19, P586, DOI 10.1109/TMM.2016.2625260
   Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Ong EJ, 2016, PROC CVPR IEEE, P2000, DOI 10.1109/CVPR.2016.220
   Ozan EC, 2016, IEEE T KNOWL DATA EN, V28, P1722, DOI 10.1109/TKDE.2016.2535287
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Stein C., 2010, DISCRETE MATH COMPUT
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Wang JC, 2014, KEY ENG MATER, V579-580, P517, DOI 10.4028/www.scientific.net/KEM.579-580.517
   Wang JF, 2015, IEEE T KNOWL DATA EN, V27, P180, DOI 10.1109/TKDE.2014.2324592
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang XJ, 2016, PROC CVPR IEEE, P2018, DOI 10.1109/CVPR.2016.222
   Wieschollek P, 2016, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2016.223
   Xia Y, 2013, IEEE I CONF COMP VIS, P3416, DOI 10.1109/ICCV.2013.424
   Zhang T., 2014, P 31 INT C MACH LEAR
   Zhang T, 2016, PROC CVPR IEEE, P2036, DOI 10.1109/CVPR.2016.224
   Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085
NR 44
TC 6
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1809
EP 1822
DI 10.1109/TMM.2017.2774009
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, T
   Fan, XP
   Zhao, DB
   Xiong, RQ
   Gao, W
AF Zhang, Tao
   Fan, Xiaopeng
   Zhao, Debin
   Xiong, Ruiqin
   Gao, Wen
TI Hybrid Intraprediction Based on Local and Nonlocal Correlations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE High efficient video coding (HEVC); intra coding; intra prediction;
   template matching prediction
ID INTRA PREDICTION
AB In the latest video coding standard, namely, high-efficiency video coding (HEVC), intra coding efficiency is significantly improved by a quadtree partition structure and more intra prediction modes. For intra coding, 35 intra modes are employed including 33 angular intra prediction (AIP) modes that are effective for blocks with strong directions, and a planar mode and a dc mode that are used to predict smooth regions. However, intra prediction in HEVC still cannot handle complicated blocks well. To deal with this problem, this paper proposes a hybrid intra prediction method to improve the intra prediction efficiency. The proposed hybrid infra prediction method consists of three parts: adaptive template matching prediction (ATMP) by exploring nonlocal correlation, combined local and nonlocal prediction by exploring both local correlation for AIP and nonlocal correlation for ATMP, and combined neighboring modes prediction that can generate smooth prediction by exploring more local correlations. Experimental results suggest that the proposed hybrid intra prediction method achieves 2.8% RD-rate reduction on average for luma compared to HEVC reference software HM-14.0 under all intra main configurations. The gain can be up to 6.9%. When integrated into joint exploration model -1.0, the proposed method still can achieve 1.2% RD-rate reduction for luma.
C1 [Zhang, Tao] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Zhang, Tao] Tencent, Beijing 100080, Peoples R China.
   [Fan, Xiaopeng; Zhao, Debin] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Xiong, Ruiqin; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100080, Peoples R China.
C3 Harbin Institute of Technology; Tencent; Harbin Institute of Technology;
   Peking University
RP Zhao, DB (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM taozhang.hit@hotmail.com; fxp@hit.edu.cn; dbzhao@hit.edu.cn;
   rqxiong@pku.edu.cn; wgao@pku.edu.cn
RI Zhao, Debin/JEP-0204-2023; Zhang, Tao/V-5392-2019
OI Xiong, Ruiqin/0000-0001-9796-0478
FU Major State Basic Research Development Program of China (973 Program)
   [2015CB351804]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2015CB351804.
CR Alshina E., 2015, VCEGAZ05 ITUT SG16Q6
   [Anonymous], 2012, Document JCTVC-K1100
   Bjontegaard G., 2001, Document VCEG-M33
   Cao XR, 2013, IEEE T IMAGE PROCESS, V22, P790, DOI 10.1109/TIP.2012.2222904
   Chen C, 2017, IEEE T CIRC SYST VID, V27, P1727, DOI 10.1109/TCSVT.2016.2556478
   Chen HM, 2016, IEEE T IMAGE PROCESS, V25, P3671, DOI 10.1109/TIP.2016.2573585
   Chen JJ, 2015, IEEE INT CONF COMM, P1, DOI [10.1109/ChinaSIP.2015.7230350, 10.1109/ICCW.2015.7247066]
   Chen Y, 2013, INT CONF ACOUST SPEE, P1734, DOI 10.1109/ICASSP.2013.6637949
   Gabriellini A, 2011, IEEE J-STSP, V5, P1282, DOI 10.1109/JSTSP.2011.2165200
   Kim I., 2014, P1002 JCTVC
   Lainema J, 2012, IEEE T CIRC SYST VID, V22, P1792, DOI 10.1109/TCSVT.2012.2221525
   Li SY, 2014, IEEE IMAGE PROC, P3146, DOI 10.1109/ICIP.2014.7025636
   Lucas Luis F R, 2016, IEEE Trans Image Process, V25, P4046, DOI 10.1109/TIP.2016.2582422
   Lucas LFR, 2015, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2015.7350973
   Qi XL, 2012, INT CONF ACOUST SPEE, P1217, DOI 10.1109/ICASSP.2012.6288107
   Said A., 2015, M37502 MPEG
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan TK, 2006, IEEE IMAGE PROC, P1693, DOI 10.1109/ICIP.2006.312685
   Tan TK, 2007, CONSUM COMM NETWORK, P405, DOI 10.1109/CCNC.2007.86
   Tao Zhang, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457833
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao W, 2015, IEEE T CIRC SYST VID, V25, P1830, DOI 10.1109/TCSVT.2015.2406199
   Xu J., 2013, JCTVCO0232 ITUT ISOI
   Xu JZ, 2016, IEEE T CIRC SYST VID, V26, P50, DOI 10.1109/TCSVT.2015.2478706
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Ye Y, 2008, IEEE IMAGE PROC, P2116, DOI 10.1109/ICIP.2008.4712205
   Yu S.-L., 2002, P 3 M JOINT VID TEAM
   Zhang L, 2011, J VIS COMMUN IMAGE R, V22, P687, DOI 10.1016/j.jvcir.2010.11.003
   Zhang T, 2017, IEEE T CIRC SYST VID, V27, P1714, DOI 10.1109/TCSVT.2016.2556518
   Zhao LP, 2016, IEEE T MULTIMEDIA, V18, P339, DOI 10.1109/TMM.2015.2512539
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
   Zhu WJ, 2015, IEEE T MULTIMEDIA, V17, P935, DOI 10.1109/TMM.2015.2428171
   Zhu WJ, 2014, IEEE T MULTIMEDIA, V16, P1316, DOI 10.1109/TMM.2014.2315782
NR 34
TC 14
Z9 14
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1622
EP 1635
DI 10.1109/TMM.2017.2775223
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100003
DA 2024-07-18
ER

PT J
AU Aguayo, M
   Bellido, L
   Lentisco, CM
   Pastor, E
AF Aguayo, Miguel
   Bellido, Luis
   Lentisco, Carlos M.
   Pastor, Encarna
TI DASH Adaptation Algorithm Based on Adaptive Forgetting Factor Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive streaming over HTTP; adaptive forgetting factor; mobile
   communication; multimedia content delivery; throughput estimation
ID HTTP; EXPERIENCE; QUALITY
AB The wide adoption of multimedia service-capable mobile devices, the availability of better networks with higher bandwidths, and the availability of platforms offering digital content has led to an increasing popularity of multimedia streaming services. However, multimedia streaming services can be subject to different factors that affect the quality perceived by the users, such as service interruptions or quality oscillations due to changing network conditions, particularly in mobile networks. Dynamic Adaptive Streaming over HTTP (DASH), leverages the use of content-distribution networks and the capabilities of the multimedia devices to allow multimedia players to dynamically adapt the quality of the media streaming to the available bandwidth and the device characteristics. While many elements of DASH are standardized, the algorithms providing the dynamic adaptation of the streaming are not. The adaptation is often based on the estimation of the throughput or a buffer control mechanism. In this paper, we present a new throughput estimation adaptation algorithm based on a statistical method named Adaptive Forgetting Factor (AFF). Using this method, the adaptation logic is able to react appropriately to the different conditions of different types of networks. A set of experiments with different traffic profiles show that the proposed algorithm improves video quality performance in both wired and wireless environments.
C1 [Aguayo, Miguel; Bellido, Luis; Lentisco, Carlos M.; Pastor, Encarna] Univ Politecn Madrid, Dept Telemat Syst Engn, E-28040 Madrid, Spain.
C3 Universidad Politecnica de Madrid
RP Aguayo, M (corresponding author), Univ Politecn Madrid, Dept Telemat Syst Engn, E-28040 Madrid, Spain.
EM aguayo@dit.upm.es; lbellido@dit.upm.es; clentisco@dit.upm.es;
   epastor@dit.upm.es
RI Aguayo, Miguel Angel/JCE-8353-2023; Bellido Triana, Luis/GZH-2187-2022;
   Pastor, Encarna/A-6449-2014
OI Pastor, Encarna/0000-0003-1840-5471; BELLIDO TRIANA,
   LUIS/0000-0001-9591-0928; Aguayo Ortuno, Miguel
   Angel/0000-0003-3242-9875
FU Spanish Ministry of Economy and Competitiveness [TEC2015-67834-R,
   TEC2015-71932-REDT]
FX This work was supported in part by the Spanish Ministry of Economy and
   Competitiveness in the context of the project GREDOS, reference
   TEC2015-67834-R (MINECO/FEDER, UE), and the project Elastic Networks,
   reference TEC2015-71932-REDT.
CR [Anonymous], 2013, 2013 20 INT PACK VID
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], 2014, 230091 ISOIEC
   [Anonymous], 2013, P IEEE 20 INT PACK V
   Blender Foundation, BIG BUCK BUNN MOV
   Bodenham D.A., 2014, THESIS
   Bodenham DA, 2013, INT CONF DAT MIN WOR, P311, DOI 10.1109/ICDMW.2013.114
   Bokani A, 2015, IEEE T MULTIMEDIA, V17, P2297, DOI 10.1109/TMM.2015.2494458
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   C. V. Forecast, 2016, CISCO PUBLIC INFORM, V9
   Cooper JE, 2000, MECH SYST SIGNAL PR, V14, P705, DOI 10.1006/mssp.2000.1322
   DASH Industry Forum, MPEG DASH REF PLAYER
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Fernández D, 2016, INT J ENG EDUC, V32, P2569
   Feuvre J. L., GPAC MULTIMEDIA OPEN
   Jain R., 1984, TR301 DEC RES
   Jeong U, 2014, INT J COMPUT SCI NET, V14, P22
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kim YH, 2013, ETRI J, V35, P27, DOI 10.4218/etrij.13.0111.0788
   Lai CF, 2013, IEEE T MULTIMEDIA, V15, P747, DOI 10.1109/TMM.2013.2240270
   Le HT, 2016, MOB INF SYST, V2016, DOI 10.1155/2016/2920850
   Lentisco CM, 2017, MOB INF SYST, V2017, DOI 10.1155/2017/1847538
   Lentisco CM, 2017, IEEE T MULTIMEDIA, V19, P173, DOI 10.1109/TMM.2016.2620605
   Lentisco CM, 2015, 2015 EUROPEAN CONFERENCE ON NETWORKS AND COMMUNICATIONS (EUCNC), P512, DOI 10.1109/EuCNC.2015.7194128
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Lin Q, 2014, IEEE GLOBE WORK, P243, DOI 10.1109/GLOCOMW.2014.7063438
   Merritt L., 2006, X264 HIGH PERFORMANC
   Miller K, 2015, IEEE T MULTIMEDIA, V17, P1309, DOI 10.1109/TMM.2015.2441002
   Ramamurthi V, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P727, DOI 10.1109/ICCNC.2015.7069436
   Ramamurthi V, 2013, IEEE GLOB COMM CONF, P1675, DOI 10.1109/GLOCOM.2013.6831314
   Sandvine Incorporated, 2016, SANDV GLOB INT PHEN
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Transparent End-to-End Packet-Switched Streaming Service (PSS), 2017, 26247 3GPP TS
   Thang TC, 2013, J COMMUN NETW-S KOR, V15, P635, DOI 10.1109/JCN.2013.000112
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 36
TC 18
Z9 18
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1224
EP 1232
DI 10.1109/TMM.2017.2764325
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400016
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Peng, YX
   Qi, JW
   Huang, X
   Yuan, YX
AF Peng, Yuxin
   Qi, Jinwei
   Huang, Xin
   Yuan, Yuxin
TI CCL: Cross-modal Correlation Learning With Multigrained Fusion by
   Hierarchical Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; fine-grained correlation; joint optimization;
   multi-task learning
ID REPRESENTATION; MODEL
AB Cross-modal retrieval has become a highlighted research topic for retrieval across multimedia data such as image and text. A two-stage learning framework is widely adopted by most existing methods based on deep neural network (DNN): The first learning stage is to generate separate representation for each modality, and the second learning stage is to get the cross-modal common representation. However, the existing methods have three limitations: 1) In the first learning stage, they only model intramodality correlation, but ignore intermodality correlation with rich complementary context. 2) In the second learning stage, they only adopt shallow networks with single-loss regularization, but ignore the intrinsic relevance of intramodality and intermodality correlation. 3) Only original instances are considered while the complementary fine-grained clues provided by their patches are ignored. For addressing the above problems, this paper proposes a cross-modal correlation learning (CCL) approach with multigrained fusion by hierarchical network, and the contributions are as follows: 1) In the first learning stage, CCL exploits multilevel association with joint optimization to preserve the complementary context from intramodality and intermodality correlation simultaneously. 2) In the second learning stage, a multitask learning strategy is designed to adaptively balance the intramodality semantic category constraints and intermodality pairwise similarity constraints. 3) CCL adopts multigrained modeling, which fuses the coarse-grained instances and fine-grained patches to make cross-modal correlation more precise. Comparing with 13 state-of-the-art methods on 6 widely-used cross-modal datasets, the experimental results show our CCL approach achieves the best performance.
C1 [Peng, Yuxin; Qi, Jinwei; Huang, Xin; Yuan, Yuxin] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM pengyuxin@pku.edu.cn
RI peng, yu/GXW-2071-2022; Peng, Yuxin/U-7376-2019
FU National Natural Science Foundation of China [61771025, 61371128,
   61532005]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61771025, Grant 61371128, and Grant 61532005. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marco Bertini. (Corresponding
   author: Yuxin Peng.)
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 2003, P ACM INT C MULT ACM
   [Anonymous], 2016, CORR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2009, P ACM INT C IM VID R
   [Anonymous], 2004, Advances in Neural Information Processing Systems
   [Anonymous], 2014, ACMMM
   Bredin H, 2007, INT CONF ACOUST SPEE, P233
   Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036
   Daras P, 2012, IEEE T MULTIMEDIA, V14, P734, DOI 10.1109/TMM.2011.2181343
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Gong YC, 2014, INT J COMPUT VISION, V106, P210, DOI 10.1007/s11263-013-0658-4
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Hu YQ, 2009, IEEE T MULTIMEDIA, V11, P1434, DOI 10.1109/TMM.2009.2032676
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y., 2010, Proc. ACM International Conference on Image and Video Re- trieval, P89, DOI DOI 10.1145/1816041.1816057
   Peng Y, 2016, P INT JOINT C ART IN, P3846
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Peng YX, 2016, IEEE T CIRC SYST VID, V26, P583, DOI 10.1109/TCSVT.2015.2400779
   Ranjan V, 2015, IEEE I CONF COMP VIS, P4094, DOI 10.1109/ICCV.2015.466
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Srivastava Neelam., 2012, The Postcolonial Gramsci, P1
   Sun Y, 2014, ADV NEUR IN, V27
   Tran TQN, 2016, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2016.225
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhai X., 2013, PROC 27 AAAI C ARTIF, P1198
   Zhai XH, 2014, IEEE T CIRC SYST VID, V24, P965, DOI 10.1109/TCSVT.2013.2276704
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhuang Y., 2013, P 27 AAAI C ART INT, P1070
   Znaidia A, 2012, INT C PATT RECOG, P1509
NR 48
TC 162
Z9 184
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 405
EP 420
DI 10.1109/TMM.2017.2742704
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Aytekin, C
   Possegger, H
   Mauthner, T
   Kiranyaz, S
   Bischof, H
   Gabbouj, M
AF Aytekin, Caglar
   Possegger, Horst
   Mauthner, Thomas
   Kiranyaz, Serkan
   Bischof, Horst
   Gabbouj, Moncef
TI Spatiotemporal Saliency Estimation by Spectral Foreground Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; foreground detection; spatiotemporal;
   saliency; spectral graph theory
ID VISUAL-ATTENTION; MODEL; IMAGE; SEGMENTATION; CUTS
AB We present a novel approach for spatiotemporal saliency detection by optimizing a unified criterion of color contrast, motion contrast, appearance, and background cues. To this end, we first abstract the video by temporal superpixels. Second, we propose a novel graph structure exploiting the saliency cues to assign the edge weights. The salient segments are then extracted by applying a spectral foreground detection method, quantum cuts, on this graph. We evaluate our approach on several public datasets for video saliency and activity localization to demonstrate the favorable performance of the proposed video quantum cuts compared to the state of the art.
C1 [Aytekin, Caglar; Gabbouj, Moncef] Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.
   [Possegger, Horst; Mauthner, Thomas; Bischof, Horst] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
   [Kiranyaz, Serkan] Qatar Univ, Dept Elect Engn, Doha 2713, Qatar.
C3 Tampere University; Graz University of Technology; Qatar University
RP Aytekin, C (corresponding author), Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.
EM caglar.aytekin@tut.fi; possegger@icg.tugraz.at; mauthner@icg.tugraz.at;
   mkiranyaz@qu.edu.qa; bischof@icg.tugraz.at; moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014; Kiranyaz, Serkan/AAK-1416-2021
OI Gabbouj, Moncef/0000-0002-9788-2323; Possegger,
   Horst/0000-0002-5427-9938; kiranyaz, serkan/0000-0003-1551-3397;
   Bischof, Horst/0000-0002-9096-6671
CR Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], 2011, P IEEE MTT S INT MIC
   [Anonymous], P ACM SIGGRAPH
   [Anonymous], INTRO QUANTUM MECH
   [Anonymous], P EUR C COMPUT VIS
   Aytekin Ç, 2015, IEEE IMAGE PROC, P1692, DOI 10.1109/ICIP.2015.7351089
   Aytekin Ç, 2014, INT C PATT RECOG, P112, DOI 10.1109/ICPR.2014.29
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chang J, 2013, PROC CVPR IEEE, P2051, DOI 10.1109/CVPR.2013.267
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Ekmekcioglu E, 2014, IEEE J-STSP, V8, P402, DOI 10.1109/JSTSP.2014.2313717
   Fang YM, 2014, IEEE T IMAGE PROCESS, V23, P3910, DOI 10.1109/TIP.2014.2336549
   Fukuchi K, 2009, IEEE INT CON MULTI, P638, DOI 10.1109/ICME.2009.5202577
   Gao DS, 2005, PROC CVPR IEEE, P282
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Itti L, 2005, PROC CVPR IEEE, P631
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Judd T., 2012, MIT CSAIL TR
   Klein DA, 2011, IEEE I CONF COMP VIS, P2214, DOI 10.1109/ICCV.2011.6126499
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Mancas M, 2011, IEEE IMAGE PROC, P229, DOI 10.1109/ICIP.2011.6116099
   Mauthner T, 2015, PROC CVPR IEEE, P2494, DOI 10.1109/CVPR.2015.7298864
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Nguyen TV, 2015, IEEE T CIRC SYST VID, V25, P77, DOI 10.1109/TCSVT.2014.2333151
   Nguyen TV, 2013, IEEE T MULTIMEDIA, V15, P1910, DOI 10.1109/TMM.2013.2272919
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Peters RJ, 2005, VISION RES, V45, P2397, DOI 10.1016/j.visres.2005.03.019
   Rahtu E, 2010, LECT NOTES COMPUT SC, V6315, P366, DOI 10.1007/978-3-642-15555-0_27
   Rodriguez M. D., 2008, PROC COMPUT VIS PATT, P1
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Singh Anurag, 2015, 4th International Conference on Pattern Recognition Applications and Methods (ICPRAM 2015). Proceedings, P201
   Sultani W, 2014, PROC CVPR IEEE, P764, DOI 10.1109/CVPR.2014.103
   Tong YB, 2011, COGN COMPUT, V3, P241, DOI 10.1007/s12559-010-9094-8
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Wen LY, 2015, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2015.7298835
   Xiao FY, 2016, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2016.107
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang LM, 2015, IEEE T MULTIMEDIA, V17, P1538, DOI 10.1109/TMM.2015.2451954
   Zhao JP, 2015, PROC CVPR IEEE, P3174, DOI 10.1109/CVPR.2015.7298937
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 48
TC 20
Z9 20
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 82
EP 95
DI 10.1109/TMM.2017.2713982
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700007
DA 2024-07-18
ER

PT J
AU Bernal, EA
   Yang, XT
   Li, Q
   Kumar, J
   Madhvanath, S
   Ramesh, P
   Bala, R
AF Bernal, Edgar A.
   Yang, Xitong
   Li, Qun
   Kumar, Jayant
   Madhvanath, Sriganesh
   Ramesh, Palghat
   Bala, Raja
TI Deep Temporal Multimodal Fusion for Medical Procedure Monitoring Using
   Wearable Sensors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action and activity recognition; deep learning; deep temporal fusion;
   egocentric vision; hand localization; medical procedures; multimodal
   fusion; wearable sensors
AB Process monitoring and verification have a wide range of uses in the medical and healthcare fields. Currently, such tasks are often carried out by a trained specialist, which makes them expensive, inefficient, and time-consuming. Recent advances in automated video-and multimodal-data-based action and activity recognition have made it possible to reduce the extent of manual intervention required to effectively carry out process supervision tasks. In this paper, we propose algorithms for automated egocentric human action and activity recognition from multimodal data, with a target application of monitoring and assisting a user perform a multistep medical procedure. We propose a supervised deep multimodal fusion framework that relies on concurrent processing of motion data acquired with wearable sensors and video data acquired with an egocentric or body-mounted camera. We demonstrate the effectiveness of the algorithm on a public multimodal dataset and conclude that automated process monitoring via the use of multiple heterogeneous sensors is a viable alternative to its manual counterpart. Furthermore, we demonstrate that the application of previously proposed adaptive sampling schemes to the video processing branch of the multimodal framework results in significant performance improvements.
C1 [Bernal, Edgar A.; Yang, Xitong; Li, Qun; Kumar, Jayant; Madhvanath, Sriganesh] PARC, Palo Alto, CA 94304 USA.
   [Bernal, Edgar A.] United Technol Res Ctr, Decis Support & Machine Intelligence, E Hartford, CT 06118 USA.
   [Yang, Xitong] Univ Maryland, Comp Sci, College Pk, MD 20742 USA.
   [Li, Qun] Microsoft Corp, Redmond, WA 98052 USA.
   [Kumar, Jayant] Univ Maryland, College Pk, MD 20740 USA.
   [Madhvanath, Sriganesh] Conduent Labs, Webster, NY 14580 USA.
   [Ramesh, Palghat; Bala, Raja] PARC, Data Analyt & Comp Vis Dept, Webster, NY 14580 USA.
C3 Raytheon Technologies; University System of Maryland; University of
   Maryland College Park; Microsoft; University System of Maryland;
   University of Maryland College Park
RP Bernal, EA (corresponding author), United Technol Res Ctr, Decis Support & Machine Intelligence, E Hartford, CT 06118 USA.
EM eabernal@gmail.com; yangxitong-bob@gmail.com; qunliuic@gmail.com;
   Jayant@umiacs.umd.edu; srig@acm.org; Palghat.Ramesh@parc.com;
   rajabala65@gmail.com
CR Amanatiadis A, 2011, MEAS SCI TECHNOL, V22, DOI 10.1088/0957-0233/22/11/114025
   American Association of Diabetes Educators, 2011, STRAT INS INJ THER D
   American Diabetes Association, 2004, Diabetes Care, V27 Suppl 1, pS106
   [Anonymous], 2012, P ICML REPR LEARN WO
   [Anonymous], 2011, J. Sensor Technol., DOI DOI 10.4236/JST.2011.12004.[73]S.D
   [Anonymous], 2014, Advances in neural information processing systems
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Babakanian A., 2012, MOBILE COMPUTING APP, V95, P32
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Banos O, 2014, SCI WORLD J, DOI 10.1155/2014/490824
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Beso A, 2005, PHARM WORLD SCI, V27, P182, DOI 10.1007/s11096-004-2270-8
   Chen B., 2010, P NIPS 2010 DEEP LEA
   Cheng Guangchun., 2015, CoRR
   Chollet F, 2015, KERAS
   Dollar P., 2005, VISUAL SURVEILLANCE, V14, P65, DOI DOI 10.1109/VSPETS.2005.1570899
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dosovitskiy A, 2014, ADV NEUR IN, V27
   Eyben F, 2011, INT CONF ACOUST SPEE, P5844
   Fathi A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3281, DOI 10.1109/CVPR.2011.5995444
   Fathi A, 2012, LECT NOTES COMPUT SC, V7572, P314, DOI 10.1007/978-3-642-33718-5_23
   Fathi A, 2011, IEEE I CONF COMP VIS, P407, DOI 10.1109/ICCV.2011.6126269
   Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872
   Gelogo Y. E., 2015, INT J SOFTW ENG ITS, V9, P195
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Grassi M, 2010, IEEE SENSOR, P1016, DOI 10.1109/ICSENS.2010.5690746
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Stacciarini TSG, 2009, REV LAT-AM ENFERM, V17, P474, DOI 10.1590/S0104-11692009000400007
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kanade T, 2012, P IEEE, V100, P2442, DOI 10.1109/JPROC.2012.2200554
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Ke Y, 2005, IEEE I CONF COMP VIS, P166
   Kerr J., 2016, AM J PREV MED, V44, P290
   Kourogi M, 2003, SECOND IEEE AND ACM INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY, PROCEEDINGS, P103, DOI 10.1109/ISMAR.2003.1240693
   Kourogi M, 2003, PROCEEDINGS OF THE IEEE INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INTEGRATION FOR INTELLIGENT SYSTEMS, P287, DOI 10.1109/MFI-2003.2003.1232672
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar Jayant, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P18, DOI 10.1109/CVPRW.2015.7301344
   Lahat D, 2015, P IEEE, V103, P1449, DOI 10.1109/JPROC.2015.2460697
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lanckriet GRG, 2004, BIOINFORMATICS, V20, P2626, DOI 10.1093/bioinformatics/bth294
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lebel Denis, 2010, Can J Hosp Pharm, V63, P323
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Lu Z, 2013, PROC CVPR IEEE, P2714, DOI 10.1109/CVPR.2013.350
   Matsuo K, 2014, IEEE COMPUT SOC CONF, P565, DOI 10.1109/CVPRW.2014.87
   McCandless T, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.30
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   O'Loughlin G., AM J PREVENTIVE MED, V44, P297
   Ogaki K., 2012, 2012 IEEE COMP SOC C, P1, DOI DOI 10.1109/CVPRW.2012.6239188
   Ordóñez FJ, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16010115
   Pirsiavash H, 2012, PROC CVPR IEEE, P2847, DOI 10.1109/CVPR.2012.6248010
   Potdar V, 2009, 2009 INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS: WAINA, VOLS 1 AND 2, P636, DOI 10.1109/WAINA.2009.192
   Prest A, 2012, IEEE T PATTERN ANAL, V34, P601, DOI 10.1109/TPAMI.2011.158
   REN XF, 2010, PROC CVPR IEEE, P3137, DOI DOI 10.1109/CVPR.2010.5540074
   Ren XF, 2009, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2009.5204360
   Ryoo MS, 2013, PROC CVPR IEEE, P2730, DOI 10.1109/CVPR.2013.352
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Sak H., 2014, CORR
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Silva A. R., 2016, AM J PREV MED, V44, P302
   Simonyan K, 2014, ADV NEUR IN, V27
   Soran B, 2015, IEEE I CONF COMP VIS, P4669, DOI 10.1109/ICCV.2015.530
   Spriggs EH, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2009.5204354
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stiefmeier T, 2006, TENTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P97
   Surendranath A., 2012, Asian Journal of Pharmaceutical and Clinical Research Vol, V5, P63
   Surie D, 2007, LECT NOTES COMPUT SC, V4611, P246
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Varma M., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1065, DOI DOI 10.1145/1553374.1553510
   Varshney U, 2007, MOBILE NETW APPL, V12, P113, DOI 10.1007/s11036-007-0017-1
   Vig E, 2012, LECT NOTES COMPUT SC, V7578, P84, DOI 10.1007/978-3-642-33786-4_7
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Xu J, 2015, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2015.7298836
   Ye GN, 2012, PROC CVPR IEEE, P3021, DOI 10.1109/CVPR.2012.6248032
   Yi WJ, 2014, INT CONF ELECTRO INF, P303, DOI 10.1109/EIT.2014.6871782
   Yu S, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-309
NR 79
TC 38
Z9 43
U1 3
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 107
EP 118
DI 10.1109/TMM.2017.2726187
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700009
DA 2024-07-18
ER

PT J
AU Li, HB
   Sun, J
   Xu, ZB
   Chen, LM
AF Li, Huibin
   Sun, Jian
   Xu, Zongben
   Chen, Liming
TI Multimodal 2D+3D Facial Expression Recognition With Deep Fusion
   Convolutional Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep fusion convolutional neural network (DF-CNN); facial expression
   recognition (FER); multimodal; textured three-dimensional (3D) face scan
ID EMOTION RECOGNITION; 3D; FACE; FRAMEWORK; DATABASE
AB This paper presents a novel and efficient deep fusion convolutional neural network (DF-CNN) for multimodal 2D+3D facial expression recognition (FER). DF-CNN comprises a feature extraction subnet, a feature fusion subnet, and a softmax layer. In particular, each textured three-dimensional (3D) face scan is represented as six types of 2D facial attribute maps (i.e., geometry map, three normal maps, curvature map, and texture map), all of which are jointly fed into DF-CNN for feature learning and fusion learning, resulting in a highly concentrated facial representation (32-dimensional). Expression prediction is performed by two ways: 1) learning linear support vector machine classifiers using the 32-dimensional fused deep features, or 2) directly performing softmax prediction using the six-dimensional expression probability vectors. Different from existing 3D FER methods, DF-CNN combines feature learning and fusion learning into a single end-to-end training framework. To demonstrate the effectiveness of DF-CNN, we conducted comprehensive experiments to compare the performance of DF-CNN with handcrafted features, pre-trained deep features, fine-tuned deep features, and state-of-the-art methods on three 3D face datasets (i.e., BU-3DFE Subset I, BU-3DFE Subset II, and Bosphorus Subset). In all cases, DF-CNN consistently achieved the best results. To the best of our knowledge, this is the first work of introducing deep CNN to 3D FER and deep learning-based feature level fusion for multimodal 2D+3D FER.
C1 [Li, Huibin; Sun, Jian; Xu, Zongben] Xi An Jiao Tong Univ, Inst Informat & Syst Sci, Sch Math & Stat, Xian 710049, Shaanxi, Peoples R China.
   [Chen, Liming] Ecole Cent Lyon, Dept Math & Informat, LIRIS UMR 5205, F-69134 Lyon, France.
C3 Xi'an Jiaotong University; Ecole Centrale de Lyon; Institut National des
   Sciences Appliquees de Lyon - INSA Lyon
RP Sun, J (corresponding author), Xi An Jiao Tong Univ, Inst Informat & Syst Sci, Sch Math & Stat, Xian 710049, Shaanxi, Peoples R China.
EM huibinli@mail.xjtu.edu.cn; jiansun@mail.xjtu.edu.cn;
   zbxu@mail.xjtu.edu.cn; liming.chen@ec-lyon.fr
OI Sun, Jian/0000-0001-7206-0641
FU NSFC [11401464, 61472313, 11622106]; Chinese Postdoctoral Science
   Foundation [2014M560785]; International Exchange Foundation of China
   NSFC; United Kingdom RS Grant [61711530242]; French Research Agency,
   l'Agence Nationale de Recherche through Jemime project
   [ANR-13-CORD-0004-02]; French Research Agency, l'Agence Nationale de
   Recherche through Biofence project [ANR-13-INSE-0004-02]; PUF 4D Vision
   project - Partner University Foundation; Agence Nationale de la
   Recherche (ANR) [ANR-13-INSE-0004] Funding Source: Agence Nationale de
   la Recherche (ANR)
FX The work of H. Li was supported in part by the NSFC under Grant
   11401464, in part by the Chinese Postdoctoral Science Foundation under
   Grant 2014M560785, in part by the International Exchange Foundation of
   China NSFC, and in part by the United Kingdom RS under Grant
   61711530242. The work of J. Sun was supported in part by the NSFC under
   Grant 61472313 and Grant 11622106. The work of L. Chen was supported in
   part by the French Research Agency, l'Agence Nationale de Recherche,
   through the Jemime project under Contract ANR-13-CORD-0004-02 and the
   Biofence project under Contract ANR-13-INSE-0004-02, and in part by the
   PUF 4D Vision project funded by the Partner University Foundation. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sen-Ching Samson Cheung.
   (Corresponding author: Jian Sun.)
CR Corneanu CA, 2016, IEEE T PATTERN ANAL, V38, P1548, DOI 10.1109/TPAMI.2016.2515606
   [Anonymous], 2010, IEEE CVPR 10 WORKSHO
   [Anonymous], 2008 23 INT S COMP I
   [Anonymous], 2015, CORR
   [Anonymous], 2015, DEEPLY LEARNING DEFO
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], 2013, 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)
   Berretti S., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4125, DOI 10.1109/ICPR.2010.1002
   Calix RA, 2010, IEEE T MULTIMEDIA, V12, P544, DOI 10.1109/TMM.2010.2052026
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chu WS, 2017, IEEE T PATTERN ANAL, V39, P529, DOI 10.1109/TPAMI.2016.2547397
   Dahmane Mohamed, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P884, DOI 10.1109/FG.2011.5771368
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Dhall A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2106, DOI 10.1109/ICCVW.2011.6130508
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Fang T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P603, DOI 10.1109/FG.2011.5771466
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Hao Tang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563052
   Hayat M, 2014, IEEE T AFFECT COMPUT, V5, P301, DOI 10.1109/TAFFC.2014.2330580
   HOFFMAN R, 1987, IEEE T PATTERN ANAL, V9, P608, DOI 10.1109/TPAMI.1987.4767955
   Huang YZ, 2010, IEEE T MULTIMEDIA, V12, P536, DOI 10.1109/TMM.2010.2052792
   Huibin Li, 2012, 2012 IEEE Fifth International Conference On Biometrics: Theory, Applications And Systems (BTAS 2012), P271, DOI 10.1109/BTAS.2012.6374588
   Kahou SE, 2016, J MULTIMODAL USER IN, V10, P99, DOI 10.1007/s12193-015-0195-2
   Khorrami P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P19, DOI 10.1109/ICCVW.2015.12
   Kim BK, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P427, DOI 10.1145/2818346.2830590
   Li HB, 2015, COMPUT VIS IMAGE UND, V140, P83, DOI 10.1016/j.cviu.2015.07.005
   Li HB, 2012, INT C PATT RECOG, P2577
   Li HB, 2011, LECT NOTES COMPUT SC, V6915, P483, DOI 10.1007/978-3-642-23687-7_44
   Li K, 2014, IEEE T MULTIMEDIA, V16, P299, DOI 10.1109/TMM.2013.2293064
   Liu MY, 2015, NEUROCOMPUTING, V159, P126, DOI 10.1016/j.neucom.2015.02.011
   Liu P, 2014, PROC CVPR IEEE, P1805, DOI 10.1109/CVPR.2014.233
   Liu P, 2014, LECT NOTES COMPUT SC, V8692, P151, DOI 10.1007/978-3-319-10593-2_11
   Maalej Ahmed, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4129, DOI 10.1109/ICPR.2010.1003
   Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012
   Mian A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P735
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Ocegueda O., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1270, DOI 10.1109/ICCVW.2011.6130397
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Qingkai Zhen, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P522, DOI 10.1007/978-3-319-14445-0_45
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P683, DOI 10.1016/j.imavis.2012.06.005
   Savran A, 2008, LECT NOTES COMPUT SC, V5372, P47, DOI 10.1007/978-3-540-89991-4_6
   Song ML, 2007, IEEE T MULTIMEDIA, V9, P1384, DOI 10.1109/TMM.2007.906591
   Soyel H, 2007, LECT NOTES COMPUT SC, V4633, P831
   Tang H, 2008, PROC 8 IEEE INT C AU, P1
   Tang Y, 2013, INT C WAVEL ANAL PAT, P1
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang J., 2006, P IEEE C COMPUTER VI, P1399
   Wang SF, 2013, IEEE T AFFECT COMPUT, V4, P34, DOI 10.1109/T-AFFC.2012.32
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wei J, 2013, IEEE ICCE, P1, DOI 10.1109/ICCE.2013.6486769
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1732, DOI 10.1109/TMM.2013.2272917
   Xi Zhao, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3724, DOI 10.1109/ICPR.2010.907
   Xudong Yang, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163090
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   Yin LJ, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P211
   Yu ZD, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P435
   Zafeiriou S, 2008, IEEE T MULTIMEDIA, V10, P1528, DOI 10.1109/TMM.2008.2007292
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhang ZY, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P454, DOI 10.1109/AFGR.1998.670990
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zhen QK, 2016, IEEE T MULTIMEDIA, V18, P1438, DOI 10.1109/TMM.2016.2557063
   Zhong L, 2015, IEEE T CYBERNETICS, V45, P1499, DOI 10.1109/TCYB.2014.2354351
NR 69
TC 137
Z9 149
U1 6
U2 105
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2816
EP 2831
DI 10.1109/TMM.2017.2713408
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200014
DA 2024-07-18
ER

PT J
AU Zhang, XP
   Xiong, HK
   Zhou, WG
   Lin, WY
   Tian, Q
AF Zhang, Xiaopeng
   Xiong, Hongkai
   Zhou, Wengang
   Lin, Weiyao
   Tian, Qi
TI Picking Neural Activations for Fine-Grained Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fine-grained recognition; regularized multiple instance leaning;
   spatially weighted Fisher Vectors (SWFV); weakly supervised part
   discovery
ID IMAGE CLASSIFICATION; FISHER VECTOR
AB It is a challenging task to recognize fine-grained subcategories due to the highly localized and subtle differences among them. Different from most previous methods that rely on object/part annotations, this paper proposes an automatic fine-grained recognition approach, which is free of any object/part annotation at both training and testing stages. The key idea includes two steps of picking neural activations computed from the convolutional neural networks, one for localization, and the other for description. The first picking step is to find distinctive neurons that are sensitive to specific patterns significantly and consistently. Based on these picked neurons, we initialize positive samples and formulate the localization as a regularized multiple instance learning task, which aims at refining the detectors via iteratively alternating between new positive sample mining and part model retraining. The second picking step is to pool deep neural activations via a spatially weighted combination of Fisher Vectors coding. We conditionally select activations to encode them into the final representation, which considers the importance of each activation. Integrating the above techniques produces a powerful framework, and experiments conducted on several extensive fine-grained benchmarks demonstrate the superiority of our proposed algorithm over the existing methods.
C1 [Zhang, Xiaopeng; Xiong, Hongkai; Lin, Weiyao] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Zhou, Wengang] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Anhui, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Shanghai Jiao Tong University; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS; University of Texas System;
   University of Texas at San Antonio (UTSA)
RP Tian, Q (corresponding author), Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
EM zxphistory@sjtu.edu.cn; xionghongkai@sjtu.edu.cn; zhwg@ustc.edu.cn;
   wylin@sjtu.edu.cn; qitian@cs.utsa.edu
RI lin, yuxi/HKF-6212-2023
OI Xiong, Hongkai/0000-0003-4552-0029; Lin, Weiyao/0000-0001-8307-7107
FU National Science Foundation of China [61425011, 61622112, 61529101,
   61472234, 61471235, 61632019]; China Scholarship Council [201506230029];
   Program of Shanghai Academic Research Leader [17XD1401900]; Microsoft
   Research Aisa Collaborative Research Award; ARO [W911NF-15-1-0290,
   W911NF-12-1-0057]; Faculty Research Gift Awards by NEC Laboratory of
   America; Faculty Research Gift Awards by NEC Laboratory of Blippar
FX This work was supported in part by the National Science Foundation of
   China under Grant 61425011, Grant 61622112, Grant 61529101, Grant
   61472234, Grant 61471235, and Grant 61632019, and in part by the China
   Scholarship Council under Grant 201506230029. The work of H. Xiong was
   supported by the Program of Shanghai Academic Research Leader under
   Grant 17XD1401900, the work of W. Lin was supported by the Microsoft
   Research Aisa Collaborative Research Award, and the work of Q. Tian was
   supported in part by the ARO under Grant W911NF-15-1-0290 and Grant
   W911NF-12-1-0057 and in part by the Faculty Research Gift Awards by NEC
   Laboratories of America and Blippar. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Martha Larson. (Corresponding author: Qi Tian.)
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2013, CORR
   [Anonymous], 2005, P 22 INT C MACHINE L
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2011, PROC CVPR WORKSHOP F
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963
   Branson S, 2014, INT J COMPUT VISION, V108, P3, DOI 10.1007/s11263-014-0698-4
   Chai Y, 2013, IEEE I CONF COMP VIS, P321, DOI 10.1109/ICCV.2013.47
   CIMPOI M, 2015, PROC CVPR IEEE, P3828, DOI DOI 10.1109/CVPR.2015.7299007
   Collobert R., 2004, RR0442 ID RES I
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Gao SH, 2014, IEEE T IMAGE PROCESS, V23, P623, DOI 10.1109/TIP.2013.2290593
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Göring C, 2014, PROC CVPR IEEE, P2489, DOI 10.1109/CVPR.2014.319
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gonzalez-Garcia A., 2016, CORR
   Gosselin PH, 2014, PATTERN RECOGN LETT, V49, P92, DOI 10.1016/j.patrec.2014.06.011
   Gupta Rajiv, 2015, P 19 INT C EV ASS SO, P1
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jaderberg Max, 2015, ADV NEURAL INFORM PR, P8
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Parkhi OM, 2011, IEEE I CONF COMP VIS, P1427, DOI 10.1109/ICCV.2011.6126398
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simon M, 2015, IEEE I CONF COMP VIS, P1143, DOI 10.1109/ICCV.2015.136
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang Y, 2015, IEEE T MULTIMEDIA, V17, P2072, DOI 10.1109/TMM.2015.2480228
   Wei XS, 2017, IEEE T IMAGE PROCESS, V26, P2868, DOI 10.1109/TIP.2017.2688133
   Xiao TJ, 2015, PROC CVPR IEEE, P842, DOI 10.1109/CVPR.2015.7298685
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xie LX, 2013, IEEE I CONF COMP VIS, P1641, DOI 10.1109/ICCV.2013.206
   Yang S., 2012, Advances in Neural Information Processing Systems, P3122
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang XP, 2016, IEEE T IMAGE PROCESS, V25, P878, DOI 10.1109/TIP.2015.2509425
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
NR 53
TC 30
Z9 33
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2736
EP 2750
DI 10.1109/TMM.2017.2710803
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200008
DA 2024-07-18
ER

PT J
AU Zhang, J
   Yang, Y
   Tian, Q
   Zhuo, L
   Liu, X
AF Zhang, Jing
   Yang, Ying
   Tian, Qi
   Zhuo, Li
   Liu, Xin
TI Personalized Social Image Recommendation Method Based on User-Image-Tag
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social image; personalized recommendation; user-image-tag model;
   tripartite graph; tag ranking
ID SEARCH
AB In the social image sharing websites (such as Flickr), users are allowed to upload images and tag them with tags. Due to the diversities of users' interests, different users may tag the same image with different tags. Therefore, tags not only reveal some important image semantic clues, but also show user's preference, which can provide a new effective solution for overcoming the semantic gap as well as realizing a personalized recommendation. In this paper, a personalized social image recommendation method based on user-image-tag model is proposed. The main contributions of our work are 1) to efficiently make use of tags, social image tags are re-ranked according to the image content; 2) to obtain user preference, a user-image-tag model is constructed with tripartite graph according to the correlation among users, images and top-ranking tags; and 3) a personalized social recommendation system is implemented based on user-image-tag model. Experimental results proved that our method can significantly improve the accuracy of personalized image recommendation.
C1 [Zhang, Jing; Yang, Ying; Zhuo, Li; Liu, Xin] Beijing Univ Technol, Signal & Informat Proc Lab, Beijing 100124, Peoples R China.
   [Zhang, Jing; Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
   [Zhuo, Li] Collaborat Innovat Ctr Elect Vehicles Beijing, Beijing, Peoples R China.
C3 Beijing University of Technology; University of Texas System; University
   of Texas at San Antonio (UTSA)
RP Zhang, J (corresponding author), Beijing Univ Technol, Signal & Informat Proc Lab, Beijing 100124, Peoples R China.
EM zhj@bjut.edu.cn; yying@emails.bjut.edu.cn; qi.tian@utsa.edu;
   zhuoli@bjut.edu.cn; liuxin2012@emails.bjut.edu.cn
RI Li, Chun/KBC-9591-2024; Yu, Chongxiu/KDM-7354-2024
OI Yu, Chongxiu/0000-0002-8221-6221; ZHANG, JING/0000-0003-1290-0738
FU Beijing Natural Science Foundation [4163071, 4142009]; Science and
   Technology Development Program of Beijing Education Committee
   [KM201410005002]; National Natural Science Foundation of China
   [61370189, 61531006, 61372149, 61429201, 61471013]; Importation and
   Development of High-Caliber Talents Project of Beijing Municipal
   Institutions [CITTCD20150311, CITTCD201404043]; ARO [W911NF-15-1-0290];
   NEC Laboratories of America; Blippar, the Funding Project for Academic
   Human Resources Development in Institutions of Higher Learning Under the
   Jurisdiction of Beijing Municipality
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grant 4163071 and Grant 4142009, in part by the Science
   and Technology Development Program of Beijing Education Committee under
   Grant KM201410005002, in part by the National Natural Science Foundation
   of China under Grant 61370189, Grant 61531006, Grant 61372149, Grant
   61429201, and Grant 61471013, and in part by the Importation and
   Development of High-Caliber Talents Project of Beijing Municipal
   Institutions under Grant CIT&TCD20150311 and Grant CIT&TCD201404043. The
   work of Q. Tian was supported in part by the ARO Grant W911NF-15-1-0290,
   and in part by the Faculty Research Gift Awards by NEC Laboratories of
   America and Blippar, the Funding Project for Academic Human Resources
   Development in Institutions of Higher Learning Under the Jurisdiction of
   Beijing Municipality. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Chengcui Zhang.
   (Corresponding author: Jing Zhang.)
CR [Anonymous], 2011, PROC WWW
   [Anonymous], 2007, P 9 WEBKDD 1 SNA KDD, DOI [10.1145/1348549.1348556, DOI 10.1145/1348549.1348556]
   [Anonymous], P ACM INT C MULT ACM
   Cai Y, 2014, NEURAL NETWORKS, V58, P98, DOI 10.1016/j.neunet.2014.05.017
   Cheng X, 2012, IEEE T MULTIMEDIA, V14, P1558, DOI 10.1109/TMM.2012.2217735
   Fang Q, 2014, IEEE T MULTIMEDIA, V16, P796, DOI 10.1109/TMM.2014.2298216
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Geng X, 2015, IEEE I CONF COMP VIS, P4274, DOI 10.1109/ICCV.2015.486
   Iwata T, 2016, IEEE T PATTERN ANAL, V38, P607, DOI 10.1109/TPAMI.2015.2469284
   Jiang Lan-chi, 2009, Mechanical & Electrical Engineering Magazine, V26, P54
   Liu D, 2011, IEEE T MULTIMEDIA, V13, P82, DOI 10.1109/TMM.2010.2087744
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu X, 2015, IEEE IMAGE PROC, P3901, DOI 10.1109/ICIP.2015.7351536
   Liu Xin., 2014, ECMOR XIV-14th European Conference on the Mathematics of Oil Recovery, V2014, P1
   Nakamoto R.Y., 2008, P 2 ACM WORKSH INF C, P11, DOI [10.1145/1458527.1458533, DOI 10.1145/1458527.1458533]
   Phelan O, 2009, P 3 ACM C REC SYST, P385, DOI DOI 10.1145/1639714.1639794
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rafailidis D, 2014, ACM T INTERACT INTEL, V3, DOI 10.1145/2487164
   Rawat YS, 2017, IEEE T CIRC SYST VID, V27, P149, DOI 10.1109/TCSVT.2016.2555658
   Schenkel R., 2008, SIGIR, DOI DOI 10.1145/1390334.1390424
   Shang MS, 2010, PHYSICA A, V389, P1259, DOI 10.1016/j.physa.2009.11.041
   Tian Q, 2012, IEEE T MULTIMEDIA, V14, P949, DOI 10.1109/TMM.2012.2208026
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Wang Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P79, DOI 10.1145/2733373.2806233
   Yang KY, 2011, IEEE T MULTIMEDIA, V13, P662, DOI 10.1109/TMM.2011.2147777
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zha ZJ, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1823746.1823747
   Zhang HW, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1079, DOI 10.1145/2733373.2806286
   Zhang J, 2015, NEUROCOMPUTING, V153, P278, DOI 10.1016/j.neucom.2014.11.027
   Zhang YC, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.154301
   Zhang ZK, 2010, PHYSICA A, V389, P179, DOI 10.1016/j.physa.2009.08.036
   Zhou T, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046115
NR 33
TC 35
Z9 41
U1 0
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2439
EP 2449
DI 10.1109/TMM.2017.2701641
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200007
DA 2024-07-18
ER

PT J
AU Alonso, A
   Aguado, I
   Salvachúa, J
   Rodríguez, P
AF Alonso, Alvaro
   Aguado, Ignacio
   Salvachua, Joaquin
   Rodriguez, Pedro
TI A Methodology for Designing and Evaluating Cloud Scheduling Strategies
   in Distributed Videoconferencing Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud; distributed; methods; scheduling; videoconferencing
AB Over the last few years, videoconferencing systems have experienced several changes that enable videoconferencing applications in personal devices to a high number of users. To efficiently attend this high and variable demand, deploying distributed videoconferencing servers in cloud-based infrastructures is highly recommended. However, videoconferencing systems have particular characteristics that impede the application of regular resource scheduling solutions used in other kinds of distributed environments. In this paper, we propose a methodology to design and evaluate scheduling strategies adapted to the necessities of each specific scenario. It involves using a new metric to estimate the resource consumption of each connection and provides a set of coefficients to evaluate the efficiency of the strategy. We test the methodology in a real setup and compare the behavior and performance of three scheduling algorithms. The conclusion is that the proposed methodology allows us to configure decision policies adapted to the requirements and necessities of a range of use cases. Thereby, we achieve a more efficient way of using cloud resources, improving the service performance and saving costs.
C1 [Alonso, Alvaro; Aguado, Ignacio; Salvachua, Joaquin; Rodriguez, Pedro] Univ Politecn Madrid, Telemat Syst Engn Dept, E-28040 Madrid, Spain.
C3 Universidad Politecnica de Madrid
RP Alonso, A (corresponding author), Univ Politecn Madrid, Telemat Syst Engn Dept, E-28040 Madrid, Spain.
EM aalonsog@dit.upm.es; iaguado@dit.upm.es; jsalvachua@dit.upm.es;
   prodriguez@dit.upm.es
RI Alonso, Álvaro/AAO-5941-2020; SALVACHUA RODRIGUEZ, JOAQUIN/O-1445-2015
OI Alonso, Álvaro/0000-0002-8456-8351; 
FU Ministerio de Economia, Industria y Competitividad
FX This work was supported by the Ministerio de Economia, Industria y
   Competitividad. The guest editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xiaoqing Zhu.
   (Corresponding author: Alvaro Alonso.)
CR Alonso A., 2016, P 7 INT C CLOUD COMP, P51
   Alonso A, 2016, 2016 IEEE 4TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2016), P25, DOI 10.1109/FiCloud.2016.12
   ALONSO Alvaro, 2013, CLOUD COMPUTING 2013, V173-178
   [Anonymous], 2011, NIST DEFINITION CLOU
   [Anonymous], P NAT C COMP GRAPH
   Bergkvist Adam, 2012, WEBRTC 1 0 IN PRESS
   Cerviño J, 2013, MOBILE NETW APPL, V18, P103, DOI 10.1007/s11036-012-0380-4
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cho HH, 2014, 2014 10TH INTERNATIONAL CONFERENCE ON HETEROGENEOUS NETWORKING FOR QUALITY, RELIABILITY, SECURITY AND ROBUSTNESS (QSHINE), P31, DOI [10.1109/QSHINE.2014.6928656, 10.4108/icst.qshine.2014.256193]
   Cola C, 2014, INT CONF SYST THEO, P430, DOI 10.1109/ICSTCC.2014.6982454
   Du J, 2016, IEEE T MULTIMEDIA, V18, P820, DOI 10.1109/TMM.2016.2537781
   Ghose D, 2000, MULTIMED TOOLS APPL, V11, P167, DOI 10.1023/A:1009681521536
   Gihun Jung, 2011, Proceedings of the 2011 International Conference on Parallel Processing Workshops (ICPPW 2011), P345, DOI 10.1109/ICPPW.2011.18
   Hindman B., 2011, Mesos: a platform for fine-grained resource sharing in the data center, P295
   Hong S, 2010, MULTIMED TOOLS APPL, V46, P463, DOI 10.1007/s11042-009-0386-5
   Kim KI, 2013, ETRI J, V35, P960, DOI 10.4218/etrij.13.2013.0076
   Li J, 2010, PROCEEDINGS OF 2010 INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND INDUSTRIAL ENGINEERING, VOLS I AND II, P37
   Lu X, 2011, INT CONF CLOUD COMPU, P296, DOI 10.1109/CCIS.2011.6045078
   Luo C, 2007, IEEE T MULTIMEDIA, V9, P1621, DOI 10.1109/TMM.2007.907467
   Martinez-Julia P, 2013, 2013 SEVENTH INTERNATIONAL CONFERENCE ON INNOVATIVE MOBILE AND INTERNET SERVICES IN UBIQUITOUS COMPUTING (IMIS 2013), P411, DOI 10.1109/IMIS.2013.76
   Qiang Y, 2009, WKDD: 2009 SECOND INTERNATIONAL WORKSHOP ON KNOWLEDGE DISCOVERY AND DATA MINING, PROCEEDINGS, P781, DOI 10.1109/WKDD.2009.191
   Rodríguez P, 2016, COMPUT STAND INTER, V44, P234, DOI 10.1016/j.csi.2015.09.004
   Rodríguez P, 2014, 2014 INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD), P61, DOI 10.1109/FiCloud.2014.20
   Sha L, 2004, REAL-TIME SYST, V28, P101, DOI 10.1023/B:TIME.0000045315.61234.1e
   Um TW, 2014, ETRI J, V36, P197, DOI 10.4218/etrij.14.2113.0085
   Wu MY, 2001, MULTIMED TOOLS APPL, V14, P79, DOI 10.1023/A:1011363808503
   Yoon C, 2010, ETRI J, V32, P634, DOI 10.4218/etrij.10.0209.0489
   Zhou L, 2011, IEEE T MULTIMEDIA, V13, P1040, DOI 10.1109/TMM.2011.2160716
NR 28
TC 4
Z9 4
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2282
EP 2292
DI 10.1109/TMM.2017.2733301
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600012
DA 2024-07-18
ER

PT J
AU Atoum, Y
   Chen, LP
   Liu, AX
   Hsu, SDH
   Liu, XM
AF Atoum, Yousef
   Chen, Liping
   Liu, Alex X.
   Hsu, Stephen D. H.
   Liu, Xiaoming
TI Automated Online Exam Proctoring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Covariance feature; gaze estimation; online exam proctoring (OEP); phone
   detection; speech detection; text detection; user verification
AB Massive open online courses and other forms of remote education continue to increase in popularity and reach. The ability to efficiently proctor remote online examinations is an important limiting factor to the scalability of this next stage in education. Presently, human proctoring is the most common approach of evaluation, by either requiring the test taker to visit an examination center, or by monitoring them visually and acoustically during exams via a webcam. However, such methods are labor intensive and costly. In this paper, we present a multimedia analytics system that performs automatic online exam proctoring. The system hardware includes one webcam, one wearcam, and a microphone for the purpose of monitoring the visual and acoustic environment of the testing location. The system includes six basic components that continuously estimate the key behavior cues: user verification, text detection, voice detection, active window detection, gaze estimation, and phone detection. By combining the continuous estimation components, and applying a temporal sliding window, we design higher level features to classify whether the test taker is cheating at any moment during the exam. To evaluate our proposed system, we collect multimedia (audio and visual) data from 24 subjects performing various types of cheating while taking online exams. Extensive experimental results demonstrate the accuracy, robustness, and efficiency of our online exam proctoring system.
C1 [Atoum, Yousef] Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
   [Chen, Liping; Liu, Alex X.; Liu, Xiaoming] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
   [Hsu, Stephen D. H.] Michigan State Univ, Dept Phys & Astron, E Lansing, MI 48824 USA.
C3 Michigan State University; Michigan State University; Michigan State
   University
RP Atoum, Y (corresponding author), Michigan State Univ, Dept Elect & Comp Engn, E Lansing, MI 48824 USA.
EM atoumyou@gmail.com; chenlipi@msu.edu; alexliu@cse.msu.edu; hsu@msu.edu;
   liuxm@cse.msu.edu
OI liu, xiaoming/0000-0003-3215-8753
FU Michigan State University Targeted Support Grants for Technology
   Development program
FX This work was supported in part by the Michigan State University
   Targeted Support Grants for Technology Development program. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tao Mei.
CR Allen I.Elaine., 2014, Babson Survey Research Group and Quahog Research Group, V3, P2014
   [Anonymous], 2008, P 10 INT C PROB METH
   Atoum Y, 2015, IEEE SIGNAL PROC LET, V22, P1089, DOI 10.1109/LSP.2014.2385794
   Baggio D. L., 2008, NATURAL USER INTERFA, P1
   Bailey S. V., 2005, AUDIO ENG SOC CONVEN
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen JX, 2014, PATTERN RECOGN LETT, V37, P32, DOI 10.1016/j.patrec.2013.07.017
   Cluskey G.R., 2011, Journal of Academic and Business Ethics, V4, P1
   Guo P, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON IT IN MEDICINE AND EDUCATION, VOLS 1 AND 2, PROCEEDINGS, P497, DOI 10.1109/ITME.2008.4743914
   Hoiem D, 2005, INT CONF ACOUST SPEE, P429
   Hung H, 2010, IEEE T MULTIMEDIA, V12, P563, DOI 10.1109/TMM.2010.2055233
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Jourabloo A, 2015, IEEE I CONF COMP VIS, P3694, DOI 10.1109/ICCV.2015.421
   Jung IY, 2009, IEEE T EDUC, V52, P340, DOI 10.1109/TE.2008.928909
   Kiliç V, 2015, IEEE T MULTIMEDIA, V17, P186, DOI 10.1109/TMM.2014.2377515
   King D L., 2014, Issues in Information Systems, V15, P20, DOI [10.48009/1_iis_2014_20-27, DOI 10.48009/1_IIS_2014_20-27]
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Lefter I, 2013, PATTERN RECOGN LETT, V34, P1953, DOI 10.1016/j.patrec.2013.01.002
   Li XC, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON COMPUTER-SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING (CSCW'15), P1129, DOI 10.1145/2675133.2675245
   Liu XM, 2007, 2007 IEEE CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P405, DOI 10.1109/AVSS.2007.4425345
   Liu XM, 2010, IMAGE VISION COMPUT, V28, P1162, DOI 10.1016/j.imavis.2009.09.016
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Reale MJ, 2011, IEEE T MULTIMEDIA, V13, P474, DOI 10.1109/TMM.2011.2120600
   Rosen Warren A., 2013, 2013 IEEE Frontiers in Education Conference (FIE), P1935, DOI 10.1109/FIE.2013.6685172
   Roth J, 2015, IEEE T INF FOREN SEC, V10, P333, DOI 10.1109/TIFS.2014.2374424
   Roth J, 2014, IEEE T IMAGE PROCESS, V23, P4611, DOI 10.1109/TIP.2014.2348802
   Sadlier DA, 2005, IEEE T CIRC SYST VID, V15, P1225, DOI 10.1109/TCSVT.2005.854237
   Savvides M., 2002, IEEE Automatic Identification Advanced Technologies, P56, DOI DOI 10.1109/ICISIP.2004.1287684
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Spriggs EH, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2009.5204354
   Tsukada A., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P2084, DOI 10.1109/ICCVW.2011.6130505
   Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wahid, 2015, P 9 INT C UB INF MAN, P95
   Xiao B, 2015, IEEE T MULTIMEDIA, V17, P1107, DOI 10.1109/TMM.2015.2432671
   Zhang YM, 2012, LECT NOTES COMPUT SC, V7574, P707, DOI 10.1007/978-3-642-33712-3_51
NR 36
TC 77
Z9 80
U1 6
U2 68
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1609
EP 1624
DI 10.1109/TMM.2017.2656064
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800017
DA 2024-07-18
ER

PT J
AU Yang, C
   Cheung, G
   Stankovic, V
AF Yang, Cheng
   Cheung, Gene
   Stankovic, Vladimir
TI Estimating Heart Rate and Rhythm via 3D Motion Tracking in Depth Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biomedical monitoring; image denoising; signal analysis
ID GRAPH LAPLACIAN REGULARIZATION; OBJECT TRACKING; NONCONTACT
AB Low-cost depth sensors, such as Microsoft Kinect, have potential for noncontact health monitoring that is robust to ambient lighting conditions. However, captured depth images typically suffer from high acquisition noise, and hence, processing them to estimate biometrics is difficult. In this paper, we propose to capture depth video of a human subject using Kinect 2.0 to estimate his/her heart rate and rhythm; as blood is pumped from the heart to circulate through the head, tiny oscillatory head motion due to Newtonian mechanics can be detected for periodicity analysis. Specifically, we first restore a captured depth video via a joint bit-depth enhancement/denoising procedure, using a graph-signal smoothness prior for regularization. Second, we track an automatically detected head region throughout the depth video to deduce 3D motion vectors. The detected vectors are fed back to the depth restoration module in a loop to ensure that the motion information in two modules is consistent, improving performance of both restoration and motion tracking. Third, the computed 3D motion vectors are projected onto its principal component for 1D signal analysis, composed of trend removal, bandpass filtering, and wavelet-based motion denoising. Finally, the heart rate is estimated via Welch power spectrum analysis, and the heart rhythm is computed via peak detection. Experimental results show accurate estimation of the heart rate and rhythm using our proposed algorithm as compared to rate and rhythm estimated by a portable oximeter.
C1 [Yang, Cheng; Stankovic, Vladimir] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XQ, Lanark, Scotland.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
C3 University of Strathclyde; Research Organization of Information &
   Systems (ROIS); National Institute of Informatics (NII) - Japan
RP Yang, C (corresponding author), Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XQ, Lanark, Scotland.
EM cheng.yang@strath.ac.uk; cheung@nii.ac.jp;
   vladimir.stankovic@strath.ac.uk
RI Cheung, Gene/AAB-9284-2020; Stankovic, Vladimir/L-6584-2016
OI Stankovic, Vladimir/0000-0002-1075-2420; Cheung,
   Gene/0000-0002-5571-4137
FU JSPS [15K12072]; European Union's Horizon 2020 research and innovation
   programme under the Marie Sklodowska-Curie grant [734331]; Grants-in-Aid
   for Scientific Research [15K12072] Funding Source: KAKEN
FX This work was supported in part by the JSPS Grant-in-Aid for Challenging
   Exploratory Research (15K12072). This project has received funding from
   the European Union's Horizon 2020 research and innovation programme
   under the Marie Sklodowska-Curie grant agreement no. 734331. This paper
   was presented in part at the IEEE International Conference on Multimedia
   and Expo, Torino, Italy, June-July 2015. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sen-Ching Samson Cheung.
CR Acampora G, 2013, P IEEE, V101, P2470, DOI 10.1109/JPROC.2013.2262913
   [Anonymous], WAVELET TOUR SIGNAL
   [Anonymous], 2008, P 2008 IEEE C COMP V
   [Anonymous], 2013, COMMUN COMPUT INF SC, DOI DOI 10.1007/978-3-642-38256-7_21
   [Anonymous], P 2014 IEEE INT S ME
   Antink CH, 2015, BIOMED OPT EXPRESS, V6, P2895, DOI 10.1364/BOE.6.002895
   Awwad S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1115, DOI 10.1145/2733373.2806295
   Balakrishnan G, 2013, PROC CVPR IEEE, P3430, DOI 10.1109/CVPR.2013.440
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Boccanfuso Laura, 2012, Social Robotics. 4th International Conference (ICSR 2012). Proceedings, P86, DOI 10.1007/978-3-642-34103-8_9
   Butterworth S., 1930, Wireless Eng, V7, P536
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cardone D, 2015, COMPUT MATH METHOD M, V2015, DOI 10.1155/2015/984353
   Chekmenev S. Y., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383443
   Chekmenev SY., 2006, INT J GRAPH VISION I, V6, P25
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Daubechies I., 1992, 10 LECT WAVELETS
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Erden F, 2016, IEEE SIGNAL PROC MAG, V33, P36, DOI 10.1109/MSP.2015.2489978
   Gault TR, 2013, IEEE COMPUT SOC CONF, P336, DOI 10.1109/CVPRW.2013.57
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hu W, 2016, IEEE SIGNAL PROC LET, V23, P242, DOI 10.1109/LSP.2015.2510379
   Huang RY, 2016, BIOMED ENG ONLINE, V15, DOI 10.1186/s12938-016-0127-8
   Imaduddin S. M., 2015, P INT C EM TECHN PES, P1
   Irani R, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 3, P118
   Kanade T., 1981, P 7 INT JOINT C ARTI, V1, P674, DOI DOI 10.5555/1623264.1623280
   Kim Y. S., 2012, P 3 DIM IM PROC APPL
   KIRBY M, 1990, IEEE T PATTERN ANAL, V12, P103, DOI 10.1109/34.41390
   Kristan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P564, DOI 10.1109/ICCVW.2015.79
   Kwon S, 2012, IEEE ENG MED BIO, P2174, DOI 10.1109/EMBC.2012.6346392
   Lachat E, 2015, INT ARCH PHOTOGRAMM, V40-5, P93, DOI 10.5194/isprsarchives-XL-5-W4-93-2015
   Lewandowska M., 2011, 2011 Federated Conference on Computer Science and Information Systems (FedCSIS), P405
   Li XB, 2014, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2014.543
   Lin KY, 2016, IEEE SENS J, V16, P1351, DOI 10.1109/JSEN.2015.2500032
   Liu MY, 2010, PROC CVPR IEEE, P1696, DOI 10.1109/CVPR.2010.5539837
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P509, DOI 10.1109/TIP.2016.2627807
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Pagliari D, 2015, SENSORS-BASEL, V15, P27569, DOI 10.3390/s151127569
   Pang JH, 2017, IEEE T IMAGE PROCESS, V26, P1770, DOI 10.1109/TIP.2017.2651400
   Pang JH, 2015, INT CONF ACOUST SPEE, P2294, DOI 10.1109/ICASSP.2015.7178380
   Pang JY, 2014, 2014 IEEE CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (PHM)
   Pietikainen M, 2011, COMPUT IMAGING VIS, V40, P1
   Poh MZ, 2011, IEEE T BIO-MED ENG, V58, P7, DOI 10.1109/TBME.2010.2086456
   Poh MZ, 2010, OPT EXPRESS, V18, P10762, DOI 10.1364/OE.18.010762
   Press W. H., 2007, NUM REC ART SCI COMP
   Rifkin R., 2003, Computer and Systems Sciences, V190, P131
   Rue H, 2005, CHAPMAN HALL CRC MON
   Shan L, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P160, DOI 10.1109/CISP.2013.6743978
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Shuai Tang, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P525, DOI 10.1007/978-3-642-37444-9_41
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Song SR, 2013, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2013.36
   Stricker R, 2014, IEEE ROMAN, P1056, DOI 10.1109/ROMAN.2014.6926392
   Suau X, 2012, IEEE T MULTIMEDIA, V14, P575, DOI 10.1109/TMM.2012.2189853
   Sun WX, 2014, IEEE T IMAGE PROCESS, V23, P3138, DOI 10.1109/TIP.2014.2326413
   Tabak G, 2015, 2015 49TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, P790, DOI 10.1109/ACSSC.2015.7421242
   Tasli H. E., 2014, P IEEE INT C IM PROC
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Verkruysse W, 2008, OPT EXPRESS, V16, P21434, DOI 10.1364/OE.16.021434
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   WELCH PD, 1967, IEEE T ACOUST SPEECH, VAU15, P70, DOI 10.1109/TAU.1967.1161901
   Wu HY, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185561
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu ZH, 2007, P NATL ACAD SCI USA, V104, P14889, DOI 10.1073/pnas.0701020104
   Xia L., 2011, P IEEE C COMP VIS PA
   Yang CH, 2014, IEEE PHOTON CONF, P1, DOI 10.1109/IPCon.2014.6994959
   Yang C, 2017, IEEE T MULTIMEDIA, V19, P822, DOI 10.1109/TMM.2016.2626969
   Yang CX, 2014, ADV INTEL SYS RES, V59, P1
   Yuan Y, 2015, IEEE T MULTIMEDIA, V17, P1125, DOI 10.1109/TMM.2015.2440996
   Zaret B., 1992, YALE U SCH MED HEART
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 76
TC 29
Z9 34
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2017
VL 19
IS 7
BP 1625
EP 1636
DI 10.1109/TMM.2017.2672198
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5ZN
UT WOS:000404061800018
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Han, TT
   Yao, HX
   Xu, CL
   Sun, XS
   Zhang, YH
   Corso, JJ
AF Han, Tingting
   Yao, Hongxun
   Xu, Chenliang
   Sun, Xiaoshuai
   Zhang, Yanhao
   Corso, Jason J.
TI Dancelets Mining for Video Recommendation Based on Dance Styles
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dancelets; dance style; LDA detector; normalized cuts; spatiotemporal
   features; video recommendation
ID ACTION RECOGNITION; CLASSIFICATION
AB Dance is a unique and meaningful type of human expression, composed of abundant and various action elements. However, existing methods based on associated texts and spatial visual features have difficulty capturing the highly articulated motion patterns. To overcome this limitation, we propose to take advantage of the intrinsic motion information in dance videos to solve the video recommendation problem. We present a novel system that recommends dance videos based on a mid-level action representation, termed Dancelets. The Dancelets are used to bridge the semantic gap between video content and high-level concept, dance style, which plays a significant role in characterizing different types of dances. The proposed method executes automatic mining of dancelets with a concatenation of normalized cut clustering and linear discriminant analysis. This ensures that the discovered dancelets are both representative and discriminative. Additionally, to exploit the motion cues in videos, we employ motion boundaries as saliency priors to generate volumes of interest and extract C3D features to capture spatiotemporal information from the mid-level patches. Extensive experiments validated on our proposed large dance dataset, HIT Dances dataset, demonstrate the effectiveness of the proposed methods for dance style-based video recommendation.
C1 [Han, Tingting; Yao, Hongxun; Sun, Xiaoshuai; Zhang, Yanhao] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Xu, Chenliang] Univ Rochester, Dept Comp Sci, Rochester, NY 14611 USA.
   [Corso, Jason J.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
C3 Harbin Institute of Technology; University of Rochester; University of
   Michigan System; University of Michigan
RP Yao, HX (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM ttinghan@hit.edu.cn; yao@hit.edu.cn; chenliang.xu@rochester.edu;
   xiaoshuaisun@hit.edu.cn; yhzhang@hit.edu.cn; jjcorso@eecs.umich.edu
RI HAN, TINGTING/GQZ-8692-2022
OI Xu, Chenliang/0000-0002-2183-822X; Corso, Jason/0000-0001-6454-9594
FU National Science Foundation of China [61133003, 61472103]
FX This work was supported in part by the National Science Foundation of
   China under Grant 61133003 and Grant 61472103. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Yap-Peng Tan. (Corresponding author: Hongxun Yao.)
CR Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Deldjoo Y, 2016, J DATA SEMANT, V5, P99, DOI 10.1007/s13740-016-0060-9
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676
   Han TT, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P915, DOI 10.1145/2733373.2806363
   Han TT, 2016, NEUROCOMPUTING, V171, P347, DOI 10.1016/j.neucom.2015.06.048
   Hariharan B, 2012, LECT NOTES COMPUT SC, V7575, P459, DOI 10.1007/978-3-642-33765-9_33
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lee YJ, 2013, IEEE I CONF COMP VIS, P1857, DOI 10.1109/ICCV.2013.233
   Li Y, 2015, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2015.7298699
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Peng XJ, 2014, IMAGE VISION COMPUT, V32, P616, DOI 10.1016/j.imavis.2014.06.011
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Rubens N, 2011, RECOMMENDER SYSTEMS HANDBOOK, P735, DOI 10.1007/978-0-387-85820-3_23
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Sun J, 2013, IEEE I CONF COMP VIS, P3400, DOI 10.1109/ICCV.2013.422
   Sun X., 2012, MIR, P268
   Uijlings J, 2015, INT J MULTIMED INF R, V4, P33, DOI 10.1007/s13735-014-0069-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang LM, 2013, PROC CVPR IEEE, P2674, DOI 10.1109/CVPR.2013.345
   Yu G, 2011, PROC CVPR IEEE, P865, DOI 10.1109/CVPR.2011.5995488
   Yu SX, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P313, DOI 10.1109/iccv.2003.1238361
   Zhang WY, 2013, IEEE I CONF COMP VIS, P2248, DOI 10.1109/ICCV.2013.280
   Zhao XJ, 2013, MULTIMEDIA SYST, V19, P3, DOI 10.1007/s00530-012-0267-z
   Zhu J, 2013, IEEE I CONF COMP VIS, P3559, DOI 10.1109/ICCV.2013.442
NR 38
TC 24
Z9 24
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 712
EP 724
DI 10.1109/TMM.2016.2631881
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500004
DA 2024-07-18
ER

PT J
AU Yang, C
   Cheung, G
   Stankovic, V
   Chan, K
   Ono, N
AF Yang, Cheng
   Cheung, Gene
   Stankovic, Vladimir
   Chan, Kevin
   Ono, Nobutaka
TI Sleep Apnea Detection via Depth Video and Audio Feature Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; monitoring; signal denoising; sleep apnea
ID SIMPLEX-METHOD; ALGORITHMS; COMPRESSION; TRANSFORM; DIAGNOSIS
AB Obstructive sleep apnea, characterized by repetitive obstruction in the upper airway during sleep, is a common sleep disorder that could significantly compromise sleep quality and quality of life in general. The obstructive respiratory events can be detected by attended in-laboratory or unattended ambulatory sleep studies. Such studies require many attachments to a patient's body to track respiratory and physiological changes, which can be uncomfortable and compromise the patient's sleep quality. In this paper, we propose to record depth video and audio of a patient using a Microsoft Kinect camera during his/her sleep, and extract relevant features to correlate with obstructive respiratory events scored manually by a scientific officer based on data collected by Philips system Alice6 LDxS that is commonly used in sleep clinics. Specifically, we first propose an alternating-frame H.264 video encoding scheme and bit recovery scheme at the decoder. Next, we perform depth video temporal denoising using a motion vector graph smoothness prior. Then, we build a dual-ellipse model and track a patient's chest and abdominal movements in the denoised videos. Finally, we extract features from both depth video and audio for classifier training and respiratory event detection. Experimental results show 1) that our depth video compression scheme outperforms a competitor that records only the 8 most significant bits, 2) our graph-based temporal denoising scheme reduces the flickering effect without over-smoothing, and 3) our trained classifiers can deduce respiratory events scored manually based on data collected by system Alice6 LDxS with high accuracy.
C1 [Yang, Cheng; Stankovic, Vladimir] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XQ, Lanark, Scotland.
   [Cheung, Gene; Ono, Nobutaka] Natl Inst Informat, Tokyo 1018430, Japan.
   [Chan, Kevin] Campbelltown Hosp, Sydney, NSW 2560, Australia.
   [Chan, Kevin] Camden Hosp, Sydney, NSW 2560, Australia.
   [Chan, Kevin] South West Local Area Hlth Serv, Liverpool, NSW 2170, Australia.
   [Chan, Kevin] Univ Western Sydney, Sch Med, Sydney, NSW 2150, Australia.
C3 University of Strathclyde; Research Organization of Information &
   Systems (ROIS); National Institute of Informatics (NII) - Japan; NSW
   Health; Campbelltown Hospital; Western Sydney University
RP Yang, C (corresponding author), Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XQ, Lanark, Scotland.
EM cheng.yang@strath.ac.uk; cheung@nii.ac.jp;
   vladimir.stankovic@strath.ac.uk; drkevinchan@yahoo.com.au;
   onono@nii.ac.jp
RI Cheung, Gene/AAB-9284-2020; Stankovic, Vladimir/L-6584-2016
OI Stankovic, Vladimir/0000-0002-1075-2420; Cheung,
   Gene/0000-0002-5571-4137; Ono, Nobutaka/0000-0003-4242-2773
FU FP7 QoSTREAM Project; JSPS [15K12072]; Grants-in-Aid for Scientific
   Research [15K12072] Funding Source: KAKEN
FX This work was supported in part by FP7 QoSTREAM Project and JSPS
   Grant-in-Aid for Challenging Exploratory Research (15K12072). The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Jane You.
CR [Anonymous], 1992, Wavelets and their Applications
   [Anonymous], 2008, A wavelet tour of signal processing: The sparse way
   [Anonymous], 2013, COMMUN COMPUT INF SC, DOI DOI 10.1007/978-3-642-38256-7_21
   [Anonymous], 2016, INT J COMPUTER VISIO
   Avalur D. S., 2013, THESIS
   Behar J, 2015, IEEE J BIOMED HEALTH, V19, P325, DOI 10.1109/JBHI.2014.2307913
   Behar J, 2013, PHYSIOL MEAS, V34, pR29, DOI 10.1088/0967-3334/34/7/R29
   Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006
   Berry RB, 2012, J CLIN SLEEP MED, V8, P597, DOI 10.5664/jcsm.2172
   Boyd S.P., 2004, Convex optimization, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441]
   Burden R., 1989, Numerical Analysis
   Centonze F, 2015, 2015 INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE FOR MULTIMEDIA UNDERSTANDING (IWCIM)
   Chan SH, 2011, IEEE T IMAGE PROCESS, V20, P3097, DOI 10.1109/TIP.2011.2158229
   Chen L, 2014, IEEE IJCNN, P1, DOI 10.1109/IJCNN.2014.6889506
   Chen ZY, 2013, INT CONF PER COMP, P145, DOI 10.4108/icst.pervasivehealth.2013.252148
   COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732
   Daubechies I., 1992, Ten lectures on wavelets, DOI [DOI 10.1137/1.9781611970104, 10.1137/1.9781611970104]
   Eberly D., 2006, 3D GAME ENGINE DESIG
   Eberly D. H., 1998, TECH REP
   Falie D, 2008, MATH COMPUT SCI ENG, P179
   GANDER W, 1994, BIT, V34, P558, DOI 10.1007/BF01934268
   Giannakopoulos Theodoros, 2014, Introduction to Audio Analysis: A MATLAB Approach, DOI DOI 10.1016/C2012-0-03524-7
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hsu C.-W., 2003, TECH REP, DOI [DOI 10.1177/02632760022050997, 10 . 1177 / 02632760022050997]
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Hu W, 2015, IEEE T IMAGE PROCESS, V24, P419, DOI 10.1109/TIP.2014.2378055
   Hu W, 2013, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2013.6659254
   Hu W, 2012, IEEE IMAGE PROC, P1297, DOI 10.1109/ICIP.2012.6467105
   Huang YW, 2006, J VLSI SIG PROC SYST, V42, P297, DOI 10.1007/s11265-006-4190-4
   Iber C., 2007, The AASM Manual for the Scoring of Sleep and Associated Events: Rules, Terminology and Technical Specifications, DOI DOI 10.1002/EJOC.201200111
   Khoshelham K, 2012, SENSORS-BASEL, V12, P1437, DOI 10.3390/s120201437
   Khushaba RN, 2011, IEEE T BIO-MED ENG, V58, P121, DOI 10.1109/TBME.2010.2077291
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Kruger B., 2014, J MOBILE MULTIMEDIA, V10, P327
   Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lee J., 2015, INT J DISTRIB SENS N, V2015
   Lee MunWai., 2007, WMVC, P23
   Linlin Jiang, 2012, 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI), P894, DOI 10.1109/BHI.2012.6211732
   Loblaw A., 2013, P WORLD C COMP SCI C
   Mack DC, 2009, IEEE T INF TECHNOL B, V13, P111, DOI 10.1109/TITB.2008.2007194
   Madadi M, 2015, PATTERN RECOGN LETT, V56, P14, DOI 10.1016/j.patrec.2015.01.012
   Malakuti K., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4004, DOI 10.1109/ICPR.2010.974
   Malhotra A, 2002, LANCET, V360, P237, DOI 10.1016/S0140-6736(02)09464-3
   Martinez M, 2012, INT C PATT RECOG, P3472
   Matyunin S., 2011, 3DTV Conference: The True Vision-Capture, Transmission and Display of 3D Video, P1
   Metsis V, 2014, PERS UBIQUIT COMPUT, V18, P19, DOI 10.1007/s00779-012-0623-1
   Min DB, 2012, IEEE T IMAGE PROCESS, V21, P1176, DOI 10.1109/TIP.2011.2163164
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Oliver e N., 2007, Journal of Communication, V2, P1, DOI DOI 10.4304/JCM.2.2.1-9
   Paalasmaa J, 2012, IEEE ENG MED BIO, P3784, DOI 10.1109/EMBC.2012.6346791
   PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203
   Pang JH, 2015, INT CONF ACOUST SPEE, P2294, DOI 10.1109/ICASSP.2015.7178380
   Pang JY, 2014, 2014 IEEE CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (PHM)
   Patwari N, 2014, IEEE T MOBILE COMPUT, V13, P1774, DOI 10.1109/TMC.2013.117
   Peppard PE, 2000, NEW ENGL J MED, V342, P1378, DOI 10.1056/NEJM200005113421901
   Pohlmann K.C., 2010, PRINCIPLES DIGITAL A, V6
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Rosin PL, 1996, PATTERN RECOGN LETT, V17, P1461, DOI 10.1016/S0167-8655(96)00102-X
   SAFAEERAD R, 1991, CVGIP-IMAG UNDERSTAN, V54, P259, DOI 10.1016/1049-9660(91)90067-Y
   Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Tomasi C, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P839, DOI 10.1109/ICCV.1998.710815
   Wang CW, 2014, IEEE T BIO-MED ENG, V61, P396, DOI 10.1109/TBME.2013.2280132
   Wang CW, 2010, INT J COMPUT VISION, V90, P313, DOI 10.1007/s11263-010-0365-3
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Yang CX, 2014, ADV INTEL SYS RES, V59, P1
   Yu J, 2014, INFORM SCIENCES, V281, P674, DOI 10.1016/j.ins.2014.01.025
   Yu J, 2014, IEEE T CYBERNETICS, V44, P2431, DOI 10.1109/TCYB.2014.2307862
   Zhang ZY, 2000, IEEE T PATTERN ANAL, V22, P1330, DOI 10.1109/34.888718
NR 73
TC 23
Z9 25
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 822
EP 835
DI 10.1109/TMM.2016.2626969
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500013
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Ning, QQ
   Zhu, JK
   Zhong, ZY
   Hoi, SCH
   Chen, C
AF Ning, Qingqun
   Zhu, Jianke
   Zhong, Zhiyuan
   Hoi, Steven C. H.
   Chen, Chun
TI Scalable Image Retrieval by Sparse Product Quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate nearest neighbor (ANN) search; image retrieval; product
   quantization; sparse representation
ID OBJECT; REPRESENTATION; TREES; SCENE
AB Fast approximate nearest neighbor ( ANN) search technique for high-dimensional feature indexing and retrieval is the crux of large-scale image retrieval. A recent promising technique is product quantization, which attempts to index high dimensional image features by decomposing the feature space into a Cartesian product of low-dimensional subspaces and quantizing each of them separately. Despite the promising results reported, their quantization approach follows the typical hard assignment of traditional quantization methods, which may result in large quantization errors, and thus, inferior search performance. Unlike the existing approaches, in this paper, we propose a novel approach called sparse product quantization (SPQ) to encoding the high-dimensional feature vectors into sparse representation. We optimize the sparse representations of the feature vectors by minimizing their quantization errors, making the resulting representation is essentially close to the original data in practice. Experiments show that the proposed SPQ technique is not only able to compress data, but also an effective encoding technique. We obtain state-of-the-art results for ANN search on four public image datasets and the promising results of content-based image retrieval further validate the efficacy of our proposed method.
C1 [Ning, Qingqun; Zhu, Jianke; Zhong, Zhiyuan; Chen, Chun] Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
   [Hoi, Steven C. H.] Singapore Management Univ, Sch Informat Syst, Singapore 188065, Singapore.
C3 Zhejiang University; Singapore Management University
RP Zhu, JK (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou 310027, Peoples R China.
EM ningqingqun@zju.edu.cn; jkzhu@zju.edu.cn; zyzhong@zju.edu.cn;
   chhoi@smu.edu.sg; chenc@zju.edu.cn
RI HOI, Steven C. H./A-3736-2011; Zhong, Zhiyuan/U-4589-2019
OI Zhong, Zhiyuan/0000-0003-4175-4741; Hoi, Steven/0000-0002-4584-3453;
   Zhu, Jianke/0000-0003-1831-0106
FU National Key Research and Development Program of China [2016YFB1001501];
   Fundamental Research Funds for the Central Universities [2015XZZX005-05]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001501, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   2015XZZX005-05.
CR [Anonymous], 2012, NIPS
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Brandt J, 2010, PROC CVPR IEEE, P1815, DOI 10.1109/CVPR.2010.5539852
   Cai Junjie., 2014, Proceedings of International Conference on Multimedia Retrieval, P407
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Ge TZ, 2014, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2014.125
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Ge TZ, 2013, PROC CVPR IEEE, P2946, DOI 10.1109/CVPR.2013.379
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Jegou H., 2014, P IEEE INT C COMP VI, P3304
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Johnson W.B., 1984, CONTEMP MATH-SINGAP, V26, P189, DOI DOI 10.1090/CONM/026/737400
   Joly A, 2011, PROC CVPR IEEE, P873, DOI 10.1109/CVPR.2011.5995709
   Kafai M, 2014, IEEE T MULTIMEDIA, V16, P1090, DOI 10.1109/TMM.2014.2305633
   Lepetit V, 2005, PROC CVPR IEEE, P775
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Norouzi M.E., 2011, ICML
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Perd'och M, 2009, PROC CVPR IEEE, P9, DOI 10.1109/CVPRW.2009.5206529
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Shakhnarovich G., 2006, NEAREST NEIGHBOR MET
   Silpa-Anan C, 2008, PROC CVPR IEEE, P2308
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Zhang SL, 2014, IEEE T IMAGE PROCESS, V23, P3671, DOI 10.1109/TIP.2014.2330794
   Zhong ZY, 2015, IEEE T MULTIMEDIA, V17, P1391, DOI 10.1109/TMM.2015.2446201
NR 47
TC 25
Z9 25
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 586
EP 597
DI 10.1109/TMM.2016.2625260
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400013
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Liu, JY
   Yang, WH
   Zhang, XF
   Guo, ZM
AF Liu, Jiaying
   Yang, Wenhan
   Zhang, Xinfeng
   Guo, Zongming
TI Retrieval Compensated Group Structured Sparsity for Image
   Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE External method; internal method; retrieval compensation;
   super-resolution; structured sparsity
ID REPRESENTATION; ALGORITHMS; REGULARIZATION; APPROXIMATION
AB Sparse representation-based image super-resolution is a well-studied topic; however, a general sparse framework that can utilize both internal and external dependencies remains unexplored. In this paper, we propose a group-structured sparse representation approach to make full use of both internal and external dependencies to facilitate image super-resolution. External compensated correlated information is introduced by a two-stage retrieval and refinement. First, in the global stage, the content-based features are exploited to select correlated external images. Then, in the local stage, the patch similarity, measured by the combination of content and high-frequency patch features, is utilized to refine the selected external data. To better learn priors from the compensated external data based on the distribution of the internal data and further complement their advantages, nonlocal redundancy is incorporated into the sparse representation model to form a group sparsity framework based on an adaptive structured dictionary. Our proposed adaptive structured dictionary consists of two parts: one trained on internal data and the other trained on compensated external data. Both are organized in a cluster-based form. To provide the desired over-completeness property, when sparsely coding a given LR patch, the proposed structured dictionary is generated dynamically by combining several of the nearest internal and external orthogonal subdictionaries to the patch instead of selecting only the nearest one as in previous methods. Extensive experiments on image super-resolution validate the effectiveness and state-of-the-art performance of the proposed method. Additional experiments on contaminated and uncorrelated external data also demonstrate its superior robustness.
C1 [Liu, Jiaying; Yang, Wenhan; Zhang, Xinfeng; Guo, Zongming] Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Liu, JY (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100871, Peoples R China.
EM liujiaying@pku.edu.cn; yangwen-han@pku.edu.cn; xfzhang@ntu.edu.sg;
   guozongming@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019; Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National High-Tech Technology R&D Program (863 Program) of China
   [2014AA015205]; National Natural Science Foundation of China [61472011]
FX This work was supported by the National High-Tech Technology R&D Program
   (863 Program) of China under Grant 2014AA015205, and by the National
   Natural Science Foundation of China under Contract 61472011.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   Aly HA, 2005, IEEE T IMAGE PROCESS, V14, P1647, DOI 10.1109/TIP.2005.851684
   [Anonymous], 2014, CONT BAS MULT IND CB, DOI DOI 10.1109/CBMI.2014.6849821
   Bai W., 2013, P IEEE VIS COMM IM P, P1
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Bengio S., 2009, Advances in Neural Information Processing Systems, V22, P82
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Burger HC, 2013, LECT NOTES COMPUT SC, V8142, P121, DOI 10.1007/978-3-642-40602-7_13
   Cai JF, 2009, NUMER MATH, V112, P509, DOI 10.1007/s00211-009-0222-x
   Chakrabarti A, 2007, IEEE T MULTIMEDIA, V9, P888, DOI 10.1109/TMM.2007.893346
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chatzichristofis SA, 2008, LECT NOTES COMPUT SC, V5008, P312
   Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306
   Eisemann M., 2010, Proceedings of Graphics Interface 2010, GI'10, P71
   Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Gregor K, 2010, P 27 INT C INT C MAC, P399
   He L, 2013, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2013.51
   HUANG JB, 2015, PROC CVPR IEEE, P5197, DOI DOI 10.1109/CVPR.2015.7299156
   Huang T.S., 1984, ADV COMPUTER VISION, V1, P317
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Katkovnik V, 2010, INT J COMPUT VISION, V86, P1, DOI 10.1007/s11263-009-0272-7
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Li X, 2001, IEEE T IMAGE PROCESS, V10, P1521, DOI 10.1109/83.951537
   Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828
   Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Mosseri I, 2013, IEEE INT CONF COMPUT
   Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376
   Rao BD, 1999, IEEE T SIGNAL PROCES, V47, P187, DOI 10.1109/78.738251
   Ren J, 2013, IEEE T IMAGE PROCESS, V22, P1454, DOI 10.1109/TIP.2012.2231690
   Sun J., 2010, CORR
   Sun J, 2011, IEEE T IMAGE PROCESS, V20, P1529, DOI 10.1109/TIP.2010.2095871
   Sun JA, 2010, PROC CVPR IEEE, P231, DOI 10.1109/CVPR.2010.5540206
   Sun L, 2012, INT C COMP PHOT, P1, DOI DOI 10.1109/ICCPHOT.2012.6215221
   Timofte R, 2016, COMPUT VIS IMAGE UND, V142, P1, DOI 10.1016/j.cviu.2015.09.008
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Timofte Radu., 2015, CoRR
   Tropp JA, 2006, SIGNAL PROCESS, V86, P572, DOI 10.1016/j.sigpro.2005.05.030
   Tropp JA, 2006, SIGNAL PROCESS, V86, P589, DOI 10.1016/j.sigpro.2005.05.031
   Wang SL, 2012, PROC CVPR IEEE, P2216, DOI 10.1109/CVPR.2012.6247930
   Wang ZY, 2015, IEEE T IMAGE PROCESS, V24, P4359, DOI 10.1109/TIP.2015.2462113
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Xiong ZW, 2013, IEEE T MULTIMEDIA, V15, P1458, DOI 10.1109/TMM.2013.2264654
   Yang CY, 2011, LECT NOTES COMPUT SC, V6494, P497, DOI 10.1007/978-3-642-19318-7_39
   Yang JC, 2012, IEEE T IMAGE PROCESS, V21, P3467, DOI 10.1109/TIP.2012.2192127
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Yue HJ, 2013, IEEE T IMAGE PROCESS, V22, P4865, DOI 10.1109/TIP.2013.2279315
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang J, 2013, IEEE DATA COMPR CONF, P331, DOI 10.1109/DCC.2013.41
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang L, 2006, IEEE T IMAGE PROCESS, V15, P2226, DOI 10.1109/TIP.2006.877407
   Zhu Y, 2014, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR.2014.373
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   ZONTAK M, 2011, PROC CVPR IEEE, P977, DOI DOI 10.1109/CVPR.2011.5995401
   Zuo WM, 2013, PROC CVPR IEEE, P1203, DOI 10.1109/CVPR.2013.159
NR 65
TC 62
Z9 64
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 302
EP 316
DI 10.1109/TMM.2016.2614427
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800007
DA 2024-07-18
ER

PT J
AU Alaa, AM
   Moon, KH
   Hsu, W
   van der Schaar, M
AF Alaa, Ahmed M.
   Moon, Kyeong H.
   Hsu, William
   van der Schaar, Mihaela
TI ConfidentCare: A Clinical Decision Support System for Personalized
   Breast Cancer Screening
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Breast cancer; clinical decision support; multimedia-based healthcare;
   personalized medicine; personalized screening policies; supervised
   learning
ID RISK; MAMMOGRAPHY; DIAGNOSIS; DENSITY; GUIDELINES; BENEFITS; THERAPY;
   MODEL; AGE
AB Breast cancer screening policies attempt to achieve timely diagnosis by regularly screening healthy women via various imaging tests. Various clinical decisions are needed to manage the screening process: selecting initial screening tests, interpreting test results, and deciding if further diagnostic tests are required. Current screening policies are guided by clinical practice guidelines (CPGs), which represent a "one-size-fits-all" approach, designed to work well (on average) for a population, and can only offer coarse expert-based patient stratification that is not rigorously validated through data. Since the risks and benefits of screening tests are functions of each patient's features, personalized screening policies tailored to the features of individuals are desirable. To address this issue, we developed ConfidentCare: a computer-aided clinical decision support system that learns a personalized screening policy from electronic health record (EHR) data. By a "personalized screening policy," we mean a clustering of women's features, and a set of customized screening guidelines for each cluster. ConfidentCare operates by computing clusters of patients with similar features, then learning the "best" screening procedure for each cluster using a supervised learning algorithm. The algorithm ensures that the learned screening policy satisfies a predefined accuracy requirement with a high level of confidence for every cluster. By applying ConfidentCare to real-world data, we show that it outperforms the current CPGs in terms of cost efficiency and false positive rates: a reduction of 31% in the false positive rate can be achieved.
C1 [Alaa, Ahmed M.; Moon, Kyeong H.; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
   [Hsu, William] Univ Calif Los Angeles, Dept Radiol Sci, Los Angeles, CA 90024 USA.
C3 University of California System; University of California Los Angeles;
   University of California System; University of California Los Angeles
RP Alaa, AM (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.
EM a7med3laa@hotmail.com; kmoon0320@ucla.edu; willhsu@mii.ucla.edu;
   mihaela@ee.ucla.edu
RI Hsu, William/AAA-1935-2021
OI Hsu, William/0000-0002-5168-070X; van der schaar,
   Mihaela/0000-0003-3933-6049
FU U.S. National Science Foundation ECCS [1462245]; Directorate For
   Engineering; Div Of Electrical, Commun & Cyber Sys [1462245] Funding
   Source: National Science Foundation
FX This work was supported by the U.S. National Science Foundation ECCS
   under Grant 1462245. The guest editor team coordinated the review of
   this manuscript and approved it for publication.
CR Aebi S, 2011, ANN ONCOL, V22, pvi12, DOI 10.1093/annonc/mdr371
   Alaa A. M., 2016, CONFIDENTCARE CLIN D
   American College of Radiology, 2013, ACR BI RADS ATLAS BR
   [Anonymous], 1993, MORGAN KAUFMANN SERI
   [Anonymous], 2015, AM CANC SOC REC EARL
   Cardoso F, 2012, ANN ONCOL, V23, P11, DOI 10.1093/annonc/mds232
   Chakraborty B, 2014, ANNU REV STAT APPL, V1, P447, DOI 10.1146/annurev-statistics-022513-115553
   Chin L, 2011, NAT MED, V17, P297, DOI 10.1038/nm.2323
   Freitas A, 2007, LECT NOTES COMPUT SC, V4654, P303
   GAIL MH, 1989, JNCI-J NATL CANCER I, V81, P1879, DOI 10.1093/jnci/81.24.1879
   Gail MH, 1999, J NATL CANCER I, V91, P1829, DOI 10.1093/jnci/91.21.1829
   Greiner R, 2002, ARTIF INTELL, V139, P137, DOI 10.1016/S0004-3702(02)00209-6
   Hamburg MA, 2010, NEW ENGL J MED, V363, P301, DOI 10.1056/NEJMp1006304
   Harding C, 2015, JAMA INTERN MED, V175, P1483, DOI 10.1001/jamainternmed.2015.3043
   Henderson LM, 2015, CANCER EPIDEM BIOMAR, V24, P1882, DOI 10.1158/1055-9965.EPI-15-0623
   Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8
   Keen J. D., 2011, ANN INTERNAL MED, V155
   Kerlikowske K, 1996, JAMA-J AM MED ASSOC, V276, P39, DOI 10.1001/jama.276.1.39
   KULKARNI SR, 1993, MACH LEARN, V11, P23, DOI 10.1023/A:1022627018023
   Laber E. B., 2014, ELECT J STAT, V8
   Liberman L, 2002, RADIOL CLIN N AM, V40, P409, DOI 10.1016/S0033-8389(01)00017-3
   Ling Charles X., 2004, P 21 INT C MACH LEAR, P69, DOI DOI 10.1109/TSMCB.2008.2007853
   Liu F, 2013, IEEE T MULTIMEDIA, V15, P1025, DOI 10.1109/TMM.2013.2244204
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Lomax S, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431215
   Molinaro Sabrina., 2015, NEW HORIZ TRANSL MED, V2, P59, DOI DOI 10.1016/J.NHTM.2014.11.017
   Moodie EEM, 2007, BIOMETRICS, V63, P447, DOI 10.1111/j.1541-0420.2006.00686.x
   Morris JK, 2008, J MED SCREEN, V15, P55, DOI [10.1258/jms.2008.007105, 10.1258/jms.2008.008got]
   Onega T, 2014, CANCER-AM CANCER SOC, V120, P2955, DOI 10.1002/cncr.28771
   Patil D., 2016, AAAS ANN M 2016 WASH
   Persello C, 2014, IEEE T GEOSCI REMOTE, V52, P6937, DOI 10.1109/TGRS.2014.2305805
   Ribeiro MX, 2008, IEEE T MULTIMEDIA, V10, P277, DOI 10.1109/TMM.2007.911837
   Rosenberg RD, 1998, RADIOLOGY, V209, P511, DOI 10.1148/radiology.209.2.9807581
   Schousboe JT, 2011, ANN INTERN MED, V155, P10, DOI 10.7326/0003-4819-155-1-201107050-00003
   Shwartz S. S.-, 2014, UNDERSTANDING MACHIN
   Smith RA, 2002, CA-CANCER J CLIN, V52, P8, DOI 10.3322/canjclin.52.1.8
   TABAR L, 1985, LANCET, V1, P829, DOI 10.1016/S0140-6736(85)92204-4
   Tice JA, 2005, BREAST CANCER RES TR, V94, P115, DOI 10.1007/s10549-005-5152-4
   von Eschenbach AC, 2002, ONCOLOGIST, V7, P170, DOI 10.1634/theoncologist.7-3-170
   Weedon-Fekjær H, 2014, BMJ-BRIT MED J, V348, DOI 10.1136/bmj.g3701
   Yu S., 2009, P INT C ART INT STAT, P12
NR 41
TC 26
Z9 30
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1942
EP 1955
DI 10.1109/TMM.2016.2589160
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lei, XJ
   Qian, XM
   Zhao, GS
AF Lei, Xiaojiang
   Qian, Xueming
   Zhao, Guoshuai
TI Rating Prediction Based on Social Sentiment From Textual Reviews
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Item reputation; rating prediction; recommender system (RS); reviews;
   sentiment influence; user sentiment
ID RECOMMENDATION
AB In recent years, we have witnessed a flourish of review websites. It presents a great opportunity to share our viewpoints for various products we purchase. However, we face an information overloading problem. How to mine valuable information from reviews to understand a user's preferences and make an accurate recommendation is crucial. Traditional recommender systems (RS) consider some factors, such as user's purchase records, product category, and geographic location. In this work, we propose a sentiment-based rating prediction method (RPS) to improve prediction accuracy in recommender systems. Firstly, we propose a social user sentimental measurement approach and calculate each user's sentiment on items/products. Secondly, we not only consider a user's own sentimental attributes but also take interpersonal sentimental influence into consideration. Then, we consider product reputation, which can be inferred by the sentimental distributions of a user set that reflect customers' comprehensive evaluation. At last, we fuse three factors-user sentiment similarity, interpersonal sentimental influence, and item's reputation similarity-into our recommender system to make an accurate rating prediction. We conduct a performance evaluation of the three sentimental factors on a real-world dataset collected from Yelp. Our experimental results show the sentiment can well characterize user preferences, which helps to improve the recommendation performance.
C1 [Lei, Xiaojiang; Qian, Xueming; Zhao, Guoshuai] Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Minist Educ, Key Lab Intelligent Networks & Network Secur, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Lei, XJ (corresponding author), Xi An Jiao Tong Univ, SMILES Lab, Xian 710049, Peoples R China.
EM xiaolei3439@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn;
   zgs2012@stu.xjtu.edu.cn
RI Zhao, Guoshuai/AAN-1271-2020
OI Zhao, Guoshuai/0000-0003-4392-8450
FU Program 973 [2012CB316400]; National Science Foundation of China
   [60903121, 61173109, 61332018]; Microsoft Research Asia
FX This work was supported in part by Program 973 No. 2012CB316400, in part
   by the National Science Foundation of China under Grant 60903121, Grant
   61173109, and Grant 61332018, and in part by Microsoft Research Asia.
   The associate editor coordinating the review of this manuscript and
   approving it for publicationwas Dr. Sen-Ching Samson Cheung.
CR [Anonymous], NEW TECHNOL LIB INFO
   [Anonymous], IEEE T PARA IN PRESS
   [Anonymous], P 2011 IEEE POW EN S
   [Anonymous], 2006, P 5 INT C LANG RES E
   [Anonymous], NATURAL LANG ENG
   [Anonymous], 2013, P 7 ACM C REC SYST
   Bedi P., 2007, PROC IJCAI 07, P2677
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Cai Y, 2014, IEEE T KNOWL DATA EN, V26, P766, DOI 10.1109/TKDE.2013.7
   Chen YY, 2013, IEEE T MULTIMEDIA, V15, P1283, DOI 10.1109/TMM.2013.2265077
   Chen ZS, 2011, IEEE T MULTIMEDIA, V13, P1371, DOI 10.1109/TMM.2011.2166380
   D'Argembeau A, 2005, EMOTION, V5, P503, DOI 10.1037/1528-3542.5.4.503
   Daud A, 2011, LECT NOTES COMPUT SC, V6612, P41, DOI 10.1007/978-3-642-20291-9_7
   Ding X., 2008, P 2008 INT C WEB SEA, P231, DOI [DOI 10.1145/1341531.1341561, 10.1145/1341531.1341561]
   Feng H, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1521
   Fletcher KK, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON WEB SERVICES (ICWS), P400, DOI 10.1109/ICWS.2015.60
   Fu ZJ, 2015, IEICE T COMMUN, VE98B, P190, DOI 10.1587/transcom.E98.B.190
   Ganu G., 2009, 12 INT WORKSH WEB DA
   Huang JM, 2010, FRONT ARTIF INTEL AP, V215, P601, DOI 10.3233/978-1-60750-606-5-601
   Jamali M., 2010, P 4 ACM C REC SYST, P135, DOI DOI 10.1145/1864708.1864736
   Jiang M, 2014, IEEE T KNOWL DATA EN, V26, P2789, DOI 10.1109/TKDE.2014.2300487
   Jiang Meng, 2012, P 21 ACM INT C INFOR, P45
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Kanayama H., 2006, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP-2006), P355
   Kawashima Takayuki, 2013, 2013 IEEE 2nd Global Conference on Consumer Electronics (GCCE), P260, DOI 10.1109/GCCE.2013.6664818
   Lee K, 2014, IEEE T MULTIMEDIA, V16, P1201, DOI 10.1109/TMM.2014.2311012
   Lei XJ, 2015, IEEE INT WORKSH MULT
   Li F., 2011, Proceedings of the Twenty-Second International joint Conference on Artificial Intelligence, V3, P1820
   Li S., 2010, P 23 INT C COMP LING, P635
   Lin YH, 2012, TOUR MANAG PERSPECT, V2-3, P35, DOI 10.1016/j.tmp.2012.01.002
   Ling G, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P105, DOI 10.1145/2645710.2645728
   Lu Yue., 2011, WWW, DOI [DOI 10.1145/1963405.1963456, 10.1145/1963405.1963456]
   Luo WJ, 2014, IEEE DATA MINING, P380, DOI 10.1109/ICDM.2014.14
   Ma Hao, 2008, P CIKM08 C INFORM KN, P931
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Mohammad Saif., 2009, P 2009 C EMPIRICAL M, P599, DOI DOI 10.3115/1699571.1699591
   Nakagawa T., 2010, HUMAN LANGUAGE TECHN, P786
   Pang B, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P79, DOI 10.3115/1118693.1118704
   Pang B., 2004, ANN M ASS COMP LING, P271, DOI [10.3115/1218955.1218990, DOI 10.3115/1218955.1218990]
   Pang B, 2005, P 43 ANN M ASS COMP, P115, DOI [10.3115/1219840.1219855, DOI 10.3115/1219840.1219855]
   Polanyi L, 2006, INFORM RETRIEVAL SER, V20, P1
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Qu L., 2010, COLING, P913
   Ren YJ, 2015, J INTERNET TECHNOL, V16, P317, DOI 10.6138/JIT.2015.16.2.20140918
   Resnick P., 1994, Transcending Boundaries, CSCW '94. Proceedings of the Conference on Computer Supported Cooperative Work, P175, DOI 10.1145/192844.192905
   Rosen-Zvi M, 2010, ACM T INFORM SYST, V28, DOI 10.1145/1658377.1658381
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Seroussi Yanir, 2011, P 22 ACM C HYPERTEXT, P47, DOI DOI 10.1145/1995966.1995976
   Shengxiang Gao, 2015, IEEE/CAA Journal of Automatica Sinica, V2, P403
   Sun BM, 2014, INT C COMP SUPP COOP, P546, DOI 10.1109/CSCWD.2014.6846903
   Taboada M, 2011, COMPUT LINGUIST, V37, P267, DOI 10.1162/COLI_a_00049
   Tan SL, 2014, IEEE T KNOWL DATA EN, V26, P1158, DOI 10.1109/TKDE.2013.116
   Tang DY, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1014
   Tso-Sutter KHL, 2008, APPLIED COMPUTING 2008, VOLS 1-3, P1995
   Venkatasubramanian Suresh., 2011, Proceedings of the 20th international conference companion on World wide web, P137
   Wang H, 2010, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P783, DOI [DOI 10.1145/1835804.1835903, 10.1145/1835804.1835903]
   Wang LX, 2013, PROCEEDINGS OF 2013 INTERNATIONAL SYMPOSIUM - WOMEN'S SURVIVAL AND DEVELOPMENT IN CURRENT CULTURAL ENVIRONMENT, P23
   Wang XY, 2015, IEEE T MULTIMEDIA, V17, P409, DOI 10.1109/TMM.2014.2385473
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Xia ZH, 2016, IEEE T PARALL DISTR, V27, P340, DOI 10.1109/TPDS.2015.2401003
   Xiong W, 2014, J COMPUT, V9, P1410, DOI 10.4304/jcp.9.6.1410-1417
   Xu JN, 2012, INT CONF E BUS ENG, P9, DOI 10.1109/ICEBE.2012.12
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zhang WS, 2013, ACM T INTEL SYST TEC, V4, DOI 10.1145/2414425.2414434
   Zhang YF, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P83, DOI 10.1145/2600428.2609579
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao ZL, 2015, PROCEEDINGS 2015 IEEE FIFTH INTERNATIONAL CONFERENCE ON BIG DATA AND CLOUD COMPUTING BDCLOUD 2015, P9, DOI 10.1109/BDCloud.2015.15
NR 67
TC 102
Z9 105
U1 1
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1910
EP 1921
DI 10.1109/TMM.2016.2575738
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800019
OA Bronze
DA 2024-07-18
ER

PT J
AU Yu, LT
   Huang, Z
   Cao, JW
   Shen, HT
AF Yu, Litao
   Huang, Zi
   Cao, Jiewei
   Shen, Heng Tao
TI Scalable Video Event Retrieval by Visual State Binary Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hashing; video event retrieval; visual state
ID CODES
AB With the exponential increase of media data on the web, fast media retrieval is becoming a significant research topic in multimedia content analysis. Among the variety of techniques, learning binary embedding (hashing) functions is one of the most popular approaches that can achieve scalable information retrieval in large databases, and it is mainly used in the near-duplicate multimedia search. However, till now most hashing methods are specifically designed for near-duplicate retrieval at the visual level rather than the semantic level. In this paper, we propose a visual state binary embedding (VSBE) model to encode the video frames, which can preserve the essential semantic information in binary matrices, to facilitate fast video event retrieval in unconstrained cases. Compared with other video binary embedding models, one advantage of our proposed VSBE model is that it only needs a limited number of key frames from the training videos for hash function training, so the computational complexity is much lower in the training phase. At the same time, we apply the pairwise constraints generated from the visual states to sketch the local properties of the events at the semantic level, so accuracy is also ensured. We conducted extensive experiments on the challenging TRECVID MED dataset, and have proved the superiority of our proposed VSBE model.
C1 [Yu, Litao; Huang, Zi; Cao, Jiewei; Shen, Heng Tao] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
C3 University of Queensland
RP Yu, LT (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM l.yu4@uq.edu.au; huang@itee.uq.edu.au; j.cao3@uq.edu.au;
   shenht@itee.uq.edu.au
RI Shen, Heng Tao/ABD-5331-2021; Cao, Jiewei/V-5378-2019
OI Cao, Jiewei/0000-0003-0681-6134; HUANG, ZI/0000-0002-9738-4949
CR [Anonymous], 2013, Book Linear cross-modal hashing for efficient multimedia search, DOI DOI 10.1145/2502081.2502107
   [Anonymous], CORR
   [Anonymous], 2013, Caffe: An open source convolutional architecture for fast feature embedding
   Cheng Y, 2014, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2014.286
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Cooper M., 2005, 2005 IEEE International Conference on Multimedia and Expo
   Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69
   Habibian A, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P17, DOI 10.1145/2647868.2654913
   Hu Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P527, DOI 10.1145/2647868.2654906
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Li X, 2013, PROC CVPR IEEE, P2419, DOI 10.1109/CVPR.2013.313
   Liu L, 2013, PATTERN RECOGN, V46, P1810, DOI 10.1016/j.patcog.2012.10.004
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Ma C, 2015, COMPUT VIS IMAGE UND, V135, P83, DOI 10.1016/j.cviu.2015.01.003
   Ma ZG, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P77, DOI 10.1145/2647868.2654907
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Sun C, 2014, PROC CVPR IEEE, P2569, DOI 10.1109/CVPR.2014.329
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Vahdat A, 2013, IEEE I CONF COMP VIS, P1185, DOI 10.1109/ICCV.2013.463
   Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36
   Wang J., 2014, CoRR
   Wang Jianfeng., 2013, ACM Multimedia, P133
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Weiss Yair, 2009, Advances in Neural Information Processing Systems, P1753, DOI DOI 10.5555/2981780.2981999
   Wu CX, 2013, IEEE T KNOWL DATA EN, V25, P1380, DOI 10.1109/TKDE.2012.76
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Ye GN, 2013, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2013.282
   Yu F. X., 2014, INT C MACH LEARN BEI
NR 33
TC 15
Z9 17
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1590
EP 1603
DI 10.1109/TMM.2016.2557059
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000012
DA 2024-07-18
ER

PT J
AU Gao, W
   Kwong, S
   Zhou, Y
   Yuan, H
AF Gao, Wei
   Kwong, Sam
   Zhou, Yu
   Yuan, Hui
TI SSIM-Based Game Theory Approach for Rate-Distortion Optimized Intra
   Frame CTU-Level Bit Allocation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit allocation; coding tree unit; game theory; high efficiency video
   coding (HEVC); Nash bargaining solution (NBS); rate control (RC);
   resource allocation; structural similarity
ID RATE-QUANTIZATION MODEL; STRUCTURAL SIMILARITY; VIDEO TRANSMISSION;
   MULTIVIEW VIDEO; EFFICIENCY; QUALITY; STANDARD; HEVC
AB A structural similarity (SSIM)-based game theory (GT) approach is proposed for rate-distortion (R-D) optimized CTU-level bit allocation in high efficiency video coding (HEVC). First, a SSIM-based bargaining game is formulated and the Nash bargaining solution (NBS) is proposed, in which a SSIM-based initial minimum utility is defined. Second, we propose a two-stage remaining bit refinement-based bit allocation scheme. The optimization scheme of the SSIM-based bargaining game sufficiently considers the different R-D characteristics of coding tree units (CTUs), in which the feasible utility set is proved to be convex based on the proposed SSIM-based utility and R-SSIM model. Compared with the other state-of-the-art CTU-level bit allocation methods, the R-D performance improvements on Bjontegaard delta bit-rate (BD-BR), Bjontegaard delta peak-signal-to-noise-ratio (BD-PSNR), and BD-SSIM metrics of the proposed method can averagely achieve significant gains, respectively. The achieved R-D performance gains have been very close to the coding performance limits from the FixedQP method. Moreover, the proposed SSIM-GT method also maintains good performances on quality smoothness, bit rate accuracy, and encoding complexity.
C1 [Gao, Wei; Kwong, Sam; Zhou, Yu] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Gao, Wei; Kwong, Sam; Zhou, Yu] Shenzhen Res Inst, Shenzhen 5180057, Peoples R China.
   [Yuan, Hui] Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
C3 City University of Hong Kong; Shandong University
RP Gao, W; Kwong, S; Zhou, Y (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.; Gao, W; Kwong, S; Zhou, Y (corresponding author), Shenzhen Res Inst, Shenzhen 5180057, Peoples R China.; Yuan, H (corresponding author), Shandong Univ, Sch Informat Sci & Engn, Jinan 250100, Peoples R China.
EM weigao5-c@my.cityu.edu.hk; cssamk@cityu.edu.hk;
   yzhou57-c@my.cityu.edu.hk; yuanhui0325@gmail.com
RI Kwong, Sam/C-9319-2012; Yuan, Hui/HDO-3699-2022; Yuan, Hui/B-6738-2013
OI Kwong, Sam/0000-0001-7484-7261; Yuan, Hui/0000-0001-5212-3393; Yuan,
   Hui/0000-0001-5212-3393; ZHOU, Yu/0000-0002-3224-0063
FU National Natural Science Foundation of China [61272289, 61571274]; City
   University of Hong Kong Strategic Research Grant [7004418]; Hong Kong
   RGC General Research Fund [9042038, CityU 11205314]; City University of
   Hong Kong Shenzhen Research Institute, Shenzhen, China; Young Scholars
   Program of Shandong University (YSPSDU) [2015WLJH39]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61272289 and Grant 61571274, in part by
   the City University of Hong Kong Strategic Research Grant 7004418, in
   part by the Hong Kong RGC General Research Fund 9042038 (CityU
   11205314), in part by City University of Hong Kong Shenzhen Research
   Institute, Shenzhen, China, and in part by the Young Scholars Program of
   Shandong University (YSPSDU) under Grant 2015WLJH39. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Adrian Munteanu.
CR [Anonymous], 2014, CHINA MINING MAGAZIN
   Bjontegaard G., 2001, VCEG M AUST TX USA A
   Boyd S., 2004, CONVEX OPTIMIZATION
   Brunet D, 2012, IEEE T IMAGE PROCESS, V21, P1488, DOI 10.1109/TIP.2011.2173206
   Choi H, 2013, IEEE J-STSP, V7, P1112, DOI 10.1109/JSTSP.2013.2272241
   Chuah SP, 2015, IEEE T MULTIMEDIA, V17, P687, DOI 10.1109/TMM.2015.2413354
   Colonnese S, 2013, IEEE T MULTIMEDIA, V15, P1800, DOI 10.1109/TMM.2013.2271475
   Gao W, 2016, IEEE T CIRC SYST VID, V26, P139, DOI 10.1109/TCSVT.2015.2444671
   Hachicha W, 2015, IEEE T MULTIMEDIA, V17, P765, DOI 10.1109/TMM.2015.2417099
   Hu SD, 2012, IEEE T IND ELECTRON, V59, P1673, DOI 10.1109/TIE.2011.2157282
   Hu SD, 2011, IEEE T CIRC SYST VID, V21, P1152, DOI 10.1109/TCSVT.2011.2138810
   Jiménez-Rodríguez L, 2013, IEEE T MULTIMEDIA, V15, P15, DOI 10.1109/TMM.2012.2199973
   Karczewicz M., 2013, 13 M JCTVC M0257 INC
   Lee B, 2014, IEEE T CIRC SYST VID, V24, P465, DOI 10.1109/TCSVT.2013.2276880
   Li B., 2012, 11 M JCTVC K0103 SHA
   Li B., 2012, 9 M JCTVC I0426 GEN
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Luo JC, 2010, IEEE T MULTIMEDIA, V12, P97, DOI 10.1109/TMM.2009.2037385
   McCann K., 2010, 1 M JCTVC A124 DRESD
   Nash JF, 1950, ECONOMETRICA, V18, P155, DOI 10.2307/1907266
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Seo CW, 2013, IEEE T IMAGE PROCESS, V22, P2442, DOI 10.1109/TIP.2013.2251647
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tang CW, 2006, IEEE T MULTIMEDIA, V8, P11, DOI 10.1109/TMM.2005.861295
   Wang MH, 2015, IEEE SIGNAL PROC LET, V22, P896, DOI 10.1109/LSP.2014.2377032
   Wang SS, 2013, IEEE J-STSP, V7, P1101, DOI 10.1109/JSTSP.2013.2272240
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang X, 2013, IEEE T BROADCAST, V59, P591, DOI 10.1109/TBC.2013.2249351
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu L, 2011, IEEE T IMAGE PROCESS, V20, P723, DOI 10.1109/TIP.2010.2063708
   Yan B, 2012, IEEE T MULTIMEDIA, V14, P936, DOI 10.1109/TMM.2012.2184743
   Yuan H, 2015, IEEE T MULTIMEDIA, V17, P2134, DOI 10.1109/TMM.2015.2477682
NR 34
TC 56
Z9 62
U1 0
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 988
EP 999
DI 10.1109/TMM.2016.2535254
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100004
DA 2024-07-18
ER

PT J
AU Zhou, P
   Zhou, YX
   Wu, DP
   Jin, H
AF Zhou, Pan
   Zhou, Yingxue
   Wu, Dapeng
   Jin, Hai
TI Differentially Private Online Learning for Cloud-Based Video
   Recommendation With Multimedia Big Data in Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Differential privacy; distributed online learning; media cloud;
   multimedia big data; online social networks (OSNs); video recommendation
AB With the rapid growth in multimedia services and the enormous offers of video content in online social networks, users have difficulty in obtaining their interests. Therefore, various personalized recommendation systems have been proposed. However, they ignore that the accelerated proliferation of social media data has led to the big data era, which has greatly impeded the process of video recommendation. In addition, none of them has considered both the privacy of users' contexts (e.g., social status, ages, and hobbies) and video service vendors' repositories, which are extremely sensitive and of significant commercial value. To handle these problems, we propose a cloud-assisted differentially private video recommendation system based on distributed online learning. In our framework, service vendors are modeled as distributed cooperative learners, recommending videos according to user's context, while simultaneously adapting the video-selection strategy based on user-click feedback to maximize total user clicks (reward). Considering the sparsity and heterogeneity of big social media data, we also propose a novel geometric differentially private model, which can greatly reduce the performance loss. Our simulation shows the proposed algorithms outperform other existing methods and keep a delicate balance between the total reward and privacy preserving level.
C1 [Zhou, Pan; Zhou, Yingxue] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Wu, Dapeng] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
   [Jin, Hai] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; State University System of
   Florida; University of Florida; Huazhong University of Science &
   Technology
RP Zhou, P; Zhou, YX (corresponding author), Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.; Wu, DP (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.; Jin, H (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
EM panzhou@hust.edu.cn; hustzhouyx@gmail.com; wu@ece.ufl.edu;
   hjin@hust.edu.cn
OI Wu, Dapeng/0000-0003-1755-0183
FU National Science Foundation of China [61401169, CNS-1116970, NSFC
   61529101]; Division Of Computer and Network Systems; Direct For Computer
   & Info Scie & Enginr [1116970] Funding Source: National Science
   Foundation
FX This work was supported by the National Science Foundation of China
   under Grant 61401169, Grant CNS-1116970, and Grant NSFC 61529101. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Alessandro Piva. (Pan Zhou and
   Yingxue Zhou contributed equally to this work.)
CR Aggarwal C.C., 2005, P 31 INT C VER LARG, P901, DOI [DOI 10.5555/1083592.1083696, 10.5555/1083592.1083696]
   [Anonymous], 2013, Social Media Retrieval
   Baluja S, 2008, WORLD WID WEB C, P895
   Brickell Justin, 2008, P 14 ACM SIGKDD INT, P70, DOI [10.1145/1401890.1401904, DOI 10.1145/1401890.1401904]
   Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626
   Cheung M., 2014, P 4 INT C ADV INF MI, P83
   Cheung M, 2015, IEEE T MULTIMEDIA, V17, P1417, DOI 10.1109/TMM.2015.2460192
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042
   Dwork C, 2010, ACM S THEORY COMPUT, P715
   Erkin Z, 2012, IEEE T INF FOREN SEC, V7, P1053, DOI 10.1109/TIFS.2012.2190726
   Gabrilovich E., 2004, P 13 INT C WORLD WID, P482, DOI DOI 10.1145/988672.988738
   Geonhyeok Go, 2010, Proceedings of the 2010 IEEE Second International Conference on Social Computing (SocialCom 2010). the Second IEEE International Conference on Privacy, Security, Risk and Trust (PASSAT 2010), P439, DOI 10.1109/SocialCom.2010.70
   Jeckmans A. J. P., 2013, Social media retrieval, P263, DOI [10.1007/978-1-4471-4555-4_12, DOI 10.1007/978-1-4471-4555-4_12]
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Jorgensen Zach., 2014, INT C EXTENDING DATA, P571, DOI 10.5441/002/edbt.2014.51
   Klarreich E, 2012, QUANTA MAGAZINE, V10, P1
   Langford John, 2008, Advances in neural information processing systems, P817
   Li L., 2010, P 19 INT C WORLD WID, DOI [10.1145/1772690.1772758, DOI 10.1145/1772690.1772758]
   Li L, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P125
   Machanavajjhala A, 2011, PROC VLDB ENDOW, V4, P440, DOI 10.14778/1988776.1988780
   McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66
   McSherry F, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P627
   Narayanan Arvind., 2006, CORR
   Newton C., 2013, THE VERGE       0801
   Chan NN, 2010, LECT NOTES COMPUT SC, V6426, P222
   Pazzani M. J., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P325
   Samuel A, 2015, IEEE T MULTIMEDIA, V17, P1484, DOI 10.1109/TMM.2015.2458299
   Shang S, 2014, P 29 ANN ACM S APPL, P266, DOI DOI 10.1145/2554850.2554924
   Slivkins A, 2014, J MACH LEARN RES, V15, P2533
   Tekin C, 2014, IEEE J-STSP, V8, P638, DOI 10.1109/JSTSP.2014.2299517
   Tekin C, 2013, ANN ALLERTON CONF, P1435, DOI 10.1109/Allerton.2013.6736696
   Tkalcic M, 2013, IEEE T MULTIMEDIA, V15, P391, DOI 10.1109/TMM.2012.2229970
   Xu J., 2015, IEEE T SIGNAL PROCES, V23, P2225
   Zhou P., 2015, DIFFERENTIALLY PRIVA
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
   Ziqi Wang, 2010, Proceedings of the 12th Asia Pacific Web Conference (APWEB 2010), P116, DOI 10.1109/APWeb.2010.60
NR 37
TC 63
Z9 66
U1 0
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1217
EP 1229
DI 10.1109/TMM.2016.2537216
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Batrinca, L
   Mana, N
   Lepri, B
   Sebe, N
   Pianesi, F
AF Batrinca, Ligia
   Mana, Nadia
   Lepri, Bruno
   Sebe, Nicu
   Pianesi, Fabio
TI Multimodal Personality Recognition in Collaborative Goal-Oriented Tasks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human-human interaction (HHI); human-machine interaction; map task;
   non-verbal behavior analysis; personality recognition
ID BEHAVIOR; TRAITS
AB Incorporating research on personality recognition into computers, both from a cognitive as well as an engineering perspective, would facilitate the interactions between humans and machines. Previous attempts on personality recognition have focused on a variety of different corpora (ranging from text to audiovisual data), scenarios (interviews, meetings), channels of communication (audio, video, text), and different subsets of personality traits (out of the five ones from the Big Five Model). Our study uses simple acoustic and visual nonverbal features extracted from multimodal data, which have been recorded in previously uninvestigated scenarios, and consider all five personality traits and not just a subset. First, we look at the human-machine interaction scenario, where we introduce the display of different "collaboration levels." Second, we look at the contribution of the human-human interaction (HHI) scenario on the emergence of personality traits. Investigating the HHI scenario creates a stronger basis for future human-agents interactions. Our goal is to study, from a computational approach, the emergence degree of the five personality traits in these two scenarios. The results demonstrate the relevance of each of the two scenarios when it comes to the degree of emergence of certain traits and the feasibility to automatically recognize personality under different conditions.
C1 [Batrinca, Ligia; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, Italy.
   [Mana, Nadia] Fdn Bruno Kessler, I-38122 Trento, Italy.
   [Lepri, Bruno] Fdn Bruno Kessler, Mobile & Social Comp Lab, I-38122 Trento, Italy.
   [Lepri, Bruno] Fdn Bruno Kessler, Complex Data Analyt Res Line, I-38122 Trento, Italy.
   [Pianesi, Fabio] Fdn Bruno Kessler, Cognit & Commun Technol Div, I-38122 Trento, Italy.
C3 University of Trento; Fondazione Bruno Kessler; Fondazione Bruno
   Kessler; Fondazione Bruno Kessler; Fondazione Bruno Kessler
RP Batrinca, L; Sebe, N (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-38122 Trento, Italy.; Mana, N (corresponding author), Fdn Bruno Kessler, I-38122 Trento, Italy.; Lepri, B (corresponding author), Fdn Bruno Kessler, Mobile & Social Comp Lab, I-38122 Trento, Italy.; Lepri, B (corresponding author), Fdn Bruno Kessler, Complex Data Analyt Res Line, I-38122 Trento, Italy.; Pianesi, F (corresponding author), Fdn Bruno Kessler, Cognit & Commun Technol Div, I-38122 Trento, Italy.
EM ligia.batrinca@gmail.com; mana@fbk.eu; lepri@fbk.eu; sebe@disi.unitn.it;
   pianesi@fbk.eu
RI Sebe, Niculae/KEC-2000-2024
OI Sebe, Niculae/0000-0002-6597-7248; Mana, Nadia/0000-0002-0678-1261
CR ANDERSON AH, 1991, LANG SPEECH, V34, P351, DOI 10.1177/002383099103400404
   Andre E., 2000, AFFECTIVE INTERACTIO
   Andrews K, 2008, SYNTHESE, V165, P13, DOI 10.1007/s11229-007-9230-5
   [Anonymous], 2006, P COLINGACL 2006 MAI
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   Aran O, 2014, IEEE T MULTIMEDIA, V16, P201, DOI 10.1109/TMM.2013.2284893
   Argamon S., 2005, JOINT ANN M INT CLAS
   ARONOVITCH CD, 1976, J SOC PSYCHOL, V99, P207, DOI 10.1080/00224545.1976.9924774
   Barron F.X., 1969, CREATIVE PERSON CREA
   Batrinca L, 2011, P 13 INT C MULT INT, P255, DOI [DOI 10.1145/2070481.2070528, 10.1145/2070481.2070528]
   Batrinca Ligia., 2013, Intelligent Virtual Agents
   Batrinca LM, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P39
   Bickmore T. W., 2005, ACM Transactions on Computer-Human Interaction, V12, P293, DOI 10.1145/1067860.1067867
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Boersma P., 2006, PRAAT SOFTWARE
   BORKENAU P, 1992, J PERS SOC PSYCHOL, V62, P645, DOI 10.1037/0022-3514.62.4.645
   BREBNER J, 1985, INDIVIDUAL DIFFERENC
   Campbell N., 2008, VERBAL NONVERBAL FEA
   Cassell J, 2003, USER MODEL USER-ADAP, V13, P89, DOI 10.1023/A:1024026532471
   Celli F, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1101, DOI 10.1145/2647868.2654977
   Chittaranjan G, 2013, PERS UBIQUIT COMPUT, V17, P433, DOI 10.1007/s00779-011-0490-1
   Coimbra MT, 2005, IEEE T CIRC SYST VID, V15, P103, DOI 10.1109/TCSVT.2004.837016
   Costa P. T., 1985, The NEO Personality Inventory Manual
   de Oliveira Rodrigo., 2011, CHI'11 Extended Abstracts on Human Factors in Computing Systems, P2191, DOI [10.1145/1979742.1979920, DOI 10.1145/1979742.1979920]
   Dewaele JM, 2000, PERS INDIV DIFFER, V28, P355, DOI 10.1016/S0191-8869(99)00106-3
   Feil-Seifer D, 2005, INT C REHAB ROBOT, P465
   Feist G.J., 2010, The Cambridge Handbook of Creativity
   FUNDER DC, 1993, J PERS SOC PSYCHOL, V64, P479, DOI 10.1037/0022-3514.64.3.479
   Furnham A, 1999, PERS INDIV DIFFER, V27, P1113, DOI 10.1016/S0191-8869(99)00053-7
   Furnham A., 1990, Handbook of language and social psychology, chapter language and personality
   Hall JA, 2010, J SOC PERS RELAT, V27, P117, DOI 10.1177/0265407509349633
   Hara F, 2004, RO-MAN 2004: 13TH IEEE INTERNATIONAL WORKSHOP ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION, PROCEEDINGS, P7, DOI 10.1109/ROMAN.2004.1374712
   Hennessey B. A., 2010, CAMBRIDGE HDB CREATI
   HOGAN R, 1994, AM PSYCHOL, V49, P493, DOI 10.1037/0003-066X.49.6.493
   Hung H, 2010, IEEE T MULTIMEDIA, V12, P563, DOI 10.1109/TMM.2010.2055233
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   KIPP M, 2002, ANVIL 3 5 USER MANUA
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Komarraju M, 2009, LEARN INDIVID DIFFER, V19, P47, DOI 10.1016/j.lindif.2008.07.001
   Kosinski M, 2013, P NATL ACAD SCI USA, V110, P5802, DOI 10.1073/pnas.1218772110
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Lepri B, 2012, IEEE T AFFECT COMPUT, V3, P443, DOI 10.1109/T-AFFC.2012.17
   Mairesse F, 2007, J ARTIF INTELL RES, V30, P457, DOI 10.1613/jair.2349
   Mehl MR, 2001, BEHAV RES METH INS C, V33, P517, DOI 10.3758/BF03195410
   Mergl R, 2006, NEUROPSYCHOBIOLOGY, V54, P114, DOI 10.1159/000098261
   Mohammadi G., 2010, Proceedings of the 2nd international workshop on Social signal processing, SSPW'10, P17, DOI [10.1145/1878116.1878123, DOI 10.1145/1878116.1878123]
   Mohammadi G, 2012, IEEE T AFFECT COMPUT, V3, P273, DOI 10.1109/T-AFFC.2012.5
   Nass C. I., 2005, Wired for speech: how voice activates and advances the human-computer relationship
   Olguin D., 2009, PERV COMP TECHN HEAL
   Perugini M., 2002, BIG 5 ASSESSMENT
   Quercia D., 2012, P ACM 2012 C COMPUTE, P955, DOI DOI 10.1145/2145204.2145346
   Reeves B., 1996, MEDIA EQUATION PEOPL
   RIGGIO RE, 1986, J PERS SOC PSYCHOL, V50, P421, DOI 10.1037/0022-3514.50.2.421
   Roth PL, 2005, INT J SELECT ASSESS, V13, P261, DOI 10.1111/j.1468-2389.2005.00323.x
   Rothbaum BO, 2006, BEHAV THER, V37, P80, DOI 10.1016/j.beth.2005.04.004
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Scherer K. R., 1979, Personality markers in speech
   SIGURDSSON JF, 1991, PERS INDIV DIFFER, V12, P617, DOI 10.1016/0191-8869(91)90259-E
   Staiano J, 2012, UBICOMP'12: PROCEEDINGS OF THE 2012 ACM INTERNATIONAL CONFERENCE ON UBIQUITOUS COMPUTING, P321
   Sternberg R.J., 1995, Defying the crowd: Cultivating creativity in a culture of conformity
   Tapus A, 2008, INTEL SERV ROBOT, V1, P169, DOI 10.1007/s11370-008-0017-4
   Valente F., 2012, INTERSPEECH
   Vinciarelli A, 2014, IEEE T AFFECT COMPUT, V5, P273, DOI 10.1109/TAFFC.2014.2330816
   Wijitha Senadeera Wijitha Senadeera, 2006, International Journal of Food Engineering, V2, P7
   YEO C, 2008, UCBEECS200879
NR 65
TC 22
Z9 24
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2016
VL 18
IS 4
BP 659
EP 673
DI 10.1109/TMM.2016.2522763
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA DH4XZ
UT WOS:000372790300009
DA 2024-07-18
ER

PT J
AU Zhang, B
   Liu, Z
   Chan, SHG
   Cheung, G
AF Zhang, Bo
   Liu, Zhi
   Chan, S. -H. Gary
   Cheung, Gene
TI Collaborative Wireless Freeview Video Streaming With Network Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed computing; multimedia computing; wireless networks
ID BROADCAST; DELAY
AB Free viewpoint video (FVV) offers compelling interactive experience by allowing users to switch to any viewing angle at any time. An FVV is composed of a large number of camera-captured anchor views, with virtual views (not captured by any camera) rendered from their nearby anchors using techniques such as depth-image-based rendering (DIBR). We consider a group of wireless users who may interact with an FVV by independently switching views. We study a novel live FVV streaming network where each user pulls a subset of anchors from the server via a primary channel. To enhance anchor availability at each user, a user generates network-coded (NC) packets using some of its anchors and broadcasts them to its direct neighbors via a secondary channel. Given limited primary and secondary channel bandwidths at the devices, we seek to maximize the received video quality (i.e., minimize distortion) by jointly optimizing the set of anchors each device pulls and the anchor combination to generate NC packets. To our best knowledge, this is among the first body of work addressing such joint optimization problem for wireless live FVV streaming with NC-based collaboration. We first formulate the problem and show that it is NP-hard. We then propose a scalable and effective algorithm called PAFV (Peer-Assisted Freeview Video). In PAFV, each node collaboratively and distributedly decides on the anchors to pull and NC packets to share so as to minimize video distortion in its neighborhood. Extensive simulation studies show that PAFV outperforms other algorithms, achieving substantially lower video distortion (often by more than 20-50%) with significantly less redundancy (by as much as 70%). Our Android-based video experiment further confirms the effectiveness of PAFV over comparison schemes.
C1 [Zhang, Bo; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Zhi] Waseda Univ, Global Informat & Telecommun Inst, Tokyo 1698555, Japan.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
C3 Hong Kong University of Science & Technology; Waseda University;
   Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan
RP Zhang, B; Chan, SHG (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.; Liu, Z (corresponding author), Waseda Univ, Global Informat & Telecommun Inst, Tokyo 1698555, Japan.; Cheung, G (corresponding author), Natl Inst Informat, Tokyo 1018430, Japan.
EM zhangbo@cs.ust.hk; liuzhi@aoni.waseda.jp; gchan@cs.ust.hk;
   cheung@nii.ac.jp
RI Zhang, Bo/GQQ-1585-2022; Cheung, Gene/AAB-9284-2020; Liu,
   Zhi/AAE-5698-2020
OI Zhang, Bo/0000-0002-2380-8681; Liu, Zhi/0000-0003-0537-4522; Cheung,
   Gene/0000-0002-5571-4137
FU Hong Kong Research Grant Council (RGC) General Research Fund [610713];
   Hong Kong Innovation and Technology Fund [UIM/246]; JSPS KAKENHI
   [15K21599]; Grants-in-Aid for Scientific Research [15K21599] Funding
   Source: KAKEN
FX This work was supported in part by the Hong Kong Research Grant Council
   (RGC) General Research Fund under Grant 610713, in part by the Hong Kong
   Innovation and Technology Fund under Grant UIM/246, and in part by the
   JSPS KAKENHI under Grant 15K21599. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shiwen Mao.
CR Aboutorab N, 2014, IEEE T COMMUN, V62, P1296, DOI 10.1109/TCOMM.2014.021614.130172
   [Anonymous], P 11 ACM INT S MOB A
   [Anonymous], TS26246 3GPP
   [Anonymous], P 21 ACM INT C MULT
   [Anonymous], 2005, JTC1SC29WG11 ISOIEC
   Caragiannis I, 2013, IEEE ACM T NETWORK, V21, P1322, DOI 10.1109/TNET.2012.2223483
   Chakareski J, 2014, IEEE T IMAGE PROCESS, V23, P931, DOI 10.1109/TIP.2013.2293419
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Cheung G, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P454
   De Raffaele Clifford, 2010, Proceedings of the Second International Conference on Advances in Multimedia (MMEDIA 2010), P55, DOI 10.1109/MMEDIA.2010.14
   Deng ZP, 2012, J VIS COMMUN IMAGE R, V23, P522, DOI 10.1016/j.jvcir.2012.01.016
   El Rouayheb SY, 2007, 2007 IEEE INFORMATION THEORY WORKSHOP, VOLS 1 AND 2, P120, DOI 10.1109/ITW.2007.4313060
   Feige U, 2004, SIAM J DISCRETE MATH, V18, P219, DOI 10.1137/S089548010240415X
   Jiao XL, 2012, IEEE T PARALL DISTR, V23, P110, DOI [10.1109/MSN.2014.9, 10.1109/TPDS.2011.106]
   Johnson D.B., 1996, Mobile Computing, DOI DOI 10.1007/978-0-585-29603-65
   Karp R, 1972, COMPLEXITY COMPUTER, V40, P85, DOI 10.1007/978-3-540-68279-08
   Khalek AA, 2012, IEEE T MOBILE COMPUT, V11, P1223, DOI 10.1109/TMC.2011.127
   Leung MF, 2007, IEEE T BROADCAST, V53, P350, DOI 10.1109/TBC.2006.889093
   Liu Z, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2330332
   Liu Z, 2013, IEEE T CIRC SYST VID, V23, P1781, DOI 10.1109/TCSVT.2013.2269019
   Macchiavello B, 2014, IEEE T MULTIMEDIA, V16, P711, DOI 10.1109/TMM.2014.2299768
   Miao Dan., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1237
   Ren DN, 2015, IEEE T MULTIMEDIA, V17, P307, DOI 10.1109/TMM.2015.2389714
   Ren DN, 2014, IEEE T MULTIMEDIA, V16, P1874, DOI 10.1109/TMM.2014.2332139
   Ren W, 2013, WIREL NETW, V19, P1121, DOI 10.1007/s11276-012-0522-4
   Solh M., 2011, Multimedia and Expo (ICME), 2011 IEEE International Conference on, P1
   Sorour S., 2012, CORR
   [Tanimoto Lab. Dept. of Inform. Eng. Nagoya Univ. Nagoya Japan], 2008, MPEG FTV TEST SEQ DO
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tian D, 2009, PROC SPIE, V7443, DOI 10.1117/12.829372
   Toni L, 2013, IEEE INT WORKSH MULT, P446, DOI 10.1109/MMSP.2013.6659330
   Wen YF, 2011, WIREL NETW, V17, P1401, DOI 10.1007/s11276-011-0344-9
   Zhang B., 2011, IEEE INT C COMMUNICA, P1
   Zhang XY, 2013, IEEE T MOBILE COMPUT, V12, P7, DOI 10.1109/TMC.2011.233
NR 35
TC 21
Z9 21
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2016
VL 18
IS 3
BP 521
EP 536
DI 10.1109/TMM.2016.2518485
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DG2WP
UT WOS:000371931600017
DA 2024-07-18
ER

PT J
AU Sánchez-Hernández, JJ
   García-Ortiz, JP
   González-Ruiz, V
   Müller, D
AF Sanchez-Hernandez, J. J.
   Garcia-Ortiz, J. P.
   Gonzalez-Ruiz, Vicente
   Mueller, Daniel
TI Interactive Streaming of Sequences of High Resolution JPEG2000 Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Channel estimation; high-resolution imaging; scalabilty; streaming media
ID FAST RATE ALLOCATION
AB The JPEG2000 image coding system was created with the intention of superseding the original JPEG standard, using a novel wavelet-based method. The main advantage of JPEG2000 is the flexibility of its code-stream, which provides new functionality related to the interactive transmission of images. For this task, JPEG2000 uses the JPIP protocol, which enables real-time spatial random access while the retrieved image is progressively displayed (streaming). The standard also foresees the compression and transmission of sequences of images by repeating this approach for each image. In this framework, this paper presents the Continue data-flow control strategy, a JPIP-compliant solution for the interactive streaming of sequences of images that are transmitted over time-varying communication channels. In this context, the random fluctuation of the capacity of the transmission channel over the time forces the clients to prefetch a minimal amount of the code-stream of each image of the beginning of the transmitted sequence before the playback starts, and the server to decide, in real-time, which amount of the code-stream of each compressed image is going to be transmitted. The estimated channel capacity is performed by clients and the rate-control at the server is straightforward, resulting in a highly scalable image retrieval system. The experiments conducted in this study demonstrate that the proposed method keeps a constant playback frame-rate under severe variations of the channel capacity, even when short prefetch times are used.
C1 [Sanchez-Hernandez, J. J.; Garcia-Ortiz, J. P.; Gonzalez-Ruiz, Vicente] Univ Almeria, Dept Informat, Almeria 04120, Spain.
   [Mueller, Daniel] European Space Agcy, Estec, NL-2200 Noordwijk, Netherlands.
C3 Universidad de Almeria; European Space Agency
RP Sánchez-Hernández, JJ (corresponding author), Univ Almeria, Dept Informat, Almeria 04120, Spain.
EM josejuan.sanchez@gmail.com; jp.garcia.ortiz@gmail.com; vruiz@ual.es;
   dmueller@rssd.esa.int
RI Ortiz, Juan Pablo Garcia/AAB-1140-2020; Ruiz, Vicente
   Gonzalez/G-9269-2015
OI Ruiz, Vicente Gonzalez/0000-0001-6495-4856; Muller,
   Daniel/0000-0001-9027-9954
FU Spanish Ministry of Economy and Competitiveness [TIN2012-37483-C03-03];
   Junta de Andalucia [P10-TIC-6548]; European Regional Development Fund
   (ERDF); Campus de Excelencia Internacional Agroalimentario (ceiA3); ESA
   Summer of Code in Space (SOCIS) initiative
FX This work was supported by the Spanish Ministry of Economy and
   Competitiveness under Grant TIN2012-37483-C03-03, and by the Junta de
   Andalucia under Grant P10-TIC-6548. This work was supported in part by
   the European Regional Development Fund (ERDF), in part by the Campus de
   Excelencia Internacional Agroalimentario (ceiA3), and in part by the ESA
   Summer of Code in Space (SOCIS) 2011 initiative. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Yap-Peng Tan.
CR [Anonymous], 2004, 1544412000 ISOIEC
   [Anonymous], 2004, 1544422004 ISOIEC
   Aulí-Llinàs F, 2008, IEEE T CIRC SYST VID, V18, P923, DOI 10.1109/TCSVT.2008.920748
   Aulí-Llinàs F, 2011, IEEE T IMAGE PROCESS, V20, P1166, DOI 10.1109/TIP.2010.2077304
   Daami M, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P387, DOI 10.1109/MMCS.1997.609643
   Dagher JC, 2003, IEEE T IMAGE PROCESS, V12, P1522, DOI 10.1109/TIP.2003.819228
   Garcia-Ortiz J., 2011, P IEEE INT C CONS EL, P380
   Garcia-Ortiz J. P., 2013, LECT NOTES ELECT ENG, P239
   Hui JY, 1996, IEEE J SEL AREA COMM, V14, P226, DOI 10.1109/49.481707
   ISO/IEC, 2005, 1544492005 ISOIEC
   Jiménez-Rodríguez L, 2013, IEEE T MULTIMEDIA, V15, P15, DOI 10.1109/TMM.2012.2199973
   Mielke M, 1998, IEEE IC COMP COM NET, P219, DOI 10.1109/ICCCN.1998.998780
   Müller D, 2009, COMPUT SCI ENG, V11, P38, DOI 10.1109/MCSE.2009.142
   Pesnell WD, 2012, SOL PHYS, V275, P3, DOI 10.1007/s11207-011-9841-3
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Taubman D, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P765
NR 16
TC 9
Z9 9
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1829
EP 1838
DI 10.1109/TMM.2015.2470595
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400012
DA 2024-07-18
ER

PT J
AU Anegekuh, L
   Sun, LF
   Jammeh, E
   Mkwawa, IH
   Ifeachor, E
AF Anegekuh, Louis
   Sun, Lingfen
   Jammeh, Emmanuel
   Mkwawa, Is-Haka
   Ifeachor, Emmanuel
TI Content-Based Video Quality Prediction for HEVC Encoded Videos Streamed
   Over Packet Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content type; crowdsourcing; high efficiency video coding (HEVC); mean
   opinion score (MOS); motion activity; motion estimation; motion vector
   (MV); picture complexity; quality of experience (QoE)
ID H.264/AVC; MODEL
AB A new reference-free, objective, video quality prediction model that takes into account video content type to predict the quality of streamed high efficiency video coding (HEVC) encoded video sequences is proposed. Research has shown that for the same encoder settings and network quality of service (NQoS), the video quality differs for different types of video content. This indicates that, in addition to encoder settings and NQoS, there may be other key parameters that impact video quality. In this work, we hypothesized that video content type is one of the key parameters that may impact the quality of streamed videos. Based on this assertion, temporal information is extracted from the motion vector (MV) information inherent in the encoded video bitstreams and spatial information is extracted from the quantisation parameter (QP) and the number of bits (Bits) of coded intra (I) and predictive (P) frames to develop a metric that quantifies the content type of different video sequences. The content type metric is subsequently used together with encoding QP setting and network packet loss rate (PLR) to develop a reference-free objective video quality prediction model for streamed HEVC encoded video sequences. This model has an accuracy of 92% when the model predicted values of sequences not used in model derivation are compared with mean opinion score (MOS) obtained through subjective method.
C1 [Anegekuh, Louis; Sun, Lingfen; Jammeh, Emmanuel; Mkwawa, Is-Haka; Ifeachor, Emmanuel] Univ Plymouth, Sch Comp Commun & Elect, Plymouth PL4 8AA, Devon, England.
C3 University of Plymouth
RP Anegekuh, L (corresponding author), Univ Plymouth, Sch Comp Commun & Elect, Plymouth PL4 8AA, Devon, England.
EM louis.anegekuh@plymouth.ac.uk
OI Jammeh, Emmanuel A./0000-0003-3826-3212; Sun,
   Lingfen/0000-0002-9921-2817; Mkwawa, Is-Haka/0000-0002-8399-0737
CR Ammar D., 2011, Proceedings of the 4th International ICST Conference on Simulation Tools and Techniques, P81
   Anegekuh L, 2014, IEEE GLOB COMM CONF, P1152, DOI 10.1109/GLOCOM.2014.7036964
   Anegekuh L, 2014, IEEE ICC, P1296, DOI 10.1109/ICC.2014.6883500
   [Anonymous], P SPIE
   [Anonymous], METH SUBJ ASS QUAL T
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], P 4 INT C CYB INF TE
   [Anonymous], SUPPORT VECTOR REGRE
   [Anonymous], 2005, SUGI 30 P
   [Anonymous], TIA921 ITUT
   [Anonymous], P 2009 IEEE INT S BR
   [Anonymous], 2013, CISC VIS NETW IND GL
   [Anonymous], MULTIMEDIA TOOLS DEC
   [Anonymous], 2009, PAK DEV REV, DOI DOI 10.1109/ICC.2009.5198850
   [Anonymous], JCTVCG1200
   [Anonymous], G1050 ITUT
   [Anonymous], P 2 INT S WIR PERV C
   [Anonymous], 2005, JTC1SC29WG11 ISOIEC
   [Anonymous], JCTVCH0072
   Argyropoulos S, 2011, INT WORK QUAL MULTIM, P31, DOI 10.1109/QoMEX.2011.6065708
   Feghali R, 2007, IEEE T BROADCAST, V53, P441, DOI 10.1109/TBC.2007.891700
   Hiramatsu K., 2010, 2010 12th IEEE International Conference on Communication Systems (ICCS 2010), P161, DOI 10.1109/ICCS.2010.5686376
   Joskowicz J, 2013, IEEE T BROADCAST, V59, P569, DOI 10.1109/TBC.2013.2277951
   Khan A, 2010, IEEE ICC
   Khan A, 2012, IEEE T MULTIMEDIA, V14, P431, DOI 10.1109/TMM.2011.2176324
   Krzanowski W., 2000, PRINCIPLES MULTIVARI, V23
   Li F, 2009, IEEE T CIRC SYST VID, V19, P1908, DOI 10.1109/TCSVT.2009.2031457
   Lin TL, 2010, IEEE T IMAGE PROCESS, V19, P722, DOI 10.1109/TIP.2009.2038834
   Nightingale J, 2012, IEEE T CONSUM ELECTR, V58, P404, DOI 10.1109/TCE.2012.6227440
   Ries Michal, 2008, Journal of Communications, V3, P41, DOI 10.4304/jcm.3.1.41-50
   Ries M, 2007, IEEE WCNC, P2670
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Van Wallendael G, 2012, INT WORK QUAL MULTIM, P7, DOI 10.1109/QoMEX.2012.6263845
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Wischik D, 2005, ACM SIGCOMM COMP COM, V35, P75, DOI 10.1145/1070873.1070884
NR 36
TC 48
Z9 50
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1323
EP 1334
DI 10.1109/TMM.2015.2444098
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000017
DA 2024-07-18
ER

PT J
AU Zhang, W
   Ngo, CW
AF Zhang, Wei
   Ngo, Chong-Wah
TI Topological Spatial Verification for Instance Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Instance search; non-planar and non-rigid objects; spatial verification;
   triangulated graph
ID IMAGE; FEATURES
AB This paper proposes an elastic spatial verification method for Instance Search, particularly for dealing with non-planar and non-rigid queries exhibiting complex spatial transformations. Different from existing models that map keypoints between images based on a linear transformation (e.g., affine, homography), our model exploits the topological arrangement of keypoints to address the non-linear spatial transformations that are extremely common in real life situations. In particular, we propose a novel technique to elastically verify the topological spatial consistency with the triangulated graph through a "sketch-and-match" scheme. The spatial topology configuration, emphasizing relative positioning rather than absolute coordinates, is first sketched by a triangulated graph, whose edges essentially capture the topological layout of the corresponding keypoints. Next, the spatial consistency is efficiently estimated as the number of common edges between the triangulated graphs. Compared to the existing methods, our technique is much more effective in modeling the complex spatial transformations of non-planar and non-rigid instances, while being compatible to instances with simple linear transformations. Moreover, our method is by nature more robust in spatial verification by considering the locations, rather than the local geometry of keypoints, which are sensitive to motions and viewpoint changes. We evaluate our method extensively on three years of TRECVID datasets, as well as our own dataset MQA, showing large improvement over other methods for the task of Instance Search.
C1 [Zhang, Wei; Ngo, Chong-Wah] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Zhang, W (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM wzhang35@cityu.edu.hk; cscwngo@cityu.edu.hk
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 118812]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China (CityU 118812). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Vasileios Mezaris.
CR [Anonymous], P TRECVID
   [Anonymous], P TRECVID WORKSH
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 1992, P 4 CAN C COMP GEOM
   [Anonymous], P TRECVID
   [Anonymous], P TRECVID
   [Anonymous], P ACM INT C MULT RET
   [Anonymous], P TRECVID
   [Anonymous], INT J COMPUT VIS
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   [Anonymous], P ACM C INT C MULT R
   [Anonymous], P TRECVID
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], P TRECVID
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Avrithis Y, 2014, INT J COMPUT VISION, V107, P1, DOI 10.1007/s11263-013-0659-3
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, IEEE T PATTERN ANAL, V32, P371, DOI 10.1109/TPAMI.2009.166
   Chum Ondrej., 2007, CIVR 07, P549, DOI DOI 10.1145/1282280.1282359
   Delaunay B., 1934, IZV AKAD NAUK SSSR O, V7, P1
   Fernando Basura, 2014, International Journal of Computer Vision, V108, P186, DOI 10.1007/s11263-014-0700-1
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2009, PROC CVPR IEEE, P1169, DOI 10.1109/CVPRW.2009.5206609
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Li ZY, 2013, IEEE I CONF COMP VIS, P2136, DOI 10.1109/ICCV.2013.454
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Nister David, 2006, CVPR
   Philbin J., 2008, PROC IEEE C COMPUT V, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tao R, 2014, PROC CVPR IEEE, P2099, DOI 10.1109/CVPR.2014.269
   Wu A. G., 2007, P ACM MM, P218
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
   Zhao WL, 2010, IEEE T MULTIMEDIA, V12, P448, DOI 10.1109/TMM.2010.2050651
   Zhu CZ, 2013, IEEE I CONF COMP VIS, P1705, DOI 10.1109/ICCV.2013.214
NR 39
TC 12
Z9 14
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1236
EP 1247
DI 10.1109/TMM.2015.2440997
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000010
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Tabatabaei, SAH
   Ur-Rehman, O
   Zivic, N
   Ruland, C
AF Tabatabaei, Seyed Amir Hossein
   Ur-Rehman, Obaid
   Zivic, Natasa
   Ruland, Christoph
TI Secure and Robust Two-Phase Image Authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Approximate message authentication code (AMAC); discrete cosine
   transform (DCT); discrete wavelet transform (DWT); error-correcting
   code; image authentication; unconditional security
ID CODES
AB A novel two-phase robust content-based image authentication scheme is introduced. The proposed scheme is constructed based on a combination of hard and soft authentication using two existing generic approximate message authentication codes (AMACs). The AMACs combine error-correcting codes with cryptographic primitives such as message authentication codes and symmetric encryption algorithms. The message authentication codes are used for hard authentication, whereas the error-correcting codes introduce a certain degree of robustness in authentication. This is achieved by correcting minor unintentional modifications as a result of common image processing operations such as quantization, compression, and noise addition. The two-phase image authentication scheme verifies the authenticity of an image in two phases. The low frequency elements of the image in a transform domain are subjected to the first phase while some higher frequency elements are left to the second phase if the first phase succeeds. The proposed scheme tolerates common content-preserving modifications in an image but can discriminate intentional modifications affecting the image content. Mathematical bounds for the accuracy and the security level of the proposed approach are estimated and the performance is compared with some other well-known schemes in the literature. The results demonstrate that the proposed scheme shows high discriminating capability and can detect different types of meaningful forgery attacks on images while preserving the robustness. It also outperforms the benchmark image authentication schemes in terms of tradeoff between robustness and fragility.
C1 [Tabatabaei, Seyed Amir Hossein; Ur-Rehman, Obaid; Zivic, Natasa; Ruland, Christoph] Univ Siegen, Chair Data Commun Syst, D-57076 Siegen, Germany.
C3 Universitat Siegen
RP Tabatabaei, SAH (corresponding author), Univ Siegen, Chair Data Commun Syst, D-57076 Siegen, Germany.
EM amir.tabatabaei@uni-siegen.de; obaid.ur-rehman@uni-siegen.de;
   natasa.zivic@uni-siegen.de; christoph.ruland@uni-siegen.de
CR [Anonymous], 2002 11 EUR SIGN PRO
   [Anonymous], P INT C IM PROC
   [Anonymous], 1999, P S CONT SEC DAT HID
   Byun SC, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P593, DOI 10.1109/ICME.2002.1035851
   Chang HT, 2009, OPT ENG, V48, DOI 10.1117/1.3127192
   Di Crescenzo G, 2005, LECT NOTES COMPUT SC, V3570, P240
   Feller N., 1968, INTRO PROBABILITY TH
   Fridrich J., 1999, MULTIMEDIA SECURITY, P29
   Ge RW, 2006, IEEE T INF FOREN SEC, V1, P56, DOI 10.1109/TIFS.2005.863504
   Graveman R. F., 1999, P ANN S ADV TEL INF
   Haouzia A, 2008, MULTIMED TOOLS APPL, V39, P1, DOI 10.1007/s11042-007-0154-3
   Hsu ICCBW, 2005, INT WORK SYS APPR D, P223
   Jing F, 2004, IEEE T IMAGE PROCESS, V13, P699, DOI 10.1109/TIP.2004.826125
   KAILASANATHAN C, 2001, P IEEE EURASIP WORKS
   Koval O, 2008, PROC SPIE, V6819, DOI 10.1117/12.764846
   Kozat SS, 2004, IEEE IMAGE PROC, P3443, DOI 10.1109/ICIP.2004.1421855
   Lu ZM, 2005, IEEE T IMAGE PROCESS, V14, P822, DOI 10.1109/TIP.2005.847324
   Menezes A. J., 1996, HDB APPL CRYPTOGRAPH, V1st
   Mihcak K, 2001, P ACM WORKSH SEC PRI, P13
   Mitzenmacher Michael, 2017, PROBABILITY COMPUTIN
   Monga V, 2006, IEEE T IMAGE PROCESS, V15, P3452, DOI 10.1109/TIP.2006.881948
   Naor M., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P33, DOI 10.1145/73007.73011
   Queluz MP, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P297, DOI 10.1109/MMSP.1998.738950
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   SAFARIC R, 2005, CONTROL ROBOTICS REM, P1
   Safavi-Naini R, 2011, DISCRET MATH ALGORIT, V3, P587, DOI 10.1142/S1793830911001425
   Shin J, 2013, IEEE CONF WIREL MOB, P253, DOI 10.1109/WiMOB.2013.6673369
   Swaminathan A, 2006, IEEE T INF FOREN SEC, V1, P215, DOI 10.1109/TIFS.2006.873601
   Tabatabaei S. A., 2013, INT C COMM IN PRESS, P727
   Tabatabaei S. Amir Hossein A. E., 2013, Proceedings of the 2013 55th International Symposium. ELMAR-2013, P79
   Tang ZJ, 2014, IEEE T KNOWL DATA EN, V26, P711, DOI 10.1109/TKDE.2013.45
   Tang Zhenjun., 2008, Journal of ubiquitous convergence technology, V2, P18, DOI DOI 10.1109/INF0P.2015.7489395
   Tonien D, 2011, DISCRET MATH ALGORIT, V3, P393, DOI 10.1142/S1793830911001292
   Tonien D, 2009, LECT NOTES COMPUT SC, V5557, P233, DOI 10.1007/978-3-642-01877-0_19
   Venkatesan R, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P664, DOI 10.1109/ICIP.2000.899541
   Xie L., 2000, P ANN FEDL S ATIRP
   Xie LH, 2001, IEEE T MULTIMEDIA, V3, P242, DOI 10.1109/6046.923823
   Zhao Y, 2013, IEEE T INF FOREN SEC, V8, P55, DOI 10.1109/TIFS.2012.2223680
   Zhu BB, 2004, IEEE SIGNAL PROC MAG, V21, P40, DOI 10.1109/MSP.2004.1276112
   Zhu GP, 2009, IEEE T INF FOREN SEC, V4, P928, DOI 10.1109/TIFS.2009.2033737
NR 40
TC 22
Z9 22
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 945
EP 956
DI 10.1109/TMM.2015.2432672
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300003
DA 2024-07-18
ER

PT J
AU Yang, C
   Liu, Y
AF Yang, Can
   Liu, Yong
TI On Achieving Short Channel Switching Delay and Playback Lag in IP-Based
   TV Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Channel switching; IPTV; over-the-top (OTT); playback lag; popularity;
   quality-of-experience (QoE); response time
ID VIDEO SYSTEMS; TIME
AB IP-based TV systems are widely used to stream video content on the Internet. Compared with the traditional broadcast TV systems, IP-based TV systems suffer from much longer channel switching delays. In this paper, we propose a new IP-based streaming framework, called fast IP-based TV (FIPTV), to achieve close-to-zero channel switching delay at the price of extra download bandwidth and increased playback lags. In FIPTV, other than the channel being watched, a client also downloads an extra combination virtual channel, called backing united stream (BUS), which consists of video segments sequentially sampled from a set of target channels that a client might switch to in the near future. Video segments downloaded from the combination channel will be cached in a local buffer. When the client issues a channel switch request to a target channel, the client will immediately playback the most recently downloaded video of the target channel, leading to close-to-zero channel switching time, but a positive playback lag. Through analysis and simulations, we show that short average playback lag can be achieved across all channels through carefully designed channel scheduling algorithms on the BUS channel by considering channel popularity. We implement the proposed streaming framework in real systems. Through experiments on the Internet, we show that the actual channel switching delay can be reduced to less than 0.25 seconds, which is much shorter than that of the popular Internet video streaming services.
C1 [Yang, Can] S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Liu, Yong] NYU, Polytech Sch Engn, Elect & Comp Engn Dept, Brooklyn, NY 11201 USA.
C3 South China University of Technology; New York University; New York
   University Tandon School of Engineering
RP Yang, C (corresponding author), S China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM cscyang@scut.edu.cn; yongliu@nyu.edu
RI yang, can/GXV-5624-2022; Yang, Can/AAF-6597-2019
OI Yang, Can/0000-0002-7235-9093
FU Science & Technology Plan Project of Guangdong, China [2012B090400016,
   2012A011100005]; State Scholarship Fund of China Scholarship Council
   [201206155004]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [1018032, 0953682] Funding Source:
   National Science Foundation
FX This work was supported by the Science & Technology Plan Project of
   Guangdong, China under Grant 2012B090400016 and Grant 2012A011100005,
   and by the State Scholarship Fund of China Scholarship Council under
   Grant 201206155004. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Yiannis
   Andreopoulos.
CR Abrahamsson H., 2012, P 2012 INT MEAS C IM, P199
   Azgin A, 2013, IEEE T BROADCAST, V59, P471, DOI 10.1109/TBC.2013.2265773
   Bejerano Y, 2009, IEEE INFOCOM SER, P1971, DOI 10.1109/INFCOM.2009.5062119
   Caenegem T. V., 2010, DVB SCENE        JUN, P7
   Cha M, 2008, IMC'08: PROCEEDINGS OF THE 2008 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P71
   Chen X., 2009, P INT C INF SCI ENG, P149
   HARDY G H, 1952, INEQUALITIES CAMBRID, P261
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Kooij R, 2006, PROCEEDINGS OF THE FIFTH IASTED INTERNATIONAL CONFERENCE ON COMMUNICATION SYSTEMS AND NETWORKS, P155
   Lee CY, 2010, IEEE T BROADCAST, V56, P321, DOI 10.1109/TBC.2010.2051494
   Li Y., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P209, DOI [10.1145/2068816, DOI 10.1145/2068816]
   Liu Y, 2010, IEEE ACM T NETWORK, V18, P1195, DOI 10.1109/TNET.2009.2038155
   Manzato DAG, 2013, IEEE J SEL AREA COMM, V31, P326, DOI 10.1109/JSAC.2013.SUP.0513029
   Narang N., M E IND TRENDS TECHN
   Nicholson C., 2014, AKAMAI RELEASES 2 Q
   Oh U, 2010, IEEE T CONSUM ELECTR, V56, P483, DOI 10.1109/TCE.2010.5505959
   Ramos FMV, 2013, IEEE COMMUN MAG, V51, P128, DOI 10.1109/MCOM.2013.6576350
   Ramos FMV, 2011, SIGNAL PROCESS-IMAGE, V26, P400, DOI 10.1016/j.image.2011.03.005
   Ramos FMV, 2010, IEEE INT CON MULTI, P1327, DOI 10.1109/ICME.2010.5583279
   Tian XH, 2013, IEEE T PARALL DISTR, V24, P327, DOI 10.1109/TPDS.2012.94
   Tian XH, 2010, IEEE NETWORK, V24, P45, DOI 10.1109/MNET.2010.5510918
   Wu D, 2010, IEEE ACM T NETWORK, V18, P1248, DOI 10.1109/TNET.2009.2038910
   Wu D, 2009, IEEE INFOCOM SER, P2726, DOI 10.1109/INFCOM.2009.5062220
   Yang C., 2011, P INT C WIR COMM NET, P1
NR 24
TC 16
Z9 18
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 1096
EP 1106
DI 10.1109/TMM.2015.2429552
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300015
DA 2024-07-18
ER

PT J
AU Chen, BC
   Chen, CS
   Hsu, WH
AF Chen, Bor-Chun
   Chen, Chu-Song
   Hsu, Winston H.
TI Face Recognition and Retrieval Using Cross-Age Reference Coding With
   Cross-Age Celebrity Dataset
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-age face recognition; face image retrieval; face recognition
ID PATTERNS
AB This paper introduces a method for face recognition across age and also a dataset containing variations of age in the wild. We use a data-driven method to address the cross-age face recognition problem, called cross-age reference coding (CARC). By leveraging a large-scale image dataset freely available on the Internet as a reference set, CARC can encode the low-level feature of a face image with an age-invariant reference space. In the retrieval phase, our method only requires a linear projection to encode the feature and thus it is highly scalable. To evaluate our method, we introduce a large-scale dataset called cross-age celebrity dataset (CACD). The dataset contains more than 160 000 images of 2,000 celebrities with age ranging from 16 to 62. Experimental results show that our method can achieve state-of-the-art performance on both CACD and the other widely used dataset for face recognition across age. To understand the difficulties of face recognition across age, we further construct a verification subset from the CACD called CACD-VS and conduct human evaluation using Amazon Mechanical Turk. CACD-VS contains 2,000 positive pairs and 2,000 negative pairs and is carefully annotated by checking both the associated image and web contents. Our experiments show that although state-of-the-art methods can achieve competitive performance compared to average human performance, majority votes of several humans can achieve much higher performance on this task. The gap between machine and human would imply possible directions for further improvement of cross-age face recognition in the future.
C1 [Chen, Bor-Chun; Chen, Chu-Song] Acad Sinica, Inst Informat Sci, Taipei 11529, Taiwan.
   [Chen, Chu-Song] Acad Sinica, Res Ctr Informat Technol Innovat CITI, Taipei 115, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
C3 Academia Sinica - Taiwan; Academia Sinica - Taiwan; National Taiwan
   University; National Taiwan University
RP Chen, BC (corresponding author), Acad Sinica, Inst Informat Sci, Taipei 11529, Taiwan.
EM sirius@umd.edu; song@iis.sinica.edu.tw; winston@csie.ntu.edu.tw
RI 俞, 小野/F-9976-2017
FU Ministry of Science and Technology of Taiwan [MOST
   103-2221-E-002-105-MY3, MOST 103-2221-E-001-010]; Excellent Research
   Projects of National Taiwan University [102R7762]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Contract MOST 103-2221-E-002-105-MY3 and
   Grant MOST 103-2221-E-001-010, and by the Excellent Research Projects of
   National Taiwan University under Grant 102R7762. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 07 UMASS TR
   [Anonymous], 2011, P 19 ACM INT C MULT, DOI [DOI 10.1145/2072298.2072345, 10.1145/2072298.2072345]
   [Anonymous], 2014, TRIIS14003 AC SIN
   Barkan O, 2013, IEEE I CONF COMP VIS, P1960, DOI 10.1109/ICCV.2013.246
   Berg T., P BRIT MACH VIS C, V1
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chen D, 2013, PROC CVPR IEEE, P3025, DOI 10.1109/CVPR.2013.389
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Fan H., 2014, ARXIV14032802V1, P1403
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Gong DH, 2013, IEEE I CONF COMP VIS, P2872, DOI 10.1109/ICCV.2013.357
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Jain A. K., 2011, HDB FACE RECOGNITION
   Jain AK, 2012, IEEE MULTIMEDIA, V19, P20, DOI 10.1109/MMUL.2012.4
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Li ZF, 2011, IEEE T INF FOREN SEC, V6, P1028, DOI 10.1109/TIFS.2011.2156787
   Ling HB, 2010, IEEE T INF FOREN SEC, V5, P82, DOI 10.1109/TIFS.2009.2038751
   Montilla A, 2009, IEEE IMAGE PROC, P2465, DOI 10.1109/ICIP.2009.5414103
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Park U, 2010, IEEE T PATTERN ANAL, V32, P947, DOI 10.1109/TPAMI.2010.14
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Simonyan K., P BRIT MACH VIS C, V1
   Suo JL, 2009, IEEE I CONF COMP VIS, P622, DOI 10.1109/ICCV.2009.5459181
   Suo JL, 2010, IEEE T PATTERN ANAL, V32, P385, DOI 10.1109/TPAMI.2009.39
   Turk M. A., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P586, DOI 10.1109/CVPR.1991.139758
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu T, 2012, LECT NOTES COMPUT SC, V7577, P58, DOI 10.1007/978-3-642-33783-3_5
   Wu Z, 2011, IEEE T PATTERN ANAL, V33, P1991, DOI 10.1109/TPAMI.2011.111
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yin Q, 2011, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2011.5995494
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
NR 38
TC 200
Z9 220
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 804
EP 815
DI 10.1109/TMM.2015.2420374
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500004
DA 2024-07-18
ER

PT J
AU Tan, X
   Wu, F
   Li, X
   Tang, SL
   Lu, WM
   Zhuang, YT
AF Tan, Xu
   Wu, Fei
   Li, Xi
   Tang, Siliang
   Lu, Weiming
   Zhuang, Yueting
TI Structured Visual Feature Learning for Classification via Supervised
   Probabilistic Tensor Factorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Maximum entropy discrimination (MED); multimedia classification;
   structural visual feature learning; supervised probabilistic tensor
   factorization
ID FACE RECOGNITION; FRAMEWORK
AB In this paper, structured visual feature learning aims at exploiting the intrinsic structural properties of mutually correlated multimedia collections (e.g., video frames or facial images) to learn a more effective feature representation for multimedia data classification. We pose structured visual feature learning as a problem of supervised tensor factorization (STF), which is capable of effectively learning multi-view visual features from structural tensorial multimedia data. In mathematics, STF is formulated as a joint optimization framework of probabilistic inference and epsilon-insensitive support vector regression. As a result, the feature representation obtained by STF not only preserves the intrinsic multi-view structural information on tensorial multimedia data, but also includes the discriminative information derived from the max-margin learning process. Using the learned discriminative visual features, we conduct a set of multimedia classification experiments on several challenging datasets, including images and videos, which demonstrate the effectiveness of our method.
C1 [Tan, Xu; Wu, Fei; Li, Xi; Tang, Siliang; Lu, Weiming; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Lu, WM (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM tanxu@zju.edu.cn; wufei@zju.edu.cn; xilizju@zju.edu.cn;
   siliang@zju.edu.cn; luwm@zju.edu.cn; yzhuang@zju.edu.cn
RI Li, Xi/L-1234-2013
OI Li, Xi/0000-0003-3023-1662
FU National Basic Research Program of China [2012CB316400]; NSFC [61472353,
   61402401]; 863 Program [2012AA012505]; China Knowledge Centre for
   Engineering Sciences and Technology; Fundamental Research Funds for the
   Central Universities; Zhejiang Provincial Natural Science Foundation of
   China [LQ14F010004, LY14F020027]
FX Manuscript received July 31, 2014; revised November 16, 2014 and
   February 16, 2015; accepted February 17, 2015. Date of publication March
   04, 2015; date of current version April 15, 2015. This work was
   supported in part by the National Basic Research Program of China under
   Grant 2012CB316400, the NSFC under Grant 61472353 and Grant 61402401,
   the 863 Program under Grant 2012AA012505, the China Knowledge Centre for
   Engineering Sciences and Technology, the Fundamental Research Funds for
   the Central Universities, and the Zhejiang Provincial Natural Science
   Foundation of China under Grant LQ14F010004 and Grant LY14F020027. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Cees Snoek.
CR Cao J., 2009, ICTMCG09001 I COMP T
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Guo ZY, 2013, IEEE T MULTIMEDIA, V15, P621, DOI 10.1109/TMM.2012.2234729
   Hazan T, 2005, IEEE I CONF COMP VIS, P50
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jaakkola T, 2000, ADV NEUR IN, V12, P470
   Joachims T., ADV KERNEL METHODS S
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   Kotsia I, 2011, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2011.5995663
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lathia N, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P796, DOI 10.1145/1571941.1572133
   Lee H, 2007, INT J NEURAL SYST, V17, P305, DOI 10.1142/S0129065707001159
   Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li H, 2009, INT CONF DAT MIN WOR, P164, DOI 10.1109/ICDMW.2009.46
   Li J, 2003, IEEE T PATTERN ANAL, V25, P1075, DOI 10.1109/TPAMI.2003.1227984
   Li Zheng, 2010, Proceedings of the SICE 2010 - 49th Annual Conference of the Society of Instrument and Control Engineers of Japan, P1187, DOI 10.1145/1873951.1874183
   Liu J, 2009, IEEE I CONF COMP VIS, P2114
   Morup M., 2006, IMM200604144 TU DENM
   Oh SM, 2008, INT J COMPUT VISION, V77, P103, DOI 10.1007/s11263-007-0062-z
   Panagakis Y, 2010, INT CONF ACOUST SPEE, P249, DOI 10.1109/ICASSP.2010.5495984
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Rendle S, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Shan H., 2011, TR11026 U MINN
   Signoretto M, 2011, NEURAL NETWORKS, V24, P861, DOI 10.1016/j.neunet.2011.05.011
   Simonyan Karen., 2014, IEEE Transactions on Pattern Analysis and Machine Intelligence, V12, P25
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Tao D., 2006, Computer_Vision_and_Pattern Recognition, P1670
   Tao DC, 2005, FIFTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P450
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang Y, 2005, INT J PATTERN RECOGN, V19, P495, DOI 10.1142/S0218001405004198
   Wu F., 2013, 27th AAAI Conference on Artificial Intelligence (AAAI 2013), P962
   Wu F, 2009, IEEE T MULTIMEDIA, V11, P868, DOI 10.1109/TMM.2009.2021724
   Xiong L., 2010, P 2010 SIAM INT C DA, P211, DOI DOI 10.1137/1.9781611972801.19
   Xu Z., 2012, ICML
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Zhang YMZ, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.83
   Zhao QB, 2014, WIRES DATA MIN KNOWL, V4, P104, DOI 10.1002/widm.1120
   Zhou XM, 2012, IEEE T MULTIMEDIA, V14, P1220, DOI 10.1109/TMM.2012.2194481
   Zhu J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, P1257, DOI DOI 10.1145/1553374.1553535
   Zhu J, 2009, J MACH LEARN RES, V10, P2531
NR 47
TC 6
Z9 7
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 660
EP 673
DI 10.1109/TMM.2015.2410135
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300008
DA 2024-07-18
ER

PT J
AU Pham, HD
   Vafi, S
AF Huu Dung Pham
   Vafi, Sina
TI Optimized Motion Energy Estimation for Group of Pictures in Multi-Level
   Error Protection of H.264/AVC Video Bitstreams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group of Pictures (GOP); Lagrange polynomial; motion energy; unequal
   error protection
ID VECTOR RECOVERY ALGORITHM; INTERPOLATION; NETWORKS; IP
AB A multi-level error protection technique for H.264/AVC video bitstreams structured by Group of Pictures (GOP) is proposed. Protection levels are determined by motion energy, which considers displacements of all macroblocks between consecutive frames. Lagrange polynomial is applied in predicting motion energy of a GOP, which is derived from previous GOPs having a high correlation with each other. The optimum number of previous GOPs is selected in order to obtain a good motion energy estimation of the current one. Conducted analyses and simulation results show that video performance obtained by the proposed technique outperforms other motion activity-based Unequal Error Protection (UEP) techniques, while a similar channel code rate is applied.
C1 [Huu Dung Pham; Vafi, Sina] Charles Darwin Univ, Sch Engn & Informat Technol, Casuarina, NT 0909, Australia.
C3 Charles Darwin University
RP Pham, HD (corresponding author), Charles Darwin Univ, Sch Engn & Informat Technol, Casuarina, NT 0909, Australia.
EM huudung.pham@cdu.edu.au; sina.vafi@cdu.edu.au
CR [Anonymous], 2011, 1449610 ISOIEC
   Berrut JP, 1997, COMPUT MATH APPL, V33, P77, DOI 10.1016/S0898-1221(97)00034-5
   Chang YC, 2008, IEEE T CONSUM ELECTR, V54, P1066, DOI 10.1109/TCE.2008.4637589
   Fang T, 2006, IEEE T IMAGE PROCESS, V15, P1323, DOI 10.1109/TIP.2005.864159
   Ha H, 2010, IEEE T CIRC SYST VID, V20, P1187, DOI 10.1109/TCSVT.2010.2051368
   Pham HD, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P4594
   Li THS, 2011, IEEE T SYST MAN CY B, V41, P736, DOI 10.1109/TSMCB.2010.2089978
   Lin S., 2004, Error Control Coding, Vsecond
   Luo ZY, 2013, IEEE T MULTIMEDIA, V15, P2208, DOI 10.1109/TMM.2013.2280561
   MABOGUNJE AO, 1991, LECT NOTES COMPUT SC, V514, P87
   Peraldo L, 2010, INT CONF ACOUST SPEE, P2330, DOI 10.1109/ICASSP.2010.5496045
   Pham H., 2014, J SIGNAL IMAGE VIDEO, P1
   Pham H., 2012, P IEEE INT C SIGN PR, P634
   Pham H., 2014, P IEEE INT C COMM IC, P1717
   Pham H B., 2013, Proc., P1
   Qu Q, 2006, IEEE T MULTIMEDIA, V8, P1033, DOI 10.1109/TMM.2006.879840
   SCHAFER RW, 1973, P IEEE, V61, P692, DOI 10.1109/PROC.1973.9150
   Seth K, 2010, IET IMAGE PROCESS, V4, P132, DOI 10.1049/iet-ipr.2008.0228
   Sklar B., 1988, Digital Communications Fundamentals and Applications, DOI DOI 10.1121/1.3598464
   Suehring K., 2011, H 264 AVC JM REFEREN
   Wenger S, 2003, IEEE T CIRC SYST VID, V13, P645, DOI 10.1109/TCSVT.2003.814966
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Winkler S, 2008, IEEE T BROADCAST, V54, P660, DOI 10.1109/TBC.2008.2000733
   Zheng JH, 2003, IEEE T BROADCAST, V49, P383, DOI 10.1109/TBC.2003.819050
NR 24
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2118
EP 2129
DI 10.1109/TMM.2014.2348712
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300004
DA 2024-07-18
ER

PT J
AU Kadu, H
   Kuo, CCJ
AF Kadu, Harshad
   Kuo, C. -C. Jay
TI Automatic Human Mocap Data Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Database management; human motion analysis; machine learning; mocap
   data; motion recognition; n-fold cross validation; suffix array; SVM;
   vector quantization
ID CONTENT-BASED RETRIEVAL; HUMAN MOTION; VECTOR QUANTIZATION; RECOGNITION
AB Automatic classification of human motion capture (mocap) data has many commercial, biomechanical, and medical applications and is the principal focus of this paper. First, we propose a multi-resolution string representation scheme based on the tree-structured vector quantization (TSVQ) to transform the time-series of human poses into codeword sequences. Then, we take the temporal variations of human poses into account via codeword sequence matching. Furthermore, we develop a family of pose-histogram-based classifiers to examine the spatial distribution of human poses. We analyze the performance of the temporal and spatial classifiers separately. To achieve a higher classification rate, we merge their decisions and soft scores using novel fusion methods. The proposed fusion solutions are tested on a wide variety of sequences from the CMU mocap database using five-fold cross validation, and a correct classification rate of 99.6% is achieved.
C1 [Kadu, Harshad; Kuo, C. -C. Jay] Univ So Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Kadu, H (corresponding author), Univ So Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
EM hkadu@usc.edu; cckuo@sipi.usc.edu
RI Kadu, Harshad/HMV-5007-2023; Kuo, C.-C. Jay/A-7110-2011
OI Kuo, C.-C. Jay/0000-0001-9474-5035
FU University of Southern California Center for High-Performance Computing
   and Communications
FX This work was supported in part by the University of Southern California
   Center for High-Performance Computing and Communications. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Gokhan Tur.
CR Abouelhoda M. I., 2004, Journal of Discrete Algorithms, V2, P53, DOI 10.1016/S1570-8667(03)00065-0
   Alexiadis D. S., 2013, IEEE T MULTIMEDIA, V16, P1391
   [Anonymous], 2011, HUMAN GESTURE BEHAV
   [Anonymous], P APSIPA ANN SUMM C
   [Anonymous], 2007, CARN MELL MOC DAT
   [Anonymous], 2005, P ACM SIGGRAPH EUR S, DOI [10.1145/1073368.1073377, DOI 10.1145/1073368.1073377]
   Arikan O, 2003, ACM T GRAPHIC, V22, P402, DOI 10.1145/882262.882284
   Arikan O, 2002, ACM T GRAPHIC, V21, P483, DOI 10.1145/566570.566606
   Cardle M., 2003, P SIGGRAPH TECH SKET
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen X, 2013, LECT NOTES COMPUT SC, V7944, P640
   Chew BS, 2011, IEEE T MULTIMEDIA, V13, P40, DOI 10.1109/TMM.2010.2082512
   Chiu CY, 2004, J VIS COMMUN IMAGE R, V15, P446, DOI 10.1016/j.jvcir.2004.04.004
   Effros M, 1998, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.1998.672131
   Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315
   Jing Wang, 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P232
   Kärkkäinen J, 2003, LECT NOTES COMPUT SC, V2719, P943
   Kasai T., 2001, Combinatorial Pattern Matching. 12th Annual Symposium, CPM 2001. Proceedings (Lecture Notes in Computer Science Vol. 2089), P181
   Kim DK, 2005, J DISCRET ALGORITHMS, V3, P126, DOI 10.1016/j.jda.2004.08.019
   Kim TH, 2003, ACM T GRAPHIC, V22, P392, DOI 10.1145/882262.882283
   Kovar L, 2004, ACM T GRAPHIC, V23, P559, DOI 10.1145/1015706.1015760
   Kovar L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P214
   Kuo MC, 2010, LECT NOTES COMPUT SC, V6298, P84, DOI 10.1007/978-3-642-15696-0_9
   Lee JH, 2002, ACM T GRAPHIC, V21, P491
   Li C.J., 2004, Proceedings of the ASME International Mechanical Engineering Congress, P1
   Li CJ, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1236471.1236475
   Liu CK, 2002, ACM T GRAPHIC, V21, P408, DOI 10.1145/566570.566596
   Liu F, 2003, COMPUT VIS IMAGE UND, V92, P265, DOI 10.1016/j.cviu.2003.06.001
   Muller Meinard., 2006, P ACM SIGGRAPHEUROGR, P137
   Müller M, 2005, ACM T GRAPHIC, V24, P677, DOI 10.1145/1073204.1073247
   NASRABADI NM, 1988, IEEE T COMMUN, V36, P957, DOI 10.1109/26.3776
   Ntouskos V., 2013, P INT C PATT REC APP
   Pullen K, 2002, ACM T GRAPHIC, V21, P501
   Reddy KK, 2012, Machine Vision and Applications, DOI DOI 10.1007/S00138-012-0450-4
   Ren L, 2005, ACM T GRAPHIC, V24, P1303, DOI 10.1145/1095878.1095882
   Rosenhahn B, 2008, COMPUT IMAGING VIS, V36, P1, DOI 10.1007/978-1-4020-6693-1
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Safonova A, 2004, ACM T GRAPHIC, V23, P514, DOI 10.1145/1015706.1015754
   Sakamoto Y., 2004, Proceedings of the 2004 acm siggraph/eurographics symposium on computer animation, P259, DOI DOI 10.1145/1028523.1028557
   Schöllhorn WI, 2002, GAIT POSTURE, V15, P180, DOI 10.1016/S0966-6362(01)00193-X
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Wu M., 2003, 16 IPPR C COMP VIS G, P605
   Wu S., 2009, P 16 ACM S VIRTUAL R, P207
   Wu SY, 2009, VISUAL COMPUT, V25, P499, DOI 10.1007/s00371-009-0345-1
   Yang K., 2004, ACM INT WORKSHOP MUL, P65, DOI DOI 10.1145/1032604.1032616
   Zordan VB, 2005, ACM T GRAPHIC, V24, P697, DOI 10.1145/1073204.1073249
NR 46
TC 33
Z9 36
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2191
EP 2202
DI 10.1109/TMM.2014.2360793
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300010
DA 2024-07-18
ER

PT J
AU Qi, H
   Stojmenovic, M
   Li, KQ
   Li, ZY
   Qu, WY
AF Qi, Heng
   Stojmenovic, Milos
   Li, Keqiu
   Li, Zhiyang
   Qu, Wenyu
TI A Low Transmission Overhead Framework of Mobile Visual Search Based on
   Vocabulary Decomposition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag of visual words; joint optimized product quantization; mobile visual
   search; vector quantization
AB Due to the bandwidth limitation in wireless networks, transmission overhead is a big problem in Mobile Visual Search (MVS). Existing work proposes transmitting the compressed local feature descriptors instead of the query image to reduce the transmission overhead. Although many kinds of compressed descriptors are proposed, designing a suitable lossless compressed descriptor has proven elusive. In this paper, we propose a novel framework for MVS with low transmission overhead rather than focusing on compressed descriptors. The key point of the proposed framework is to migrate the vector quantization in the bag of visual words model from the server to the client. In this framework, no matter what descriptors are used, the client only transmits the ID numbers of the visual words to the server, thereby reaching the minimal possible transmission overhead. To achieve this goal, we present vocabulary decomposition by which we can decompose the large vocabulary into several small ones satisfying storage constraints on mobile devices. In this paper, we first formulate vocabulary decomposition as an optimization problem. We then present Joint Product Quantization (JPQ) and Joint Optimized Product Quantization (JOPQ) to address the proposed optimization problem. Finally, we conduct a large number of simulation experiments and real experiments. The experimental results show that the proposed framework outperforms the existing framework by reducing more than 95% of the transmission overhead.
C1 [Qi, Heng; Li, Keqiu] Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.
   [Stojmenovic, Milos] Singidunum Univ, Dept Informat & Comp, Belgrade, Serbia.
   [Li, Zhiyang; Qu, Wenyu] Dalian Maritime Univ, Sch Informat & Technol, Dalian 116026, Peoples R China.
C3 Dalian University of Technology; Dalian Maritime University
RP Qi, H (corresponding author), Dalian Univ Technol, Sch Comp Sci & Technol, Dalian 116023, Peoples R China.
EM hengqi@dlut.edu.cn; milos22@gmail.com; keqiu@dlut.edu.cn;
   lizy0205@gmail.com; eunice.qu@gmail.com
FU National Science Foundation for Distinguished Young Scholars of China
   [61225010]; NSFC [61173161, 61173162, 61173165, 61300187, 61300189,
   61370199]; China Postdoctoral Science Foundation [2013M530916,
   2014T70248]; Fundamental Research Funds for the Central Universities
   [DUT13ZD101, DUT13JS04]; Serbian Ministry of Science and Education
   [TR32054]
FX This work was supported by the National Science Foundation for
   Distinguished Young Scholars of China under Grant 61225010, NSFC under
   Grants 61173161, 61173162, 61173165, 61300187, 61300189, and 61370199,
   the China Postdoctoral Science Foundation funded project under Grants
   2013M530916 and 2014T70248, and the Fundamental Research Funds for the
   Central Universities under Grants DUT13ZD101 and DUT13JS04. This work
   was supported in part by the Serbian Ministry of Science and Education
   under Grant TR32054, "Digital signal processing, and the synthesis of an
   information security system." The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Jing-Ming Guo.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P BRIT MACH VIS C
   [Anonymous], 2010, P INT MOB MULT WORKS
   [Anonymous], 2011, P 2 ANN ACM C MULTIM
   Bay H., 2008, COMPUT VIS IMAGE UND, V10, P346, DOI DOI 10.1016/j.cviu.2007.09.014
   Chandrakanth V, 2009, IEEE RAD CONF, P945, DOI 10.1109/RADAR.2009.4976955
   Chandrasekhar V, 2010, IEEE IMAGE PROC, P3885, DOI 10.1109/ICIP.2010.5649937
   Girod B, 2011, IEEE SIGNAL PROC MAG, V28, P61, DOI 10.1109/MSP.2011.940881
   Hua S. W. G., 2007, PROC 11 IEEE INT C C, P1
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Ke Y, 2004, PROC CVPR IEEE, P506
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Philbin J., 2008, P CVPR, P1
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Su Y., 2013, ACM Multimedia Conference, MM'13, Barcelona, Spain, October 21-25, 2013, P73
   Weiss Y., 2008, NIPS, V1, P4
NR 17
TC 7
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1963
EP 1972
DI 10.1109/TMM.2014.2345026
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300014
DA 2024-07-18
ER

PT J
AU Maratsolas, E
   Koutsakis, P
   Lazaris, A
AF Maratsolas, Evangelos
   Koutsakis, Polychronis
   Lazaris, Aggelos
TI Video Activity-Based Traffic Policing: A New Paradigm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Group-of-pictures; modeling; traffic policing; video
ID CODING STANDARD; QUALITY; SCHEME; MPEG-4; MODEL
AB The constant development of new multimedia applications, which are "greedy" in terms of bandwidth and Quality of Service (QoS) requirements, calls for new approaches to the traffic policing problem. This paper proposes, analyzes and presents an extensive performance evaluation of such a new approach, namely the activity-based video traffic policing. Using as a motivation recent work, which has shown that the classic traffic policing mechanisms provide unnecessarily strict policing for conforming but bursty video users, we propose five simple and efficient new mechanisms, which take into consideration and exploit video activity and the Group-of Pictures (GoP) pattern of the video traces. Contrary to the classic approach, our mechanisms do not use a token generator based on a fixed rate, but vary the token generation rate according to specific video activity-based algorithms. The results show significant improvement for conforming users, and reveal that dynamic traffic policing can provide much higher efficiency than the widely used static mechanisms. One of the new mechanisms, the Frame Size Aware Token Bucket, is shown to clearly outperform all other policing approaches for conforming users, and to provide comparable policing results with the classic mechanisms for non-conforming video users.
C1 [Maratsolas, Evangelos; Koutsakis, Polychronis] Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Greece.
   [Lazaris, Aggelos] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Technical University of Crete; University of Southern California
RP Maratsolas, E (corresponding author), Tech Univ Crete, Dept Elect & Comp Engn, Khania 73100, Greece.
EM emaratsolas@gmail.com; polk@telecom.tuc.gr; alazaris@usc.edu
OI Koutsakis, Polychronis/0000-0002-4168-0888
CR Albuquerque CVN, 1998, ITS '98 PROCEEDINGS - SBT/IEEE INTERNATIONAL TELECOMMUNICATIONS SYMPOSIUM, VOLS 1 AND 2, P177, DOI 10.1109/ITS.1998.713113
   [Anonymous], 2010, PROC OSDI
   [Anonymous], 2004, ACM T EMBED COMPUT S
   Ash G.R., 2006, Traffic Engineering and QoS Optimization of Integrated Voice Data Networks
   Cherry SM, 2004, IEEE SPECTRUM, V41, P42, DOI 10.1109/MSPEC.2004.1317876
   Chiruvolu G., 1998, P IEEE ICC
   Dai M, 2009, IEEE T MULTIMEDIA, V11, P1010, DOI 10.1109/TMM.2009.2021802
   Dyson DA, 1999, MOBILE NETW APPL, V4, P87, DOI 10.1023/A:1019182311883
   Etoh M, 2005, P IEEE, V93, P111, DOI 10.1109/JPROC.2004.839605
   Fidler M, 2004, COMPUT NETW, V44, P463, DOI 10.1016/j.comnet.2003.12.004
   Fonesca N. L. S., 2000, ACM Transactions on Modeling and Computer Simulation, V10, P104, DOI 10.1145/364996.365003
   Frey M., 2000, IEEE ACM T NETW, V8
   Goudarzi P., 2011, P 3 INT C ULTR MOD T
   Greengrass J, 2009, IEEE INTERNET COMPUT, V13, P70, DOI 10.1109/MIC.2009.14
   Kang SH, 2005, IEEE T MULTIMEDIA, V7, P1139, DOI 10.1109/TMM.2005.858401
   Knightly EW, 1998, IEEE INFOCOM SER, P635, DOI 10.1109/INFCOM.1998.665084
   Kong PY, 2003, IEEE ACM T NETWORK, V11, P994, DOI 10.1109/TNET.2003.820249
   Koutsakis P, 2009, IEEE T MOBILE COMPUT, V8, P1153, DOI 10.1109/TMC.2009.18
   Lazaris A, 2008, PERFORM EVALUATION, V65, P51, DOI 10.1016/j.peva.2007.02.004
   Le Boudec JY, 2002, IEEE ACM T NETWORK, V10, P329, DOI 10.1109/TNET.2002.1012365
   Marpe D, 2006, IEEE COMMUN MAG, V44, P134, DOI 10.1109/MCOM.2006.1678121
   McKeown N, 2008, ACM SIGCOMM COMPUT C, V38
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   ORS T, 1995, P 3 IFIP WORKSH PERF
   Procissi G, 2002, COMPUT COMMUN, V25, P1009, DOI 10.1016/S0140-3664(02)00015-4
   RATHGEB EP, 1991, IEEE J SEL AREA COMM, V9, P325, DOI 10.1109/49.76630
   REIBMAN AR, 1995, IEEE ACM T NETWORK, V3, P329, DOI 10.1109/90.392392
   Rose O, 1997, PERFORM EVALUATION, V30, P69, DOI 10.1016/S0166-5316(96)00054-5
   Sairamesh J., P 3 IEEE INT C COMP
   Sotiropoulos J., 2012, P IEEE WCNC
   Tsai C.-F., 2007, P IEEE ICC
   Van der Auwera G, 2008, IEEE T BROADCAST, V54, P698, DOI 10.1109/TBC.2008.2000422
   van Haalen R., 2007, P 15 IEEE WORKSH LOC
NR 33
TC 6
Z9 6
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1446
EP 1459
DI 10.1109/TMM.2014.2310592
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600024
DA 2024-07-18
ER

PT J
AU Serrà, J
   Müller, M
   Grosche, P
   Arcos, JL
AF Serra, Joan
   Mueller, Meinard
   Grosche, Peter
   Arcos, Josep Ll
TI Unsupervised Music Structure Annotation by Time Series Structure
   Features and Segment Similarity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Music information retrieval; Time series analysis; Unsupervised
   learning; Content-based retrieval
ID AUDIO
AB Automatically inferring the structural properties of raw multimedia documents is essential in today's digitized society. Given its hierarchical and multi-faceted organization, musical pieces represent a challenge for current computational systems. In this article, we present a novel approach to music structure annotation based on the combination of structure features with time series similarity. Structure features encapsulate both local and global properties of a time series, and allow us to detect boundaries between homogeneous, novel, or repeated segments. Time series similarity is used to identify equivalent segments, corresponding to musically meaningful parts. Extensive tests with a total of five benchmark music collections and seven different human annotations show that the proposed approach is robust to different ground truth choices and parameter settings. Moreover, we see that it outperforms previous approaches evaluated under the same framework.
C1 [Serra, Joan; Arcos, Josep Ll] IIIA CSIC, Bellaterra 08193, Spain.
   [Mueller, Meinard] Int Audio Labs Erlangen, D-91058 Erlangen, Germany.
   [Grosche, Peter] Univ Saarland, D-66123 Saarbrucken, Germany.
   [Grosche, Peter] Max Planck Inst Informat, D-66123 Saarbrucken, Germany.
C3 Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - Instituto
   de Investigacion en Inteligencia Artificial (IIIA); University of
   Erlangen Nuremberg; Saarland University; Max Planck Society
RP Serrà, J (corresponding author), IIIA CSIC, Campus UAB S-N, Bellaterra 08193, Spain.
EM jserra@iiia.csic.es; meinard.mueller@audiolabs-erlangen.de;
   pgrosche@mpi-inf.mpg.de; arcos@iiia.csic.es
RI ARCOS, JOSEP LLUIS/B-1793-2008; Serrà, Joan/E-3250-2010; Arcos, Josep
   Lluis/U-6445-2019; Mueller, Meinard/U-2097-2019
OI Arcos, Josep Lluis/0000-0001-7751-1210; Mueller,
   Meinard/0000-0001-6062-7524; Grosche, Peter/0000-0002-5525-5233
FU EU Feder; Cluster of Excellence on Multimodal Computing and Interaction
   at Saarland University; DFG MU [2682/5-1];  [ICT -2011-8-318770]; 
   [2009-SGR-1434];  [JAEDOC069/2010]
FX The work of J. Serra and J. Ll. Arcos was supported by ICT
   -2011-8-318770 and 2009-SGR-1434. The work of J. Serra also was
   supported by JAEDOC069/2010 and EU Feder funds. The work of M. Muller
   and P. Grosche was supported by the Cluster of Excellence on Multimodal
   Computing and Interaction at Saarland University and DFG MU 2682/5-1.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Mitsunori Ogihara.
CR [Anonymous], 2007, INFORM RETRIEVAL MUS
   Arnold D., 2012, OXFORD COMPANION MUS
   Ball P., 2010, MUSIC INSTINCT MUSIC
   Barrington L, 2010, IEEE T AUDIO SPEECH, V18, P602, DOI 10.1109/TASL.2009.2036306
   Bimbot F., 2012, PROC 13 INT SOC MUSI, P235
   Boutard G., 2006, P 1 WORKSH LEARN SEM, P87
   Chen R., 2011, 12th International Society for Music Information Retrieval Conference (ISMIR 2011), P477
   Dannenberg R.B., 2002, Proceedings of the International Symposium on Music Information Retrieval, P63
   Dannenberg R. B., 2008, HDB SIGNAL PROCESSIN, V1, P305
   Downie JS, 2010, STUD COMPUT INTELL, V274, P93
   Ewert S, 2009, INT CONF ACOUST SPEE, P1869, DOI 10.1109/ICASSP.2009.4959972
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P452, DOI 10.1109/ICME.2000.869637
   Gómez E, 2006, INFORMS J COMPUT, V18, P294, DOI 10.1287/ijoc.1040.0126
   Goto M, 2006, IEEE T AUDIO SPEECH, V14, P1783, DOI 10.1109/TSA.2005.863204
   Jensen K, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/73205
   Kaiser F., 2012, MUSIC INFORM RETRIEV
   Kantz H., 2004, NONLINEAR TIME SERIE, Vsecond, DOI DOI 10.1017/CBO9780511755798
   Levy M, 2008, IEEE T AUDIO SPEECH, V16, P318, DOI 10.1109/TASL.2007.910781
   Lu L., 2004, PROC ACM MULTIMEDIA, P275
   Lukashevich H., 2008, Proc. of the 10th International Society of Music Information Retrieval, P375
   Maddage NC, 2006, IEEE MULTIMEDIA, V13, P65, DOI 10.1109/MMUL.2006.3
   Martin B., 2011, MUSIC INFORM RETRIEV
   Marwan N, 2007, PHYS REP, V438, P237, DOI 10.1016/j.physrep.2006.11.001
   Mauch M., 2009, ISMIR, P231
   Müller M, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/89686
   Müller M, 2011, IEEE J-STSP, V5, P1088, DOI 10.1109/JSTSP.2011.2112333
   Muller M., 2011, ISMIR, P615
   Nishimura T., 2002, ISMIR, P287
   Ong B. S., 2006, THESIS U POMPEU FABR
   Patel A.D., 2007, MUSIC LANGUAGE BRAIN
   Paulus J., 2010, Ismir, P625
   Paulus J, 2009, IEEE T AUDIO SPEECH, V17, P1159, DOI 10.1109/TASL.2009.2020533
   Peeters G, 2004, LECT NOTES COMPUT SC, V2771, P143
   Peeters G., 2009, P INT WORKSHOP LEARN, P75
   Peeters Geoffroy., 2007, Proceedings of the International Symposium on Music Information Retrieval, P35
   Peiszer E., 2007, THESIS VIENNA U TECH
   Sargent G., 2011, P 12 INT SOC MUS INF, P483
   Serra J., 2012, Proc. of the 26th AAAI Conference on Artificial Intelligence, P1613
   Serrà J, 2008, IEEE T AUDIO SPEECH, V16, P1138, DOI 10.1109/TASL.2008.924595
   Serrà J, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/9/093017
   Simonoff J. S., 1996, Smooting Methods in Statistics
   Smith J. B. L., 2010, THESIS MCGILL U MONT
   Turnbull D., 2007, ISMIR, P51
   Weiss RJ, 2011, IEEE J-STSP, V5, P1240, DOI 10.1109/JSTSP.2011.2145356
NR 44
TC 33
Z9 45
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1229
EP 1240
DI 10.1109/TMM.2014.2310701
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600006
DA 2024-07-18
ER

PT J
AU Anttonen, A
   Mämmelä, A
AF Anttonen, Antti
   Mammela, Aarne
TI Interruption Probability of Wireless Video Streaming With Limited Video
   Lengths
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Finite video length; playback interruption; queueing theory; wireless
   video streaming
ID NETWORKS
AB In this paper, we consider a simple queueing theoretic method to predict the video interruption probability for a given video length. Specifically, a mobile user is streaming a video with a limited length and variable bit rate video encoding. The playback interruptions are caused by random packet delays occurring in the wireless link between the source and destination nodes. The dynamics of the playback buffer in the user terminal is modeled as a G/G/1 queue. To evaluate the video interruption probability, a simple asymptotic method has been presented for the case in which the video length approaches infinity. However, in many practical cases, the video length is limited, hindering the usage of the asymptotic method. We obtain a simple and closed-form upper bound for the analysis of the interruption probability that incorporates the effect of finite video lengths with known statistical delay parameters. Furthermore, a useful method is presented to select between the proposed method and the asymptotic method whose relative accuracy changes with the video length and statistical properties of the buffer load size. The accuracy of the proposed analytical method is compared with the existing methods. Finally, we address some practical challenges in buffer dimensioning when the statistical delay parameters are unknown and estimated with a finite number of received packets.
C1 [Anttonen, Antti; Mammela, Aarne] VTT Tech Res Ctr Finland, FI-90571 Oulu, Finland.
C3 VTT Technical Research Center Finland
RP Anttonen, A (corresponding author), VTT Tech Res Ctr Finland, FI-90571 Oulu, Finland.
EM antti.anttonen@vtt.fi; aarne.mammela@vtt.fi
OI Mammela, Aarne/0000-0002-6659-4126
FU COIN project - Tekes; SANTACLOUDS project - Tekes
FX This work was supported in part by the SANTACLOUDS and COIN projects
   funded by Tekes. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Tommaso Melodia.
CR Cermak GW, 2009, IEEE J-STSP, V3, P336, DOI 10.1109/JSTSP.2009.2014495
   Cox R.P, 1965, The Theory of Stochastic Processes
   DUDA A, 1986, IEEE J SEL AREA COMM, V4, P905, DOI 10.1109/JSAC.1986.1146391
   KOBAYASHI H, 1974, J ACM, V21, P316, DOI 10.1145/321812.321827
   Lehmann EL., 1998, THEORY POINT ESTIMAT, DOI 10.1007/b98854
   Li GS, 2009, IEEE T COMMUN, V57, P520, DOI 10.1109/TCOMM.2009.02.060701
   Liang GF, 2008, IEEE T MULTIMEDIA, V10, P1128, DOI 10.1109/TMM.2008.2001364
   Liew CH, 2005, IEE P-COMMUN, V152, P749, DOI 10.1049/ip-com:20045014
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   ParandehGheibi A, 2011, IEEE J SEL AREA COMM, V29, P1064, DOI 10.1109/JSAC.2011.110516
   Salah K, 2011, IET COMMUN, V5, P1978, DOI 10.1049/iet-com.2010.0868
   Simon M. K., 2005, DIGITAL COMMUNICATIO
   Xu YD, 2012, IEEE INFOCOM SER, P1826, DOI 10.1109/INFCOM.2012.6195557
   Yang J, 2011, IEEE T MULTIMEDIA, V13, P1141, DOI 10.1109/TMM.2011.2160158
NR 14
TC 15
Z9 16
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1176
EP 1180
DI 10.1109/TMM.2014.2306656
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800022
DA 2024-07-18
ER

PT J
AU Bhattacharya, S
   Mehran, R
   Sukthankar, R
   Shah, M
AF Bhattacharya, Subhabrata
   Mehran, Ramin
   Sukthankar, Rahul
   Shah, Mubarak
TI Classification of Cinematographic Shots Using Lie Algebra and its
   Application to Complex Event Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cinematographic shots; homography; lie algebra; multimedia event
   recognition; shot classification
ID CAMERA MOTION PARAMETERS; QUALITATIVE ESTIMATION; VIDEO
AB In this paper, we propose a discriminative representation of a video shot based on its camera motion and demonstrate how the representation can be used for high level multimedia tasks like complex event recognition. In our technique, we assume that a homography exists between a pair of subsequent frames in a given shot. Using purely image-based methods, we compute homography parameters that serve as coarse indicators of the ambient camera motion. Next, using Lie algebra, we map the homography matrices to an intermediate vector space that preserves the intrinsic geometric structure of the transformation. The mappings are stacked temporally to generate vector time-series per shot. To extract meaningful features from time-series, we propose an efficient linear dynamical system based technique. The extracted temporal features are further used to train linear SVMs as classifiers for a particular shot class. In addition to demonstrating the efficacy of our method on a novel dataset, we extend its applicability to recognize complex events in large scale videos under unconstrained scenarios. Our empirical evaluations on eight cinematographic shot classes show that our technique performs close to approaches that involve extraction of 3-D trajectories using computationally prohibitive structure from motion techniques.
C1 [Bhattacharya, Subhabrata; Shah, Mubarak] Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32826 USA.
   [Mehran, Ramin] Microsoft Corp, Redmond, WA 98052 USA.
   [Sukthankar, Rahul] Google Res, Mountain View, CA 94043 USA.
C3 State University System of Florida; University of Central Florida;
   Microsoft; Google Incorporated
RP Bhattacharya, S (corresponding author), Univ Cent Florida, Ctr Res Comp Vis, Orlando, FL 32826 USA.
EM subh@cs.ucf.edu; rmehran@microsoft.com; rahuls@cs.cmu.edu;
   shah@cs.ucf.edu
RI Sahoo, Sarat Kumar/J-8765-2014
OI Sahoo, Sarat Kumar/0000-0001-5734-6844; Shah,
   Mubarak/0000-0001-6172-5572
FU Intelligence Advanced Research Projects Activity (IARPA) via Department
   of Interior National Business Center [D11PC20066]
FX This work was supported by the Intelligence Advanced Research Projects
   Activity (IARPA) via Department of Interior National Business Center
   contract number D11PC20066. The U.S. Government is authorized to
   reproduce and distribute reprints for Governmental purposes
   notwithstanding any copyright annotation thereon. Disclaimer: The views
   and conclusions contained herein are those of the authors and should not
   be interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of IARPA, DoI/NBC, or the
   U.S. Government. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Christophe De
   Vleeschouwer.
CR [Anonymous], 1976, Grammar of the film language
   Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32
   Bhattacharya S., 2006, P ACM MULT, P361
   Bhattacharya S, 2011, AUGMENT VIS REAL, V1, P221, DOI 10.1007/978-3-642-11568-4_10
   DIMONTE CL, 1990, INT CONF ACOUST SPEE, P2539, DOI 10.1109/ICASSP.1990.116119
   Doulamis AD, 2004, IEEE T CIRC SYST VID, V14, P757, DOI 10.1109/TCSVT.2004.828348
   Drummond T, 2000, INT J COMPUT VISION, V37, P21, DOI 10.1023/A:1008125412549
   Ekenel H. K., 2010, P INT WORKSH AUT INF
   Fablet R, 2002, IEEE T IMAGE PROCESS, V11, P393, DOI 10.1109/TIP.2002.999674
   Fan JP, 2004, IEEE T MULTIMEDIA, V6, P70, DOI 10.1109/TMM.2003.819583
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Li RN, 2010, PROC CVPR IEEE, P2038, DOI 10.1109/CVPR.2010.5539880
   Liu CX, 2009, COMPUT VIS IMAGE UND, V113, P415, DOI 10.1016/j.cviu.2008.08.002
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Nam J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P353, DOI 10.1109/ICIP.1998.723496
   Ngo CW, 2003, IEEE T IMAGE PROCESS, V12, P341, DOI 10.1109/TIP.2003.809020
   Ngo CW, 2002, INT J COMPUT VISION, V50, P127, DOI 10.1023/A:1020341931699
   Over P., 2005, P TRECVID 2005
   Park SC, 2004, PATTERN RECOGN, V37, P767, DOI 10.1016/j.patcog.2003.07.012
   Qi Y, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P689
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Srinivasan MV, 1997, PATTERN RECOGN, V30, P593, DOI 10.1016/S0031-3203(96)00106-9
   Takagi S, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P461
   Wang HL, 2009, IEEE T CIRC SYST VID, V19, P1529, DOI 10.1109/TCSVT.2009.2022705
   Wang S., 2008, P IEEE ICIP
   Wang ZS, 2010, PROC CVPR IEEE, P879, DOI 10.1109/CVPR.2010.5540125
   Williams B., 2008, P IEEE RSJ IROS
   Zhai Y., 2005, P TREC VID RETR EV W
   Zhu XQ, 2002, LECT NOTES COMPUT SC, V2532, P1128
NR 32
TC 19
Z9 23
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 686
EP 696
DI 10.1109/TMM.2014.2300833
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, J
   Li, ZC
   Tang, JH
   Jiang, Y
   Lu, HQ
AF Liu, Jing
   Li, Zechao
   Tang, Jinhui
   Jiang, Yu
   Lu, Hanqing
TI Personalized Geo-Specific Tag Recommendation for Photos on Social
   Websites
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geo-location preference; personalized tag recommendation; subspace
   learning; tagging history; user preference
ID IMAGE ANNOTATION
AB Social tagging becomes increasingly important to organize and search large-scale community-contributed photos on social websites. To facilitate generating high-quality social tags, tag recommendation by automatically assigning relevant tags to photos draws particular research interest. In this paper, we focus on the personalized tag recommendation task and try to identify user-preferred, geo-location-specific as well as semantically relevant tags for a photo by leveraging rich contexts of the freely available community-contributed photos. For users and geo-locations, we assume they have different preferred tags assigned to a photo, and propose a subspace learning method to individually uncover the both types of preferences. The goal of our work is to learn a unified subspace shared by the visual and textual domains to make visual features and textual information of photos comparable. Considering the visual feature is a lower level representation on semantics than the textual information, we adopt a progressive learning strategy by additionally introducing an intermediate subspace for the visual domain, and expect it to have consistent local structure with the textual space. Accordingly, the unified subspace is mapped from the intermediate subspace and the textual space respectively. We formulate the above learning problems into a united form, and present an iterative optimization with its convergence proof. Given an untagged photo with its geo-location to a user, the user-preferred and the geo-location-specific tags are found by the nearest neighbor search in the corresponding unified spaces. Then we combine the obtained tags and the visual appearance of the photo to discover the semantically and visually related photos, among which the most frequent tags are used as the recommended tags. Experiments on a large-scale data set collected from Flickr verify the effectivity of the proposed solution.
C1 [Liu, Jing; Jiang, Yu; Lu, Hanqing] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Li, Zechao; Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Nanjing
   University of Science & Technology
RP Liu, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jliu@nlpr.ia.ac.cn; zechao.li@gmail.com; jinhuitang@mail.njust.edu.cn;
   yjiang@nlpr.ia.ac.cn; luhq@nlpr.ia.ac.cn
RI Tang, Jinhui/KBR-0891-2024; Jiang, Yu/KVY-8749-2024
OI Tang, Jinhui/0000-0001-9008-222X
FU 973 Program [2010CB327905]; National Natural Science Foundation of China
   [61103059, 61272329]; Program for New Century Excellent Talents in
   University [NCET-12-0632]; Open Projects Program of National Laboratory
   of Pattern Recognition
FX This work was supported in part by the 973 Program (Project No.
   2010CB327905), the National Natural Science Foundation of China (Grant
   No. 61103059 and 61272329), the Program for New Century Excellent
   Talents in University under Grant NCET-12-0632, and Open Projects
   Program of National Laboratory of Pattern Recognition. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Alberto Del Bimbo.
CR Ames M., 2007, P ACM CHI
   [Anonymous], P ACM MULT
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], 2004, P S COMP GEOM
   Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48
   Cai Y., 2011, P ACM WSDM
   Chen H., 2008, P ACM MULT
   Chen YN, 2011, IEEE T PATTERN ANAL, V33, P1073, DOI 10.1109/TPAMI.2010.197
   Eom W., 2011, P IEEE ICIP
   Garg N., 2008, P ACM REC SYST
   He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55
   Kleban J, 2009, P CIVR
   Lee S, 2010, PATTERN RECOGN LETT, V31, P976, DOI 10.1016/j.patrec.2009.12.024
   Li X., 2012, P ACM ICMR
   Li X., 2010, P CIVR
   Li X., 2011, P ACM MULT
   Li Z., 2010, P ACM MM
   Li Z., 2012, P AAAI
   Liu J, 2009, PATTERN RECOGN, V42, P218, DOI 10.1016/j.patcog.2008.04.012
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mei T, 2010, IEEE MULTIMEDIA, V17, P16, DOI 10.1109/MMUL.2010.82
   Moxley E., 2008, P ACM MIR
   Moxley E., 2009, P ICME
   Nanopoulos A, 2010, IEEE T AUDIO SPEECH, V18, P407, DOI 10.1109/TASL.2009.2033973
   Nie F., 2010, P NIPS, P1
   Qi G.-J., 2011, P ACM WWW
   Qian XM, 2013, NEUROCOMPUTING, V111, P144, DOI 10.1016/j.neucom.2012.12.021
   Rafailidis D, 2013, IEEE T SYST MAN CY-S, V43, P673, DOI 10.1109/TSMCA.2012.2208186
   Rendle S., 2010, P ACM WSDM
   Rendle S., 2009, P ACM KDD
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Sawant N., 2010, P ACM MIR
   Serdyukov P., 2009, P ACM SIGIR
   Shen Y., 2010, P ACM MULT
   Sigurbjornsson B., 2008, P ACM WWW
   Silva A., 2011, P ACM SIGSPATIAL INT
   Song Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921595
   Sun A., 2011, P ACM MULT
   Symeonidis P., 2008, P ACM WSDM
   Takashita T., 2010, P ICCSA
   Tang J., 2009, P ACM MULT
   Tang JH, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2501643.2501651
NR 42
TC 39
Z9 46
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2014
VL 16
IS 3
BP 588
EP 600
DI 10.1109/TMM.2014.2302732
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD3BJ
UT WOS:000333111500002
DA 2024-07-18
ER

PT J
AU Chen, JY
   Lin, CH
   Hsu, PC
   Chen, CH
AF Chen, Jyun-Yuan
   Lin, Chao-Hung
   Hsu, Po-Chi
   Chen, Chung-Hao
TI Point Cloud Encoding for 3D Building Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cyber city modeling; point cloud encoding; 3D model retrieval
ID SHAPE; SEARCH
AB An increasing number of three-dimensional (3D) building models are being made available on Web-based model-sharing platforms. Motivated by the concept of data reuse, an encoding approach is proposed for 3D building model retrieval using point clouds acquired by airborne light detection and ranging (LiDAR) systems. To encode LiDAR point clouds with sparse, noisy, and incomplete sampling, we introduce a novel encoding scheme based on a set of low-frequency spherical harmonic basis functions. These functions provide compact representation and ease the encoding difficulty coming from inherent noises of point clouds. Additionally, a data filling and resampling technique is proposed to solve the aliasing problem caused by the sparse and incomplete sampling of point clouds. Qualitative and quantitative analyses of LiDAR data show a clear superiority of the proposed method over related methods. A cyber campus generated by retrieving 3D building models with airborne LiDAR point clouds demonstrates the feasibility of the proposed method.
C1 [Chen, Jyun-Yuan; Lin, Chao-Hung; Hsu, Po-Chi] Natl Cheng Kung Univ, Dept Geomat, Tainan 70101, Taiwan.
   [Chen, Chung-Hao] Old Dominion Univ, Dept Elect & Comp Engn, Norfolk, VA 23529 USA.
C3 National Cheng Kung University; Old Dominion University
RP Chen, JY (corresponding author), Natl Cheng Kung Univ, Dept Geomat, Tainan 70101, Taiwan.
EM slanla@gmail.com; linhung@mail.ncku.edu.tw; misa76730@hotmail.com;
   CXCHEN@odu.edu
OI Lin, Chao-Hung/0000-0001-8126-8794
FU Headquarters of University Advancement at the National Cheng Kung
   University; Ministry of Education, Taiwan, ROC; National Science Council
   of Taiwan [NSC 102-2221-E-006-194, NSC 101-2221-E-006-257-MY2]
FX This work was supported in part by the Headquarters of University
   Advancement at the National Cheng Kung University, which is sponsored by
   the Ministry of Education, Taiwan, ROC, and in part by the National
   Science Council of Taiwan (Contract of NSC 102-2221-E-006-194 and NSC
   101-2221-E-006-257-MY2). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Zhihai
   (Henry) He.
CR Akgül CB, 2009, IEEE T PATTERN ANAL, V31, P1117, DOI 10.1109/TPAMI.2009.25
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Biasotti S, 2008, PATTERN RECOGN, V41, P2855, DOI 10.1016/j.patcog.2008.02.003
   BRECHBUHLER C, 1995, COMPUT VIS IMAGE UND, V61, P154, DOI 10.1006/cviu.1995.1013
   Chao MW, 2011, COMPUT ANIMAT VIRT W, V22, P295, DOI 10.1002/cav.396
   Charaniya A.P., 2004, CVPRW'04, Proceedings of the IEEE 2004 Conference on Computer Vision and Pattern Recognition Workshop, 27 June - 2 July 2004, Baltimore, Md, V3, P1
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   Jain V, 2007, COMPUT AIDED DESIGN, V39, P398, DOI 10.1016/j.cad.2007.02.009
   Kazhdan M., 2003, P EUR ACM SIGGRAPH S, V6, P156
   Kazhdan M., 2006, P 4 EUR S GEOM PROC, P61, DOI DOI 10.2312/SGP/SGP06/061-070
   Li TH, 1997, ANN I STAT MATH, V49, P341, DOI 10.1023/A:1003171131391
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Mademlis A, 2009, IEEE T MULTIMEDIA, V11, P1422, DOI 10.1109/TMM.2009.2032690
   Novotni M, 2004, COMPUT AIDED DESIGN, V36, P1047, DOI 10.1016/j.cad.2004.01.005
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P, 2007, PATTERN RECOGN, V40, P2437, DOI 10.1016/j.patcog.2006.12.026
   Ricard J, 2005, PATTERN RECOGN LETT, V26, P2174, DOI 10.1016/j.patrec.2005.03.030
   Shen L, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P294
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Stavropoulos G, 2010, IEEE T MULTIMEDIA, V12, P692, DOI 10.1109/TMM.2010.2053023
   Tam GKL, 2007, IEEE T VIS COMPUT GR, V13, P470, DOI 10.1109/TVCG.2007.1011
   Veltkamp RC, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P215, DOI 10.1109/SMI.2008.4547974
   Wilson JP, 2012, GEOMORPHOLOGY, V137, P107, DOI 10.1016/j.geomorph.2011.03.012
   Yeh IC, 2011, IEEE T VIS COMPUT GR, V17, P1178, DOI 10.1109/TVCG.2010.124
NR 30
TC 29
Z9 33
U1 3
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 337
EP 345
DI 10.1109/TMM.2013.2286580
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800005
DA 2024-07-18
ER

PT J
AU Li, K
   Dai, QH
   Wang, RP
   Liu, YB
   Xu, F
   Wang, J
AF Li, Kai
   Dai, Qionghai
   Wang, Ruiping
   Liu, Yebin
   Xu, Feng
   Wang, Jue
TI A Data-Driven Approach for Facial Expression Retargeting in Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial expression; expression retargeting; expression synthesis;
   expression similarity metric; data-driven
AB This paper presents a data-driven approach for facial expression retargeting in video, i.e., synthesizing a face video of a target subject that mimics the expressions of a source subject in the input video. Our approach takes advantage of a pre-existing facial expression database of the target subject to achieve realistic synthesis. First, for each frame of the input video, a new facial expression similarity metric is proposed for querying the expression database of the target person to select multiple candidate images that are most similar to the input. The similarity metric is developed using a metric learning approach to reliably handle appearance difference between different subjects. Secondly, we employ an optimization approach to choose the best candidate image for each frame, resulting in a retrieved sequence that is temporally coherent. Finally, a spatio-temporal expression mapping method is employed to further improve the synthesized sequence. Experimental results show that our system is capable of generating high quality facial expression videos that match well with the input sequences, even when the source and target subjects have big identity difference. In addition, extensive evaluations demonstrate the high accuracy of the learned expression similarity metric and the effectiveness of our retrieval strategy.
C1 [Li, Kai; Dai, Qionghai; Liu, Yebin] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Automat, Beijing 100084, Peoples R China.
   [Li, Kai] Tsinghua Univ, Grad Sch Shenzhen, Beijing 100084, Peoples R China.
   [Wang, Ruiping] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Xu, Feng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Wang, Jue] Adobe Syst Inc, Seattle, WA 98103 USA.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua University; Chinese Academy of Sciences; Institute of Computing
   Technology, CAS; Microsoft; Microsoft Research Asia; Adobe Systems Inc.
RP Li, K (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Automat, Beijing 100084, Peoples R China.
EM l-k10@mails.tsinghua.edu.cn; qionghaidai@tsinghua.edu.cn;
   wangruiping@ict.ac.cn; liuyebin@tsinghua.edu.cn; fengxu@microsoft.com;
   juewang@ieee.org
RI Liu, Yebin/L-7393-2019; Dai, Qionghai/ABD-5298-2021; Wang,
   Jue/GVU-0480-2022
OI Dai, Qionghai/0000-0001-7043-3061; Wang, Jue/0000-0002-3641-3136; Li,
   Kai/0000-0002-8336-9684
FU National Basic Research Project [2010CB731800]; NSFC [61035002,
   61120106003]
FX This work was supported by the National Basic Research Project (No.
   2010CB731800) and the Project of NSFC (No. 61035002 & 61120106003). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR [Anonymous], 1997, Parallel Optimization: Theory, Algorithms and Applications
   [Anonymous], 2009, P 2009 ACM SIGGRAPHE, DOI [DOI 10.1145/1599470.1599472, 10.1145/1599470.1599472]
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2003, COMPUT GRAPH FORUM, V22, P641, DOI 10.1111/1467-8659.t01-1-00712
   COOTES TF, 1995, COMPUT VIS IMAGE UND, V61, P38, DOI 10.1006/cviu.1995.1004
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cormen T. H., 2009, Introduction to Algorithms, VSecond
   Dale K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024164
   Davis J. V., 2007, ICML, P209
   Deng Y, 2011, IEEE T IMAGE PROCESS, V20, P2329, DOI 10.1109/TIP.2011.2109729
   Ekman P., 1978, Facial action coding system
   Kemelmacher-Shlizerman I, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964956
   Kemelmacher-Shlizerman I, 2010, LECT NOTES COMPUT SC, V6311, P341, DOI 10.1007/978-3-642-15549-9_25
   Kovar L., 2003, ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P214
   Li H, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778769
   Li K, 2012, PROC CVPR IEEE, P57, DOI 10.1109/CVPR.2012.6247658
   Littlewort G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P298, DOI 10.1109/FG.2011.5771414
   Litwinowicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P409, DOI 10.1145/192161.192270
   LIU C., 2009, Ph.D. Thesis
   Liu ZC, 2001, COMP GRAPH, P271
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Lyons M, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P200, DOI 10.1109/AFGR.1998.670949
   Niu ZH, 2009, IEEE SIGNAL PROC LET, V16, P897, DOI 10.1109/LSP.2009.2026457
   Noh JY, 2001, COMP GRAPH, P277, DOI 10.1145/383259.383290
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pighin F., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P75, DOI 10.1145/280814.280825
   SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055
   Saragih Jason M, 2011, Proc Int Conf Autom Face Gesture Recognit, P117, DOI 10.1109/FG.2011.5771400
   Seol Y, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2159516.2159519
   Tena JR, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964971
   Vlasic D, 2005, ACM T GRAPHIC, V24, P426, DOI 10.1145/1073204.1073209
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Weise T, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964972
   Williams L., 1990, Proceedings of SIGGRAPH, V24, P235
   Xu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964927
   Yang F, 2012, PROC CVPR IEEE, P861, DOI 10.1109/CVPR.2012.6247759
   Young D.M., 1971, ITERATIVE SOLUTION L
   Zhang L, 2004, ACM T GRAPHIC, V23, P548, DOI 10.1145/1015706.1015759
   Zhang QS, 2006, IEEE T VIS COMPUT GR, V12, P48, DOI 10.1109/TVCG.2006.9
NR 40
TC 24
Z9 27
U1 1
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 299
EP 310
DI 10.1109/TMM.2013.2293064
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800002
DA 2024-07-18
ER

PT J
AU Li, ZH
   Zhang, BJ
   Yu, Y
   Shen, JL
   Wang, Y
AF Li, Zhonghua
   Zhang, Bingjun
   Yu, Yi
   Shen, Jialie
   Wang, Ye
TI Query-Document-Dependent Fusion: A Case Study of Multimodal Music
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Information retrieval; multimodal; query-document-dependent fusion.
AB In recent years, multimodal fusion has emerged as a promising technology for effective multimedia retrieval. Developing the optimal fusion strategy for different modalities (e. g., content, metadata) has been the subject of intensive research. Given a query, existing methods derive a unified fusion strategy for all documents with the underlying assumption that the relative significance of a modality remains the same across all documents. However, this assumption is often invalid. We thus propose a general multimodal fusion framework, query-document-dependent fusion (QDDF), which derives the optimal fusion strategy for each query-document pair via intelligent content analysis of both queries and documents. By investigating multimodal fusion strategies adaptive to both queries and documents, we demonstrate that existing multimodal fusion approaches are special cases of QDDF and propose two QDDF approaches to derive fusion strategies. The dual-phase QDDF explicitly derives and fuses query-and document-dependent weights, and the regression-based QDDF determines the fusion weight for a query-document pair via a regression model derived from training data. To evaluate the proposed approaches, comprehensive experiments have been conducted using a multimedia data set with around 17 K full songs and over 236 K social queries. Results indicate that the regression-based QDDF is superior in handling single-dimension queries. In comparison, the dual-phase QDDF outperforms existing approaches for most query types. We found that document-dependent weights are instrumental in enhancing multimedia fusion performance. In addition, efficiency analysis demonstrates the scalability of QDDF over large data sets.
C1 [Li, Zhonghua; Zhang, Bingjun; Yu, Yi; Wang, Ye] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
   [Shen, Jialie] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
C3 National University of Singapore; Singapore Management University
RP Li, ZH (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM lizhongh@comp.nus.edu.sg; bingjun@comp.nus.edu.sg; yuy@comp.nus.edu.sg;
   jlshen@smu.edu.sg; wangye@comp.nus.edu.sg
RI Wang, Ye/KGL-6405-2024; SHEN, Jialie/E-8573-2012; Shen,
   Jialie/AAX-6851-2020
OI Wang, Ye/0000-0002-0123-1260; 
FU Singapore National Research Foundation under its International Research
   Centre @ Singapore Funding Initiative
FX This work was supported by the Singapore National Research Foundation
   under its International Research Centre @ Singapore Funding Initiative
   and administered by the IDM Programme Office.
CR [Anonymous], 2008, P 25 INT C MACHINE L, DOI [10.1145/1390156.1390273, DOI 10.1145/1390156.1390273]
   [Anonymous], MM 08
   [Anonymous], 2010, P 10 ANN JOINT C DIG, DOI [DOI 10.1145/1816123.1816146, 10.1145/1816123.1816146, 10.1145/1816123.18161461,2,4,6,7]
   [Anonymous], P NIST TRECVID WORKS
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   BARTELL B, 1995, CURRENT TRENDS IN CONNECTIONISM, P345
   Bartell B. T., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P173
   Benitez AB, 1998, IEEE INTERNET COMPUT, V2, P59, DOI 10.1109/4236.707692
   Cai KK, 2007, MUE: 2007 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND UBIQUITOUS ENGINEERING, PROCEEDINGS, P1021
   Cui B., 2007, P ACM MM 07, P1055
   Fox E. A., 1994, Second Text REtrieval Conference (TREC-2) (NIST-SP 500-215), P243
   Kang I.-H., 2003, P 26 ANN INT ACM SIG, P64
   Kennedy L. S., 2005, 13th Annual ACM International Conference on Multimedia, P882, DOI 10.1145/1101149.1101339
   Kennedy L, 2008, P IEEE, V96, P567, DOI 10.1109/JPROC.2008.916345
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Laurier C, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P688, DOI 10.1109/ICMLA.2008.96
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Li Z., 2011, P 19 ACM INT C MULTI, P1105
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Mayer R., 2011, P 12 INT SOC MUS INF, P675
   Mayer Rudolf, 2008, P 16 ACM INT C MULT, P159, DOI [10.1145/1459359.1459382, DOI 10.1145/1459359.1459382]
   McFee Brian., 2009, ISMIR, P513
   Neumayer R, 2007, LECT NOTES COMPUT SC, V4425, P724
   Ngo C.-W., 2007, P NIST TRECVID WORKS
   Orio N., 2011, INT SOC MUS INF RETR, P603
   PORTER MF, 1980, PROGRAM-AUTOM LIBR, V14, P130, DOI 10.1108/eb046814
   Robertson S., 1995, 4 TEXT RETRIEVAL C T, P73
   Rong Yan, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P324
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Shalev-Shwartz S., 2007, P 24 INT C MACH LEAR, P807
   Shen JL, 2006, IEEE T MULTIMEDIA, V8, P1179, DOI 10.1109/TMM.2006.884618
   SMEATON A, 2003, IMAGE VIDEO RETRIEVA, P451
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Tzanetakis G., 2000, Organised Sound, V4, P169, DOI DOI 10.1017/S1355771800003071
   Voorhees E. M., 1995, SIGIR Forum, P172
   Wu S., 2002, Proceedings of the Eleventh International Conference on Information and Knowledge Management. CIKM 2002, P648, DOI 10.1145/584792.584908
   Wu Y, 2004, IEEE IMAGE PROC, P2391
   Wu Y., 2004, ACM INT C MULTIMEDIA, P572, DOI DOI 10.1145/1027527.1027665
   Xie LX, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1499
   Yan R., 2004, PROC ACM INT C MULTI, P548
   Yom-Tov E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P512, DOI 10.1145/1076034.1076121
   Yom-Tov E., 2005, P ACM SIGIR QUER PRE
   Zhang B., 2009, P 17 ACM INT C MULT, P213
   Zhang BJ, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P403, DOI 10.1145/1571941.1572011
NR 45
TC 4
Z9 4
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1830
EP 1842
DI 10.1109/TMM.2013.2280437
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900009
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhang, XG
   Huang, TJ
   Tian, YH
   Geng, MC
   Ma, SW
   Gao, W
AF Zhang, Xianguo
   Huang, Tiejun
   Tian, Yonghong
   Geng, Mingchao
   Ma, Siwei
   Gao, Wen
TI Fast and Efficient Transcoding Based on Low-Complexity Background
   Modeling and Adaptive Block Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Background modeling; classification; surveillance and conference videos;
   transcoding.
ID MOTION ESTIMATION; MPEG-2; H.264
AB It is in urgent need to develop fast and efficient transcoding methods so as to remarkably save the storage of surveillance videos and synchronously transmit conference videos over different bandwidths. Towards this end, the special characteristics of these videos, e. g., the relatively static background, should be utilized for transcoding. Therefore, we propose a fast and efficient transcoding method (FET) based on background modeling and block classification in this paper. To improve the transcoding efficiency, FET adds the background picture, which is modeled from the originally decoded frames in low complexity, into stream in the form of an intra-coded G-picture. And then, FET utilizes the reconstructed G-picture as the long-term reference frame to transcode the following frames. This is mainly because our theoretical analyses show that G-picture can significantly improve the transcoding performance. To reduce the complexity, FET utilizes an adaptive threshold updating model for block classification and then adopts different transcoding strategies for different categories. This is due to the following statistics: after dividing blocks into categories of foreground, background and hybrid ones, different block categories have different distributions of prediction modes, motion vectors and reference frames. Extensive experiments on transcoding high-bit-rate H. 264/AVC streams to low-bit-rate ones are carried out to evaluate our FET. Over the traditional full-decoding-and-full-encoding methods, FET can save more than 35% of the transcoding bit-rate with a speed-up ratio of larger than 10 on the surveillance videos. On the conference videos which should be transcoded more timely, FET achieves more than 20 times speed- up ratio with 0.2 dB gain.
C1 [Zhang, Xianguo; Huang, Tiejun; Tian, Yonghong; Geng, Mingchao; Ma, Siwei; Gao, Wen] Peking Univ, Natl Engn Lab Video Technol, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Peking University
RP Zhang, XG (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Inst Digital Media, Beijing 100871, Peoples R China.
EM tjhuang@pku.edu.cn; yhtian@pku.edu.cn
RI Huang, Tiejun/D-6161-2011
FU National Basic Research Program of China [2009CB320906]; Chinese
   National Natural Science Foundation [61035001, 61121002, 61176139]
FX This work was supported in part by grants from the National Basic
   Research Program of China under contract No. 2009CB320906, the Chinese
   National Natural Science Foundation under contract No. 61035001,
   61121002 and 61176139.
CR [Anonymous], 2008, 35 VCEG M BERL GERM
   [Anonymous], Q6SG16 ITUT
   [Anonymous], 1984, H120 ITUT
   Fung KT, 2005, IEEE INT SYMP CIRC S, P908
   GIROD B, 1987, IEEE J SEL AREA COMM, V5, P1140, DOI 10.1109/JSAC.1987.1146632
   Haque M., 2008, P IEEE C COMP VIS PA
   Hata T., 2005, P IEEE INT WORKSH MU
   Jin X, 2011, SIGNAL PROCESS-IMAGE, V26, P130, DOI 10.1016/j.image.2011.01.002
   Kalva H, 2008, IEEE T CONSUM ELECTR, V54, P657, DOI 10.1109/TCE.2008.4560143
   Kim K, 2005, REAL-TIME IMAGING, V11, P172, DOI 10.1016/j.rti.2004.12.004
   Leontaris A, 2007, IEEE T IMAGE PROCESS, V16, P1726, DOI 10.1109/TIP.2007.896681
   Liu D, 2010, IEEE T CIRC SYST VID, V20, P325, DOI 10.1109/TCSVT.2009.2031442
   Liu YZ, 2007, J VIS COMMUN IMAGE R, V18, P253, DOI 10.1016/j.jvcir.2007.01.003
   Lu X, 2005, IEEE INT SYMP CIRC S, P1246
   Mingchao Geng, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P61, DOI 10.1109/ICME.2012.65
   Paul M, 2010, INT CONF ACOUST SPEE, P734, DOI 10.1109/ICASSP.2010.5495033
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Shin Y., 2010, INF SCI APPL     APR
   Tang Q, 2010, IEEE T CIRC SYST VID, V20, P262, DOI 10.1109/TCSVT.2009.2031521
   Vetro A, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P17, DOI 10.1109/ISCAS.2000.858677
   Wiegand T, 1999, IEEE T CIRC SYST VID, V9, P70, DOI 10.1109/76.744276
   Wu CD, 2009, IEEE IMAGE PROC, P3697, DOI 10.1109/ICIP.2009.5414226
   Xianguo Zhang, 2010, 2010 28th Picture Coding Symposium (PCS 2010), P78, DOI 10.1109/PCS.2010.5702583
   Youn J, 1999, IEEE T MULTIMEDIA, V1, P30, DOI 10.1109/6046.748169
   Zhang P., 2004, P PAC RIM C MULT OCT, P278
NR 25
TC 15
Z9 17
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1769
EP 1785
DI 10.1109/TMM.2013.2280117
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900005
DA 2024-07-18
ER

PT J
AU Phung, D
   Gupta, SK
   Nguyen, T
   Venkatesh, S
AF Dinh Phung
   Gupta, Sunil Kumar
   Thin Nguyen
   Venkatesh, Svetha
TI Connectivity, Online Social Capital, and Mood: A Bayesian Nonparametric
   Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective computing; online social capital; mental health; Bayesian
   nonparametrics
ID CORE AFFECT; HEALTH; BENEFITS; EMOTION
AB Social capital indicative of community interaction and support is intrinsically linked to mental health. Increasing online presence is now the norm. Whilst social capital and its impact on social networks has been examined, its underlying connection to emotional response such as mood, has not been investigated. This paper studies this phenomena, revisiting the concept of "online social capital" in social media communities using measurable aspects of social participation and social support. We establish the link between online capital derived from social media and mood, demonstrating results for different cohorts of social capital and social connectivity. We use novel Bayesian nonparametric factor analysis to extract the shared and individual factors in mood transition across groups of users of different levels of connectivity, quantifying patterns and degree of mood transitions. Using more than 1.6 million users from Live Journal, we show quantitatively that groups with lower social capital have fewer positive moods and more negative moods, than groups with higher social capital. We show similar effects in mood transitions. We establish a framework of how social media can be used as a barometer for mood. The significance lies in the importance of online social capital to mental well-being in overall. In establishing the link between mood and social capital in online communities, this work may suggest the foundation of new systems to monitor online mental well-being.
C1 [Dinh Phung; Gupta, Sunil Kumar; Thin Nguyen; Venkatesh, Svetha] Deakin Univ, Fac Sci & Technol, Ctr Pattern Recognit & Data Analyt PRaDA, Malvern, Australia.
C3 Deakin University
RP Phung, D (corresponding author), Deakin Univ, Fac Sci & Technol, Ctr Pattern Recognit & Data Analyt PRaDA, Malvern, Australia.
EM dinh.phung@deakin.edu.au; sunil.gupta@deakin.edu.au;
   thin.nguyen@deakin.edu.au; svetha.venkatesh@deakin.edu.au
RI Nguyen, Thin/IXD-7832-2023; Phung, Dinh Q/D-1328-2012
OI Nguyen, Thin/0000-0003-3467-8963; Phung, Dinh/0000-0002-9977-8247;
   gupta, sunil/0000-0002-4669-9940; Venkatesh, Svetha/0000-0001-8675-6631
CR [Anonymous], 1999, Tech. Rep. C-1
   [Anonymous], P ACM C HUM FACT COM
   [Anonymous], P ACM WORKSH STYL AN
   [Anonymous], P INT C ART INT STAT
   [Anonymous], 2012, P 21 INT C WORLD WID
   [Anonymous], 2007, Artificial Intelligence and Statistics, P564
   [Anonymous], 2007, P MACH LEARN RES
   [Anonymous], P 19 ACM C HYP HYP N
   [Anonymous], 2007, The development and psychometric properties of LIWC2007
   Best SJ, 2006, SOC SCI COMPUT REV, V24, P395, DOI 10.1177/0894439306286855
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Bollen J., 2011, J COMPUTAT SCI
   Burke M, 2011, 29TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P571
   Cornwell EY, 2009, J HEALTH SOC BEHAV, V50, P31, DOI 10.1177/002214650905000103
   Ellison NB, 2007, J COMPUT-MEDIAT COMM, V12, P1143, DOI 10.1111/j.1083-6101.2007.00367.x
   FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360
   Fowler JH, 2008, BMJ-BRIT MED J, V337, DOI 10.1136/bmj.a2338
   Ghahramani Z., 2006, NIPS 2005), P475
   Gupta S., 2012, P SIAM INT C DAT MIN
   Gupta S.K., 2010, Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, P1169, DOI DOI 10.1145/1835804.1835951
   Gupta S. K., 2012, P 12 SIAM INT C DAT
   Gupta S. K., 2012, P 21 INT C PATT REC
   Gupta S. K., 2011, P 15 PAC AS C ADV KN
   Gupta SK, 2012, P 28 UNC ART INT UAI
   HJORT NL, 1990, ANN STAT, V18, P1259, DOI 10.1214/aos/1176347749
   Holt-Lunstad J, 2010, PLOS MED, V7, DOI 10.1371/journal.pmed.1000316
   Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758
   Jia Yangqing, 2014, P ACM INT C MULT
   Kawachi I, 1999, AM J PUBLIC HEALTH, V89, P1187, DOI 10.2105/AJPH.89.8.1187
   Kawachi I, 1997, AM J PUBLIC HEALTH, V87, P1491, DOI 10.2105/AJPH.87.9.1491
   Kazienko P, 2006, LECT NOTES ARTIF INT, V4252, P417
   Knowles D, 2007, LECT NOTES COMPUT SC, V4666, P381
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lomas J, 1998, SOC SCI MED, V47, P1181, DOI 10.1016/S0277-9536(98)00190-7
   Mauss I, 2009, COGNITION EMOTION, V23, P209, DOI 10.1080/02699930802204677
   MEEDS E., 2007, Adv. in Neural Information Processing Systems NIPS, V19, P977
   Nguyen T., 2012, KNOWL INF SYST KAIS
   Nguyen T., 2013, P AAAI INT C WEBL SO
   Nguyen T, 2010, LECT NOTES ARTIF INT, V6119, P283
   Paisley J., 2009, P 26 ANN INT C MACH, P777
   Piliavin JA, 2007, J HEALTH SOC BEHAV, V48, P450, DOI 10.1177/002214650704800408
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Russell JA, 2009, COGNITION EMOTION, V23, P1259, DOI 10.1080/02699930902809375
   SETHURAMAN J, 1994, STAT SINICA, V4, P639
   Smith M., 2008, Proceedings of the 2nd PhD workshop on Information and knowledge management, P17, DOI DOI 10.1145/1458550.1458554
   Spitz RA, 1945, PSYCHOANAL STUD CHIL, V1, P53, DOI 10.1080/00797308.1945.11823126
   Tang J, 2012, IEEE T AFFECT COMPUT, V3, P132, DOI 10.1109/T-AFFC.2011.23
   Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985
   Titsias M., 2008, ADV NEURAL INF PROCE, V19
   Valenzuela S, 2009, J COMPUT-MEDIAT COMM, V14, P875, DOI 10.1111/j.1083-6101.2009.01474.x
   VicHealth, 2010, OPP SOC CONN DET MEN, V11
   Wellman B, 2001, SCIENCE, V293, P2031, DOI 10.1126/science.1065547
   Wiebe J., 2007, TUTORIAL EUROLAN SUM
   Wilkinson RG, 1997, BRIT MED J, V314, P591, DOI 10.1136/bmj.314.7080.591
   Wood F., 2007, Advances in Neural Information Processing Systems, V19, P1513
NR 57
TC 12
Z9 15
U1 2
U2 52
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1316
EP 1325
DI 10.1109/TMM.2013.2264274
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400009
DA 2024-07-18
ER

PT J
AU Liu, F
   Zhang, Y
   Liu, SQ
   Zhang, B
   Liu, Q
   Yang, Y
   Zhang, B
   Luo, JW
   Shan, BC
   Bai, J
AF Liu, Fei
   Zhang, Yue
   Liu, Shuangquan
   Zhang, Bin
   Liu, Qing
   Yang, Yi
   Zhang, Bin
   Luo, Jianwen
   Shan, Baoci
   Bai, Jing
TI Monitoring of Tumor Response to Au Nanorod-Indocyanine Green Conjugates
   Mediated Therapy With Fluorescence Imaging and Positron Emission
   Tomography
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dual-modality imaging; fluorescence; photodynamic therapy; photothermal
   therapy; [F-18]FDG-PET
ID GROWTH-FACTOR RECEPTOR; GOLD NANORODS; PREDICTION; CHEMOTHERAPY; SURGERY
AB Fluorescence imaging can track the expression of fluorescent protein and F-18-fluorodeoxyglucose based positron emission tomography ([F-18]FDG-PET) can evaluate the changes of [F-18]FDG uptake in tumor cells during the antitumor treatment. In this work, fluorescence imaging and [F-18]FDG-PET were both employed to monitor tumor response to Au nanorod-indocyanine green (AuNR-ICG) conjugates mediated therapy in a subcutaneous MDA-MB-231 mouse xenograft model. Serial fluorescence and [F-18]FDG-PET images following the antitumor treatment were obtained and quantitative analysis revealed significant decreases in fluorescence intensity and metabolic activity in tumors treated with AuNR-ICG conjugates under near infrared laser irradiation. The results suggest that the combination of fluorescence and [F-18]FDG-PET imaging can provide a noninvasive tool to assess the tumor response to antitumor therapy on a molecular scale.
C1 [Liu, Fei; Zhang, Yue; Zhang, Bin; Liu, Qing; Luo, Jianwen; Bai, Jing] Tsinghua Univ, Dept Biomed Engn, Sch Med, Beijing 100084, Peoples R China.
   [Liu, Shuangquan; Shan, Baoci] Chinese Acad Sci, Key Lab Nucl Analyt Tech, Inst High Energy Phys, Beijing 100049, Peoples R China.
   [Yang, Yi] Tianjin Med Univ, Canc Inst & Hosp, Publ Lab, Tianjin 300060, Peoples R China.
   [Zhang, Bin] Tianjin Med Univ, Canc Inst & Hosp, Natl Key Lab Breast Canc Prevent & Treatment, Dept Breast Canc Surg, Tianjin 300060, Peoples R China.
   [Luo, Jianwen] Tsinghua Univ, Ctr Biomed Imaging Res, Beijing 100084, Peoples R China.
C3 Tsinghua University; Chinese Academy of Sciences; Institute of High
   Energy Physics, CAS; Tianjin Medical University; Tianjin Medical
   University; Tsinghua University
RP Liu, F (corresponding author), Tsinghua Univ, Dept Biomed Engn, Sch Med, Beijing 100084, Peoples R China.
EM deabj@tsinghua.edu.cn
RI bai, jing/IUP-9367-2023; Luo, Jianwen/D-5612-2011
OI Luo, Jianwen/0000-0001-9215-5568; Shan, Baoci/0000-0001-7417-5063; Liu,
   Shuangquan/0000-0001-8354-5927
FU National Basic Research Program of China (973) [2011CB707701]; National
   Natural Science Foundation of China [81227901, 81071191, 81271617];
   National Major Scientific Instrument and Equipment Development Project
   [2011YQ030114]; National Science and Technology Support Program
   [2012BAI23B00]
FX This work was supported in part by the National Basic Research Program
   of China (973) under Grant No. 2011CB707701; the National Natural
   Science Foundation of China under Grant No. 81227901, 81071191,
   81271617; the National Major Scientific Instrument and Equipment
   Development Project under Grant No. 2011YQ030114; and the National
   Science and Technology Support Program under Grant No. 2012BAI23B00. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Tom Eichele.
CR Avril N, 2005, J CLIN ONCOL, V23, P7445, DOI 10.1200/JCO.2005.06.965
   Bäumler W, 1999, BRIT J CANCER, V80, P360, DOI 10.1038/sj.bjc.6690363
   Chen XY, 2004, CANCER RES, V64, P8009, DOI 10.1158/0008-5472.CAN-04-1956
   Cong HP, 2010, LANGMUIR, V26, P4188, DOI 10.1021/la9032223
   Fickweiler S, 1997, J PHOTOCH PHOTOBIO B, V38, P178, DOI 10.1016/S1011-1344(96)07453-2
   Gotoh K, 2009, J SURG ONCOL, V100, P75, DOI 10.1002/jso.21272
   Huff TB, 2007, NANOMEDICINE-UK, V2, P125, DOI 10.2217/17435889.2.1.125
   Jang B, 2011, ACS NANO, V5, P1086, DOI 10.1021/nn102722z
   Ke S, 2003, CANCER RES, V63, P7870
   Kuo WS, 2012, BIOMATERIALS, V33, P3270, DOI 10.1016/j.biomaterials.2012.01.035
   Kuo WS, 2010, ANGEW CHEM INT EDIT, V49, P2711, DOI 10.1002/anie.200906927
   Li ZM, 2010, MOL PHARMACEUT, V7, P94, DOI 10.1021/mp9001415
   Liu F, 2012, INT J AUTOM COMPUT, V9, P232, DOI 10.1007/s11633-012-0639-z
   Liu SQ, 2011, IEEE T NUCL SCI, V58, P51, DOI 10.1109/TNS.2010.2068310
   Mahmood U, 1999, RADIOLOGY, V213, P866, DOI 10.1148/radiology.213.3.r99dc14866
   Manning HC, 2008, CLIN CANCER RES, V14, P7413, DOI 10.1158/1078-0432.CCR-08-0239
   Massoud TF, 2003, GENE DEV, V17, P545, DOI 10.1101/gad.1047403
   Medarova Z, 2009, CANCER RES, V69, P1182, DOI 10.1158/0008-5472.CAN-08-2001
   Miyashiro I, 2008, ANN SURG ONCOL, V15, P1640, DOI 10.1245/s10434-008-9872-7
   Niidome T, 2009, J BIOMAT SCI-POLYM E, V20, P1203, DOI 10.1163/156856209X452953
   Ntziachristos V, 2005, NAT BIOTECHNOL, V23, P313, DOI 10.1038/nbt1074
   Orendorff CJ, 2006, J PHYS CHEM B, V110, P3990, DOI 10.1021/jp0570972
   Petrovsky A, 2003, CANCER RES, V63, P1936
   Sau TK, 2004, LANGMUIR, V20, P6414, DOI 10.1021/la049463z
   Sheth RA, 2009, GYNECOL ONCOL, V112, P616, DOI 10.1016/j.ygyno.2008.11.018
   Stroobants S, 2003, EUR J CANCER, V39, P2012, DOI 10.1016/S0959-8049(03)00073-X
   Su H, 2006, CLIN CANCER RES, V12, P5659, DOI 10.1158/1078-0432.CCR-06-0368
   Tseng JR, 2008, J NUCL MED, V49, P129, DOI 10.2967/jnumed.106.038836
   Wang SL, 2007, CANCER BIOL THER, V6, P1649, DOI 10.4161/cbt.6.10.4948
   Yang M, 2000, P NATL ACAD SCI USA, V97, P1206, DOI 10.1073/pnas.97.3.1206
   Yun MK, 2010, CHINESE PHYS C, V34, P231, DOI 10.1088/1674-1137/34/2/015
   Zhang B, 2011, J BIOMED OPT, V16, DOI 10.1117/1.3665438
NR 32
TC 6
Z9 6
U1 3
U2 69
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1025
EP 1030
DI 10.1109/TMM.2013.2244204
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600006
DA 2024-07-18
ER

PT J
AU Vallet, F
   Essid, S
   Carrive, J
AF Vallet, Felicien
   Essid, Slim
   Carrive, Jean
TI A Multimodal Approach to Speaker Diarization on TV Talk-Shows
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fusion; joint audiovisual processing; multimodality; speaker
   diarization; SVM classification; talk-show; unsupervised learning
ID AUDIO
AB In this article, we propose solutions to the problem of speaker diarization of TV talk-shows, a problem for which adapted multimodal approaches, relying on other streams of data than only audio, remain largely under exploited. Hence we propose an original system that leverages prior knowledge on the structure of this type of content, especially the visual information relating to the active speakers, for an improved diarization performance. The architecture of this system can be decomposed into two main stages. First a reliable training set is created, in an unsupervised fashion, for each participant of the TV program being processed. This data is assembled by the association of visual and audio descriptors carefully selected in a clustering cascade. Then, Support Vector Machines are used for the classification of the speech data (of a given TV program). The performance of this new architecture is assessed on two French talk-show collections: Le Grand Echiquier and On n'a pas tout dit. The results show that our new system outperforms state-of-the-art methods, thus evidencing the effectiveness of kernel-based methods, as well as visual cues, in multimodal approaches to speaker diarization of challenging contents such as TV talk-shows.
C1 [Vallet, Felicien; Carrive, Jean] Inst Natl Audiovisuel, Res Dept, F-94366 Bry Sur Marne, France.
   [Essid, Slim] CNRS, LTCI, Telecom ParisTech, Inst Mines Telecom, F-75014 Paris, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris; Centre National de la Recherche Scientifique (CNRS)
RP Vallet, F (corresponding author), Inst Natl Audiovisuel, Res Dept, F-94366 Bry Sur Marne, France.
CR Miro XA, 2012, IEEE T AUDIO SPEECH, V20, P356, DOI 10.1109/TASL.2011.2125954
   [Anonymous], 2007, EURASIP J APPL SIG P
   [Anonymous], 2009, NIST RICH TRANSCR 20
   [Anonymous], P INT WORKSH STAT CO
   Bäckström T, 2006, SIGNAL PROCESS, V86, P3286, DOI 10.1016/j.sigpro.2006.01.010
   Bendris M., 2010, P INT C MACH VIS HON
   Bendris M., 2010, P CONT BAS MULT IND
   Bendris M., 2011, THESIS TELECOM PARIS
   Besson P, 2008, IEEE T MULTIMEDIA, V10, P63, DOI 10.1109/TMM.2007.911302
   Bigot B., 2010, P ACM WORKSH SEARCH
   Bonastre J.-F., 2008, J ET PAR AV FRANC JU
   Bozonnet S., 2010, P INT C AC SPEECH SI
   Bozonnet S., 2010, P EUR SIGN PROC C AA
   Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   Brown L., 2002, YOUR PUBLIC BEST COM
   Carletta J., 2005, P MACH LEARN MULT IN
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen M.-Y., 2004, P INT C AC SPEECH SI
   Clements S., 2004, SHOW RUNNER PRODUCIN
   Dielmann A., 2010, P MULT SIGN PROC SAI
   Essid S, 2006, IEEE T AUDIO SPEECH, V14, P68, DOI 10.1109/TSA.2005.860351
   Everingham M., 2006, P BRIT MACH VIS C ED
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Friedland G., 2009, P ACM INT C MULT BEI
   Friedland G., 2009, P INT C AC SPEECH SI
   Gillet O, 2007, IEEE T CIRC SYST VID, V17, P347, DOI 10.1109/TCSVT.2007.890831
   HUNG H, 2008, P WORKSH MULT MULT S
   Jaffre G., 2004, P INT C AD PERS FUS
   Khoury E. E., 2007, P INT WORKSH CONT BA
   Li D., 2003, P ACM INT C MULT BER
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Maji S., 2008, P COMP VIS PATT REC
   Patterson E. K., 2002, P INT C AC SPEECH SI
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH, VVolume 14
   Richard G., 2007, P INT C AC SPEECH SI
   Salamin H, 2012, IEEE T MULTIMEDIA, V14, P338, DOI 10.1109/TMM.2011.2173927
   Sargin ME, 2007, IEEE T MULTIMEDIA, V9, P1396, DOI 10.1109/TMM.2007.906583
   Scholkopf B., 2001, LEARNING KERNELS SUP
   SNEDECOR G W, 1980
   Tranter SE, 2006, IEEE T AUDIO SPEECH, V14, P1557, DOI 10.1109/TASL.2006.878256
   Vajaria H., 2006, P INT C PATT REC HON
   Vallet F., 2012, TV CONTENT ANAL TECN
   Vallet F., 2010, P INT C IM PROC HONG
   Wooters C, 2008, LECT NOTES COMPUT SC, V4625, P509
   Wu TF, 2004, J MACH LEARN RES, V5, P975
   Zhang C, 2008, IEEE T MULTIMEDIA, V10, P1541, DOI 10.1109/TMM.2008.2007344
   Zhou SK, 2006, IEEE T PATTERN ANAL, V28, P917, DOI 10.1109/TPAMI.2006.120
NR 47
TC 23
Z9 24
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2013
VL 15
IS 3
BP 509
EP 520
DI 10.1109/TMM.2012.2233724
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109ZM
UT WOS:000316410900004
DA 2024-07-18
ER

PT J
AU Jiang, YG
   Wang, J
   Xue, XY
   Chang, SF
AF Jiang, Yu-Gang
   Wang, Jun
   Xue, Xiangyang
   Chang, Shih-Fu
TI Query-Adaptive Image Search With Hash Codes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Query-adaptive image search; scalability; hash codes; weighted Hamming
   distance
ID SCENE
AB Scalable image search based on visual similarity has been an active topic of research in recent years. State-of-the-art solutions often use hashing methods to embed high-dimensional image features into Hamming space, where search can be performed in real-time based on Hamming distance of compact hash codes. Unlike traditional metrics (e.g., Euclidean) that offer continuous distances, the Hamming distances are discrete integer values. As a consequence, there are often a large number of images sharing equal Hamming distances to a query, which largely hurts search results where fine-grained ranking is very important. This paper introduces an approach that enables query-adaptive ranking of the returned images with equal Hamming distances to the queries. This is achieved by firstly offline learning bitwise weights of the hash codes for a diverse set of predefined semantic concept classes. We formulate the weight learning process as a quadratic programming problem that minimizes intra-class distance while preserving inter-class relationship captured by original raw image features. Query-adaptive weights are then computed online by evaluating the proximity between a query and the semantic concept classes. With the query-adaptive bitwise weights, returned images can be easily ordered by weighted Hamming distance at a finer-grained hash code level rather than the original Hamming distance level. Experiments on a Flickr image dataset show clear improvements from our proposed approach.
C1 [Jiang, Yu-Gang; Xue, Xiangyang] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Xue, Xiangyang] Fudan Univ, Dept Comp Sci, Shanghai 200433, Peoples R China.
   [Wang, Jun] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Chang, Shih-Fu] Columbia Univ, Dept Elect Engn, New York, NY 10027 USA.
   [Chang, Shih-Fu] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
C3 Fudan University; Fudan University; International Business Machines
   (IBM); Columbia University; Columbia University
RP Jiang, YG (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
EM ygj@fudan.edu.cn; wangjun@us.ibm.com; xyxue@fudan.edu.cn;
   sfchang@ee.columbia.edu
FU national 973 program [2010CB327900]; National Natural Science Foundation
   of China [61201387, 61228205]; STCSM, China [10511500703, 12XD1400900]
FX Manuscript received November 22, 2011; revised April 23, 2012 and July
   16, 2012; accepted July 22, 2012. Date of publication November 30, 2012;
   date of current version January 15, 2013. This work was supported in
   part by a national 973 program (No. 2010CB327900), two programs from the
   National Natural Science Foundation of China (No. 61201387 and No.
   61228205), and two STCSM's programs (No. 10511500703 and No.
   12XD1400900), China. The associate editor coordinating the review of
   this manuscript and approving it for publication was Shin'ichi Satoh.
CR [Anonymous], 2006, P IEEE C COMP VIS PA
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P S THEOR COMP
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2006, P IEEE COMPUTER SOC
   [Anonymous], ACM INT C IM VID
   [Anonymous], 2009, NUSWIDE: A real-world web image database from National University of Singapore, DOI DOI 10.1145/1646396.1646452
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2003, P IEEE INT C COMP VI
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, P IEEE C COMP VIS PA
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Bronstein M.M., 2010, P IEEE C COMP VIS PA
   Chum O., 2009, P IEEE C COMP VIS PA
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Goldberger J., 2004, ADV NEURAL INF PROCE
   Goodman J. E., 2004, HDB DISCRETE COMPUTA
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Horster E., 2008, P ACM INT C MULT
   Jegou H., 2010, INT J COMPUT VISION, V87, p[191, 2371]
   Jegou Herve, 2008, P INT C AC SPEECH SI
   Jia Y, 2010, PROC CVPR IEEE, P3392, DOI 10.1109/CVPR.2010.5540006
   Jiang Y., 2011, P ACM INT C MULT RET
   Kulis B., 2009, ADV NEURAL INF PROCE
   Lin R.-S., 2010, P IEEE C COMP VIS PA
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mu Y., 2010, P IEEE C COMP VIS PA
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Salakhutdinov R., 2007, P WORKSH ACM SIGIR C
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   SMITH JR, 1996, P ACM INT C MULT
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Weiss Y., 2008, ADV NEURAL INF PROCE, V21, P1
   Xing E.P., Advances in neural information processing systems, 2003, P521
   Xu H., 2011, P IEEE INT C COMP VI
   Yang J., 2008, P ACM INT C IM VID R
   Zobel J, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1132956.1132959
NR 44
TC 59
Z9 66
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 442
EP 453
DI 10.1109/TMM.2012.2231061
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500018
OA Green Published
DA 2024-07-18
ER

PT J
AU Xia, SY
   Shao, M
   Luo, JB
   Fu, Y
AF Xia, Siyu
   Shao, Ming
   Luo, Jiebo
   Fu, Yun
TI Understanding Kin Relationships in a Photo
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context; face recognition; kinship verification; semantics
ID FACE RECOGNITION; CONTEXT
AB There is an urgent need to organize and manage images of people automatically due to the recent explosion of such data on the Web in general and in social media in particular. Beyond face detection and face recognition, which have been extensively studied over the past decade, perhaps the most interesting aspect related to human-centered images is the relationship of people in the image. In this work, we focus on a novel solution to the latter problem, in particular the kin relationships. To this end, we constructed two databases: the first one named UB KinFace Ver2.0, which consists of images of children, their young parents and old parents, and the second one named Family Face. Next, we develop a transfer subspace learning based algorithm in order to reduce the significant differences in the appearance distributions between children and old parents facial images. Moreover, by exploring the semantic relevance of the associated metadata, we propose an algorithm to predict the most likely kin relationships embedded in an image. In addition, human subjects are used in a baseline study on both databases. Experimental results have shown that the proposed algorithms can effectively annotate the kin relationships among people in an image and semantic context can further improve the accuracy.
C1 [Xia, Siyu] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Shao, Ming; Fu, Yun] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
C3 Southeast University - China; University of Rochester; State University
   of New York (SUNY) System; State University of New York (SUNY) Buffalo
RP Xia, SY (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
EM xia081@gmail.com; mingshao@buffalo.edu; jiebo.luo@rochester.edu;
   yunfu@buffalo.edu
RI Luo, Jiebo/AAI-7549-2020; Xia, Siyu/AAB-6524-2020
OI Luo, Jiebo/0000-0002-4516-9729
FU IC Postdoctoral Research Fellowship [2011-11071400006]; AFOSR
   [FA9550-12-1-0201]; Google Faculty Research Awards
FX This work was supported in part by the IC Postdoctoral Research
   Fellowship under Award 2011-11071400006, AFOSR Award FA9550-12-1-0201,
   and by Google Faculty Research Awards. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Qi Tian.
CR Adini Y, 1997, IEEE T PATTERN ANAL, V19, P721, DOI 10.1109/34.598229
   Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   Alvergne A, 2007, EVOL HUM BEHAV, V28, P135, DOI 10.1016/j.evolhumbehav.2006.08.008
   [Anonymous], 2007, P 24 INT C MACHINE L, DOI DOI 10.1145/1273496.1273592
   [Anonymous], 2007, P 13 ACM SIGKDD INT, DOI DOI 10.1145/1281192.1281218
   [Anonymous], IEEE C COMP VIS PATT
   Arnold A., 2007, P 7 IEEE INT C DAT M, P77, DOI [10.1109/ICDMW.2007.109, DOI 10.1109/ICDMW.2007.109]
   Bartlett MS, 2002, IEEE T NEURAL NETWOR, V13, P1450, DOI 10.1109/TNN.2002.804287
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Berg TL, 2004, PROC CVPR IEEE, P848
   CAO ZM, 2010, PROC CVPR IEEE, P2707, DOI DOI 10.1109/CVPR.2010.5539992
   Chen T, 2006, IEEE T PATTERN ANAL, V28, P1519, DOI 10.1109/TPAMI.2006.195
   Dai W., 2007, Proceedings of the National Conference on Artificial Intelligence, V1
   Dai Wenyuan, 2007, P 24 INT C MACHINE L, P193
   Dal Martello MF, 2006, J VISION, V6, P1356, DOI 10.1167/6.12.2
   DeBruine LM, 2009, VISION RES, V49, P38, DOI 10.1016/j.visres.2008.09.025
   Evgeniou A., 2007, ADV NEURAL INFORM PR, V19
   Fang RG, 2010, IEEE IMAGE PROC, P1577, DOI 10.1109/ICIP.2010.5652590
   Gallagher AC, 2009, PROC CVPR IEEE, P256, DOI 10.1109/CVPRW.2009.5206828
   Gallagher AC, 2007, PROC CVPR IEEE, P3680
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197
   He XF, 2005, IEEE I CONF COMP VIS, P1208
   He XF, 2004, ADV NEUR IN, V16, P153
   Huang GB, 2007, IEEE I CONF COMP VIS, P237, DOI 10.1109/iccv.2007.4408858
   Jain AnilK., 2005, Handbook of Face Recognition
   Jia K, 2008, IEEE T IMAGE PROCESS, V17, P873, DOI 10.1109/TIP.2008.922421
   Jiang Jing, 2007, ANN M ASS COMP LING, P264, DOI DOI 10.1145/1273496.1273558
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lawrence N.D., 2004, P INT C MACHINE LEAR, P512
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Mihalkova L., 2007, AAAI, V22, P608
   Naaman M, 2005, ACM-IEEE J CONF DIG, P178, DOI 10.1145/1065385.1065430
   Ng S., 2011, BITS OUR MINDS ITCH
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Ramanathan N., 2006, IEEE COMP SOC C COMP, P387, DOI [DOI 10.1109/CVPR.2006.187, 10.1109/CVPR.2006.187]
   Shao M., 2011, IEEE Conference on Computer Vision and Pattern Recognition Workshop on Biometrics, P65
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Singla P., 2008, IEEE C COMP VIS PATT
   Stone Z, 2010, P IEEE, V98, P1408, DOI 10.1109/JPROC.2010.2044551
   Su Y, 2010, INT CONF ACOUST SPEE, P1270, DOI 10.1109/ICASSP.2010.5495414
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang G, 2010, LECT NOTES COMPUT SC, V6315, P169, DOI 10.1007/978-3-642-15555-0_13
   Wong JJ, 2010, NEURAL COMPUT APPL, V19, P33, DOI 10.1007/s00521-008-0225-z
   Xia S., 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, P2539, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-422
   Yagnik J., 2007, International Workshop on Multimedia Information Retrieval, P285
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang N., 2007, Tech. Rep. 07-49, P7
   Zhang TH, 2008, LECT NOTES COMPUT SC, V5302, P725, DOI 10.1007/978-3-540-88682-2_55
NR 49
TC 167
Z9 187
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1046
EP 1056
DI 10.1109/TMM.2012.2187436
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300010
DA 2024-07-18
ER

PT J
AU Xiu, XY
   Cheung, G
   Liang, J
AF Xiu, Xiaoyu
   Cheung, Gene
   Liang, Jie
TI Delay-Cognizant Interactive Streaming of Multiview Video With Free
   Viewpoint Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Media interaction; multiview video; video streaming; view synthesis
ID FRAME; 3DTV
AB In interactive multiview video streaming (IMVS), a client receives and observes one of many available viewpoints of the same scene and periodically requests from the server view switches to neighboring views, as the video is played back in time uninterruptedly. One key technical challenge is to design a frame coding structure that facilitates periodic view switching and achieves an optimal tradeoff between storage cost and expected transmission rate. In this paper, we first propose three significant improvements over existing IMVS systems and then study the corresponding frame structure optimization. First, using depth-image-based rendering, the new IMVS system enables free viewpoint switching, i.e., by encoding and transmitting both texture and depth maps of captured views, a client can select and synthesize any virtual view from an almost continuum of viewpoints between the left-most and right-most captured views. Second, the IMVS system adopts a more realistic Markovian view-switching model with memory that more accurately captures user behaviors than previous memoryless models [1]. A view-switching model is used in predicting client's future view-switching patterns. Third, assuming that the round-trip-time (RTT) delay during server-client communication is nonnegligible, during an IMVS session, the IMVS system additionally transmits redundant frames RTT into future playback, so that zero-delay view switching can be achieved. Given these improvements, we formalize a new joint optimization of the frame coding structure, transmission schedule, and quantization parameters of the texture and depth maps of multiple camera views. We propose an iterative algorithm to achieve fast and near-optimal solutions. The convergence of the algorithm is also demonstrated. Experimental results show that the proposed optimized rate-allocation method requires 38% lower transmission rate than the fixed rate-allocation scheme. In addition, with the same storage, the transmission rate of the optimized frame structure can be up to 55% lower than that of an I-frame-only structure and 27% lower than that of the structure without distributed source coding frames.
C1 [Xiu, Xiaoyu; Liang, Jie] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
   [Cheung, Gene] Natl Inst Informat, Tokyo 1018430, Japan.
C3 Simon Fraser University; Research Organization of Information & Systems
   (ROIS); National Institute of Informatics (NII) - Japan
RP Xiu, XY (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM xxa4@sfu.ca; cheung@nii.ac.jp; jiel@sfu.ca
RI Cheung, Gene/AAB-9284-2020
OI Cheung, Gene/0000-0002-5571-4137; Liang, Jie/0000-0003-3003-4343
CR Aaron A., 2004, P IEEE INT WORKSH MU
   [Anonymous], 1992, Data networks
   [Anonymous], P SPIE
   Cheung G., 2009, P 17 INT PACK VID WO
   Cheung G., 2008, P IEEE INT WORKSH MU
   Cheung G., 2009, P IEEE INT C IM PROC
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Cheung N., 2007, P PICT COD S LISB PO
   Cheung N.-M., 2009, P 27 PICT COD S CHIC
   Cheung N.-M., 2006, P IS T SPIE VIS COMM
   Fujii T, 2002, PROC SPIE, V4864, P175, DOI 10.1117/12.454905
   Gokturk S., 2004, P C COMP VIS PATT RE
   Jian-Guang Lou, 2005, 13th Annual ACM International Conference on Multimedia, P161
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Mark W., 1997, P S INT 3 D GRAPH NE
   Merkle P., 2007, P IEEE INT C IM PROC
   Merkle P, 2007, IEEE T CIRC SYST VID, V17, P1461, DOI 10.1109/TCSVT.2007.903665
   Shimizu S, 2007, IEEE T CIRC SYST VID, V17, P1485, DOI 10.1109/TCSVT.2007.903773
   Shum H., 2007, IMAGE BASED RENDERIN
   Tanimoto M., 2009, DEPTH ESTIMATION REF
   Tanimoto M., 2008, VIEW SYNTHESIS ALGOR
   TAUBMAN D, 2003, P IEEE INT C IM PROC
   Tekalp AM, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.905878
   Wee S, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA89
   Wong CK, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P90
   Zhang C., 2009, P IEEE INT WORKSH MU
NR 28
TC 35
Z9 38
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1109
EP 1126
DI 10.1109/TMM.2012.2191267
PN 2
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400001
DA 2024-07-18
ER

PT J
AU Yang, LJ
   Hanjalic, A
AF Yang, Linjun
   Hanjalic, Alan
TI Prototype-Based Image Search Reranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; image search reranking; light supervision
AB The existing methods for image search reranking suffer from the unreliability of the assumptions under which the initial text-based image search result is employed in the reranking process. In this paper, we propose a prototype-based reranking method to address this problem in a supervised, but scalable fashion. The typical assumption that the top-images in the text-based search result are equally relevant is relaxed by linking the relevance of the images to their initial rank positions. Then, we employ a number of images from the initial search result as the prototypes that serve to visually represent the query and that are subsequently used to construct meta rerankers. By applying different meta rerankers to an image from the initial result, reranking scores are generated, which are then aggregated using a linear model to produce the final relevance score and the new rank position for an image in the reranked search result. Human supervision is introduced to learn the model weights offline, prior to the online reranking process. While model learning requires manual labeling of the results for a few queries, the resulting model is query independent and therefore applicable to any other query. The experimental results on a representative web image search dataset comprising 353 queries demonstrate that the proposed method outperforms the existing supervised and unsupervised reranking approaches. Moreover, it improves the performance over the text-based image search engine by more than 25.48%.
C1 [Yang, Linjun] Microsoft Res Asia, Media Comp Group, Beijing, Peoples R China.
   [Hanjalic, Alan] Delft Univ Technol, Dept Intelligent Syst, Fac Elect Engn Math & Comp Sci, Delft, Netherlands.
C3 Microsoft; Microsoft Research Asia; Delft University of Technology
RP Yang, LJ (corresponding author), Microsoft Res Asia, Media Comp Group, Beijing, Peoples R China.
EM lin-juny@microsoft.com; a.hanjalic@tudelft.nl
CR [Anonymous], 1999, Advances in Kernel Methods-Support Vector Learning
   [Anonymous], P MMM
   [Anonymous], P KDD
   [Anonymous], P ACM MULT
   [Anonymous], P ECCV
   [Anonymous], P KDD
   [Anonymous], P ACM MULT
   [Anonymous], P SIGIR
   [Anonymous], P CVPR
   [Anonymous], P CVPR
   [Anonymous], P ACM MULT
   [Anonymous], P ACM MULT
   [Anonymous], P CIVR
   [Anonymous], P ICME
   [Anonymous], INT J COMPUT VISION
   [Anonymous], 1999, ADV KERNEL METHODS S
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   [Anonymous], P ICCV IEEE COMP SOC
   [Anonymous], P ACM MULT
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Laskov P, 2006, J MACH LEARN RES, V7, P1909
   Lowe DG, 2004, INT J COMPUT VIS, V60
   Manning Christopher D., 2008, INTRO INFORM RETRIEV
   Schroff F., 2007, P ICCV
   Tian XM, 2010, IEEE T IMAGE PROCESS, V19, P805, DOI 10.1109/TIP.2009.2035866
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 27
TC 13
Z9 16
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 871
EP 882
DI 10.1109/TMM.2012.2187778
PN 2
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700018
DA 2024-07-18
ER

PT J
AU Bao, BK
   Li, T
   Yan, SC
AF Bao, Bing-Kun
   Li, Teng
   Yan, Shuicheng
TI Hidden-Concept Driven Multilabel Image Annotation and Label Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image annotation; label ranking; nonnegative data factorization
AB Conventional semisupervised image annotation algorithms usually propagate labels predominantly via holistic similarities over image representations and do not fully consider the label locality, inter-label similarity, and intra-label diversity among multilabel images. Taking these problems into consideration, we present the hidden-concept driven image annotation and label ranking algorithm (HDIALR), which conducts label propagation based on the similarity over a visually semantically consistent hidden-concepts space. The proposed method has the following characteristics: 1) each holistic image representation is implicitly decomposed into label representations to reveal label locality: the decomposition is guided by the so-called hidden concepts, characterizing image regions and reconstructing both visual and nonvisual labels of the entire image; 2) each label is represented by a linear combination of hidden concepts, while the similar linear coefficients reveal the inter-label similarity; 3) each hidden concept is expressed as a respective subspace, and different expressions of the same label over the subspace then induce the intra-label diversity; and 4) the sparse coding-based graph is proposed to enforce the collective consistency between image labels and image representations, such that it naturally avoids the dilemma of possible inconsistency between the pairwise label similarity and image representation similarity in multilabel scenario. These properties are finally embedded in a regularized nonnegative data factorization formulation, which decomposes images representations into label representations over both labeled and unlabeled data for label propagation and ranking. The objective function is iteratively optimized by a convergence provable updating procedure. Extensive experiments on three benchmark image datasets well validate the effectiveness of our proposed solution to semisupervised multilabel image annotation and label ranking problem.
C1 [Bao, Bing-Kun; Li, Teng] Chinese Acad Sci, Inst Automat, Beijing 100049, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Singapore
RP Bao, BK (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100049, Peoples R China.
EM bingkunbao@gmail.com
RI Yan, Shuicheng/HCI-1431-2022
FU IDM Project Office, Media Development Authority of Singapore
   [NRF2007IDM-IDM002-069]
FX This work was supported in part by the IDM Project Office, Media
   Development Authority of Singapore under Project Grant
   NRF2007IDM-IDM002-069 on "Life Spaces." The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Eckehard G. Steinbach.
CR [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2007, ACM MULTIMEDIA, DOI DOI 10.1145/1291233.1291379
   [Anonymous], P ECCV, DOI DOI 10.1007/11744023_
   [Anonymous], 2009, Proceedings of the 26th Annual International Conference on Machine Learning
   [Anonymous], 2007, ADV NEURAL INFORM PR
   Bao BK, 2011, PATTERN RECOGN, V44, P598, DOI 10.1016/j.patcog.2010.10.001
   Bao BK, 2009, P INT C INT MULT COM, P17
   Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654
   Chen G., 2008, P SIAM INT C DAT MIN, P410, DOI DOI 10.1137/1.9781611972788.37
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Cheng B, 2010, IEEE T IMAGE PROCESS, V19, P858, DOI 10.1109/TIP.2009.2038764
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Har-Peled S., 2002, ALGORITHMIC LEARNING, P267
   Hüllermeier E, 2008, ARTIF INTELL, V172, P1897, DOI 10.1016/j.artint.2008.08.002
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Kuhn H. W., 1951, NONLINEAR PROGRAMMIN
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li T, 2011, IEEE T IMAGE PROCESS, V20, P2301, DOI 10.1109/TIP.2010.2103081
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu XB, 2009, IEEE DATA MINING, P307, DOI 10.1109/ICDM.2009.18
   LIU Y, 2006, P NAT C ART INT INN, V21, P666
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo JB, 2005, PATTERN RECOGN, V38, P919, DOI 10.1016/j.patcog.2004.11.001
   Marr D., 1982, Visual perception
   Nowak S., 2010, P INT C MULTIMEDIA I, P35
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Ueda N., 2002, ADV NEURAL INFORMATI, P721
   Vasconcelos N., 2001, P IEEE C COMP VIS PA, V1
   Wang H, 2009, IEEE I CONF COMP VIS, P2029, DOI 10.1109/ICCV.2009.5459447
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu X, 2004, LECT NOTES ARTIF INT, V3056, P272
   Yao B., 2010, P IEEE COMP VIS PATT
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
   Zhu S., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P274, DOI 10.1145/1076034.1076082
NR 42
TC 18
Z9 23
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 199
EP 210
DI 10.1109/TMM.2011.2170557
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100019
DA 2024-07-18
ER

PT J
AU Piro, G
   Grieco, LA
   Boggia, G
   Fortuna, R
   Camarda, P
AF Piro, Giuseppe
   Grieco, Luigi Alfredo
   Boggia, Gennaro
   Fortuna, Rossella
   Camarda, Pietro
TI Two-Level Downlink Scheduling for Real-Time Multimedia Services in LTE
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cellular networks; downlink scheduling; long-term evolution (LTE);
   quality-of-experience (QoE); quality-of-service (QoS)
AB Long-term evolution represents an emerging technology that promises a broadband and ubiquitous Internet access. But several aspects have to be considered for providing effective multimedia services to mobile users. In particular, in this work, we consider the design of a quality-of-service (QoS) aware packet scheduler for real-time downlink communications. To this aim, a novel two-level scheduling algorithm is conceived. The upper level exploits an innovative approach based on discrete-time linear control theory. Instead, at the lower level, a proportional fair scheduler has been properly tailored to our purposes. The performance and the complexity of the proposed scheme have been evaluated both theoretically and by using simulations. A comparison with recently proposed scheduling strategies has been also presented, considering several network conditions and real-time multimedia flows. Particular attention has been devoted to the evaluation of the quality-of-experience (QoE) provided to end users. Results have clearly shown that the proposed approach is able to greatly outperform the existing ones especially in the presence of real-time video flows.
C1 [Piro, Giuseppe; Grieco, Luigi Alfredo; Boggia, Gennaro; Fortuna, Rossella; Camarda, Pietro] Politecn Bari, DEE Dip Elettrotecn & Elettron, I-70125 Bari, Italy.
C3 Politecnico di Bari
RP Piro, G (corresponding author), Politecn Bari, DEE Dip Elettrotecn & Elettron, I-70125 Bari, Italy.
EM g.piro@poliba.it; a.grieco@poliba.it; g.boggia@poliba.it;
   r.fortuna@poliba.it; camarda@poliba.it
OI Piro, Giuseppe/0000-0003-3783-5565
FU Apulia region, Italy [CIPE 20/04, DM01];  [PS 025]
FX Manuscript received August 11, 2010; revised January 19, 2011 and April
   20, 2011; accepted April 27, 2011. Date of publication May 10, 2011;
   date of current version September 16, 2011. A preliminary version of
   this work appeared as A Two-level Scheduling Algorithm for QoS Support
   in the Downlink of LTE Cellular Networks in Proc. European Wireless,
   EW2010, Lucca, Italy, April 2010. This work was supported in part by
   Apulia region, Italy, A.Q.P. Research Project "Modelli Innovativi per
   Sistemi Meccatronici," Del. CIPE 20/04, DM01 and Strategic Project PS
   025. The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Gene Cheung.
CR 3GPP, R1081483 3GPP TSGRAN
   *3GPP, 25814 3GPP TS
   3GPP, 23203 3GPP TS
   [Anonymous], 3GPP TS 36.211 version 13.0.0 Release 13
   [Anonymous], P INT S WIR PERV COM
   [Anonymous], P IEEE VEH TECHN C V
   [Anonymous], P IEEE 9 MAL INT C C
   [Anonymous], 36101 3GPP TS
   Assaad M., 2008, Vehicular Technology Conference, P1, DOI DOI 10.1109/VETECF.2008.381
   Astrom K. J., 1995, COMPUTER CONTROLLED
   BOGGIA G, 2007, P 2 WORKSH MULTIMEDI
   Boggia G, 2007, IEEE ACM T NETWORK, V15, P323, DOI 10.1109/TNET.2007.892881
   Bokhari FA, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/212783
   Camp T, 2002, WIREL COMMUN MOB COM, V2, P483, DOI 10.1002/wcm.72
   Dahlman E., 2008, 3G Evolution: HSPA and LTE for Mobile Broadband, V2nd
   Ekström H, 2009, IEEE COMMUN MAG, V47, P76, DOI 10.1109/MCOM.2009.4785383
   Halonen T., 2003, GSM, GPRS And EDGE Performance
   *ITU, 1996, G109 ITUT
   Khirman S., 2002, P PASS ACT MEAS PAM
   Kwan R, 2009, IEEE SIGNAL PROC LET, V16, P461, DOI 10.1109/LSP.2009.2016449
   Lin Yan, 2008, P INT C WIR COMM NET
   Luo HY, 2010, IEEE COMMUN MAG, V48, P102, DOI 10.1109/MCOM.2010.5402671
   MARDIAK M, 2010, P 53 INT S ELMAR SEP
   Mcqueen Darren, 2009, IEEE Communications Magazine, V47, P44, DOI 10.1109/MCOM.2009.4785379
   Monghal G., 2008, P IEEE VEH TECHN C V
   Na S, 2002, LECT NOTES COMPUT SC, V2402, P47
   Nuaymi L., 2008, WIMAX TECHNOLOGY BRO
   PIRO G, IEEE T VEH IN PRESS
   Qian Y., 2009, P INT C WIR COMM SIG, P1
   SADIQ B, IEEE ACM T IN PRESS
   Sadiq B, 2009, EURASIP J WIREL COMM, DOI 10.1155/2009/510617
   Sesia S., 2009, UMTS LONG TERM EVOLU
   SHAKKOTTAI S, 2001, P 17 INT TEL C ITC 0
   Su GM, 2007, IEEE J-STSP, V1, P280, DOI 10.1109/JSTSP.2007.901518
   Wang X, 2007, P IEEE, V95, P2410, DOI 10.1109/JPROC.2007.907120
   ZHE Z, 2005, P 1 INT WORKSH VID P
   Zheng YR, 2003, IEEE T COMMUN, V51, P920, DOI 10.1109/TCOMM.2003.813259
NR 37
TC 170
Z9 197
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1052
EP 1065
DI 10.1109/TMM.2011.2152381
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300018
DA 2024-07-18
ER

PT J
AU Ponec, M
   Sengupta, S
   Chen, MH
   Li, J
   Chou, PA
AF Ponec, Miroslav
   Sengupta, Sudipta
   Chen, Minghua
   Li, Jin
   Chou, Philip A.
TI Optimizing Multi-Rate Peer-to-Peer Video Conferencing Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multimedia traffic management; quality-of-service; videoconferencing and
   collaboration environment
ID TCP VEGAS; NETWORK; MULTICAST
AB We consider multi-rate peer-to-peer multiparty video conferencing applications, where different receivers in the same group can receive videos at different rates using, for example, scalable layered coding. The quality of video received by each receiver can be modeled as a concave utility function of the video bitrate. We study and address the unique challenges introduced by maximizing utility in the multi-rate setting as compared to the single-rate case. We first determine an optimal set of tree structures for routing multi-rate content using scalable layered coding. We then develop Primal and Primal-dual based distributed algorithms to maximize aggregate utility of all receivers in all groups by multi-tree routing and show their convergence. These algorithms can be easily implemented and deployed on today's Internet. We have built a prototype video conferencing system to show that this approach converges to optimal bitrates to improve user experience and offers automatic adaptation to network conditions and user preferences.
C1 [Ponec, Miroslav] Akamai Technol GmbH, Media & CDN Engn Dept, Unterfoehring, Germany.
   [Sengupta, Sudipta; Li, Jin; Chou, Philip A.] Microsoft Res, Redmond, WA 98052 USA.
   [Chen, Minghua] Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
C3 Microsoft; Chinese University of Hong Kong
RP Ponec, M (corresponding author), Akamai Technol GmbH, Media & CDN Engn Dept, Betastr 10B, Unterfoehring, Germany.
EM mponec@akamai.com; sudipta@microsoft.com; minghua@ie.cuhk.edu.hk;
   jinl@microsoft.com; pa-chou@microsoft.com
RI Chen, Minghua/A-7476-2012
OI Chen, Minghua/0000-0003-4763-0037
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Akkus IE, 2011, J NETW COMPUT APPL, V34, P137, DOI 10.1016/j.jnca.2010.08.006
   Alexandros E., 2006, Journal of Zhejiang University (Science), V7, P696, DOI 10.1631/jzus.2006.A0696
   [Anonymous], MOSEK OPT SOFTW
   [Anonymous], IEEE ACM T NETW
   [Anonymous], P IEEE WORKSH INF AS
   [Anonymous], LOW EXTRA DELAY BACK
   [Anonymous], P ACM SIGCOMM AS WOR
   BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716
   CHEN L, 2007, P IEEE INFOCOM ANCH
   CHEN M, 2006, THESIS U CALIFORNIA
   Chen M., 2008, P ACM SIGMETRICS JUN
   Chen X, 2011, P 21 INT WORKSH NETW
   CHIU DM, 2006, P IEEE 2 WORKSH NETW, P2
   DEB S, 2004, IEEE T AUTOMAT CONTR, V49, P274
   Floyd S., 1994, Computer Communication Review, V24, P8, DOI 10.1145/205511.205512
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   KAR K, 2001, P IEEE INFOCOM ANCH
   Kar K, 2006, IEEE J SEL AREA COMM, V24, P1464, DOI 10.1109/JSAC.2006.879353
   Kelly FP, 1998, J OPER RES SOC, V49, P237, DOI 10.2307/3010473
   La R., 1998, ISSUES TCP VEGAS
   LIANG C, 2002, IEEE ACM T IN PRESS
   LIU S, 2005, IEEE ACM T NETW, V5
   Low SH, 2002, J ACM, V49, P207, DOI 10.1145/506147.506152
   Lun DS, 2006, IEEE T INFORM THEORY, V52, P2608, DOI 10.1109/TIT.2006.874523
   Norberg A., UTORRENT TRANSPORT P
   Ponec M., 2009, P 2009 IEEE INT C MU
   PURI R, 1999, P IEEE AS C SIGN SYS
   Schwarz H., 2006, P IEEE INT C IM PROC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Voice T., 2006, THESIS U CAMBRIDGE C
   WELZL M, 2011, SURVEY LOWER THAN BE
   Wu YN, 2006, IEEE J SEL AREA COMM, V24, P1475, DOI 10.1109/JSAC.2006.879356
   Yan X., 2007, P 2007 IEEE INT S IN
NR 35
TC 8
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 856
EP 868
DI 10.1109/TMM.2011.2161759
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300003
DA 2024-07-18
ER

PT J
AU Tang, SY
   Blenn, N
   Doerr, C
   Van Mieghem, P
AF Tang, Siyu
   Blenn, Norbert
   Doerr, Christian
   Van Mieghem, Piet
TI Digging in the Digg Social News Website
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content dissemination; friendship relations; social media website; user
   characteristics
AB The rise of social media aggregating websites provides platforms where users can actively publish, evaluate, and disseminate content in a collaborative way. In this paper, we present a large-scale empirical study about "Digg.com", one of the biggest social media aggregating websites. Our analysis is based on crawls of 1.5 million users and 10 million published stories on Digg. We study the distinct network structure, the collaborative user characteristics, and the content dissemination process on Digg. We empirically illustrate that friendship relations are used effectively in disseminating half of the content, although there exists a high overlap between the interests of friends. A successful content dissemination process can also be performed by random users who are browsing and digging stories. Since 88% of the published content on Digg is defined as news, it is important for the content to obtain sufficient votes in a short period of time before becoming obsolete. Finally, we show that the synchronization of users' activities in time is the key to a successful content dissemination process. The dynamics between users' voting activities consequently decrease the efficiency of friendship relations during content dissemination. The results presented in this paper define basic observations and measurements to understand the underlying mechanism of disseminating content in current online social news aggregators. These findings are helpful to understand the influence of service interfaces and user behaviors on content dissemination.
C1 [Tang, Siyu; Blenn, Norbert; Doerr, Christian; Van Mieghem, Piet] Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
C3 Delft University of Technology
RP Tang, SY (corresponding author), Delft Univ Technol, Fac Elect Engn Math & Comp Sci, NL-2600 GA Delft, Netherlands.
EM S.Tang@tudelft.nl; N.Blenn@tudelft.nl; C.Doerr@tudelft.nl;
   P.F.A.VanMieghem@tudelft.nl
FU TRANS
FX The work was supported in part by TRANS (http://www.trans-research.nl).
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Daniel Gatica-Perez.
CR AHN YY, 2007, P 16 INT C WORLD WID, P844
   Amaral LAN, 2000, P NATL ACAD SCI USA, V97, P11149, DOI 10.1073/pnas.200327197
   [Anonymous], 2003, Diffusion of Innovations
   [Anonymous], P 19 INT WORLD WID W
   Backstrom L., 2006, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, P54
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Benevenuto F, 2009, IMC'09: PROCEEDINGS OF THE 2009 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P49
   Boyd DM, 2007, J COMPUT-MEDIAT COMM, V13, P210, DOI 10.1111/j.1083-6101.2007.00393.x
   Cha M, 2009, WWW 09 P 18 INT WORL, DOI DOI 10.1145/1526709.1526806
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   DREZNER DW, 2004, AM POLIT SCI ASS, P2
   Ebel H, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.035103
   Fono D., 2006, Internet Research Annual, V4
   Garlaschelli D, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.268701
   Huberman BA, 1999, NATURE, V401, P131, DOI 10.1038/43604
   Jeong H, 2001, NATURE, V411, P41, DOI 10.1038/35075138
   Juran JM., 1974, Quality Control Handbook
   Katz Elihue Paul Lazarsfeld., 1955, Personal Influence, Nova Iorque
   Kossinets G., 2008, P 14 ACM SIGKDD INT, P435, DOI DOI 10.1145/1401890.1401945
   Lerman K., 2010, INFORM CONTAGION EMP
   LERMAN K, 2007, P 1 INT C WEBL SOC M
   Leskovec J., 2008, P 17 INT C WORLD WID, DOI 10.1145/1367497.1367620
   Liben-Nowell D, 2005, P NATL ACAD SCI USA, V102, P11623, DOI 10.1073/pnas.0503018102
   Lorenz MO, 1905, PUBL AM STAT ASSOC, V9, P209, DOI 10.2307/2276207
   MISLOVE A, 2007, P 7 ACM SIGCOMM C IN, P42
   Mislove A., 2008, P 1 WORKSH ONL SOC N, P25, DOI 10.1145/1397735.1397742
   Mitzenmacher M., 2004, Internet Math., V1, P226, DOI DOI 10.1080/15427951.2004.10129088
   Montgomery AL, 2001, INTERFACES, V31, P90, DOI 10.1287/inte.31.2.90.10630
   Mossa S, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.138701
   Newman MEJ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208701
   Porter J., 2008, Designing for the Social Web
   Raynes-Goldie K., 2004, 1 MONDAY, V9
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Van Mieghem P, 2010, EUR PHYS J B, V76, P643, DOI 10.1140/epjb/e2010-00219-x
   Van Mieghem P., 2006, Performance analysis of communications networks and systems
   VANMIEGHEM P, EUR J PHY B IN PRESS
   WILKINSON DM, 2007, P 2007 INT S WIK, P164
   WILLIAMSON BA, 2007, EMARKETER SOCIAL NET
   Wu F, 2007, P NATL ACAD SCI USA, V104, P17599, DOI 10.1073/pnas.0704916104
NR 39
TC 21
Z9 23
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 1163
EP 1175
DI 10.1109/TMM.2011.2159706
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300027
DA 2024-07-18
ER

PT J
AU Shen, YB
   Hsu, CH
   Hefeeda, M
AF Shen, Yuanbin
   Hsu, Cheng-Hsin
   Hefeeda, Mohamed
TI Efficient Algorithms for Multi-Sender Data Transmission in Swarm-Based
   Peer-to-Peer Streaming Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Peer-to-peer networks; quality optimization; transmission scheduling
ID THROUGHPUT
AB In mesh-based peer-to-peer (P2P) streaming systems, each video sequence is divided into segments, which are then streamed from multiple senders to a receiver. The receiver needs to coordinate the senders by specifying a transmission schedule for each of them. We consider the problem of scheduling segment transmission in P2P streaming systems, where different segments have different weights in terms of quality improvements to the received video. Our goal is to compute the transmission schedule for each receiver in order to maximize the perceived video quality. We first show that this scheduling problem is NP-Complete. We then present an integer linear programming (ILP) formulation for it, so that it can be solved with any ILP solver. This optimal solution, however, is computationally expensive and is not suitable for real-time P2P streaming systems. Thus, we propose two approximation algorithms to solve this segment scheduling problem. These algorithms provide theoretical guarantees on the worst-case performance. The first algorithm considers the weight of each video segment. The second algorithm is simpler and it assumes that segments carry equal weights. We analyze the performance and complexity of the two algorithms. In addition, we rigorously evaluate the proposed algorithms with simulations and experiments using a prototype implementation. Our simulation and experimental results show that the proposed algorithms outperform other algorithms that are commonly used in deployed P2P streaming systems and that have been recently proposed in the literature.
C1 [Shen, Yuanbin; Hefeeda, Mohamed] Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
   [Hsu, Cheng-Hsin] Deutsch Telekom R&D Lab USA, Los Altos, CA 94022 USA.
C3 Simon Fraser University; Deutsche Telekom AG
RP Shen, YB (corresponding author), Simon Fraser Univ, Sch Comp Sci, Surrey, BC V3T 0A3, Canada.
EM ysa57@cs.sfu.ca; cheng-hsin.hsu@telekom.com; mhefeeda@cs.sfu.ca
FU Natural Sciences and Engineering Research Council (NSERC) of Canada;
   British Columbia Innovation Council
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council (NSERC) of Canada and in part by the British Columbia
   Innovation Council. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Thinh Nguyen.
CR Agarwal V, 2005, P SOC PHOTO-OPT INS, V5680, P13, DOI 10.1117/12.587465
   Annapureddy S., 2007, Proc. Int'l WWW Conference, P903
   Bar-Noy A, 2001, SIAM J COMPUT, V31, P331, DOI 10.1137/S0097539799354138
   Cai Y, 2007, IEEE J SEL AREA COMM, V25, P140, DOI 10.1109/JSAC.2007.070114
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Chakareski J., 2009, P SPIE INT C VIS COM
   Choffnes DR, 2008, ACM SIGCOMM COMP COM, V38, P363, DOI 10.1145/1402946.1403000
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   Goemans MX, 1997, PROCEEDINGS OF THE EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P591
   Gra K., 2008, Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P99, DOI DOI 10.1145/1496046.1496069
   HEI X, 2007, IEEE T MULTIMEDIA, V9, P405
   HSU C, 2008, P ACM INT WORKSH REA
   Hsu C., 2010, P 1 ANN ACM SIGMM C, P169
   Kellerer H., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P418, DOI 10.1145/237814.237989
   Kowalski G, 2009, 2009 11TH IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM 2009), P243, DOI 10.1109/ISM.2009.55
   Li JM, 2009, J NETW COMPUT APPL, V32, P901, DOI 10.1016/j.jnca.2009.01.001
   Liang C, 2008, INT CON DISTR COMP S, P53, DOI 10.1109/ICDCS.2008.103
   Liu B, 2008, IEEE INT SYM MULTIM, P242
   Liu JC, 2008, P IEEE, V96, P11, DOI 10.1109/JPROC.2007.909921
   Liu ZY, 2008, I C NETWORK PROTOCOL, P94, DOI 10.1109/ICNP.2008.4697028
   M'Hallah R, 2005, EUR J OPER RES, V160, P471, DOI 10.1016/j.ejor.2003.06.027
   Magharei N, 2007, IEEE INFOCOM SER, P1424
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Pai V, 2005, LECT NOTES COMPUT SC, V3640, P127
   Pinedo M., 2012, Scheduling: Theory, algorithms, and systems, V29
   Rodriguez P, 2006, ACM SIGCOMM COMP COM, V36, P75, DOI 10.1145/1111322.1111339
   SHEN Y, 2010, THESIS S FRASER U SU
   Shojania H, 2009, IEEE INFOCOM SER, P459, DOI 10.1109/INFCOM.2009.5061951
   TU YC, 2005, ACM T MULTIM COMPUT, V1, pNIL40
   Wang Y., 2002, VIDEO PROCESSING COM
   Xu DY, 2006, MULTIMEDIA SYST, V11, P383, DOI 10.1007/s00530-006-0015-3
   Ye Y., 1997, INTERIOR POINT ALGOR
   Zhang M, 2007, IEEE J SEL AREA COMM, V25, P1678, DOI 10.1109/JSAC.2007.071207
   Zhang M, 2009, IEEE T PARALL DISTR, V20, P97, DOI 10.1109/TPDS.2008.69
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   Zhang XY, 2004, IEEE J SEL AREA COMM, V22, P18, DOI 10.1109/JSAC.2003.818780
NR 36
TC 10
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 762
EP 775
DI 10.1109/TMM.2011.2108644
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300015
OA Green Published
DA 2024-07-18
ER

PT J
AU Chen, YM
   Bajic, IV
   Saeedi, P
AF Chen, Yue-Meng
   Bajic, Ivan V.
   Saeedi, Parvaneh
TI Moving Region Segmentation From Compressed Video Using Global Motion
   Estimation and Markov Random Fields
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion segmentation; global motion estimation; global motion
   compensation; Markov Random Field; compressed video
ID OBJECT SEGMENTATION; IMAGES
AB In this paper, we propose an unsupervised segmentation algorithm for extracting moving regions from compressed video using global motion estimation (GME) and Markov random field (MRF) classification. First, motion vectors (MVs) are compensated from global motion and quantized into several representative classes, from which MRF priors are estimated. Then, a coarse segmentation map of the MV field is obtained using a maximum a posteriori estimate of the MRF label process. Finally, the boundaries of segmented moving regions are refined using color and edge information. The algorithm has been validated on a number of test sequences, and experimental results are provided to demonstrate its advantages over state-of-the-art methods.
C1 [Chen, Yue-Meng; Bajic, Ivan V.; Saeedi, Parvaneh] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Chen, YM (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM yuemengc@sfu.ca; ibajic@ensc.sfu.ca; psaeedi@sfu.ca
RI Bajic, Ivan/I-1241-2013
OI Bajic, Ivan/0000-0003-3154-5743
FU NSERC/CCA New Media Initiative [STPGP 350740]
FX This work was supported in part by the NSERC/CCA New Media Initiative
   Grant STPGP 350740. The associate editor coordinating review of this
   manuscript and approving it for publication was Dr. Chia-Wen Lin.
CR Babu RV, 2004, IEEE T CIRC SYST VID, V14, P462, DOI 10.1109/TCSVT.2004.825536
   BESAG J, 1986, J R STAT SOC B, V48, P259
   BROUARD O, 2008, P IEEE ICIP 08 DEC, P1552
   Chen Y, 2009, P I CIVIL ENG-STR B, V162, P45, DOI 10.1680/stbu.2009.162.1.45
   CHEN YM, 2010, P IEEE ICME SING JUL, P760
   DANTE A, 2003, P ICIP, P393
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Ewerth R., 2007, P ICIAP SEP, P92
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Haller M, 2009, 2009 10TH INTERNATIONAL WORKSHOP ON IMAGE ANALYSIS FOR MULTIMEDIA INTERACTIVE SERVICES, P49, DOI 10.1109/WIAMIS.2009.5031429
   Kato Z, 2001, PATTERN RECOGN LETT, V22, P309, DOI 10.1016/S0167-8655(00)00106-9
   Kato Z, 2008, IMAGE VISION COMPUT, V26, P361, DOI 10.1016/j.imavis.2006.12.004
   RITCH M, 2007, P IEEE INT C IM PROC, V6, P301
   Shi XL, 2007, OPT ENG, V46, DOI 10.1117/1.2784773
   Smolic A, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P271, DOI 10.1109/ICIP.2000.899352
   Su YB, 2005, IEEE T CIRC SYST VID, V15, P232, DOI 10.1109/TCSVT.2004.841656
   Vasconcelos N, 2001, IEEE T PATTERN ANAL, V23, P217, DOI 10.1109/34.908972
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   Zeng W, 2005, REAL-TIME IMAGING, V11, P290, DOI 10.1016/j.rti.2005.04.008
   Zhong D, 1999, IEEE T CIRC SYST VID, V9, P1259, DOI 10.1109/76.809160
NR 20
TC 32
Z9 39
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 421
EP 431
DI 10.1109/TMM.2011.2127464
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700002
DA 2024-07-18
ER

PT J
AU Ling, CH
   Lin, CW
   Su, CW
   Chen, YS
   Liao, HYM
AF Ling, Chih-Hung
   Lin, Chia-Wen
   Su, Chih-Wen
   Chen, Yong-Sheng
   Liao, Hong-Yuan Mark
TI Virtual Contour Guided Video Object Inpainting Using Posture Mapping and
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object completion; posture mapping; posture sequence retrieval;
   synthetic posture; video inpainting
ID IMAGE; COMPLETION; CAMERA
AB This paper presents a novel framework for object completion in a video. To complete an occluded object, our method first samples a 3-D volume of the video into directional spatio-temporal slices, and performs patch-based image in-painting to complete the partially damaged object trajectories in the 2-D slices. The completed slices are then combined to obtain a sequence of virtual contours of the damaged object. Next, a posture sequence retrieval technique is applied to the virtual contours to retrieve the most similar sequence of object postures in the available non-occluded postures. Key-posture selection and indexing are used to reduce the complexity of posture sequence retrieval. We also propose a synthetic posture generation scheme that enriches the collection of postures so as to reduce the effect of insufficient postures. Our experiment results demonstrate that the proposed method can maintain the spatial consistency and temporal motion continuity of an object simultaneously.
C1 [Ling, Chih-Hung; Chen, Yong-Sheng; Liao, Hong-Yuan Mark] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
   [Su, Chih-Wen; Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
C3 National Yang Ming Chiao Tung University; National Tsing Hua University;
   Academia Sinica - Taiwan
RP Ling, CH (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu, Taiwan.
RI Lin, Chia-Wen/ABH-6075-2020; chen, yu-cheng/IQT-1648-2023; Chen,
   Yu/Y-3292-2019; Liao, Hong-Yuan Mark/AAQ-5514-2021; Chen,
   Yi/JBR-7728-2023; Chen, Yi/HIR-2608-2022; Lin, Chia-Wen/M-4571-2013
OI Lin, Chia-Wen/0000-0002-9097-2318; Chen, Yong-Sheng/0000-0002-5581-850X
FU National Science Council (NSC), Taiwan, R.O.C. [NSC99-2631-H-001-020]
FX This work was supported in part by the Research of Techniques and Tools
   for Digital Archives program sponsored by the National Science Council
   (NSC), Taiwan, R.O.C., under grant NSC99-2631-H-001-020. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Nadia Magnenat-Thalmann.
CR [Anonymous], P ACM C MULT AUGSB G
   [Anonymous], NTHU VID INP PROJ
   [Anonymous], PROTECTING PRIVACY V
   [Anonymous], IEEE T PATTERN ANAL
   Bai X, 2007, IEEE T PATTERN ANAL, V29, P449, DOI 10.1109/TPAMI.2007.59
   Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Bertalmío M, 2001, PROC CVPR IEEE, P355
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Cheung SCS, 2006, IEEE IMAGE PROC, P705, DOI 10.1109/ICIP.2006.312432
   Cheung V, 2005, PROC CVPR IEEE, P42
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Corman T.H., 2001, INTRO ALGORITHMS, V2nd
   Cremers D, 2006, INT J COMPUT VISION, V69, P335, DOI 10.1007/s11263-006-7533-5
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Ding T., 2007, PROC IEEE C COMPUT V, P1
   Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383
   Elgammal A, 2004, PROC CVPR IEEE, P681
   Haritaoglu I., 1998, P 3 FACE GESTURE REC, P222
   Jia JY, 2006, IEEE T PATTERN ANAL, V28, P832, DOI 10.1109/TPAMI.2006.108
   Jia YT, 2005, VISUAL COMPUT, V21, P601, DOI 10.1007/s00371-005-0313-3
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li Y, 2005, ACM T GRAPHIC, V24, P595, DOI 10.1145/1073204.1073234
   Liang YM, 2009, IEEE T SYST MAN CY B, V39, P268, DOI 10.1109/TSMCB.2008.2005643
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Patwardhan KA, 2007, IEEE T IMAGE PROCESS, V16, P545, DOI 10.1109/TIP.2006.888343
   Shen YP, 2006, INT C PATT RECOG, P63
   Shih TK, 2009, IEEE T CIRC SYST VID, V19, P347, DOI 10.1109/TCSVT.2009.2013519
   Sun J, 2005, ACM T GRAPHIC, V24, P861, DOI 10.1145/1073204.1073274
   Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009
   Yatziv L, 2006, IEEE T IMAGE PROCESS, V15, P1120, DOI 10.1109/TIP.2005.864231
NR 30
TC 23
Z9 27
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 292
EP 302
DI 10.1109/TMM.2010.2095000
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800011
DA 2024-07-18
ER

PT J
AU Wang, M
   Sheng, YL
   Liu, B
   Hua, XS
AF Wang, Meng
   Sheng, Yelong
   Liu, Bo
   Hua, Xian-Sheng
TI In-Image Accessibility Indication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Accessibility indication; colorblindness; poster image
AB There are about 8% of men and 0.8% of women suffering from colorblindness. Due to the loss of certain color information, regions or objects in several images cannot be recognized by these viewers and this may degrade their perception and understanding of the images. This paper introduces an in-image accessibility indication scheme, which aims to automatically point out regions in which the content can hardly be recognized by colorblind viewers in a manually designed image. The proposed method first establishes a set of points around which the patches are not prominent enough for colorblind viewers due to the loss of color information. The inaccessible regions are then detected based on these points via a regularization framework. This scheme can be applied to check the accessibility of designed images, and consequently it can be used to help designers improve the images, such as modifying the colors of several objects or components. To our best knowledge, this is the first work that attempts to detect regions with accessibility problems in images for colorblindness. Experiments are conducted on 1994 poster images and empirical results have demonstrated the effectiveness of our approach.
C1 [Wang, Meng; Hua, Xian-Sheng] Microsoft Res Asia, Beijing 100096, Peoples R China.
   [Sheng, Yelong] Beihang Univ, Beijing 100191, Peoples R China.
   [Liu, Bo] Univ Sci & Technol China, Hefei 230027, Peoples R China.
C3 Microsoft Research Asia; Microsoft; Beihang University; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Wang, M (corresponding author), Microsoft Res Asia, Beijing 100096, Peoples R China.
EM mengwang@microsoft.com; shengyelongking@cse.buaa.edu.cn;
   kfliubo@mail.ustc.edu.cn; xshua@microsoft.com
CR *ACC RES, ACC RES AD
   Brettel H, 1997, J OPT SOC AM A, V14, P2647, DOI 10.1364/JOSAA.14.002647
   HONG R, 2009, P INT C MULT EXP, P1663
   Huang HB, 2007, IEEE SIGNAL PROC LET, V14, P711, DOI 10.1109/LSP.2007.898333
   Jefferson L, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1535
   Laccarino G., 2006, WWW, P919
   Liu B, 2009, IEEE INT CON MULTI, P906, DOI 10.1109/ICME.2009.5202642
   Marr D., 1982, Vision
   Moreno L, 2008, IEEE MULTIMEDIA, V15, P52, DOI 10.1109/MMUL.2008.85
   Nam J, 2005, IEEE T MULTIMEDIA, V7, P435, DOI 10.1109/TMM.2005.846801
   Rasche K, 2005, IEEE COMPUT GRAPH, V25, P22, DOI 10.1109/MCG.2005.54
   RASCHE K, 2005, P EUR, P423
   Wang M, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE OF NATURAL PRODUCT AND TRADITIONAL MEDICINE, VOLS 1 AND 2, P291, DOI 10.1145/1631272.1631314
   Xu R, 2005, IEEE T NEURAL NETWOR, V16, P645, DOI 10.1109/TNN.2005.845141
   Yang S, 2003, IEEE IMAGE PROC, P453
   Yang S, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246014
NR 16
TC 8
Z9 10
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2010
VL 12
IS 4
BP 330
EP 336
DI 10.1109/TMM.2010.2046364
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 596ER
UT WOS:000277668100009
DA 2024-07-18
ER

PT J
AU Luan, TH
   Cai, LX
   Shen, X
AF Luan, Tom H.
   Cai, Lin X.
   Shen, Xuemin (Sherman)
TI Impact of Network Dynamics on User's Video Quality: Analytical Framework
   and QoS Provision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Diffusion approximation; playout buffer; quality-driven
ID WIRELESS; DELAY; TRANSMISSION; COMMUNICATION; DELIVERY; CHANNELS;
   STREAMS; PLAYOUT; JITTER
AB We develop an analytical framework to investigate the impacts of network dynamics on the user perceived video quality. Our investigation stands from the end user's perspective by analyzing the receiver playout buffer. In specific, we model the playback buffer at the receiver by a G/G/1/infinity and G/G/1/N queue, respectively, with arbitrary patterns of packet arrival and playback. We then examine the transient queue length of the buffer using the diffusion approximation. We obtain the closed-form expressions of the video quality in terms of the start-up delay, fluency of video playback and packet loss, and represent them by the network statistics, i.e., the average network throughput and delay jitter. Based on the analytical framework, we propose adaptive playout buffer management schemes to optimally manage the threshold of video playback towards the maximal user utility, according to different quality-of-service requirements of end users. The proposed framework is validated by extensive simulations.
C1 [Luan, Tom H.; Cai, Lin X.; Shen, Xuemin (Sherman)] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
C3 University of Waterloo
RP Luan, TH (corresponding author), Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
EM hluan@bbcr.uwaterloo.ca; lcai@bbcr.uwaterloo.ca; xshen@bbcr.uwaterloo.ca
RI Luan, Tom/ABA-2407-2021; Luan, Tom/I-4399-2017; Luan, Tom
   H./HLG-0711-2023; Shen, Xuemin/AAH-2564-2020; Cai, Lin/ABF-4168-2022
OI Luan, Tom H./0000-0002-5215-7443; Shen, Xuemin/0000-0002-4140-287X; Cai,
   Lin/0000-0001-8509-0452
FU Natural Sciences and Engineering Council (NSERC) of Canada; Research In
   Motion (RIM)
FX This work was supported jointly by the Natural Sciences and Engineering
   Council (NSERC) of Canada under Strategic Grant and Research In Motion
   (RIM). The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Aggelos K. Katsaggelos.
CR Abramowitz M., 1964, HDB MATH FUNCTIONS, Vfifth
   [Anonymous], 1977, THEORY STOCHASTIC PR
   Berry RA, 2002, IEEE T INFORM THEORY, V48, P1135, DOI 10.1109/18.995554
   Birge J. R., 1997, Introduction to Stochastic Programming
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Czachorski F. P. T., 2004, P HET NET
   DUA A, 2007, P IEEE GLOBECOM
   Eisenberg Y, 2002, IEEE T CIRC SYST VID, V12, P411, DOI 10.1109/TCSVT.2002.800309
   Fitzek FHP, 2001, IEEE NETWORK, V15, P40, DOI 10.1109/65.967596
   Galluccio L, 2005, IEEE T WIREL COMMUN, V4, P2777, DOI 10.1109/TWC.2005.858028
   GELENBE E, 1975, J ACM, V22, P261, DOI 10.1145/321879.321888
   Girod B, 2002, WIREL COMMUN MOB COM, V2, P573, DOI 10.1002/wcm.87
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kleinrock L, 1976, Queueing Systems, V2
   Kumwilaisak W, 2003, IEEE J SEL AREA COMM, V21, P1685, DOI 10.1109/JSAC.2003.816445
   Laoutaris N, 2004, PERFORM EVALUATION, V55, P251, DOI 10.1016/j.peva.2003.07.004
   Laoutaris N, 2002, IEEE NETWORK, V16, P30, DOI 10.1109/MNET.2002.1002997
   LIANG G, 2007, P IEEE INF
   Liang GF, 2008, IEEE T MULTIMEDIA, V10, P1128, DOI 10.1109/TMM.2008.2001364
   Liu JC, 2004, IEEE T MULTIMEDIA, V6, P87, DOI 10.1109/TMM.2003.819753
   Louchard G., 1983, PROBABILITY THEORY C
   Mao SW, 2006, IEEE T MULTIMEDIA, V8, P1063, DOI 10.1109/TMM.2006.879845
   Tong XL, 2007, IEEE T MOBILE COMPUT, V6, P1343, DOI 10.1109/TMC.2007.1063
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Wu DP, 2001, P IEEE, V89, P6, DOI 10.1109/5.904503
   Xu J, 2007, IEEE T WIREL COMMUN, V6, P2305, DOI 10.1109/TWC.2007.05838
   Zhang Q, 2005, P IEEE, V93, P123, DOI 10.1109/JPROC.2004.839603
   [No title captured]
NR 29
TC 91
Z9 98
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 64
EP 78
DI 10.1109/TMM.2009.2036294
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800006
OA Green Published
DA 2024-07-18
ER

PT J
AU Haghani, E
   Parekh, S
   Calin, D
   Kim, E
   Ansari, N
AF Haghani, Ehsan
   Parekh, Shyam
   Calin, Doru
   Kim, Eunyoung
   Ansari, Nirwan
TI A Quality-Driven Cross-Layer Solution for MPEG Video Streaming Over
   WiMAX Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross layer; MPEG; quality-of-service; video; wireless; WiMAX
AB Extensive efforts have been focused on deploying broadband wireless networks. Providing mobile users with high speed network connectivity will let them run various multimedia applications on their wireless devices. Satisfying users with different quality-of-service requirements while optimizing resource allocation is a challenging problem. In this paper, we discuss the challenges and possible solutions for transmitting MPEG video streams over WiMAX networks. We will briefly describe the MPEG traffic model suggested by the WiMAX Forum. A cross-layer solution for enhancing the performance of WiMAX networks with respect to MPEG video streaming applications is explained. Our solution uses the characteristics of MPEG traffic to give priority to the more important frames and protect them against dropping. Besides, it is simple and compatible with the IEEE 802.16 standards and thus easily deployable. It is shown that the proposed solutions will improve the video quality over WiMAX networks.
C1 [Haghani, Ehsan; Ansari, Nirwan] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
   [Parekh, Shyam; Calin, Doru; Kim, Eunyoung] Bell Labs, Murray Hill, NJ 07974 USA.
C3 New Jersey Institute of Technology; AT&T
RP Haghani, E (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM Ehsan@njit.edu; Sparekh@alcatel-lucent.com; Calin@alcatel-lucent.com;
   Ekim@alcatel-lucent.com; Nirwan.Ansari@njit.edu
RI Ansari, Nirwan/N-1264-2019
OI Ansari, Nirwan/0000-0001-8541-3565
CR [Anonymous], ACM MOBILE COMPUTING
   Ansari N, 2002, IEEE T BROADCAST, V48, P337, DOI 10.1109/TBC.2002.806794
   BALACHANDRAN K, 2008, P IEEE SARN S APR, P1
   Girod B, 2002, WIREL COMMUN MOB COM, V2, P573, DOI 10.1002/wcm.87
   Hassan M, 2007, IEEE T CIRC SYST VID, V17, P1017, DOI 10.1109/TCSVT.2007.903128
   HEMY M, 1999, P IEEE PACK VID
   Hillested O., 2007, P PACK VID NOV, P26
   Hsu CY, 1999, IEEE J SEL AREA COMM, V17, P756, DOI 10.1109/49.768193
   Kim HS, 2008, IEEE T CONSUM ELECTR, V54, P171, DOI 10.1109/TCE.2008.4470040
   KRUNZ M, 1997, ACM SIGMETRICS PERFO, V25, P192
   LAZAR A, 1993, P IEEE GLOBECOM 93, P835
   Matrawy A, 2002, 2002 IEEE 4TH INTERNATIONAL WORKSHOP ON NETWORKED APPLIANCES, PROCEEDINGS, P249, DOI 10.1109/IWNA.2001.980870
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SHE J, 2008, P IEEE WIR COMM NETW, P3139
   SHE J, 2007, P IEEE INT C AINA NI, P451
   van Beek P., 2005, P IEEE INT C IM PROC, V2, P173
   Wang JF, 2007, IEEE J SEL AREA COMM, V25, P712, DOI 10.1109/JSAC.2007.070508
   *WIMAX FOR, WIMAX SYST EV METH V
   Wu DP, 2001, P IEEE, V89, P6, DOI 10.1109/5.904503
   ZHU X, 2007, P EUR SIGN PROC C EU, P1462
NR 20
TC 12
Z9 13
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1140
EP 1147
DI 10.1109/TMM.2009.2026099
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700010
DA 2024-07-18
ER

PT J
AU Albanese, M
   Chellappa, R
   Moscato, V
   Picariello, A
   Subrahmanian, VS
   Turaga, P
   Udrea, O
AF Albanese, Massimiliano
   Chellappa, Rama
   Moscato, Vincenzo
   Picariello, Antonio
   Subrahmanian, V. S.
   Turaga, Pavan
   Udrea, Octavian
TI A Constrained Probabilistic Petri Net Framework for Human Activity
   Detection in Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Algorithms; machine vision; Petri nets; surveillance
ID SURVEILLANCE
AB Recognition of human activities in restricted settings such as airports, parking lots and banks is of significant interest in security and automated surveillance systems. In such settings, data is usually in the form of surveillance videos with wide variation in quality and granularity. Interpretation and identification of human activities requires an activity model that a) is rich enough to handle complex multi-agent interactions, b) is robust to uncertainty in low-level processing and c) can handle ambiguities in the unfolding of activities. We present a computational framework for human activity representation based on Petri nets. We propose an extension-Probabilistic Petri Nets (PPN)-and show how this model is well suited to address each of the above requirements in a wide variety of settings. We then focus on answering two types of questions: (i) what are the minimal sub-videos in which a given activity is identified with a probability above a certain threshold and (ii) for a given video, which activity from a given set occurred with the highest probability? We provide the PPN-MPS algorithm for the first problem, as well as two different algorithms (naive PPN-MPA and PPN-MPA) to solve the second. Our experimental results on a dataset consisting of bank surveillance videos and an unconstrained TSA tarmac surveillance dataset show that our algorithms are both fast and provide high quality results.
C1 [Albanese, Massimiliano; Chellappa, Rama; Subrahmanian, V. S.; Turaga, Pavan; Udrea, Octavian] Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
   [Moscato, Vincenzo] Univ Naples Federico II, Multimedia Informat Syst Res Grp, Naples, Italy.
   [Picariello, Antonio] Univ Naples Federico II, Dipartimento Informat & Sistemist, Naples, Italy.
C3 University System of Maryland; University of Maryland College Park;
   University of Naples Federico II; University of Naples Federico II
RP Albanese, M (corresponding author), Univ Maryland, Inst Adv Comp Studies, College Pk, MD 20742 USA.
EM albanese@umiacs.umd.edu; rama@umiacs.umd.edu; vmoscato@unina.it;
   picus@unina.it; vs@umiacs.umd.edu; pturaga@umiacs.umd.edu;
   udrea@umiacs.umd.edu
RI Picariello, Antonio/G-9062-2012; Subrahmanian,
   Venkatramanan/ABA-7399-2021; Moscato, Vincenzo/H-2526-2012; Chellappa,
   Rama/B-6573-2012; Turaga, Pavan/W-6186-2019; Chellappa,
   Rama/AAV-8690-2020; Albanese, Massimiliano/H-5093-2019; PICARIELLO,
   Antonio/L-6820-2015
OI Albanese, Massimiliano/0000-0002-2675-5810; PICARIELLO,
   Antonio/0000-0003-4804-1007; Moscato, Vincenzo/0000-0002-0754-7696
FU AFOSR [FA95500610405, FA95500510298]; DAAD [190310202]; NSF [0540216];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [0540216] Funding Source: National Science Foundation
FX Manuscript received January 10, 2008; revised March 14, 2008. Current
   version published December 10, 2008. This work was supported in part by
   AFOSR Grants FA95500610405 and FA95500510298, ARO Grant DAAD 190310202
   and NSF Grant 0540216. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Michael G.
   Christel.
CR Albanese M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1802
   Bobick A, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P39, DOI 10.1109/ACV.1996.571995
   Brand M, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P94, DOI 10.1109/AFGR.1996.557249
   BREMOND F, 1997, P INT FLOR AI RES S
   BUXTON H, 1995, ARTIF INTELL, V78, P431, DOI 10.1016/0004-3702(95)00041-0
   CAI Q, 1996, P 13 INT C PATT REC
   Cardoso J, 1999, IEEE T SYST MAN CY B, V29, P573, DOI 10.1109/3477.790440
   CASTEL C, 1996, P ECCV WORKSH CONC D
   CHELLAPPA R, 2008, UNDERSTANDING EVENTS
   CHEN S, 1990, SOFTWARE MAINTENANCE, V2, P3
   DAVIES TF, 1994, ISRAEL J MED SCI, V30, P2
   GHANEM N, 2004, P 2 IEEE WORKSH EV M, P112
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Hongeng S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P84, DOI 10.1109/ICCV.2001.937608
   HUANG T, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P966
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   Joo SW, 2006, IEEE IMAGE PROC, P2897, DOI 10.1109/ICIP.2006.313035
   KHAN S, 2000, P AS C COMP VIS JAN
   Lesire C, 2005, LECT NOTES COMPUT SC, V3536, P329
   MARSAN MA, 1991, MICROELECTRON RELIAB, V31, P699, DOI 10.1016/0026-2714(91)90010-5
   Moore D, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P770
   MURATA T, 1989, P IEEE, V77, P541, DOI 10.1109/5.24143
   Pearl J., 1988, PROBABILISTIC REASON
   Prati A, 2003, IEEE T PATTERN ANAL, V25, P918, DOI 10.1109/TPAMI.2003.1206520
   Shet VD, 2005, AVSS 2005: ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, PROCEEDINGS, P224
   Siebel NilsT., 2004, ECCV 2004 workshop Applications of Computer Vision (ACV), P103
   Siebel NT, 2002, LECT NOTES COMPUT SC, V2353, P373
   Vaswani N, 2003, PROC CVPR IEEE, P633
   VU VT, 2003, P 18 INT JOINT C ART
   Zhu Y, 2000, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2000.855879
NR 30
TC 32
Z9 35
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1429
EP 1443
DI 10.1109/TMM.2008.2010417
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600001
DA 2024-07-18
ER

PT J
AU Borges, PVK
   Mayer, J
   Izquierdo, E
AF Borges, Paulo Vinicius Koerich
   Mayer, Joceli
   Izquierdo, Ebroul
TI Document Image Processing for Paper Side Communications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB This paper proposes the use of higher order statistical moments in document image processing to improve the performance of systems which transmit side information through the print and scan channel. Examples of such systems are multilevel 2-D bar codes and certification via text luminance modulation. These systems print symbols with different luminances, according to the target side information. In previous works, the detection of a received symbol is usually performed by evaluating the average luminance or spectral characteristics of the received signal. This paper points out that, whenever halftoning algorithms are used in the printing process, detection can be improved by observing that third and fourth order statistical moments of the transmitted symbol also change, depending on the luminance level. This work provides a thorough analysis for those moments used as detection metrics. A print and scan channel model is exploited to derive the relationship between the modulated luminance level and the higher order moments of a halftone image. This work employs a strategy to merge the different moments into a single metric to achieve a reduced detection error rate. A transmission protocol for printed documents is proposed which takes advantage of the resulting higher robustness achieved with the combined detection metrics. The applicability of the introduced document image analysis approach is validated by comprehensive computer simulations.
C1 [Borges, Paulo Vinicius Koerich; Mayer, Joceli] Univ Fed Santa Catarina, Dept Elect Engn, LPDS, BR-88040900 Florianopolis, SC, Brazil.
   [Borges, Paulo Vinicius Koerich; Izquierdo, Ebroul] Queen Mary Univ London, Dept Elect Engn, Multimedia & Vis Lab, London E1 4NS, England.
C3 Universidade Federal de Santa Catarina (UFSC); University of London;
   Queen Mary University London
RP Borges, PVK (corresponding author), Univ Fed Santa Catarina, Dept Elect Engn, LPDS, BR-88040900 Florianopolis, SC, Brazil.
EM paulo.vinicius@elec.qmul.ac.uk; mayer@eel.ufsc.br;
   ebroul.izquierdo@elec.qmul.ac.uk
RI Borges, Paulo V K/B-7538-2011; Mayer, Joceli/B-2035-2008
FU CNPq [202288/2006-4]
FX Manuscript received March 15, 2007; revised May 28, 2008. Current
   version published November 17, 2008. This work was supported by CNPq,
   Procurement 202288/2006-4. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Xiaolin Wu.
CR ALATTAR AM, 2004, P SPIE, V5306
   AMANO T, 1999, IEEE P 5 INT C DOC A
   Bayer BE., 1973, 20OPTIMUM METHOD 2 L
   BHATTACHARJYA AK, 1999, P IEEE INT C IM PROC, V2
   Borges PVK, 2007, SIGNAL PROCESS, V87, P1754, DOI 10.1016/j.sigpro.2007.01.029
   BORGES PV, 2006, IEEE INT C AC SPEECH
   Brassil JT, 1999, P IEEE, V87, P1181, DOI 10.1109/5.771071
   Cannons J, 2004, IEEE T IMAGE PROCESS, V13, P1393, DOI 10.1109/TIP.2004.834660
   COX I, 2002, DIGITAL WATERMAKING
   Duda R., 1973, Pattern Classification and Scene Analysis
   Huang D, 2001, IEEE T CIRC SYST VID, V11, P1237, DOI 10.1109/76.974678
   JIANG M, 2005, IEEE WORKSH MULT SIG
   Kay S. M., 1993, FUNDAMENTALS STAT SI, V1
   LI X, 2004, 7 INT C SIGN PROC IC, V3
   Manolakis D. G., 2000, Statistical and Adaptive Signal Processing
   MEI Q, 2001, SPIE P SECURITY WA 3, V2
   Mese M, 2002, IEEE T CIRCUITS-I, V49, P790, DOI 10.1109/TCSI.2002.1010034
   MORI S, 1992, P IEEE, V80, P1029, DOI 10.1109/5.156468
   NORRIS M, 2004, P INT C IM SCI SYST
   QUINTELA ND, 2003, P SPIE
   Sklar B., 2003, DIGITAL COMMUNICATIO, V2nd
   SOLANKI K, 2005, P SPIE ELECT IMAG
   Theodoridis S, 2006, PATTERN RECOGNITION, 3RD EDITION, P1
   ULICHNEY R, 1988, DIGITAL HALFTONING
   ULICHNEY RA, 1988, P IEEE, V76, P56, DOI 10.1109/5.3288
   VILLAN R, 2006, P SPIE ELECT IMAG
   VILLAN R, 2007, P SPIE IST EL IM 200
   Villán R, 2006, IEEE T INF FOREN SEC, V1, P405, DOI 10.1109/TIFS.2006.885022
   VOLOSHYNOVSKIY S, 2004, P PIE ELECT IMAG
   WONG PW, 2001, IEEE T IMAGE PROCESS, V10
   Wu M, 2004, IEEE T MULTIMEDIA, V6, P528, DOI 10.1109/tmm.2004.830814
   Xu YH, 1999, IEEE T PATTERN ANAL, V21, P1280, DOI 10.1109/34.817408
   YANG H, 2004, P IEEE INT C MULT EX
NR 33
TC 8
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2008
VL 10
IS 7
BP 1277
EP 1287
DI 10.1109/TMM.2008.2004906
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 378HB
UT WOS:000261310700005
DA 2024-07-18
ER

PT J
AU Demircin, MU
   van Beek, P
   Altunbasak, Y
AF Demircin, Mehmet U.
   van Beek, Peter
   Altunbasak, Yucel
TI Delay-Constrained and R-D Optimized Transrating for High-Definition
   Video Streaming Over WLANs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE HDTV; multimedia streaming; video codecs; wireless LAN
ID WIRELESS CHANNELS; ADAPTATION; NETWORKS
AB In this study, we propose bit-rate adaptation techniques for compressed high-definition video transmission over wireless home networks. Application-layer methods are developed to minimize duality impairments caused by wireless channel degradations. A transrater located at a home video server adjusts the transmission rate of the video stream based on bandwidth measurements. We consider delay constraints of multiple video frames across various time scales, and dynamically select an appropriate time scale for bit-rate adjustments. This novel feature prevents playout buffer underflows and avoids unnecessary rate reductions. Furthermore, distortion due to transrating is minimized by coding type-dependent rate allocation among the frames to be transmitted in the projected time scale. Using a realistic simulation setup, we demonstrated that the proposed technique significantly reduces video glitches. Up to 3.12 dB average PSNR improvement is achieved, compared to a non-delay constrained rate adaptation method, which also utilizes bandwidth measurements. Our approach maintains compliance with existing wireless local area (WLAN) devices and standards, as well as consumer video standards and applications.
C1 [Demircin, Mehmet U.; Altunbasak, Yucel] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
   [van Beek, Peter] Sharp Labs Amer, Camas, WA 98607 USA.
C3 University System of Georgia; Georgia Institute of Technology; Hisense
RP Demircin, MU (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM demircin@ece.gatech.edu; pvanbeek@sharplabs.com; yucel@ece.gatech.edu
RI Demircin, Mehmet Umut/AGX-7237-2022
CR [Anonymous], MSRTR200135
   [Anonymous], 2012, 802112007 IEEE
   BEGEN AC, 2005, IEEE INT C MULT EXP
   BOLOT JC, 1999, IEEE INT C COMP COMM
   Cabrera J, 2002, IEEE T CIRC SYST VID, V12, P496, DOI 10.1109/TCSVT.2002.800306
   CHAKARESKI J, 2002, P INT C AC SPEECH SI
   de los Reyes G, 2000, IEEE J SEL AREA COMM, V18, P1063, DOI 10.1109/49.848256
   DEMIRCIN MU, 2006, IEEE INT C IM PROC I
   DEMIRCIN MU, 2004, IEEE WIR COMM NETW C
   Dempsey BJ, 1996, COMPUT NETWORKS ISDN, V28, P719, DOI 10.1016/0169-7552(95)00051-8
   Girod B., 2000, COMPRESSED VIDEO NET
   Hsu C.-Y., 1999, IEEE J SEL AREAS COM, V17
   Hurley P, 2001, IEEE NETWORK, V15, P60, DOI 10.1109/65.923942
   Jain M, 2003, IEEE ACM T NETWORK, V11, P537, DOI 10.1109/TNET.2003.815304
   Kalman M, 2004, IEEE T CIRC SYST VID, V14, P841, DOI 10.1109/TCSVT.2004.828335
   Kim T, 2005, IEEE J SEL AREA COMM, V23, P344, DOI 10.1109/JSAC.2004.839390
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   MIAO Z, 2002, PACK VID WORKSH
   MIAO Z, 2000, AS C SIGN SYST COMP
   MUKHERJEE B, 2000, TIME LINED TCP TCP F
   PAPADOPOULOS C, 1996, RETRANSMISSION BASED
   PAXSON V, 1997, P ACM SIGCOMM CANN F, P139
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   PRASAD RS, 2003, IEEE NETWORK, V17
   Reibman AR, 1992, IEEE T CIRC SYST VID, V2, P361, DOI 10.1109/76.168912
   STOCKHAMMER T, 2004, IEEE T MULTIMEDIA, V6
   van Beek P, 2004, PROC SPIE, V5308, P647, DOI 10.1117/12.527629
   VANBEEK P, 2005, IEEE INT C IM PROC I
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Viéron J, 2004, IEEE T MULTIMEDIA, V6, P634, DOI [10.1109/TMM.2004.830805, 10.1109/tmm.2004.830805]
   WAH BW, 2000, IEEE INT S MULT SOFT
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
NR 32
TC 7
Z9 8
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1155
EP 1168
DI 10.1109/TMM.2008.2001383
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600018
DA 2024-07-18
ER

PT J
AU Wang, W
   Peng, DM
   Wang, HG
   Sharif, H
   Chen, HH
AF Wang, Wei
   Peng, Dongming
   Wang, Honggang
   Sharif, Hamid
   Chen, Hsiao-Hwa
TI Energy-Constrained Distortion Reduction Optimization for Wavelet-Based
   Coded Image Transmission in Wireless Sensor Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intra image diversity; cross layer optimization; unequal error
   protection; wireless sensor networks
ID ERROR PROTECTION; COMPRESSION; MULTIMEDIA
AB Image transmissions in Wireless Multimedia Sensor Networks (WMSNs) are often energy constrained. They also have requirement on distortion minimization, which may be achieved through Unequal Error Protection (UEP) based communication approaches. In related literature with regard to wireless multimedia transmissions, significantly different importance levels between image-pixel-position information and image-pixel-value information have not been fully exploited by existing UEP schemes. In this paper, we propose an innovative image-pixel-position information based resource allocation scheme to optimize image transmission quality with strict energy budget constraint for image applications in WMSNs, and it works by exploring these uniquely different importance levels among image data streams. Network resources are optimally allocated cross PHY, MAC and APP layers regarding inter-segment dependency, and energy efficiency is assured while the image transmission quality is optimized. Simulation results have demonstrated the effectiveness of the proposed approach in achieving the optimal image quality and energy efficiency. The performance gain in terms of distortion reduction is especially prominent with strict energy budget constraints and lower image compression ratios.
C1 [Wang, Wei; Peng, Dongming; Wang, Honggang; Sharif, Hamid] Univ Nebraska, Dept Elect & Comp Engn, Lincoln, NE 68508 USA.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 701, Taiwan.
C3 University of Nebraska System; University of Nebraska Lincoln; National
   Cheng Kung University
RP Wang, W (corresponding author), Univ Nebraska, Dept Elect & Comp Engn, Lincoln, NE 68508 USA.
EM wwang@unlnotes.unl.edu; dpeng@unlnotes.unl.edu; hwang@unlnotes.unl.edu;
   hsharif@unlnotes.unl.edu; hshwchen@ieee.org
RI Sharif, Haidar/AAR-6783-2021; Wang, Honggang/D-6079-2013
OI Sharif, Haidar/0000-0001-7235-6004; Sharif-Kashani,
   Hamid/0000-0001-6229-2043; Wang, Honggang/0000-0001-9475-2630
CR Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Creusere CD, 1997, IEEE T IMAGE PROCESS, V6, P1436, DOI 10.1109/83.624967
   Cui SG, 2005, IEEE T WIREL COMMUN, V4, P2349, DOI 10.1109/TWC.2005.853882
   Dam T., 2003, the 1st International Conference on Embedded Networked Sensor Systems, P171, DOI DOI 10.1145/958491.958512
   Gan T, 2006, IEEE T IMAGE PROCESS, V15, P819, DOI 10.1109/TIP.2005.863960
   Hamzaoui R, 2005, IEEE SIGNAL PROC MAG, V22, P91, DOI 10.1109/MSP.2005.1550192
   Haykin Simon., 1994, COMMUNICATION SYSTEM, V3-rd, P510
   Kim JH, 2002, GENET MED, V4, p62S, DOI 10.1097/00125817-200211001-00013
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Li Z, 2007, IEEE T MULTIMEDIA, V9, P837, DOI 10.1109/TMM.2007.893338
   Ozcelebi T, 2007, IEEE T MULTIMEDIA, V9, P826, DOI 10.1109/TMM.2007.895670
   Rhee Injong., 2005, SENSYS 05, P90
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Schurgers C, 2001, ISLPED'01: PROCEEDINGS OF THE 2001 INTERNATIONAL SYMPOSIUM ON LOWPOWER ELECTRONICS AND DESIGN, P96, DOI 10.1109/LPE.2001.945382
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Stallings William., 2000, DATA COMPUTER COMMUN, V7-th, P85
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Tillo T, 2007, IEEE T IMAGE PROCESS, V16, P673, DOI 10.1109/TIP.2007.891152
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   van Dijk TA, 2006, DISCOURSE STUD, V8, P5, DOI 10.1177/1461445606059544
   Vandegraaff Nick, 2007, Expert Reviews in Molecular Medicine, V9, P1, DOI 10.1017/S1462399407000257
   Wang GJ, 2007, IEEE MULTIMEDIA, V14, P74, DOI 10.1109/MMUL.2007.16
   WANG HM, IEEE T WIRE IN PRESS
   Wang H, 2007, IEEE ICC, P3776, DOI 10.1109/ICC.2007.622
   WANG W, 2007, P IEEE VTC APR, P124
   Wang W, 2007, GLOB TELECOMM CONF, P976
   Wang W, 2007, IEEE WCNC, P4094
   Wu HM, 2006, COMPUT NETW, V50, P2873, DOI 10.1016/j.comnet.2005.09.039
   Wu ZY, 2005, IEEE T COMMUN, V53, P1648, DOI 10.1109/TCOMM.2005.857142
   Wu ZY, 2005, IEEE T IMAGE PROCESS, V14, P1020, DOI 10.1109/TIP.2005.851681
   Yuan Y, 2006, IEEE T VEH TECHNOL, V55, P856, DOI 10.1109/TVT.2006.873837
   Zhang Y, 2007, ACS CHEM BIOL, V2, P320, DOI 10.1021/cb7000044
NR 33
TC 52
Z9 58
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1169
EP 1180
DI 10.1109/TMM.2008.2001354
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600019
DA 2024-07-18
ER

PT J
AU Sim, JY
   Lee, SU
AF Sim, Jae-Young
   Lee, Sang-Uk
TI Compression of 3-D point visual data using vector quantization and
   rate-distortion optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE QSplat representation; rate-distortion optimization; 3-D point data
   compression; vector quantization
ID SET
AB In this paper, we propose adaptive and flexible quantization and compression algorithms for 3-D point data using vector quantization (VQ) and rate-distortion (R-D) optimization. The point data are composed of the position and the radius of sphere based on QSplat representation. The positions of child spheres are first transformed to the local coordinate system, which is determined by the parent-children relationship. The local coordinate transform makes the positions more compactly distributed in 3-D space, facilitating an effective application of VQ. We also develop a constrained encoding method for the radius data, which can provide a hole-free surface rendering at the decoder side. Furthermore, R-D optimized compression algorithm is proposed in order to allocate an optimal bitrate to each sphere. Experimental results show that the proposed algorithm can effectively compress the original 3-D point geometry at various bitrates.
C1 [Sim, Jae-Young] Samsung Adv Inst Technol, Comp & Intelligence Lab, Yongin, South Korea.
   [Lee, Sang-Uk] Seoul Natl Univ, Sch Elect Engn & Comp Sci, Seoul 151742, South Korea.
C3 Samsung; Seoul National University (SNU)
RP Sim, JY (corresponding author), Samsung Adv Inst Technol, Comp & Intelligence Lab, Yongin, South Korea.
EM jaeyoung@ieee.org; sanguk@ipl.snu.ac.kr
RI Sim, Jae-Young/F-3757-2010
CR Alexander M, 2001, INTERNETWEEK, P21
   [Anonymous], 2012, VECTOR QUANTIZATION
   Chou PH, 2002, IEEE T VIS COMPUT GR, V8, P373, DOI 10.1109/TVCG.2002.1044522
   Fleishman S, 2003, ACM T GRAPHIC, V22, P997, DOI 10.1145/944020.944023
   GROSSMAN JP, 1998, THESIS MIT CAMBRIDGE
   GUMHOLD S, 2005, P 6 ISR KOR BI NAT C, P125
   Khalil H, 2001, IEEE T IMAGE PROCESS, V10, P15, DOI 10.1109/83.892439
   Lee ES, 2000, EIGHTH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, PROCEEDINGS, P225, DOI 10.1109/PCCGA.2000.883945
   Levoy M, 2000, COMP GRAPH, P131, DOI 10.1145/344779.344849
   Levoy Marc, 1985, The use of points as a display primitive
   Ochotta T., 2004, P S POINT BAS GRAPH
   Pfister H, 2000, COMP GRAPH, P335, DOI 10.1145/344779.344936
   Rusinkiewicz S, 2000, COMP GRAPH, P343, DOI 10.1145/344779.344940
   SAYWOOD K, 2000, INTRO DATA COMPRESSI
   SHOHAM Y, 1988, IEEE T ACOUST SPEECH, V36, P1445, DOI 10.1109/29.90373
   Sim JY, 2005, IEEE T MULTIMEDIA, V7, P1191, DOI 10.1109/TMM.2005.858410
   SIM JY, 2004, P SPIE VCIP SAN JOS
   WASCHBUSCH M., 2004, Proceedings of Eurographics Symposium on Point-Based Graphics 2004, P95
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
NR 19
TC 11
Z9 13
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 305
EP 315
DI 10.1109/TMM.2008.917349
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100001
DA 2024-07-18
ER

PT J
AU Shyu, ML
   Xie, ZX
   Chen, M
   Chen, SC
AF Shyu, Mei-Ling
   Xie, Zongxing
   Chen, Min
   Chen, Shu-Ching
TI Video semantic event/concept detection using a subspace-based multimedia
   data mining framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data mining; eigenspace; eigenvalue; event/concept detection; principal
   component; video semantics analysis
ID EVENT DETECTION
AB In this paper, a subspace-based multimedia data mining framework is proposed for video semantic analysis, specifically video event/concept detection, by addressing two basic issues, i.e., semantic gap and rare event/concept detection. The proposed framework achieves full automation via multimodal content analysis and intelligent integration of distance-based and rule-based data mining techniques. The content analysis process facilitates the comprehensive video analysis by extracting low-level and middle-level features from audio/visual channels. The integrated data mining techniques effectively address these two basic issues by alleviating the class imbalance issue along the process and by reconstructing and refining the feature dimension automatically. The promising experimental performance on goal/corner event detection and sports/commercials/building concepts extraction from soccer videos and TRECVID news collections demonstrates the effectiveness of the proposed framework. Furthermore, its unique domain-free characteristic indicates the great potential of extending the proposed multimedia data mining framework to a wide range of different application domains.
C1 [Shyu, Mei-Ling; Xie, Zongxing] Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
   [Chen, Min] Univ Montana, Dept Comp Sci, Missoula, MT 59812 USA.
   [Chen, Shu-Ching] Florida Int Univ, Sch Comp & Informat Sci, Distributed Multimedia Informat Syst Lab, Miami, FL 33199 USA.
C3 University of Miami; University of Montana System; University of
   Montana; State University System of Florida; Florida International
   University
RP Shyu, ML (corresponding author), Univ Miami, Dept Elect & Comp Engn, Coral Gables, FL 33124 USA.
EM shyu@rniami.edu; zxie@umsis.miami.edu; chen@cs.umt.edu; chens@cs.fiu.edu
RI Chen, Min/N-9350-2015
OI Chen, Min/0000-0002-0960-4447
CR AMIR A, 2003, NIST TRECVID
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], P 13 ANN ACM INT C M
   Chen M, 2006, IEEE SIGNAL PROC MAG, V23, P38, DOI 10.1109/MSP.2006.1621447
   Chen S.-C., 2005, VIDEO DATA MANAGEMEN, P217
   CHEN SC, 2001, INT J ARTIFICIAL INT, V10, P715
   Chen SC, 2006, INT J COMPUT APPL T, V27, P312, DOI 10.1504/IJCAT.2006.012001
   Dagtas S, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P91, DOI 10.1109/MMSP.2001.962717
   Ekin A, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I, PROCEEDINGS, P169
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Gao S, 2007, INT CONF ACOUST SPEE, P981
   HAN B, 2003, SUPPORT VECTOR MACHI
   Han M., 2002, Proc. ACM Multimedia, P347, DOI [DOI 10.1145/641007.641081, 10.1145/641007.641081]
   Leonardi R, 2004, IEEE T CIRC SYST VID, V14, P634, DOI 10.1109/TCSVT.2004.826751
   Li BX, 2003, IEEE IMAGE PROC, P17
   Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Quinlan J. R., 1993, PROGRAMS MACHINE LEA
   Quirk M, 2006, LANCET INFECT DIS, V6, P13, DOI 10.1016/S1473-3099(05)70314-5
   Sadlier DA, 2005, IEEE T CIRC SYST VID, V15, P1225, DOI 10.1109/TCSVT.2005.854237
   SRIKANTH M, P NIST TRECVID 2005
   TJONDRONEGORO D, 2005, P ACM MULTIMEDIA, P1035
   TSENG VS, 2005, P 6 INT WORKSH MULT, P37
   Witten I. H., 2005, DATA MINING PRACTICA
   Wu X, 2006, IEEE SIGNAL PROC MAG, V23, P59
   Xiong ZY, 2006, IEEE SIGNAL PROC MAG, V23, P18
   Zhu XQ, 2005, IEEE T KNOWL DATA EN, V17, P665, DOI 10.1109/TKDE.2005.83
NR 27
TC 87
Z9 97
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2008
VL 10
IS 2
BP 252
EP 259
DI 10.1109/TMM.2007.911830
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 254HC
UT WOS:000252576700009
DA 2024-07-18
ER

PT J
AU Gómez, AM
   Peinado, AM
   Sánchez, V
   Rubio, AJ
AF Gomez, Angel M.
   Peinado, Antonio M.
   Sanchez, Victoria
   Rubio, Antonio J.
TI Combining media-specific FEC and error concealment for robust
   distributed speech recognition over loss-prone packet channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 30th IEEE International Conference on Acoustics, Speech, and Signal
   Processing
CY MAR 19-23, 2005
CL Philadelphia, PA
SP IEEE
DE distributed speech recognition; error concealment; forward error
   correction; packet loss; packet switching; MMSE estimation; weighted
   Viterbi recognition
ID CODES
AB This paper presents a mixed recovery scheme for robust distributed speech recognition (DSR) implemented over a packet channel which suffers packet losses. The scheme combines media-specific forward error correction (FEC) and error concealment (EC). Media-specific FEC is applied at the client side, where FEC bits representing strongly quantized versions of the speech vectors are introduced. At the server side, the information provided by those FEC bits is used by the EC algorithm to improve the recognition performance. We investigate the adaptation of two different EC techniques, namely minimum mean square error (MMSE) estimation, which operates at the decoding stage, and weighted Viterbi recognition (WVR), where EC is applied at the recognition stage, in order to be used along with FEC. The experimental results show that a significant increase in recognition accuracy can be obtained with very little bandwidth increase, which may be null in practice, and a limited increase in latency, which in any case is not so critical for an application such as DSR.
C1 Univ Granada, Dept Teoria Senal Telemat & Comunicac, Fac Ciencias, E-18071 Granada, Spain.
C3 University of Granada
RP Gómez, AM (corresponding author), Univ Granada, Dept Teoria Senal Telemat & Comunicac, Fac Ciencias, E-18071 Granada, Spain.
EM amgg@ugr.es; amp@ugr.es; victoria@ugr.es; rubio@ugr.es
RI García, Angel Manuel Gómez/C-6856-2012; Peinado, Antonio M./C-2401-2012
OI García, Angel Manuel Gómez/0000-0002-9995-3068; Peinado, Antonio
   M./0000-0001-8214-6676; Sanchez Calle, Victoria
   Eugenia/0000-0003-1546-9728
CR [Anonymous], P IEEE INFOCOM SAN F
   [Anonymous], 2003, RFC3550
   Bernard A, 2002, IEEE T SPEECH AUDI P, V10, P570, DOI 10.1109/TSA.2002.808141
   Boulis C, 2002, IEEE T SPEECH AUDI P, V10, P580, DOI 10.1109/TSA.2002.804532
   CARDENALLOPEZ A, 2004, P ICASSP, V1, P49
   *ESTI, ETSI ES 201 108 V1 1
   *ETSI, 2003, ETSI ES 202 212 V1 1
   *ETSI, 2003, ES 202 050 V1 1 3
   *ETSI, 2003, ETSI ES 202 211 V1 1
   GOMEZ AM, 2003, P EUR
   Hirsch Hans-Gunter, 2000, ASR2000 AUT SPEECH R
   JAMES A, 2004, P ICSLP
   JAMES AB, 2004, P ICASSP
   JIANG W, 2000, P NOSSDAV 2000 JUN
   Pearce D., 2000, AVIOS 2000 SPEECH AP
   Peinado AM, 2005, IEEE T WIREL COMMUN, V4, P14, DOI 10.1109/TWC.2004.840198
   PEINADO AM, 2003, SPEECH COMMUN
   PEINADO AM, 1990, P EUSIPCO 90, V2, P1243
   PEINADO AM, 2005, P ICASSP 05 PHIL PA
   PERKINS CS, 1998, IEEE NETWORK MAG SEP
   POSTEL J, 1980, 793 RFC USC INF SCI
   Postel J, 1980, RFC768 USC INF SCI I
   POTAMIANOS A, 2001, P ICASSP MAY
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   RISKIN EA, 2001, P EUR 2001 SEP
   VAISHAMPAYAN VA, 1992, IEEE T INFORM THEORY, V38, P1230, DOI 10.1109/18.144704
   YOMA NB, 1998, P ICASSP MAY, V2
   2005, RFC4060
   2003, RFC3557
NR 29
TC 14
Z9 15
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1228
EP 1238
DI 10.1109/TMM.2006.884611
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700012
DA 2024-07-18
ER

PT J
AU Taskiran, CM
   Pizlo, Z
   Amir, A
   Ponceleon, D
   Delp, EJ
AF Taskiran, Cuneyt M.
   Pizlo, Zygmunt
   Amir, Arnon
   Ponceleon, Dulce
   Delp, Edward J.
TI Automated video program summarization using speech transcripts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE speech transcripts; summarization evaluation; video summarization
ID EXTRACTION
AB Compact representations of video data greatly enhances efficient video browsing. Such representations provide the user with information about the content of the particular sequence being examined while preserving the essential message. We propose a method to automatically generate video summaries using transcripts obtained by automatic speech recognition. We divide the full program into segments based on pause detection and derive a score for each segment, based on the frequencies of the words and bigrams it contains. Then, a summary is generated by selecting the segments with the highest score to duration ratios while at the same time maximizing the coverage of the summary over the full program. We developed an experimental design and a user study to judge the quality of the generated video summaries. We compared the informativeness of the proposed algorithm with two other algorithms for three different programs. The results of the user study demonstrate that the proposed algorithm produces more informative summaries than the other two algorithms.
C1 Motorola Labs, Multimedia Res Lab, Schaumburg, IL 60196 USA.
   Purdue Univ, Dept Psychol Sci, W Lafayette, IN 47907 USA.
   IBM Corp, Almaden Res Ctr, San Jose, CA 95120 USA.
   Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 Legend Holdings; Lenovo; Purdue University System; Purdue University;
   International Business Machines (IBM); Purdue University System; Purdue
   University
RP Taskiran, CM (corresponding author), Motorola Labs, Multimedia Res Lab, Schaumburg, IL 60196 USA.
EM cuneyt.taskiran@motorola.com; pizlo@psych.purdue.edu;
   arnon@almaden.ibm.com; dulce@almaden.ibm.com; ace@ecn.purdue.edu
RI Delp, Edward J/C-3616-2013
OI Amir, Arnon/0009-0000-6814-165X; Delp, Edward/0000-0002-2909-7323
CR Agnihotri L, 2001, PROC SPIE, V4315, P599, DOI 10.1117/12.410973
   Amir A., 2003, VIDEO MINING
   Amir A., 2000, P 33 ANN HAW INT C S
   ANER A, 2002, P IEEE INT C MULT EX
   [Anonymous], INFORM RETRIEVAL 93
   Arons B., 1997, ACM Transactions on Computer-Human Interaction, V4, P3, DOI 10.1145/244754.244758
   BAHL LR, 1995, INT CONF ACOUST SPEE, P41, DOI 10.1109/ICASSP.1995.479268
   Barzilay R., 1997, P INT SCAL TEXT SUMM
   Cabasson R, 2003, P SOC PHOTO-OPT INS, V5021, P272, DOI 10.1117/12.476291
   CHANG SF, 2000, P IEEE INT C MULT EX
   Chen SS, 2002, SPEECH COMMUN, V37, P69, DOI 10.1016/S0167-6393(01)00060-7
   CHRISTEL M, 1999, P IEEE C ADV DIG LIB
   Christel M. G., 1998, CHI 98. Human Factors in Computing Systems. CHI 98 Conference Proceedings, P171, DOI 10.1145/274644.274670
   COOPER M, 2002, INT WORKSH MULT SIGN
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   *DISC HOM VID, 1999, WHY DOGS SMIL CHIMP
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Dunning T., 1993, Computational Linguistics, V19, P61
   Ekin A, 2003, PROC SPIE, V5021, P339, DOI 10.1117/12.476238
   FARM D, 2002, P IEEE INT C MULT EX, P89
   Ferman AM, 2003, IEEE T MULTIMEDIA, V5, P244, DOI 10.1109/TMM.2003.811617
   Gong YH, 2000, PROC CVPR IEEE, P174, DOI 10.1109/CVPR.2000.854772
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   Huang Q, 1999, INT CONF ACOUST SPEE, P3025, DOI 10.1109/ICASSP.1999.757478
   Iacob SM, 1999, P SOC PHOTO-OPT INS, V3846, P181, DOI 10.1117/12.360422
   JING H, 1998, P AAAI S INT SUMM PA
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Li BX, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P169
   Lienhart R, 2000, PROC SPIE, V3972, P378
   LUHN HP, 1958, IBM J RES DEV, V2, P159, DOI 10.1147/rd.22.0159
   Ma Y.-F., 2002, ACM MULTIMEDIA, P533
   MANI I, 1998, TIPSTER SVMMAC TEXT
   Manning C.D., 1999, FDN STAT NATURAL LAN
   Martello Silvano, 1990, Knapsack Problems: Algorithms and Computer Implementations
   OH J, 2000, P IEEE INT C MULT EX
   *PBS HOM VID, 2002, M TWAIN
   *PBS NOVA, 1997, KINGD SEAH
   Pfeiffer S, 1996, J VIS COMMUN IMAGE R, V7, P345, DOI 10.1006/jvci.1996.0030
   Pickering MJ, 2003, LECT NOTES COMPUT SC, V2728, P425
   PISINGER D, 1995, EUR J OPER RES, V87, P175, DOI 10.1016/0377-2217(94)00013-3
   PONCELEON D, 2001, P 34 HAW INT C SYST
   RATAKONDA K, 1999, P SOC PHOTO-OPT INS, V3653, P1531
   Rubner Y, 1997, P ARPA IM UND WORKSH
   Sparck-Jones K, 2000, INFORM PROCESS MANAG, V36, P779, DOI 10.1016/S0306-4573(00)00015-7
   STEFANIDIS A, 2000, P INT WORKSH ADV SPA
   Taniguchi Y, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P427
   Taskiran C, 2004, IEEE T MULTIMEDIA, V6, P103, DOI 10.1109/TMM.2003.819783
   Taskiran CM, 2002, PROC SPIE, V4676, P371
   TRUONG BT, 2000, P 8 ACM MULT C LOS A, P290
   Tseng BL, 2002, P SOC PHOTO-OPT INS, V4676, P359
   Uchihashi S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P383, DOI 10.1145/319463.319654
   Vasconcelos N., 1998, P IEEE INT C IM PROC
   WACTLAR HD, 2000, P IMAGINA 2000 C MON
   Yahiaoui I, 2003, EURASIP J APPL SIG P, V2003, P48, DOI 10.1155/S1110865703210052
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
NR 56
TC 50
Z9 62
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 775
EP 791
DI 10.1109/TMM.2006.876282
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300012
DA 2024-07-18
ER

PT J
AU Zhang, C
   Tsaoussidis, V
AF Zhang, Chi
   Tsaoussidis, Vassilis
TI TCP smoothness and window adjustment strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE congestion control; fairness; multimedia communication; smoothness;
   transport protocols
ID CONGESTION AVOIDANCE
AB We observe that even when the system throughput is relatively stable, end users of media-streaming applications do not necessarily experience smooth throughput, due to the unsynchronized window adjustments triggered by random congestion indications. We analyze and evaluate the negative impact of random window adjustments on smoothness, short-term fairness, and long-term fairness. We further propose an experimental congestion avoidance mechanism, namely TCP(alpha, beta, gamma, delta), based on coordinated window adjustments. The flow-level smoothness is enhanced significantly for media-streaming applications, without a cost on fairness and responsiveness. Responsiveness is even boosted when bandwidth is underutilized.
C1 Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
   Democritus Univ Thrace, Dept Elect & Comp Engn, Xanthi 67100, Greece.
C3 State University System of Florida; Florida International University;
   Democritus University of Thrace
RP Zhang, C (corresponding author), Florida Int Univ, Sch Comp Sci, Miami, FL 33199 USA.
EM czhang@cs.fiu.edu; vt-saousi@ee.duth.gr
OI tsaoussidis, vassilis/0000-0003-0783-043X
CR ALLMAN M, 1999, RFC2581          APR
   [Anonymous], 2000, 2914 RFC
   [Anonymous], P ACM SIGCOMM 98
   Appenzeller G, 2004, ACM SIGCOMM COMP COM, V34, P281, DOI 10.1145/1030194.1015499
   BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716
   CHIU DM, 1989, COMPUT NETWORKS ISDN, V17, P1, DOI 10.1016/0169-7552(89)90019-6
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Floyd S., 1997, RED DISCUSSIONS SETT
   Floyd S, 1993, IEEE ACM T NETWORK, V1, P397, DOI 10.1109/90.251892
   Hengartner U., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1546, DOI 10.1109/INFCOM.2000.832553
   Misra Vishal., 2000, Pro eedings of the onferen e on Appli ations, Te hnologies, Ar hite tures, and Proto ols for Computer Communi ation, P151
   REJAIE R, 1999, P IEEE INFOCOM 1 APR, P215
   SISALEM D, 1998, P INT WORKSH NETW OP, P215
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   Yang YR, 2001, IEEE INFOCOM SER, P1716, DOI 10.1109/INFCOM.2001.916669
   Yang YR, 2000, 2000 INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P187, DOI 10.1109/ICNP.2000.896303
   Zhang C, 2002, IEEE SYMP COMP COMMU, P291, DOI 10.1109/ISCC.2002.1021692
   ZHANG C, 2003, P ACM NOSSDAV 20 JUN, P131
NR 18
TC 8
Z9 9
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 600
EP 609
DI 10.1109/TMM.2006.870725
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Aygün, RS
   Patil, AS
AF Aygün, RS
   Patil, AS
TI <i>PressBase</i>:: A presentation synchronization database for
   distributed multimedia systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE database triggers; multimedia synchronization
AB Multimedia presentations are the basic objects of multimedia databases. Since a multimedia presentation is not an instant display of a query result, the control knowledge (or synchronization requirements) has to be incorporated into the database and necessary precautions have to be taken for a lengthy presentation. Active databases provide a mechanism for incorporation of control knowledge by using event-condition-action (ECA) rules. In this paper, we describe how multimedia synchronization can be handled within a database using ECA rules. We present a prototype presentation synchronization database, named as PressBase, for distributed multimedia systems. We have adopted one of the synchronization models, SynchRuler [5], and then incorporated into a relational database system.
C1 Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
C3 University of Alabama System; University of Alabama Huntsville
RP Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
EM raygun@cs.uah.edu; aditya.patil@adsenv.com
RI Patil, Anjali S/HHM-3383-2022; Aygun, Ramazan/HTQ-3507-2023
OI Aygun, Ramazan/0000-0001-7244-7475
CR ALLEN J, 2002, 6 WORLD C INT DES PR
   ALLEN J, 2005, IN PRESS IEEE T KNOW, V17, P1706
   ALLEN JF, 1983, COMMUN ACM, V26, P832, DOI 10.1145/182.358434
   Aygun RS, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P275, DOI 10.1109/ITCC.2001.918805
   AYGUN RS, 2001, 2001 ACM MULT C OTT, P471
   Bailey B., 1998, Proceedings ACM Multimedia 98, P257, DOI 10.1145/290747.290779
   BUCHMANN A, 1999, ACTIVE RULES DATABAS, P29
   CHAKRAVARTY S, 1995, P 11 INT C DAT ENG
   COURTIAT J, 1994, P ACM MULT SAN FRANC, V94, P133
   DAYAL U, 1988, P 3 INT C DAT KNOWL
   HAMAKAWA R, 1994, MULTIMEDIA SYSTEMS, V2, P26
   Herman I, 1998, MULTIMEDIA SYST, V6, P88, DOI 10.1007/s005300050078
   Schnepf J, 1996, IEEE J SEL AREA COMM, V14, P114, DOI 10.1109/49.481698
   STONEBRAKER M, 1991, COMMUN ACM, V34
   Widom J., 1996, ACTIVE DATABASE SYST
NR 15
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 289
EP 296
DI 10.1109/TMM.2005.864275
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300010
DA 2024-07-18
ER

PT J
AU Andrade, EL
   Woods, JC
   Khan, E
   Ghanbari, M
AF Andrade, EL
   Woods, JC
   Khan, E
   Ghanbari, M
TI Region-based analysis and retrieval for tracking of semantic objects and
   provision of augmented information in interactive sport scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image processing; image region analysis; image segmentation; image
   sequence analysis; object detection; tracking
ID SEGMENTATION
AB This paper introduces techniques for segmentation and tracking based on analysis of region derived descriptors. By partitioning the image into a series of homogeneous regions, the problem of object extraction changes from pixel based to database analysis. A region based approach has distinct advantages over pixel. In particular, it has low dimensionality, is resilient to rotation, shear, and photometric changes, preserves boundaries, gives implied segmentation, provides comprehensive description, has reduced drift, and permits detailed analysis. The method is amenable to the introduction of prior knowledge permitting the simplification of segmentation, which is otherwise an ill-posed problem. Objects are defined as collections of contiguous regions with known statistics, allowing their identification by performing correlations in the database. The interframe difference of region based sequences is high for low-motion scenes due to segmentation noise, but performs consistently better for high motion, making it suitable for tracking. When applied to tracking the method proves robust, occlusion insensitive, and unlike the other techniques, it re-establishes lost lock.
C1 Univ Essex, Dept Elect Engn Syst, Colchester CO4 3SQ, Essex, England.
C3 University of Essex
RP Univ Essex, Dept Elect Engn Syst, Colchester CO4 3SQ, Essex, England.
EM eaneto@inf.ed.ac.uk; woodjt@essex.ac.uk; ekhan@lycos.com;
   ghan@essex.ac.uk
RI Ghanbari, Mohammad/L-4053-2019
OI Ghanbari, Mohammad/0000-0002-5482-8378; Khan, Ekram/0000-0001-8983-7584
CR ADAMS R, 1994, IEEE T PATTERN ANAL, V16, P641, DOI 10.1109/34.295913
   Andrade EL, 2003, ELECTRON LETT, V39, P600, DOI 10.1049/el:20030439
   [Anonymous], 2003, Standard Codecs: Image Compression to Advanced Video Coding
   BEVERIDGE JR, 1989, INT J COMPUT VISION, V2, P311, DOI 10.1007/BF00158168
   Carson C, 2002, IEEE T PATTERN ANAL, V24, P1026, DOI 10.1109/TPAMI.2002.1023800
   Comaniciu D, 2000, PROC CVPR IEEE, P142, DOI 10.1109/CVPR.2000.854761
   Delaigle JF, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA489
   Ferman AM, 2002, IEEE T IMAGE PROCESS, V11, P497, DOI 10.1109/TIP.2002.1006397
   Fu Y, 2002, IEEE T IMAGE PROCESS, V11, P135, DOI 10.1109/83.982821
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   HIEU TN, 2002, IEEE T IMAGE PROCESS, V11, P1081
   JONG BK, 2003, PATT RECOGNIT LETT, V24, P113
   Kompatsiaris I, 2001, IEEE IMAGE PROC, P658, DOI 10.1109/ICIP.2001.959131
   Lipton AJ, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P8, DOI 10.1109/ACV.1998.732851
   MRAZEK P, 2001, THESIS CZECH TU PRAG
   Pei SC, 1998, IEEE T CIRC SYST VID, V8, P181, DOI 10.1109/76.664103
   Salembier P, 2000, IEEE T IMAGE PROCESS, V9, P561, DOI 10.1109/83.841934
   Scholte RHJ, 2001, J RES ADOLESCENCE, V11, P71, DOI 10.1111/1532-7795.00004
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   SIWISKOTT L, 1997, IEEE T PATTERN ANAL, V19, P775
   Tsaig Y, 2002, IEEE T CIRC SYST VID, V12, P597, DOI 10.1109/TCSVT.2002.800513
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   WOODS JC, 2000, THESIS U ESSEX COLCH
   WOODS JC, 2003, P IASTED INT C MOD I, P501
NR 24
TC 18
Z9 20
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2005
VL 7
IS 6
BP 1084
EP 1096
DI 10.1109/TMM.2005.854417
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 986SY
UT WOS:000233471200009
DA 2024-07-18
ER

PT J
AU Sun, XD
   Foote, J
   Kimber, D
   Manjunath, BS
AF Sun, XD
   Foote, J
   Kimber, D
   Manjunath, BS
TI Region of interest extraction and virtual camera control based on
   panoramic video capturing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG; panoramic video; region of interest; virtual camera control
AB We present a system for automatically extracting the region of interest (ROI) and controlling virtual cameras' control based on panoramic video. It targets applications such as classroom lectures and video conferencing. For capturing panoramic video, we use the FlyCam system that produces high resolution, wide-angle video by stitching video images from multiple stationary cameras. To generate conventional video, a region of interest can be cropped from the panoramic video. We propose methods for ROI detection, tracking, and virtual camera control that work in both the uncompressed and compressed domains. The ROI is located from motion and color information in the uncompressed domain and macroblock information in the compressed domain, and tracked using a Kalman filter. This results in virtual camera control that simulates human controlled video recording. The system has no physical camera motion and the virtual camera parameters are readily available for video indexing.
C1 Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   FX Palo Alto Lab, Palo Alto, CA 94304 USA.
C3 University of California System; University of California Santa Barbara
RP Microsoft Corp, Redmond, WA 98052 USA.
EM xinding@gmail.com; foote@fxpal.com; kimber@fxpal.com; manj@ece.ucsb.edu
RI Manjunath, B S/AAM-8190-2020
OI Manjunath, B S/0000-0003-2804-3611
CR BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   BAUMBERG A, 1994, IEEE WORKSH MOT NON
   BERGEN JR, 1992, P 2 EUR C COMP VIS, P237
   Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   Chen S. E., 1993, Computer Graphics Proceedings, P279, DOI 10.1145/166117.166153
   Cutler R, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P416, DOI 10.1109/AFGR.1998.670984
   CUTLER R, 2002, P ACM MULT
   Darrell T, 1998, PROC CVPR IEEE, P601, DOI 10.1109/CVPR.1998.698667
   Foote J, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1419, DOI 10.1109/ICME.2000.871033
   GLEICHER M, 1992, COMP GRAPH, V26, P331, DOI 10.1145/142920.134088
   HASKELL G, 1997, DIGITAL VIDEO INTRO
   HE LW, 1996, P 23 ANN C COMP GRAP, P217
   Lee Dar-Shyang., 2002, P 10 ACM INT C MULT, P493
   Majumder A, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P169, DOI 10.1145/319463.319485
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   NAYAR S, 1999, P ICM 99, P482
   Nicolescu M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1581, DOI 10.1109/ICME.2000.871071
   OROURKE J, 1980, IEEE T PATTERN ANAL, V2, P522, DOI 10.1109/TPAMI.1980.6447699
   Stiefelhagen R, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P3, DOI 10.1145/319463.319464
   SUN X, 2001, ISOIECJTC1SC29WG11P7
   Sun X., 2001, P ACM MULT, P329, DOI DOI 10.1145/500141.500191
   SWAMINATHAN R, 1999, P IEEE C COMP VIS PA, P413
   Tao H, 2000, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2000.854760
   Teodosio L., 1993, Proceedings ACM Multimedia 93, P39, DOI 10.1145/166266.166270
   Wang C, 1998, INT CONF ACOUST SPEE, P3737, DOI 10.1109/ICASSP.1998.679696
   Wang HL, 1997, IEEE T CIRC SYST VID, V7, P615, DOI 10.1109/76.611173
   Wei J, 2001, IEEE T PATTERN ANAL, V23, P896, DOI 10.1109/34.946992
   ZHANG FJ, 1989, ACTA MATH APPL SIN-E, V1, P1
   Zhen J. Y., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P413, DOI 10.1109/CVPR.1991.139725
   Zobel M, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P901, DOI 10.1109/ICIP.2002.1039118
NR 30
TC 9
Z9 22
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 981
EP 990
DI 10.1109/TMM.2005.854388
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nam, J
   Ro, YM
   Huh, Y
   Kim, M
AF Nam, J
   Ro, YM
   Huh, Y
   Kim, M
TI Visual content adaptation according to user perception characteristics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE accessibility; color temperature preference; color vision deficiency;
   Digital Item Adaptation (DIA); low vision; MPEG-21; personalizing; user
   characteristics
ID ENHANCEMENT
AB Adapting multimedia content to users' preferences and perceptual characteristics is a key direction for enabling personalized multimedia services. In this paper, we address the problem of tailoring visual content within the MPEG-21 Digital Item Adaptation (DIA) framework to meet users' visual perception characteristics. In particular, we present methods for adapting visual content to accommodate color vision deficiency and low-vision capabilities. In addition, we present methods for adapting visual content according to user preferences for color temperature. Finally, we report on experiments that adapt visual content within the MPEG-21 DIA framework.
C1 ETRI, Taejon 305350, South Korea.
   ICU, Multimedia Informat Proc Grp, Taejon 305714, South Korea.
   Hanyang Univ, Dept Math, Seoul 133791, South Korea.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI);
   Hanyang University
RP Nam, J (corresponding author), ETRI, Taejon 305350, South Korea.
EM namjeho@etri.re.kr; yro@icu.ac.kr; yshuh@hanyang.ac.kr; mkim@icu.ac.kr
RI Ro, Yong Man/ABF-6817-2020; Kim, Munchurl/AAQ-9591-2020; Kim,
   Munchurl/C-1759-2011; Ro, Yong Man/C-1731-2011
OI Ro, Yong Man/0000-0001-5306-6853; Huh, Youngsik/0000-0001-7211-2982
CR [Anonymous], 1996, P 23 ANN C COMP GRAP, DOI DOI 10.1145/237170.237262
   [Anonymous], FARNSWORTH MUNSELL 1
   Burnett I, 2003, IEEE MULTIMEDIA, V10, P60, DOI 10.1109/MMUL.2003.1237551
   Fairchild M.D., 1997, COLOR APPEARANCE MOD
   HARDY LH, 1954, J OPT SOC AM, V44, P509, DOI 10.1364/JOSA.44.000509
   *ISO IEC, 2002, 159385 ISO IEC
   *ISO IEC, 2004, 210007 ISO IEC
   *ISO IEC, 2004, 159383 ISO IEC
   JI TL, 1994, IEEE T MED IMAGING, V13, P573, DOI 10.1109/42.363111
   KIM MS, 2000, J KOREAN OPHTHALMOL, V41
   Kovács G, 2001, COLOR RES APPL, V26, pS273, DOI 10.1002/1520-6378(2001)26:1+<::AID-COL59>3.0.CO;2-Y
   Lennie P, 2002, VISUAL IMPAIRMENTS D
   MCINTYRE D, 2002, COLOR BLINDNESS CAUS
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   NAM J, 2002, JTCISC29WG11 ISO IEC
   Onoda M, 1998, APCCAS '98 - IEEE ASIA-PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P93, DOI 10.1109/APCCAS.1998.743666
   Park DS, 2003, P SOC PHOTO-OPT INS, V5008, P285, DOI 10.1117/12.475440
   PELI E, 1984, OPT ENG, V23, P47, DOI 10.1117/12.7973251
   POKORNY J, 1977, J OPT SOC AM, V67, P1196, DOI 10.1364/JOSA.67.001196
   Rigden C, 1999, BRIT TELECOMMUN ENG, V17, P291
   Ro YM, 2004, INT J IMAG SYST TECH, V14, P16, DOI 10.1002/ima.20002
   ROBERTSON R, 1968, J OPT SOC AM, V58
   Sharpe LT, 1998, J NEUROSCI, V18, P10053
   SMITH VC, 1975, VISION RES, V15, P161, DOI 10.1016/0042-6989(75)90203-5
   SUSSTRUNK S, 2001, P SPIE EL IM 2001, V4300
   Tseng BL, 2004, IEEE MULTIMEDIA, V11, P42, DOI 10.1109/MMUL.2004.1261105
   van Beek P, 2003, IEEE SIGNAL PROC MAG, V20, P40, DOI 10.1109/MSP.2003.1184338
   VETRO A, 2003, IEEE SIGNAL PROCESS, V20
   Viénot F, 2001, P SOC PHOTO-OPT INS, V4300, P199
   Walraven J, 1997, FIFTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS, AND APPLICATIONS, P17
   Yang S, 2003, IEEE IMAGE PROC, P453
   YANG S, 2002, REPORT CE VISUAL A 1
   2004, TR159388 ISO IEC TR
NR 33
TC 33
Z9 36
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 435
EP 445
DI 10.1109/TMM.2005.846801
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200006
DA 2024-07-18
ER

PT J
AU Bao, P
   Gourley, D
AF Bao, P
   Gourley, D
TI Real-time rendering of 3-D scenes using subband 3-D warping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compressed domain processing; low-power digital and analog circuits for
   multimedia; videoconferencing and collaboration; environment
AB Three techniques for increasing the frame rates of three-dimensional warping, namely subsampling, wavelet baseband, and wavelet transform (including high frequencies) are presented. Additionally noninteger and integer splats are applied for the techniques. The Haar baseband and subsampling methods using integer splats; both produce excellent frame rates and perceptual quality of warped images.
C1 Chinese Univ Hong Kong, Dept Informat Engn, Shatin, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Nanyang Technol Univ, Sch Engn, Singapore 2263, Singapore.
EM ASPBao@ntu.edu.sg; paulbao@ie.cuhk.edu.hk
CR [Anonymous], THESIS U N CAROLINA
   BEARDOW P, 2001, ENABLING WIRELESS IN
   CHANG C, 2002, P IEEE PAC RIM C MUL
   CHEN D, WAVELET BASED REAL T
   MARK WR, 1998, TR98011 U N CAR COMP
   McMillan L, 1997, IMAGE BASED APPROACH
   Rakkolainen I, 2001, COMPUT GRAPH-UK, V25, P619, DOI 10.1016/S0097-8493(01)00090-5
   STOLLNITZ EJ, 1995, IEEE COMPUT GRAPH, V15, P76, DOI 10.1109/38.376616
NR 8
TC 3
Z9 3
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 786
EP 790
DI 10.1109/TMM.2004.837248
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200001
DA 2024-07-18
ER

PT J
AU Hands, DS
AF Hands, DS
TI A basic multimedia quality model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia; objective measurement; perceptual quality
ID AUDIO QUALITY; PERCEPTION
AB This paper describes two experiments designed to develop a basic multimedia predictive quality metric. In Experiment 1, two head and shoulder audio-video sequences were used for test material. Experiment 2 used one of the head and shoulder sequences from Experiment 1 together with a different, high-motion sequence. In both experiments, subjects assessed the audio quality first, followed by the video quality and finally a third test evaluated multimedia quality. The results of these studies found that human subjects integrate audio and video quality together using a multiplicative rule. A regression analysis using the subjective quality test data from each experiment found that: 1) for head and shoulder content, both modalities contribute significantly to the predictive power of the resultant model, although audio quality is weighted slightly higher than video quality and 2) for high-motion content, video quality is weighted significantly higher than audio quality.
C1 British Telecommun PLC, Ipswich IP5 3RE, Suffolk, England.
C3 British Telecom (BT)
RP British Telecommun PLC, Ipswich IP5 3RE, Suffolk, England.
EM david.2.hands@bt.com
CR [Anonymous], FDN INFORMATION INTE
   *ANSI, 1995, TIA1594124 ANSI
   APTEKER RT, 1995, IEEE MULTIMEDIA, V2, P32, DOI 10.1109/93.410510
   Beerends JG, 1999, J AUDIO ENG SOC, V47, P355
   BEERENDS JG, 1992, J AUDIO ENG SOC, V40, P963
   Creusere CD, 2002, IEEE DATA COMPR CONF, P152, DOI 10.1109/DCC.2002.999953
   DIXON NF, 1980, PERCEPTION, V9, P719, DOI 10.1068/p090719
   Hayashi T., 1999, 1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451), P515, DOI 10.1109/MMSP.1999.794135
   Hollier MP, 1999, BT TECHNOL J, V17, P35, DOI 10.1023/A:1009666623193
   HOLLIER MP, 1996, P I ELECT ENG VISION, V141, P203
   JONES C, 1998, P IEEE 6 INT WORKSH
   KARUNASEKERA SA, 1995, IEEE T IMAGE PROCESS, V4, P713, DOI 10.1109/83.388074
   MACDIARMID IF, 1982, EBU REV TECHNICAL, V192, P70
   NARITA N, 1994, IEEE T BROADCAST, V40, P7, DOI 10.1109/11.272416
   Narita N, 1997, IEEE T BROADCAST, V43, P26, DOI 10.1109/11.566821
   RAN XN, 1995, IEEE T IMAGE PROCESS, V4, P401, DOI 10.1109/83.370671
   RIHS S, 1995, INFLUENCE AUDIO PERC
   Soh K., 2001, Transactions of the Institute of Electronics, Information and Communication Engineers A, VJ84-A, P1305
   Steinmetz R, 1996, IEEE J SEL AREA COMM, V14, P61, DOI 10.1109/49.481694
   Storms RL, 2000, PRESENCE-TELEOP VIRT, V9, P557, DOI 10.1162/105474600300040385
   Vahedian A., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P367, DOI 10.1109/ICIP.1999.822919
   Wijesekera D, 1999, MULTIMEDIA SYST, V7, P486, DOI 10.1007/s005300050149
   Yahya Y., 1999, Malaysian Journal of Computer Science, V12, P9
NR 23
TC 115
Z9 124
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 806
EP 816
DI 10.1109/TMM.2004.837233
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200004
DA 2024-07-18
ER

PT J
AU Lee, KM
   Street, WN
AF Lee, KM
   Street, WN
TI Cluster-driven refinement for content-based digital image retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE clustering; digital image retrieval; refinement; shape-based indexing;
   weighted distance
ID HOUGH TRANSFORM
AB Increasing application demands are pushing databases toward providing effective and efficient support for content-based retrieval over multimedia objects. In addition to adequate retrieval techniques, it is also important to enable some form of adaptation to users' specific needs. This paper introduces a new refinement method for retrieval based on the learning of the users' specific preferences. The proposed system indexes objects based on shape and groups them into a set of clusters, with each cluster represented by a prototype. Clustering constructs a taxonomy of objects by forming groups of closely-related objects. The proposed approach to learn the users' preferences is to refine corresponding clusters from objects provided by the users in the foreground, and to simultaneously adapt the database index in the background. Queries can be performed based solely on shape, or on a combination of shape with other features such as color. Our experimental results show that the system successfully adapts queries into databases with only a small amount of feedback from the users. The quality of the returned results is superior to that of a color-based query, and continues to improve with further use.
C1 Duksung Womens Univ, Dept Comp Sci, Seoul 132714, South Korea.
   Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA.
C3 Duksung Women's University; University of Iowa
RP Duksung Womens Univ, Dept Comp Sci, Seoul 132714, South Korea.
EM kmlee@duksung.ac.kr; nick-street@uiowa.edu
CR [Anonymous], 1996, CUCS00596 COL U
   Aslandogan YA, 1999, IEEE T KNOWL DATA EN, V11, P56, DOI 10.1109/69.755615
   Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   DeMarsicoi M, 1997, IMAGE VISION COMPUT, V15, P119, DOI 10.1016/S0262-8856(96)01114-6
   Enser P. G. B., 1993, Journal of Document and Text Management, V1, P25
   Kassim AA, 1999, IMAGE VISION COMPUT, V17, P737, DOI 10.1016/S0262-8856(98)00156-5
   Keister L.H., 1994, CHALLENGES INDEXING, P7
   Lee KM, 2003, MACH VISION APPL, V13, P222, DOI 10.1007/s00138-002-0061-6
   Lee KM, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P64, DOI 10.1109/WACV.2000.895404
   Lee KM, 2002, PATTERN RECOGN LETT, V23, P865, DOI 10.1016/S0167-8655(01)00161-1
   Loncaric S, 1998, PATTERN RECOGN, V31, P983, DOI 10.1016/S0031-2023(97)00122-2
   Minka TP, 1997, PATTERN RECOGN, V30, P565, DOI 10.1016/S0031-3203(96)00113-6
   Nastar C, 1998, PROC CVPR IEEE, P547, DOI 10.1109/CVPR.1998.698659
   Pass G, 1999, MULTIMEDIA SYST, V7, P234, DOI 10.1007/s005300050125
   Roth I., 1995, Perception and Representation. Current Issues
   Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413
   RUI Y, 1998, P IS T SPIE STOR RET, V4, P25
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tan KL, 2000, DATA KNOWL ENG, V32, P271, DOI 10.1016/S0169-023X(99)00039-7
   Yoshitaka A, 1999, IEEE T KNOWL DATA EN, V11, P81, DOI 10.1109/69.755617
NR 24
TC 15
Z9 18
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 817
EP 827
DI 10.1109/TMM.2004.837235
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200005
DA 2024-07-18
ER

PT J
AU Mojsilovic, A
   Rogowitz, BE
AF Mojsilovic, A
   Rogowitz, BE
TI Semantic metric for image library exploration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image retrieval; semantic modeling
ID RETRIEVAL; VOCABULARY; GRAMMAR
AB We propose a method for semantic categorization and retrieval of photographic images based on low-level image descriptors derived from perceptual experiments. The method applies multidimensional scaling and hierarchical clustering analysis to identify candidate semantic categories into which human observers organize images. Through a series of subjective experiments we refine our definition of these categories and select a set of low-level image features that uniquely describe them. We then devise a new image similarity metric and develop a prototype system, which identifies the semantic category of the image and retrieves similar images from the database. We tested the metric on a set of new images and compared the categorization results with that of human observers. Our results provide a good match to human performance, thus validating the use of human judgments to develop semantic descriptors. Our method can be used for the enhancement of current retrieval methods, better organization of image/video databases, and the development of more intuitive navigation schemes, browsing methods and user interfaces.
C1 IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
C3 International Business Machines (IBM)
RP IBM Corp, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM aleksand@us.ibm.com; rogowitz@us.ibm.com
CR [Anonymous], 1955, 553 NBS
   BACH JR, 1993, IEEE T KNOWL DATA EN, V5, P619, DOI 10.1109/69.234774
   CHANG SF, 1995, P IEEE INT C IM PROC, P531
   Comaniciu D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1197, DOI 10.1109/ICCV.1999.790416
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   FERMAN AM, 1999, P IEEE INT C IM PROC
   Fleck M M, 1996, P EUR C COMP VIS, P593, DOI DOI 10.1007/3-540-61123-1_173
   GOMES J, 2002, P EUR C COMP VIS ECC
   Hermes T., 1995, Proceedings of the SPIE - The International Society for Optical Engineering, V2420, P394, DOI 10.1117/12.205310
   Li YB, 1997, P SOC PHOTO-OPT INS, V3022, P340, DOI 10.1117/12.263422
   MINKA T, 1996, 365 MIT MED LAB
   Mojsilovic A, 2000, IEEE T IMAGE PROCESS, V9, P38, DOI 10.1109/83.817597
   Mojsilovic A, 2000, IEEE T IMAGE PROCESS, V9, P417, DOI 10.1109/83.826779
   MOJSILOVIC A, IN PRESS IEEE T IMAG
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   NIBLACK W, 1994, P SPIE STOR RETR IM, P172
   ROGOWITZ B, 1997, P SPIE
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Szummer M, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P42, DOI 10.1109/CAIVD.1998.646032
   TAMURA H, 1978, IEEE T SYST MAN CYB, V8, P460, DOI 10.1109/TSMC.1978.4309999
   VALIAYA A, 1988, P IEEE WORKSH CONT B, P3
   Wang J.Z., 2001, IEEE Transactions on Pattern Analysis and Machine Intelligence, V23
   ZHU W, 1998, P INT WORKSH CONT BA, P31
NR 23
TC 22
Z9 27
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 828
EP 838
DI 10.1109/TMM.2004.839607
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200006
DA 2024-07-18
ER

PT J
AU Väänänen, R
   Huopaniemi, J
AF Väänänen, R
   Huopaniemi, J
TI Advanced AudioBIFS:: Virtual acoustics modeling in MPEG-4 scene
   description
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE MPEG-4; scene description languages; virtual acoustics; 3-D sound
ID MULTIMEDIA
AB We present the virtual acoustics modeling framework that is a part of the MPEG-4 standard. A scene description language called the BInary Format for Scenes (BIFS) is defined within MPEG-4 for making multimedia presentations that include various types of audio and visual data. BIFS also provides means for creating three-dimensional (3-D) virtual worlds or scenes, where visual and sound objects can be positioned and given temporal behavior. Local interaction between the user and the scene can be added to MPEG-4 applications. Typically the user can navigate in a 3-D scene so that it is viewed from different positions. In case that there are sound source objects in a scene, the sounds may be spatialized so that they are heard coming from the positions defined for them. A subset of BIFS, called the Advanced AudioBIFS aims at enhanced modeling of 3-D sound environments. In this framework, sounds can be given positions, and also the virtual environment where they appear can be associated with acoustic properties that allow modeling of phenomena such as air absorption, Doppler effect, sound reflections, and reverberation. These features can be used for adding room acoustic effects to sound in the MPEG-4 terminal, and for creating immersive 3-D audiovisual scenes.
C1 Aalto Univ, Lab Acoust & Audio Signal Proc, FIN-02150 Espoo, Finland.
   Nokia Res Ctr, Nokia Grp, FIN-00045 Helsinki, Finland.
C3 Aalto University; Nokia Corporation; Nokia Finland; Siemens AG; Nokia
   Siemens Networks
RP Nokia Res Ctr, Multimedia Technol Lab, FIN-00045 Helsinki, Finland.
EM riitta.vaananen@nokia.com; jyri.huopaniemi@nokia.com
CR [Anonymous], 2002, MPEG 4 BOOK
   [Anonymous], 147721 ISOIEC
   BEGAULT D, 1994, 3 D SOUND VIRTUAL EA
   BORISH J, 1984, J ACOUST SOC AM, V75, P1827, DOI 10.1121/1.390983
   Brandenburg K, 2000, SIGNAL PROCESS-IMAGE, V15, P423, DOI 10.1016/S0923-5965(99)00056-9
   *CARROUSO, 2002, CREAT ASS REND REAL
   *ISO IEC, 1992, 11172 ISOIEC
   *ISO IEC, 2000, 14496 ISOIEC
   *ISO IEC, 1994, 13818 ISOIEC
   *ISO TC, 1993, 43 ISOTC
   Jot J.-M., 1997, P ICMC, P236
   Jot JM, 1999, MULTIMEDIA SYST, V7, P55, DOI 10.1007/s005300050111
   JOT JM, 1995, P INT COMP MUS C BAN, P294
   *MICR, 2002, DIR 8 1 DIR AUD
   Savioja L, 1999, J AUDIO ENG SOC, V47, P675
   Scheirer ED, 1999, IEEE T MULTIMEDIA, V1, P237, DOI 10.1109/6046.784463
   VAANANEN R, P AES 22 INT C VIRT, P289
   WALSH AE, 2001, MPEG 4 JUMPSTART
   *WEB3D CONS, 2001, INF TECHN COMP GRAPH
   WENZEL EM, 1994, P INT C AUD DISPL IC, V18, P127
   2000, 109 AES CONV LOS ANG
NR 21
TC 11
Z9 20
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 661
EP 675
DI 10.1109/TMM.2004.834864
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800001
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Xiang, Z
   Zhu, WW
   Gao, LX
AF Zhang, Q
   Xiang, Z
   Zhu, WW
   Gao, LX
TI Cost-based cache replacement and server selection for multimedia proxy
   across wireless Internet
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE caching; multimedia proxy; replacement policy; server selection;
   streaming media; wireless Internet
AB Multimedia proxy plays an important role in multimedia streaming over wireless Internet. Since wireless network exhibits different characteristics from the Internet, multimedia proxy caching over wireless Internet faces additional challenges. In this paper, we present a study of cache replacement for a single server and server selection for multiple servers across wireless Internet. By considering multiple objectives of multimedia proxy, we design a unified cost metric to measure proxy performance in wireless Internet. Based on the defined unified cost metric, we propose a novel replacement algorithm for single-server and a new server-selection policy for multiple servers to improve the end-to-end performance such as throughput, media quality, and start-up latency. To effectively handle errors occurred on wireless link, channel-adaptive unequal error protection is deployed according to distinct quality of service requirements of layered or scalable media. Simulation results demonstrate that our approaches achieve significantly better performance than the known cache-replacement algorithms and sever selection schemes, respectively.
C1 Microsoft Res Asia, Beijing 100080, Peoples R China.
   Univ Massachusetts, Amherst, MA 01002 USA.
C3 Microsoft Research Asia; Microsoft; University of Massachusetts System;
   University of Massachusetts Amherst
RP Microsoft Res Asia, Beijing 100080, Peoples R China.
EM qianz@microsoft.com; i-zxiang@microsoft.com; wwzhu@microsoft.com;
   lgao@ecs.umass.edu
RI Yan, Miaochen/JLL-5061-2023; Zhang, Qian/B-9058-2009
OI Zhang, Qian/0000-0001-9205-1881
CR BRESLAU L, 1999, P IEEE INFOCOM NEW Y
   CAO P, 1997, 2 WEB CACH WORKSH BO
   Carter RL, 1997, IEEE INFOCOM SER, P1014, DOI 10.1109/INFCOM.1997.631117
   CHOU H, 1985, P 11 VLDB C STOCKH S
   CROVELLA ME, 1995, P 3 IEEE WORKSH ARCH, P158
   DAN A, 1990, PERF E R SI, V18, P143, DOI 10.1145/98460.98525
   Dykes S. G., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1361, DOI 10.1109/INFCOM.2000.832533
   Fan L, 1999, PERFORMANCE EVALUATION REVIEW, SPECIAL ISSUE, VOL 27 NO 1, JUNE 1999, P178, DOI 10.1145/301453.301557
   FU Z, 1999, P 7 ACM INT C MULT, P469
   LI S, 1999, JTC1SC29WG11 ISOIEC
   Low CP, 2000, GLOB TELECOMM CONF, P1329, DOI 10.1109/GLOCOM.2000.891846
   MARGARITIDIS M, 2000, P IEEE ICME 00, V3, P1241
   NAM DH, P IEEE TENCON 1999, P966
   Oneil Elizabeth J, 1993, ACM SIGMOD RECORD, V22, P297
   REJAIE R, 1999, P 4 INT WEB CACH WOR
   SAYAL A, 1998, P INT SERV PERF WORK
   TEWARI R, 1998, P SPIE ACM C MULT CO
   VASS J, 1999, P IEEE WORKSH MULT S, P45
   Yu F, 2003, IEEE T CIRC SYST VID, V13, P257, DOI 10.1109/TCSVT.2003.809829
   Zhang Q, 2001, IEEE T MULTIMEDIA, V3, P339, DOI 10.1109/6046.944477
   ZHANG Q, 2000, P IEEE WCNC 2000 CHI
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 22
TC 21
Z9 26
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 587
EP 598
DI 10.1109/tmm.2004.830816
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800007
OA Green Submitted
DA 2024-07-18
ER

EF