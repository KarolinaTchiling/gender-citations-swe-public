FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Liu, JY
   Li, ST
   Dian, RW
   Song, Z
AF Liu, Jinyang
   Li, Shutao
   Dian, Renwei
   Song, Ze
TI Focus Relationship Perception for Unsupervised Multi-Focus Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Loss measurement; Data mining; Visual
   perception; Tensors; Optimization; Multi-focus image fusion;
   unsupervised learning
ID GENERATIVE ADVERSARIAL NETWORK; CONVOLUTIONAL NEURAL-NETWORK;
   CONSTRAINTS; TRANSFORM; ALGORITHM; FRAMEWORK; ENSEMBLE
AB Multi-focus image fusion can extract the focus regions from different source images and combine them into a fully clear image. Existing unsupervised methods typically use gradient information to measure the focus regions in images and generate a fusion weight map, but ordinary gradient operators are difficult to measure information accurately in regions with weaker textures. In addition, using only gradient information as a constraint cannot make the model fully distinguish all the focus regions in the image, which seriously restricts the clarity of the fusion image. To address these issues, a novel unsupervised multi-focus image fusion method is proposed in this paper. Specifically, a neighborhood information fusion network is designed to generate an initial fusion weight map. It can capture features within different neighborhood ranges at once, which enhances the information association between different regions. In addition, to further improve the feature extraction ability of the model in the regions with low texture information, a local difference evaluation loss function is proposed. It is combined with the gradient measure loss function to constrain the network. Finally, a fusion weight optimization module is proposed to improve the clarity of the fusion image in the repeated defocusing regions and overexposed regions of different source images, which redistributes the weights of different source images. The proposed fusion method is compared with advanced methods on three public multi-focus datasets. Experimental results indicate that the proposed method has achieved better performance in qualitative and quantitative aspects.
C1 [Liu, Jinyang; Li, Shutao; Song, Ze] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
   [Liu, Jinyang; Li, Shutao; Dian, Renwei; Song, Ze] Hunan Univ, Key Lab Visual Percept & Artificial Intelligence H, Changsha 410082, Peoples R China.
   [Dian, Renwei] Hunan Univ, Sch Robot, Changsha 410082, Peoples R China.
C3 Hunan University; Hunan University; Hunan University
RP Li, ST (corresponding author), Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Peoples R China.
EM jinyangliu@hnu.edu.cn; shutao_li@hnu.edu.cn; drw@hnu.edu.cn;
   songz@hnu.edu.cn
RI liu, jinyang/KHX-8400-2024; Li, Shutao/Y-3102-2019
OI Li, Shutao/0000-0002-0585-9848; Song, Ze/0000-0002-6585-765X; Liu,
   Jinyang/0000-0003-0429-7422
FU National Natural Science Foundation of China
FX No Statement Available
CR Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Cao L, 2015, IEEE SIGNAL PROC LET, V22, P220, DOI 10.1109/LSP.2014.2354534
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Chen Y, 2009, IMAGE VISION COMPUT, V27, P1421, DOI 10.1016/j.imavis.2007.12.002
   Gao Y, 2018, J VIS COMMUN IMAGE R, V55, P586, DOI 10.1016/j.jvcir.2018.07.004
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Huang J, 2020, NEURAL COMPUT APPL, V32, P15119, DOI 10.1007/s00521-020-04863-1
   Jung H, 2020, IEEE T IMAGE PROCESS, V29, P3845, DOI 10.1109/TIP.2020.2966075
   Li HG, 2019, IEEE SENS J, V19, P9755, DOI 10.1109/JSEN.2019.2928818
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li JX, 2020, IEEE T IMAGE PROCESS, V29, P4816, DOI 10.1109/TIP.2020.2976190
   Li M, 2006, PATTERN RECOGN LETT, V27, P1948, DOI 10.1016/j.patrec.2006.05.004
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P105, DOI 10.1109/TCSVT.2021.3056725
   Liu SM, 2020, IEEE T CIRC SYST VID, V30, P1374, DOI 10.1109/TCSVT.2019.2901809
   Liu Y, 2022, INFORM FUSION, V86-87, P1, DOI 10.1016/j.inffus.2022.06.001
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Ma HY, 2020, IEEE T IMAGE PROCESS, V29, P8668, DOI 10.1109/TIP.2020.3018261
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Mustafa HT, 2019, IMAGE VISION COMPUT, V85, P26, DOI 10.1016/j.imavis.2019.03.001
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Piella G., 2003, Information Fusion, V4, P259, DOI 10.1016/S1566-2535(03)00046-0
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Shreyamsha Kumar BK, 2015, SIGNAL IMAGE VIDEO P, V9, P1193, DOI 10.1007/s11760-013-0556-9
   Song QY, 2023, IEEE T NEUR NET LEAR, V34, P10028, DOI 10.1109/TNNLS.2022.3163771
   Song Y, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-3, P401, DOI 10.1109/ROBIO.2006.340210
   Sujatha K, 2018, MULTIMED TOOLS APPL, V77, P1735, DOI 10.1007/s11042-016-4312-3
   [唐霖峰 Tang Linfeng], 2023, [中国图象图形学报, Journal of Image and Graphics], V28, P3
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Wang ZS, 2022, IEEE T CIRC SYST VID, V32, P3360, DOI 10.1109/TCSVT.2021.3109895
   Xiao B, 2021, IEEE T IMAGE PROCESS, V30, P163, DOI 10.1109/TIP.2020.3033158
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu S, 2020, Arxiv, DOI arXiv:2002.04780
   Yang B, 2012, INFORM FUSION, V13, P10, DOI 10.1016/j.inffus.2010.04.001
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P4771, DOI 10.1109/TCSVT.2021.3054584
   Yang Y, 2019, IEEE T COMPUT IMAG, V5, P262, DOI 10.1109/TCI.2018.2889959
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang XC, 2022, IEEE T PATTERN ANAL, V44, P4819, DOI 10.1109/TPAMI.2021.3078906
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang Y, 2017, INFORM FUSION, V35, P81, DOI 10.1016/j.inffus.2016.09.006
   Zhao F, 2023, IEEE T MULTIMEDIA, V25, P966, DOI 10.1109/TMM.2021.3134565
   Zhao WD, 2019, IEEE T CIRC SYST VID, V29, P1102, DOI 10.1109/TCSVT.2018.2821177
   Zhao ZX, 2022, IEEE T CIRC SYST VID, V32, P1186, DOI 10.1109/TCSVT.2021.3075745
NR 52
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6155
EP 6165
DI 10.1109/TMM.2023.3347099
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600047
DA 2024-08-05
ER

PT J
AU Liu, MK
   Jiao, LC
   Liu, X
   Li, LL
   Liu, F
   Yang, SY
   Zhang, XR
AF Liu, Mengkun
   Jiao, Licheng
   Liu, Xu
   Li, Lingling
   Liu, Fang
   Yang, Shuyuan
   Zhang, Xiangrong
TI Bio-Inspired Multi-Scale Contourlet Attention Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Biological system modeling; Biology; Feature extraction;
   Brain modeling; Computational modeling; Neurons; Biologically inspired
   visual model; contourlet pooling; multi-directions; multi-scales;
   Shannon block attention module
ID CONVOLUTIONAL NEURAL-NETWORK; OBJECT RECOGNITION; SCENE CLASSIFICATION;
   RECEPTIVE-FIELDS; TEXTURE; FEATURES; IMAGES; MODEL; SELECTIVITY;
   FRAMEWORK
AB Inspired by the sparse and hierarchical features representation in the ventral stream of the human visual system, the biologically inspired multi-scale contourlet attention network (BMCAnet) is proposed to extract robust discriminative features. First, we constructed the multi-scale contourlet filter banks as a population of neurons in the primary visual cortex (V1), and extracted sparse features in a multi-scale and multi-direction way. It simulated a simple cell in V1 that responds to stimuli in a specific direction. Second, in order to refine contourlet features adaptively, the Shannon block attention module (SBAM) is introduced by integrating Shannon entropy as the third branch of the channel attention module (CAM), thus the weights of contourlet coefficients can be learned adaptively. Third, the responses of the spatial and spectral features are pooled by the proposed contourlet pooling layer to obtain the invariant structure features with the specified rules, which roughly stimulate the pooling process of complex cells in the V1 area. Last, the combination of global average pooling (GAP) and full connection (FC) is used for classification. The competitive results on eight databases demonstrate that the BMCAnet can effectively extract sparse and effective features for the classification tasks.
C1 [Liu, Mengkun; Jiao, Licheng; Liu, Xu; Li, Lingling; Liu, Fang; Yang, Shuyuan; Zhang, Xiangrong] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Joint Int Res Lab Intelligent Percept & Computat, Sch Artificial Intelligence,Minist Educ,Int Res C, Xian 710071, Peoples R China.
C3 Xidian University
RP Jiao, LC (corresponding author), Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Joint Int Res Lab Intelligent Percept & Computat, Sch Artificial Intelligence,Minist Educ,Int Res C, Xian 710071, Peoples R China.
EM mengkunliu31@163.com; lchjiao@mail.xidian.edu.cn; xuliu361@163.com;
   llli@xidian.edu.cn; f63liu@163.com; syyang@xidian.edu.cn;
   xrzhang@mail.xidian.edu.cn
OI liu, mengkun/0000-0002-5696-5237
FU Key Scientific Technological Innovation Research Project by Ministry of
   Education
FX No Statement Available
CR Alhichri H, 2023, ANN GIS, V29, P121, DOI 10.1080/19475683.2023.2165544
   Alhichri H, 2021, IEEE ACCESS, V9, P14078, DOI 10.1109/ACCESS.2021.3051085
   Alkhatib M, 2019, IEEE T IMAGE PROCESS, V28, P5407, DOI 10.1109/TIP.2019.2916742
   Alotaibi A, 2020, INT J ADV COMPUT SC, V11, P293
   [Anonymous], 2013, Digital color: Acquisition, perception, coding and rendering
   Anwer RM, 2021, NEURAL PROCESS LETT, V53, P1523, DOI 10.1007/s11063-021-10463-4
   Anwer RM, 2018, ISPRS J PHOTOGRAMM, V138, P74, DOI 10.1016/j.isprsjprs.2018.01.023
   Aradhya AMS, 2022, INFORM SCIENCES, V607, P638, DOI 10.1016/j.ins.2022.05.100
   BAMBERGER RH, 1992, IEEE T SIGNAL PROCES, V40, P882, DOI 10.1109/78.127960
   Bi Q, 2020, IEEE T IMAGE PROCESS, V29, P4911, DOI 10.1109/TIP.2020.2975718
   Brincat SL, 2004, NAT NEUROSCI, V7, P880, DOI 10.1038/nn1278
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chaltyopadhyay N., 2022, Int. J. Res. Appl. Sci. Eng. Technol, V10, P1317
   Chang JR, 2015, Arxiv, DOI arXiv:1511.02583
   Chen F. X., 2017, P AAAI SPRING S SER, P541
   Chen HY, 2023, IEEE T MULTIMEDIA, V25, P5826, DOI 10.1109/TMM.2022.3199556
   Chen J, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3063287
   Cheng G, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3081421
   Cheng G, 2018, IEEE T GEOSCI REMOTE, V56, P2811, DOI 10.1109/TGRS.2017.2783902
   Cheng G, 2017, IEEE GEOSCI REMOTE S, V14, P1735, DOI 10.1109/LGRS.2017.2731997
   Cheng G, 2017, P IEEE, V105, P1865, DOI 10.1109/JPROC.2017.2675998
   Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007
   Cimpoi M, 2014, PROC CVPR IEEE, P3606, DOI 10.1109/CVPR.2014.461
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dana KJ, 1999, ACM T GRAPHIC, V18, P1, DOI 10.1145/300776.300778
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Do MN, 2002, IEEE IMAGE PROC, P357
   Dong YS, 2021, IEEE T CIRC SYST VID, V31, P1684, DOI 10.1109/TCSVT.2020.3014526
   Donoho DL, 1998, IEEE T INFORM THEORY, V44, P2435, DOI 10.1109/18.720544
   Duan C., 2019, P 2 INT C INF SCI EL, P18
   Fang J, 2019, IEEE T GEOSCI REMOTE, V57, P7492, DOI 10.1109/TGRS.2019.2913816
   Florindo JB, 2021, EXPERT SYST APPL, V179, DOI 10.1016/j.eswa.2021.115027
   Fraz H., 2009, P IEEE 10 ANN WIR MI, P1
   Fujieda S., 2018, ARXIV
   Elmegreen BG, 2015, Arxiv, DOI arXiv:1511.05633
   Ganj A., 2023, Iranian J. Sci. Technol., Trans. Elect. Eng., P1
   Gao J, 2024, IEEE T NEUR NET LEAR, V35, P7999, DOI 10.1109/TNNLS.2022.3223212
   Gardner E., 2000, Principlesof Neural Science, V4, P451
   Graham B, 2015, Arxiv, DOI arXiv:1412.6071
   Gupta D, 2018, BIOCYBERN BIOMED ENG, V38, P262, DOI 10.1016/j.bbe.2017.12.005
   Hansen T, 2004, NEURAL NETWORKS, V17, P647, DOI 10.1016/j.neunet.2004.04.002
   Hassanzadeh T, 2022, NEUROCOMPUTING, V488, P271, DOI 10.1016/j.neucom.2022.02.003
   Hayman E, 2004, LECT NOTES COMPUT SC, V2034, P253
   He NJ, 2020, IEEE T NEUR NET LEAR, V31, P1461, DOI 10.1109/TNNLS.2019.2920374
   He NJ, 2018, IEEE T GEOSCI REMOTE, V56, P6899, DOI 10.1109/TGRS.2018.2845668
   Le HT, 2022, IEEE T CIRC SYST VID, V32, P5331, DOI 10.1109/TCSVT.2022.3144184
   Hofmann M, 2022, IEEE T NEUR NET LEAR, V33, P3094, DOI 10.1109/TNNLS.2021.3050422
   Hu W., 2020, ADV NEURAL INFORM PR, P19111, DOI DOI 10.1007/978-981-4021-75-3_9
   Huang YZ, 2011, IEEE T SYST MAN CY B, V41, P1668, DOI 10.1109/TSMCB.2011.2158418
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229
   Ishii N, 2022, COMM COM INF SC, V1600, P15, DOI 10.1007/978-3-031-08223-8_2
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Ji Y., 2022, P INT C NEUR NETW IN, V12258, P178
   Jiang JM, 2021, IEEE T MULTIMEDIA, V23, P1454, DOI 10.1109/TMM.2020.2999183
   Jiang Y, 2024, IEEE T MULTIMEDIA, V26, P1920, DOI 10.1109/TMM.2023.3290432
   Khosrowabadi R, 2014, IEEE T NEUR NET LEAR, V25, P609, DOI 10.1109/TNNLS.2013.2280271
   Krishnan KG, 2016, PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS, SIGNAL PROCESSING AND NETWORKING (WISPNET), P1408, DOI 10.1109/WiSPNET.2016.7566368
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai QX, 2021, IEEE T MULTIMEDIA, V23, P2086, DOI 10.1109/TMM.2020.3007321
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464
   Lei CY, 2023, Arxiv, DOI arXiv:2304.00219
   Li J, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12091366
   Li WM, 2020, IEEE J-STARS, V13, P1986, DOI 10.1109/JSTARS.2020.2988477
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao Zhibin., 2016, P 2016 IEEE WINT C A, P1, DOI [DOI 10.1109/WACV.2016.7477624, 10.1109/WACV.2016.7477624]
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Liu MK, 2021, IEEE T NEUR NET LEAR, V32, P2636, DOI 10.1109/TNNLS.2020.3007412
   Liu Y, 2021, IEEE T CYBERNETICS, V51, P4439, DOI 10.1109/TCYB.2020.3035613
   Liu ZL, 2021, IEEE T SYST MAN CY-S, V51, P209, DOI 10.1109/TSMC.2020.3043147
   Lu XQ, 2019, IEEE T GEOSCI REMOTE, V57, P7894, DOI 10.1109/TGRS.2019.2917161
   Mallat S., 1999, A wavelet Tour of Signal Processing
   Mirsadeghi M, 2023, NEURAL COMPUT APPL, V35, P15891, DOI 10.1007/s00521-023-08567-0
   Mukherjee P, 2022, Arxiv, DOI arXiv:2204.11449
   Mutch J., 2006, IEEE COMP SOC C COMP, V1, P11, DOI DOI 10.1109/CVPR.2006.200
   Nakamura K, 2019, IEEE ACCESS, V7, P118857, DOI 10.1109/ACCESS.2019.2937139
   Neumann H., 2003, Handbook of Brain Theory and Neural Networks, P271
   Nuding U, 2007, BIOSYSTEMS, V89, P273, DOI 10.1016/j.biosystems.2006.04.025
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pasupathy A, 1999, J NEUROPHYSIOL, V82, P2490, DOI 10.1152/jn.1999.82.5.2490
   Pawan SJ, 2023, MACH VISION APPL, V34, DOI 10.1007/s00138-023-01401-6
   Plesovskaya Ekaterina, 2022, Procedia Computer Science, P368, DOI 10.1016/j.procs.2022.11.021
   Poggio T, 2014, Arxiv, DOI arXiv:1406.1770
   Potlapalli H, 1998, IEEE T IND ELECTRON, V45, P142, DOI 10.1109/41.661315
   Qi K, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13040569
   Quiroga RQ, 2009, CURR BIOL, V19, P1308, DOI 10.1016/j.cub.2009.06.060
   Raghavan K, 2021, IEEE T NEUR NET LEAR, V32, P4323, DOI 10.1109/TNNLS.2020.3017434
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Roig G., 2018, J. Vis., V18, P902
   Sadeghzadeh H, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-22291-0
   Saranya M., 2022, PROX INT C COMP COMM, P116
   Sato I, 2015, Arxiv, DOI arXiv:1505.03229
   Serre T, 2005, PROC CVPR IEEE, P994
   Shi CP, 2020, IEEE J-STARS, V13, P5194, DOI 10.1109/JSTARS.2020.3018307
   Siagian C, 2007, IEEE T PATTERN ANAL, V29, P300, DOI 10.1109/TPAMI.2007.40
   Singh A, 2022, ADV INTELL SYST COMP, V1371, P89, DOI 10.1007/978-981-16-3097-2_8
   Singh S, 2015, BIOMED SIGNAL PROCES, V18, P91, DOI 10.1016/j.bspc.2014.11.009
   Smith AT, 2001, CEREB CORTEX, V11, P1182, DOI 10.1093/cercor/11.12.1182
   Song TC, 2022, EXPERT SYST APPL, V191, DOI 10.1016/j.eswa.2021.116327
   Song TC, 2018, IEEE T CIRC SYST VID, V28, P1565, DOI 10.1109/TCSVT.2017.2671899
   Sun GC, 2022, APPL INTELL, V52, P3066, DOI 10.1007/s10489-021-02630-w
   Tao ZY, 2021, IEEE SIGNAL PROC LET, V28, P1215, DOI 10.1109/LSP.2021.3088052
   Vasilyev I., 2022, P IEEE 20 JUB WORLD
   Venkataraman SR, 2022, Arxiv, DOI arXiv:2210.11092
   Walther D, 2002, LECT NOTES COMPUT SC, V2525, P472
   Wan M., 2013, PROC INT C MACH LEA, P1058
   Wang QR, 2019, IEEE T MULTIMEDIA, V21, P1839, DOI 10.1109/TMM.2018.2890360
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xia GS, 2017, IEEE T GEOSCI REMOTE, V55, P3965, DOI 10.1109/TGRS.2017.2685945
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Xu KJ, 2022, IEEE T NEUR NET LEAR, V33, P5751, DOI 10.1109/TNNLS.2021.3071369
   Yang Y., 2010, P 18 SIGSPATIAL INT, DOI [10.1145/1869790.1869829, DOI 10.1145/1869790.1869829]
   Yu YL, 2020, IEEE T GEOSCI REMOTE, V58, P519, DOI 10.1109/TGRS.2019.2937830
   Yu YL, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10071158
   Zhang B, 2019, IEEE J-STARS, V12, P2636, DOI 10.1109/JSTARS.2019.2919317
   Zhang HA, 2023, IEEE T MULTIMEDIA, V25, P6958, DOI 10.1109/TMM.2022.3216477
   Zhang SP, 2017, IEEE T NEUR NET LEAR, V28, P2357, DOI 10.1109/TNNLS.2016.2586194
   Zhang W, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11050494
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zheng XT, 2022, IEEE T IMAGE PROCESS, V31, P4251, DOI 10.1109/TIP.2022.3177322
   Zou Q, 2015, IEEE GEOSCI REMOTE S, V12, P2321, DOI 10.1109/LGRS.2015.2475299
NR 123
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2824
EP 2837
DI 10.1109/TMM.2023.3304448
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400004
DA 2024-08-05
ER

PT J
AU Lu, JW
   Wang, GH
   Cai, Y
   Wu, X
AF Lu, Jianwei
   Wang, Guohua
   Cai, Yi
   Wu, Xin
TI Towards Automated Infographic Authoring From Natural Language Statement
   With Multiple Proportional Facts
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data visualization; Task analysis; Data models; Surveys; Semantics; Deep
   learning; Costs; Infographic; natural language processing; deep
   learning; visualization; automated infographic authoring
ID DESIGN
AB Infographics, which usually contain many well-designed visual elements, have significant advantages in delivering information efficiently and accurately. Previous research shows that proportion-related infographics make up the majority of all infographics. However, the creation of proportion-related infographics is difficult for general users. Recently, many researchers focus on generating infographics from the text with a single proportional fact. Our further research found that users tend to create infographics with multiple proportional facts. Existing research lacks modeling of relations between different facts, resulting in poor performance when generating infographics with multiple facts. In this paper, we model the relationship of different proportional facts based on the results of our investigation and design a deep learning-based model to classify them. At the same time, we also optimize the ability to extract multiple proportional facts from text. The experiments show that our model outperforms existing models when visualizing text with multiple proportional facts.
C1 [Lu, Jianwei; Wang, Guohua; Cai, Yi; Wu, Xin] South China Univ Technol, Sch Software Engn, Guangzhou 510642, Peoples R China.
   [Wu, Xin] South China Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510642, Peoples R China.
EM 202021046039@mail.scut.edu.cn; ghwang@scut.edu.cn; ycai@scut.edu.cn;
   sexinwu@mail.scut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], Jsverbalexpressions
   [Anonymous], 2013, PROC 1 INT C LEARN R
   [Anonymous], Regulex
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Artacho-Ramírez MA, 2008, INT J IND ERGONOM, V38, P942, DOI 10.1016/j.ergon.2008.02.020
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Brock J., 2016, Data Design-The Visual Display of Qualitative and Quantitative Information
   CASNER SM, 1991, ACM T GRAPHIC, V10, P111, DOI 10.1145/108360.108361
   Chen W, 2016, IEEE T MULTIMEDIA, V18, P2247, DOI 10.1109/TMM.2016.2614221
   Chen XJ, 2020, EXPERT SYST APPL, V141, DOI 10.1016/j.eswa.2019.112948
   Chen ZT, 2020, IEEE T VIS COMPUT GR, V26, P917, DOI 10.1109/TVCG.2019.2934810
   Cui WW, 2022, IEEE T VIS COMPUT GR, V28, P173, DOI 10.1109/TVCG.2021.3114856
   Cui WW, 2020, IEEE T VIS COMPUT GR, V26, P906, DOI 10.1109/TVCG.2019.2934785
   Dai AM, 2015, ADV NEUR IN, V28
   Desolneux A, 2004, THEORY DECIS LIB A, V38, P71
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Han X, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169
   Harrison L, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1187, DOI 10.1145/2702123.2702545
   Hirschman L., 2001, Natural Language Engineering, V7, P275, DOI 10.1017/S1351324901002807
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Lan Z., 2019, INT C LEARN REPRESEN
   Li Y., 2008, Proceedings of the 2008 Conference on Emperical Methods in Natural Language Processing, P21
   Lin J, 2023, IEEE T VIS COMPUT GR, V29, P1705, DOI 10.1109/TVCG.2021.3130071
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Lu M, 2020, PROCEEDINGS OF THE 2020 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI'20), DOI 10.1145/3313831.3376263
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Manolis Savva, 2011, P 24 ANN ACM S USER, P393
   Ouyang Long, 2022, Advances in Neural Information Processing Systems, V35, P27730, DOI 10.1177/01454455830072006
   Pennington J., 2014, P C EMP METH NAT LAN, P1532, DOI DOI 10.3115/V1/D14-1162
   Peters ME, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1499, DOI 10.5771/9783845286846
   Power BI, ABOUT US
   Qian CY, 2021, IEEE T VIS COMPUT GR, V27, P443, DOI 10.1109/TVCG.2020.3030448
   Qiu XP, 2020, SCI CHINA TECHNOL SC, V63, P1872, DOI 10.1007/s11431-020-1647-3
   Radford A., 2019, OpenAI blog, V1, P9
   Radford A, 2021, PR MACH LEARN RES, V139
   Rajpurkar P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P784
   Sadiku M. N. O.., 2016, International Journal of Engineering Research and Advanced Technology, V2, P11, DOI [10.31695/IJERAT, DOI 10.31695/IJERAT]
   Tableau, About us
   Tyagi A, 2022, COMPUT GRAPH FORUM, V41, P121, DOI 10.1111/cgf.14527
   Tyagi J., 2021, arXiv
   Vaswani A, 2017, ADV NEUR IN, V30
   Venngage, ABOUT US
   Visme, ABOUT US
   Wang Y, 2021, COMPUT GRAPH FORUM, V40, P507, DOI 10.1111/cgf.14325
   Wu A., 2021, IEEE Trans. Visual. Comput. Graph.
   Yin YF, 2022, IEEE T MULTIMEDIA, V24, P890, DOI 10.1109/TMM.2021.3060951
   Yuan LP, 2022, IEEE T VIS COMPUT GR, V28, P4252, DOI 10.1109/TVCG.2021.3085327
   Zandie R., 2020, ARXIV200302958, P276
   Zhang NY, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING: SYSTEM DEMONSTRATIONS, P1
   Zhang WQ, 2020, IEEE T MULTIMEDIA, V22, P1032, DOI 10.1109/TMM.2019.2935678
   Zhang ZY, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1441
   Zheng CM, 2021, IEEE T MULTIMEDIA, V23, P2520, DOI 10.1109/TMM.2020.3013398
NR 54
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7101
EP 7113
DI 10.1109/TMM.2024.3360722
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000030
DA 2024-08-05
ER

PT J
AU Nie, WZ
   Wen, X
   Liu, J
   Chen, JW
   Wu, JC
   Jin, GQ
   Lu, J
   Liu, AA
AF Nie, Weizhi
   Wen, Xin
   Liu, Jing
   Chen, Jiawei
   Wu, Jiancan
   Jin, Guoqing
   Lu, Jing
   Liu, An-An
TI Knowledge-Enhanced Causal Reinforcement Learning Model for Interactive
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data models; Recommender systems; Training; Reinforcement learning;
   Estimation; Correlation; Computational modeling; Causal inference; data
   sparsity; group performance; interactive recommender system; knowledge
   graph; offline reinforcement learning
AB Owing to its inherently dynamic nature and economical training cost, offline reinforcement learning (RL) is typically employed to implement an interactive recommender system (IRS). A crucial challenge in offline RL-based IRSs is the data sparsity issue, i.e., it is hard to mine user preferences well from the limited number of user-item interactions. In this article, we propose a knowledge-enhanced causal reinforcement learning model (KCRL) to mitigate data sparsity in IRSs. We make technical extensions to the offline RL framework in terms of the reward function and state representation. Specifically, we first propose a group preference-injected causal user model (GCUM) to learn user satisfaction (i.e., reward) estimation. We introduce beneficial group preference information, namely, the group effect, via causal inference to compensate for incomplete user interests extracted from sparse data. Then, we learn the RL recommendation policy with the reward given by the GCUM. We propose a knowledge-enhanced state encoder (KSE) to generate knowledge-enriched user state representations at each time step, which is assisted by a self-constructed user-item knowledge graph. Extensive experimental results on real-world datasets demonstrate that our model significantly outperforms the baselines.
C1 [Nie, Weizhi; Wen, Xin; Liu, Jing; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Chen, Jiawei] Zhejiang Univ, Hangzhou 310058, Peoples R China.
   [Wu, Jiancan] Univ Sci & Technol China, Hefei 230026, Peoples R China.
   [Jin, Guoqing] Peoples Daily Online, State Key Lab Commun Content Cognit, Beijing 100733, Peoples R China.
   [Lu, Jing] Kuaishou Technol, Beijing 100084, Peoples R China.
   [Liu, An-An] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
C3 Tianjin University; Zhejiang University; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Lu, J (corresponding author), Kuaishou Technol, Beijing 100084, Peoples R China.; Liu, AA (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230088, Peoples R China.
EM weizhinie@tju.edu.cn; wenxin113@tju.edu.cn; jliu_tju@tju.edu.cn;
   sleepyhunt@zju.edu.cn; wujcan@gmail.com; jinguoqing@people.cn;
   lvjing06@kuaishou.com; anan0422@gmail.com
RI Wen, Xin/GRJ-9085-2022
OI Wen, Xin/0000-0003-3070-4482; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China
FX No Statement Available
CR Bai J., 2019, NEURIPS, P10734
   Bai L, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534101
   Bordes A., 2013, Advances in Neural Information Processing Systems, V26
   Chen HK, 2019, AAAI CONF ARTIF INTE, P3312
   Chen M, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P456, DOI 10.1145/3289600.3290999
   Chen XC, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207010
   Chen XS, 2019, 36 INT C MACHINE LEA, V97
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Dadoun A, 2019, COMPANION OF THE WORLD WIDE WEB CONFERENCE (WWW 2019 ), P896, DOI 10.1145/3308560.3316535
   Ding L, 2022, AAAI CONF ARTIF INTE, P11864
   Dulac-Arnold G, 2016, Arxiv, DOI arXiv:1512.07679
   Fu MS, 2022, IEEE T CYBERNETICS, V52, P12028, DOI 10.1109/TCYB.2021.3089941
   Gao CM, 2024, ACM T INFORM SYST, V42, DOI 10.1145/3594871
   Gao CM, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P540, DOI 10.1145/3511808.3557220
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Hidasi B., 2016, ICLR POSTER
   Hirano K, 2003, ECONOMETRICA, V71, P1161, DOI 10.1111/1468-0262.00442
   Hong DC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1721, DOI 10.1145/3397271.3401225
   Ji GL, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P687
   Kingma D. P., 2015, P INT C LEARN REPR, P1
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Koren Y, 2008, KDD, P426
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI [10.1145/1772690.1772758, DOI 10.1145/1772690.1772758]
   Lin XR, 2022, AAAI CONF ARTIF INTE, P1610
   Lin YK, 2015, AAAI CONF ARTIF INTE, P2181
   Liu Fan, 2023, IEEE Transactions on Multimedia, P7149, DOI 10.1109/TMM.2022.3217449
   Liu F, 2020, NEUROCOMPUTING, V417, P255, DOI 10.1016/j.neucom.2020.07.057
   Luo C, 2014, IEEE DATA MINING, P917, DOI 10.1109/ICDM.2014.64
   Miki T, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abk2822
   Mnih V, 2013, Arxiv, DOI arXiv:1312.5602
   Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579
   Nan GS, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9683
   Pearl J., 2009, Causality, DOI DOI 10.1017/CBO9780511803161
   Qin W, 2023, IEEE T MULTIMEDIA, V25, P1033, DOI 10.1109/TMM.2021.3136717
   RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350
   Schnabel T, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P513, DOI 10.1145/3159652.3159700
   Schnabel T, 2016, PR MACH LEARN RES, V48
   Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Shani G, 2005, J MACH LEARN RES, V6, P1265
   Si ZH, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P224, DOI 10.1145/3485447.3511951
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HW, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3307, DOI 10.1145/3308558.3313417
   Wang HW, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P417, DOI 10.1145/3269206.3271739
   Wang HZ, 2017, AAAI CONF ARTIF INTE, P2695
   Wang K, 2021, AAAI CONF ARTIF INTE, V35, P4427
   Wang X, 2019, AAAI CONF ARTIF INTE, P5329
   Wang ZL, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2195, DOI 10.1145/3485447.3512092
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xin X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P125, DOI 10.1145/3331184.3331188
   Xu Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2227, DOI 10.1145/2783258.2788602
   Yao LY, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3444944
   Yue Yisong, 2009, PROC ICML, P1201
   Zhang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P41, DOI 10.1145/3404835.3462892
   Zhang Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P11, DOI 10.1145/3404835.3462875
   Zhao H, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P635, DOI 10.1145/3097983.3098063
   Zhao XY, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P95, DOI 10.1145/3240323.3240374
   Zhao XX, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1411, DOI 10.1145/2505515.2505690
   Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994
   Zhou SJ, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P179, DOI 10.1145/3397271.3401174
NR 61
TC 2
Z9 2
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1129
EP 1142
DI 10.1109/TMM.2023.3276505
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700016
DA 2024-08-05
ER

PT J
AU Rao, J
   Meng, X
   Ding, L
   Qi, SH
   Liu, XB
   Zhang, M
   Tao, DC
AF Rao, Jun
   Meng, Xv
   Ding, Liang
   Qi, Shuhan
   Liu, Xuebo
   Zhang, Min
   Tao, Dacheng
TI Parameter-Efficient and Student-Friendly Knowledge Distillation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Smoothing methods; Knowledge transfer; Data models; Adaptation
   models; Predictive models; Knowledge engineering; Knowledge
   distillation; parameter-efficient; image classification
AB Pre-trained models are frequently employed in multimodal learning. However, these models have too many parameters and need too much effort to fine-tune the downstream tasks. Knowledge distillation (KD) is a method to transfer knowledge using the soft label from this pre-trained teacher model to a smaller student, where the parameters of the teacher are fixed (or partially) during training. Recent studies show that this mode may cause difficulties in knowledge transfer due to the mismatched model capacities. To alleviate the mismatch problem, adjustment of temperature parameters, label smoothing and teacher-student joint training methods (online distillation) to smooth the soft label of a teacher network, have been proposed. But those methods rarely explain the effect of smoothed soft labels to enhance the KD performance. The main contributions of our work are the discovery, analysis, and validation of the effect of the smoothed soft label and a less time-consuming and adaptive transfer of the pre-trained teacher's knowledge method, namely PESF-KD by adaptive tuning soft labels of the teacher network. Technically, we first mathematically formulate the mismatch as the sharpness gap between teacher's and student's predictive distributions, where we show such a gap can be narrowed with the appropriate smoothness of the soft label. Then, we introduce an adapter module for the teacher and only update the adapter to obtain soft labels with appropriate smoothness. Experiments on various benchmarks including CV and NLP show that PESF-KD can significantly reduce the training cost while obtaining competitive results compared to advanced online distillation methods.
C1 [Rao, Jun] JD Explore Acad, Beijing, Peoples R China.
   [Rao, Jun; Meng, Xv; Qi, Shuhan; Liu, Xuebo; Zhang, Min] Harbin Inst Technol, Shenzhen 518055, Peoples R China.
   [Ding, Liang; Tao, Dacheng] Univ Sydney, Sch Comp Sci, Sydney, NSW 2006, Australia.
   [Qi, Shuhan] Guangdong Prov Key Lab Novel Secur Intelligence Te, Shenzhen 518000, Peoples R China.
C3 Harbin Institute of Technology; University of Sydney
RP Qi, SH (corresponding author), Harbin Inst Technol, Shenzhen 518055, Peoples R China.; Qi, SH (corresponding author), Guangdong Prov Key Lab Novel Secur Intelligence Te, Shenzhen 518000, Peoples R China.
EM rao7jun@gmail.com; mxx0822@foxmail.com; liangding.liam@gmail.com;
   shuhanqi@cs.hitsz.edu.cn; liuxuebo@hit.edu.cn; minzhang@suda.edu.cn;
   dacheng.tao@sydney.edu.au
OI shuhan, qi/0000-0002-6903-145X
FU National Natural Science Foundation of China
FX No Statement Available
CR Aghajanyan Armen, 2021, P 59 ANN M ASS COMP, V1, P7319, DOI DOI 10.18653/V1/2021.ACL-LONG.568
   Bhat P, 2021, IEEE COMPUT SOC CONF, P2672, DOI 10.1109/CVPRW53098.2021.00301
   Chandrasegaran K., 2022, International Conference on Machine Learning, P2890
   Chen DF, 2020, AAAI CONF ARTIF INTE, V34, P3430
   Chen PG, 2021, PROC CVPR IEEE, P5006, DOI 10.1109/CVPR46437.2021.00497
   Cho JH, 2019, IEEE I CONF COMP VIS, P4793, DOI 10.1109/ICCV.2019.00489
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guo J., 2022, OpenReview preprint
   Hao ZW, 2022, IEEE T MULTIMEDIA, V24, P4262, DOI 10.1109/TMM.2022.3192663
   He J., 2022, PROC INT C LEARN REP, P1
   He S., 2022, FINDINGS ASS COMPUTA, P2184
   Heo B, 2019, AAAI CONF ARTIF INTE, P3779
   Hinton G., 2014, NEURIPS WORKSHOPS
   Houlsby N, 2019, PR MACH LEARN RES, V97
   Hu E. J., PROC INT C LEARN REP, P1
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Jin X, 2019, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2019.00143
   Kim J, 2018, ADV NEUR IN, V31
   Kornblith S, 2019, PR MACH LEARN RES, V97
   Krizhevsky A, 2009, CIFAR-10 dataset
   Lan X, 2018, ADV NEUR IN, V31
   Li Lujun, 2022, ADV NEURAL INFORM PR, V35, P635
   Li XLS, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P4582
   Lialin V, 2023, Arxiv, DOI [arXiv:2303.15647, 10.48550/arXiv.2303.15647]
   Liu Z., 2021, PROC BRIT MACH VIS C, P1
   Mirzadeh SI, 2020, AAAI CONF ARTIF INTE, V34, P5191
   Müller R, 2019, ADV NEUR IN, V32
   Park Dae Young, 2021, ADV NEURAL INFORM PR, V34, P5
   Park W, 2019, PROC CVPR IEEE, P3962, DOI 10.1109/CVPR.2019.00409
   Passalis N, 2018, LECT NOTES COMPUT SC, V11215, P283, DOI 10.1007/978-3-030-01252-6_17
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Qiushan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11017, DOI 10.1109/CVPR42600.2020.01103
   Rao J, 2023, IEEE T MULTIMEDIA, V25, P8383, DOI 10.1109/TMM.2023.3236837
   Rao J, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3383, DOI 10.1145/3459637.3482194
   Rao J, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P2727, DOI 10.1145/3477495.3531715
   Socher R, 2013, P 2013 C EMP METH NA, P935
   Stanton S, 2021, 35 C NEURAL INFORM P, V34
   Sun SQ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4323
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian Y., 2020, ICLR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang A., PROC INT C LEARN REP, P1
   Xu CW, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P10653
   Xu CW, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7859
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang L, 2022, PROC CVPR IEEE, P10164, DOI 10.1109/CVPR52688.2022.00993
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhong QH, 2024, Arxiv, DOI arXiv:2208.10160
   Zhou WCS, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P7037
   Zhu Y., 2021, P IEEECVF INT C COMP, P5057
NR 51
TC 3
Z9 3
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4230
EP 4241
DI 10.1109/TMM.2023.3321480
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100055
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Shen, S
   Li, WH
   Huang, XK
   Zhu, Z
   Zhou, J
   Lu, JW
AF Shen, Shuai
   Li, Wanhua
   Huang, Xiaoke
   Zhu, Zheng
   Zhou, Jie
   Lu, Jiwen
TI SD-NeRF: Towards Lifelike Talking Head Animation via Spatially-Adaptive
   Dual-Driven NeRFs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; neural radiance fields; talking head video
   synthesis
AB Recent years have witnessed great progress in audio-driven talking head animation. Among these methods, the 3D-based ones better preserve the 3D consistency of the generated head and produce more natural results compared with 2D-based approaches. However, most 3D-based methods employ 3D morphable face models as the intermediate representation and involve multi-stage training, which may lead to error accumulation. To alleviate this problem, in this article, we propose a fully end-to-end talking head animation method, which implicitly grasps the 3D structures by learning a conditional Neural Radiance Field (NeRF). As NeRF has proven to be an effective tool for 3D modeling, one can learn dynamic neural radiance fields conditioned on audio signals for talking head synthesis. Furthermore, we argue that audio signals cannot fully drive a lifelike talking head. When people are talking, they usually show many spontaneous facial movements like blinks and brow movements, which makes talkers natural and real. These movements cannot be fully driven by the audio signals since they are highly unrelated to the audio. Therefore, we incorporate motion information as another driving factor and develop an audio-motion dual-driven NeRF model to take a step toward more lifelike talking head synthesis. On this basis, as audio and motion mainly affect different regions of the human face, we propose a Spatially-adaptive Dual-driven NeRF (SD-NeRF), which fuses these two driven factors with a spatially-adaptive cross-attention mechanism. Quantitative and qualitative results demonstrate that, with finer facial controls, our method produces more realistic talking head videos compared with existing advanced works.
C1 [Shen, Shuai; Li, Wanhua; Huang, Xiaoke; Zhu, Zheng; Zhou, Jie; Lu, Jiwen] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Lu, JW (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol BNRist, Dept Automat, Beijing 100084, Peoples R China.
EM shens19@mails.tsinghua.edu.cn; wanhua016@gmail.com;
   hxk21@mails.tsinghua.edu.cn; zhengzhu@ieee.org; jzhou@tsinghua.edu.cn;
   lujiwen@tsinghua.edu.cn
RI Zhu, Zheng/T-8680-2019; Li, Wanhua/AAE-9197-2021; Lu, Jiwen/C-5291-2009
OI Zhu, Zheng/0000-0001-7510-9949; Huang, Xiaoke/0000-0003-1658-6432; Li,
   Wanhua/0000-0002-2730-0543; Lu, Jiwen/0000-0002-6121-5529; Zhu,
   Zheng/0000-0002-4435-1692
FU National Natural Science Foundation of China
FX No Statement Available
CR Andrew Alex M, 2001, Kybernetes
   Athar S., 2023, P IEEE 17 INT C AUT, P1
   Athar S, 2022, PROC CVPR IEEE, P20332, DOI 10.1109/CVPR52688.2022.01972
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Carion N., 2020, EUR C COMP VIS, P213
   Chan ER, 2021, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR46437.2021.00574
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chung JS, 2017, LECT NOTES COMPUT SC, V10117, P251, DOI 10.1007/978-3-319-54427-4_19
   Chung Joon Son, 2017, P BRIT MACH VIS C
   Cowen AS, 2021, NATURE, V589, P251, DOI 10.1038/s41586-020-3037-7
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Das Dipanjan, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P408, DOI 10.1007/978-3-030-58577-8_25
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Eskimez SE, 2021, IEEE T MULTIMEDIA, V24, P3480, DOI 10.1109/TMM.2021.3099900
   Fan X, 2021, IEEE T MULTIMEDIA, V23, P1252, DOI 10.1109/TMM.2020.2994506
   Fried O, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323028
   Gafni G, 2021, PROC CVPR IEEE, P8645, DOI 10.1109/CVPR46437.2021.00854
   Guo YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5764, DOI 10.1109/ICCV48922.2021.00573
   Hannun A, 2014, Arxiv, DOI arXiv:1412.5567
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Ji XY, 2021, PROC CVPR IEEE, P14075, DOI 10.1109/CVPR46437.2021.01386
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kingma D.P., 2015, PROC INT C LEARN RE
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lele Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P35, DOI 10.1007/978-3-030-58545-7_3
   Li B, 2022, IEEE T MULTIMEDIA, V24, P4077, DOI 10.1109/TMM.2021.3113786
   Li W., 2022, Advances in Neural Information Processing Systems, V35, P35313
   Li WH, 2022, LECT NOTES COMPUT SC, V13672, P562, DOI 10.1007/978-3-031-19775-8_33
   Liu X, 2022, LECT NOTES COMPUT SC, V13697, P106, DOI 10.1007/978-3-031-19836-6_7
   Lu YX, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480484
   Mildenhall B, 2022, COMMUN ACM, V65, P99, DOI 10.1145/3503250
   Oechsle M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5569, DOI 10.1109/ICCV48922.2021.00554
   Park K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5845, DOI 10.1109/ICCV48922.2021.00581
   Prajwal KR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P484, DOI 10.1145/3394171.3413532
   Pumarola A, 2021, PROC CVPR IEEE, P10313, DOI 10.1109/CVPR46437.2021.01018
   Qian XY, 2022, IEEE T MULTIMEDIA, V24, P942, DOI 10.1109/TMM.2021.3061800
   Rahaman N, 2019, PR MACH LEARN RES, V97
   Schwarz K., 2020, P 34 INT C NEURAL IN
   Shen S, 2023, PROC CVPR IEEE, P1982, DOI 10.1109/CVPR52729.2023.00197
   Shen S, 2022, LECT NOTES COMPUT SC, V13672, P666, DOI [10.1007/978-3-031-19775-8_39, 10.1007/978-3-031-19775-839]
   Song LS, 2022, IEEE T INF FOREN SEC, V17, P585, DOI 10.1109/TIFS.2022.3146783
   Srinivasan PP, 2021, PROC CVPR IEEE, P7491, DOI 10.1109/CVPR46437.2021.00741
   Suwajanakorn S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073640
   Tang J., 2022, arXiv
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Thies Justus, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P716, DOI 10.1007/978-3-030-58517-4_42
   Thies J, 2016, PROC CVPR IEEE, P2387, DOI 10.1109/CVPR.2016.262
   Tretschk E, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12939, DOI 10.1109/ICCV48922.2021.01272
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang S, 2021, P INT JOINT C ART IN, P1098, DOI DOI 10.24963/IJCAI.2021/152
   Wang TC, 2021, PROC CVPR IEEE, P10034, DOI 10.1109/CVPR46437.2021.00991
   Wang XY, 2022, IEEE T MULTIMEDIA, V24, P4028, DOI 10.1109/TMM.2021.3111485
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Websdale D, 2022, IEEE T MULTIMEDIA, V24, P2539, DOI 10.1109/TMM.2021.3087020
   Xue C, 2023, IEEE T MULTIMEDIA, V25, P418, DOI 10.1109/TMM.2021.3127029
   Yao S., 2022, arXiv
   Ye Z., 2023, P INT C LEARN REPR, P1
   Yu LY, 2022, IEEE T MULTIMEDIA, V24, P2950, DOI 10.1109/TMM.2021.3091863
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zakharov E, 2019, IEEE I CONF COMP VIS, P9458, DOI 10.1109/ICCV.2019.00955
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang ZM, 2021, PROC CVPR IEEE, P3660, DOI 10.1109/CVPR46437.2021.00366
   Zheng AH, 2022, IEEE T MULTIMEDIA, V24, P338, DOI 10.1109/TMM.2021.3050089
   Zhou H, 2021, PROC CVPR IEEE, P4174, DOI 10.1109/CVPR46437.2021.00416
   Zhou Y, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417774
NR 68
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3221
EP 3234
DI 10.1109/TMM.2023.3308441
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200039
DA 2024-08-05
ER

PT J
AU Shu, YY
   Li, Q
   Liu, LQ
   Xu, GD
AF Shu, Yangyang
   Li, Qian
   Liu, Lingqiao
   Xu, Guandong
TI Semi-Supervised Adversarial Learning for Attribute-Aware Photo Aesthetic
   Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Task analysis; Training; Adversarial machine learning; Image
   color analysis; Annotations; Semisupervised learning; Semi-supervised
   adversarial learning; aesthetic attributes; photo aesthetic assessment
AB Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photo's aesthetic quality. However, annotating aesthetic attributes is a time-consuming, costly, and error-prone task, which leads to the issue that photos available are partially annotated with attributes. To alleviate this issue, we propose a novel semi-supervised adversarial learning method for photo aesthetic assessment from partially attribute-annotated photos, which can greatly reduce the reliance on manual attribute annotation. Specifically, the proposed method consists of a score-attributes generator R, a photo generator G, and a discriminator D. The score-attributes generator learns the aesthetic score and attributes simultaneously to capture their dependencies and construct better feature representations. The photo generator reconstructs the photo by feeding aesthetic attributes, score, and informative feature representation. A discriminator is used to force the convergence of the features-attributes-score tuples generated from the score-attributes generator, the photo generator, and the ground-truth distribution in labeled data for training data. The proposed method significantly outperforms the state of the art, increasing the Spearman rank-order correlation coefficient (SRCC) from the existing best reported of 0.726 to 0.761 on Aesthetic and attributes database and 0.756 to 0.774 on Aesthetic visual analysis database, respectively.
C1 [Shu, Yangyang; Xu, Guandong] Univ Technol Sydney, Data Sci & Machine Intelligence Lab, Sydney, NSW 2007, Australia.
   [Shu, Yangyang; Xu, Guandong] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Li, Qian] Curtin Univ, Sch Engn Comp & Math Sci, Perth, WA 6102, Australia.
   [Liu, Lingqiao] Univ Adelaide, Sch Comp Sci, Adelaide, SA 5005, Australia.
C3 University of Technology Sydney; University of Technology Sydney; Curtin
   University; University of Adelaide
RP Xu, GD (corresponding author), Univ Technol Sydney, Data Sci & Machine Intelligence Lab, Sydney, NSW 2007, Australia.; Xu, GD (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM 13278003@student.uts.edu.au; qli@curtin.edu.au;
   Lingqiao.Liu@adelaide.edu.au; Guandong.Xu@uts.edu.au
RI Xu, Guandong/ABE-6430-2020; Xu, Guandong/HSH-3463-2023
OI Xu, Guandong/0000-0003-4493-6663; li, qian/0000-0002-8308-9551
FU Australian Research Council
FX No Statement Available
CR Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Dong JH, 2019, ADV NEUR IN, V32
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   He D, 2016, ADV NEUR IN, V29
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herron S, 2003, P SOC PHOTO-OPT INS, V5008, P365, DOI 10.1117/12.472021
   Hongtao Yang, 2019, 2019 IEEE/ACIS 18th International Conference on Computer and Information Science (ICIS). Proceedings, P294
   Hosu V, 2019, PROC CVPR IEEE, P9367, DOI 10.1109/CVPR.2019.00960
   Hou L., 2016, arXiv preprint arXiv, DOI DOI 10.48550/ARXIV.1611.05916
   JLB D. P. K., 2015, PROC 3 INT C LEARN R, P1
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Ke Y., 2006, CVPR, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Lai W.-S., 2017, PROC 31 INT C NEURAL, P353
   Li LD, 2019, IEEE INT CON MULTI, P430, DOI 10.1109/ICME.2019.00081
   Liu D, 2020, IEEE WINT CONF APPL, P3558, DOI [10.1109/WACV45572.2020.9093412, 10.1109/wacv45572.2020.9093412]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu X, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P457, DOI 10.1145/2647868.2654927
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Lv P, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1328, DOI 10.1145/3240508.3240635
   Malu G, 2017, Arxiv, DOI arXiv:1707.03981
   Mingyuan Fan, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P531, DOI 10.1109/ICMEW.2017.8026291
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Pan BW, 2019, AAAI CONF ARTIF INTE, P679
   Shu YY, 2022, IEEE T MULTIMEDIA, V24, P876, DOI 10.1109/TMM.2021.3060955
   Shu YY, 2020, NEUROCOMPUTING, V404, P304, DOI 10.1016/j.neucom.2020.04.142
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wakabayashi H., 1990, Patent, Patent No. 4937609
   Wang ZY, 2017, IEEE IJCNN, P941, DOI 10.1109/IJCNN.2017.7965953
   Wang ZT, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P656, DOI 10.1145/3343031.3350919
   Zeng H, 2020, IEEE T IMAGE PROCESS, V29, P1548, DOI 10.1109/TIP.2019.2941778
   Zhang XD, 2019, IEEE T MULTIMEDIA, V21, P2815, DOI 10.1109/TMM.2019.2911428
   Zhu HP, 2022, IEEE T NEUR NET LEAR, V33, P4084, DOI [10.1080/1206212X.2021.1886417, 10.1109/TNNLS.2021.3055816]
NR 39
TC 5
Z9 5
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4086
EP 4096
DI 10.1109/TMM.2021.3117709
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100027
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Su, ST
   Zhu, JC
   Gao, LL
   Song, JK
AF Su, Sitong
   Zhu, Junchen
   Gao, Lianli
   Song, Jingkuan
TI Utilizing Greedy Nature for Multimodal Conditional Image Synthesis in
   Transformers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Image synthesis; Visualization; Image segmentation; Task
   analysis; Image reconstruction; Computer architecture; multimodal
   learning; transformer
AB Multimodal Conditional Image Synthesis(MCIS) aims to generate images according to different modalities input and their combination, which allows users to describe their requirements in complementary ways, e.g. segmentation for shapes and text for attributes. Despite satisfying results in MCIS, a non-trivial issue is neglected. Some modalities are fully optimized and dominate the generation, while other modalities are sub-optimized and fail to contribute their complementary information. We coin this phenomenon as Modality Bias. Our analysis reveals that generative models own greedy nature. Specifically, the modality that shares less semantic gap with the synthesized modality will be greedily incorporated and thus takes a larger proportion in synthesis. The main idea of previous works in Modality Bias is to punish the greedy nature, which hurts the performance of dominant modalities and impedes their contribution to multimodal synthesis. Instead, we propose to utilize the greedy nature by setting dominant modalities as guidance for sub-optimized modalities through coordinated feature space, named Coordinated Knowledge Mining. Afterwards, improved uni-modalities are aggregated by fusing coordinated features to further boost the performance of multimodal image synthesis, called Coordinated Knowledge Fusion. Extensive experiments prove that our method not only increases uni-modal performance by a large margin, but also promotes multimodal image synthesis by fully utilizing complementary information from different modalities.
C1 [Su, Sitong; Zhu, Junchen; Gao, Lianli] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [Song, Jingkuan] Univ Elect Sci & Technol China, Sichuan Prov Peoples Hosp, Inst Neurol, Chengdu 610072, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Sichuan Provincial People's
   Hospital
RP Song, JK (corresponding author), Univ Elect Sci & Technol China, Sichuan Prov Peoples Hosp, Inst Neurol, Chengdu 610072, Peoples R China.
EM sitongsu9796@gmail.com; junchen.zhu@hotmail.com;
   lianli.gao@uestc.edu.cn; jingkuan.song@gmail.com
OI Zhu, Junchen/0000-0002-3872-6689; song, jingkuan/0000-0002-2549-8322
FU National Key Ramp;D Program of China
FX No Statement Available
CR Alaniz S., 2022, PROC ICLR WORKSHOP D
   Cadene R, 2019, ADV NEUR IN, V32
   Cao C., 2021, ADV NEURAL INFORM PR, P18433
   Chang HW, 2022, PROC CVPR IEEE, P11305, DOI 10.1109/CVPR52688.2022.01103
   Dhamo H, 2020, PROC CVPR IEEE, P5212, DOI 10.1109/CVPR42600.2020.00526
   Ding M., 2021, Advances in Neural Information Processing Systems (NeurIPS-21), V34, P19822
   Du C., 2021, arXiv
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fan W-C, 2023, P AAAI C ART INT, V37, P579
   Farshad A., 2021, PROC BRIT MACH VIS C
   Gafni O, 2022, LECT NOTES COMPUT SC, V13675, P89, DOI 10.1007/978-3-031-19784-0_6
   Gao LL, 2022, IEEE T IMAGE PROCESS, V31, P202, DOI 10.1109/TIP.2021.3120867
   Gat I., 2020, ADV NEURAL INFORM PR, V33, P3197
   Nair NG, 2022, Arxiv, DOI arXiv:2206.05039
   Gu SY, 2022, PROC CVPR IEEE, P10686, DOI 10.1109/CVPR52688.2022.01043
   Guan WL, 2022, IEEE T IMAGE PROCESS, V31, P4733, DOI 10.1109/TIP.2022.3187290
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3409, DOI 10.1109/TMM.2022.3160360
   Huang X, 2022, LECT NOTES COMPUT SC, V13676, P91, DOI 10.1007/978-3-031-19787-1_6
   Huang YP, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1138, DOI 10.1145/3474085.3481540
   Ivgi M, 2021, IEEE IMAGE PROC, P2428, DOI 10.1109/ICIP42928.2021.9506651
   Jing P., 2023, IEEE IEEE Trans. Multimedia, early access, DOI [10.1109/TMM2023.3246796, DOI 10.1109/TMM2023.3246796]
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim T., 2022, P IEEECVF C COMPUTER, P16526
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liuetal X., 2019, P ADV NEUR INF PROC, V32
   Lu J., 2022, PROC INT C LEARN REP
   Mittal G., 2019, PROC DEEP GENERATIVE
   Nichol A, 2022, PR MACH LEARN RES
   Nie LQ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3234, DOI 10.1145/3503161.3548180
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peng XK, 2022, PROC CVPR IEEE, P8228, DOI 10.1109/CVPR52688.2022.00806
   Radford A, 2021, PR MACH LEARN RES, V139
   Ramesh A., 2022, Hierarchical text-conditional image generation with clip latents, DOI 10.48550/arXiv.2204.06125
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Saharia C., 2022, Advances in Neural Information Processing Systems, V35
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Su ST, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1784, DOI 10.1145/3474085.3475326
   Sushko V, 2022, INT J COMPUT VISION, V130, P2903, DOI 10.1007/s11263-022-01673-x
   Tan HC, 2023, IEEE T MULTIMEDIA, V25, P8620, DOI 10.1109/TMM.2023.3238554
   Wang P., 2022, PROC INT C MACH LEAR, p23 318
   Wang WL, 2022, Arxiv, DOI arXiv:2207.00050
   Wang WH, 2022, Arxiv, DOI arXiv:2208.10442
   Winterbottom T., 2020, PROC BRIT MACH VIS C
   Wu CF, 2022, LECT NOTES COMPUT SC, V13676, P720, DOI 10.1007/978-3-031-19787-1_41
   Wu N, 2022, PR MACH LEARN RES
   Xia WH, 2021, PROC CVPR IEEE, P2256, DOI 10.1109/CVPR46437.2021.00229
   Xu YH, 2023, IEEE T MULTIMEDIA, V25, P8346, DOI 10.1109/TMM.2023.3235495
   Zeng P., 2022, PROC INT JOINT C ART, V5
   Zeng PP, 2022, IEEE T IMAGE PROCESS, V31, P5936, DOI 10.1109/TIP.2022.3205212
   Zhang H., 2021, arXiv
   Zhang J, 2023, IEEE T MULTIMEDIA, V25, P8988, DOI 10.1109/TMM.2023.3243659
   Zhang Z, 2021, ADV NEUR IN
   Zhu JC, 2023, IEEE T PATTERN ANAL, V45, P3311, DOI 10.1109/TPAMI.2022.3186752
NR 56
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2354
EP 2366
DI 10.1109/TMM.2023.3295094
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100026
DA 2024-08-05
ER

PT J
AU Wang, CM
   Fu, HY
   Ma, HD
AF Wang, Chuanming
   Fu, Huiyuan
   Ma, Huadong
TI Learning Mutually Exclusive Part Representations for Fine-Grained Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image classification; Filter banks; Annotations;
   Training; Semantics; Task analysis; Fine-grained image classification;
   attention learning; mutually exclusive; part representations;
   multi-granularity
AB Fine-grained image classification (FGIC) aims to separate different subcategories from one general superclass, which requires the classification model to extract distinctive representations from subtle yet discriminative regions of the objects. Learning multiple part representations can give a detailed description of the object from different perspectives, boosting the classification performance. However, it still remains a challenging problem to effectively locate diverse parts and extract their features without the assistance of part annotations. In this article, we present a novel method to achieve accurate fine-grained image classification by learning a set of diverse and discriminative part representations without requiring additional supervision. Firstly, our method utilizes a simple attention interaction module to lead learned spatial attentions to focus on different parts, resulting in mutually exclusive part representations. Then, to reduce the impairment of channel coupling among part representations, a part-wise channel weighting module is designed to adjust the amplitudes of different representations adaptively, making them to be diverse along the channel dimension. Moreover, to ensure comprehensive and sufficient part representations, our method introduces multi-granularity feature learning. It enables the extraction of part representations from different semantic and content levels, capturing fine-grained details effectively. To evaluate our method, extensive experiments are conducted on various benchmark fine-grained image datasets, and the results show that our method can achieve outstanding performance for FGIC, demonstrating its effectiveness.
C1 [Wang, Chuanming; Fu, Huiyuan; Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications
RP Ma, HD (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
EM wcm@bupt.edu.cn; fhy@bupt.edu.cn; mhd@bupt.edu.cn
RI Wang, Chuanming/JCP-0842-2023
OI Wang, Chuanming/0000-0001-6932-6226
FU Innovation Research Group Project of NSFC Under
FX No Statement Available
CR Cai SJ, 2017, IEEE I CONF COMP VIS, P511, DOI 10.1109/ICCV.2017.63
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325
   Ding Y, 2019, IEEE I CONF COMP VIS, P6598, DOI 10.1109/ICCV.2019.00670
   Engin M, 2018, LECT NOTES COMPUT SC, V11206, P629, DOI 10.1007/978-3-030-01216-8_38
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu T, 2019, Arxiv, DOI arXiv:1901.09891
   Huang C, 2017, IEEE T MULTIMEDIA, V19, P673, DOI 10.1109/TMM.2016.2631122
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li GJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5273, DOI 10.1145/3474085.3475646
   Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu CB, 2020, IEEE T MULTIMEDIA, V22, P1785, DOI 10.1109/TMM.2019.2954747
   Liu HB, 2022, IEEE T MULTIMEDIA, V24, P2902, DOI 10.1109/TMM.2021.3090274
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P1105, DOI 10.1109/TMM.2021.3118216
   Liu X, 2017, Arxiv, DOI arXiv:1603.06765
   Liu X, 2017, AAAI CONF ARTIF INTE, P4190
   Luo W, 2019, IEEE I CONF COMP VIS, P8241, DOI 10.1109/ICCV.2019.00833
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mnih V, 2014, ADV NEUR IN, V27
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Sun M, 2018, LECT NOTES COMPUT SC, V11220, P834, DOI 10.1007/978-3-030-01270-0_49
   Wang DQ, 2015, IEEE I CONF COMP VIS, P2399, DOI 10.1109/ICCV.2015.276
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei XS, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-022-3513-y
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Wei YC, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/8875910
   Welinder P., 2010, California Inst. Technol., Tech. Rep. CNS-TR-2010-001
   Yang XH, 2022, PROC CVPR IEEE, P7389, DOI 10.1109/CVPR52688.2022.00725
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Yu CJ, 2018, LECT NOTES COMPUT SC, V11220, P595, DOI 10.1007/978-3-030-01270-0_35
   Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129
   Zhang LB, 2022, IEEE T MULTIMEDIA, V24, P4409, DOI 10.1109/TMM.2021.3117064
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2017, IEEE T MULTIMEDIA, V19, P2736, DOI 10.1109/TMM.2017.2710803
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
   Zhuang PQ, 2020, AAAI CONF ARTIF INTE, V34, P13130
   Zixuan Huang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8659, DOI 10.1109/CVPR42600.2020.00869
NR 51
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3113
EP 3124
DI 10.1109/TMM.2023.3307235
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700013
DA 2024-08-05
ER

PT J
AU Wang, X
   Kong, WF
   Zhang, QD
   Yang, Y
   Zhao, TS
   Jiang, JM
AF Wang, Xu
   Kong, Weifeng
   Zhang, Qiudan
   Yang, You
   Zhao, Tiesong
   Jiang, Jianmin
TI Distortion-Aware Self-Supervised Indoor 360<SUP>°</SUP>Depth Estimation
   via Hybrid Projection Fusion and Structural Regularities
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Feature extraction; Distortion; Training; Self-supervised
   learning; Image reconstruction; Task analysis; 360(degrees) image; depth
   estimation; self-supervised learning; structural regularity
ID DEPTH; IMAGES
AB Owing to the rapid development of emerging 360(degrees)panoramic imaging techniques, indoor 360(degrees)depth estimation has aroused extensive attention in the community. Due to the lack of available ground truth depth data, it is extremely urgent to model indoor 360(degrees)depth estimation in self-supervised mode. However, self-supervised 360 degrees depth estimation suffers from two major limitations. One is the distortion and network training problems caused by Equirectangular projection (ERP), and the other is that texture-less regions are quite difficult to back-propagate in self-supervised mode. Hence, to address the above issues, we introduce spherical view synthesis for learning self-supervised 360(degrees)depthestimation. Specifically, to alleviate the ERP-related problems, we first propose a dual-branch distortion-aware network to produce the coarse depth map, including a distortion-aware module and a hybrid projection fusion module. Subsequently, the coarse depth map is utilized for spherical view synthesis, in which a spherically weighted loss function for view reconstruction and depth smoothing is investigated to optimize the projection distribution problem of360(degrees)images. In addition, two structural regularities of indoor360(degrees)scenes are devised as two additional supervisory signals to efficiently optimize our self-supervised 360(degrees)depth estimation model, containing the principal-direction normal constraint and the co-planar depth constraint. The principal-direction normal constraint is designed to align the normal of the 360(degrees)imagewith the direction of the vanishing points. Meanwhile, we employ the co-planar depth constraint to fit the estimated depth of each pixel through its 3D plane. Finally, a depth map is obtained for the 360(degrees)image. Experimental results illustrate that our proposed method achieves superior performance than the current advanced depth estimation methods on four publicly available datasets
C1 [Wang, Xu; Kong, Weifeng; Zhang, Qiudan; Jiang, Jianmin] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Yang, You] Huangzhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Zhao, Tiesong] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
C3 Shenzhen University; Huazhong University of Science & Technology; Fuzhou
   University
RP Zhang, QD (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM wangxu@szu.edu.cn; weifengkong.cs@gmail.com; qiudanzhang@szu.edu.cn;
   yangyou@hust.edu.cn; t.zhao@fzu.edu.cn; jianmin.jiang@szu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Armeni I., 2017, arXiv
   Bai J., 2022, arXiv
   Chang A, 2017, INT CONF 3D VISION, P667, DOI 10.1109/3DV.2017.00081
   Chang Shu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P572, DOI 10.1007/978-3-030-58529-7_34
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Coughlan J.M., 1999, ICCV, V2, P941, DOI DOI 10.1109/ICCV.1999.790349
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Derpanis KG, 2010, Overview of the ransac algorithm, V4, P2
   Ding LJ, 2001, PATTERN RECOGN, V34, P721, DOI 10.1016/S0031-3203(00)00023-6
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Farbiz F, 2005, IEEE T MULTIMEDIA, V7, P514, DOI 10.1109/TMM.2005.846787
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fernandez-Labrador C, 2018, IEEE ROBOT AUTOM LET, V3, P3153, DOI 10.1109/LRA.2018.2850532
   Godard C, 2019, IEEE I CONF COMP VIS, P3827, DOI 10.1109/ICCV.2019.00393
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guizilini V., 2019, PROC INT C LEARN REP
   Hasegawa Y., 2022, arXiv
   Jiang HL, 2021, INT CONF 3D VISION, P741, DOI 10.1109/3DV53792.2021.00083
   Jiang HL, 2021, IEEE ROBOT AUTOM LET, V6, P1519, DOI 10.1109/LRA.2021.3058957
   Jin L, 2020, PROC CVPR IEEE, P886, DOI 10.1109/CVPR42600.2020.00097
   Jin YL, 2024, IEEE T MULTIMEDIA, V26, P2096, DOI 10.1109/TMM.2022.3177942
   Khasanova R, 2017, IEEE INT CONF COMP V, P860, DOI 10.1109/ICCVW.2017.106
   Kingma D. P., 2015, PROC INT C LEARN, P1, DOI DOI 10.1063/1.4902458
   Kong W, 2022, LECT NOTES COMPUT SC, V13631, P438, DOI 10.1007/978-3-031-20868-3_32
   Li BY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12643, DOI 10.1109/ICCV48922.2021.01243
   Li R, 2023, IEEE T MULTIMEDIA, V25, P5626, DOI 10.1109/TMM.2022.3197367
   Li X, 2023, IEEE T PATTERN ANAL, V45, P3139, DOI 10.1109/TPAMI.2022.3180392
   Li YY, 2022, PROC CVPR IEEE, P2791, DOI 10.1109/CVPR52688.2022.00282
   Ling CW, 2022, IEEE T MULTIMEDIA, V24, P2938, DOI 10.1109/TMM.2021.3091308
   Liu CX, 2021, IEEE T MULTIMEDIA, V23, P2843, DOI 10.1109/TMM.2020.3017924
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Rey-Area M, 2022, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR52688.2022.00374
   Shao SW, 2023, IEEE T MULTIMEDIA, V25, P7660, DOI 10.1109/TMM.2022.3224810
   Sun C, 2021, PROC CVPR IEEE, P2573, DOI 10.1109/CVPR46437.2021.00260
   Szabó P, 2023, IEEE T MULTIMEDIA, V25, P3245, DOI 10.1109/TMM.2022.3157556
   Tateno K, 2018, LECT NOTES COMPUT SC, V11220, P732, DOI 10.1007/978-3-030-01270-0_43
   Wang FE, 2023, IEEE T PATTERN ANAL, V45, P5448, DOI 10.1109/TPAMI.2022.3203516
   Wang FE, 2020, PROC CVPR IEEE, P459, DOI 10.1109/CVPR42600.2020.00054
   Wang FE, 2019, LECT NOTES COMPUT SC, V11365, P53, DOI 10.1007/978-3-030-20873-8_4
   Wu YR, 2023, ACM T EMBED COMPUT S, V22, DOI 10.1145/3587038
   Wu YR, 2023, IEEE T NETW SCI ENG, V10, P3086, DOI 10.1109/TNSE.2022.3151502
   Yang ZH, 2018, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2018.00031
   Yin ZC, 2018, PROC CVPR IEEE, P1983, DOI 10.1109/CVPR.2018.00212
   Yu ZH, 2019, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2019.00112
   Zehao Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P206, DOI 10.1007/978-3-030-58586-0_13
   Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700
   Zioulis N, 2019, INT CONF 3D VISION, P690, DOI 10.1109/3DV.2019.00081
   Zioulis N, 2018, LECT NOTES COMPUT SC, V11210, P453, DOI 10.1007/978-3-030-01231-1_28
   Zou DP, 2019, IEEE T ROBOT, V35, P999, DOI 10.1109/TRO.2019.2915140
NR 51
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3998
EP 4011
DI 10.1109/TMM.2023.3318470
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300017
DA 2024-08-05
ER

PT J
AU Xiong, LZ
   Xu, JH
   Yang, CN
   Zhang, XP
AF Xiong, Lizhi
   Xu, Jianhua
   Yang, Ching-Nung
   Zhang, Xinpeng
TI CMCF-Net: An End-to-End Context Multiscale Cross-Fusion Network for
   Robust Copy-Move Forgery Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Copy-move forgery detection; deep neural network; depthwise separable
   convolution; image forensics
ID IMAGES; SIFT
AB Image copy-move forgery detection (CMFD) has become a challenging problem due to increasingly powerful editing software that makes forged images increasingly realistic. Existing algorithms that directly connect multiple scales of features in the encoder part may not effectively aggregate contextual information, resulting in poor performance. In this paper, an end-to-end context multiscale cross-fusion network (CMCF-Net) is proposed to detect image copy-move forgery. The proposed network consists of a multiscale feature extraction fusion (MSF) module and a multi-information fusion decoding (MFD) module. Multiscale information is efficiently extracted and fused in the MSF module utilizing stacked-scale feature fusion, which improves the network's forgery localization ability on objects of different scales. The MFD module employs contextual information combination and weighted fusion of multiscale information to guide the network in obtaining relevant clues from correlated information at multiple different scales. Experimental results and analysis have demonstrated that the proposed CMCF-Net achieves the best localization results with higher robustness.
C1 [Xiong, Lizhi] Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
   [Xiong, Lizhi] Minist Educ, Engn Res Ctr Digital Forens, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Nanjing 210044, Peoples R China.
   [Xu, Jianhua] Nanjing Univ Informat Sci & Technol, Software Engn, Nanjing 210044, Peoples R China.
   [Yang, Ching-Nung] Natl Dong Hwa Univ, Dept Comp Sci & Informat Engn, Hualien 974301, Taiwan.
   [Zhang, Xinpeng] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Information Science & Technology; National Dong Hwa
   University; Fudan University
RP Xiong, LZ (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp Sci, Nanjing 210044, Peoples R China.
EM lzxiong16@163.com; 1142918366@qq.com; cnyang@gms.ndhu.edu.tw;
   zhangxinpeng@fudan.edu.cn
RI Xiong, Lizhi/KCK-1464-2024
FU National Natural Science Foundation of China
FX No Statement Available
CR Alcantarilla PF, 2012, LECT NOTES COMPUT SC, V7577, P214, DOI 10.1007/978-3-642-33783-3_16
   Barni M, 2021, IEEE T INF FOREN SEC, V16, P1825, DOI 10.1109/TIFS.2020.3045903
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Chen BJ, 2021, IEEE T MULTIMEDIA, V23, P3506, DOI 10.1109/TMM.2020.3026868
   Chen BJ, 2018, IEEE ACCESS, V6, P56637, DOI 10.1109/ACCESS.2018.2871952
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Christlein V, 2012, IEEE T INF FOREN SEC, V7, P1841, DOI 10.1109/TIFS.2012.2218597
   Cozzolino D, 2015, IEEE T INF FOREN SEC, V10, P2284, DOI 10.1109/TIFS.2015.2455334
   Emam M, 2016, MULTIMED TOOLS APPL, V75, P11513, DOI 10.1007/s11042-015-2872-2
   Ferreira A, 2016, IEEE T IMAGE PROCESS, V25, P4729, DOI 10.1109/TIP.2016.2593583
   Howard AG, 2017, Arxiv, DOI [arXiv:1704.04861, 10.48550/arXiv.1704.04861]
   Gani G, 2020, J INF SECUR APPL, V54, DOI 10.1016/j.jisa.2020.102510
   Hashmi MF, 2013, INT CONF INTELL SYST, P188, DOI 10.1109/ISDA.2013.6920733
   Huang DY, 2017, MULTIMED TOOLS APPL, V76, P1509, DOI 10.1007/s11042-015-3152-x
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Islam A, 2020, PROC CVPR IEEE, P4675, DOI 10.1109/CVPR42600.2020.00473
   Jaiswal AK, 2022, NEURAL PROCESS LETT, V54, P75, DOI 10.1007/s11063-021-10620-9
   Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374
   Johnston P, 2019, DIGIT INVEST, V29, P67, DOI 10.1016/j.diin.2019.03.006
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Kaur N, 2022, MULTIMED TOOLS APPL, V81, P38817, DOI 10.1007/s11042-022-13105-6
   Ketenci S, 2013, 2013 36TH INTERNATIONAL CONFERENCE ON TELECOMMUNICATIONS AND SIGNAL PROCESSING (TSP), P813, DOI 10.1109/TSP.2013.6614051
   Li FY, 2023, IEEE T MULTIMEDIA, V25, P7851, DOI 10.1109/TMM.2022.3231110
   Li YM, 2019, IEEE T INF FOREN SEC, V14, P1307, DOI 10.1109/TIFS.2018.2876837
   Li YA, 2013, FORENSIC SCI INT, V224, P59, DOI 10.1016/j.forsciint.2012.10.031
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YQ, 2022, IEEE T IMAGE PROCESS, V31, P541, DOI 10.1109/TIP.2021.3132828
   Mahmood T, 2018, J VIS COMMUN IMAGE R, V53, P202, DOI 10.1016/j.jvcir.2018.03.015
   Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Prakash CS, 2019, MULTIMED TOOLS APPL, V78, P23535, DOI 10.1007/s11042-019-7629-x
   Pun CM, 2015, IEEE T INF FOREN SEC, V10, P1705, DOI 10.1109/TIFS.2015.2423261
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Simonyan K., 2014, C TRACK P
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tralic I., 2013, CoMoFoD-New databasefor copy-move forgery detection, P49
   Wang C, 2023, IEEE T INF FOREN SEC, V18, P1064, DOI 10.1109/TIFS.2023.3234861
   Wang XY, 2017, MULTIMED TOOLS APPL, V76, P23353, DOI 10.1007/s11042-016-4140-5
   Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294
   Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu Y, 2018, LECT NOTES COMPUT SC, V11210, P170, DOI 10.1007/978-3-030-01231-1_11
   Wu Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1480, DOI 10.1145/3123266.3123411
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Yang JX, 2021, DIGIT SIGNAL PROCESS, V113, DOI 10.1016/j.dsp.2021.103032
   Zandi M, 2016, IEEE T INF FOREN SEC, V11, P2499, DOI 10.1109/TIFS.2016.2585118
   Zhong JL, 2020, IEEE T INF FOREN SEC, V15, P2134, DOI 10.1109/TIFS.2019.2957693
   Zhu Y, 2020, IEEE T IND INFORM, V16, P6714, DOI 10.1109/TII.2020.2982705
NR 48
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6090
EP 6101
DI 10.1109/TMM.2023.3345160
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600020
DA 2024-08-05
ER

PT J
AU Zhang, SH
   Tian, Y
   Zhang, YL
   Tian, M
   Huang, YP
AF Zhang, Sihui
   Tian, Yi
   Zhang, Yilei
   Tian, Mei
   Huang, Yaping
TI Domain-Consistent and Uncertainty-Aware Network for Generalizable Gaze
   Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaze estimation; domain adaptation; uncertainty learning; unsupervised
   learning
ID ENHANCEMENT
AB Unsupervised domain adaptive (UDA) gaze estimation aims to predict gaze directions of unlabeled target face or eye images given a set of annotated source images, which has been widely applied in practical applications. However, existing methods still perform poorly due to two major challenges. 1) There exists large personalized differences and style discrepancies between source and target samples, which leads the learned source model easily collapsing to biased results. 2) Data uncertainties inherent in reference samples will affect the generalization ability of their models. To tackle the above challenges, in this paper, we propose a novel Domain-Consistent and Uncertainty-Aware (DCUA) network for generalizable gaze estimation. Our DCUA network employs a two-phase framework where a primary training sub-network (PTNet) and a refined adaptation sub-network (RANet) are trained on the source and target domain, respectively. Firstly, to obtain robust and pure gaze-related features, we propose twain domain consistent constraints, that is, the intra-domain consistent constraint and the inter-domain consistent constraint. These two constraints could eliminate the impact of gaze-irrelevant factors by maintaining consistency between label and feature space. Secondly, to further improve the adaptability of our model, we propose dual uncertainty perception modules, which include an intrinsic uncertainty module and an extrinsic uncertainty module. These modules help DCUA network distinguish inferior reference samples and avoid overfitting to them. Experiments on four cross-domain gaze estimation tasks demonstrate the effectiveness of our method.
C1 [Zhang, Sihui; Tian, Yi; Zhang, Yilei; Tian, Mei; Huang, Yaping] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University
RP Tian, Y (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM 21120440@bjtu.edu.cn; tianyi@bjtu.edu.cn; 21120445@bjtu.edu.cn;
   mtian@bjtu.edu.cn; yphuang@bjtu.edu.cn
RI Zhang, Sihui/KPB-7955-2024
OI Zhang, Sihui/0009-0004-4780-8362
FU National Natural Science Foundation of China
FX No Statement Available
CR Bao YW, 2022, PROC CVPR IEEE, P4197, DOI 10.1109/CVPR52688.2022.00417
   Cantarini G, 2022, IEEE WINT CONF APPL, P3341, DOI 10.1109/WACV51458.2022.00340
   Chang J, 2020, PROC CVPR IEEE, P5709, DOI 10.1109/CVPR42600.2020.00575
   Chen MH, 2019, IEEE I CONF COMP VIS, P2090, DOI 10.1109/ICCV.2019.00218
   Chen SY, 2023, IEEE T MULTIMEDIA, V25, P3166, DOI 10.1109/TMM.2022.3156820
   Chen XY, 2021, INT C MACHINE LEARNI, V139
   Chen ZK, 2023, IEEE T PATTERN ANAL, V45, P1174, DOI 10.1109/TPAMI.2022.3148386
   Cheng YH, 2022, AAAI CONF ARTIF INTE, P436
   Cheng YH, 2020, AAAI CONF ARTIF INTE, V34, P10623
   Cheng YH, 2020, IEEE T IMAGE PROCESS, V29, P5259, DOI 10.1109/TIP.2020.2982828
   Cui YW, 2023, IEEE T MULTIMEDIA, V25, P6422, DOI 10.1109/TMM.2022.3208743
   Funes Mora K.A., 2014, P S EYE TRACK RES AP, P255, DOI 10.1145/2578153.2578190
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Guo ZD, 2020, Arxiv, DOI arXiv:2011.07526
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu P., 2020, PROC INT C NEURAL IN
   Kellnhofer P, 2019, IEEE I CONF COMP VIS, P6911, DOI 10.1109/ICCV.2019.00701
   Kendall A., 2017, Adv Neural Inf Process Syst, V30
   Konrad R, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3361330
   Leo M, 2020, INFORMATION, V11, DOI 10.3390/info11030128
   Li WH, 2021, PROC CVPR IEEE, P13891, DOI 10.1109/CVPR46437.2021.01368
   Liu G, 2021, IEEE T PATTERN ANAL, V43, P1092, DOI 10.1109/TPAMI.2019.2957373
   Liu YF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3815, DOI 10.1109/ICCV48922.2021.00381
   Lu F, 2016, IEEE T MULTIMEDIA, V18, P1772, DOI 10.1109/TMM.2016.2576284
   Lv K, 2021, IEEE T MULTIMEDIA, V23, P4198, DOI 10.1109/TMM.2020.3038311
   Minjie Cai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14380, DOI 10.1109/CVPR42600.2020.01440
   Park S, 2019, IEEE I CONF COMP VIS, P9367, DOI 10.1109/ICCV.2019.00946
   Sun YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3682, DOI 10.1109/ICCV48922.2021.00368
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Valenti R, 2012, IEEE T IMAGE PROCESS, V21, P802, DOI 10.1109/TIP.2011.2162740
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang K, 2019, PROC CVPR IEEE, P8604, DOI [10.1109/CVPR.2019.00881, 10.1109/CVPR.2019.01218]
   Wang YM, 2022, PROC CVPR IEEE, P19354, DOI 10.1109/CVPR52688.2022.01877
   Xucong Zhang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P365, DOI 10.1007/978-3-030-58558-7_22
   Yan CG, 2021, IEEE T PATTERN ANAL, V43, P1445, DOI 10.1109/TPAMI.2020.2975798
   Yan CG, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3472810
   Yan CG, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3468872
   Yan CG, 2022, IEEE T CIRC SYST VID, V32, P43, DOI 10.1109/TCSVT.2021.3067449
   Yan CG, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3404374
   Yu Y, 2020, PROC CVPR IEEE, P7312, DOI 10.1109/CVPR42600.2020.00734
   Yu Y, 2019, PROC CVPR IEEE, P11929, DOI 10.1109/CVPR.2019.01221
   Zhang H, 2021, IEEE T IMAGE PROCESS, V30, P5287, DOI 10.1109/TIP.2021.3082298
   Zhang XC, 2019, IEEE T PATTERN ANAL, V41, P162, DOI 10.1109/TPAMI.2017.2778103
   Zhang X, 2017, IEEE COMPUT SOC CONF, P2299, DOI 10.1109/CVPRW.2017.284
   Zheng ZD, 2021, INT J COMPUT VISION, V129, P1106, DOI 10.1007/s11263-020-01395-y
   Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI [10.1007/978-3-030-01219-9_, 10.1007/978-3-030-01219-9_18]
NR 48
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6996
EP 7011
DI 10.1109/TMM.2024.3358948
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000074
DA 2024-08-05
ER

PT J
AU Dai, Y
   Chen, BT
   Gao, LL
   Song, J
   Shen, HT
AF Dai, Yan
   Chen, Beitao
   Gao, Lianli
   Song, Jingkuan
   Shen, Heng Tao
TI DMH-CL: Dynamic Model Hardness Based Curriculum Learning for Complex
   Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Pose estimation; Computational modeling; Dynamic scheduling;
   Task analysis; Robustness; Training data; Multi-person Pose Estimation;
   Dynamic Model Hardness; Curriculum Learning
ID RECOGNITION; NETWORK
AB When dealing with crowds, occlusions, and truncations in complex scenes, existing solutions for multi-person pose estimation remain challenging. This is because all examples are randomly organized and equally treated by previous methods during training, which ignores that examples vary significantly in their difficulty levels. Once trained, hard examples are underutilized due to the high proportion of simple training examples, resulting in poor model robustness for complex scenes. To tackle this, we propose a novel training strategy termed DMH-CL for complex pose estimation, brought from curriculum learning (CL) which mainly addresses easy examples in the early training stage and hard ones in the later stage. Different from typical CL methods, we define easy/hard examples via mining both the dataset-specific statistical difficulty and the multi-model evaluated difficulty. After that, we adopt an annealing arrangement strategy to construct learning courses from easy to hard. Furthermore, we introduce a model learning feedback indicator, i.e., Dynamic Model Hardness (DMH) to conduct course scheduling, and to explicitly explore hard poses and utilize the knowledge learned from easy poses to better handle complex scenes as well. Our DMH-CL is model-agnostic and can be easily applied to various pose estimators including single-stage models and two-stage models, and achieves significant improvements on two challenging benchmarks especially for complex scenes. Notably, it achieves substantial performance gains of 2.6% and 4.6% for hard poses compared to the strong single-stage model PETR on CrowdPose and COCO datasets, respectively. Source codes and models are publicly available online.
C1 [Dai, Yan; Chen, Beitao; Gao, Lianli; Song, Jingkuan; Shen, Heng Tao] Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Peoples R China.
   [Dai, Yan; Chen, Beitao; Gao, Lianli; Song, Jingkuan; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Dai, Yan] China Elect Technol Grp Corp, Res Inst 29, Chengdu 610036, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; China Electronics Technology
   Group
RP Song, JK (corresponding author), Univ Elect Sci & Technol China, Future Media Ctr, Chengdu 611731, Peoples R China.; Song, JK (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM yandai1019@gmail.com; chenbeitao@gmail.com; lianli.gao@uestc.edu.cn;
   jingkuan.song@gmail.com; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI song, jingkuan/0000-0002-2549-8322
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Dai Y, 2021, AAAI CONF ARTIF INTE, V35, P1193
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Fangyun Wei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P527, DOI 10.1007/978-3-030-58607-2_31
   Ghiasi G, 2021, PROC CVPR IEEE, P2917, DOI 10.1109/CVPR46437.2021.00294
   Graves A, 2017, PR MACH LEARN RES, V70
   Hacohen G, 2019, PR MACH LEARN RES, V97
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ionescu RT, 2016, PROC CVPR IEEE, P2157, DOI 10.1109/CVPR.2016.237
   Kingma D. P., 2014, arXiv
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Mao WA, 2021, PROC CVPR IEEE, P9030, DOI 10.1109/CVPR46437.2021.00892
   Matiisen T, 2020, IEEE T NEUR NET LEAR, V31, P3732, DOI 10.1109/TNNLS.2019.2934906
   Newell A, 2017, ADV NEUR IN, V30
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Nie XC, 2019, IEEE I CONF COMP VIS, P6950, DOI 10.1109/ICCV.2019.00705
   Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17
   Platanios EA, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P1162
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Secil S, 2022, ROBOT CIM-INT MANUF, V73, DOI 10.1016/j.rcim.2021.102253
   Shi DH, 2022, PROC CVPR IEEE, P11059, DOI 10.1109/CVPR52688.2022.01079
   Shi DH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3079, DOI 10.1145/3474085.3475447
   Shotton J, 2013, COMMUN ACM, V56, P116, DOI 10.1145/2398356.2398381
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HQ, 2023, INT J COMPUT VISION, V131, P607, DOI 10.1007/s11263-022-01721-6
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang X, 2022, IEEE T PATTERN ANAL, V44, P4555, DOI 10.1109/TPAMI.2021.3069908
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xiong Zinan, 2022, 2022 IEEE 5th International Conference on Multimedia Information Processing and Retrieval (MIPR), P228, DOI 10.1109/MIPR54900.2022.00048
   Yang Y, 2019, IEEE T MULTIMEDIA, V21, P809, DOI 10.1109/TMM.2018.2867742
   Zhang Ji, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P2586, DOI 10.1145/3503161.3547835
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhou T., 2020, P INT C NEUR INF PRO, P8602
   Zhou TF, 2023, IEEE T PATTERN ANAL, V45, P8296, DOI 10.1109/TPAMI.2023.3239194
   Zhou TF, 2021, PROC CVPR IEEE, P1622, DOI 10.1109/CVPR46437.2021.00167
NR 45
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3180
EP 3193
DI 10.1109/TMM.2023.3307935
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200031
DA 2024-08-05
ER

PT J
AU Ding, HJ
   Wang, B
   Kang, GL
   Li, WJ
   He, CH
   Zhao, Y
   Wei, YC
AF Ding, Haojie
   Wang, Bin
   Kang, Guoliang
   Li, Weijia
   He, Conghui
   Zhao, Yao
   Wei, Yunchao
TI DropQueries: A Simple Way to Discover Comprehensive Segment
   Representations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Semantic segmentation; Decoding; Transformers; Buildings;
   Benchmark testing; Task analysis
AB Inspired by the recent progress in object detection (i.e., DETR), the set prediction mechanism significantly advances the research of semantic segmentation and achieves state-of-the-art performance on popular segmentation benchmarks. The generic pipeline of such a mechanism often firstly takes learnable query features to predict classes and segment masks separately and then blends these class-aware segment masks into the final segmentation mask. One key factor behind the successful training of this pipeline is to apply the bipartite matching strategy between the set of predictions and ground-truth segments. However, we find that the bipartite matching-based assignment often tends to segment one target class with only a few learnable queries, making many other pre-defined queries useless. In this article, we propose a simple way, named DropQueries (DQ), to facilitate the set prediction based segmentation architectures. At each iteration of training, our DQ randomly and independently drops each learnable query with a certain probability before bipartite matching. In this way, more queries are encouraged to participate in the segmentation process to discover comprehensive segment representations. We conduct extensive experiments using MaskFormer and Mask2Former as two basic yet powerful segmentation architectures. Without bells and whistles, our DQ strategy can bring consistent improvements over strong baselines on popular semantic segmentation benchmarks, including ADE 20 K, Cityscapes, COCO Stuff 10 K and VSPW.
C1 [Ding, Haojie; Zhao, Yao; Wei, Yunchao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.
   [Ding, Haojie; Zhao, Yao; Wei, Yunchao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wang, Bin; He, Conghui] Shanghai Artificial Intelligence Lab, Shanghai 201203, Peoples R China.
   [Kang, Guoliang] Beihang Univ, Coll Comp Sci & Technol, Beijing 310058, Peoples R China.
   [Li, Weijia] Sun Yat Sen Univ, Sch Geospatial Engn & Sci, Zhuhai 510275, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Beihang
   University; Sun Yat Sen University
RP Wei, YC (corresponding author), Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network, Beijing 100044, Peoples R China.; Wei, YC (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM haojie.ding@bjtu.edu.cn; wangbin@pjlab.org.cn; kgl.prml@gmail.com;
   liweij29@mail.sysu.edu.cn; heconghui@pjlab.org.cn; yzhao@bjtu.edu.cn;
   yunchao.wei@bjtu.edu.cn
RI He, Conghui/AAZ-3323-2021
OI Wang, Bin/0000-0002-5625-2966; ding, haojie/0009-0007-0746-4376; Zhao,
   Yao/0000-0002-8581-9554
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR [Anonymous], 2021, arXiv
   [Anonymous], 2022, P IEEE C COMP VIA FL
   [Anonymous], 2021, P AID NEUR INF PROC, P12077
   [Anonymous], 2021, P INT C LEAM REPR
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Carion N., 2020, EUR C COMP VIS, P213
   Chen G., 2017, ARXIV
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Q, 2023, P INT C COMP VIC
   Chen Y. G., 2018, PROSE EA C COMP VIS, P813
   Cheng A. G., 2021, P AB NEUR INF PROC X, P17664
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Chong II, 2000, P IEEE C COMP VIS PA, P12472
   Cordis M., 2016, P IEEE C COMP VIS PA, P3211
   Demovitskiy A., 2021, P C LEARN REPR
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao Y., 2021, IEEE T MULTIMED 0329, DOI [10.1109/TMM20233262973, DOI 10.1109/TMM20233262973]
   Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P844, DOI 10.1109/ICCV48922.2021.00090
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kirillov A, 2019, PROC CVPR IEEE, P9396, DOI 10.1109/CVPR.2019.00963
   Kulm W., 1955, NAV RES LOGISTICS 4, V23, P97
   Li Z., 2023, E T MULTIMEDIA 0322, V22, DOI 10.110WTMM 2021 3260631
   Liang Chen, 2022, P ADV NEUR INF PROC, V35, P31360
   Lietal, 2013, P IEEE C COMP VIS PA, P3041
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu SA, 2023, PROC CVPR IEEE, P11319, DOI 10.1109/CVPR52729.2023.01089
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   LLeshchilov F., 2019, P INT C LEAN REPR
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma H., 2022, IEEE T MULTIMEDIA, V25, P2774
   Miao JX, 2021, PROC CVPR IEEE, P4131, DOI 10.1109/CVPR46437.2021.00412
   Qiu I, 2021, IEEE T MULTIME VID, V22, P3019
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Thu X., 2021, P C LEARN REPR
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang B, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3663
   Wang IL., 2021, P IEEE C COMP MS PAT, P5499
   Wang J., 2001, IEEE T PATTERN RECOG, V43, P3349, DOI [10.1109/TPAML2020.2983686, DOI 10.1109/TPAML2020.2983686]
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xian T., 2010, P FAT C COMP VIS, P418
   Yuan Y., 2020, P FAT C COMP K, P190
   Yuan YH, 2021, INT J COMPUT VISION, V129, P2375, DOI 10.1007/s11263-021-01465-9
   Zhang WW, 2021, ADV NEUR IN, V34
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhong Z., 2020, P IEEE CVF C COMP VI, P13062
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 56
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3481
EP 3490
DI 10.1109/TMM.2023.3311909
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200036
DA 2024-08-05
ER

PT J
AU Ding, SZ
   Chen, JZ
   Wang, Y
   Kang, Y
   Song, WG
   Cheng, J
   Cao, Y
AF Ding, Saizhe
   Chen, Jinze
   Wang, Yang
   Kang, Yu
   Song, Weiguo
   Cheng, Jie
   Cao, Yang
TI E-MLB: Multilevel Benchmark for Event-Based Camera Denoising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event camera; event denoising; nonreference denoising metric
AB Event cameras, such as dynamic vision sensors (DVS), are biologically inspired vision sensors that have advanced over conventional cameras in high dynamic range, low latency and low power consumption, showing great application potential in many fields. Event cameras are more sensitive to junction leakage current and photocurrent as they output differential signals, losing the smoothing function of the integral imaging process in the RGB camera. The logarithmic conversion further amplifies noise, especially in low-contrast conditions. Recently, researchers proposed a series of datasets and evaluation metrics but limitations remain: 1) the existing datasets are small in scale and insufficient in noise diversity, which cannot reflect the authentic working environments of event cameras; and 2) the existing denoising evaluation metrics are mostly referenced evaluation metrics, relying on APS information or manual annotation. To address the above issues, we construct a large-scale event denoising dataset (multilevel benchmark for event denoising, E-MLB) for the first time, which consists of 100 scenes, each with four noise levels, that is 12 times larger than the largest existing denoising dataset. We also propose the first nonreference event denoising metric, the event structural ratio (ESR), which measures the structural intensity of given events. ESR is inspired by the contrast metric, but is independent of the number of events and projection direction. Based on the proposed benchmark and ESR, we evaluate the most representative denoising algorithms, including classic and SOTA, and provide denoising baselines under various scenes and noise levels. The corresponding results and codes are available at https://github.com/KugaMaxx/cuke-emlb.
C1 [Ding, Saizhe; Song, Weiguo] Univ Sci & Technol China, State Key Lab Fire Sci, Hefei 230026, Peoples R China.
   [Chen, Jinze; Wang, Yang; Kang, Yu; Cao, Yang] Univ Sci & Technol China, Sch Informat, Hefei 230026, Peoples R China.
   [Cheng, Jie] Huawei Technol Co Ltd, Shenzhen 518063, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Huawei Technologies
RP Wang, Y (corresponding author), Univ Sci & Technol China, Sch Informat, Hefei 230026, Peoples R China.
EM dszh2020@mail.ustc.edu.cn; chjz@mail.ustc.edu.cn; ywang120@ustc.edu.cn;
   kangduyu@ustc.edu.cn; wgsong@ustc.edu.cn; chengjie8@huawei.com;
   forrest@ustc.edu.cn
RI chen, xu/JNT-3068-2023; wang, zhe/JNE-3510-2023; Wang,
   Zejun/KBB-8454-2024; wu, p/JDW-5015-2023; wang, mengyi/KEI-9461-2024;
   CHEN, WENJIE/JQW-1608-2023; chen, si/JPK-4258-2023; xu,
   chen/JNE-5010-2023; Zhang, Xiaofeng/JMC-6060-2023; li, bo/JJC-2664-2023;
   Jiang, Yuan/JED-3759-2023; Song, Weiguo/D-6041-2016; wang,
   chen/JED-7289-2023; LIU, HUI/JPX-8014-2023; shi, chen/KEH-8339-2024;
   sheng, chen/JEO-8801-2023; li, jing/KHY-5337-2024; chen,
   chen/JGD-3057-2023
OI Zhang, Xiaofeng/0000-0003-2738-3286; Song, Weiguo/0000-0001-5390-6787; 
FU National Natural Science Foundation of China
FX No Statement Available
CR Afshar S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20061600
   Baldwin RW, 2020, PROC CVPR IEEE, P1698, DOI 10.1109/CVPR42600.2020.00177
   Baldwin RW, 2019, LECT NOTES COMPUT SC, V11663, P395, DOI 10.1007/978-3-030-27272-2_35
   Chen J., 2022, arXiv
   Czech D, 2016, P IEEE RAS-EMBS INT, P19, DOI 10.1109/BIOROB.2016.7523452
   Delbruck T., 2008, P INT S SECURE LIF, P21
   Duan PQ, 2021, PROC CVPR IEEE, P12819, DOI 10.1109/CVPR46437.2021.01263
   Duan PQ, 2022, IEEE T PATTERN ANAL, V44, P8261, DOI 10.1109/TPAMI.2021.3113344
   Feng Y, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10062024
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gallego G, 2019, PROC CVPR IEEE, P12272, DOI 10.1109/CVPR.2019.01256
   Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407
   Gallego G, 2018, IEEE T PATTERN ANAL, V40, P2402, DOI 10.1109/TPAMI.2017.2769655
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Gehrig D, 2020, INT J COMPUT VISION, V128, P601, DOI 10.1007/s11263-019-01209-w
   Guo SS, 2020, Arxiv, DOI arXiv:2006.01687
   Guo S, 2023, IEEE T PATTERN ANAL, V45, P785, DOI 10.1109/TPAMI.2022.3152999
   Guo SS, 2020, ASIA S PACIF DES AUT, P452, DOI 10.1109/ASP-DAC47756.2020.9045268
   Han J, 2020, PROC CVPR IEEE, P1727, DOI 10.1109/CVPR42600.2020.00180
   Hu YH, 2021, IEEE COMPUT SOC CONF, P1312, DOI 10.1109/CVPRW53098.2021.00144
   Khodamoradi A, 2021, IEEE T EMERG TOP COM, V9, P15, DOI 10.1109/TETC.2017.2788865
   Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21
   Kueng B, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P16, DOI 10.1109/IROS.2016.7758089
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Li SQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4460, DOI 10.1109/ICCV48922.2021.00444
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lin S., 2022, PROC IEEECVF C COMPU, P16344
   Liu HJ, 2015, IEEE INT SYMP CIRC S, P722, DOI 10.1109/ISCAS.2015.7168735
   Mueggler E., 2017, Fast event-based corner detection
   Nozaki Y, 2017, IEEE T ELECTRON DEV, V64, P3239, DOI 10.1109/TED.2017.2717848
   Padala V, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00118
   Pan LY, 2020, PROC CVPR IEEE, P1669, DOI 10.1109/CVPR42600.2020.00174
   Rebecq H., 2018, C ROB LEARN, P969, DOI DOI 10.1007/978-3-319-24574-4_28
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143
   Rudnev V., 2021, P IEEECVF INT C COMP, P12385
   Taverni G, 2018, IEEE T CIRCUITS-II, V65, P677, DOI 10.1109/TCSII.2018.2824899
   Tulyakov S., 2022, arXiv
   Tulyakov S, 2021, PROC CVPR IEEE, P16150, DOI 10.1109/CVPR46437.2021.01589
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Wang YX, 2019, PROC CVPR IEEE, P6351, DOI 10.1109/CVPR.2019.00652
   Wang ZHW, 2020, PROC CVPR IEEE, P1606, DOI 10.1109/CVPR42600.2020.00168
   Weikersdorfer D, 2014, IEEE INT CONF ROBOT, P359, DOI 10.1109/ICRA.2014.6906882
   Wu JJ, 2021, IEEE T MULTIMEDIA, V23, P1148, DOI 10.1109/TMM.2020.2993957
   Wu JJ, 2020, INT CONF ACOUST SPEE, P4437, DOI [10.1109/ICASSP40776.2020.9053002, 10.1109/icassp40776.2020.9053002]
   Xie XM, 2018, PROC SPIE, V10615, DOI 10.1117/12.2305260
   Xu JT, 2016, OPT ENG, V55, DOI 10.1117/1.OE.55.6.063103
   Yu Zhiyang, 2021, P IEEECVF INT C COMP, P14589
   Zhang Xiang, 2022, P IEEECVF C COMPUTER, P17765
   Zhou Y, 2021, IEEE T ROBOT, V37, P1433, DOI 10.1109/TRO.2021.3062252
   Zhu AZ, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108
NR 52
TC 2
Z9 2
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 65
EP 76
DI 10.1109/TMM.2023.3260638
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500005
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Gong, SJ
   Yang, J
   Zhang, SS
AF Gong, Shenjian
   Yang, Jian
   Zhang, Shanshan
TI Adaptive Teaching for Cross-Domain Crowd Counting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowd counting; domain adaptation; mean teacher
AB The main challenge of Unsupervised Domain Adaptation (UDA) crowd counting is the large domain gap between a synthetic domain with annotations (source) and a real-world domain of interest without annotations (target). Previous mainstream UDA crowd counting methods either employ feature alignment or a semi-supervised learning paradigm via pseudo-labels. We for the first time combine both of their advantages and propose an Adversarial Mean Teacher (AMT) framework. On the one hand, we optimize the student model with domain adversarial learning. On the other hand, we feed perturbed target images to the teacher model to generate pseudo-labels. Furthermore, to improve the quality of the pseudo-labels, we propose an Adaptive Teaching (AT) module, consisting of pseudo-label refinement and credible pseudo-label selection. Concretely, we first generate two candidate pseudo-labels from the prediction of the teacher model and obtain a refined pseudo-label by mixing them at the pixel-level. Moreover, we introduce an auxiliary task of foreground-background classification to assist credible region selection and only activate supervision signals on those regions. Extensive experiments on four real-world crowd counting benchmarks demonstrate the effectiveness of our method namely Cross-Domain Adaptive Teacher (CDAT).
C1 [Gong, Shenjian; Yang, Jian; Zhang, Shanshan] Nanjing Univ Sci & Technol, PCA Lab, Nanjing 210094, Peoples R China.
   [Gong, Shenjian; Yang, Jian; Zhang, Shanshan] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens Inf, Minist Educ, Nanjing 210094, Peoples R China.
   [Gong, Shenjian; Yang, Jian; Zhang, Shanshan] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology; Nanjing University of Science & Technology
RP Zhang, SS (corresponding author), Nanjing Univ Sci & Technol, PCA Lab, Nanjing 210094, Peoples R China.
EM shenjiangong@njust.edu.cn; csjyang@njust.edu.cn;
   shanshan.zhang@njust.edu.cn
RI Gong, Shenjian/IAQ-9846-2023
OI Gong, Shenjian/0000-0002-5231-4146
FU National Natural Science Foundation of China
FX No Statement Available
CR Cheng ZQ, 2022, PROC CVPR IEEE, P19606, DOI 10.1109/CVPR52688.2022.01902
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Gao JY, 2023, IEEE T NEUR NET LEAR, V34, P4803, DOI 10.1109/TNNLS.2021.3124272
   Gao JY, 2021, IEEE T CYBERNETICS, V51, P4822, DOI 10.1109/TCYB.2020.3034316
   Gao JY, 2020, IEEE T CIRC SYST VID, V30, P3486, DOI 10.1109/TCSVT.2019.2919139
   Gong S, 2022, P IEEE CVF C COMP VI, P7542
   Han T, 2020, INT CONF ACOUST SPEE, P1848, DOI [10.1109/ICASSP40776.2020.9054768, 10.1109/icassp40776.2020.9054768]
   Huang SY, 2020, INT CONF ACOUST SPEE, P2578, DOI [10.1109/icassp40776.2020.9053070, 10.1109/ICASSP40776.2020.9053070]
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Li YJ, 2022, PROC CVPR IEEE, P7571, DOI 10.1109/CVPR52688.2022.00743
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lin H, 2022, PROC CVPR IEEE, P19596, DOI 10.1109/CVPR52688.2022.01901
   Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849
   Liu WZ, 2022, PROC CVPR IEEE, P5331, DOI 10.1109/CVPR52688.2022.00527
   Liu YT, 2020, IEEE T IMAGE PROCESS, V29, P6800, DOI 10.1109/TIP.2020.2994410
   Ma YM, 2022, IEEE IMAGE PROC, P3256, DOI 10.1109/ICIP46576.2022.9897322
   Miao YQ, 2020, AAAI CONF ARTIF INTE, V34, P11765
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P212, DOI 10.1007/978-3-030-58621-8_13
   Sindagi VA, 2019, 2019 16TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS), DOI 10.1109/avss.2019.8909889
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Tian Y, 2021, Arxiv, DOI arXiv:2109.14483
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Yang S., 2022, P AAAI C ARTIFICIAL, P23
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhang J, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6436, DOI 10.1145/3503161.3547863
   Zhang Q, 2020, AAAI CONF ARTIF INTE, V34, P12837
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 29
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2943
EP 2952
DI 10.1109/TMM.2023.3305815
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400021
DA 2024-08-05
ER

PT J
AU Guo, HF
   Kwong, S
   Ye, DJ
   Wang, SQ
AF Guo, Haifeng
   Kwong, Sam
   Ye, Dongjie
   Wang, Shiqi
TI Enhanced Context Mining and Filtering for Learned Video Compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video compression; Context modeling; Transformers; Image coding;
   Entropy; Filtering; Codes; Learned video compression; end-to-end
   training approach; enhanced context mining; in loop filtering
AB The Deep Contextual Video Compression framework (DCVC) utilizes a conditional coding paradigm, where the context is extracted and employed as a condition for the contextual encoder-decoder and entropy model. In this paper, we propose enhanced context mining and filtering to improve the compression efficiency of DCVC. Firstly, considering the context of DCVC is generated without supervision and redundancy may exist among context channels, an enhanced context mining model is proposed to mitigate redundancy across context channels to obtain superior context features. Then, we introduce a transformer-based enhancement network as a filtering module to capture long-distance dependencies and further enhance compression efficiency. The transformer-based enhancement adopts a full-resolution pipeline and calculates self-attention across channel dimensions. By combining the local modeling ability of the enhanced context mining model and the non-local modeling ability of the transformer-based enhancement network, our model outperforms LDP configurations of Versatile Video Coding (VVC), achieving an average bit savings of 6.7% in terms of MS-SSIM.
C1 [Guo, Haifeng; Ye, Dongjie; Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Kwong, Sam] Lingnan Univ, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Lingnan University
RP Kwong, S (corresponding author), Lingnan Univ, Hong Kong, Peoples R China.
EM haifenguo2-c@my.cityu.edu.hk; samkwong@ln.edu.hk; dj.ye@my.cityu.edu.hk;
   shiqwang@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; Guo, Haifeng/KIK-1038-2024
OI Kwong, Sam/0000-0001-7484-7261; Guo, Haifeng/0000-0001-7517-6359; YE,
   Dongjie/0000-0002-4118-5625
FU Key Project of Science and Technology Innovation 2030
FX No Statement Available
CR Adaloglou N, 2020, Al Summer
   Agustsson E, 2019, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2019.00031
   Agustsson Eirikur, 2020, P IEEE CVF C COMP VI, P8503, DOI DOI 10.1109/CVPR42600.2020.00853
   Balle J., 2017, P INT C LEARN REPR
   Balle J., 2018, PROC INT C LEARN REP
   Ballé J, 2016, PICT COD SYMP, DOI 10.1109/pcs.2016.7906310
   Begaint J., 2020, arXiv
   Bjontegaard G., 2001, Calculation of Average PSNR Differences between RDcurves
   Bossen F., 2013, JCTVCL1100
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding DD, 2021, P IEEE, V109, P1494, DOI 10.1109/JPROC.2021.3059994
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Gao CS, 2023, IEEE T MULTIMEDIA, V25, P721, DOI 10.1109/TMM.2021.3130754
   Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260
   Guo HF, 2023, IEEE SIGNAL PROC LET, V30, P673, DOI 10.1109/LSP.2023.3277343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu YY, 2020, AAAI CONF ARTIF INTE, V34, P11013
   Hu ZH, 2022, PROC CVPR IEEE, P5911, DOI 10.1109/CVPR52688.2022.00583
   Hu ZH, 2021, PROC CVPR IEEE, P1502, DOI 10.1109/CVPR46437.2021.00155
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Johnston N, 2018, PROC CVPR IEEE, P4385, DOI 10.1109/CVPR.2018.00461
   Katsenou AV, 2021, IEEE T MULTIMEDIA, V23, P26, DOI 10.1109/TMM.2020.2976591
   Lee J, 2018, PROC INT C LEARN REP
   Li J., 2021, P ADV NEUR INF PROC, V34, p18 114
   Li JH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1503, DOI 10.1145/3503161.3547845
   Lin JP, 2020, PROC CVPR IEEE, P3543, DOI 10.1109/CVPR42600.2020.00360
   Lin K, 2023, IEEE T CIRC SYST VID, V33, P3502, DOI 10.1109/TCSVT.2022.3233221
   Liu D, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3368405
   Liu HH, 2020, IEEE T IMAGE PROCESS, V29, P641, DOI 10.1109/TIP.2019.2933743
   Liu X, 2019, PROC CVPR IEEE, P7000, DOI 10.1109/CVPR.2019.00717
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2018, INT C LEARN REPR
   Lu G, 2021, IEEE T PATTERN ANAL, V43, P3292, DOI 10.1109/TPAMI.2020.2988453
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Luo P, 2021, IEEE T PATTERN ANAL, V43, P712, DOI 10.1109/TPAMI.2019.2932062
   Ma D, 2022, IEEE T MULTIMEDIA, V24, P3847, DOI 10.1109/TMM.2021.3108943
   Ma D, 2021, IEEE J-STSP, V15, P378, DOI 10.1109/JSTSP.2020.3043064
   Ma SW, 2020, IEEE T CIRC SYST VID, V30, P1683, DOI 10.1109/TCSVT.2019.2910119
   Mei YQ, 2023, INT J COMPUT VISION, V131, P3207, DOI 10.1007/s11263-023-01843-5
   Mentzer F., 2020, Advances in Neural Information Processing Systems, V33, P11913
   Mentzer F., 2022, P ADV NEUR INF PROC, V35, P13091
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Mercat A, 2020, MMSYS'20: PROCEEDINGS OF THE 2020 MULTIMEDIA SYSTEMS CONFERENCE, P297, DOI 10.1145/3339825.3394937
   Minnen D, 2018, ADV NEUR IN, V31
   Pan ZQ, 2020, IEEE T IMAGE PROCESS, V29, P5352, DOI 10.1109/TIP.2020.2982534
   Paszke A, 2019, ADV NEUR IN, V32
   Pham CDK, 2021, IEEE COMPUT SOC CONF, P1861, DOI 10.1109/CVPRW53098.2021.00206
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sheng XH, 2023, IEEE T MULTIMEDIA, V25, P7311, DOI 10.1109/TMM.2022.3220421
   Sullivan G.J., 2014, High Efficiency Video Coding (HEVC)"
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun HM, 2020, IEEE T MULTIMEDIA, V22, P2764, DOI 10.1109/TMM.2019.2963620
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Toderici G, 2017, PROC CVPR IEEE, P5435, DOI 10.1109/CVPR.2017.577
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HG, 2016, IEEE IMAGE PROC, P1509, DOI 10.1109/ICIP.2016.7532610
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xu JJ, 2019, ADV NEUR IN, V32
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang R, 2021, IEEE J-STSP, V15, P388, DOI 10.1109/JSTSP.2020.3043590
   Yang R, 2020, PROC CVPR IEEE, P6627, DOI 10.1109/CVPR42600.2020.00666
   Yi X., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM2022.3233245, DOI 10.1109/TMM2022.3233245]
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang F, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102912
   Zhang Y., 2018, PROC INT C LEARN REP
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhang Y, 2020, INFORM SCIENCES, V506, P395, DOI 10.1016/j.ins.2019.07.096
   Zhao TS, 2023, IEEE T MULTIMEDIA, V25, P6411, DOI 10.1109/TMM.2022.3208516
   Zhengxue Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7936, DOI 10.1109/CVPR42600.2020.00796
   Zhihao Hu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P193, DOI 10.1007/978-3-030-58536-5_12
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
NR 76
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3814
EP 3826
DI 10.1109/TMM.2023.3316429
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300011
DA 2024-08-05
ER

PT J
AU Han, MY
   Zhan, YB
   Luo, Y
   Hu, H
   Su, KH
   Du, B
AF Han, Mengya
   Zhan, Yibing
   Luo, Yong
   Hu, Han
   Su, Kehua
   Du, Bo
TI Textual Enhanced Adaptive Meta-Fusion for Few-Shot Visual Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot visual recognition; semantic information; multimodal fusion
AB Few-shot learning (FSL) is a challenging task that aims to train a classifier to recognize novel categories, where only a few annotated examples are available in each category. Recently, many FSL approaches have been proposed based on the meta-learning paradigm, which attempts to learn transferable knowledge from similar tasks by designing a meta-learner. However, most of these approaches only exploit the information from visual modality and do not utilize ones from additional modalities (e.g., textual description). Since the labeled examples in FSL are limited, increasing the information on the examples is a probable solution to improve the classification performance. This motivates us to propose a novel meta-learning method, termed textual enhanced adaptive meta-fusion FSL (TAMF-FSL), which leverages both the visual information from the visual image and semantic information from language supervision. Specifically, TAMF-FSL exploits the semantic information of textual description to improve the visual-based models. We first employ a text encoder to learn the semantic features of each visual category, and then design a modality alignment module and meta-fusion module to align and fuse the visual and semantic features for final prediction. Extensive experiments show that the proposed method outperforms many recent or competitive FSL counterparts on two popular datasets.
C1 [Han, Mengya; Luo, Yong; Su, Kehua; Du, Bo] Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Inst Artificial Intelligence, Wuhan 430072, Peoples R China.
   [Han, Mengya; Luo, Yong; Su, Kehua; Du, Bo] Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430072, Peoples R China.
   [Han, Mengya; Luo, Yong; Su, Kehua; Du, Bo] Hubei Luojia Lab, Wuhan 430079, Peoples R China.
   [Zhan, Yibing] JD Explore Acad, Beijing 101111, Peoples R China.
   [Hu, Han] Beijing Inst Technol, Sch Informat & Elect, Beijing 100081, Peoples R China.
C3 Wuhan University; Wuhan University; Beijing Institute of Technology
RP Luo, Y; Su, KH (corresponding author), Wuhan Univ, Natl Engn Res Ctr Multimedia Software, Sch Comp Sci, Inst Artificial Intelligence, Wuhan 430072, Peoples R China.; Luo, Y; Su, KH (corresponding author), Wuhan Univ, Hubei Key Lab Multimedia & Network Commun Engn, Wuhan 430072, Peoples R China.
EM myhan1996@whu.edu.cn; zhanyibing@jd.com; yluo180@gmail.com;
   hhu@bit.edu.cn; skh@whu.edu.cn; dubo@whu.edu.cn
OI Luo, Yong/0000-0002-2296-6370; Hu, Han/0000-0001-7532-0496; Su,
   Kehua/0000-0002-1384-9762
FU National Key Research and Development Program of China
FX No Statement Available
CR Boney R, 2018, Arxiv, DOI arXiv:1711.10856
   Chen W.-Y., 2017, P INT C LEARN REPR
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   Chen YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9042, DOI 10.1109/ICCV48922.2021.00893
   Chen ZY, 2021, PROC CVPR IEEE, P13658, DOI 10.1109/CVPR46437.2021.01345
   Chen ZT, 2019, IEEE T IMAGE PROCESS, V28, P4594, DOI 10.1109/TIP.2019.2910052
   Deng SL, 2023, IEEE T MULTIMEDIA, V25, P5763, DOI 10.1109/TMM.2022.3198880
   Elsken Thomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12362, DOI 10.1109/CVPR42600.2020.01238
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Guan Weili, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P268, DOI 10.1145/3503161.3548020
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong RC, 2015, IEEE T MULTIMEDIA, V17, P1980, DOI 10.1109/TMM.2015.2476657
   Hou RB, 2019, ADV NEUR IN, V32
   Hu Y, 2022, IEEE T MULTIMEDIA, V24, P2473, DOI 10.1109/TMM.2021.3082292
   Kai Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13467, DOI 10.1109/CVPR42600.2020.01348
   Lee H, 2020, PR MACH LEARN RES, V119
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li H., 2021, P IEEE INT C MULT EX, P1
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3240195
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Pahde F, 2019, IEEE WINT CONF APPL, P218, DOI 10.1109/WACV.2019.00029
   Pahde F, 2018, IEEE IMAGE PROC, P156, DOI 10.1109/ICIP.2018.8451372
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Ravi S., 2017, INT C LEARN REPR
   Ren M., 2018, 6 INT C LEARN REPR
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rusu A.A., 2019, ICLR
   Simon C, 2020, PROC CVPR IEEE, P4135, DOI 10.1109/CVPR42600.2020.00419
   Snell J, 2017, ADV NEUR IN, V30
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sun Y., 2022, Advances in Neural Information Processing Systems, V35, P37484
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O., 2016, P ADV NEUR INF PROC, P3637
   Wang BW, 2021, IEEE COMPUT SOC CONF, P2294, DOI 10.1109/CVPRW53098.2021.00259
   Wang DP, 2023, IEEE T MULTIMEDIA, V25, P2966, DOI 10.1109/TMM.2022.3154149
   Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760
   Xing C., 2019, ADV NEURAL INFORM PR, P4847
   Xu XZ, 2022, IEEE T MULTIMEDIA, V24, P2752, DOI 10.1109/TMM.2021.3087098
   Xu ZX, 2021, NEUROCOMPUTING, V432, P124, DOI 10.1016/j.neucom.2020.08.034
   Yiluan Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13496, DOI 10.1109/CVPR42600.2020.01351
   Yonglong Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P266, DOI 10.1007/978-3-030-58568-6_16
   Zhang RX, 2018, ADV NEUR IN, V31
   Zhang Y, 2021, Arxiv, DOI arXiv:2106.14467
   Zhang Z., 2021, P INT JOINT C ART IN, P3420
   Zhong X, 2023, IEEE T MULTIMEDIA, V25, P1979, DOI 10.1109/TMM.2022.3141886
   Zhou F, 2022, IEEE T CIRC SYST VID, V32, P6863, DOI 10.1109/TCSVT.2022.3173687
   Zhou Y., 2022, arXiv
NR 53
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2408
EP 2418
DI 10.1109/TMM.2023.3295731
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100021
DA 2024-08-05
ER

PT J
AU Hyun, MH
   Lee, B
   Kim, M
AF Hyun, Myung Han
   Lee, Bumshik
   Kim, Munchurl
TI A VVC Intra Rate Control With Small Bit Fluctuations Using a Lagrange
   Multiplier Adjustment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE rate-distortion optimization (RDO); lagrange multiplier adjustment
   (LMA); rate control (RC); versatile video coding (VVC)
ID RATE-DISTORTION-OPTIMIZATION; FRAME RATE CONTROL
AB Since the emergence of high-quality multimedia processing applications such as video streaming, digital editing, archiving, etc. these days, an intra coding rate control (RC) is becoming an indispensable and important technology. In this article, a frame-level intra RC scheme for Versatile Video Coding (VVC) using a Lagrange multiplier adjustment (LMA) is proposed. The VVC test model (VTM) uses an R-lambda model-based rate control. However, the estimation performance of target bits based on an R-lambda-QP relation is decreased because the distortion dependencies among consecutive frames are not considered especially for intra RC. Thus, in a rate-distortion optimization (RDO) based encoding, the lambda values determined for given quantization parameter (QP) values should be elaborately controlled to increase the target bits estimation performance. In our work, we focus on the intra RC scheme by taking advantage of particle-filtering-based prediction (PFP) for distortion estimates, and precise per-frame lambda values can be derived for an appropriate RDO process that can lead to small bit-fluctuations. Our extensive experimental results demonstrate that our RC scheme using the per-frame LMA is superior to the default RC (VTM-16.0rc1) method and the state-of-the-art RC methods with significant margins of average 15.57%, 15.31% and 31.13% improvements in terms of the normalized root mean square error (NRMSE) for All Intra (AI) configuration of VVC, respectively.
C1 [Hyun, Myung Han] Gyeongsang Natl Univ, Dept Control & Robot Engn, Jinju 52828, Gyeongsangnam D, South Korea.
   [Lee, Bumshik] Chosun Univ, Dept Informat & Commun Engn, Gwangju 61452, South Korea.
   [Kim, Munchurl] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
C3 Gyeongsang National University; Chosun University; Korea Advanced
   Institute of Science & Technology (KAIST)
RP Kim, M (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
EM drwise@gnu.ac.kr; bslee@chosun.ac.kr; mkimee@kaist.ac.kr
OI Lee, Bumshik/0000-0003-2482-1869; Hyun, Myung Han/0000-0002-8364-6127
CR [Anonymous], Fujifilm Digital Cameras (X-T3)
   [Anonymous], 1995, ITU T RECOMMENDATION
   [Anonymous], 2022, Digital cinema system specification (DCSS) ver. 1.4.2
   Bjontegaard G., 2001, P ITU T VID COD EXP
   Bossen F., 2019, Document JVET-N1010
   Bross B., 2020, PROC 20 M TELECONFER, P1
   Bross B, 2021, IEEE T CIRC SYST VID, V31, P3736, DOI 10.1109/TCSVT.2021.3101953
   Browne A, 2022, PROC 25 M TELECONFER, P1
   Chen Y, 2020, INT CONF ACOUST SPEE, P4422, DOI [10.1109/ICASSP40776.2020.9054633, 10.1109/icassp40776.2020.9054633]
   Dong XC, 2022, IEEE T MULTIMEDIA, V24, P400, DOI 10.1109/TMM.2021.3052348
   Gao W, 2016, IEEE T MULTIMEDIA, V18, P988, DOI 10.1109/TMM.2016.2535254
   Gao YB, 2019, IEEE T CIRC SYST VID, V29, P546, DOI 10.1109/TCSVT.2017.2787190
   Gong YC, 2017, IEEE T CIRC SYST VID, V27, P1304, DOI 10.1109/TCSVT.2016.2539718
   Guo HW, 2020, IEEE T BROADCAST, V66, P113, DOI 10.1109/TBC.2019.2917402
   He J, 2018, IEEE T CIRC SYST VID, V28, P3424, DOI 10.1109/TCSVT.2017.2751519
   Hyun MH, 2021, IEEE T CIRC SYST VID, V31, P782, DOI 10.1109/TCSVT.2020.2989185
   Hyun MH, 2020, IEEE ACCESS, V8, P227255, DOI 10.1109/ACCESS.2020.3046043
   Imran N, 2015, SPRINGERPLUS, V4, DOI 10.1186/s40064-015-1300-4
   Joint Video Expoloration Team (JVET) of ITU-T VCEG and ISO/IEC MPEG, 2022, VVC reference software
   Karczewicz M, 2013, document JCTVC-M0257
   Li B, 2014, IEEE T IMAGE PROCESS, V23, P3841, DOI 10.1109/TIP.2014.2336550
   Li S, 2016, IEEE T CIRC SYST VID, V26, P117, DOI 10.1109/TCSVT.2015.2450131
   Park SH, 2021, IEEE T MULTIMEDIA, V23, P4388, DOI 10.1109/TMM.2020.3042062
   Sanchez V, 2015, INT CONF ACOUST SPEE, P1250, DOI 10.1109/ICASSP.2015.7178170
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Wang MH, 2015, IEEE SIGNAL PROC LET, V22, P896, DOI 10.1109/LSP.2014.2377032
   Wang XW, 2020, IEEE T IMAGE PROCESS, V29, P9458, DOI 10.1109/TIP.2020.3028280
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P688, DOI 10.1109/TCSVT.2003.815168
   Wiegand T., PROC 7 M
   Zacarias I, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/2354603
   Zhang F, 2019, IEEE T CIRC SYST VID, V29, P3121, DOI 10.1109/TCSVT.2018.2873837
   Zhao TS, 2016, IEEE T IMAGE PROCESS, V25, P2997, DOI 10.1109/TIP.2016.2556941
NR 34
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6811
EP 6821
DI 10.1109/TMM.2024.3355633
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600049
DA 2024-08-05
ER

PT J
AU Lai, L
   Chen, J
   Wu, QY
AF Lai, Lvlong
   Chen, Jian
   Wu, Qingyao
TI Zero-Shot Single-View Point Cloud Reconstruction via Cross-Category
   Knowledge Transferring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot; point cloud; reconstruction
AB Single-view point cloud reconstruction aims to generate a 3D point cloud of an object given one 2D image taken from an arbitrary viewpoint. Most previous works assume that all the test categories have been present to the model during training. However, it is impossible to know all the test categories that the model will meet in advance. And we discover these methods can not deal with novel categories well. Therefore, in this article, we investigate a more realistic and challenging setting of single-view point cloud reconstruction, zero-shot, where the model's performance on novel categories is pursued. Towards this task, we propose the Cross-Category Knowledge Transferring Network (CCKTN), which maintains a knowledge bank to mine transferable knowledge from known categories to help reconstruct novel categories. Additionally, we conduct auxiliary learning for the point cloud reconstruction model with the point cloud autoencoder via sharing the same knowledge bank. This design enables the knowledge bank to collect more fruitful 3D knowledge of point clouds. Moreover, we devise a diversity loss regularization for the knowledge vectors to guarantee their diversities, further enhancing CCKTN's performance. Comprehensive experiments conducted on ShapeNet and ModelNet datasets show CCKTN's superiority towards existing methods and demonstrate CCKTN's effectiveness for reconstructing novel category objects.
C1 [Lai, Lvlong; Chen, Jian] South China Univ Technol, Sch Software Engn, Guangzhou 510641, Peoples R China.
   [Lai, Lvlong] Pazhou Lab, Guangzhou 510641, Peoples R China.
   [Wu, Qingyao] South China Univ Technol, Sch Software Engn, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Peoples R China.
   [Wu, Qingyao] Peng Cheng Lab, Guangzhou 510641, Peoples R China.
C3 South China University of Technology; Pazhou Lab; South China University
   of Technology
RP Wu, QY (corresponding author), South China Univ Technol, Sch Software Engn, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Peoples R China.; Wu, QY (corresponding author), Peng Cheng Lab, Guangzhou 510641, Peoples R China.
EM selailvlong@mail.scut.edu.cn; ellachen@scut.edu.cn; qyw@scut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Achlioptas P, 2018, PR MACH LEARN RES, V80
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298801
   Cai JR, 2022, PROC CVPR IEEE, P8080, DOI 10.1109/CVPR52688.2022.00792
   Cheng TY, 2022, AAAI CONF ARTIF INTE, P427
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Gan YS, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.119209
   Gowda B, 2021, INT CONF IMAG VIS, DOI 10.1109/IVCNZ54163.2021.9653224
   Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030
   Kim J, 2022, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR52688.2022.00431
   Kim M, 2022, IEEE T MULTIMEDIA, V24, P4342, DOI 10.1109/TMM.2021.3115626
   Li B, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22218235
   Li H, 2023, IEEE T MULTIMEDIA, V25, P4752, DOI 10.1109/TMM.2022.3181457
   Li Z., 2018, arXiv
   Lin Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2834, DOI [10.114510.1145/3474085.3475611, 10.1145/3474085.3475611]
   Lin Y, 2021, AAAI CONF ARTIF INTE, V35, P2064
   Liu AA, 2022, IEEE T CIRC SYST VID, V32, P8809, DOI 10.1109/TCSVT.2022.3191761
   Liu S, 2019, ADV NEUR IN, V32
   Mandikal P, 2019, Arxiv, DOI arXiv:1807.07796
   Mandikal P, 2019, IEEE WINT CONF APPL, P1052, DOI 10.1109/WACV.2019.00117
   Michalkiewicz Mateusz, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P614, DOI 10.1007/978-3-030-58595-2_37
   Navaneet KL, 2020, PROC CVPR IEEE, P1129, DOI 10.1109/CVPR42600.2020.00121
   Pang J., 2021, P IEEE CVF C COMP VI, P7453
   Paszke A, 2019, ADV NEUR IN, V32
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song G, 2019, IEEE T MULTIMEDIA, V21, P1261, DOI 10.1109/TMM.2018.2877122
   Sukhbaatar S, 2019, Arxiv, DOI arXiv:1907.01470
   Tang Y., 2021, arXiv
   Thai A, 2021, INT CONF 3D VISION, P85, DOI 10.1109/3DV53792.2021.00019
   Valada A, 2018, IEEE INT CONF ROBOT, P6939, DOI 10.1109/ICRA.2018.8462979
   Vaswani A, 2017, ADV NEUR IN, V30
   Wallace B, 2019, IEEE I CONF COMP VIS, P3817, DOI 10.1109/ICCV.2019.00392
   Wang JQ, 2021, IEEE T CIRC SYST VID, V31, P4909, DOI 10.1109/TCSVT.2021.3051377
   Wang W, 2023, IEEE T MULTIMEDIA, V25, P2661, DOI 10.1109/TMM.2022.3149716
   Wang WJ, 2022, PROC CVPR IEEE, P7055, DOI 10.1109/CVPR52688.2022.00693
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu Y, 2022, IEEE T CIRC SYST VID, V32, P5510, DOI 10.1109/TCSVT.2022.3152800
   Chang AX, 2015, Arxiv, DOI [arXiv:1512.03012, DOI 10.48550/ARXIV.1512.03012]
   Xing Z, 2022, Arxiv, DOI arXiv:2208.00183
   Xu CX, 2022, PROC CVPR IEEE, P6478, DOI 10.1109/CVPR52688.2022.00638
   Yan SM, 2023, Arxiv, DOI [arXiv:2201.00785, 10.48550/arXiv.2201.00785]
   Yang G, 2022, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR52688.2022.00183
   Yang GD, 2019, IEEE I CONF COMP VIS, P4540, DOI 10.1109/ICCV.2019.00464
   Yang XH, 2023, Arxiv, DOI arXiv:2208.02676
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   You K., 2021, P ACM MULT AS, P1
   Yu JY, 2022, PROC CVPR IEEE, P17813, DOI 10.1109/CVPR52688.2022.01731
   Zhang Ji, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P2586, DOI 10.1145/3503161.3547835
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
NR 50
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1448
EP 1459
DI 10.1109/TMM.2023.3282467
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700020
DA 2024-08-05
ER

PT J
AU Liu, H
   Sun, YQ
   Bandoh, Y
   Kitahara, M
   Satoh, S
AF Liu, Hong
   Sun, Yongqing
   Bandoh, Yukihiro
   Kitahara, Masaki
   Satoh, Shin'ichi
TI Deep Counterfactual Representation Learning for Visual Recognition
   Against Weather Corruptions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Meteorology; Representation learning; Adaptation models; Training;
   Feature extraction; Data models; Robustness; Causal effect;
   counterfactual regularization; information-theoretic representation
   learning; robustness; and generalization; weather corruptions
ID AUTONOMOUS VEHICLES; OBJECT DETECTION; RAIN
AB Deep learning has been widely studied for processing and understanding multimedia data, and it does help improve performance. Recent research has shown that deep models are vulnerable to images containing adverse weather corruptions, leading to a safety risk for numerous safety-critical systems (e.g., autonomous driving systems). There are two problems with the current situation. First, collecting data under different weather scenarios is highly difficult in practice. Second, the performance degrades significantly when the training and test data are from different distributions, as exemplified by the weather corrupted test data. As a result, it is challenging to train a model without access to the images containing variations of various weather conditions, and it is difficult to make trained model generalized to unknown data under different weather conditions. In this paper, we introduce a Counterfactual Representation Learning (CRL) method to address these problems. Without access to training data including weather condition variations, our CRL makes the model resistant to unseen test data that has been corrupted by weather condition variations. Our basic idea is inspired by the perspective of counterfactual regularization. We build a causal model that introduces a counterfactual variable to eliminate the unobserved characteristics brought about by weather conditions. In particular, such a counterfactual variable is approximated by randomly shuffled features, echoing the previous empirical observation that the shuffling technique can perturb the shape details while preserving the local textures. We use information theoretic representation learning to encourage the neural networks to learn more powerful and robust features, which consist of two components. We conduct experiments on five benchmark datasets, namely, CIFAR-100-C, ImageNet-C, KITTI-C, BDD100 k, and CityScapes-C, all of which contain weather corruption. The results of our experiments show that our proposed method can not only be a plug-and-play technique but also work nicely for both object recognition and detection.
C1 [Liu, Hong; Satoh, Shin'ichi] Natl Inst Informat, Tokyo 1010003, Japan.
   [Sun, Yongqing] Nihon Univ, Tokyo 1020074, Japan.
   [Bandoh, Yukihiro; Kitahara, Masaki] NTT Corp, Comp & Data Sci Labs, Tokyo 1008116, Japan.
C3 Research Organization of Information & Systems (ROIS); National
   Institute of Informatics (NII) - Japan; Nihon University; Nippon
   Telegraph & Telephone Corporation
RP Sun, YQ (corresponding author), Nihon Univ, Tokyo 1020074, Japan.
EM lynnliu.xmu@gmail.com; nakahara.eisei@nihon-u.ac.jp;
   yukihiro.bandou@ntt.com; masaki.kitahara@ntt.com; satoh@nii.ac.jp
OI Sun, Yongqing/0000-0003-3116-2371; Satoh, Shin'ichi/0000-0001-6995-6447
CR [Anonymous], 2016, PROC INT C LEARN REP
   Aslam A, 2021, IMAGE VISION COMPUT, V106, DOI 10.1016/j.imavis.2020.104095
   Bahnsen CH, 2019, IEEE T INTELL TRANSP, V20, P2802, DOI 10.1109/TITS.2018.2872502
   Bahnsen CH, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P123, DOI 10.5220/0007361301230130
   Bai HY, 2021, AAAI CONF ARTIF INTE, V35, P6705
   Bengio Y, 2021, COMMUN ACM, V64, P58, DOI 10.1145/3448250
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Chang S.-F., 2018, Frontiers of Multimedia Research
   Chen Q, 2021, PROC CVPR IEEE, P13034, DOI 10.1109/CVPR46437.2021.01284
   Contributors M., 2020, MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark
   Diaz-Ruiz CA, 2022, PROC CVPR IEEE, P21351, DOI 10.1109/CVPR52688.2022.02069
   Dittadi A., 2021, PROC INT C LEARN REP, P1
   Erichson NB, 2022, Arxiv, DOI [arXiv:2202.01263, 10.48550/arXiv.2202.01263, DOI 10.48550/ARXIV.2202.01263, DOI 10.48550/ARXIV.2202.012637]
   Fan Y., 2020, PROC DAGM GERMAN C 5, P101
   Funke C. M., 2022, PROC INT C LEARN REP, P1
   Ganin Y, 2016, J MACH LEARN RES, V17
   Geirhos R., 2019, Proceedings of the 7th International Conference on Learning Representations
   Geirhos R., 2019, PROC INTCONF LEARN R, P1
   Gong R, 2021, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR46437.2021.00824
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D., 2019, PROC INT C LEARN REP
   Hendrycks D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8320, DOI 10.1109/ICCV48922.2021.00823
   Hendrycks Dan, 2019, ARXIV191202781
   Hermann K., 2020, ADV NEURAL INFORM PR, V33, P19000
   Hnewa M, 2021, IEEE SIGNAL PROC MAG, V38, P53, DOI 10.1109/MSP.2020.2984801
   Hu W., 2017, P 34 INT C MACHINE L, P1558
   Huang ZL, 2021, Arxiv, DOI arXiv:2106.03650
   Islam M. A., 2021, PROC INT C LEARN REP, P1
   Kaddour J, 2022, Arxiv, DOI [arXiv:2206.15475, 10.48550/arXiv.2206.15475]
   Kamann Christoph, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8825, DOI 10.1109/CVPR42600.2020.00885
   Kamann C, 2021, INT J COMPUT VISION, V129, P462, DOI 10.1007/s11263-020-01383-2
   Kingma D. P., 2014, arXiv
   Kishida I., 2020, Incorporating horizontal connections in convolution by spatial shuffling
   Li Y., 2021, PROC INT C LEARN REP, P1
   Lim S. H., 2022, PROC INT C LEARN REP, P1
   Liu H, 2023, ARTIF INTELL-AMST, V317, DOI 10.1016/j.artint.2023.103877
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Madry A, 2018, INT C LEARN REPR, DOI 10.48550/arXiv.1706.06083
   Mao X, 2022, Advances in Neural Information Processing Systems, P7520
   Michaelis C., 2019, PROC INT C NEURAL IN, P1
   Montero M. L., 2021, PROC INT C LEARN REP, P1
   Mummadi C. K., 2021, PROC INT C LEARN REP, P1
   Niu YL, 2021, PROC CVPR IEEE, P12695, DOI 10.1109/CVPR46437.2021.01251
   Paszkeet al A., 2019, PROC INT C NEURAL IN
   Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021
   Quinn T, 2018, TLS-TIMES LIT SUPPL, P31
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rice L., 2020, PMLR, P8093
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Rusak E, 2023, Arxiv, DOI arXiv:2104.12928
   Salman H., 2020, PROC INT C NEURAL IN
   Schölkopf B, 2021, P IEEE, V109, P612, DOI 10.1109/JPROC.2021.3058954
   Shan YH, 2019, NEUROCOMPUTING, V367, P31, DOI 10.1016/j.neucom.2019.08.022
   Shi B., 2020, PROC INT C MACH LEAR
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Sindagi Vishwanath A., 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P763, DOI 10.1007/978-3-030-58568-6_45
   Smilkov D., 2017, PROC ICML WORKSHOP V, P1
   Steiner A. P., 2022, Trans. Mach. Learn. Res.
   Tian R, 2022, Arxiv, DOI arXiv:2204.12143
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tsipras D., 2019, P 7 INT C LEARN REPR
   Valanarasu JMJ, 2022, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR52688.2022.00239
   Verma V, 2019, PR MACH LEARN RES, V97
   Wang ZX, 2020, INT CONF FRONT HAND, P157, DOI 10.1109/ICFHR2020.2020.00038
   Wiles O., 2022, PROC INT C LEARN REP, P1
   Wu Y., 2019, Detec-tron2
   Xie C, 2020, PROC CVPR IEEE, P816, DOI 10.1109/CVPR42600.2020.00090
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Zagoruyko S., 2016, Wide residual networks, DOI DOI 10.5244/C.30.87
   Zang SZ, 2019, IEEE VEH TECHNOL MAG, V14, P103, DOI 10.1109/MVT.2019.2892497
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang HL, 2022, PROC CVPR IEEE, P8014, DOI 10.1109/CVPR52688.2022.00786
   Zhang L, 2019, Arxiv, DOI arXiv:1906.03347
   Zhang YX, 2023, ISPRS J PHOTOGRAMM, V196, P146, DOI 10.1016/j.isprsjprs.2022.12.021
   Zhang Z., 2019, PMLR, P7502
   Zhao YY, 2022, IEEE T CIRC SYST VID, V32, P7019, DOI 10.1109/TCSVT.2022.3179021
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Ziwei Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12403, DOI 10.1109/CVPR42600.2020.01242
NR 84
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5257
EP 5272
DI 10.1109/TMM.2023.3330534
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600029
DA 2024-08-05
ER

PT J
AU Qin, QB
   Xie, KZ
   Zhang, WF
   Wang, CD
   Huang, L
AF Qin, Qibing
   Xie, Kezhen
   Zhang, Wenfeng
   Wang, Chengduan
   Huang, Lei
TI Deep Neighborhood Structure-Preserving Hashing for Large-Scale Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Semantics; Quantization (signal); Feature extraction;
   Training; Dogs; Convolutional neural networks; Adaptive margin; deep
   hashing; image retrieval; large variances; neighborhood
   structure-preserving; quadruplet loss; quadruple regularization
ID QUANTIZATION
AB Deep hashing integrates the advantages of deep learning and hashing technology, and has become the mainstream of the large-scale image retrieval field. However, when training the deep hashing models, most of the existing approaches regard the similarity margin of image pairs as a constant. Once similarity distance exceeds the fixed margin, the network will not learn anything, which easily results in model collapses. In this paper, we address this dilemma with a novel unified deep hashing framework, termed Deep Neighborhood Structure-preserving Hashing (DNSH), to generate the similarity-preserving and discriminative hash codes. Specifically, by extracting the discriminative object characteristics with large variances, we design an adaptive margin quadruplet loss to further explore the underlying similarity relationship between image pairs, reflecting the correct semantic structure among its neighbors. Based on the quadruple form, we develop a quadruple regularization to decrease quantization errors between binary-like embedding and hashing codes. Furthermore, through learning bit balance and bit independent terms jointly, we present the binary code constraint loss to alleviate redundancy in different bits. Extensive evaluations on four popular benchmark datasets demonstrate that our proposed deep hashing framework achieves an excellent performance than the comparison methods.
C1 [Qin, Qibing; Wang, Chengduan] Weifang Univ, Sch Comp Engn, Weifang 261061, Peoples R China.
   [Xie, Kezhen; Huang, Lei] Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266005, Peoples R China.
   [Zhang, Wenfeng] Chongqing Normal Univ, Coll Comp & Informat Sci, Chongqing 401333, Peoples R China.
C3 Weifang University; Ocean University of China; Chongqing Normal
   University
RP Wang, CD (corresponding author), Weifang Univ, Sch Comp Engn, Weifang 261061, Peoples R China.; Huang, L (corresponding author), Ocean Univ China, Fac Informat Sci & Engn, Qingdao 266005, Peoples R China.
EM qinbing@wfu.edu.cn; xiekezhen@stu.ouc.edu.cn; itzhangwf@cqnu.edu.cn;
   20111182@wfu.edu.cn; huangl@ouc.edu.cn
OI Huang, Lei/0000-0003-4087-3677; Qin, Qibing/0000-0001-7976-318X; Zhang,
   Wengfeng/0000-0001-7459-2510
FU Natural Science Foundation of Shandong Province
FX No Statement Available
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   Andoni A, 2015, ADV NEUR IN, V28
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Charikar M.S., 2002, P THIRY 4 ANN ACM S, P380, DOI DOI 10.1145/509907.509965
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YX, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3155283
   Chen YD, 2019, IEEE I CONF COMP VIS, P9795, DOI 10.1109/ICCV.2019.00989
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2008, ACM COMPUT SURV, V40, DOI 10.1145/1348246.1348248
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Doan KD, 2022, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR52688.2022.00923
   Fu CY, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108264
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P1344, DOI 10.1109/TIP.2017.2652730
   Ha M. L., 2021, arXiv, DOI DOI 10.48550/ARXIV.2107.06187
   Hoe JT, 2021, ADV NEUR IN, V34
   Huang CQ, 2018, IEEE T IMAGE PROCESS, V27, P4490, DOI 10.1109/TIP.2018.2839522
   Hyvärinen A, 2009, COMPUT IMAGING VIS, V39, P1
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Kararnan S, 2019, ICMR'19: PROCEEDINGS OF THE 2019 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P192, DOI 10.1145/3323873.3325038
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li Q, 2017, ADV NEUR IN, V30
   Li SY, 2019, IEEE I CONF COMP VIS, P8211, DOI 10.1109/ICCV.2019.00830
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Li ZW, 2022, IEEE T NEUR NET LEAR, V33, P6999, DOI 10.1109/TNNLS.2021.3084827
   Liang YC, 2022, IEEE T IMAGE PROCESS, V31, P949, DOI 10.1109/TIP.2021.3137653
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W., 2011, PROC 28 INT C MACH L, P1
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Lowe D. G., 1999, Computer vision, V2, P1150, DOI [10.1109/ICCV.1999.790410, DOI 10.1109/ICCV.1999.790410]
   Lu XQ, 2018, IEEE T IMAGE PROCESS, V27, P106, DOI 10.1109/TIP.2017.2755766
   Luo X, 2023, ACM T KNOWL DISCOV D, V17, DOI 10.1145/3532624
   Massiceti D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10798, DOI 10.1109/ICCV48922.2021.01064
   Nie XS, 2021, APPL SOFT COMPUT, V109, DOI 10.1016/j.asoc.2021.107467
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qin QB, 2021, IEEE T CIRC SYST VID, V31, P2852, DOI 10.1109/TCSVT.2020.3032402
   Qin QB, 2021, INFORM SCIENCES, V567, P116, DOI 10.1016/j.ins.2021.03.006
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang M, 2023, IEEE T MULTIMEDIA, V25, P2164, DOI 10.1109/TMM.2022.3143694
   Wang YB, 2019, IEEE T IMAGE PROCESS, V28, P2665, DOI 10.1109/TIP.2018.2889269
   Wang ZJ, 2021, IEEE T MULTIMEDIA, V23, P1274, DOI 10.1109/TMM.2020.2995267
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xu J, 2018, AAAI CONF ARTIF INTE, P7436
   Yao T., 2016, IJCAI, P3931
   Yu FX, 2014, PR MACH LEARN RES, V32, P946
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Yuan ZQ, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3078451
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang M., 2021, PROC 17 INT C MACH V, P1
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang Z, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3442204
   Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
   Zhu J, 2022, INT J MACH LEARN CYB, V13, P1, DOI 10.1007/s13042-021-01330-8
   Zhu J, 2019, NEUROCOMPUTING, V366, P161, DOI 10.1016/j.neucom.2019.07.082
NR 69
TC 3
Z9 3
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1881
EP 1893
DI 10.1109/TMM.2023.3289765
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800012
DA 2024-08-05
ER

PT J
AU Shi, JA
   Wang, YS
   Yu, ZT
   Li, GX
   Hong, XP
   Wang, F
   Gong, YH
AF Shi, Jingang
   Wang, Yusi
   Yu, Zitong
   Li, Guanxin
   Hong, Xiaopeng
   Wang, Fei
   Gong, Yihong
TI Exploiting Multi-Scale Parallel Self-Attention and Local Variation via
   Dual-Branch Transformer-CNN Structure for Face Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face hallucination; super-resolution; neural networks; Transformer
ID IMAGE; HALLUCINATION; REGRESSION
AB Recently, deep learning technique has been widely employed to deal with face super-resolution (FSR) problem. It aims to predict the nonlinear relationship between the low-resolution (LR) face images and corresponding high-resolution (HR) ones, which could recover the high-frequency details from the LR degraded textures. However, either CNN-based or Transformer-based approaches mostly enhance the details by exploiting the relationship of local pixels or patches on LR features, the nonlocal features are not fully taken into account for producing high-frequency textures. To improve the above problem, we design a novel dual-branch module which consists of Transformer and CNN respectively. The Transformer branch extracts multiple scale feature embeddings and explores local and nonlocal self-attention simultaneously. Thus, the parallel self-attention mechanism has superior capabilities to capture the local and nonlocal dependencies on face image in the face reconstruction. Furthermore, the traditional CNNs usually extract features by combining pixels in a local convolutional kernel, it may be not effective to recover lost high-frequency details since the variations of local pixels are not well measured, which is important in recovering vivid edges and contours. To this end, we propose the local variation based attention block on the CNN branch, which could enhance the capabilities by directly extracting features from the variation of neighboring pixels. Finally, the Transformer-branch and CNN-branch are combined together by the modulation block to fuse both nonlocal and local advantages from two branches. Experimental results demonstrate the effectiveness of the proposed method when compared with state-of-the-art approaches.
C1 [Shi, Jingang; Wang, Yusi; Li, Guanxin; Wang, Fei; Gong, Yihong] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Yu, Zitong] Great Bay Univ, Fac Comp Sci, Dongguan 523000, Guangdong, Peoples R China.
   [Hong, Xiaopeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Xi'an Jiaotong University; Harbin Institute of Technology
RP Wang, F (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM jingang.shi@hotmail.com; yusiwang@xjtu.edu.cn; yuzitong@gbu.edu.cn;
   liguanxin@xjtu.edu.cn; hongxiaopeng@ieee.org; feynmanw@xjtu.edu.cn;
   ygong@mail.xjtu.edu.cn
RI HONG, Xiaopeng/K-3594-2017
OI HONG, Xiaopeng/0000-0002-0611-0636
FU National Natural Science Foundation of China
FX No Statement Available
CR Baker S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P83, DOI 10.1109/AFGR.2000.840616
   Cao JZ, 2023, Arxiv, DOI arXiv:2106.06847
   Chen CF, 2021, IEEE T IMAGE PROCESS, V30, P1219, DOI 10.1109/TIP.2020.3043093
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen L, 2020, IEEE T IMAGE PROCESS, V29, P9002, DOI 10.1109/TIP.2020.3023580
   Chen Y, 2018, PROC CVPR IEEE, P2492, DOI 10.1109/CVPR.2018.00264
   Chu XX, 2021, ADV NEUR IN
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dosovitskiy A., 2021, PROC ICLR
   Feng CM, 2021, LECT NOTES COMPUT SC, V12906, P307, DOI 10.1007/978-3-030-87231-1_30
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Huang H, 2010, PATTERN RECOGN, V43, P2532, DOI 10.1016/j.patcog.2010.02.007
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Jiang JJ, 2014, IEEE T MULTIMEDIA, V16, P1268, DOI 10.1109/TMM.2014.2311320
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Kalarot R, 2020, IEEE WINT CONF APPL, P359, DOI [10.1109/WACV45572.2020.9093399, 10.1109/wacv45572.2020.9093399]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li M, 2023, IEEE T MULTIMEDIA, V25, P919, DOI 10.1109/TMM.2021.3134839
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li ZZ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P544, DOI 10.1145/3474085.3475207
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu C, 2007, INT J COMPUT VISION, V75, P115, DOI 10.1007/s11263-006-0029-5
   Liu LC, 2023, IEEE T NEUR NET LEAR, V34, P2490, DOI 10.1109/TNNLS.2021.3106773
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu T, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5501, DOI 10.1145/3474085.3475682
   Ma C, 2020, PROC CVPR IEEE, P5568, DOI 10.1109/CVPR42600.2020.00561
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Sajjadi Mehdi S. M., 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P4501, DOI 10.1109/ICCV.2017.481
   Shi JG, 2019, IEEE T MULTIMEDIA, V21, P2223, DOI 10.1109/TMM.2019.2898752
   Shi JG, 2018, IEEE T IMAGE PROCESS, V27, P2980, DOI 10.1109/TIP.2018.2813163
   Shi JG, 2014, PATTERN RECOGN, V47, P3520, DOI 10.1016/j.patcog.2014.04.023
   Shi Jingang, 2022, P INT JOINT C ART IN, P1306, DOI DOI 10.24963/IJCAI.2022/182
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Song YB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4537
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wang XT, 2019, LECT NOTES COMPUT SC, V11133, P63, DOI 10.1007/978-3-030-11021-5_5
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZD, 2022, PROC CVPR IEEE, P17662, DOI 10.1109/CVPR52688.2022.01716
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P318, DOI 10.1007/978-3-319-46454-1_20
   Yu ZT, 2023, INT J COMPUT VISION, V131, P1307, DOI 10.1007/s11263-023-01758-1
   Yu ZT, 2022, PROC CVPR IEEE, P4176, DOI 10.1109/CVPR52688.2022.00415
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zeng X, 2018, IEEE T CYBERNETICS, V48, P716, DOI 10.1109/TCYB.2017.2655027
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang QL, 2021, ADV NEUR IN, V34
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XY, 2023, IEEE T MULTIMEDIA, V25, P2503, DOI 10.1109/TMM.2022.3147664
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P1924, DOI 10.1109/TMM.2020.3005025
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhu SZ, 2016, LECT NOTES COMPUT SC, V9909, P614, DOI 10.1007/978-3-319-46454-1_37
   Zou WB, 2023, IEEE T MULTIMEDIA, V25, P4623, DOI 10.1109/TMM.2022.3179926
NR 58
TC 11
Z9 11
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2608
EP 2620
DI 10.1109/TMM.2023.3301225
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400022
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Sun, HB
   He, XT
   Peng, YX
AF Sun, Hongbo
   He, Xiangteng
   Peng, Yuxin
TI HCL: Hierarchical Consistency Learning for Webly Supervised Fine-Grained
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image recognition; Noise measurement; Training; Semantics; Task
   analysis; Data models; Supervised learning; Closed-set noise;
   hierarchical consistency learning; open-set noise; webly supervised
   fine-grained recognition
ID ATTENTION
AB Webly supervised fine-grained recognition aims to distinguish subordinate categories (e.g., bird species) with freely available web data. It has significant research and application value for alleviating the costly professional manual annotations' dependence in the fine-grained recognition task. Nevertheless, there exists label noise in web data to decrease the model's recognition performance. Most existing methods attempt to select clean data via loss analyses, which favors easy samples to hinder mining subtle differences contained in hard samples. Inspired by the intrinsic trait of consistent semantic predictions among different hierarchies of clean samples in fine-grained recognition, we propose a hierarchical consistency learning (HCL) approach for detecting noisy samples and capturing multi-hierarchy discriminative clues simultaneously. Specifically, our HCL approach works in a coarse-to-fine order, which first explores the semantic consistency between the image level and object level through prediction distribution conformance analyses. The open-set noise (i.e., samples irrelevant to any fine-grained subcategory) is thus detected, and the visual object information is highlighted with image-object contrastive learning. Then, the semantic consistency between object-level and part-level prediction distributions is utilized for detecting closed-set noise (i.e., samples mislabeled as other fine-grained subcategories), and local discriminative information is enhanced with object-part contrastive learning. Extensive experiments and analyses on three widely-used webly supervised fine-grained benchmark datasets demonstrate that the proposed HCL approach can achieve new state-of-the-art.
C1 [Sun, Hongbo; He, Xiangteng; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [He, Xiangteng; Peng, Yuxin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Peking University; Peng Cheng Laboratory
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.; Peng, YX (corresponding author), Peng Cheng Lab, Shenzhen 518055, Peoples R China.
EM sunhongbo@pku.edu.cn; hexiangteng@pku.edu.cn; pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Arpit D, 2017, PR MACH LEARN RES, V70
   Cai ZH, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109063
   Chang DL, 2021, PROC CVPR IEEE, P11471, DOI 10.1109/CVPR46437.2021.01131
   Chen T, 2020, PR MACH LEARN RES, V119
   Du RY, 2022, IEEE T PATTERN ANAL, V44, P9521, DOI 10.1109/TPAMI.2021.3126668
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Goldberger E., 2017, INPROC INT C LEARN R
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He H., 2020, P IEEE CVF C COMP VI, P9729
   He J, 2022, AAAI CONF ARTIF INTE, P852
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2019, INT J COMPUT VISION, V127, P1235, DOI 10.1007/s11263-019-01176-2
   He XT, 2019, IEEE T CIRC SYST VID, V29, P1394, DOI 10.1109/TCSVT.2018.2834480
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Hu YQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4239, DOI 10.1145/3474085.3475561
   Huang L., 2020, ADV NEURAL INF PROCE, P19365
   Kong S, 2017, PROC CVPR IEEE, P7025, DOI 10.1109/CVPR.2017.743
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P1105, DOI 10.1109/TMM.2021.3118216
   Ma X., 2020, INT C MACHINE LEARNI, P6543
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Malach E, 2017, ADV NEUR IN, V30
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Radford A, 2021, PR MACH LEARN RES, V139
   Recasens A, 2018, LECT NOTES COMPUT SC, V11213, P52, DOI 10.1007/978-3-030-01240-3_4
   Ren MY, 2018, PR MACH LEARN RES, V80
   Ruoyi Du, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P153, DOI 10.1007/978-3-030-58565-5_10
   Song KT, 2020, IEEE T IMAGE PROCESS, V29, P7006, DOI 10.1109/TIP.2020.2996736
   Sun HB, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5853, DOI 10.1145/3503161.3548308
   Sun ZR, 2023, IEEE T MULTIMEDIA, V25, P3284, DOI 10.1109/TMM.2022.3158001
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Sun ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10582, DOI 10.1109/ICCV48922.2021.01043
   Sun ZR, 2022, IEEE T MULTIMEDIA, V24, P1093, DOI 10.1109/TMM.2021.3116430
   Tan M, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3492221
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang YS, 2019, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2019.00041
   Wang ZH, 2020, AAAI CONF ARTIF INTE, V34, P12289
   Wei XS, 2022, IEEE T PATTERN ANAL, V44, P8927, DOI 10.1109/TPAMI.2021.3126648
   Xiaojiang Peng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P786, DOI 10.1007/978-3-030-58517-4_46
   Xu Q, 2023, IEEE T MULTIMEDIA, V25, P9015, DOI 10.1109/TMM.2023.3244340
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yi K, 2019, PROC CVPR IEEE, P7010, DOI 10.1109/CVPR.2019.00718
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yu XR, 2019, PR MACH LEARN RES, V97
   Zhang YK, 2021, Arxiv, DOI arXiv:2103.07756
   Zhang ZL, 2018, ADV NEUR IN, V31
   Zheng J., 2019, P 33 INT C NEUR INF, P4277
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zou J. Y., 2013, Advances in Neural Information Processing Systems, V26, P2238
NR 53
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5108
EP 5119
DI 10.1109/TMM.2023.3330076
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600005
DA 2024-08-05
ER

PT J
AU Tan, JP
   Yang, XJ
   Yang, ZJ
   Chen, RH
   Lu, YY
   Lin, L
AF Tan, Junpeng
   Yang, Xiaojun
   Yang, Zhijing
   Chen, Ruihan
   Lu, Yongyi
   Lin, Liang
TI Extensible Max-Min Collaborative Retention for Online Mini-Batch
   Learning Hash Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Codes; Quantization (signal); Minimax techniques;
   Correlation; Data mining; Task analysis; Supervised hash retrieval;
   multi-dimensional space; semantic similarity discrimination;
   out-of-extension; max-min optimization
ID CONSTRUCTION; QUANTIZATION
AB Along with the concern of similarity measures in linear space, supervised online hash methods have been applied to the retrieval task. However, they ignored multi-dimensional space semantic mining and association characteristics will cause quantization errors of information hash code: 1) The similarity relation of discretized data needs to be considered in different spaces; 2) Latent semantic features need to be continuously embedded into hash code learning; 3) The correlation between the structure similarity and discrete hash matrices needs to be continuously optimized. To tackle these challenges, this paper proposes a novel Extensible Max-min Collaborative Retention Online Hash retrieval method based on mini-batch training data (EMCROH). It mainly includes the Max-min Bayesian Similarity Sparse Latent Hash module (MBSSLH), and the Repetition Collaborative Projection Learning module (RCPL). Specifically, MBSSLH is a max-min optimization model. Firstly, to explore the semantic similarity of multi-dimensional space, we propose a novel liner and nonlinear semantic similarity discrimination mechanism based on the log maximum likelihood similarity estimation with Euclidean space and minimize the input batch data features with a common projection matrix. Moreover, to further mine the potential semantic information of the discretization, we also propose a robust sparse discrete latent semantic information extraction submodule based on double latent factors. RCPL can extend the data externally using the repetition collaborative projection matrix with robustness regularization constraint. Finally, a novel max-min embedding iterative step is proposed to solve the batch discrete optimization problem based on Augmented Lagrange Multipliers (ALM) with Alternating Direction Minimization (ADM). Extensive experiments on several well-known large databases demonstrate that EMCROH outperforms the state-of-the-art hash methods.
C1 [Tan, Junpeng; Yang, Xiaojun; Yang, Zhijing; Chen, Ruihan; Lu, Yongyi] Guangdong Univ Technol, Sch Informat Engn, Guangzhou, Peoples R China.
   [Tan, Junpeng; Yang, Zhijing] Guangdong Prov Key Lab Project Intellectual Proper, Guangzhou 510665, Peoples R China.
   [Lin, Liang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510275, Peoples R China.
C3 Guangdong University of Technology; Sun Yat Sen University
RP Yang, XJ (corresponding author), Guangdong Univ Technol, Sch Informat Engn, Guangzhou, Peoples R China.
EM tjeepgdut@foxmail.com; yangxj18@gdut.edu.cn; yzhj@gdut.edu.cn;
   2112103075@mail2.gdut.edu.cn; yylu1989@gmail.com; linliang@ieee.org
RI ; Liang, Lin/IQR-8601-2023
OI Chen, Ruihan/0000-0003-4095-5533; Liang, Lin/0000-0003-2248-3755
FU National Natural Science Foundation of China
FX No Statement Available
CR Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Cakir F, 2017, COMPUT VIS IMAGE UND, V156, P162, DOI 10.1016/j.cviu.2016.10.009
   Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Chen R., 2023, Expert Syst. with Appl., V237
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   Chen Y, 2020, IEEE T IMAGE PROCESS, V29, P3596, DOI 10.1109/TIP.2020.2963952
   Chen ZX, 2018, PROC CVPR IEEE, P6838, DOI 10.1109/CVPR.2018.00715
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cheng Zhi-Qi, 2017, P 2017ACM INT C MULT, P287
   Czumaj A, 2020, PROCEEDINGS OF THE THIRTY-FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA'20), P2973
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Feng YA, 2019, IEEE T MULTIMEDIA, V21, P1762, DOI 10.1109/TMM.2018.2885237
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   He SY, 2018, NEUROCOMPUTING, V282, P32, DOI 10.1016/j.neucom.2017.12.005
   Heo JP, 2019, IEEE T PATTERN ANAL, V41, P2084, DOI 10.1109/TPAMI.2018.2853161
   Houle ME, 2013, LECT NOTES COMPUT SC, V8199, P16, DOI 10.1007/978-3-642-41062-8_3
   Hu LJ, 2019, J PARALLEL DISTR COM, V132, P127, DOI 10.1016/j.jpdc.2019.06.003
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242
   Jiang QY, 2019, IEEE T IMAGE PROCESS, V28, P3490, DOI 10.1109/TIP.2019.2897944
   Jin S, 2021, IEEE T IMAGE PROCESS, V30, P6130, DOI 10.1109/TIP.2021.3091895
   Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865
   Li N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2397
   Li PD, 2022, IEEE T MULTIMEDIA, V24, P981, DOI 10.1109/TMM.2021.3062480
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Lin MB, 2020, IEEE T IMAGE PROCESS, V29, P5289, DOI 10.1109/TIP.2020.2981879
   Lin MB, 2019, AAAI CONF ARTIF INTE, P8722
   Lin MB, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1635, DOI 10.1145/3240508.3240519
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1379, DOI 10.1145/3397271.3401086
   Luo KY, 2023, IEEE T MULTIMEDIA, V25, P9082, DOI 10.1109/TMM.2023.3245400
   Nguyen P. A., 2017, P TREC VID RETR EV
   Ozan EC, 2016, IEEE T KNOWL DATA EN, V28, P2884, DOI 10.1109/TKDE.2016.2597834
   Skopal T, 2009, J DISCRET ALGORITHMS, V7, P62, DOI 10.1016/j.jda.2008.09.013
   Sun GL, 2018, MULTIMED TOOLS APPL, V77, P17731, DOI 10.1007/s11042-017-5245-1
   Tan JP, 2023, INFORM SCIENCES, V648, DOI 10.1016/j.ins.2023.119571
   Tian X, 2021, IEEE T MULTIMEDIA, V23, P1210, DOI 10.1109/TMM.2020.2994509
   Wang D, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107479
   Wang JB, 2021, IEEE T IMAGE PROCESS, V30, P6321, DOI 10.1109/TIP.2021.3093387
   Wang JD, 2019, IEEE T PATTERN ANAL, V41, P1308, DOI 10.1109/TPAMI.2018.2835468
   Wang SN, 2020, IEEE T SIGNAL INF PR, V6, P196, DOI 10.1109/TSIPN.2020.2975356
   Wang SN, 2021, IEEE T CYBERNETICS, V51, P4089, DOI 10.1109/TCYB.2019.2894020
   Wang YX, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P871, DOI 10.1145/3394171.3413971
   Wang YX, 2022, IEEE T CYBERNETICS, V52, P10064, DOI 10.1109/TCYB.2021.3059886
   Weng ZY, 2021, IEEE T MULTIMEDIA, V23, P1868, DOI 10.1109/TMM.2020.3004962
   Xu XX, 2020, IEEE T BIG DATA, V8, P1118, DOI 10.1109/TBDATA.2020.3027379
   Yang XJ, 2023, INFORM SCIENCES, V644, DOI 10.1016/j.ins.2023.03.035
   Yao T, 2019, PATTERN RECOGN, V89, P1, DOI 10.1016/j.patcog.2018.12.012
   Yuan X, 2018, LECT NOTES COMPUT SC, V11208, P141, DOI 10.1007/978-3-030-01225-0_9
   Zhang C, 2023, IEEE T KNOWL DATA EN, V35, P6475, DOI 10.1109/TKDE.2022.3172216
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zheng CQ, 2022, IEEE T IMAGE PROCESS, V31, P5881, DOI 10.1109/TIP.2022.3203216
NR 55
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6743
EP 6758
DI 10.1109/TMM.2024.3355646
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600040
DA 2024-08-05
ER

PT J
AU Wu, S
   Zhao, GS
   Qian, XM
AF Wu, Sen
   Zhao, Guoshuai
   Qian, Xueming
TI Resolving Zero-Shot and Fact-Based Visual Question Answering via
   Enhanced Fact Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Knowledge based systems; Question
   answering (information retrieval); Predictive models; Knowledge graphs;
   Feature extraction; Visual question answering; zero-shot; knowledge
   graph
AB Practical applications with visual question answering (VQA) systems are challenging, and recent research has aimed at investigating this important field. Many issues related to real-world VQA applications must be considered. Although existing methods have focused on adding external knowledge and other descriptive information to assist in reasoning, they are limited by the impact of information retrieval errors on downstream tasks and the misalignment of the aggregated information. Thus, the overall performance of these models must be improved. To address these challenges, we propose a novel VQA model that utilizes a differentiated pretrained model to represent the input information and connects the input data with three external knowledge components through a common feature space. To combine the information in the three feature spaces, we propose an information aggregation strategy that employs a weighted score to aggregate the information in the relation and entity spaces in the answer prediction process. The experimental results show that our method achieves good performance in fact-based and zero-shot VQA tasks and achieves state-of-the-art performance with the ZS-F-VQA dataset.
C1 [Wu, Sen; Zhao, Guoshuai] Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Zhao, Guoshuai; Qian, Xueming] Shaanxi Yulan Jiuzhou Intelligent Optoelect Techno, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Key Lab Intelligent Networks & Network Secur, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University
RP Zhao, GS (corresponding author), Xi An Jiao Tong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM a1967675298@stu.xjtu.edu.cn; guoshuai.zhao@xjtu.edu.cn;
   qianxm@mail.xjtu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12
   Chen J., 2021, P 30 INT JOINT C ART, P4366, DOI DOI 10.24963/IJCAI.2021/597
   Chen Z, 2021, LECT NOTES COMPUT SC, V12922, P146, DOI 10.1007/978-3-030-88361-4_9
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Gao Peng, 2015, ADV NEURAL INFORM PR, P2296, DOI DOI 10.48550/ARXIV.1505.05612
   Gardères F, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P489
   Geng YX, 2021, Arxiv, DOI arXiv:2106.15047
   Guan Weili, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P268, DOI 10.1145/3503161.3548020
   Han YD, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4528, DOI 10.1145/3474085.3475609
   Hu HX, 2018, PROC CVPR IEEE, P5428, DOI 10.1109/CVPR.2018.00569
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Jing TT, 2022, IEEE T IMAGE PROCESS, V31, P3657, DOI 10.1109/TIP.2022.3173815
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Kim JH, 2018, ADV NEUR IN, V31
   Kumar A, 2016, PR MACH LEARN RES, V48
   Li H, 2019, PROC CVPR IEEE, P6312, DOI 10.1109/CVPR.2019.00648
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Lu JS, 2016, ADV NEUR IN, V29
   Ma L, 2016, AAAI CONF ARTIF INTE, P3567
   Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9
   Marino K, 2019, PROC CVPR IEEE, P3190, DOI 10.1109/CVPR.2019.00331
   Narasimhan M, 2018, ADV NEUR IN, V31
   Narasimhan M, 2018, LECT NOTES COMPUT SC, V11212, P460, DOI 10.1007/978-3-030-01237-3_28
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shah S, 2019, AAAI CONF ARTIF INTE, P8876
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Teney D, 2016, Arxiv, DOI arXiv:1611.05546
   Vatashsky Ben-Zion, 2020, PROC IEEE C COMPUT V, P10376
   Wang P, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1290
   Wang P, 2018, IEEE T PATTERN ANAL, V40, P2413, DOI 10.1109/TPAMI.2017.2754246
   Wang QQ, 2023, IEEE T NEUR NET LEAR, V34, P7635, DOI 10.1109/TNNLS.2022.3145048
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P1771, DOI 10.1109/TIP.2020.3048626
   Whitehead S, 2021, PROC CVPR IEEE, P5628, DOI 10.1109/CVPR46437.2021.00558
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Yan CX, 2022, IEEE T PATTERN ANAL, V44, P9733, DOI 10.1109/TPAMI.2021.3127346
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang HY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P695, DOI 10.1145/3474085.3475234
   Zhang J, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108469
   Zheng WF, 2021, PATTERN RECOGN, V120, DOI 10.1016/j.patcog.2021.108153
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhu YK, 2016, PROC CVPR IEEE, P4995, DOI 10.1109/CVPR.2016.540
   Zhu ZH, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1097
NR 44
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1790
EP 1800
DI 10.1109/TMM.2023.3289729
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800029
DA 2024-08-05
ER

PT J
AU Yang, JC
   Cheng, C
   Xiao, S
   Lan, GP
   Wen, JB
AF Yang, Jiachen
   Cheng, Chen
   Xiao, Shuai
   Lan, Guipeng
   Wen, Jiabao
TI High Fidelity Face-Swapping With Style ConvTransformer and Latent Space
   Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE StyleGAN; face-swapping; GAN inversion; attribute-editing
AB Face-swapping technology has been widely used in people's life, and people also put forward higher requirements for it. Most of the current face-swapping methods are difficult to generate a high-definition face image. Through StyleGAN, we can generate high-definition face images. However, face-swapping with StyleGAN is still challenging. Firstly, we need to map the target image to the latent space of StyleGAN. Many tasks need to map the input image to a new latent space for face-swapping, because identity features are complex and challenging to map to specific latent space layers directly. So face-swapping is completed in the remapping process, which consumes excess computing resources for reconstruction. And the generated image is difficult to maintain the original image color, face attributes, background and other attributes. We propose a new method, which only edits the code of w+ latent space of StyleGAN to complete the face-swapping and generate high-definition face images. We propose the GAN inversion method to improve the effect of face swapping, which combines convolution networks' advantages in extracting texture features and the benefits of transformers in extracting structure features. In the latent space of StyleGAN, the low-level feature layer is dominated by structure information, and the high-level feature layer is overwhelmed by texture information. Furthermore, we propose latent space selection, through which the neural network can learn disentangled representations of identity information in the latent space. Finally, we improved the post-processing process of face swapping to keep the image's background. Our method can complete face-swapping by editing the w+ space. Thus, high-quality face image can be generated and a lot of computing resource is saved on image reconstruction. At the same time, our method can keep other attributes better in the face-swapping process.
C1 [Yang, Jiachen; Cheng, Chen; Xiao, Shuai; Lan, Guipeng; Wen, Jiabao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Xiao, S (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yangjiachen@tju.edu.cn; 2021234089@tju.edu.cn; xs611@tju.edu.cn;
   lgp@tju.edu.cn; wen_jiabao@tju.edu.cn
OI Yang, Jiachen/0000-0003-2558-552X; Wen, Jiabao/0000-0003-2303-9613;
   Xiao, Shuai/0000-0003-4058-8120; Lan, Guipeng/0000-0001-7321-7460
FU National Natural Science Foundation of China
FX No Statement Available
CR Alaluf Y, 2022, PROC CVPR IEEE, P18490, DOI 10.1109/CVPR52688.2022.01796
   Bitouk D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360638
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dai R, 2022, PROC CVPR IEEE, P20009, DOI 10.1109/CVPR52688.2022.01941
   Deng JK, 2022, IEEE T PATTERN ANAL, V44, P5962, DOI 10.1109/TPAMI.2021.3087709
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Gan MG, 2023, IEEE T MULTIMEDIA, V25, P3799, DOI 10.1109/TMM.2022.3166025
   Ghose S, 2023, IEEE T MULTIMEDIA, V25, P4508, DOI 10.1109/TMM.2022.3177894
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Huo J, 2022, IEEE T IMAGE PROCESS, V31, P3347, DOI 10.1109/TIP.2022.3154238
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jie Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9898, DOI 10.1109/CVPR42600.2020.00992
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T., 2020, P IEEE CVF C COMP VI, P8110, DOI [10.1109/cvpr42600.2020.00813, DOI 10.1109/CVPR42600.2020.00813]
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Korshunova I, 2017, IEEE I CONF COMP VIS, P3697, DOI 10.1109/ICCV.2017.397
   Lan GP, 2024, IEEE INTELL SYST, V39, P29, DOI 10.1109/MIS.2022.3217391
   Li LZ, 2020, Arxiv, DOI arXiv:1912.13457
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Liu S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P82, DOI 10.1145/3123266.3123431
   Liu YH, 2023, IEEE T MULTIMEDIA, V25, P3343, DOI 10.1109/TMM.2022.3159115
   Liu Z., 2020, arXiv
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Raghu M, 2021, ADV NEUR IN, V34
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Shen Y., 2020, P IEEECVF C COMPUTER, P9243
   Shi Jing, 2022, P IEEECVF C COMPUTER, P19730
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Shu XB, 2022, IEEE T PATTERN ANAL, V44, P3300, DOI 10.1109/TPAMI.2021.3050918
   Simonyan K, 2014, Arxiv, DOI [arXiv:1312.6034, DOI 10.48550/ARXIV.1312.6034]
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Sun YL, 2020, IEEE T INF FOREN SEC, V15, P2679, DOI 10.1109/TIFS.2020.2975921
   Tao YS, 2023, IEEE T MULTIMEDIA, V25, P4586, DOI 10.1109/TMM.2022.3178599
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang TF, 2022, PROC CVPR IEEE, P11369, DOI 10.1109/CVPR52688.2022.01109
   Xiao S., 2023, IEEE Trans. Multimedia, DOI [10.1109/TM.2023.3279993, DOI 10.1109/TM.2023.3279993]
   Xiao SQ, 2023, INT J PAVEMENT ENG, V24, DOI [10.1080/10298436.2022.2027415, 10.1109/IECON49645.2022.9968611]
   Xiong W, 2020, PROC CVPR IEEE, P5839, DOI 10.1109/CVPR42600.2020.00588
   Xu YY, 2022, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR52688.2022.00749
   Yang JC, 2022, COMPUT ELECTR ENG, V103, DOI 10.1016/j.compeleceng.2022.108322
   Yang JC, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22134697
   Yang JC, 2021, FUTURE GENER COMP SY, V125, P127, DOI 10.1016/j.future.2021.06.043
   Yang ZY, 2021, IEEE T CIRC SYST VID, V31, P3433, DOI 10.1109/TCSVT.2020.3038720
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang M, 2023, BIOMED SIGNAL PROCES, V80, DOI 10.1016/j.bspc.2022.104302
   Zhao Y, 2023, SIGNAL PROCESS, V203, DOI 10.1016/j.sigpro.2022.108782
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2021, Arxiv, DOI arXiv:2012.09036
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 53
TC 6
Z9 6
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3604
EP 3615
DI 10.1109/TMM.2023.3313256
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200037
DA 2024-08-05
ER

PT J
AU Yang, Q
   Li, YQ
   Li, CL
   Wang, H
   Yan, S
   Wei, L
   Dai, WR
   Zou, JN
   Xiong, HK
   Frossard, P
AF Yang, Qin
   Li, Yuqi
   Li, Chenglin
   Wang, Hao
   Yan, Sa
   Wei, Li
   Dai, Wenrui
   Zou, Junni
   Xiong, Hongkai
   Frossard, Pascal
TI SVGC-AVA: 360-Degree Video Saliency Prediction With Spherical
   Vector-Based Graph Convolution and Audio-Visual Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Feature extraction; Convolution; Streaming media;
   Correlation; Position measurement; Kernel; 360-degree videos; saliency
   prediction; spherical vector-based graph convolution; audio-visual
   attention
ID DEEP AGENT; MOVEMENT; HEAD
AB Viewers of 360-degree videos are provided with both visual modality to characterize their surrounding views and audio modality to indicate the sound direction. Though both modalities are important for saliency prediction, little work has been done by jointly exploiting them, which is mainly due to the lack of audio-visual saliency datasets and insufficient exploitation of the multi-modality. In this article, we first construct an audio-visual saliency dataset with 57 360-degree videos watched by 63 viewers. Through a deep analysis of the constructed dataset, we find that the human gaze can be attracted by the auditory cues, resulting in a more concentrated saliency map if the sound source's location is further provided. To jointly exploit the visual and audio features and their correlation, we further design a saliency prediction network for 360-degree videos (SVGC-AVA) based on spherical vector-based graph convolution and audio-visual attention. The proposed spherical vector-based graph convolution can process visual and audio features directly in the sphere domain, thus avoiding projection distortion incurred by traditional CNN-based predictors. In addition, the audio-visual attention scheme explores self-modal and cross-modal correlation for both modalities, which are further hierarchically processed with the U-Net's multi-scale structure of SVGC-AVA. Evaluations on both our and public datasets validate that SVGC-AVA can achieve higher prediction accuracy, both qualitatively and subjectively.
C1 [Yang, Qin; Li, Yuqi; Li, Chenglin; Wang, Hao; Yan, Sa; Wei, Li; Dai, Wenrui; Zou, Junni; Xiong, Hongkai] Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab, CH-1015 Lausanne, Switzerland.
C3 Shanghai Jiao Tong University; Swiss Federal Institutes of Technology
   Domain; Ecole Polytechnique Federale de Lausanne
RP Li, CL; Zou, JN (corresponding author), Shanghai Jiao Tong Univ, Sch Elect Informat & Elect Engn, Shanghai 200240, Peoples R China.
EM yangqin@sjtu.edu.cn; liyuqi@sjtu.edu.cn; lcl1985@sjtu.edu.cn;
   wanghao1403@gmail.com; yansa2018@sjtu.edu.cn; wei_li@sjtu.edu.cn;
   daiwenrui@sjtu.edu.cn; zoujunni@sjtu.edu.cn; xionghongkai@sjtu.edu.cn;
   pascal.frossard@epfl.ch
OI Xiong, Hongkai/0000-0003-4552-0029; Frossard, Pascal/0000-0002-4010-714X
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], Droonon F1
   [Anonymous], US
   YOUTUBE
   Aytar Y, 2016, ADV NEUR IN, V29
   Berdun EB, 2022, COMPUT GRAPH-UK, V106, P200, DOI 10.1016/j.cag.2022.06.002
   Blue Ripple Sound, about us
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Cao YQ, 2021, IEEE IMAGE PROC, P1429, DOI 10.1109/ICIP42928.2021.9506408
   Chao F.-Y., 2020, IEEE INT CONF MULTI, P1, DOI [DOI 10.1109/icmew46912.2020.9105956, 10.1109/ICMEW46912.2020.9105956]
   Chao FY, 2020, IEEE I C VI COM I PR, P355, DOI 10.1109/vcip49819.2020.9301766
   Chao FY, 2021, IEEE T MULTIMEDIA, V23, P1811, DOI 10.1109/TMM.2020.3003642
   Chao FY, 2018, IEEE INT CONF MULTI
   Chen JZ, 2021, NEUROCOMPUTING, V428, P248, DOI 10.1016/j.neucom.2020.12.011
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cheng Q, 2022, IEEE T MULTIMEDIA, V24, P1529, DOI 10.1109/TMM.2021.3067205
   Cokelek M, 2021, PROCEEDINGS OF 17TH INTERNATIONAL CONFERENCE ON MACHINE VISION APPLICATIONS (MVA 2021), DOI 10.23919/MVA51890.2021.9511406
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   De Abreu A, 2017, INT WORK QUAL MULTIM
   Duan HY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6549, DOI 10.1145/3503161.3547955
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Fearghail CO, 2018, LECT NOTES COMPUT SC, V11318, P308, DOI 10.1007/978-3-030-04028-4_34
   Gutiérrez J, 2018, SIGNAL PROCESS-IMAGE, V69, P35, DOI 10.1016/j.image.2018.05.003
   Hu HN, 2017, PROC CVPR IEEE, P1396, DOI 10.1109/CVPR.2017.153
   Jain S, 2021, IEEE INT C INT ROBOT, P3520, DOI 10.1109/IROS51168.2021.9635989
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Lebreton P, 2018, IEEE INT CONF MULTI
   Li D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201391
   Lv HR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P682, DOI 10.1145/3394171.3413733
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Min XK, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/2996463
   Morgado P., 2018, P ADV NEUR INF PROC, V31, P1
   Qiao ML, 2021, IEEE T MULTIMEDIA, V23, P748, DOI 10.1109/TMM.2020.2987682
   Tavakoli HR, 2020, Arxiv, DOI arXiv:1905.10693
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Steam VR, Steam
   Su YC, 2017, LECT NOTES COMPUT SC, V10114, P154, DOI 10.1007/978-3-319-54190-7_10
   Tatler BW, 2007, J VISION, V7, DOI 10.1167/7.14.4
   Tollmar K, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085163
   Tsiami A, 2020, PROC CVPR IEEE, P4765, DOI 10.1109/CVPR42600.2020.00482
   van der Burg E, 2008, J VISION, V8, DOI 10.1167/8.5.2
   vive, About us
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   White BJ., 2011, The Oxford handbook of eye movements, P195, DOI DOI 10.1093/OXFORDHB/9780199539789.013.0011
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Yang Q, 2020, PROC CVPR IEEE, P4302, DOI 10.1109/CVPR42600.2020.00436
   Yao SY, 2021, IEEE IMAGE PROC, P1604, DOI 10.1109/ICIP42928.2021.9506089
   Zhang YQ, 2020, IEEE J-STSP, V14, P27, DOI 10.1109/JSTSP.2019.2955824
   Zhang ZH, 2018, LECT NOTES COMPUT SC, V11211, P504, DOI 10.1007/978-3-030-01234-2_30
   Zhu D, 2021, P IEEE INT C MULT EX, P1
   Zhu DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3576857
   Zhu YC, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3565024
   Zhu YC, 2020, Arxiv, DOI arXiv:1912.05971
   Zhu YC, 2022, IEEE T CIRC SYST VID, V32, P4188, DOI 10.1109/TCSVT.2021.3126590
   Zhu YC, 2021, ACM T MULTIM COMPUT, V16, DOI 10.1145/3410455
   Zhu YC, 2020, IEEE T MULTIMEDIA, V22, P2331, DOI 10.1109/TMM.2019.2957986
   Zhu YC, 2018, SIGNAL PROCESS-IMAGE, V69, P15, DOI 10.1016/j.image.2018.05.010
NR 61
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3061
EP 3076
DI 10.1109/TMM.2023.3306596
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700009
DA 2024-08-05
ER

PT J
AU Yang, WW
   Zhao, YQ
   Yang, BL
   Shen, JB
AF Yang, Wenwu
   Zhao, Yeqing
   Yang, Bailin
   Shen, Jianbing
TI Learning 3D Face Reconstruction From the Cycle-Consistency of Dynamic
   Faces
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D face reconstruction; deep learning; unsupervised learning
ID MORPHABLE MODEL; FACIAL ANIMATION; SINGLE IMAGE
AB Reconstructinga 3D face from a single image is a crucial task in numerous multimedia applications. Face images with ground-truth 3D face shapes are scarce, so unsupervised deep learning methods, which rely primarily on the free supervision signal derived from the visual disparity between the input image and the rendered counterpart of the predicted 3D face, have proven superior for reconstructing 3D faces. However, it is challenging for such techniques to decouple the dynamic 3D face properties such as pose or expression from a single 2D image, especially when similar local visual appearance changes can be caused by both pose and expression motion, resulting in imprecise 3D face reconstruction. In this article, a novel cycle-consistency in dynamic 3D face characteristics is introduced as a free supervisory signal for learning accurate 3D face shapes from unlabeled facial images. The main idea of cycle-consistency is to explicitly inject the head pose or facial expression variation between video frames into a face image, and then to extract and reverse the injected variation in order to reconstruct the face image to its original state. In our model, a CNN network with multiple branches is proposed to disentangle 3D face properties like identity, expression, pose, and texture from 2D facial images, one branch for each 3D face property. During training, our model learns to completely decouple the dynamic 3D face properties (pose and expression) to be useful for performing cycle-consistent face reconstruction. Extensive experiments demonstrate the superiority of our approach. On the challenging AFLW2000-3D, MICC Florence, and NoW datasets, our method outperforms or is on par with the state of the art.
C1 [Yang, Wenwu; Zhao, Yeqing] Zhejiang Gongshang Univ, Sch Comp & Informat Engn, Hangzhou 310018, Peoples R China.
   [Yang, Bailin] Zhejiang Gongshang Univ, Sch Comp & Informat Engn, Sch Stat & Math, Hangzhou 310018, Peoples R China.
   [Shen, Jianbing] Univ Macau, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau 999078, Peoples R China.
C3 Zhejiang Gongshang University; Zhejiang Gongshang University; University
   of Macau
RP Yang, BL (corresponding author), Zhejiang Gongshang Univ, Sch Comp & Informat Engn, Sch Stat & Math, Hangzhou 310018, Peoples R China.
EM wwyang@zjgsu.edu.cn; zyqlalalala@gmail.com; ybl@zjgsu.edu.cn;
   shenjianbingcg@gmail.com
OI Yang, Bailin/0000-0003-1754-5595
FU Zhejiang Provincial Natural Science Foundation of China
FX No Statement Available
CR Abrantes GA, 1999, IEEE T CIRC SYST VID, V9, P290, DOI 10.1109/76.752096
   Amin SH, 2007, 14TH INTERNATIONAL CONFERENCE ON IMAGE ANALYSIS AND PROCESSING, PROCEEDINGS, P413, DOI 10.1109/ICIAP.2007.4362813
   Bagdanov Andrew D, 2011, P JOINT ACM WORKSH H, P79
   Bas A, 2017, LECT NOTES COMPUT SC, V10117, P377, DOI 10.1007/978-3-319-54427-4_28
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao C, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275093
   Cao C, 2014, IEEE T VIS COMPUT GR, V20, P413, DOI 10.1109/TVCG.2013.249
   Chang JR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9660, DOI 10.1109/ICCV48922.2021.00954
   Chen AP, 2019, IEEE I CONF COMP VIS, P9428, DOI 10.1109/ICCV.2019.00952
   Chen SY, 2023, IEEE T MULTIMEDIA, V25, P3166, DOI 10.1109/TMM.2022.3156820
   Chen YJ, 2020, IEEE T IMAGE PROCESS, V29, P8696, DOI 10.1109/TIP.2020.3017347
   Chen Z, 2022, IEEE T CIRC SYST VID, V32, P8383, DOI 10.1109/TCSVT.2022.3192422
   Cudeiro D, 2019, PROC CVPR IEEE, P10093, DOI 10.1109/CVPR.2019.01034
   Danecek R, 2022, PROC CVPR IEEE, P20279, DOI 10.1109/CVPR52688.2022.01967
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   EKMAN P, 1976, ENVIRON PSYCH NONVER, V1, P56, DOI 10.1007/BF01115465
   Feng Y, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459936
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Galteri L, 2019, COMPUT VIS IMAGE UND, V185, P31, DOI 10.1016/j.cviu.2019.05.002
   Gecer B, 2022, IEEE T PATTERN ANAL, V44, P4879, DOI 10.1109/TPAMI.2021.3084524
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Gilani SZ, 2018, PROC CVPR IEEE, P1896, DOI 10.1109/CVPR.2018.00203
   Gutierrez-Osuna R, 2005, IEEE T MULTIMEDIA, V7, P33, DOI 10.1109/TMM.2004.840611
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang W, 2021, IEEE T MULTIMEDIA, V24, P3327, DOI 10.1109/TMM.2021.3096068
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jiang L, 2019, IEEE INT CONF COMP V, P504, DOI 10.1109/ICCVW.2019.00063
   Jianzhu Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P152, DOI 10.1007/978-3-030-58529-7_10
   Khan A, 2021, NEURAL COMPUT APPL, V33, P5951, DOI 10.1007/s00521-020-05373-w
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Li Q., 2021, P IEEE INT JOINT C B, P1
   Li Q, 2017, ADV NEUR IN, V30
   Lin JK, 2020, PROC CVPR IEEE, P5890, DOI 10.1109/CVPR42600.2020.00593
   Liu P., 2020, P AS C COMP VIX, P154
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Mallikarjun BR, 2021, PROC CVPR IEEE, P4789, DOI 10.1109/CVPR46437.2021.00476
   Morales A, 2021, COMPUT SCI REV, V40, DOI 10.1016/j.cosrev.2021.100400
   Nagrani A, 2020, COMPUT SPEECH LANG, V60, DOI 10.1016/j.csl.2019.101027
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Piao JT, 2019, IEEE I CONF COMP VIS, P9397, DOI 10.1109/ICCV.2019.00949
   Piotraschke M, 2016, PROC CVPR IEEE, P3418, DOI 10.1109/CVPR.2016.372
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Ramamoorthi R, 2001, COMP GRAPH, P497, DOI 10.1145/383259.383317
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Ruan ZY, 2021, IEEE T IMAGE PROCESS, V30, P5793, DOI 10.1109/TIP.2021.3087397
   Sanyal S, 2019, PROC CVPR IEEE, P7755, DOI 10.1109/CVPR.2019.00795
   Shang Jiaxiang, 2020, EUROPEAN C COMPUTER, V2360, P53
   Song LS, 2021, PROC CVPR IEEE, P2236, DOI 10.1109/CVPR46437.2021.00227
   Tewari A, 2019, PROC CVPR IEEE, P10804, DOI 10.1109/CVPR.2019.01107
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE I CONF COMP VIS, P3735, DOI 10.1109/ICCV.2017.401
   Tran L, 2019, PROC CVPR IEEE, P1126, DOI 10.1109/CVPR.2019.00122
   Tran L, 2021, IEEE T PATTERN ANAL, V43, P157, DOI 10.1109/TPAMI.2019.2927975
   Tu XG, 2022, IEEE T CIRC SYST VID, V32, P1805, DOI 10.1109/TCSVT.2021.3083257
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Wang Q., 2021, arXiv
   Wen YD, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13269, DOI 10.1109/ICCV48922.2021.01304
   Wu CY, 2021, INT CONF 3D VISION, P453, DOI 10.1109/3DV53792.2021.00055
   Yi HW, 2019, PROC CVPR IEEE, P7655, DOI 10.1109/CVPR.2019.00785
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zeng XX, 2019, IEEE I CONF COMP VIS, P2315, DOI 10.1109/ICCV.2019.00240
   Zhang HW, 2023, IEEE T PATTERN ANAL, V45, P12287, DOI 10.1109/TPAMI.2023.3271691
   Zhang HW, 2018, IEEE T INF FOREN SEC, V13, P2409, DOI 10.1109/TIFS.2018.2800901
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhao J, 2020, INT J COMPUT VISION, V128, P460, DOI 10.1007/s11263-019-01252-7
   Zhao J, 2017, ADV NEUR IN, V30
   Zhao J, 2018, PROC CVPR IEEE, P2207, DOI 10.1109/CVPR.2018.00235
   Zhao J, 2019, IEEE T PATTERN ANAL, V41, P2380, DOI 10.1109/TPAMI.2018.2858819
   Zhao JB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
   Zhu XY, 2014, INT C PATT RECOG, P4044, DOI 10.1109/ICPR.2014.693
NR 74
TC 0
Z9 0
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3663
EP 3675
DI 10.1109/TMM.2023.3322895
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200004
DA 2024-08-05
ER

PT J
AU Yu, NN
   Shi, H
   Han, YH
AF Yu, Nana
   Shi, Hong
   Han, Yahong
TI Joint Correcting and Refinement for Balanced Low-Light Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Lighting; Feature extraction; Brightness; Image
   enhancement; Distortion; Visualization; Low-light enhancement; back
   projection; color correction; illumination adjustment
ID HISTOGRAM EQUALIZATION; QUALITY ASSESSMENT
AB Low-light image enhancement tasks demand an appropriate balance among brightness, color, and illumination. While existing methods often focus on one aspect of the image without considering how to pay attention to this balance, which will cause problems of color distortion and overexposure etc. This seriously affects both human visual perception and the performance of high-level visual models. In this work, a novel synergistic structure is proposed which can balance brightness, color, and illumination more effectively. Specifically, the proposed method, so-called Joint Correcting and Refinement Network (JCRNet), which mainly consists of three stages to balance brightness, color, and illumination of enhancement. Stage 1: we utilize a basic encoder-decoder and local supervision mechanism to extract local information and more comprehensive details for enhancement. Stage 2: cross-stage feature transmission and spatial feature transformation further facilitate color correction and feature refinement. Stage 3: we employ a dynamic illumination adjustment approach to embed residuals between predicted and ground truth images into the model, adaptively adjusting illumination balance. Extensive experiments demonstrate that the proposed method exhibits comprehensive performance advantages over 21 state-of-the-art methods on 9 benchmark datasets. Furthermore, a more persuasive experiment has been conducted to validate our approach the effectiveness in downstream visual tasks (e.g., saliency detection). Compared to several enhancement models, the proposed method effectively improves the segmentation results and quantitative metrics of saliency detection.
C1 [Yu, Nana; Shi, Hong; Han, Yahong] Tianjin Univ, Coll Intelligence & Comp, Tianjin Key Lab Machine Learning, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Han, YH (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin Key Lab Machine Learning, Tianjin 300072, Peoples R China.
EM yunana@tju.edu.cn; serena@tju.edu.cn; yahong@tju.edu.cn
OI Han, Yahong/0000-0003-2768-1398
FU National Natural Science Foundation of China
FX No Statement Available
CR Abdullah-Al-Wadud M, 2007, IEEE T CONSUM ELECTR, V53, P593, DOI 10.1109/TCE.2007.381734
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   CHARBONNIER P, 1994, IEEE IMAGE PROC, P168
   Cheng HD, 2004, DIGIT SIGNAL PROCESS, V14, P158, DOI 10.1016/j.dsp.2003.07.002
   Cheng Y, 2014, IEEE INT CON MULTI
   Co L.H.T., 2022, Artificial Intelligence Technology, P137
   Cui Z., 2022, PROC BRIT MACH VIS C, P1
   Dang-Nguyen D.-T., 2015, P 6 ACM MULT SYST C, P219
   Guo CL, 2020, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR42600.2020.00185
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Han YH, 2021, FRONT INFORM TECH EL, V22, P625, DOI 10.1631/FITEE.2000722
   Hao SJ, 2020, IEEE T MULTIMEDIA, V22, P3025, DOI 10.1109/TMM.2020.2969790
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   Hoyer L, 2022, PROC CVPR IEEE, P9914, DOI 10.1109/CVPR52688.2022.00969
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Jiang YF, 2021, IEEE T IMAGE PROCESS, V30, P2340, DOI 10.1109/TIP.2021.3051462
   Kim G, 2022, IEEE T INTELL TRANSP, V23, P2494, DOI 10.1109/TITS.2021.3117868
   Kui Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8343, DOI 10.1109/CVPR42600.2020.00837
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   Lee C, 2013, IEEE T IMAGE PROCESS, V22, P5372, DOI 10.1109/TIP.2013.2284059
   Li B, 2023, IEEE INT CON MULTI, P1319, DOI 10.1109/ICME55011.2023.00229
   Li CY, 2022, IEEE T PATTERN ANAL, V44, P9396, DOI 10.1109/TPAMI.2021.3126387
   Li YJ, 2023, IEEE T MULTIMEDIA, V25, P1359, DOI 10.1109/TMM.2022.3141604
   Li ZW, 2023, NEUROCOMPUTING, V518, P332, DOI 10.1016/j.neucom.2022.10.083
   Lim S, 2021, IEEE T MULTIMEDIA, V23, P4272, DOI 10.1109/TMM.2020.3039361
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu JY, 2022, IEEE T CIRC SYST VID, V32, P5026, DOI 10.1109/TCSVT.2022.3144455
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu RS, 2021, PROC CVPR IEEE, P10556, DOI 10.1109/CVPR46437.2021.01042
   Liu R, 2022, FRONT COMPUT SCI-CHI, V16, DOI 10.1007/s11704-021-1248-1
   Loh YP, 2019, COMPUT VIS IMAGE UND, V178, P30, DOI 10.1016/j.cviu.2018.10.010
   Loshchilov I., 2017, INT C LEARNING REPRE
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Ma L, 2023, IEEE T MULTIMEDIA, V25, P3573, DOI 10.1109/TMM.2022.3162493
   Ma L, 2022, PROC CVPR IEEE, P5627, DOI 10.1109/CVPR52688.2022.00555
   Ma L, 2022, IEEE T NEUR NET LEAR, V33, P5666, DOI 10.1109/TNNLS.2021.3071245
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mou C, 2022, PROC CVPR IEEE, P17378, DOI 10.1109/CVPR52688.2022.01688
   Park GH, 2008, IEEE T CONSUM ELECTR, V54, P1981, DOI 10.1109/TCE.2008.4711262
   Paszke A., 2017, PROC INT C NEURAL I
   Qu J., 2023, arXiv
   Ren XT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351427
   Sasagawa Yukihiro, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P345, DOI 10.1007/978-3-030-58589-1_21
   Srinivasan S., 2006, P 9 AS S INF DISPL, P152
   Tu ZZ, 2023, IEEE T MULTIMEDIA, V25, P4163, DOI 10.1109/TMM.2022.3171688
   Vonikakis V, 2018, MULTIMED TOOLS APPL, V77, P9211, DOI 10.1007/s11042-017-4783-x
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang J, 2022, IEEE T CIRC SYST VID, V32, P2949, DOI 10.1109/TCSVT.2021.3099120
   Wang LW, 2020, IEEE T IMAGE PROCESS, V29, P7984, DOI 10.1109/TIP.2020.3008396
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wang WC, 2023, SIGNAL PROCESS-IMAGE, V118, DOI 10.1016/j.image.2023.117016
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Y, 1999, IEEE T CONSUM ELECTR, V45, P68, DOI 10.1109/30.754419
   Wang YF, 2022, AAAI CONF ARTIF INTE, P2604
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei C., 2018, PROC BRIT MACH VIS C, P1
   Wu WH, 2022, PROC CVPR IEEE, P5891, DOI 10.1109/CVPR52688.2022.00581
   Xu XG, 2022, PROC CVPR IEEE, P17693, DOI 10.1109/CVPR52688.2022.01719
   Yuan L, 2012, LECT NOTES COMPUT SC, V7575, P771, DOI 10.1007/978-3-642-33765-9_55
   Zamir Syed Waqas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P492, DOI 10.1007/978-3-030-58595-2_30
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang Y., 2023, arXiv
   Zhang YH, 2021, INT J COMPUT VISION, V129, P1013, DOI 10.1007/s11263-020-01407-x
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zhuang PX, 2022, IEEE T IMAGE PROCESS, V31, P5442, DOI 10.1109/TIP.2022.3196546
NR 67
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6310
EP 6324
DI 10.1109/TMM.2023.3348333
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600055
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yuan, ZQ
   Liu, YH
   Xu, H
   Gao, K
AF Yuan, Ziqi
   Liu, Yihe
   Xu, Hua
   Gao, Kai
TI Noise Imitation Based Adversarial Training for Robust Multimodal
   Sentiment Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Robust multimodal sentiment analysis; imperfection topology; adversarial
   training; semantic reconstruction
AB As an inevitable phenomenon in real-world applications, data imperfection has emerged as one of the most critical challenges for multimodal sentiment analysis. However, existing approaches tend to overly focus on a specific type of imperfection, leading to performance degradation in real-world scenarios where multiple types of noise exist simultaneously. In this work, we formulate the imperfection with the modality feature missing at the training period and propose the noise intimation based adversarial training framework to improve the robustness against various potential imperfections at the inference period. Specifically, the proposed method first uses temporal feature erasing as the augmentation for noisy instances construction and exploits the modality interactions through the self-attention mechanism to learn multimodal representation for original-noisy instance pairs. Then, based on paired intermediate representation, a novel adversarial training strategy with semantic reconstruction supervision is proposed to learn unified joint representation between noisy and perfect data. For experiments, the proposed method is first verified with the modality feature missing, the same type of imperfection as the training period, and shows impressive performance. Moreover, we show that our approach is capable of achieving outstanding results for other types of imperfection, including modality missing, automation speech recognition error and attacks on text, highlighting the generalizability of our model. Finally, we conduct case studies on general additive distribution, which introduce background noise and blur into raw video clips, further revealing the capability of our proposed method for real-world applications.
C1 [Yuan, Ziqi; Xu, Hua] Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
   [Liu, Yihe; Gao, Kai] Hebei Univ Sci & Technol, Sch Informat Sci & Engn, Shijiazhuang 050018, Peoples R China.
C3 Tsinghua University; Hebei University of Science & Technology
RP Xu, H (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
EM yzq21@mails.tsinghua.edu.cn; 512796310@qq.com;
   xuhua@mail.tsinghua.edu.cn; gaokai@hebust.edu.cn
OI Yuan, Ziqi/0000-0003-2397-2163
FU National Natural Science Foundation of China
FX No Statement Available
CR Baevski A, 2020, Advances in neural information processing systems, V33, P12449, DOI 10.5555/3495724.3496768
   Balakrishnan S., 2022, arXiv
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bao H., 2021, arXiv preprint arXiv:2106.08254, DOI DOI 10.48550/ARXIV.2106.08254
   Binghua Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P431, DOI 10.1007/978-3-030-58586-0_26
   Chakraborty A, 2021, CAAI T INTELL TECHNO, V6, P25, DOI 10.1049/cit2.12028
   Chi H., 2022, P 2 C AS PAC CHAPT A, P121
   Chumachenko K, 2022, INT C PATT RECOG, P2822, DOI 10.1109/ICPR56361.2022.9956592
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Goodfellow I., 2014, P ADV NEUR PROC SYST
   Grosman J., 2021, Fine-tuned XLSR-53 large model for speech recognition in Dutch
   Han W, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9180
   Han Wei, 2022, arXiv
   Hazarika D., 2022, NAACL 2022 2022 C N, P685, DOI 10.18653/v1/2022.naacl50
   Hazarika D, 2020, Arxiv, DOI arXiv:2005.03545
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   Hsu WN, 2021, IEEE-ACM T AUDIO SPE, V29, P3451, DOI 10.1109/TASLP.2021.3122291
   Kingma D. P., 2014, arXiv
   Lakomkin E, 2019, IEEE INT CONF ROBOT, P7976, DOI [10.1109/icra.2019.8794468, 10.1109/ICRA.2019.8794468]
   Lian Z, 2023, IEEE T PATTERN ANAL, V45, P8419, DOI 10.1109/TPAMI.2023.3234553
   Liang P.P., 2021, PREPRINT
   Liang PP, 2022, Arxiv, DOI arXiv:2209.03430
   Liang PP, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1569
   Ma M., 2022, PROC CVPR IEEE, P18177, DOI [10.1109/CVPR52688.2022.01764, DOI 10.1109/CVPR52688.2022.01764]
   Ma MM, 2021, AAAI CONF ARTIF INTE, V35, P2302
   Mai Sijie, 2023, IEEE Transactions on Multimedia, P4121, DOI 10.1109/TMM.2022.3171679
   Mai SJ, 2020, AAAI CONF ARTIF INTE, V34, P164
   Makhzani A, 2016, Arxiv, DOI arXiv:1511.05644
   Mao H., 2022, arXiv
   Mao HS, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P204
   Pang T., 2020, arXiv
   Parthasarathy S, 2020, COMPANION PUBLICATON OF THE 2020 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION (ICMI '20 COMPANION), P400, DOI 10.1145/3395035.3425202
   Pham H., 2018, arXiv
   Pham H, 2019, AAAI CONF ARTIF INTE, P6892
   Rahman W, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2359, DOI 10.18653/v1/2020.acl-main.214
   Reed S, 2016, PR MACH LEARN RES, V48
   Soleymani M, 2017, IMAGE VISION COMPUT, V65, P3, DOI 10.1016/j.imavis.2017.08.003
   Sun LC, 2023, Arxiv, DOI arXiv:2208.07589
   Tang J., 2021, LONG PAPERS, V1, P5301, DOI DOI 10.18653/V1/2021.ACL-LONG.412
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Tsai YL, 2019, IEEE PHOTON CONF, DOI [10.1109/ipcon.2019.8908433, 10.1109/ijcnn.2019.8852023]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang D, 2023, IEEE T MULTIMEDIA, V25, P4909, DOI 10.1109/TMM.2022.3183830
   Wang ZL, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P2514, DOI 10.1145/3366423.3380000
   Wu Y, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P1397
   Yang K, 2021, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR46437.2021.00335
   Yu WM, 2021, AAAI CONF ARTIF INTE, V35, P10790
   Yuan ZQ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4400, DOI 10.1145/3474085.3475585
   Zadeh A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2236
   Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94
   Zeng JD, 2023, IEEE T MULTIMEDIA, V25, P6301, DOI 10.1109/TMM.2022.3207572
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhao JM, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2608
NR 54
TC 3
Z9 3
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 529
EP 539
DI 10.1109/TMM.2023.3267882
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000030
DA 2024-08-05
ER

PT J
AU Zhang, JC
   Liao, QM
   Ma, HY
   Xue, JH
   Yang, WM
   Liu, SJ
AF Zhang, Juncheng
   Liao, Qingmin
   Ma, Haoyu
   Xue, Jing-Hao
   Yang, Wenming
   Liu, Shaojun
TI Exploit the Best of Both End-to-End and Map-Based Methods for
   Multi-Focus Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-focus; image fusion; deep learning
ID TRANSFORM; FRAMEWORK
AB Multi-focus image fusion is a technique to fuse the images focused on different depth ranges to generate an all-in-focus image. Existing deep learning approaches to multi-focus image fusion can be categorized as end-to-end methods and decision map based methods. End-to-end methods can generate natural fusion near the focus-defocus boundaries (FDB), but the output is often inconsistent with the input in the areas far from the boundaries (FFB). On the contrary, decision map based methods can preserve original images in the FFB areas, but often generate artifacts near the FDB. In this article, we propose a dual-branch network for multi-focus image fusion (DB-MFIF) to exploit the best of both worlds, achieving better results in both FDB and FFB areas, i.e. with naturally sharper FDB areas and more consistent FFB areas with the inputs. In our DB-MFIF, an end-to-end branch and a decision map based branch are proposed to mutually assist each other. In addition, to this end, two map-based loss functions are also proposed. Experiments show that our method surpasses existing algorithms on multiple datasets, both qualitatively and quantitatively, and achieves the state-of-the-art performance.
C1 [Zhang, Juncheng; Liao, Qingmin; Ma, Haoyu; Yang, Wenming] Tsinghua Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhang, Juncheng; Liao, Qingmin; Ma, Haoyu; Yang, Wenming] Tsinghua Univ, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Xue, Jing-Hao] UCL, Dept Stat Sci, London WC1E 6BT, England.
   [Liu, Shaojun] Shenzhen Technol Univ, Coll Hlth Sci & Environm Engn, Shenzhen 518118, Peoples R China.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   University of London; University College London; Shenzhen Technology
   University
RP Liu, SJ (corresponding author), Shenzhen Technol Univ, Coll Hlth Sci & Environm Engn, Shenzhen 518118, Peoples R China.
EM zjc16@mails.tsinghua.edu.cn; liaoqm@tsinghua.edu.cn;
   hy-ma17@mails.tsinghua.edu.cn; jinghao.xue@ucl.ac.uk; yangelwm@163.com;
   liusj14@tsinghua.org.cn
RI Zhang, Juncheng/KHW-2470-2024
OI Liu, Shaojun/0000-0002-9201-9301; Yang, Wenming/0000-0002-2506-1286;
   Xue, Jing-Hao/0000-0003-1174-610X
FU National Natural Science Foundation of China
FX No Statement Available
CR Bai XZ, 2012, APPL OPTICS, V51, P338, DOI 10.1364/AO.51.000338
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P655, DOI 10.1109/TMM.2021.3057493
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   De I, 2013, INFORM FUSION, V14, P136, DOI 10.1016/j.inffus.2012.01.007
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   Hossny M, 2008, ELECTRON LETT, V44, P1066, DOI 10.1049/el:20081754
   Jiang J., 2021, P IEEE INT C MULT EX, P1
   Kaur P., 2015, Int. J. Comput. Appl., V114, P26
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   LI H, 1995, GRAPH MODEL IM PROC, V57, P235, DOI 10.1006/gmip.1995.1022
   Li JX, 2020, IEEE T IMAGE PROCESS, V29, P4816, DOI 10.1109/TIP.2020.2976190
   Li M, 2006, PATTERN RECOGN LETT, V27, P1948, DOI 10.1016/j.patrec.2006.05.004
   Liang PW, 2022, LECT NOTES COMPUT SC, V13678, P719, DOI 10.1007/978-3-031-19797-0_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2022, INFORM FUSION, V86-87, P1, DOI 10.1016/j.inffus.2022.06.001
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Ma BY, 2022, NEUROCOMPUTING, V470, P204, DOI 10.1016/j.neucom.2021.10.115
   Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9
   Ma HY, 2020, IEEE T IMAGE PROCESS, V29, P8668, DOI 10.1109/TIP.2020.3018261
   Ma HY, 2019, IEEE INT CON MULTI, P1150, DOI 10.1109/ICME.2019.00201
   Nejati M, 2015, INFORM FUSION, V25, P72, DOI 10.1016/j.inffus.2014.10.004
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Wan T, 2013, PATTERN RECOGN LETT, V34, P1001, DOI 10.1016/j.patrec.2013.03.003
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu N, 2017, PROC CVPR IEEE, P311, DOI 10.1109/CVPR.2017.41
   Yang Y, 2015, IEEE SENS J, V15, P2824, DOI 10.1109/JSEN.2014.2380153
   Zhang JC, 2020, PATTERN RECOGN LETT, V138, P370, DOI 10.1016/j.patrec.2020.08.002
   Zhang Q, 2018, PATTERN RECOGN, V83, P299, DOI 10.1016/j.patcog.2018.06.003
   Zhang Q, 2009, SIGNAL PROCESS, V89, P1334, DOI 10.1016/j.sigpro.2009.01.012
   Zhang XC, 2022, IEEE T PATTERN ANAL, V44, P4819, DOI 10.1109/TPAMI.2021.3078906
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao F, 2023, IEEE T MULTIMEDIA, V25, P966, DOI 10.1109/TMM.2021.3134565
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
NR 40
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6411
EP 6423
DI 10.1109/TMM.2024.3350924
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600031
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhu, DD
   Zhang, KW
   Zhang, NN
   Zhou, QQ
   Min, XK
   Zhai, GT
   Yang, XK
AF Zhu, Dandan
   Zhang, Kaiwei
   Zhang, Nana
   Zhou, Qiangqiang
   Min, Xiongkuo
   Zhai, Guangtao
   Yang, Xiaokang
TI Unified Audio-Visual Saliency Model for Omnidirectional Videos With
   Spatial Audio
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual saliency; spatial audio; sound source location-awareness;
   audio content attributes-adaptive; audio energy map; omnidirectional
   videos
ID PREDICTION
AB Spatial audio is a crucial component of omnidirectional videos (ODVs), which can provide an immersive experience by enabling viewers to perceive sound sources in all directions. However, most visual attention modeling works for ODVs focus only on visual cues, and audio modality is rather rarely considered. Additionally, the existing audio-visual saliency models for ODVs lack spatial audio location-awareness (i.e. sound source location-agnostic) and audio content attributes discriminability (i.e. audio content attributes-agnostic). To this end, we propose a novel audio-visual perception saliency (AVPS) model with spatial audio location-awareness and audio content attributes-adaptive to efficiently address the problem of fixation prediction in ODVs. Specifically, we first utilize the improved group equivariant convolutional neural network (G-CNN) with eidetic 3D LSTM (E3D-LSTM) to extract spatial-temporal visual features. Then we perceive sound source locations by computing the audio energy map (AEM) of the audio information in ODVs. Subsequently, we introduce SoundNet to extract audio features with multiple attributes. Finally, we develop an audio-visual feature fusion module to adaptively integrate spatial-temporal visual features and spatial auditory information to generate the final audio-visual saliency map. Extensive experiments in three audio modalities validate the effectiveness of the proposed model. Meanwhile, the performance of the proposed model is superior to the other 10 state-of-the-art saliency models.
C1 [Zhu, Dandan] East China Normal Univ, Inst AI Educ, Shanghai 200333, Peoples R China.
   [Zhang, Kaiwei; Min, Xiongkuo; Zhai, Guangtao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Zhang, Nana] Donghua Univ, Sch Comp Sci & Technol, Shanghai 201620, Peoples R China.
   [Zhou, Qiangqiang] Jiangxi Normal Univ, Sch Software, Nanchang 330022, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 East China Normal University; Shanghai Jiao Tong University; Donghua
   University; Jiangxi Normal University; Shanghai Jiao Tong University
RP Zhang, KW (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM ddzhu@dhu.edu.cn; zhangkaiwei@sjtu.edu.cn; nnzhang@dhu.edu.cn;
   zqqcus@gmail.com; minxiongkuo@sjtu.edu.cn; zhanguangtao@sjtu.edu.cn;
   xkyang1@sjtu.edu.cn
RI Zhang, Kaiwei/IAP-8055-2023
OI Zhang, Kaiwei/0000-0002-1620-736X
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
CR Aytar Y, 2016, ADV NEUR IN, V29
   Berdun EB, 2022, COMPUT GRAPH-UK, V106, P200, DOI 10.1016/j.cag.2022.06.002
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Chao F.-Y., 2020, IEEE INT CONF MULTI, P1, DOI [DOI 10.1109/icmew46912.2020.9105956, 10.1109/ICMEW46912.2020.9105956]
   Chao FY, 2020, IEEE I C VI COM I PR, P355, DOI 10.1109/vcip49819.2020.9301766
   Chao FY, 2018, IEEE INT CONF MULTI
   Cheng HT, 2018, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2018.00154
   Cohen TS, 2016, PR MACH LEARN RES, V48
   Coutrot A, 2016, SPRINGER SER COG NEU, V10, P291, DOI 10.1007/978-1-4939-3435-5_16
   Coutrot A, 2014, IEEE IMAGE PROC, P1100, DOI 10.1109/ICIP.2014.7025219
   Coutrot A, 2014, J VISION, V14, DOI 10.1167/14.8.5
   Dahou Yasser, 2021, Pattern Recognition. ICPR International Workshops and Challenges. Proceedings. Lecture Notes in Computer Science (LNCS 12663), P305, DOI 10.1007/978-3-030-68796-0_22
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   de la Fuente YS, 2017, MULTIMED TOOLS APPL, V76, P5631, DOI 10.1007/s11042-016-4097-4
   Gorji S, 2018, PROC CVPR IEEE, P7501, DOI 10.1109/CVPR.2018.00783
   Guo FJ, 2017, IEEE INFOCOM SER, DOI 10.1109/TCYB.2017.2761361
   Harel C., 2006, ADV NEURAL INF PROCE, P1
   He S, 2019, PROC CVPR IEEE, P10198, DOI 10.1109/CVPR.2019.01045
   Hou L., 2008, ADV NEURAL INF PROCE, P1
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang L, 2018, LECT NOTES COMPUT SC, V11218, P625, DOI 10.1007/978-3-030-01264-9_37
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Lai QX, 2020, IEEE T IMAGE PROCESS, V29, P1113, DOI 10.1109/TIP.2019.2936112
   Le Meur O, 2006, IEEE T PATTERN ANAL, V28, P802, DOI 10.1109/TPAMI.2006.86
   Li D, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201391
   Mahadevan V, 2010, IEEE T PATTERN ANAL, V32, P171, DOI 10.1109/TPAMI.2009.112
   Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3
   Min K, 2019, IEEE I CONF COMP VIS, P2394, DOI 10.1109/ICCV.2019.00248
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P3805, DOI 10.1109/TIP.2020.2966082
   Monroy R, 2018, SIGNAL PROCESS-IMAGE, V69, P26, DOI 10.1016/j.image.2018.05.005
   Nasrabadi AT, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P273, DOI 10.1145/3304109.3325812
   Ozcinar C, 2018, INT WORK QUAL MULTIM, P1
   Ozcinar C, 2021, SIGNAL IMAGE VIDEO P, V15, P493, DOI 10.1007/s11760-020-01769-2
   Ozcinar C, 2019, IEEE J EM SEL TOP C, V9, P217, DOI 10.1109/JETCAS.2019.2895096
   Rebuffi S.A., 2020, P IEEECVF C COMPUTER, P8839, DOI [10.1109/CVPR42600.2020.00886, DOI 10.1109/CVPR42600.2020.00886]
   Rudoy D, 2013, PROC CVPR IEEE, P1147, DOI 10.1109/CVPR.2013.152
   Sitzmann V, 2018, IEEE T VIS COMPUT GR, V24, P1633, DOI 10.1109/TVCG.2018.2793599
   Souza LS, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107028
   Sun MJ, 2019, IEEE T CYBERNETICS, V49, P2900, DOI 10.1109/TCYB.2018.2832053
   Tavakoli A., 2020, ACM Sym. Eye Track. Res. Appl., P1
   Tliba M., 2021, 11 INT C PATT REC SY, P31
   Tollmar K, 2017, ACM SIGGRAPH 2017 TALKS, DOI 10.1145/3084363.3085163
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2021, IEEE T PATTERN ANAL, V43, P2413, DOI 10.1109/TPAMI.2020.2966453
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Y., 2019, INT C LEARN REPR
   Wang Z, 2022, IEEE T CYBERNETICS, V52, P1490, DOI 10.1109/TCYB.2020.2989158
   Xiongkuo Min, 2015, 2015 Visual Communications and Image Processing (VCIP), P1, DOI 10.1109/VCIP.2015.7457921
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Yang S, 2020, IEEE T MULTIMEDIA, V22, P2163, DOI 10.1109/TMM.2019.2947352
   Yun H, 2022, LECT NOTES COMPUT SC, V13695, P422, DOI 10.1007/978-3-031-19833-5_25
   Zhang YF, 2021, IEEE WINT CONF APPL, P484, DOI 10.1109/WACV48630.2021.00053
   Zhou T, 2021, COMPUT VIS MEDIA, V7, P37, DOI 10.1007/s41095-020-0199-z
   Zhu D, 2021, PROC IEEE INT C MULT, P1
   Zhu YC, 2022, IEEE T CIRC SYST VID, V32, P4188, DOI 10.1109/TCSVT.2021.3126590
NR 62
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 764
EP 775
DI 10.1109/TMM.2023.3271022
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000008
DA 2024-08-05
ER

PT J
AU Chen, YH
   Zheng, H
   Li, YC
   Ouyang, WL
   Zhu, J
AF Chen, Yihong
   Zheng, Hao
   Li, Yanchun
   Ouyang, Wanli
   Zhu, Jiang
TI Online Handwritten Chinese Character Recognition Based on 1-D
   Convolution and Two-Streams Transformers
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE OLHCCR; two-streams transformers; 1-D convolution; vision transformer
AB As one of the classic problems of pattern recognition, the online Handwritten Chinese Character Recognition (OLHCCR) has attracted the attention of many researchers. Yet, it remains challenging due to complex glyphs, numerous strokes, and huge categories. Existing methods utilize temporal features or spatial features to recognize handwritten characters, which results in recognition errors due to the character with non-standard stroke order. This paper proposes a new OLHCCR model based on 1-D Convolution and Two-Streams Transformers. The model has a 1-D Transformer and a Vision Transformer, and the 1-D Transformer contains a 1-D Convolution layer and Transformers, that is, the model has overall structure of Two-Streams Transformers with 1-D Convolution. So, the model is named as C-TST. It can fuse temporal and spatial features of Chinese character to achieve high recognition accuracy and fast recognition speed. Specifically, each online handwritten Chinese character is represented by a trajectory sequence. The original trajectory sequence is preprocessed to enhance the information density of each trajectory point and features difference among trajectory points. Then, the result after preprocessing is input into the 1-D convolution layer to obtain shallow temporal features, which are used also as the input of the Transformers to capture the temporal features. Simultaneously, character image is generated by processing the original trajectory sequence, and then fed into the Vision Transformer to capture the spatial features. By fusing the captured temporal and spatial features of online handwritten Chinese character, the proposed C-TST achieves a recognition accuracy of 97.90% on ICDAR-2013 and a state-of-the-art recognition accuracy of 97.38% on IAHCC-UCAS2016.
C1 [Chen, Yihong] China West Normal Univ, Internet Things Percept & Big Data Anal Key Lab Na, Nanchong 637000, Peoples R China.
   [Zheng, Hao] China West Normal Univ, Sch Comp, Nanchong 637000, Peoples R China.
   [Li, Yanchun; Zhu, Jiang] Xiangtan Univ, Key Lab Hunan Prov Internet Things & Informat Secu, Xiangtan 411105, Peoples R China.
   [Ouyang, Wanli] Univ Sydney, Sydney, NSW 2006, Australia.
   [Ouyang, Wanli] Shanghai AI Lab, Shanghai 200232, Peoples R China.
C3 China West Normal University; China West Normal University; Xiangtan
   University; University of Sydney; Shanghai Artificial Intelligence
   Laboratory
RP Li, YC (corresponding author), Xiangtan Univ, Key Lab Hunan Prov Internet Things & Informat Secu, Xiangtan 411105, Peoples R China.
EM cyhswpi@126.com; zhenghao9812@gmail.com; ycli@xtu.edu.cn;
   wanli.ouyang@sydney.edu.au; zhu_jiang@xtu.edu.cn
OI yanchun, Li/0000-0002-2754-9883
FU National Key Research and Development Program of China
FX No Statement Available
CR Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen XX, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3440756
   Ciresan D, 2015, IEEE IJCNN
   Deng X, 2023, IEEE T MULTIMEDIA, V25, P4013, DOI 10.1109/TMM.2022.3171095
   Dong YN, 2022, IEEE T IMAGE PROCESS, V31, P1559, DOI 10.1109/TIP.2022.3144017
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Fang SC, 2021, PROC CVPR IEEE, P7094, DOI 10.1109/CVPR46437.2021.00702
   Fogel S, 2020, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR42600.2020.00438
   Gan J, 2023, PATTERN RECOGN, V137, DOI 10.1016/j.patcog.2023.109317
   Gan J, 2020, PATTERN RECOGN LETT, V129, P190, DOI 10.1016/j.patrec.2019.11.028
   Gan J, 2019, INFORM SCIENCES, V478, P375, DOI 10.1016/j.ins.2018.11.035
   Gazda M, 2022, IEEE T SYST MAN CY-S, V52, P78, DOI 10.1109/TSMC.2020.3048892
   Ghosh T, 2022, COMPUT SCI REV, V46, DOI 10.1016/j.cosrev.2022.100515
   Han DC, 2023, IEEE I CONF COMP VIS, P5938, DOI 10.1109/ICCV51070.2023.00548
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Jiang Y., 2021, P 35 C NEUR INF PROC, P14745
   Jiang Y., 2023, Knowl.-Based Syst., V266
   Jin LW, 2011, INT J DOC ANAL RECOG, V14, P53, DOI 10.1007/s10032-010-0116-6
   Lai SX, 2017, PATTERN RECOGN LETT, V89, P60, DOI 10.1016/j.patrec.2017.02.011
   Li YX, 2023, IEEE T MULTIMEDIA, V25, P2140, DOI 10.1109/TMM.2022.3143324
   Li ZY, 2020, PATTERN RECOGN, V107, DOI 10.1016/j.patcog.2020.107471
   Lin X, 2023, IEEE T MULTIMEDIA, V25, P50, DOI 10.1109/TMM.2021.3120873
   Liu CL, 2013, PATTERN RECOGN, V46, P155, DOI 10.1016/j.patcog.2012.06.021
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu PF, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3560815
   Liu X, 2020, IEEE T NEUR NET LEAR, V31, P4637, DOI 10.1109/TNNLS.2019.2956965
   Liu YW, 2022, INT J INTELL SYST, V37, P135, DOI 10.1002/int.22620
   Liu ZC, 2021, PR MACH LEARN RES, V139
   Long T, 2008, PATTERN RECOGN, V41, P2916, DOI 10.1016/j.patcog.2008.02.009
   Magrofuoco N, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3465400
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Paszke A, 2019, ADV NEUR IN, V32
   Pei JL, 2023, IEEE T MULTIMEDIA, V25, P1964, DOI 10.1109/TMM.2022.3141891
   Peng DZ, 2021, LECT NOTES COMPUT SC, V12823, P157, DOI 10.1007/978-3-030-86334-0_11
   Qu W., 2016, IEEE INT C MULTIMEDI, P1
   Qu XW, 2018, PATTERN RECOGN LETT, V111, P9, DOI 10.1016/j.patrec.2018.04.001
   Qu XW, 2018, PATTERN RECOGN, V78, P267, DOI 10.1016/j.patcog.2018.01.021
   Raghu M, 2021, ADV NEUR IN, V34
   Ren HQ, 2019, PATTERN RECOGN, V93, P179, DOI 10.1016/j.patcog.2019.04.015
   Ren HQ, 2017, IEEE INT CON MULTI, P841, DOI 10.1109/ICME.2017.8019443
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Santurkar S, 2018, ADV NEUR IN, V31
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun ZQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3591, DOI 10.1109/ICCV48922.2021.00359
   Vaswani A., 2017, PROC 31 C NEURAL INF, V30, P1
   Wang TQ, 2018, AAAI CONF ARTIF INTE, P2540
   Wang YB, 2023, IEEE T PATTERN ANAL, V45, P2208, DOI 10.1109/TPAMI.2022.3165153
   Wei XH, 2018, PATTERN RECOGN, V76, P679, DOI 10.1016/j.patcog.2017.09.044
   Xie E., 2021, ADV NEURAL INF PROCE, V34, P12077
   Xie ZC, 2018, IEEE T PATTERN ANAL, V40, P1903, DOI 10.1109/TPAMI.2017.2732978
   Xu W., 2015, 7 INT C INTERNETMULT, P1
   Xu X., 2019, ADV NEURAL INF PROCE, P1
   Yang FZ, 2020, PROC CVPR IEEE, P5790, DOI 10.1109/CVPR42600.2020.00583
   Yang WX, 2016, PATTERN RECOGN, V58, P190, DOI 10.1016/j.patcog.2016.04.007
   Yang WX, 2015, PROC INT CONF DOC, P551, DOI 10.1109/ICDAR.2015.7333822
   Yin F, 2013, PROC INT CONF DOC, P1464, DOI 10.1109/ICDAR.2013.218
   Zhang JS, 2020, PATTERN RECOGN, V103, DOI 10.1016/j.patcog.2020.107305
   Zhang XY, 2018, IEEE T PATTERN ANAL, V40, P849, DOI 10.1109/TPAMI.2017.2695539
   Zhang XY, 2017, PATTERN RECOGN, V61, P348, DOI 10.1016/j.patcog.2016.08.005
   Zhao B, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2916757
   Zhong ZY, 2015, PROC INT CONF DOC, P846, DOI 10.1109/ICDAR.2015.7333881
NR 62
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5769
EP 5781
DI 10.1109/TMM.2023.3339589
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100014
DA 2024-08-05
ER

PT J
AU Ding, FF
   Li, JJ
   Tian, WY
   Zhang, SQ
   Yuan, WQ
AF Ding, Feifei
   Li, Jianjun
   Tian, Wanyong
   Zhang, Shanqing
   Yuan, Wenqiang
TI Unsupervised Domain Adaptation via Risk-Consistent Estimators
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Training; Adaptation models; Estimation; Feature
   extraction; Entropy; Data models; Noise transition matrix; pseudo-label;
   self-training; unsupervised domain adaptation
AB Unsupervised domain adaptation (UDA) attempts to learn domain invariant representations and has achieved significant progress, whereas self-training-based UDA methods have shown powerful performance. However, due to the domain gap, pseudo-labels selected through high confidence scores or uncertainty inevitably contain noise, leading to inaccurate predictions. To address this issue, we propose a novel risk-consistent training method. Specifically, both clean and noisy classifiers are introduced to estimate the noise transition matrix. The clean classifier is exploited to assign pseudo-labels for target data in each iteration. The noisy classifier is then trained with noisy target samples, and the optimal parameters are obtained through a closed-form solution. Heuristically, we also pre-train a domain predictor to select a target-like source example for the noise transition matrix estimation. In addition, we design an uncertainty-guided regularization to generate soft pseudo-labels and avoid overconfident predictions. Extensive experimental results show the effectiveness of our method, and state-of-the-art performance has been achieved. Codes are available at https://github.com/feifei-cv/RCE.
C1 [Ding, Feifei; Li, Jianjun; Zhang, Shanqing; Yuan, Wenqiang] Hangzhou Dianzi Univ, Sch Comp Sci & Engn, Hangzhou 310018, Peoples R China.
   [Tian, Wanyong] China Elect Technol Grp Corp, Key Lab Data Link Technol, Xian, Peoples R China.
C3 Hangzhou Dianzi University; China Electronics Technology Group
RP Li, JJ (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Engn, Hangzhou 310018, Peoples R China.
EM dingfeifei7@gmail.com; jianjun.li@hdu.edu.cn; frogustc@163.com;
   sqzhang@hdu.edu.cn; wqyuan@hdu.edu.cn
RI DING, FEI/KLD-8925-2024
OI ding, feifei/0000-0002-0934-2942; Li, Jianjun/0000-0001-6658-9709
FU Key Research and Development Plan of Zhejiang
FX No Statement Available
CR Aila T., 2017, P INT C LEARN REPR
   Algan G, 2021, KNOWL-BASED SYST, V215, DOI 10.1016/j.knosys.2021.106771
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Chen Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3399, DOI 10.1145/3474085.3475496
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Fu S, 2023, INFORM SCIENCES, V622, P83, DOI 10.1016/j.ins.2022.11.129
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ge PF, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109088
   Grandvalet Y, 2004, Proceedings of NIPS, V17
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hongxin Wei, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13723, DOI 10.1109/CVPR42600.2020.01374
   Kang GL, 2022, IEEE T PATTERN ANAL, V44, P1793, DOI 10.1109/TPAMI.2020.3029948
   Kumar A, 2020, PR MACH LEARN RES, V119
   Kye SM, 2022, LECT NOTES COMPUT SC, V13685, P717, DOI 10.1007/978-3-031-19806-9_41
   Li B, 2020, Arxiv, DOI arXiv:2006.13352
   Li MJ, 2023, IEEE T PATTERN ANAL, V45, P3918, DOI 10.1109/TPAMI.2022.3181116
   Li Xuefeng, 2021, P MACHINE LEARNING R, V139
   Liang J, 2021, PROC CVPR IEEE, P16627, DOI 10.1109/CVPR46437.2021.01636
   Liu Hong, 2021, ADV NEURAL INFORM PR, V34
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lu YW, 2022, IEEE T IMAGE PROCESS, V31, P5303, DOI 10.1109/TIP.2022.3193758
   Lu YW, 2021, IEEE T MULTIMEDIA, V23, P2056, DOI 10.1109/TMM.2020.3007340
   Luo X, 2023, NEURAL NETWORKS, V157, P216, DOI 10.1016/j.neunet.2022.10.015
   Ma WX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P5620, DOI 10.1145/3503161.3548229
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Pan YW, 2019, PROC CVPR IEEE, P2234, DOI 10.1109/CVPR.2019.00234
   Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240
   Peng XC, 2017, Arxiv, DOI arXiv:1710.06924
   Prabhu V, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8538, DOI 10.1109/ICCV48922.2021.00844
   Qiang WW, 2023, IEEE T KNOWL DATA EN, V35, P3014, DOI 10.1109/TKDE.2021.3112815
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K, 2019, IEEE I CONF COMP VIS, P8049, DOI 10.1109/ICCV.2019.00814
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sharma A, 2021, PROC CVPR IEEE, P5357, DOI 10.1109/CVPR46437.2021.00532
   Sohn K., 2020, Advances in Neural Information Processing Systems, P596
   Sun T, 2022, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR52688.2022.00705
   Tarvainen A, 2017, ADV NEUR IN, V30
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Wang Z, 2020, PROC CVPR IEEE, P4523, DOI 10.1109/CVPR42600.2020.00458
   Wei C., 2021, PROC 9 INT C LEARN R, P1
   Yabin Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P781, DOI 10.1007/978-3-030-58548-8_45
   Yao Yu, 2020, ADV NEURAL INFORM PR, P7260, DOI DOI 10.48550/ARXIV.2006.07805
   Ying Jin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P464, DOI 10.1007/978-3-030-58589-1_28
   Zhang LL, 2023, IEEE T PATTERN ANAL, V45, P3848, DOI 10.1109/TPAMI.2022.3183586
   Zhang Y., 2021, arXiv
   Zhang Y., 2019, P 36 INT C MACH LEAR, P7404
   Zhang Y, 2021, PR MACH LEARN RES, V139
   Zheng GQ, 2021, AAAI CONF ARTIF INTE, V35, P11053
   Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608
NR 54
TC 2
Z9 2
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1179
EP 1187
DI 10.1109/TMM.2023.3277275
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700012
DA 2024-08-05
ER

PT J
AU He, SY
   Wei, JW
   Zhang, CN
   Xu, X
   Song, JK
   Yang, Y
   Shen, HT
AF He, Shiyuan
   Wei, Jiwei
   Zhang, Chaoning
   Xu, Xing
   Song, Jingkuan
   Yang, Yang
   Shen, Heng Tao
TI Boosting Adversarial Training with Hardness-Guided Attack Strategy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Robustness; Data models; Perturbation methods; Adaptation
   models; Standards; Predictive models; Adversarial defense; adversarial
   training; model robustness
ID ROBUSTNESS
AB The susceptibility of deep neural networks (DNNs) to adversarial examples has raised significant concerns regarding the security and reliability of artificial intelligence systems. These examples contain maliciously crafted perturbations not perceptible to the human eye but can cause the model to make wrong predictions. Adversarial training (AT) is the de facto standard method for enhancing adversarial robustness. However, the improved robustness is often at the cost of a significant drop in standard accuracy for clean samples. Numerous works have attempted to alleviate this trade-off by identifying its causes. A key factor lies in the variability of clean samples, which leads to different adversarial examples being generated using the same attack strategy. The other factor is the disruption of the underlying data structure caused by adversarial perturbations. To overcome these challenges, we propose a novel adversarial training framework named Hardness-Guided Sample-Dependent Adversarial Training (HGSD-AT), which dynamically adjusts the attack strategy based on the hardness of the current adversarial sample to further improve the robustness of the model. By utilizing the two types of constraints which construct from a temporal perspective and spatial distribution perspective, our method directly learns the impact of attack methods on the model, rather than the indirect effects associated with sample distribution. This approach aims to improve the generation of adversarial examples while simultaneously enhancing the robustness and accuracy of DNNs. Our approach exhibits superior performance in terms of both robustness and natural accuracy compared to state-of-the-art defense methods, as validated through comprehensive experiments conducted on three benchmark datasets.
C1 [He, Shiyuan; Wei, Jiwei; Xu, Xing; Song, Jingkuan; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.
   [He, Shiyuan; Wei, Jiwei; Xu, Xing; Song, Jingkuan; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
   [Zhang, Chaoning] Korea Adv Inst Sci & Technol, Sch Elect Engn, Robot & Comp Vis Lab, Daejeon 611731, South Korea.
   [Yang, Yang] Univ Elect Sci & Technol China, Inst Elect & Informat Engn, Zhongshan 523808, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   Electronic Science & Technology of China; Korea Advanced Institute of
   Science & Technology (KAIST); University of Electronic Science &
   Technology of China; Peng Cheng Laboratory
RP Wei, JW (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 611731, Peoples R China.; Wei, JW (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM mathematic6@gmail.com
RI Shen, Heng Tao/ABD-5331-2021
OI song, jingkuan/0000-0002-2549-8322; Wei, Jiwei/0000-0003-3912-1742
FU National Natural Science Foundation of China
FX No Statement Available
CR Akhtar N, 2018, PROC CVPR IEEE, P3389, DOI 10.1109/CVPR.2018.00357
   Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Athalye A, 2018, PR MACH LEARN RES, V80
   Bai J., 2021, PROC INT C LEARN REP, P1
   Bai Y., 2021, PROC INT C LEARN REP, P1
   Cai QZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3740
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Cheng X., 2022, arXiv, DOI 10.48550/arXiv.2205.15130
   Croce F, 2020, PR MACH LEARN RES, V119
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15701, DOI 10.1109/ICCV48922.2021.01543
   de Jorge Aranda P., 2022, Advances in Neural Information Processing Systems, V35, P12881
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Du YL, 2019, IEEE T MULTIMEDIA, V21, P555, DOI 10.1109/TMM.2018.2887018
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Goodfellow I. J, 2017, PROC INT C LEARN REP
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Gu J., 2021, PROC INT C LEARN REP, P1
   Gu JD, 2021, PROC CVPR IEEE, P14304, DOI 10.1109/CVPR46437.2021.01408
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Ilyas A, 2019, ADV NEUR IN, V32
   Jia XJ, 2021, Arxiv, DOI arXiv:2108.00422
   Jia XJ, 2022, PROC CVPR IEEE, P13388, DOI 10.1109/CVPR52688.2022.01304
   Jiawang Bai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P618, DOI 10.1007/978-3-030-58452-8_36
   Kannan H, 2018, Arxiv, DOI arXiv:1803.06373
   Krizhevsky A, 2009, CIFAR-10 dataset
   Lee S, 2020, PROC CVPR IEEE, P269, DOI 10.1109/CVPR42600.2020.00035
   Li Siyuan, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P610, DOI 10.1145/3503161.3548138
   Li YM, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108472
   Liu C., 2020, NeurIPS, V33, P21476
   Madry A., 2018, INT C LEARN REPR, P1
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Pang X., 2020, Adv. NeuralInf. Process. Syst., V33, P7779
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Rice L., 2020, PMLR, P8093
   Rifai S, 2011, Proceedings of the 28th International Conference on Machine Learning (ICML-11)
   Roth K, 2020, ADV NEUR IN, V33
   Sitawarin Chawin, 2021, AISec '21: Proceedings of the 14th ACM Workshop on Artificial Intelligence and Security, P25, DOI 10.1145/3474369.3486878
   Song CB, 2021, Arxiv, DOI arXiv:2109.00678
   Szegedy C., 2014, P 2 INT C LEARN REPR, P1
   Wan C, 2023, IEEE T MULTIMEDIA, V25, P9572, DOI 10.1109/TMM.2023.3255742
   Wang H., 2020, Advances in Neural Information Processing Systems, P7449
   Wang QZ, 2021, ADV NEUR IN, V34
   Wang XS, 2021, PROC CVPR IEEE, P1924, DOI 10.1109/CVPR46437.2021.00196
   Wang XG, 2021, PROC CVPR IEEE, P16352, DOI 10.1109/CVPR46437.2021.01609
   Wang Y., 2020, PROC INT C LEARN REP
   Wang YS, 2019, PR MACH LEARN RES, V97
   Wei JW, 2022, IEEE T PATTERN ANAL, V44, P6534, DOI 10.1109/TPAMI.2021.3088863
   Wei Jiwei, 2020, P IEEE CVF C COMP VI, P13005
   Wei XX, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P954
   Wong E., 2020, PROC INT C LEARN REP
   Wu DX, 2020, ADV NEUR IN, V33
   Xiong YF, 2022, PROC CVPR IEEE, P14963, DOI 10.1109/CVPR52688.2022.01456
   Xu X, 2021, IEEE T IND INFORM, V17, P4117, DOI 10.1109/TII.2020.3024643
   Xu Y, 2019, PROC CVPR IEEE, P4130, DOI 10.1109/CVPR.2019.00426
   Yin B., 2021, P INT JOINT C ART IN, P1252
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang, 2020, INT C MACHINE LEARNI, P11278
   Zhang HY, 2019, PR MACH LEARN RES, V97
   Zhang J., 2021, PROC INT C LEARN REP, P1
   Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485
   Zheng YL, 2020, Arxiv, DOI arXiv:2010.02529
   Zhu KJ, 2023, IEEE I CONF COMP VIS, P4401, DOI 10.1109/ICCV51070.2023.00408
NR 68
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7748
EP 7760
DI 10.1109/TMM.2024.3371211
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000024
DA 2024-08-05
ER

PT J
AU He, SH
   Xu, DW
   Yang, L
   Liang, WP
AF He, Songhan
   Xu, Dawen
   Yang, Lin
   Liang, Weipeng
TI Adaptive HEVC Video Steganography With High Performance Based on
   Attention-Net and PU Partition Modes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video steganography; attention-net; HEVC; PU partition mode;
   STC
ID ALGORITHM; CNN
AB With the increasing popularity of digital video, video steganography has become a hot research topic in the field of covert communication and privacy protection. The existing prediction unit (PU) based video steganography often tends to result in large bit rate increase, which is also easily noticeable to the steganography analyst. To solve this problem, an adaptive steganography for HEVC video based on attention-net and PU partition modes is proposed. First, the distortion of modified PUs is analyzed from the perspective of rate distortion optimization at the group of pictures (GOP) level, and we find that modifying PU will lead to distortion accumulation and abnormal bitrate increase. Therefore, an adaptive distortion function based on the improved rate distortion cost is designed, and the embedding distortion is minimized by using Syndrome-Trellis Code (STC) steganography coding. Meanwhile, a super-resolution convolutional neural network with non-local sparse attention-net filter is proposed to replace the in-loop filter in HEVC to reconstruct the reference frame, thereby reducing the bitrate cost and improving the visual quality of stego-video. Experimental results show that the proposed algorithm can achieve superior perceptual quality and bitrate performance comparing with the sate-of-the-art works.
C1 [He, Songhan; Yang, Lin; Liang, Weipeng] Ningbo Univ, Fac Elect Engn & Comp Sci, Ningbo 315211, Peoples R China.
   [Xu, Dawen] Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
C3 Ningbo University; Ningbo University of Technology
RP Xu, DW (corresponding author), Ningbo Univ Technol, Sch Cyber Sci & Engn, Ningbo 315211, Peoples R China.
EM 2111082349@nbu.edu.cn; dawenxu@126.com; 2011082340@nbu.edu.cn;
   2111082360@nbu.edu.cn
RI he, songhan/KVY-0154-2024
OI xu, dawen/0000-0002-9619-8407
FU National Natural Science Foundation of China
FX No Statement Available
CR BjAntegaard G., 2001, Tech Rep. ITU-TVCEGM33
   Chang PC, 2014, J VIS COMMUN IMAGE R, V25, P239, DOI 10.1016/j.jvcir.2013.10.007
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Ding DD, 2020, IEEE T CIRC SYST VID, V30, P1871, DOI 10.1109/TCSVT.2019.2935508
   Ding DD, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954565
   Dong Y, 2023, IEEE T DEPEND SECURE, V20, P769, DOI 10.1109/TDSC.2022.3144139
   Dong Y, 2023, IEEE T MULTIMEDIA, V25, P2698, DOI 10.1109/TMM.2022.3150180
   Dong Y, 2017, LECT NOTES COMPUT SC, V10431, P149, DOI 10.1007/978-3-319-64185-0_12
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He PS, 2021, IEEE T MULTIMEDIA, V23, P3179, DOI 10.1109/TMM.2020.3021234
   He SH, 2022, J VIS COMMUN IMAGE R, V87, DOI 10.1016/j.jvcir.2022.103549
   Jct-Vc, 2014, HEVC reference model HM16.15 gitlab
   Jia CM, 2019, IEEE T IMAGE PROCESS, V28, P3343, DOI 10.1109/TIP.2019.2896489
   Kingma D. P., 2014, arXiv
   Li ZH, 2023, IEEE T DEPEND SECURE, V20, P606, DOI 10.1109/TDSC.2022.3140899
   Li ZH, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11081015
   Li ZH, 2019, CMC-COMPUT MATER CON, V59, P563, DOI 10.32604/cmc.2019.05565
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu JD, 2022, IEEE T MULTIMEDIA, V24, P2084, DOI 10.1109/TMM.2021.3075858
   Liu YX, 2019, MULTIMED TOOLS APPL, V78, P6459, DOI 10.1007/s11042-018-6320-y
   Van LP, 2015, IEEE IMAGE PROC, P3610, DOI 10.1109/ICIP.2015.7351477
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Mengyuan Guo, 2020, Digital Forensics and Watermarking. 18th International Workshop, IWDW 2019. Revised Selected Papers. Lecture Notes in Computer Science (LNCS 12022), P293, DOI 10.1007/978-3-030-43575-2_25
   [盛琪 Sheng Qi], 2017, [光电子·激光, Journal of Optoelectronics·Laser], V28, P433
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Wang Y, 2018, PROCEEDINGS OF THE 6TH ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY (IH&MMSEC'18), P97, DOI 10.1145/3206004.3206020
   Wu ST, 2020, IEEE T MULTIMEDIA, V22, P256, DOI 10.1109/TMM.2019.2920605
   [徐健 Xu Jian], 2015, [光电子·激光, Journal of Optoelectronics·Laser], V26, P1753
   Yang J, 2018, MULTIMED TOOLS APPL, V77, P11979, DOI 10.1007/s11042-017-4844-1
   Yang YY, 2019, MULTIMED TOOLS APPL, V78, P8423, DOI 10.1007/s11042-018-6859-7
   Yao YZ, 2016, SIGNAL PROCESS, V128, P531, DOI 10.1016/j.sigpro.2016.05.004
   Zhai LM, 2020, IEEE T INF FOREN SEC, V15, P1762, DOI 10.1109/TIFS.2019.2949428
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3827, DOI 10.1109/TIP.2018.2815841
   Zhang ZJ, 2021, EURASIP J WIREL COMM, V2021, DOI 10.1186/s13638-021-01895-6
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
NR 36
TC 8
Z9 8
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 687
EP 700
DI 10.1109/TMM.2023.3269663
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000020
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Liu, J
   Fan, ZW
   Yang, ZW
   Su, YT
   Yang, XK
AF Liu, Jing
   Fan, Zhiwei
   Yang, Ziwen
   Su, Yuting
   Yang, Xiaokang
TI Multi-Stage Spatio-Temporal Fusion Network for Fast and Accurate Video
   Bit-Depth Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image reconstruction; Task analysis; Fuses; Motion
   compensation; Distortion; Image color analysis; Video bit-depth
   enhancement; multiple stages; spatio-temporal fusion
ID EXPANSION
AB For video bit-depth enhancement (VBDE) tasks, inter-frame information is critical for removing false contours and recovering the details in low bit-depth (LBD) videos. However, due to different structural distortions and complex motions in the neighboring frames, it is difficult to effectively utilized inter-frame information. Most algorithms rely on alignment operations to provide information of neighboring frames, suffering from slow inference speed due to the complex alignment module design. Meanwhile, most existing methods sequentially perform the intra-frame feature extractions and inter-frame information fusions, but fail to efficiently fuse spatio-temporal information. Therefore, in this paper, we propose a two-stage progressive group (TSPG) network to find complementary information related to the target frame without adopting an alignment operation. To simultaneously achieve intra-frame feature extractions and inter-frame feature fusions, we propose a parallel spatio-temporal fusion (PSTF) module with a dual-branch spatial-temporal residual (DSTR) block to focus on more useful temporal information while ensuring a faster inference speeds. Extensive experiments on public datasets demonstrate that our proposed multi-stage spatio-temporal fusion network (named MSTFN) can quickly and effectively eliminate false contours and recover high quality target frames. Furthermore, our method outperforms the state-of-the-art methods in terms of both PSNR and SSIM, and can reach faster inference speeds.
C1 [Liu, Jing; Fan, Zhiwei; Yang, Ziwen; Su, Yuting] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Yang, Xiaokang] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
C3 Tianjin University; Shanghai Jiao Tong University
RP Liu, J (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jliu_tju@tju.edu.cn; zhwe.fan@qq.com; yangs163email@163.com;
   ytsu@tju.edu.cn; xkyang@sjtu.edu.cn
FU Shanghai Rising Star
FX No Statement Available
CR Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Byun J, 2019, LECT NOTES COMPUT SC, V11362, P67, DOI 10.1007/978-3-030-20890-5_5
   Caballero J, 2017, PROC CVPR IEEE, P2848, DOI 10.1109/CVPR.2017.304
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, 10.48550/arXiv.1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Cheng CH, 2009, IEEE INT SYMP CIRC S, P944, DOI 10.1109/ISCAS.2009.5117913
   Daly S, 2004, PROC SPIE, V5292, P130, DOI 10.1117/12.526937
   Drulea M, 2011, IEEE INT C INTELL TR, P318, DOI 10.1109/ITSC.2011.6082986
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Isobe Takashi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8005, DOI 10.1109/CVPR42600.2020.00803
   Kim TH, 2018, LECT NOTES COMPUT SC, V11207, P111, DOI 10.1007/978-3-030-01219-9_7
   Kingma D. P., 2014, arXiv
   Li S, 2019, PROC CVPR IEEE, P10514, DOI 10.1109/CVPR.2019.01077
   Li Wenbo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P335, DOI 10.1007/978-3-030-58607-2_20
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liu J., 2017, INT FORUM DIGITAL TV, P255
   Liu J, 2022, IEEE T MULTIMEDIA, V24, P4212, DOI 10.1109/TMM.2021.3115039
   Liu J, 2022, IEEE T CIRC SYST VID, V32, P2773, DOI 10.1109/TCSVT.2021.3098707
   Liu J, 2019, IEEE T MULTIMEDIA, V21, P2397, DOI 10.1109/TMM.2019.2897909
   Liu J, 2019, IEEE T IMAGE PROCESS, V28, P4926, DOI 10.1109/TIP.2019.2912294
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P4860, DOI 10.1109/TIP.2018.2803306
   Lu T, 2024, IEEE T NEUR NET LEAR, V35, P3938, DOI 10.1109/TNNLS.2022.3201448
   Mittal G, 2012, 2012 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Nair V., 2010, ICML, P807
   Pengfei Wan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P170, DOI 10.1109/ICME.2012.118
   Punnappurath A, 2022, IEEE T PATTERN ANAL, V44, P9718, DOI 10.1109/TPAMI.2021.3125692
   Sun W, 2020, NEUROCOMPUTING, V403, P1, DOI 10.1016/j.neucom.2020.04.039
   Tian YP, 2020, PROC CVPR IEEE, P3357, DOI 10.1109/CVPR42600.2020.00342
   Ulichney R, 1998, P SOC PHOTO-OPT INS, V3300, P232, DOI 10.1117/12.298285
   Wan PF, 2016, IEEE T IMAGE PROCESS, V25, P2896, DOI 10.1109/TIP.2016.2553523
   Wan PF, 2014, IEEE IMAGE PROC, P4052, DOI 10.1109/ICIP.2014.7025823
   Wan PF, 2012, IEEE IMAGE PROC, P953, DOI 10.1109/ICIP.2012.6467019
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XT, 2019, IEEE COMPUT SOC CONF, P1954, DOI 10.1109/CVPRW.2019.00247
   Wang YZ, 2023, IEEE T CIRC SYST VID, V33, P2533, DOI 10.1109/TCSVT.2022.3224940
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZQ, 2023, IEEE T MULTIMEDIA, V25, P1161, DOI 10.1109/TMM.2021.3139743
   X. Foundation, 2016, ABOUT US
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yi P, 2019, IEEE I CONF COMP VIS, P3106, DOI 10.1109/ICCV.2019.00320
   Zhang CX, 2021, IEEE T MULTIMEDIA, V24, P3340, DOI 10.1109/TMM.2021.3096083
   Zhao Y, 2021, IEEE T CIRC SYST VID, V31, P2063, DOI 10.1109/TCSVT.2020.2982505
   Zhao Y, 2019, IEEE T IMAGE PROCESS, V28, P2847, DOI 10.1109/TIP.2019.2891131
NR 44
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2444
EP 2455
DI 10.1109/TMM.2023.3296225
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100004
DA 2024-08-05
ER

PT J
AU Nai, K
   Chen, SM
AF Nai, Ke
   Chen, Shaomiao
TI Learning a Novel Ensemble Tracker for Robust Visual Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object tracking; ensemble learning; historical tracker snapshots;
   tracker snapshot verification scheme; online weight assign algorithm
ID ONLINE OBJECT TRACKING; CORRELATION FILTERS; FRAMEWORK
AB In this article, we propose a novel historical snapshot-based ensemble tracker (HSET) to address visual object tracking. Specifically, our HSET tracker collects multiple historical tracker snapshots to model various appearance patterns of the target object during tracking, and performs ensemble operations based on these tracker snapshots to successfully detect the target object. To obtain diverse and representative tracker snapshots for ensemble tracking, we design a tracker snapshot verification scheme to handle dynamical appearance variations of the target object and alleviate unreliable tracker snapshots. Furthermore, the weights of different tracker snapshots are given by an online weight assign algorithm with consideration of both historical appearance information and recent appearance information of the target object. By employing ensemble learning and historical tracker snapshots, the proposed HSET method can get impressive generalization power and tracking robustness to handle significant appearance changes and model drift. Extensive experimental results on public tracking benchmarks indicate that the proposed HSET tracking algorithm reaches encouraging tracking performance compared to multiple state-of-the-art tracking algorithms.
C1 [Nai, Ke] Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha, Peoples R China.
   [Nai, Ke] Hunan Univ, Sch Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Chen, Shaomiao] Hunan Univ Sci & Technol, Sch Comp Sci & Engn, Xiangtan 411201, Peoples R China.
C3 Changsha University of Science & Technology; Hunan University; Hunan
   University of Science & Technology
RP Nai, K (corresponding author), Changsha Univ Sci & Technol, Sch Comp & Commun Engn, Changsha, Peoples R China.
EM naike_hnu@hnu.edu.cn; csm123@hnust.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2006, BMVC06
   Avidan S, 2007, IEEE T PATTERN ANAL, V29, P261, DOI 10.1109/TPAMI.2007.35
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Bingyan Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P429, DOI 10.1007/978-3-030-58542-6_26
   Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P621, DOI 10.1109/ICCVW.2015.84
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.129, 10.1109/ICCV.2017.128]
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gundogdu E, 2017, IEEE T IMAGE PROCESS, V26, P5270, DOI 10.1109/TIP.2017.2733199
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hu WM, 2023, IEEE T PATTERN ANAL, V45, P3072, DOI 10.1109/TPAMI.2022.3172932
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Javed S, 2022, IEEE T CYBERNETICS, V52, P12259, DOI 10.1109/TCYB.2021.3086194
   Kristan M, 2019, LECT NOTES COMPUT SC, V11129, P3, DOI 10.1007/978-3-030-11009-3_1
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li JT, 2017, IEEE T IMAGE PROCESS, V26, P2736, DOI 10.1109/TIP.2017.2686601
   Li MH, 2021, IEEE T MULTIMEDIA, V23, P105, DOI 10.1109/TMM.2020.2978623
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li SJ, 2023, IEEE T IMAGE PROCESS, V32, P750, DOI 10.1109/TIP.2022.3232941
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nai K, 2022, PATTERN RECOGN, V130, DOI 10.1016/j.patcog.2022.108775
   Nai K, 2018, IEEE T IMAGE PROCESS, V27, P4958, DOI 10.1109/TIP.2018.2848465
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Ning JF, 2016, PROC CVPR IEEE, P4266, DOI 10.1109/CVPR.2016.462
   Qi YK, 2019, IEEE T PATTERN ANAL, V41, P1116, DOI 10.1109/TPAMI.2018.2828817
   Shen QH, 2022, PROC CVPR IEEE, P8091, DOI 10.1109/CVPR52688.2022.00793
   Shuai Jia, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P69, DOI 10.1007/978-3-030-58529-7_5
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Tang F, 2022, PROC CVPR IEEE, P8731, DOI 10.1109/CVPR52688.2022.00854
   Uzkent B, 2018, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2018.00129
   Wang HD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3533253
   Wang N, 2021, INT J COMPUT VISION, V129, P400, DOI 10.1007/s11263-020-01357-4
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang W, 2018, IEEE T CIRC SYST VID, V28, P1609, DOI 10.1109/TCSVT.2017.2684759
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yin YJ, 2015, IEEE T CYBERNETICS, V45, P1988, DOI 10.1109/TCYB.2014.2363078
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhang ZY, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108003
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhu XF, 2022, IEEE T MULTIMEDIA, V24, P301, DOI 10.1109/TMM.2021.3050073
   Zhu XF, 2021, IEEE T CIRC SYST VID, V31, P557, DOI 10.1109/TCSVT.2020.2979480
NR 77
TC 0
Z9 0
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3194
EP 3206
DI 10.1109/TMM.2023.3307939
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200034
DA 2024-08-05
ER

PT J
AU Peng, B
   Lin, GT
   Lei, JJ
   Qin, TY
   Cao, XC
   Ling, NM
AF Peng, Bo
   Lin, Guoting
   Lei, Jianjun
   Qin, Tianyi
   Cao, Xiaochun
   Ling, Nam
TI Contrastive Multi-View Learning for 3D Shape Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D shape clustering; multi-view learning; contrastive learning; graph
   construction
AB Unsupervised 3D shape clustering is emerging as a promising research topic in multimedia and computer vision field. Considering the flexibility of acquiring multiple views for 3D shapes, this paper proposes a contrastive multi-view learning network (CMVL-Net) to cluster unlabeled 3D shapes from multiple views. To the best of our knowledge, this is the first multi-view-oriented 3D shape deep clustering method. The key to this method lies in how to capture highly discriminative 3D shape features suitable for clustering. By exploring consistency and complementarity among multiple views, a cross-view contrastive clustering mechanism is proposed to learn clustering-specified discriminative 3D shape features. To obtain a more compact 3D shape clustering structure, a consensus graph-guided contrastive constraint is designed to encourage cluster-wise consistency learning under the guidance of potential category associations among shapes. Experimental results on two widely used benchmark datasets demonstrate the effectiveness of the proposed method.
C1 [Peng, Bo; Lin, Guoting; Lei, Jianjun; Qin, Tianyi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Cao, Xiaochun] Sun Yat Sen Univ, Sch Cyber Sci & Technol, Shenzhen Campus, Shenzhen 518107, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Sci & Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; Sun Yat Sen University; Santa Clara University
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM bpeng@tju.edu.cn; gtlin@tju.edu.cn; jjlei@tju.edu.cn; tyqin@tju.edu.cn;
   caoxiaochun@mail.sysu.edu.cn; nling@scu.edu
OI Peng, Bo/0000-0002-6616-453X; Qin, Tianyi/0000-0002-9322-7321
FU National Natural Science Foundation of China
FX No Statement Available
CR Abavisani M, 2018, IEEE J-STSP, V12, P1601, DOI 10.1109/JSTSP.2018.2875385
   Achlioptas P, 2018, PR MACH LEARN RES, V80
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P4381, DOI 10.1109/TIP.2015.2463223
   Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9
   Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI [10.1109/ICCV.2017.627, 10.1109/ICCV.2017.626]
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Fogel S, 2019, IEEE COMPUT GRAPH, V39, P16, DOI 10.1109/MCG.2018.2881524
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han ZZ, 2019, AAAI CONF ARTIF INTE, P8376
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong XT, 2020, IEEE T VIS COMPUT GR, V26, P1442, DOI 10.1109/TVCG.2018.2873724
   Huang J., 2020, P IEEE CVF C COMP VI, P8846, DOI DOI 10.1109/CVPR42600.2020.00887
   Huang ZY, 2021, IEEE T IMAGE PROCESS, V30, P5352, DOI 10.1109/TIP.2021.3083072
   Ji X, 2019, IEEE I CONF COMP VIS, P9864, DOI 10.1109/ICCV.2019.00996
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Lei JJ, 2023, IEEE T IND INFORM, V19, P7377, DOI 10.1109/TII.2022.3210589
   Li XL, 2022, IEEE T PATTERN ANAL, V44, P9725, DOI 10.1109/TPAMI.2021.3125687
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Li ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2952
   Lin GT, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12157384
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P1744, DOI 10.1109/TIP.2020.3048623
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Mei GF, 2022, Arxiv, DOI arXiv:2202.02543
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025
   Pielawski N., 2020, Advances in neural information processing systems, Vvol 33, P18433
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Runwu Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14607, DOI 10.1109/CVPR42600.2020.01463
   Sanghi Aditya, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P626, DOI 10.1007/978-3-030-58526-6_37
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tan QY, 2018, PROC CVPR IEEE, P5841, DOI 10.1109/CVPR.2018.00612
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Trosten DJ, 2021, PROC CVPR IEEE, P1255, DOI 10.1109/CVPR46437.2021.00131
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wei X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P397, DOI 10.1109/ICCV48922.2021.00046
   Wen C., 2023, IEEE Trans. Multimedia, DOI [10.1109/TMM.2023.3265177, DOI 10.1109/TMM.2023.3265177]
   Wu JJ, 2016, ADV NEUR IN, V29
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xu YZ, 2020, IEEE T MULTIMEDIA, V22, P2950, DOI 10.1109/TMM.2020.2966882
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang MX, 2023, IEEE T PATTERN ANAL, V45, P1055, DOI 10.1109/TPAMI.2022.3155499
   Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119
   Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029
   Yang Z, 2019, IEEE I CONF COMP VIS, P7504, DOI 10.1109/ICCV.2019.00760
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Zhang CQ, 2019, PROC CVPR IEEE, P2572, DOI 10.1109/CVPR.2019.00268
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhuang CX, 2019, IEEE I CONF COMP VIS, P6001, DOI 10.1109/ICCV.2019.00610
NR 55
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6262
EP 6272
DI 10.1109/TMM.2023.3347842
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600027
DA 2024-08-05
ER

PT J
AU Ren, HW
   Wang, SS
   Ma, SW
   Gao, W
AF Ren, Huiwen
   Wang, Shanshe
   Ma, Siwei
   Gao, Wen
TI SVT-AVS3: An Open-Source High-Performance AVS3 Encoder With Scalable
   Video Technology
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IEEE 1857.10 (AVS3); scalable video technology; ultrahigh-definition;
   video compression
AB Advanced compression technologies have succeeded in exploring compact representations of intricate video contents, but their dramatic increase in computational complexity causes severe challenges when deploying new-generation video codecs. In this article, we revisited the essential compression characteristics of IEEE1857.10/AVS3 from the perspective of reducing the computational cost of different application scenarios as well as its merits in terms of accommodating parallel implementation in high-performance multicore computing platforms by leveraging the scalable video technology (SVT) architecture. A hybrid acceleration scheme is constructed to extract texture and contextual information for pruning massive encoding candidates, while the visual quality of the reconstructed images is well considered in terms of adaptive coding budgets. Furthermore, we have carefully studied the compression performance of existing AVS3 coding tools within different technical parameters and rate-distortion granularity, where the parametric encapsulation and resource scheduling derived a series of preset configurations capable of offering satisfactory trade-offs for various applications. Therefore, we propose the first and fastest AVS3 standard-compliant encoder with the capability of real-time processing video signals up to 8 K resolution, which may hopefully further benefit the emerging 8K-UHD video industry.
C1 [Ren, Huiwen; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University
RP Wang, SS (corresponding author), Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
EM hwren@pku.edu.cn; sswang@pku.edu.cn; swma@pku.edu.cn; wgao@pku.edu.cn
OI Ren, Huiwen/0000-0002-9229-1874
FU National Key Research and Development Project
FX No Statement Available
CR [Anonymous], Intel intrinsics guide
   Bartnik C., 2020, JVET-DocQ 0791
   Bjontegaard G., 2001, ITU SG16 Doc. VCEG-M33
   Cai R., 2021, P IEEE INT C MULT EX, P1
   Chen Y. Zhao, 2019, AVS-Doc, VM4643
   de Souza DF, 2017, IEEE T MULTIMEDIA, V19, P459, DOI 10.1109/TMM.2016.2625261
   Fan K., 2019, AVS-Doc. N2654
   Fan K, 2017, IEEE T CIRC SYST VID, V27, P2726, DOI 10.1109/TCSVT.2016.2595327
   FFmpeg,, FFmpeg release 4.4
   Han X., 2020, IEEE INT CONF MULTI, P1, DOI [10.1109/ICMEW46912.2020.9106009, DOI 10.1109/icmew46912.2020.9106009]
   hhi, 8 K Berlin test sequences
   Intel, Scalable video technology
   Lei M, 2019, IEEE IMAGE PROC, P4120, DOI [10.1109/icip.2019.8803421, 10.1109/ICIP.2019.8803421]
   Li J., 2018, AVS-Doc M4632
   Li J., 2019, AVS-Doc M4726
   Li J., 2018, AVS-Doc, M4488
   Lin H. Y., 2018, P IEEE INT C CONS EL, P1
   Lin K, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954561
   Lu H., 2018, AVS-Doc M4451
   Lu X, 2018, AVS-Doc M4451
   Luo FL, 2019, IEEE T MULTIMEDIA, V21, P851, DOI 10.1109/TMM.2018.2867260
   Ouyang F., 2018, AVS-Doc, VM4512
   Parois R, 2016, CONF DESIGN ARCHIT, P235, DOI 10.1109/DASIP.2016.7853830
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   VideoLAN,, x265 HEVCencoder
   Wang L., 2018, AVS-Doc M4409
   Wang M, 2019, PICT COD SYMP, DOI 10.1109/pcs48520.2019.8954510
   Wang X., 2018, AVS-Doc, VM4466
   Wang Z, 2018, IEEE IMAGE PROC, P2550, DOI 10.1109/ICIP.2018.8451258
   Wang Z, 2018, IEEE T IMAGE PROCESS, V27, P1475, DOI 10.1109/TIP.2017.2778564
   Wang Z, 2017, IEEE DATA COMPR CONF, P23, DOI 10.1109/DCC.2017.70
   Xu G., 2018, AVS-Doc M4609
   Xu K., 2018, AVS-Doc M4609
   Zhao L, 2019, IEEE T IMAGE PROCESS, V28, P4832, DOI 10.1109/TIP.2019.2913545
   Zhao X, 2020, PROC SPIE, V11510, DOI 10.1117/12.2570003
NR 35
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3291
EP 3301
DI 10.1109/TMM.2023.3309549
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200027
DA 2024-08-05
ER

PT J
AU Sun, LD
   Zhang, K
   Zhang, F
   Wan, WB
   Sun, JD
AF Sun, Ludan
   Zhang, Kai
   Zhang, Feng
   Wan, Wenbo
   Sun, Jiande
TI Deep Rank-<i>N</i> Decomposition Network for Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Task analysis; Image reconstruction;
   Transforms; Sun; Transformers; low-rank priors; rank-N decomposition;
   rank-1 component; decomposition network
ID GENERATIVE ADVERSARIAL NETWORK; FRAMEWORK; ENHANCEMENT
AB Existing deep neural network (DNN)-based image fusion methods seldom consider low-rank priors for the decomposition of source images, which cannot efficiently model base and detail components in images. To exploit the low-rank priors better, we propose a deep rank-N decomposition network (DRDec-Net) according to the rank-N decomposition of source images. Specifically, a rank-N decomposition model is first established by imposing low-rank priors on the base component of source images. Then, based on the decomposition model, we construct DRDec-Net, which is composed of low-rank decomposition (LRD) modules, a detail fusion (DetailF) module, and a low-rank fusion (LRF) module. In DRDec-Net, it is assumed that source images share the same base component, which is expressed as the sum of rank-1 components. We employ N cascaded LRD modules to extract these rank-1 components from source images. Meanwhile, detail components are obtained by subtracting the base component from source images. Next, the extracted rank-1 components and detail components are integrated by LRF and DetailF modules to produce the base component and detail component of the fused image. Finally, the sum of the two obtained components is regarded as the fused image. Compared to some state-of-the-art methods, experimental results demonstrate that the proposed DRDec-Net can produce a better performance on three image fusion tasks, including infrared and visible images, multi-exposure images, and multi-focus images.
C1 [Sun, Ludan; Zhang, Kai; Wan, Wenbo; Sun, Jiande] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Zhang, Feng] Univ Jinan, Sch Informat Sci & Engn, Jinan 250022, Peoples R China.
C3 Shandong Normal University; University of Jinan
RP Zhang, K (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM sld78658326@163.com; zhangkainuc@163.com; fengzhangpl@163.com;
   wanwenbo@sdnu.edu.cn; jiandesun@hotmail.com
FU Natural Science Foundation of Shandong Province of China
FX No Statement Available
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bai XZ, 2011, OPT EXPRESS, V19, P8444, DOI 10.1364/OE.19.008444
   Du J, 2016, NEUROCOMPUTING, V194, P326, DOI 10.1016/j.neucom.2016.02.047
   Gao SQ, 2022, IEEE T PATTERN ANAL, V44, P3224, DOI 10.1109/TPAMI.2020.3046476
   Haghighat M, 2014, I C APPL INF COMM TE, P424
   He KJ, 2023, IEEE T MULTIMEDIA, V25, P4943, DOI 10.1109/TMM.2022.3185887
   Hong YC, 2023, IEEE T MULTIMEDIA, V25, P7101, DOI 10.1109/TMM.2022.3217446
   Jian LH, 2021, IEEE T MULTIMEDIA, V24, P3314, DOI 10.1109/TMM.2021.3096088
   Jung H, 2020, IEEE T IMAGE PROCESS, V29, P3845, DOI 10.1109/TIP.2020.2966075
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li J, 2018, INFRARED PHYS TECHN, V89, P129, DOI 10.1016/j.infrared.2018.01.003
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liang PW, 2022, LECT NOTES COMPUT SC, V13678, P719, DOI 10.1007/978-3-031-19797-0_41
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776
   Liu Y, 2015, J VIS COMMUN IMAGE R, V31, P208, DOI 10.1016/j.jvcir.2015.06.021
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Y, 2015, INFORM FUSION, V23, P139, DOI 10.1016/j.inffus.2014.05.004
   Luo XQ, 2023, IEEE T MULTIMEDIA, V25, P608, DOI 10.1109/TMM.2021.3129354
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Paul S, 2016, J CIRCUIT SYST COMP, V25, DOI 10.1142/S0218126616501231
   Petrovic V, 2005, IEEE I CONF COMP VIS, P1866
   Petrovic VS, 2004, IEEE T IMAGE PROCESS, V13, P228, DOI 10.1109/tip.2004.823821
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qiang Zhang, 2021, Pattern Recognition, V113, DOI 10.1016/j.patcog.2020.107752
   Rao Dongyu, 2023, IEEE Trans Image Process, VPP, DOI 10.1109/TIP.2023.3273451
   Rao YJ, 2023, INFORM FUSION, V92, P336, DOI 10.1016/j.inffus.2022.12.007
   Tang W, 2023, IEEE T MULTIMEDIA, V25, P5413, DOI 10.1109/TMM.2022.3192661
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Wu KL, 2023, IEEE T MULTIMEDIA, V25, P5690, DOI 10.1109/TMM.2022.3198327
   Xu H, 2022, COMPUT VIS IMAGE UND, V218, DOI 10.1016/j.cviu.2022.103407
   Xu H, 2020, AAAI CONF ARTIF INTE, V34, P12484
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang SY, 2010, INFORM FUSION, V11, P78, DOI 10.1016/j.inffus.2009.05.001
   Yin HT, 2015, NEUROCOMPUTING, V148, P600, DOI 10.1016/j.neucom.2014.07.003
   Yue J., 2023, Dif-fusion: Towards highcolor fidelity in infrared and visible image fusion with diffusion models
   Zhang H, 2021, IEEE T COMPUT IMAG, V7, P1134, DOI 10.1109/TCI.2021.3119954
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2021, INT J COMPUT VISION, V129, P2761, DOI 10.1007/s11263-021-01501-8
   Zhang H, 2021, INFORM FUSION, V66, P40, DOI 10.1016/j.inffus.2020.08.022
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang K, 2023, INFORM FUSION, V93, P227, DOI 10.1016/j.inffus.2022.12.026
   Zhang Q, 2016, IEEE T IMAGE PROCESS, V25, P2045, DOI 10.1109/TIP.2016.2524212
   Zhang XC, 2021, INFORM FUSION, V74, P111, DOI 10.1016/j.inffus.2021.02.005
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhao ZX, 2023, IEEE I CONF COMP VIS, P8048, DOI 10.1109/ICCV51070.2023.00742
   Zhao ZX, 2023, PROC CVPR IEEE, P5906, DOI 10.1109/CVPR52729.2023.00572
   Zhao ZX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P970
   Zhou HB, 2023, IEEE T MULTIMEDIA, V25, P635, DOI 10.1109/TMM.2021.3129609
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
NR 58
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7335
EP 7348
DI 10.1109/TMM.2024.3366150
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000071
DA 2024-08-05
ER

PT J
AU Sun, Y
   Xu, LB
   Bao, Q
   Liu, W
   Gao, WP
   Fu, YL
AF Sun, Yu
   Xu, Lubing
   Bao, Qian
   Liu, Wu
   Gao, Wenpeng
   Fu, Yili
TI Learning Monocular Regression of 3D People in Crowds via Scene-Aware
   Blending and De-Occlusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human in Occlusion; 3D Human Mesh Recovery; Image Blending; De-occlusion
ID POSE
AB In this study, we address the challenge of estimating 3D body pose, shape, and depth relationships from single RGB images in crowded scenes. The difficulty lies in the limited availability of in-the-wild training samples, which feature densely populated scenes. To mitigate this issue, we introduce a synthesis-based approach that fuses multiple human samples into a single composite scene. Our innovative scene-aware blending technique maintains human-scene consistency by positioning individuals within plausible locations and adjusting their scales to conform to 3D settings. Furthermore, our method enables flexible per-subject occlusion management during the blending process, bolstering the robustness of 3D human body representations through a novel de-occlusion training scheme. We present a one-stage model, CBD, designed to learn monocular regression of 3D people in crowds by leveraging blending and de-occlusion techniques. Our quantitative and qualitative evaluations on four benchmark datasets reveal that CBD surpasses existing state-of-the-art approaches in terms of 3D human pose and mesh regression accuracy, thereby establishing it as a promising solution for monocular 3D human mesh recovery in densely populated scenes.
C1 [Sun, Yu; Xu, Lubing; Gao, Wenpeng; Fu, Yili] Harbin Inst Technol, Harbin 150001, Peoples R China.
   [Bao, Qian; Liu, Wu] Explore Acad JDcom, Beijing 100176, Peoples R China.
C3 Harbin Institute of Technology
RP Fu, YL (corresponding author), Harbin Inst Technol, Harbin 150001, Peoples R China.
EM yusunhit@gmail.com; xlubing952@gmail.com; baoqian@jd.com; liuwu1@jd.com;
   wpgao@hit.edu.cn; meylfu@hit.edu.cn
RI Sun, Yu/HNB-9860-2023
OI Sun, Yu/0000-0003-3525-2753; gao, wenpeng/0000-0001-7771-0084; fu, yi
   li/0000-0003-2075-2384
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Choi H, 2022, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR52688.2022.00153
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Ghafoor M, 2023, IEEE T MULTIMEDIA, V25, P3311, DOI 10.1109/TMM.2022.3158068
   Hesse N, 2018, LECT NOTES COMPUT SC, V11070, P792, DOI 10.1007/978-3-030-00928-1_89
   Hua GL, 2023, IEEE T MULTIMEDIA, V25, P1832, DOI 10.1109/TMM.2022.3171102
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jiang W, 2020, PROC CVPR IEEE, P5578, DOI 10.1109/CVPR42600.2020.00562
   Johnson S., 2010, BMVC, V2, P5, DOI DOI 10.5244/C.24.12
   Joo H, 2021, INT CONF 3D VISION, P42, DOI 10.1109/3DV53792.2021.00015
   Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Khirodkar R, 2022, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR52688.2022.00176
   Kocabas M, 2020, PROC CVPR IEEE, P5252, DOI 10.1109/CVPR42600.2020.00530
   Kolotouros N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11585, DOI 10.1109/ICCV48922.2021.01140
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Lin SC, 2021, PROC CVPR IEEE, P8758, DOI 10.1109/CVPR46437.2021.00865
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Ling J, 2021, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR46437.2021.00924
   Liu Q. Bao, 2022, ACM Comput.Surv., V55, P1
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Lu Y, 2021, IEEE T MULTIMEDIA, V23, P3657, DOI 10.1109/TMM.2020.3029941
   M. Contributors, 2020, MM Segmentation: Openmmlab semantic segmentation toolbox and benchmark
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Miangoleh SMH, 2021, PROC CVPR IEEE, P9680, DOI 10.1109/CVPR46437.2021.00956
   Moon G, 2019, IEEE I CONF COMP VIS, P10132, DOI 10.1109/ICCV.2019.01023
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Patel P, 2021, PROC CVPR IEEE, P13463, DOI 10.1109/CVPR46437.2021.01326
   Pavlakos G, 2019, IEEE I CONF COMP VIS, P803, DOI 10.1109/ICCV.2019.00089
   Sun Y, 2023, PROC CVPR IEEE, P8856, DOI 10.1109/CVPR52729.2023.00855
   Sun Y, 2022, PROC CVPR IEEE, P13233, DOI 10.1109/CVPR52688.2022.01289
   Sun Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11159, DOI 10.1109/ICCV48922.2021.01099
   Sun Y, 2019, IEEE I CONF COMP VIS, P5348, DOI 10.1109/ICCV.2019.00545
   Tianshu Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7374, DOI 10.1109/CVPR42600.2020.00740
   von Marcard T, 2018, LECT NOTES COMPUT SC, V11214, P614, DOI 10.1007/978-3-030-01249-6_37
   Wu JH, 2019, IEEE INT CON MULTI, P1480, DOI 10.1109/ICME.2019.00256
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xu ZB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15303, DOI 10.1109/ICCV48922.2021.01504
   Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229
   Zanfir E., 2018, INPROC ADV NEURAL IN, P5253
   Zhang HR, 2023, IEEE T MULTIMEDIA, V25, P3868, DOI 10.1109/TMM.2022.3167887
   Zhang HW, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11426, DOI 10.1109/ICCV48922.2021.01125
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
   Zhen J., 2020, P EUR C COMP VIS, P550, DOI 10.1007/978-3030-58555-6
   Zhou XY, 2019, Arxiv, DOI arXiv:1904.07850
   Zhou Y, 2019, PROC CVPR IEEE, P5738, DOI 10.1109/CVPR.2019.00589
NR 50
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2289
EP 2302
DI 10.1109/TMM.2023.3294820
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100009
DA 2024-08-05
ER

PT J
AU Tan, XB
   Wang, SY
   Xu, X
   Zheng, Q
   Yang, J
   Chen, SW
AF Tan, Xiaobin
   Wang, Shunyi
   Xu, Xiang
   Zheng, Quan
   Yang, Jian
   Chen, Shuangwu
TI DACOD360: Deadline-Aware Content Delivery for 360-Degree Video Streaming
   Over MEC Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Bandwidth; Quality of experience; Servers; Bit rate;
   Resource management; Prefetching; 360-degree video; quality of
   experience (QoE); mobile edge computing (MEC); deep reinforcement
   learning (DRL); cooperative bargaining game (CBG)
ID LOW-LATENCY; CHALLENGES; ALLOCATION
AB The proliferation of 360-degree video applications has brought significant challenges to existing networks. To meet the requirements of high transmission rate, low interaction latency, and high reliability, Mobile Edge Computing (MEC) has emerged as a promising technology that enables caching and processing at network edges. In this article, we present DACOD360, a deadline-aware content delivery system for the 360-degree video streaming over MEC networks. To address the challenges such as unpredictable viewports, uneven cached tiles, concurrent requests, and dynamic bandwidth, we formulate the deadline-aware delivery problem as a long-term integer program model to maximize the Quality of Experience (QoE) under the constraints of network bandwidth, cache capacity, and deadline. This optimization problem is a complex sequential decision that considers both deadline-constrained service quality at the temporal scale and multi-user resource allocation at the spatial scale. To solve it, we decompose the original problem into two sub-problems and solve them iteratively using Deep Reinforcement Learning (DRL) and Cooperative Bargaining Game (CBG). Comprehensive experiments are conducted in a wide variety of environments, and the results demonstrate that our proposed scheme outperforms the state-of-the-art schemes in terms of long-term QoE, traffic reduction, and other metrics.
C1 [Tan, Xiaobin; Wang, Shunyi; Xu, Xiang; Zheng, Quan; Yang, Jian; Chen, Shuangwu] Univ Sci & Technol China USTC, Dept Automat, Hefei 230026, Peoples R China.
   [Tan, Xiaobin; Zheng, Quan; Yang, Jian; Chen, Shuangwu] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230031, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zheng, Q (corresponding author), Univ Sci & Technol China USTC, Dept Automat, Hefei 230026, Peoples R China.; Zheng, Q (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230031, Peoples R China.
EM xbtan@ustc.edu.cn; wsy12@mail.ustc.edu.cn; xux20@mail.ustc.edu.cn;
   qzheng@ustc.edu.cn; jianyang@ustc.edu.cn; chensw@ustc.edu.cn
OI Yang, Jian/0000-0002-7329-4738; Tan, Xiaobin/0000-0001-7489-2839
FU National Key Ramp;D Program of China
FX No Statement Available
CR Bastug E, 2017, IEEE COMMUN MAG, V55, P110, DOI 10.1109/MCOM.2017.1601089
   Breslau L, 1999, IEEE INFOCOM SER, P126, DOI 10.1109/INFCOM.1999.749260
   Carlsson N, 2017, IEEE T MULTIMEDIA, V19, P1637, DOI 10.1109/TMM.2017.2673412
   Chakareski J, 2020, IEEE T IMAGE PROCESS, V29, P6330, DOI 10.1109/TIP.2020.2986547
   Chen MZ, 2017, IEEE T WIREL COMMUN, V16, P3520, DOI 10.1109/TWC.2017.2683482
   Colman-Meixner C, 2019, IEEE T BROADCAST, V65, P392, DOI 10.1109/TBC.2019.2901387
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Dai JM, 2020, IEEE T CIRC SYST VID, V30, P3843, DOI 10.1109/TCSVT.2019.2946755
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Hou XS, 2021, IEEE T MULTIMEDIA, V23, P716, DOI 10.1109/TMM.2020.2987693
   Hu YC, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/413203
   Huawei iLab, 2018, Cloud VR network solution whitepaper
   Kelly F, 1997, EUR T TELECOMMUN, V8, P33, DOI 10.1002/ett.4460080106
   Laboratory for Future Networks USTC, 2022, China environment for network innovations (Ceni-Hefei)
   Liu RS, 2010, IEEE INFOCOM SER
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Madarasingha C, 2022, INT CONF PERVAS COMP, P34, DOI 10.1109/PerCom53586.2022.9762386
   Mahzari A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P173, DOI 10.1145/3240508.3240680
   Maniotis P, 2022, IEEE T MULTIMEDIA, V24, P386, DOI 10.1109/TMM.2021.3052339
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Messiahdas R., 2021, Augmented reality & virtual reality is now a reality for enterprises-the future is here
   Mi D, 2020, IEEE T BROADCAST, V66, P555, DOI 10.1109/TBC.2020.2977546
   Mnih V, 2016, PR MACH LEARN RES, V48
   Nikbakht R, 2022, 2022 IEEE CONFERENCE ON STANDARDS FOR COMMUNICATIONS AND NETWORKING, CSCN, P208, DOI 10.1109/CSCN57023.2022.10051020
   Pang HT, 2019, IEEE INFOCOM SER, P991, DOI 10.1109/INFOCOM.2019.8737395
   Park H, 2007, IEEE T SIGNAL PROCES, V55, P3496, DOI 10.1109/TSP.2007.893755
   Perfecto C, 2020, IEEE T COMMUN, V68, P2491, DOI 10.1109/TCOMM.2020.2965527
   Qian F., 2016, P 5 WORKSH ALL THING, P1, DOI DOI 10.1145/2980055.2980056
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Qu ZH, 2020, IEEE T MOBILE COMPUT, V19, P288, DOI 10.1109/TMC.2019.2893917
   Riiser H., 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Tang LP, 2017, 2017 USENIX ANNUAL TECHNICAL CONFERENCE (USENIX ATC '17), P111
   Tekbiyik N, 2013, IEEE T WIREL COMMUN, V12, P1699, DOI 10.1109/TWC.2013.021213.120523
   Tran TX, 2018, IEEE T MOBILE COMPUT, V17, P2729, DOI 10.1109/TMC.2018.2818723
   Tran TX, 2017, IEEE NETWORK, V31, P35, DOI 10.1109/MNET.2017.1600307
   Tran TX, 2017, IEEE COMMUN MAG, V55, P54, DOI 10.1109/MCOM.2017.1600863
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wei XK, 2022, IEEE T MOBILE COMPUT, V21, P3428, DOI 10.1109/TMC.2021.3058099
   Xiao H, 2022, IEEE J SEL AREA COMM, V40, P1615, DOI 10.1109/JSAC.2022.3145813
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yadav PK, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3724, DOI 10.1145/3394171.3413550
   Yaqoob A, 2020, IEEE COMMUN SURV TUT, V22, P2801, DOI 10.1109/COMST.2020.3006999
   Yuan H, 2018, IEEE T MULTIMEDIA, V20, P183, DOI 10.1109/TMM.2017.2724850
   Zhang R, 2022, IEEE J SEL AREA COMM, V40, P694, DOI 10.1109/JSAC.2021.3119144
   Zhang YX, 2021, IEEE T MOBILE COMPUT, V20, P2338, DOI 10.1109/TMC.2020.2978187
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhong LJ, 2023, IEEE T MOBILE COMPUT, V22, P4405, DOI 10.1109/TMC.2022.3162147
NR 50
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4168
EP 4182
DI 10.1109/TMM.2023.3321439
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100053
DA 2024-08-05
ER

PT J
AU Wang, JH
   Wei, YY
   Zhang, Z
   Fan, JC
   Zhao, Y
   Yang, Y
   Wang, M
AF Wang, Junhu
   Wei, Yanyan
   Zhang, Zhao
   Fan, Jicong
   Zhao, Yang
   Yang, Yi
   Wang, Meng
TI Progressive Stereo Image Dehazing Network via Cross-View Region
   Interaction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image restoration; Task analysis; Feature extraction; Cameras; Decoding;
   Correlation; Image color analysis; Cross-view region interaction;
   progressive; stereo image dehazing
ID VISION
AB Stereo image dehazing aims to restore haze-free images by leveraging the complementary information contained in binocular images. Current methods primarily focus on designing image-level modules and pipelines to utilize complementary information between the left and right-view images. However, these image-level cross-view interactions overlook regional differences in haze concentration and stereo image disparity maps. Consequently, we propose a Progressive Stereo Image Dehazing Network via Cross-view Region Interaction, termed PSIDNet, which fully considers the internal characteristics and external manifestation of haze and disparity, and explicitly addresses the stereo image dehazing task by a regional-aware interactive mechanism. Specifically, we divide hazy images into regions and independently interact with left and right-view information at region levels, meaning weights are not shared across regional patches. This approach allows us to treat different regions with different priorities, i.e., concentrate on regional patches with heavier haze concentration and larger disparities, hence enabling more accurate restoration of hazy images. Furthermore, we introduce an effective cross-view region interactive block that extracts information based on the channel dimension of dual views and later adopts matrix multiplication to generate mutual attention maps based on the fused features. Extensive experiments on synthetic and real-scenario datasets demonstrate the efficacy of our method, compared to other related monocular and stereo image dehazing and restoration methods.
C1 [Wang, Junhu; Wei, Yanyan; Zhang, Zhao; Zhao, Yang; Wang, Meng] Hefei Univ Technol, Sch Comp Sci & Informat Engn, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230009, Peoples R China.
   [Fan, Jicong] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.
   [Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW 2007, Australia.
C3 Hefei University of Technology; The Chinese University of Hong Kong,
   Shenzhen; University of Technology Sydney
RP Zhang, Z; Zhao, Y (corresponding author), Hefei Univ Technol, Sch Comp Sci & Informat Engn, Key Lab Knowledge Engn Big Data, Minist Educ, Hefei 230009, Peoples R China.
EM wangjunhu0610@outlook.com; weiyy@hfut.edu.cn; cszzhang@gmail.com;
   fanjicong@cuhk.edu.cn; yzhao@hfut.edu.cn; yi.yang@uts.edu.au;
   eric.mengwang@gmail.com
OI Wei, Yanyan/0000-0001-8818-6740
FU National Natural Science Foundation of China
FX No Statement Available
CR Ali U, 2023, PATTERN RECOGN, V140, DOI 10.1016/j.patcog.2023.109522
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen DD, 2019, IEEE WINT CONF APPL, P1375, DOI 10.1109/WACV.2019.00151
   Chu XJ, 2022, IEEE COMPUT SOC CONF, P1238, DOI 10.1109/CVPRW56347.2022.00130
   Dong Y, 2020, AAAI CONF ARTIF INTE, V34, P10729
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Gao YY, 2019, IEEE T MULTIMEDIA, V21, P351, DOI 10.1109/TMM.2018.2856095
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Guo CL, 2022, PROC CVPR IEEE, P5802, DOI 10.1109/CVPR52688.2022.00572
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang J, 2023, IEEE T MULTIMEDIA, V25, P2978, DOI 10.1109/TMM.2022.3154152
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jiangxin Dong, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P188, DOI 10.1007/978-3-030-58577-8_12
   Jung E., 2020, C ROBOT LEARNING, P651
   Kaihao Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P71, DOI 10.1007/978-3-030-58583-9_5
   Kirillov A, 2023, Arxiv, DOI arXiv:2304.02643
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Lin CY, 2023, IEEE T MULTIMEDIA, V25, P3089, DOI 10.1109/TMM.2022.3155937
   Liu H, 2022, PROC CVPR IEEE, P5821, DOI 10.1109/CVPR52688.2022.00574
   Liu SL, 2023, Arxiv, DOI arXiv:2303.05499
   McCartney E. J., 1976, Optics of the atmosphere. Scattering by molecules and particles
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Nie J, 2022, IEEE T CIRC SYST VID, V32, P3334, DOI 10.1109/TCSVT.2021.3105685
   Pang YW, 2020, PROC CVPR IEEE, P5930, DOI 10.1109/CVPR42600.2020.00597
   Qili Deng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P722, DOI 10.1007/978-3-030-58539-6_43
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Shen Hao, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P7, DOI 10.1145/3581783.3612299
   Shi ZF, 2021, IEEE INT C INT ROBOT, P3829, DOI 10.1109/IROS51168.2021.9636216
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Song YD, 2023, IEEE T IMAGE PROCESS, V32, P1927, DOI 10.1109/TIP.2023.3256763
   Wang YQ, 2021, IEEE COMPUT SOC CONF, P766, DOI 10.1109/CVPRW53098.2021.00086
   Wang YZ, 2023, IEEE T INTELL TRANSP, V24, P11321, DOI 10.1109/TITS.2023.3277709
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Yang GR, 2019, PROC CVPR IEEE, P899, DOI 10.1109/CVPR.2019.00099
   Yang ZH, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6377, DOI 10.1145/3503161.3548177
   Ye T, 2022, LECT NOTES COMPUT SC, V13679, P130, DOI 10.1007/978-3-031-19800-7_8
   Yu H, 2022, LECT NOTES COMPUT SC, V13679, P181, DOI 10.1007/978-3-031-19800-7_11
   Zamir SW, 2021, PROC CVPR IEEE, P14816, DOI 10.1109/CVPR46437.2021.01458
   Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613
   Zhang KH, 2022, INT J COMPUT VISION, V130, P1754, DOI 10.1007/s11263-022-01620-w
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang YF, 2017, IEEE IMAGE PROC, P3205, DOI 10.1109/ICIP.2017.8296874
   Zhang Z, 2023, PATTERN RECOGN, V143, DOI 10.1016/j.patcog.2023.109740
   Zhang Z, 2022, PROC CVPR IEEE, P1889, DOI 10.1109/CVPR52688.2022.00194
   Zhao SY, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6220, DOI 10.1145/3503161.3548113
   Zheng CJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P424, DOI [10.1145/3474085.3475181, 10.1007/978-3-030-73689-7_41]
   Zheng LR, 2023, IEEE T MULTIMEDIA, V25, P6794, DOI 10.1109/TMM.2022.3214780
   Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125
NR 53
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7490
EP 7502
DI 10.1109/TMM.2024.3368918
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000015
DA 2024-08-05
ER

PT J
AU Wang, JY
   Jiang, YQ
   Long, Y
   Sun, XY
   Pagnucco, M
   Song, Y
AF Wang, Junyan
   Jiang, Yiqi
   Long, Yang
   Sun, Xiuyu
   Pagnucco, Maurice
   Song, Yang
TI Deconfounding Causal Inference for Zero-Shot Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Training; Task analysis; Feature extraction; Visualization;
   Training data; Three-dimensional displays; Zero-shot learning; action
   recognition; causal inference
AB Zero-shot action recognition (ZSAR) aims to recognize unseen action categories in the test set without corresponding training examples. Most existing zero-shot methods follow the feature generation framework to transfer knowledge from seen action categories to model the feature distribution of unseen categories. However, due to the complexity and diversity of actions, it remains challenging to generate unseen feature distribution, especially for the cross-dataset scenario when there is a potentially larger domain shift. This article proposes a Deconfounding Ca USAl GAN (DeCalGAN) for generating unseen action video features with the following technical contributions: 1) Our model unifies compositional ZSAR with traditional visual-semantic models to incorporate local object information with global semantic information for feature generation. 2) A GAN-based architecture is proposed for causal inference and unseen distribution discovery. 3) A deconfounding module is proposed to refine representations of local objects and global semantic information confounder in the training data. Action descriptions and random object features after causal inference are then used to discover unseen distributions of novel actions in different datasets. Our extensive experiments on Cross-Dataset Zero-Shot Action Recognition (CD-ZSAR) demonstrate substantial improvement over the UCF101 and HMDB51 standard benchmarks for this problem.
C1 [Wang, Junyan; Pagnucco, Maurice; Song, Yang] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
   [Jiang, Yiqi; Sun, Xiuyu] Alibaba Grp, DAMO Acad, Hangzhou 311121, Peoples R China.
   [Long, Yang] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
C3 University of New South Wales Sydney; Alibaba Group; Durham University
RP Song, Y (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW 2052, Australia.
EM junyan.wang@unsw.edu.au; yiqi.jyq@alibaba-inc.com; yang.long@ieee.org;
   xiuyu.sxy@alibaba-inc.com; morri@unsw.edu.au; yang.song1@unsw.edu.au
RI Pagnucco, Maurice/K-7061-2014
OI Pagnucco, Maurice/0000-0001-7712-6646; Long, Yang/0000-0002-2445-6112;
   Song, Yang/0000-0003-1283-1672
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111
   Atzmon Y, 2020, ADV NEUR IN, V33
   Bishay M., 2019, ARXIV190709021
   Brattoli B, 2020, PROC CVPR IEEE, P4612, DOI 10.1109/CVPR42600.2020.00467
   Carreira J, 2019, Arxiv, DOI arXiv:1907.06987
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen SZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13618, DOI 10.1109/ICCV48922.2021.01338
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao JY, 2021, IEEE T PATTERN ANAL, V43, P3476, DOI 10.1109/TPAMI.2020.2985708
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Gong MM, 2016, PR MACH LEARN RES, V48
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim J.-H., 2016, PROC INT C LEARN REP, P1
   Kim TS, 2021, AAAI CONF ARTIF INTE, V35, P1817
   Kocaoglu M., 2018, PROC INT C LEARN REP, P1
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li JJ, 2019, PROC CVPR IEEE, P7394, DOI 10.1109/CVPR.2019.00758
   Li Y, 2018, LECT NOTES COMPUT SC, V11219, P647, DOI 10.1007/978-3-030-01267-0_38
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Lin CC, 2022, PROC CVPR IEEE, P19946, DOI 10.1109/CVPR52688.2022.01935
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Mandal D, 2019, PROC CVPR IEEE, P9977, DOI 10.1109/CVPR.2019.01022
   Mettes P, 2021, INT J COMPUT VISION, V129, P1954, DOI 10.1007/s11263-021-01454-y
   Mettes P, 2017, IEEE I CONF COMP VIS, P4453, DOI 10.1109/ICCV.2017.476
   Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047
   Naeem MF, 2021, PROC CVPR IEEE, P953, DOI 10.1109/CVPR46437.2021.00101
   Pearl J, 2000, Causality: Models, reasoning and inference, V19, DOI DOI 10.1017/CBO9780511803161
   Radford A, 2021, PR MACH LEARN RES, V139
   Rehurek R., 2011, GENSIM STAT SEMANTIC
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152
   Rubin DB, 2005, J AM STAT ASSOC, V100, P322, DOI 10.1198/016214504000001880
   Sanh V, 2020, Arxiv, DOI [arXiv:1910.01108, 10.48550/arXiv.1910.01108]
   Scholkopf Bernhard, 2012, P INT C MACHINE LEAR, P459
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Wang LM, 2021, PROC CVPR IEEE, P1895, DOI 10.1109/CVPR46437.2021.00193
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Xu X, 2017, INT J COMPUT VISION, V123, P309, DOI 10.1007/s11263-016-0983-5
   Xu X, 2016, LECT NOTES COMPUT SC, V9906, P343, DOI 10.1007/978-3-319-46475-6_22
   Yang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1, DOI 10.1145/3404835.3462823
   Zhang Z, 2015, PATTERN ANAL APPL, V18, P157, DOI 10.1007/s10044-013-0349-3
   Zhu Y, 2018, PROC CVPR IEEE, P9436, DOI 10.1109/CVPR.2018.00983
   Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111
NR 52
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3976
EP 3986
DI 10.1109/TMM.2023.3318300
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300018
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Wang, XM
   Chen, HL
   Sun, P
   Li, JJ
   Zhang, AQ
   Liu, WF
   Jiang, N
AF Wang, Xiaomeng
   Chen, Honglong
   Sun, Peng
   Li, Junjian
   Zhang, Anqing
   Liu, Weifeng
   Jiang, Nan
TI AdvST: Generating Unrestricted Adversarial Images via Style Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial images; style transfer; semantic information; edge
   information; photorealistic attack; artistic attack
ID CONVOLUTIONAL NEURAL-NETWORK; ROBUSTNESS
AB Recent years have witnessed extensive applications of Deep Neural Networks (DNNs) in various vision tasks. However, DNNs are vulnerable to adversarial images crafted by introducing perturbations into inputs to induce incorrect predictions. Unlike L-p -norm restricted adversarial attacks, many unrestricted attacks have been proposed by modifying attributes of the image (e.g., edge, color), while the critical components of the image are preserved. However, most existing unrestricted attacks easily introduce unnatural distortions, colors, stains and schemes, in the generated adversarial images. This paper proposes a novel unrestricted attack (named AdvST) to create stylized, natural-looking, and high-transferability adversarial images. The basic idea of AdvST is to embed adversarial perturbations when transferring the style from the reference image onto the original image (i.e., rendering the original image's semantic contents into the reference image's style). To further improve the image quality of generated adversarial images, we refine two kinds of reference images (i.e., photographs and artworks) based on different attractive styles and design two attacks accordingly. For photorealistic attack, we incorporate semantic information obtained from segmentation maps to improve the photo realism of adversarial images. For artistic attack, we propose integrating edge information extracted by the Laplace operator to preserve the structural integrity of the original image. Extensive experimental results validate the superior performance of AdvST in terms of adversarial image quality and black-box transferability compared to benchmark methods.
C1 [Wang, Xiaomeng; Chen, Honglong; Li, Junjian; Zhang, Anqing; Liu, Weifeng] China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266580, Peoples R China.
   [Sun, Peng] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410012, Peoples R China.
   [Jiang, Nan] East China Jiaotong Univ, Coll Informat Engn, Nanchang 330013, Peoples R China.
C3 China University of Petroleum; Hunan University; East China Jiaotong
   University
RP Chen, HL (corresponding author), China Univ Petr East China, Coll Control Sci & Engn, Qingdao 266580, Peoples R China.
EM upcwxm@outlook.com; chenhl@upc.edu.cn; psun@hnu.edu.cn;
   ljj1016cc@163.com; zhang_anqing@126.com; liuwf@upc.edu.cn;
   jiangnan1018@gmail.com
OI jiang, nan/0000-0003-1712-1872; Sun, Peng/0000-0001-6221-8142
FU Shandong Provincial Taishan Scholar Program
FX No Statement Available
CR Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   Bhattad A., 2020, P INT C LEARN REPR
   Caesar H, 2018, PROC CVPR IEEE, P1209, DOI 10.1109/CVPR.2018.00132
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Y-L, 2016, BMVC
   Das N., 2017, P IEEE C COMP VIS PA, P1
   Duan RJ, 2020, PROC CVPR IEEE, P997, DOI 10.1109/CVPR42600.2020.00108
   Duda R.O., 2000, Pattern Classification and Scene Analysis
   Engstrom L, 2019, PR MACH LEARN RES, V97
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   Guo ZC, 2023, IEEE T MULTIMEDIA, V25, P38, DOI 10.1109/TMM.2021.3120544
   Hosseini H, 2018, IEEE COMPUT SOC CONF, P1695, DOI 10.1109/CVPRW.2018.00212
   Champandard AJ, 2016, Arxiv, DOI arXiv:1603.01768
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A., 2017, P INT C LEARN REPR
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Liu Y., 2017, INT C COMPUTER VISIO, P1939
   Lu M, 2017, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2017.270
   Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740
   Mechrez R, 2018, LECT NOTES COMPUT SC, V11218, P800, DOI 10.1007/978-3-030-01264-9_47
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nesti F, 2022, IEEE WINT CONF APPL, P2826, DOI 10.1109/WACV51458.2022.00288
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Radford L., 2016, Unsupervised representation learning with deep convolutional generative adversarial networks, P1
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shamsabadi AS, 2020, PROC CVPR IEEE, P1148, DOI 10.1109/CVPR42600.2020.00123
   Shamsabadi AS, 2020, INT CONF ACOUST SPEE, P1898, DOI [10.1109/ICASSP40776.2020.9054368, 10.1109/icassp40776.2020.9054368]
   Song Y, 2018, ADV NEUR IN, V31
   Szegedy C., 2014, P 2 INT C LEARN REPR, P1
   Talebi H, 2018, IEEE T IMAGE PROCESS, V27, P3998, DOI 10.1109/TIP.2018.2831899
   Tramer A., 2018, Ensemble adversarial training: Attacks and defenses, P1
   Ulyanov D, 2016, PR MACH LEARN RES, V48
   Wan C, 2023, IEEE T MULTIMEDIA, V25, P9572, DOI 10.1109/TMM.2023.3255742
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wang ZB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7619, DOI 10.1109/ICCV48922.2021.00754
   Xiao C., 2018, P INT C LEARN REPR
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
   Yuan S., 2022, P NEUR INF PROC SYST, P1
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhao Z., 2020, P BRIT MACH VIS C, P1
   Zhao ZY, 2020, PROC CVPR IEEE, P1036, DOI 10.1109/CVPR42600.2020.00112
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 50
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4846
EP 4858
DI 10.1109/TMM.2023.3327017
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100016
DA 2024-08-05
ER

PT J
AU Wang, XW
   Wang, LJ
   Su, YT
   Zhang, YD
   Liu, AA
AF Wang, Xiaowen
   Wang, Lanjun
   Su, Yuting
   Zhang, Yongdong
   Liu, An-An
TI MCDAN: A Multi-Scale Context-Enhanced Dynamic Attention Network for
   Diffusion Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Predictive models; Information diffusion; Feature extraction; Data
   mining; Task analysis; Context modeling; Annotations; User intention
   understanding; information diffusion prediction; context interaction;
   graph neural networks
AB Information diffusion prediction aims at predicting the target users in the information diffusion path on social networks. Prior works mainly focus on the observed structure or sequence of cascades, trying to predict to whom this cascade will be infected passively. In this study, we argue that user intent understanding is also a key part of information diffusion prediction. We thereby propose a novel Multi-scale Context-enhanced Dynamic Attention Network (MCDAN) to predict which user will most likely join the observed current cascades. Specifically, to consider the global interactive relationship among users, we take full advantage of user friendships and global cascading relationships, which are extracted from the social network and historical cascades, respectively. To refine the model's ability to understand the user's preference for the current cascade, we propose a multi-scale sequential hypergraph attention module to capture the dynamic preference of users at different time scales. Moreover, we design a contextual attention enhancement module to strengthen the interaction of user representations within the current cascade. Finally, to engage the user's own susceptibility, we construct a susceptibility label for each user based on user susceptibility analysis and use the rank of this label for auxiliary prediction. We conduct experiments over four widely used datasets and show that MCDAN significantly overperforms the state-of-the-art models. The average improvements are up to 5.41% in terms of Hits@100 and 8.47% in terms of MAP@100, respectively.
C1 [Wang, Xiaowen; Wang, Lanjun; Su, Yuting; Liu, An-An] Tianjin Univ, Tianjin 300072, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, LJ; Liu, AA (corresponding author), Tianjin Univ, Tianjin 300072, Peoples R China.
EM wang.lanjun@outlook.com; anan0422@gmail.com
RI Wang, Xiao-Wen/HTL-3465-2023
OI Wang, Xiao-Wen/0000-0002-0543-2537; Wang, Lanjun/0000-0002-7696-5330;
   Jing, Peiguang/0000-0003-2648-7358
FU National Key Research and Development Program of China
FX No Statement Available
CR [Anonymous], 1999, INMATES ARE RUNNING
   Chen WL, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7008, DOI 10.1145/3503161.3551568
   Chen YB, 2021, FUTURE INTERNET, V13, DOI 10.3390/fi13060147
   Feng SB, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108119
   Fudong Li, 2010, Proceedings of the 2010 International Conference on Emerging Security Technologies (EST 2010), P77, DOI 10.1109/EST.2010.26
   Gao Q, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16020143
   Gou CC, 2018, KNOWL INF SYST, V57, P721, DOI 10.1007/s10115-017-1143-0
   Gu HQ, 2018, IEEE INT CON MULTI
   Hodas NO, 2014, SCI REP-UK, V4, DOI 10.1038/srep04343
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Islam MR, 2018, IEEE DATA MINING, P1055, DOI 10.1109/ICDM.2018.00134
   Jia AL, 2018, COMPUT NETW, V140, P112, DOI 10.1016/j.comnet.2018.05.004
   Ketkar N., 2021, Deep Learning with Python, P27, DOI [DOI 10.1007/978-1-4842-5364-9_2, DOI 10.1007/978-1-4842-5364-92N]
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kumar S, 2022, INFORM SCIENCES, V607, P1617, DOI 10.1016/j.ins.2022.06.075
   Lai X, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4565, DOI 10.1145/3394171.3416273
   Li D, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1477, DOI 10.1145/2505515.2507823
   Li GM, 2022, WIREL COMMUN MOB COM, V2022, DOI 10.1155/2022/2515962
   Li J, 2019, TEH VJESN, V26, P1670, DOI 10.17559/TV-20190603165825
   Liao DL, 2019, AAAI CONF ARTIF INTE, P200
   Liu XY, 2023, J KING SAUD UNIV-COM, V35, DOI 10.1016/j.jksuci.2023.101852
   Ma H., 2012, P 21 INT C WORLD WID, P231, DOI [10.1145/2187836.2187868, DOI 10.1145/2187836.2187868]
   Mueller J., 2016, PROC 3 MULTIDISCIPLI, P1
   Oro E, 2018, IEEE T MULTIMEDIA, V20, P1195, DOI 10.1109/TMM.2017.2763324
   Qiao HL, 2023, PROCEEDINGS OF THE 32ND ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2023, P2065, DOI 10.1145/3583780.3615041
   Saura JR, 2021, J INNOV KNOWL, V6, P92, DOI 10.1016/j.jik.2020.08.001
   Samanta B, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2679
   Sang L, 2021, IEEE T MULTIMEDIA, V23, P2019, DOI 10.1109/TMM.2020.3007330
   Sankar A, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P510, DOI 10.1145/3336191.3371811
   Sun F, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P1441, DOI 10.1145/3357384.3357895
   Sun L, 2022, AAAI CONF ARTIF INTE, P4156
   Wang HY, 2020, AAAI CONF ARTIF INTE, V34, P246
   Wang J, 2017, IEEE DATA MINING, P475, DOI 10.1109/ICDM.2017.57
   Wang K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4570, DOI 10.1145/3394171.3416294
   Wang YC, 2017, PR MACH LEARN RES, V54, P1375
   Wang ZT, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1795, DOI 10.1145/3269206.3269275
   Wu JM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P7045, DOI 10.1145/3503161.3551576
   Wu L, 2016, AAAI CONF ARTIF INTE, P279
   [邢玲 Xing Ling], 2016, [计算机应用研究, Application Research of Computers], V33, P661
   Yang C, 2021, IEEE T KNOWL DATA EN, V33, P1128, DOI 10.1109/TKDE.2019.2939796
   Yang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4033
   Yang Y, 2015, AAAI CONF ARTIF INTE, P367
   Yuan CY, 2021, LECT NOTES ARTIF INT, V12459, P347, DOI 10.1007/978-3-030-67664-3_21
   Zeng FZ, 2022, NAACL 2022: THE 2022 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES, P4105
   Zeng H., 2016, Modern Econ. Inf., V16, P306
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao L, 2020, GEOINFORMATICA, V24, P443, DOI 10.1007/s10707-019-00376-9
   Zhao Z, 2018, IEEE T MULTIMEDIA, V20, P430, DOI 10.1109/TMM.2017.2740022
   Zhong E., 2012, Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '12, P696, DOI DOI 10.1145/2339530.2339641
   Zhou F, 2021, ACM COMPUT SURV, V54, DOI 10.1145/3433000
NR 50
TC 0
Z9 0
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7850
EP 7862
DI 10.1109/TMM.2024.3372371
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000031
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Wen, X
   Nie, WZ
   Liu, J
   Su, YT
   Zhang, YD
   Liu, AA
AF Wen, Xin
   Nie, Weizhi
   Liu, Jing
   Su, Yuting
   Zhang, Yongdong
   Liu, An-An
TI CDCM: ChatGPT-Aided Diversity-Aware Causal Model for Interactive
   Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive recommendation; accuracy; diversity; monotony effect; causal
   inference; ChatGPT
AB In recent years, interactive recommender systems (IRSs) have attracted extensive interest. Existing IRSs are typically implemented with offline reinforcement learning (RL). They are devoted to improving recommendation accuracy by optimizing the extraction of users' inherent preferences. However, there hasn't been much attention on recommendation diversity, which could result in the monotony effect, i.e., categories of recommended items are consistently fixed and unchanging. In this paper, we center on category diversification in IRSs while largely preserving or even boosting recommendation accuracy. To this end, we propose a ChatGPT-aided diversity-aware causal model (CDCM) to enhance the offline RL framework with causal inference and ChatGPT. Specifically, we first propose a diversity-aware causal user model (DCUM) to estimate user satisfaction. This model disentangles the causal effect of users' inherent preferences and the monotony effect to obtain user satisfaction with both accuracy and diversity. Then, DCUM is used to assist the RL agent in recommendation policy learning. A ChatGPT-aided state encoder (CSE) is proposed to provide user state representation for each time step of policy learning. With the help of ChatGPT, CSE incorporates multi-category information in line with users' potential preferences to promote diverse and relevant category recommendations. Extensive experiment results on two real-world datasets validate the superiority of our CDCM regarding both accuracy and diversity.
C1 [Wen, Xin; Nie, Weizhi; Liu, Jing; Su, Yuting; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Tianjin University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Nie, WZ; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM wenxin113@tju.edu.cn; weizhinie@tju.edu.cn; jliu_tju@tju.edu.cn;
   ytsu@tju.edu.cn; zhyd73@ustc.edu.cn; anan0422@gmail.com
OI Jing, Peiguang/0000-0003-2648-7358; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China
FX No Statement Available
CR Ashkan A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1742
   Bai J., 2019, NEURIPS, P10734
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Chen HK, 2019, AAAI CONF ARTIF INTE, P3312
   Chen LM, 2018, ADV NEUR IN, V31
   Chen M, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P456, DOI 10.1145/3289600.3290999
   Chen XS, 2019, 36 INT C MACHINE LEA, V97
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Cheng PZ, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P183, DOI 10.1145/3038912.3052585
   Ding L, 2022, AAAI CONF ARTIF INTE, P11864
   Dulac-Arnold G, 2016, Arxiv, DOI arXiv:1512.07679
   Fu MS, 2022, IEEE T CYBERNETICS, V52, P12028, DOI 10.1109/TCYB.2021.3089941
   Gan L, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P2001, DOI 10.1145/3397271.3401213
   Gao CM, 2023, PROCEEDINGS OF THE 46TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, SIGIR 2023, P238, DOI 10.1145/3539618.3591636
   Gao CM, 2024, ACM T INFORM SYST, V42, DOI 10.1145/3594871
   Gao CM, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P540, DOI 10.1145/3511808.3557220
   Gao CM, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P3953, DOI 10.1145/3511808.3557624
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   Hidasi B., 2016, ICLR POSTER
   Hong DC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1721, DOI 10.1145/3397271.3401225
   Iqbal M., 2018, SIGIR WorkshopeCommerce, V2319
   Kim Y, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2740
   Kingma D.P., 2014, Proc. of ICLR
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Koren Y, 2008, KDD, P426
   Leiter C, 2023, Arxiv, DOI [arXiv:2302.13795, DOI 10.48550/ARXIV.2302.13795]
   Li L., 2010, P 19 INT C WORLD WID, P661, DOI [10.1145/1772690.1772758, DOI 10.1145/1772690.1772758]
   Liang YL, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P747, DOI 10.1145/3404835.3462957
   Lin RH, 2023, WORLD WIDE WEB, V26, P2471, DOI 10.1007/s11280-022-01135-x
   Lin XR, 2022, AAAI CONF ARTIF INTE, P1610
   Liu F, 2020, NEUROCOMPUTING, V417, P255, DOI 10.1016/j.neucom.2020.07.057
   Liu HF, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, DOI 10.1145/3503161.3548302
   Liu YH, 2019, Arxiv, DOI [arXiv:1907.11692, DOI 10.48550/ARXIV.1907.11692]
   Liu Y, 2020, AAAI CONF ARTIF INTE, V34, P4932
   MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281
   Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579
   Nan GS, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9683
   Nie WZ, 2024, IEEE T MULTIMEDIA, V26, P1129, DOI 10.1109/TMM.2023.3276505
   Pan WB, 2023, Arxiv, DOI arXiv:2304.04256
   Park YJ, 2013, IEEE T KNOWL DATA EN, V25, P1904, DOI 10.1109/TKDE.2012.119
   Pearl J., 2009, Causality, DOI DOI 10.1017/CBO9780511803161
   Qin Lijing, 2014, P SIAM INT C DAT MIN, P461
   Qin W, 2023, IEEE T MULTIMEDIA, V25, P1033, DOI 10.1109/TMM.2021.3136717
   Rendle S., 2009, UAI 2009, P452
   RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350
   Schnabel T, 2016, PR MACH LEARN RES, V48
   Schulman J, 2017, Arxiv, DOI [arXiv:1707.06347, DOI 10.48550/ARXIV.1707.06347]
   Schulman P., 2016, P 4 INT C LEARN REPR
   Sha Chaofeng, 2016, P 25 INT JOINT C ART, P3868
   Shani G, 2005, J MACH LEARN RES, V6, P1265
   Si ZH, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P224, DOI 10.1145/3485447.3511951
   Sidana S, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P427, DOI 10.1145/3240323.3240400
   Simoudis E., 1996, KDD 96 P 2 INT C KNO, P226
   Tan YM, 2023, Arxiv, DOI arXiv:2303.07992
   Touvron H, 2023, Arxiv, DOI [arXiv:2302.13971, DOI 10.48550/ARXIV.2302.13971]
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HZ, 2017, AAAI CONF ARTIF INTE, P2695
   Wang K, 2021, AAAI CONF ARTIF INTE, V35, P4427
   Wang ZL, 2022, PROCEEDINGS OF THE ACM WEB CONFERENCE 2022 (WWW'22), P2195, DOI 10.1145/3485447.3512092
   WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Wu Q, 2019, Arxiv, DOI arXiv:1905.06589
   Xiao T, 2021, AAAI CONF ARTIF INTE, V35, P4512
   Yao LY, 2021, ACM T KNOWL DISCOV D, V15, DOI 10.1145/3444944
   Zhang M, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P123
   Zhang RY, 2022, AAAI CONF ARTIF INTE, P11694
   Zhang X, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P41, DOI 10.1145/3404835.3462892
   Zhang Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P11, DOI 10.1145/3404835.3462875
   Zhao XY, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P95, DOI 10.1145/3240323.3240374
   Zhao XX, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1411, DOI 10.1145/2505515.2505690
   Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994
   Zheng Y, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P401, DOI 10.1145/3442381.3449835
   Zhou SJ, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P179, DOI 10.1145/3397271.3401174
NR 74
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6488
EP 6500
DI 10.1109/TMM.2024.3352397
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600024
DA 2024-08-05
ER

PT J
AU Xiao, F
   Pu, ZD
   Chen, JQ
   Gao, XP
AF Xiao, Fen
   Pu, Zhengdong
   Chen, Jiaqi
   Gao, Xieping
TI DGFNet: Depth-Guided Cross-Modality Fusion Network for RGB-D Salient
   Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Fuses; Task analysis; Semantics;
   Data mining; Visualization; RGB-D salient object detection; depth
   quality; cross-modal feature fusion
ID IMAGE
AB RGB-D salient object detection (SOD) focuses on utilizing the complementary cues of RGB and depth modalities to detect and segment salient regions. However, many proposed methods train their models in a simple multi-modal manner, ignoring the differences between these two modalities in the contribution of salient detection. Furthermore, the quality of depth datasets varies significantly between individuals and is another important factor affecting model performance. To address the aforementioned issues, this article proposes a novel depth-guided fusion network framework (DGFNet) for the RGB-D SOD task. To avoid the influence of low-quality depth maps on RGB-D SOD, we design a depth map enhanced algorithm which jointly models salient detection and depth estimation to improve the quality of depth. Also, we propose a depth attention mechanism to encode valuable spatial information for SOD, which is then used in depth-guided fusion (DGF) module to guide the fusion of cross-modality features at each level. Extensive experiments on seven commonly tested datasets demonstrate that our DGFNet outperforms the 23 state-of-the-art RGB-D-based SOD methods.
C1 [Xiao, Fen; Pu, Zhengdong; Chen, Jiaqi; Gao, Xieping] Xiangtan Univ, Key Lab Intelligent Comp & Informat Proc, Minist Educ, Xiangtan 411105, Peoples R China.
   [Xiao, Fen; Chen, Jiaqi] Xiangtan Univ, Sch Comp Sci, Xiangtan 411105, Peoples R China.
   [Pu, Zhengdong] Xiangtan Univ, Sch Math & Computat Sci, Xiangtan 411105, Peoples R China.
   [Gao, Xieping] Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410006, Peoples R China.
C3 Xiangtan University; Xiangtan University; Xiangtan University; Hunan
   Normal University
RP Xiao, F; Gao, XP (corresponding author), Xiangtan Univ, Key Lab Intelligent Comp & Informat Proc, Minist Educ, Xiangtan 411105, Peoples R China.; Gao, XP (corresponding author), Hunan Normal Univ, Coll Informat Sci & Engn, Changsha 410006, Peoples R China.
EM xiaof@xtu.edu.cn; bobpuluck@163.com; 1975148761@qq.com;
   xpgao@hunnu.edu.cn
OI Xiao, Fen/0000-0001-7511-9418
FU National Science and Technology Major Project
FX No Statement Available
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P4296, DOI 10.1109/TIP.2020.2968250
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen Q, 2021, AAAI CONF ARTIF INTE, V35, P1063
   Chen ZY, 2021, IEEE T IMAGE PROCESS, V30, P7012, DOI 10.1109/TIP.2020.3028289
   Cheng Y, 2014, IEEE INT CON MULTI
   Chongyi Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P225, DOI 10.1007/978-3-030-58598-3_14
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2022, IEEE T IMAGE PROCESS, V31, P6800, DOI 10.1109/TIP.2022.3216198
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   de San Roman PP, 2017, COMPUT VIS IMAGE UND, V164, P82, DOI [10.1016/j.cviu.2017.03.001, 10.1016/j.cviu2017.03.001]
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Ding Y, 2019, J VIS COMMUN IMAGE R, V61, P1, DOI 10.1016/j.jvcir.2019.03.019
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2021, IEEE T NEUR NET LEAR, V32, P2075, DOI 10.1109/TNNLS.2020.2996406
   Fidon L, 2018, LECT NOTES COMPUT SC, V10670, P64, DOI 10.1007/978-3-319-75238-9_6
   Fu K., 2020, P IEEE CVF C COMP VI, P3052
   Gongyang Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P665, DOI 10.1007/978-3-030-58520-4_39
   Guo T., 2016, P IEEE INT C MULT EX, P1
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ji W, 2022, IEEE T IMAGE PROCESS, V31, P2321, DOI 10.1109/TIP.2022.3154931
   Ji W, 2021, PROC CVPR IEEE, P9466, DOI 10.1109/CVPR46437.2021.00935
   Jin WD, 2021, IEEE T IMAGE PROCESS, V30, P3376, DOI 10.1109/TIP.2021.3060167
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li GY, 2020, IEEE T IMAGE PROCESS, V29, P4873, DOI 10.1109/TIP.2020.2976689
   Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359
   Liu N, 2022, IEEE T PATTERN ANAL, V44, P9026, DOI 10.1109/TPAMI.2021.3122139
   Liu ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4481, DOI 10.1145/3474085.3475601
   Liu ZY, 2022, IEEE T CIRC SYST VID, V32, P4486, DOI 10.1109/TCSVT.2021.3127149
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Nian Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13753, DOI 10.1109/CVPR42600.2020.01377
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Sun P, 2021, PROC CVPR IEEE, P1407, DOI 10.1109/CVPR46437.2021.00146
   Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244
   Wang FY, 2022, IEEE T IMAGE PROCESS, V31, P1285, DOI 10.1109/TIP.2022.3140606
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang Y, 2020, VISUAL COMPUT, V36, P683, DOI 10.1007/s00371-019-01646-1
   Wei SK, 2019, IEEE T IMAGE PROCESS, V28, P4580, DOI 10.1109/TIP.2019.2913513
   Wen HF, 2021, IEEE T IMAGE PROCESS, V30, P9179, DOI 10.1109/TIP.2021.3123548
   Yang Y, 2022, IEEE T CIRC SYST VID, V32, P5346, DOI 10.1109/TCSVT.2022.3144852
   Yongri Piao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9057, DOI 10.1109/CVPR42600.2020.00908
   Zhai YJ, 2021, IEEE T IMAGE PROCESS, V30, P8727, DOI 10.1109/TIP.2021.3116793
   Zhang C, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2094, DOI 10.1145/3474085.3475364
   Zhang Jing, 2021, P IEEE CVF INT C COM, P4338
   Zhang M, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4107, DOI [10.1145/33941713413969, 10.1145/3394171.3413969]
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang S., 2022, IEEE Trans.Multimedia, P1
   Zhang W., 2021, P IEEE INT C MULT EX, P1
   Zhang WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P731, DOI 10.1145/3474085.3475240
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 63
TC 1
Z9 1
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2648
EP 2658
DI 10.1109/TMM.2023.3301280
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400015
DA 2024-08-05
ER

PT J
AU Yao, Y
   Huang, LC
   Wang, H
   Chang, Q
   Ren, YZ
   Xiao, FJ
AF Yao, Ye
   Huang, Linchao
   Wang, Hui
   Chang, Qi
   Ren, Yizhi
   Xiao, Fengjun
TI Robust Adaptive Steganography Based on Adaptive STC-ECC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Security; Robustness; Steganography; Transform coding; Discrete cosine
   transforms; Image coding; Error correction codes; Robust steganography;
   matching robust; candidate cover; adaptive STC-ECC strategy
ID RESISTING JPEG COMPRESSION; DITHER MODULATION; STEGANALYSIS; CODES
AB With the increasing popularity of Online Social Networks (OSNs), covert communication is rapidly shifting from lossless channels like email to lossy channels, specifically social networks. In response to this trend, robust adaptive steganography has emerged as a powerful technique for concealing information in lossy transport channels. Previous approaches have aimed to address the challenge of JPEG image compression during transmission by utilizing static compression-resistant domains, Syndrome-Trellis Codes (STC), and Error Correction Codes (ECC). However, reliance on a significant number of ECC check codes to ensure robustness could inadvertently affect security. In response to this challenge, we introduce the "Adaptive STC-ECC" strategy, which enhances security by minimizing the number of check codes without compromising robustness. We further improve the robustness by simulating the embedding process and strategically placing the wet point in unstable cover elements. Furthermore, we exploit the residual information between the pre-cover and cover images to adjust the distortion and accurately determine the direction of the dither modulation, thus improving the overall security. Extensive experiments have been conducted to evaluate the performance of our proposed approach, and the results demonstrate its superior robustness and security compared to existing state-of-the-art approaches.
C1 [Yao, Ye; Huang, Linchao; Wang, Hui; Chang, Qi; Ren, Yizhi] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310018, Peoples R China.
   [Xiao, Fengjun] Hangzhou Dianzi Univ, Zhejiang Informatizat Dev Inst, Hangzhou 310018, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University
RP Xiao, FJ (corresponding author), Hangzhou Dianzi Univ, Zhejiang Informatizat Dev Inst, Hangzhou 310018, Peoples R China.
EM yaoye@hdu.edu.cn; hlinchaw@hdu.edu.cn; h.wang@hdu.edu.cn;
   qichang@hdu.edu.cn; renyz@hdu.edu.cn; bhxfj@126.com
OI Yao, Ye/0000-0002-7012-9307; chang, qi/0009-0003-8051-5521; Wang,
   Hui/0000-0001-6620-3319
FU National Natural Science Foundation of China
FX No Statement Available
CR Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Borges PVK, 2008, IEEE T MULTIMEDIA, V10, P1479, DOI 10.1109/TMM.2008.2007294
   Boroumand M, 2019, IEEE T INF FOREN SEC, V14, P1181, DOI 10.1109/TIFS.2018.2871749
   Butora J, 2024, IEEE T DEPEND SECURE, V21, P2394, DOI 10.1109/TDSC.2023.3306379
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen KJ, 2019, IEEE T INF FOREN SEC, V14, P1052, DOI 10.1109/TIFS.2018.2869353
   Denemark T., 2015, PROC IEEE INT WORKSH, P1
   Duan XL, 2023, EXPERT SYST APPL, V229, DOI 10.1016/j.eswa.2023.120416
   Filler T, 2011, IEEE T INF FOREN SEC, V6, P920, DOI 10.1109/TIFS.2011.2134094
   Fridrich J, 2009, Steganography in Digital Media: Principles Algorithms and Applications
   Guo LJ, 2015, IEEE T INF FOREN SEC, V10, P2669, DOI 10.1109/TIFS.2015.2473815
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2015, IEEE T INF FOREN SEC, V10, P219, DOI 10.1109/TIFS.2014.2364918
   Huang YF, 2012, IEEE T INF FOREN SEC, V7, P1865, DOI 10.1109/TIFS.2012.2218599
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Kodovsky J, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P63
   Li WJ, 2023, IEEE T MULTIMEDIA, V25, P8320, DOI 10.1109/TMM.2023.3234812
   Li WX, 2020, IEEE T COMMUN, V68, P3948, DOI 10.1109/TCOMM.2020.2982624
   Qiao T, 2021, SIGNAL PROCESS, V183, DOI 10.1016/j.sigpro.2021.108048
   REED IS, 1960, J SOC IND APPL MATH, V8, P300, DOI 10.1137/0108018
   Setiadi DIM, 2023, SIGNAL PROCESS, V206, DOI 10.1016/j.sigpro.2022.108908
   Su WK, 2018, IEEE T CIRC SYST VID, V28, P3545, DOI 10.1109/TCSVT.2018.2865537
   Wu XS, 2022, SIGNAL PROCESS, V196, DOI 10.1016/j.sigpro.2022.108522
   Xu DW, 2014, IEEE T INF FOREN SEC, V9, P596, DOI 10.1109/TIFS.2014.2302899
   Yin ZX, 2021, IEEE T SIGNAL INF PR, V7, P336, DOI 10.1109/TSIPN.2021.3081373
   Zeng K, 2024, IEEE T MULTIMEDIA, V26, P299, DOI 10.1109/TMM.2023.3264628
   Zeng K, 2023, IEEE T CIRC SYST VID, V33, P4893, DOI 10.1109/TCSVT.2023.3250750
   Zeng K, 2022, SIGNAL PROCESS, V195, DOI 10.1016/j.sigpro.2022.108498
   Zhang Y, 2020, J REAL-TIME IMAGE PR, V17, P115, DOI 10.1007/s11554-019-00905-7
   Zhang Y, 2018, MULTIMED TOOLS APPL, V77, P17913, DOI 10.1007/s11042-017-4506-3
   Zhang Y, 2017, MULTIMED TOOLS APPL, V76, P3649, DOI 10.1007/s11042-016-3914-0
   Zhang Y, 2016, SECUR COMMUN NETW, V9, P2957, DOI 10.1002/sec.1502
   Zhang Y, 2015, PROCEEDINGS 10TH INTERNATIONAL CONFERENCE ON AVAILABILITY, RELIABILITY AND SECURITY ARES 2015, P461, DOI 10.1109/ARES.2015.53
   Zhao ZZ, 2019, IEEE T INF FOREN SEC, V14, P1843, DOI 10.1109/TIFS.2018.2885438
   Zhou H, 2019, IEEE T MULTIMEDIA, V21, P1384, DOI 10.1109/TMM.2018.2882088
   Zhu ZQ, 2019, IEEE ACCESS, V7, P168613, DOI 10.1109/ACCESS.2019.2953504
NR 36
TC 1
Z9 1
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5477
EP 5489
DI 10.1109/TMM.2023.3334487
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600034
DA 2024-08-05
ER

PT J
AU Ye, SM
   Wang, H
   Tan, MK
   Liu, F
AF Ye, Senmao
   Wang, Huan
   Tan, Mingkui
   Liu, Fei
TI Recurrent Affine Transformation for Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Text-to-image; generative adversarial network; recurrent neural network;
   spatial attention
AB Text-to-image synthesis aims to generate realistic images conditioned on text descriptions. Recently, conditional affine transformations (CATs), such as conditional batch normalization and instance normalization, have been applied to different layers to control the contents in synthesized images. However, the isolated CAT blocks predict the batch statistics of neighboring layers independently. What's more, CATs are simple multilayer perceptions that are hard to optimize. To address above issues, we propose a recurrent affine transformation (RAT) that connects all the CAT blocks with a recurrent neural network for modeling the long-term dependency between CAT blocks. To verify the effectiveness of RAT, we conduct both microscopic and macroscopic analyses of RAT, which not only demonstrates the effectiveness of RAT but also turns out to be a useful perspective to analyze how GANs fuse conditional information. In addition, we apply a spatial attention mechanism to the discriminator, which helps the text description to supervise the generator to synthesize more relevant image contents. Extensive experiments on the CUB, Oxford-102, and COCO datasets demonstrate the proposed model's superiority in comparison to state-of-the-art models.
C1 [Ye, Senmao; Tan, Mingkui; Liu, Fei] South China Univ Technolody, Sch Software Engn, Guangzhou 510000, Peoples R China.
   [Wang, Huan] Guangdong Inst Sci & Tech Informat, Ind Technol Res Ctr, Guangzhou 510000, Peoples R China.
RP Liu, F (corresponding author), South China Univ Technolody, Sch Software Engn, Guangzhou 510000, Peoples R China.
EM senmaoy@gmail.com; wanghuan6@email.szu.edu.cn; mingkuitan@scut.edu.cn;
   feiliu@scut.edu.cn
OI ye, senmao/0000-0002-1978-3805; Liu, Fei/0000-0001-9415-0496
FU National Natural Science Foundation of China
FX No Statement Available
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Brock A., 2019, P INT C LEARN REPR
   Dong MJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P708
   Dumoulin V., 2017, 5 INT C LEARN REPR I
   Fang SC, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P715
   Feng FX, 2022, IEEE T MULTIMEDIA, V24, P2112, DOI 10.1109/TMM.2021.3075997
   Graves A, 2014, Arxiv, DOI arXiv:1410.5401
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Hensel M, 2017, ADV NEUR IN, V30
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3409, DOI 10.1109/TMM.2022.3160360
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Li FY, 2022, IEEE COMPUT SOC CONF, P1031, DOI 10.1109/CVPRW56347.2022.00116
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li XZ, 2020, AAAI CONF ARTIF INTE, V34, P11434
   Liao WT, 2022, PROC CVPR IEEE, P18166, DOI 10.1109/CVPR52688.2022.01765
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mansimov E., 2016, P 4 INT C LEARN REPR
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Peng J, 2022, IEEE T MULTIMEDIA, V24, P4356, DOI 10.1109/TMM.2021.3116416
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reddy M.D.M., 2021, UGC Care Group I Journal, V14, P71
   Reed S. E., 2016, Proc. of Advances in Neural Information Processing Systems NIPS, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Salimans T, 2016, ADV NEUR IN, V29
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Tao M., 2022, P IEEE CVF C COMP VI, P16494
   Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20
   Welinder P., 2010, Computation & Neural Systems Technical Report
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang Z., 2021, P IEEE INT JOINT C N, P1
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
   Zhu XZ, 2019, IEEE I CONF COMP VIS, P6687, DOI 10.1109/ICCV.2019.00679
NR 38
TC 5
Z9 5
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 462
EP 473
DI 10.1109/TMM.2023.3266607
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000010
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, JH
   Shi, JL
   Zou, DP
   Shu, X
   Bai, SQ
   Lu, JW
   Zhu, HW
   Ni, J
   Sun, YH
AF Zhang, Jiahui
   Shi, Jinlong
   Zou, Danping
   Shu, Xin
   Bai, Suqin
   Lu, Jiawen
   Zhu, Haowei
   Ni, Jun
   Sun, Yunhan
TI EPM-Net: Efficient Feature Extraction, Point-Pair Feature Matching for
   Robust 6-D Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 6-D pose estimation; feature extraction; point-pair features (PPF);
   RGB-D images
ID 3D OBJECT DETECTION; ATTENTION; DEPTH
AB Estimating the 6-D poses of objects from RGB-D images holds great potential for several applications. However, given that the 6-D pose estimation accuracy is significantly affected by occlusion and noise between the objects in an image, this paper proposes a novel 6-D pose estimation method based on Efficient feature extraction and Point-pair feature matching. Specifically, we develop the Efficient channel attention Convolutional Neural Network (ECNN) and SO(3)-Encoder modules to extract 2-D features from the RGB image and SO(3)-equivariant features from the depth image, respectively. These features are fused in the DenseFusion module to obtain 3-D features in the camera space. Meanwhile, we exploit CAD model priors to obtain 3-D features in the model space through the model feature encoder, and then we globally regress the 3-D features in the camera and model space. According to these features, we generate oriented point clouds in each space, and then conduct point-pair feature matching to obtain pose information. Finally, we perform direct pose regression on the 3-D features in the camera and model space, and then resulting point-pair feature matching pose information is combined with the direct point-wise pose regression information to enhance pose prediction accuracy. Experimental results on three widely used benchmarking datasets demonstrate that our method achieves state-of-the-art performance, particularly for severe occluded scenes.
C1 [Zhang, Jiahui; Shi, Jinlong; Shu, Xin; Bai, Suqin; Lu, Jiawen; Zhu, Haowei; Ni, Jun] Jiangsu Univ Sci & Technol, Coll Comp Sci & Technol, Zhenjiang 21200, Peoples R China.
   [Zou, Danping] Shanghai Jiao Tong Univ, Inst Percept & Nav, Shanghai 200240, Peoples R China.
   [Sun, Yunhan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210008, Peoples R China.
C3 Jiangsu University of Science & Technology; Shanghai Jiao Tong
   University; Nanjing University
RP Shi, JL; Shu, X (corresponding author), Jiangsu Univ Sci & Technol, Coll Comp Sci & Technol, Zhenjiang 21200, Peoples R China.
EM 211210701419@stu.just.edu.cn; jsjxy_sjl@just.edu.cn; dpzou@sjtu.edu.cn;
   shuxin@just.edu.cn; jsjxy_bsq@just.edu.cn; 209070030@stu.just.edu.cn;
   zhu_hao_wei@163.com; 209070008@just.edu.cn; sunyh@smail.nju.edu.cn
OI Zhu, Haowei/0000-0003-0229-2935; Shu, Xin/0000-0003-1079-8434
FU Civil Aviation Smart Airport Theory and Key Laboratory
FX No Statement Available
CR Barron JT, 2013, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2013.10
   Brachmann E, 2014, LECT NOTES COMPUT SC, V8690, P536, DOI 10.1007/978-3-319-10605-2_35
   Chen W, 2020, PROC CVPR IEEE, P4232, DOI 10.1109/CVPR42600.2020.00429
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng CY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P12180, DOI 10.1109/ICCV48922.2021.01198
   Do TT, 2018, Arxiv, DOI [arXiv:1802.10367, 10.48550/ARXIV.1802.10367, DOI 10.48550/ARXIV.1802.10367]
   Drost B, 2010, PROC CVPR IEEE, P998, DOI 10.1109/CVPR.2010.5540108
   Ge JY, 2022, COMPUT ELECTR ENG, V100, DOI 10.1016/j.compeleceng.2022.107896
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   He K., 2017, P IEEE INT C COMP VI, P2961
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He W., 2020, P IEEE CVF C COMP VI, P11629, DOI [10.1109/CVPR42600.2020.01165, DOI 10.1109/CVPR42600.2020.01165]
   He YS, 2021, PROC CVPR IEEE, P3002, DOI 10.1109/CVPR46437.2021.00302
   Hinterstoisser S., 2012, ACCV, P548
   Hinterstoisser S, 2011, IEEE I CONF COMP VIS, P858, DOI 10.1109/ICCV.2011.6126326
   Hodan T, 2015, IEEE INT C INT ROBOT, P4421, DOI 10.1109/IROS.2015.7354005
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang WL, 2022, IEEE T MULTIMEDIA, V24, P3025, DOI 10.1109/TMM.2021.3092149
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kehl W, 2017, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2017.169
   Kendall A, 2015, IEEE I CONF COMP VIS, P2938, DOI 10.1109/ICCV.2015.336
   Li HY, 2022, LECT NOTES COMPUT SC, V13669, P369, DOI 10.1007/978-3-031-20077-9_22
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li SQ, 2012, IEEE T PATTERN ANAL, V34, P1444, DOI 10.1109/TPAMI.2012.41
   Li YM, 2019, COGN COMPUT, V11, P459, DOI 10.1007/s12559-019-9624-y
   Liu XB, 2023, IEEE T IND ELECTRON, V70, P8203, DOI 10.1109/TIE.2022.3212422
   Lu JW, 2023, IEEE T INSTRUM MEAS, V72, DOI 10.1109/TIM.2023.3244236
   Mnih V, 2014, ADV NEUR IN, V27
   Moenning C., 2003, 565 U CAMBR COMP LAB, V565, P1
   Niu ZY, 2021, NEUROCOMPUTING, V452, P48, DOI 10.1016/j.neucom.2021.03.091
   Pan HR, 2022, COMPUT GRAPH FORUM, V41, P371, DOI 10.1111/cgf.14684
   Park K, 2019, IEEE INT CONF ROBOT, P7207, DOI 10.1109/icra.2019.8794448
   Park K, 2019, IEEE I CONF COMP VIS, P7667, DOI 10.1109/ICCV.2019.00776
   Peng SD, 2019, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR.2019.00469
   Pereira Nuno, 2020, 2020 19th IEEE International Conference on Machine Learning and Applications (ICMLA), P71, DOI 10.1109/ICMLA51294.2020.00021
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Song C, 2020, PROC CVPR IEEE, P428, DOI 10.1109/CVPR42600.2020.00051
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   Song SR, 2014, LECT NOTES COMPUT SC, V8694, P634, DOI 10.1007/978-3-319-10599-4_41
   Sperber M, 2018, Arxiv, DOI arXiv:1803.09519
   Sundermeyer M, 2018, LECT NOTES COMPUT SC, V11210, P712, DOI 10.1007/978-3-030-01231-1_43
   Sutskever I., 2014, Advances in Neural Information Processing Systems, P3104
   Tejani A, 2014, LECT NOTES COMPUT SC, V8694, P462, DOI 10.1007/978-3-319-10599-4_30
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Wang C, 2019, PROC CVPR IEEE, P3338, DOI 10.1109/CVPR.2019.00346
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/ISQED48828.2020.9137057, 10.1109/isqed48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Xiang Y, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
   Xing XJ, 2022, IEEE T IND ELECTRON, V69, P10281, DOI 10.1109/TIE.2021.3121721
   Xu DF, 2018, PROC CVPR IEEE, P244, DOI 10.1109/CVPR.2018.00033
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu ZL, 2022, Arxiv, DOI arXiv:2205.03536
   Zhan I. H., 2022, arXiv
   Zhang J, 2023, IEEE T IMAGE PROCESS, V32, P6115, DOI 10.1109/TIP.2023.3328478
   Zhou GL, 2021, IEEE T MULTIMEDIA, V23, P1630, DOI 10.1109/TMM.2020.3001533
   Zhou GY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2773, DOI 10.1109/ICCV48922.2021.00279
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu ML, 2014, IEEE INT CONF ROBOT, P3936, DOI 10.1109/ICRA.2014.6907430
   Zhu YB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P465, DOI 10.1145/3343031.3350928
NR 58
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5120
EP 5130
DI 10.1109/TMM.2023.3330116
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MI8A6
UT WOS:001193072800012
DA 2024-08-05
ER

PT J
AU Zhang, JJ
   Wang, MY
   Jiang, HR
   Zhang, XY
   Yan, CG
   Zeng, D
AF Zhang, Junjie
   Wang, Mingyan
   Jiang, Haoran
   Zhang, Xinyu
   Yan, Chenggang
   Zeng, Dan
TI STAT: Multi-Object Tracking Based on Spatio-Temporal Topological
   Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tracking; Feature extraction; Adaptation models; Benchmark testing;
   Visualization; Topology; Symbols; Multi-object tracking; adaptive
   association; spatio-temporal topology
ID MULTIPLE-OBJECT TRACKING
AB The mainstream tracking-by-detection paradigm for multi-object tracking generally conducts detection first, followed by Re-IDentification (Re-ID) and motion estimation. The associations between the predicted boxes and existing tracks are then performed via visual and motion association. However, challenges such as irregular motion patterns, similar appearances, and frequent occlusions often arise, making object tracking a nontrivial task. In this article, we propose a multi-object tracker based on Spatio-TemporAl Topological (STAT) constraints to address the above issues. More specifically, we design the Feature Adaptive Association Module (FAAM) to establish the association between motion and appearance regionally, completing a complementary combination of appearance and motion features. Among these, the Appearance Feature Update Module (AFUM) is proposed to manage the appearance updates of tracked objects by imposing constraints based on the spatial locations and the degree of object occlusion, while temporal consistency is adopted to smooth the appearance states of tracks to mitigate the accumulation of appearance noise. Moreover, the Robust Motion Tracking Module (RMTM) is established to reduce the impact of irregular motions and certain unreliable detection results. The proposed module includes a higher weighted momentum term to accommodate the excessive motion amplitude and considers low-confidence boxes accompanied by the stage-wise association strategy for high-confidence boxes. Extensive experiments on DanceTrack and benchmark MOT datasets verify the effectiveness of our STAT tracker, especially the state-of-the-art results on DanceTrack, which is characterized by irregular motion and indistinguishable appearance attributes.
C1 [Zhang, Junjie; Wang, Mingyan; Jiang, Haoran; Zeng, Dan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
   [Zhang, Xinyu] Univ Adelaide, Adelaide, SA 5005, Australia.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Sch Commun Engn, Hangzhou 310018, Peoples R China.
   [Yan, Chenggang] Hangzhou Dianzi Univ, Lishui Inst, Lishui 323000, Peoples R China.
C3 Shanghai University; University of Adelaide; Hangzhou Dianzi University;
   Hangzhou Dianzi University
RP Zeng, D (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Key Lab Specialty Fiber Opt & Opt Access Networks, Joint Int Res Lab Specialty Fiber Opt & Adv Commun, Shanghai 200444, Peoples R China.
EM junjie_zhang@shu.edu.cn; 21721393@shu.edu.cn; jianghaoran@shu.edu.cn;
   xyzhang0717@gmail.com; cgyan@hdu.edu.cn; dzeng@shu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Aharon N, 2022, Arxiv, DOI [arXiv:2206.14651, DOI 10.48550/ARXIV.2206.14651]
   Ahn H, 2019, IEEE C ELECTR PERFOR, DOI [10.1109/epeps47316.2019.193195, 10.1007/s00779-019-01296-z]
   Bergmann P, 2019, IEEE I CONF COMP VIS, P941, DOI 10.1109/ICCV.2019.00103
   Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Brown R.G., 1992, Introduction to random signals and applied Kalman filtering, V3
   Cao JK, 2023, PROC CVPR IEEE, P9686, DOI 10.1109/CVPR52729.2023.00934
   Chu P, 2023, IEEE WINT CONF APPL, P4859, DOI 10.1109/WACV56688.2023.00485
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Dendorfer P., 2020, arXiv
   Dendorfer Patrick, 2022, ADV NEURAL INFORM PR, V35, P15657, DOI DOI 10.48550/ARXIV.2210.07681
   Du YH, 2023, IEEE T MULTIMEDIA, V25, P8725, DOI 10.1109/TMM.2023.3240881
   Du YH, 2021, IEEE INT CONF COMP V, P2809, DOI 10.1109/ICCVW54120.2021.00315
   Gao TZ, 2022, IEEE T MULTIMEDIA, V24, P995, DOI 10.1109/TMM.2021.3062489
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girbau A., 2022, PROC 33 BRIT MACH VI, P362
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Guo S, 2021, PROC CVPR IEEE, P8132, DOI 10.1109/CVPR46437.2021.00804
   Gustafsson F, 2002, IEEE T SIGNAL PROCES, V50, P425, DOI 10.1109/78.978396
   He LX, 2020, Arxiv, DOI arXiv:2006.02631
   Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797
   Kong J, 2022, IEEE T CIRC SYST VID, V32, P7746, DOI 10.1109/TCSVT.2022.3182709
   Liang C, 2022, IEEE T IMAGE PROCESS, V31, P3182, DOI 10.1109/TIP.2022.3165376
   Luiten J, 2021, INT J COMPUT VISION, V129, P548, DOI 10.1007/s11263-020-01375-2
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Meinhardt T, 2022, PROC CVPR IEEE, P8834, DOI 10.1109/CVPR52688.2022.00864
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Nasseri MH, 2023, J VIS COMMUN IMAGE R, V90, DOI 10.1016/j.jvcir.2022.103750
   Pang JM, 2021, PROC CVPR IEEE, P164, DOI 10.1109/CVPR46437.2021.00023
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Smith G. L., 1962, Application of statistical filter theory to the optimal estimation of position and velocity on board a circumlunar vehicle, V135
   Stadler D, 2022, IEEE WINT CONF APPL, P133, DOI 10.1109/WACVW54805.2022.00019
   Sun PZ, 2022, PROC CVPR IEEE, P20961, DOI 10.1109/CVPR52688.2022.02032
   Sun PZ, 2021, Arxiv, DOI arXiv:2012.15460
   Tomasi C., 2012, Comput. Vis. Sampler, P1
   UNADKAT SB, 2000, INT SER COMPUTAT INT, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2023, IEEE T MULTIMEDIA, V25, P1256, DOI 10.1109/TMM.2022.3140919
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang MY, 2022, IEEE INT WORKSH MULT, DOI 10.1109/MMSP55362.2022.9948977
   Wang YX, 2021, IEEE INT CONF ROBOT, P13708, DOI 10.1109/ICRA48506.2021.9561110
   Wang Z., 2020, P 16 EUR C COMP VIS, P107, DOI [10.1007/978-3-030-58621-8_7, DOI 10.1007/978-3-030-58621-8_7]
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yan B, 2022, LECT NOTES COMPUT SC, V13681, P733, DOI 10.1007/978-3-031-19803-8_43
   Yang P, 2023, IEEE T MULTIMEDIA, V25, P7456, DOI 10.1109/TMM.2022.3222614
   Yu E, 2023, IEEE T MULTIMEDIA, V25, P2686, DOI 10.1109/TMM.2022.3150169
   Zeng FG, 2022, LECT NOTES COMPUT SC, V13687, P659, DOI 10.1007/978-3-031-19812-0_38
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang YF, 2022, LECT NOTES COMPUT SC, V13682, P1, DOI 10.1007/978-3-031-20047-2_1
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 56
TC 1
Z9 1
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4445
EP 4457
DI 10.1109/TMM.2023.3323852
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100051
DA 2024-08-05
ER

PT J
AU Zhang, K
   Yang, Y
   Yu, J
   Jiang, HL
   Fan, JP
   Huang, QM
   Han, WD
AF Zhang, Ke
   Yang, Yan
   Yu, Jun
   Jiang, Hanliang
   Fan, Jianping
   Huang, Qingming
   Han, Weidong
TI Multi-Task Paired Masking With Alignment Modeling for Medical
   Vision-Language Pre-Training
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Image reconstruction; Visualization; Medical diagnostic
   imaging; Transformers; Semantics; Multitasking; Medical vision-language
   pre-training; joint image-text reconstruction; cross-modal alignment
AB In recent years, the growing demand for medical imaging diagnosis has placed a significant burden on radiologists. As a solution, Medical Vision-Language Pre-training (Med-VLP) methods have been proposed to learn universal representations from medical images and reports, benefiting downstream tasks without requiring fine-grained annotations. However, existing methods have overlooked the importance of cross-modal alignment in joint image-text reconstruction, resulting in insufficient cross-modal interaction. To address this limitation, we propose a unified Med-VLP framework based on Multi-task Paired Masking with Alignment (MPMA) to integrate the cross-modal alignment task into the joint image-text reconstruction framework to achieve more comprehensive cross-modal interaction, while a Global and Local Alignment (GLA) module is designed to assist self-supervised paradigm in obtaining semantic representations with rich domain knowledge. Furthermore, we introduce a Memory-Augmented Cross-Modal Fusion (MA-CMF) module to fully integrate visual information to assist report reconstruction and fuse the multi-modal representations adequately. Experimental results demonstrate that the proposed unified approach outperforms previous methods in all downstream tasks, including uni-modal, cross-modal, and multi-modal tasks.
C1 [Zhang, Ke; Yang, Yan; Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
   [Jiang, Hanliang] Zhejiang Univ, Sir Run Run Shaw Hosp, Reg Med Ctr, Natl Inst Resp Dis,Coll Med, Hangzhou 310016, Peoples R China.
   [Fan, Jianping] Lenovo Res, AI Lab, Beijing 100094, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Han, Weidong] Zhejiang Univ Hangzhou, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310018, Peoples R China.
   [Han, Weidong] Zhejiang Normal Univ Jinhua, Coll Math Med, Jinhua 321017, Zhejiang, Peoples R China.
C3 Hangzhou Dianzi University; Zhejiang University; Legend Holdings;
   Lenovo; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Zhejiang University
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.; Han, WD (corresponding author), Zhejiang Univ Hangzhou, Sir Run Run Shaw Hosp, Coll Med, Dept Med Oncol, Hangzhou 310018, Peoples R China.
EM ke.zhang@hdu.edu.cn; yangyan@hdu.edu.cn; yujun@hdu.edu.cn;
   aock@zju.edu.cn; jfan1@lenovo.com; qmhuang@ucas.ac.cn; hanwd@zju.edu.cn
OI Jiang, Hanliang/0000-0003-0902-882X; Yan, Yang/0000-0001-5598-1692;
   Zhang, Ke/0000-0002-9855-003X; Fan, Jianping/0000-0003-2290-1785
FU National Natural Science Foundation of China
FX No Statement Available
CR Banerjee S., 2005, P ACL WORKSHOP INTRI, P65, DOI DOI 10.3115/1626355.1626389
   Boecking B, 2022, LECT NOTES COMPUT SC, V13696, P1, DOI 10.1007/978-3-031-20059-5_1
   Cai XY, 2023, IEEE T MULTIMEDIA, V25, P845, DOI 10.1109/TMM.2021.3132724
   Chen XL, 2015, Arxiv, DOI arXiv:1504.00325
   Chen Z., 2021, Long Papers, P5904
   Chen ZH, 2022, LECT NOTES COMPUT SC, V13435, P679, DOI 10.1007/978-3-031-16443-9_65
   Chen ZH, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1439
   Cheng ZQ, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P292, DOI 10.1145/3078971.3079025
   Cheng ZQ, 2017, PROC CVPR IEEE, P4169, DOI 10.1109/CVPR.2017.444
   Cheng ZQ, 2017, IEEE T MULTIMEDIA, V19, P1170, DOI 10.1109/TMM.2016.2647386
   Cheng ZQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1365, DOI 10.1145/2964284.2964326
   Demner-Fushman D, 2016, J AM MED INFORM ASSN, V23, P304, DOI 10.1093/jamia/ocv080
   Devlin J, 2018, ARXIV
   Do T, 2021, LECT NOTES COMPUT SC, V12905, P64, DOI 10.1007/978-3-030-87240-3_7
   Finn C, 2017, PR MACH LEARN RES, V70
   Gong HF, 2022, IEEE T MED IMAGING, V41, P3332, DOI 10.1109/TMI.2022.3185008
   Gong HF, 2021, PROCEEDINGS OF THE 2021 INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR '21), P456, DOI 10.1145/3460426.3463584
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He XH, 2021, ACL-IJCNLP 2021: THE 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 2, P708
   Huang SC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3922, DOI 10.1109/ICCV48922.2021.00391
   Irvin J, 2019, AAAI CONF ARTIF INTE, P590
   Johnson Alistair EW, 2019, Mimic-cxr-jpg, a large publicly available database of labeled chest radiographs
   Kafle M., 2017, P 10 INT C NAT LANG, P198, DOI DOI 10.18653/V1/W17-3529
   Khare Y, 2021, I S BIOMED IMAGING, P1033, DOI 10.1109/ISBI48211.2021.9434063
   Kim JH, 2018, ADV NEUR IN, V31
   Lau JJ, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.251
   Li CY, 2019, AAAI CONF ARTIF INTE, P6666
   Li YY, 2018, ADV NEUR IN, V31
   Lin C.-Y., 2004, ANN M ASS COMP LING, P74
   Liu FL, 2021, PROC CVPR IEEE, P13748, DOI 10.1109/CVPR46437.2021.01354
   Loshchilov I., 2017, INT C LEARNING REPRE
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Ma J, 2023, IEEE T NEUR NET LEAR, V34, P1380, DOI 10.1109/TNNLS.2021.3105284
   Nguyen BD, 2019, LECT NOTES COMPUT SC, V11767, P522, DOI 10.1007/978-3-030-32251-9_57
   Nguyen P. A., 2017, PROC TREC VIDEO RETR
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pelka O, 2018, LECT NOTES COMPUT SC, V11043, P180, DOI 10.1007/978-3-030-01364-6_20
   Qin H, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P448
   Radford A, 2021, PR MACH LEARN RES, V139
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Ruixue Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P437, DOI 10.1007/978-3-030-58529-7_26
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shih G, 2019, RADIOL-ARTIF INTELL, V1, DOI 10.1148/ryai.2019180041
   Tay Y., 2023, PROC 11 INT C LEARN, P1
   Touvron H, 2023, Arxiv, DOI [arXiv:2302.13971, DOI 10.48550/ARXIV.2302.13971]
   Vaswani A, 2017, ADV NEUR IN, V30
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang Fuying, 2022, ADV NEURAL INF PROCE, V35, P33536
   Wang Z., 2022, P 2022 C EMPIRICAL M, P3876
   Wang ZY, 2022, IEEE T MED IMAGING, V41, P2803, DOI 10.1109/TMI.2022.3171661
   Wu C., 2023, P IEEECVF INT C COMP, P21372
   Wu YH, 2016, Arxiv, DOI [arXiv:1609.08144, DOI 10.48550/ARXIV.1609.08144]
   Yan B, 2022, AAAI CONF ARTIF INTE, P2982
   Yang Y, 2023, IEEE T MULTIMEDIA, V25, P167, DOI 10.1109/TMM.2021.3122542
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   You D, 2021, LECT NOTES COMPUT SC, V12903, P72, DOI 10.1007/978-3-030-87199-4_7
   Zhan LM, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2345, DOI 10.1145/3394171.3413761
   Zhang K, 2023, IEEE Transactions on Multimedia
   Zhang Y., 2022, P 7 MACHINE LEARNING, P2
   Zhou H.-Y., 2023, PROC 11 INT C LEARN, P1
   Zhou HY, 2022, NAT MACH INTELL, V4, P32, DOI 10.1038/s42256-021-00425-9
NR 63
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4706
EP 4721
DI 10.1109/TMM.2023.3325965
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100042
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, YQ
   Qian, Q
   Wang, HS
   Liu, C
   Chen, WH
   Wang, F
AF Zhang, Yuqi
   Qian, Qi
   Wang, Hongsong
   Liu, Chong
   Chen, Weihua
   Wang, Fan
TI Graph Convolution Based Efficient Re-Ranking for Visual Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Image retrieval; Representation learning;
   Convolution; Costs; Training; Visual retrieval; re-ranking; person
   re-identification; video-based person re-identification
ID PERSON REIDENTIFICATION; ACCURATE; NETWORK
AB Visual retrieval tasks such as image retrieval and person re-identification (Re-ID) aim at effectively and thoroughly searching images with similar content or the same identity. After obtaining retrieved examples, re-ranking is a widely adopted post-processing step to reorder and improve the initial retrieval results by making use of the contextual information from semantically neighboring samples. Prevailing re-ranking approaches update distance metrics and mostly rely on inefficient crosscheck set comparison operations while computing expanded neighbors based distances. In this work, we present an efficient re-ranking method which refines initial retrieval results by updating features. Specifically, we reformulate re-ranking based on Graph Convolution Networks (GCN) and propose a novel Graph Convolution based Re-ranking (GCR) for visual retrieval tasks via feature propagation. To accelerate computation for large-scale retrieval, a decentralized and synchronous feature propagation algorithm which supports parallel or distributed computing is introduced. In particular, the plain GCR is extended for cross-camera retrieval and an improved feature propagation formulation is presented to leverage affinity relationships across different cameras. It is also extended for video-based retrieval, and Graph Convolution based Re-ranking for Video (GCRV) is proposed by mathematically deriving a novel profile vector generation method for the tracklet. Without bells and whistles, the proposed approaches achieve state-of-the-art performances on seven benchmark datasets from three different tasks, i.e., image retrieval, person Re-ID and video-based person Re-ID.
C1 [Zhang, Yuqi; Qian, Qi; Chen, Weihua; Wang, Fan] Alibaba Grp, Beijing 100102, Peoples R China.
   [Wang, Hongsong] Southeast Univ, Dept Comp Sci & Engn, Nanjing 210096, Peoples R China.
   [Liu, Chong] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100190, Peoples R China.
C3 Alibaba Group; Southeast University - China; Chinese Academy of
   Sciences; Institute of Software, CAS
RP Wang, HS (corresponding author), Southeast Univ, Dept Comp Sci & Engn, Nanjing 210096, Peoples R China.
EM gongyou.zyq@alibaba-inc.com; qi.qian@alibaba-inc.com;
   hongsongwang@seu.edu.cn; liuchong@ios.ac.cn; kugang.cwh@alibaba-inc.com;
   fan.w@alibaba-inc.com
OI Wang, Fan/0000-0001-7320-1119; Wang, Hongsong/0000-0002-9464-1778
FU Southeast University Start-Up Grant for New Faculty
FX No Statement Available
CR Aich A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P152, DOI 10.1109/ICCV48922.2021.00022
   Bai S., 2022, P IEEECVF C COMPUTER, P7339
   Bai S, 2019, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2019.00083
   Bai S, 2017, IEEE I CONF COMP VIS, P774, DOI 10.1109/ICCV.2017.90
   Bai X, 2020, PATTERN RECOGN, V98, DOI 10.1016/j.patcog.2019.107036
   Bingyi Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P726, DOI 10.1007/978-3-030-58565-5_43
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   El-Nouby A, 2021, Arxiv, DOI [arXiv:2102.05644, DOI 10.48550/ARXIV.2102.05644]
   Gong X, 2022, IEEE T MULTIMEDIA, V24, P217, DOI 10.1109/TMM.2021.3050082
   Gu H., 2022, P IEEE CVF C COMP VI, P4744
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He TY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1470, DOI 10.1109/ICCV48922.2021.00152
   Jegou H, 2007, PROC CVPR IEEE, P9
   Ji P, 2017, ADV NEUR IN, V30
   Jin W, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P66, DOI 10.1145/3394486.3403049
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu CQ, 2019, PROCEEDINGS OF THE THIRD INTERNATIONAL SYMPOSIUM - EDUCATIONAL RESEARCH AND EDUCATIONAL TECHNOLOGY, 2019, P243
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Philbin J., 2007, PROC IEEE CONFCOMPUT, P1
   Porrello Angelo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P93, DOI 10.1007/978-3-030-58607-2_6
   Qin DF, 2011, PROC CVPR IEEE, P777, DOI 10.1109/CVPR.2011.5995373
   Radenovic F, 2019, IEEE T PATTERN ANAL, V41, P1655, DOI 10.1109/TPAMI.2018.2846566
   Radenovic F, 2018, PROC CVPR IEEE, P5706, DOI 10.1109/CVPR.2018.00598
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Sarlin PE, 2020, PROC CVPR IEEE, P4937, DOI 10.1109/CVPR42600.2020.00499
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen YT, 2018, PROC CVPR IEEE, P2265, DOI 10.1109/CVPR.2018.00241
   Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30
   Si TZ, 2022, PATTERN RECOGN, V124, DOI 10.1016/j.patcog.2021.108462
   Siméoni O, 2019, PROC CVPR IEEE, P11643, DOI 10.1109/CVPR.2019.01192
   Song YX, 2022, Arxiv, DOI arXiv:2207.00287
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410
   Tan F., 2021, P IEEE CVF INT C COM, p12 105
   Tang ZY, 2023, IEEE T MULTIMEDIA, V25, P7917, DOI 10.1109/TMM.2022.3231103
   Tian XM, 2011, IEEE MULTIMEDIA, V18, P12, DOI 10.1109/MMUL.2011.36
   Tolias Giorgos, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P460, DOI 10.1007/978-3-030-58452-8_27
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu H, 2022, AAAI CONF ARTIF INTE, P2703
   Wu H, 2022, PROC CVPR IEEE, P9479, DOI 10.1109/CVPR52688.2022.00927
   Wu H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11396, DOI 10.1109/ICCV48922.2021.01122
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Wu X., 2022, Pattern Recognit., V121
   Wu YM, 2020, IEEE T IMAGE PROCESS, V29, P8821, DOI 10.1109/TIP.2020.3001693
   Yan C, 2022, IEEE T MULTIMEDIA, V24, P1665, DOI 10.1109/TMM.2021.3069562
   Yan YC, 2020, PROC CVPR IEEE, P2896, DOI 10.1109/CVPR42600.2020.00297
   Yang M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11752, DOI 10.1109/ICCV48922.2021.01156
   Yao X., 2022, PatternRecognit., V129
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu ZX, 2022, IEEE T MULTIMEDIA, V24, P4482, DOI 10.1109/TMM.2021.3119133
   Zang XH, 2022, IEEE T IND INFORM, V18, P8776, DOI 10.1109/TII.2022.3151766
   Zhang T, 2019, PR MACH LEARN RES, V97
   Zhang T, 2019, LECT NOTES COMPUT SC, V11365, P466, DOI 10.1007/978-3-030-20873-8_30
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhang XM, 2020, Arxiv, DOI arXiv:2012.07620
   Zhang YQ, 2022, INT CONF ACOUST SPEE, P2704, DOI 10.1109/ICASSP43922.2022.9747298
   Zhang YQ, 2020, IEEE T IMAGE PROCESS, V29, P1001, DOI 10.1109/TIP.2019.2926208
   Zhao JN, 2021, PROC CVPR IEEE, P2225, DOI 10.1109/CVPR46437.2021.00226
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhou KY, 2022, IEEE T PATTERN ANAL, V44, P5056, DOI 10.1109/TPAMI.2021.3069237
   Zhu XG, 2021, IMAGE VISION COMPUT, V115, DOI 10.1016/j.imavis.2021.104289
   Zhu Y., 2021, arXiv
NR 82
TC 1
Z9 1
U1 4
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1089
EP 1101
DI 10.1109/TMM.2023.3276167
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700013
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhang, Z
   He, D
   Liu, S
   Xiao, BH
   Durrani, TS
AF Zhang, Zhong
   He, Di
   Liu, Shuang
   Xiao, Baihua
   Durrani, Tariq S.
TI Completed Part Transformer for Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Person ReID; transformer; adaptive refined tokens
ID NETWORK
AB Recently, part information of pedestrian images has been demonstrated to be effective for person re-identification (ReID), but the part interaction is ignored when using Transformer to learn long-range dependencies. In this article, we propose a novel transformer network named Completed Part Transformer (CPT) for person ReID, where we design the part transformer layer to learn the completed part interaction. The part transformer layer includes the intra-part layer and the part-global layer, where they consider long-range dependencies from the aspects of the intra-part interaction and the part-global interaction, simultaneously. Furthermore, in order to overcome the limitation of fixed number of the patch tokens in the transformer layer, we propose the Adaptive Refined Tokens (ART) module to focus on learning the interaction between the informative patch tokens in the pedestrian image, which improves the discrimination of the pedestrian representation. Extensive experimental results on four person ReID datasets, i.e., MSMT17, Market1501, DukeMTMC-reID, and CUHK03, demonstrate that the proposed method achieves a new state-of-the-art performance, e.g., it achieves 68.0% mAP and 84.6% Rank-1 accuracy on MSMT17.
C1 [Zhang, Zhong; He, Di; Liu, Shuang] Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tra, Tianjin 300387, Peoples R China.
   [Xiao, Baihua] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Durrani, Tariq S.] Univ Strathclyde, Dept Elect & Elect Engn, Glasgow G1 1XW, Scotland.
C3 Tianjin Normal University; Chinese Academy of Sciences; Institute of
   Automation, CAS; University of Strathclyde
RP Liu, S (corresponding author), Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tra, Tianjin 300387, Peoples R China.
EM zhong.zhang8848@gmail.com; clarkhedi@gmail.com;
   shuangliu.tjnu@gmail.com; baihua.xiao@ia.ac.cn; t.durrani@strath.ac.uk
OI xiao, bai hua/0000-0003-3941-1141; Durrani, Tariq/0000-0002-8813-6118;
   He, Di/0000-0002-3449-6935
FU National Natural Science Foundation of China
FX No Statement Available
CR Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973
   Chen TL, 2019, IEEE I CONF COMP VIS, P8350, DOI 10.1109/ICCV.2019.00844
   Chen WH, 2017, AAAI CONF ARTIF INTE, P3988
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Cheng D, 2020, IEEE T CYBERNETICS, V50, P561, DOI 10.1109/TCYB.2018.2869739
   Ding CX, 2022, IEEE T PATTERN ANAL, V44, P1474, DOI 10.1109/TPAMI.2020.3024900
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Feng YC, 2021, IEEE T CYBERNETICS, V51, P1849, DOI 10.1109/TCYB.2019.2909480
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Jia MX, 2023, IEEE T MULTIMEDIA, V25, P1294, DOI 10.1109/TMM.2022.3141267
   Jiang Y., 2021, P 35 C NEUR INF PROC, P14745
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li JN, 2022, IEEE T PATTERN ANAL, V44, P622, DOI 10.1109/TPAMI.2019.2929036
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P163, DOI 10.1145/3240508.3240573
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liao SC, 2021, ADV NEUR IN
   Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ma ZX, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P1487, DOI 10.1145/3474085.3475283
   Rao YM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1005, DOI 10.1109/ICCV48922.2021.00106
   Ren XA, 2023, IEEE T MULTIMEDIA, V25, P4387, DOI 10.1109/TMM.2022.3174768
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Si TZ, 2023, IEEE T MULTIMEDIA, V25, P4323, DOI 10.1109/TMM.2022.3174414
   Sun BY, 2022, IEEE T CYBERNETICS, V52, P738, DOI 10.1109/TCYB.2020.2979262
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang FQ, 2016, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2016.144
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang T, 2022, AAAI CONF ARTIF INTE, P2540
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xu FR, 2022, IEEE T CYBERNETICS, V52, P11014, DOI 10.1109/TCYB.2021.3105970
   Yang F, 2019, PATTERN RECOGN, V86, P143, DOI 10.1016/j.patcog.2018.08.015
   Yang Y, 2014, LECT NOTES COMPUT SC, V8689, P536, DOI 10.1007/978-3-319-10590-1_35
   Yao HT, 2019, IEEE T IMAGE PROCESS, V28, P2860, DOI 10.1109/TIP.2019.2891888
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang Q, 2022, IEEE T IMAGE PROCESS, V31, P352, DOI 10.1109/TIP.2021.3128330
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhang ZZ, 2020, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR42600.2020.00325
   Zhang Z, 2021, PROC CVPR IEEE, P12131, DOI 10.1109/CVPR46437.2021.01196
   Zhang Z, 2022, IEEE T CIRC SYST VID, V32, P1160, DOI 10.1109/TCSVT.2021.3074745
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhu HW, 2022, PROC CVPR IEEE, P4682, DOI 10.1109/CVPR52688.2022.00465
   ZHU K, 2020, ECCV, DOI DOI 10.1007/978-3-030-58580-8_21
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 62
TC 4
Z9 4
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2303
EP 2313
DI 10.1109/TMM.2023.3294816
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100027
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Zhou, SL
   Tan, WM
   Yan, B
AF Zhou, Shili
   Tan, Weimin
   Yan, Bo
TI A Motion Distillation Framework for Video Frame Interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Computational modeling; Optical flow; Kernel; Interpolation;
   Motion estimation; Correlation; Deep learning; frame interpolation;
   knowledge distillation; optical flow
ID IMAGE QUALITY; OPTICAL-FLOW; ENHANCEMENT
AB In recent years, we have seen the success of deep video enhancement models. However, the performance improvement of new methods has gradually entered a bottleneck period. Optimizing model structures or increasing training data brings less and less improvement. We argue that existing models with advanced structures have not fully demonstrated their performance and demand further exploration. In this study, we statistically analyze the relationship between motion estimation accuracy and video interpolation quality of existing video frame interpolation methods, and find that only supervising the final output leads to inaccurate motion and further affects the interpolation performance. Based on this important observation, we propose a general motion distillation framework that can be widely applied to flow-based and kernel-based video frame interpolation methods. Specifically, we begin by training a teacher model, which uses the ground-truth target frame and adjacent frames to estimate motion. These motion estimates then guide the training of a student model for video frame interpolation. Our experimental results demonstrate the effectiveness of this approach in enhancing performance across diverse advanced video interpolation model structures. For example, after applying our motion distillation framework, the CtxSyn model achieves a PSNR gain of 3.047 dB.
C1 [Zhou, Shili; Tan, Weimin; Yan, Bo] Fudan Univ, Shanghai Collaborat Innovat Ctr Intelligent Visual, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
C3 Fudan University
RP Tan, WM; Yan, B (corresponding author), Fudan Univ, Shanghai Collaborat Innovat Ctr Intelligent Visual, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai 201203, Peoples R China.
EM 19110240004@fudan.edu.cn; wmtan@fudan.edu.cn; byan@fudan.edu.cn
FU NSFC
FX No Statement Available
CR Ates HF, 2013, OPT ENG, V52, DOI 10.1117/1.OE.52.7.071505
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Butler D., 2012, Tech. Rep. MPI-IS-TR-006
   Chen H, 2011, IEEE DATA COMPR CONF, P449, DOI 10.1109/DCC.2011.53
   Cheng XH, 2022, IEEE T PATTERN ANAL, V44, P7029, DOI 10.1109/TPAMI.2021.3100714
   Choi J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3363550
   Choi M, 2020, AAAI CONF ARTIF INTE, V34, P10663
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Ding TY, 2021, PROC CVPR IEEE, P7997, DOI 10.1109/CVPR46437.2021.00791
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Ersanilli E., 2020, arXiv, DOI 10.31234/osf.io/g7rzq
   Heo B, 2019, AAAI CONF ARTIF INTE, P3779
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hu P, 2022, PROC CVPR IEEE, P3543, DOI 10.1109/CVPR52688.2022.00354
   Huang ZW, 2022, LECT NOTES COMPUT SC, V13674, P624, DOI 10.1007/978-3-031-19781-9_36
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Inamoto N, 2007, IEEE T MULTIMEDIA, V9, P1155, DOI 10.1109/TMM.2007.902832
   James Jerin Geo, 2023, P IEEE CVF WINT C AP, P5078
   Janai J, 2018, LECT NOTES COMPUT SC, V11220, P713, DOI 10.1007/978-3-030-01270-0_42
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Junheum Park, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P109, DOI 10.1007/978-3-030-58568-6_7
   Kalluri T, 2023, IEEE WINT CONF APPL, P2070, DOI 10.1109/WACV56688.2023.00211
   Kingma D., 2015, P INT C LEARN REPR S, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Kong LT, 2022, PROC CVPR IEEE, P1968, DOI 10.1109/CVPR52688.2022.00201
   Kuroki Y, 2007, J SOC INF DISPLAY, V15, P61, DOI 10.1889/1.2451560
   Kuroki Y, 2014, J SOC INF DISPLAY, V22, P191, DOI 10.1002/jsid.237
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li C, 2018, 2018 INTERNATIONAL CONFERENCE ON AUDIO, LANGUAGE AND IMAGE PROCESSING (ICALIP), P132, DOI 10.1109/ICALIP.2018.8455233
   Liu L, 2020, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR42600.2020.00652
   Liu PP, 2019, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2019.00470
   Liu PP, 2019, AAAI CONF ARTIF INTE, P8770
   Liu YL, 2019, AAAI CONF ARTIF INTE, P8794
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCV.2017.478, 10.1109/ICCVW.2017.361]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Loshchilov I, 2019, Arxiv, DOI [arXiv:1711.05101, 10.48550/arXiv.1711.05101, DOI 10.48550/ARXIV.1711.05101]
   Luo KM, 2021, PROC CVPR IEEE, P1045, DOI 10.1109/CVPR46437.2021.00110
   Mathieu M., 2015, ICLR
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Meister S, 2018, AAAI CONF ARTIF INTE, P7251
   Meyer S, 2018, PROC CVPR IEEE, P498, DOI 10.1109/CVPR.2018.00059
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Meyer Simone., 2018, BMVC
   Morris C, 2023, Arxiv, DOI arXiv:2302.08455
   Niklaus S, 2020, PROC CVPR IEEE, P5436, DOI 10.1109/CVPR42600.2020.00548
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ranzato M, 2016, Arxiv, DOI [arXiv:1412.6604, DOI 10.48550/ARXIV.1412.6604]
   Reda FA, 2018, LECT NOTES COMPUT SC, V11211, P747, DOI 10.1007/978-3-030-01234-2_44
   Reda FA, 2019, IEEE I CONF COMP VIS, P892, DOI 10.1109/ICCV.2019.00098
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Ren Z, 2017, AAAI CONF ARTIF INTE, P1495
   Romero A, 2015, Arxiv, DOI arXiv:1412.6550
   Shishido H, 2019, J VIS COMMUN IMAGE R, V62, P68, DOI 10.1016/j.jvcir.2019.04.010
   Shurui Gui, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14001, DOI 10.1109/CVPR42600.2020.01402
   Sim H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14469, DOI 10.1109/ICCV48922.2021.01422
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Tu ZG, 2022, IEEE T IMAGE PROCESS, V31, P6517, DOI 10.1109/TIP.2022.3212905
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Usman M, 2016, IEEE T MULTIMEDIA, V18, P831, DOI 10.1109/TMM.2016.2537200
   Wang Y, 2018, PROC CVPR IEEE, P4884, DOI 10.1109/CVPR.2018.00513
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CY, 2018, LECT NOTES COMPUT SC, V11212, P425, DOI 10.1007/978-3-030-01237-3_26
   Xia T., 2017, PROC 9 INT C GRAPHIC
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yamada K, 2013, IEEE IMAGE PROC, P2072, DOI 10.1109/ICIP.2013.6738427
   Yan B, 2021, IEEE T BROADCAST, V67, P174, DOI 10.1109/TBC.2020.3028323
   Yang YT, 2007, IEEE T CIRC SYST VID, V17, P1700, DOI 10.1109/TCSVT.2007.903806
   Yu JJ, 2016, LECT NOTES COMPUT SC, V9915, P3, DOI 10.1007/978-3-319-49409-8_1
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
NR 78
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3728
EP 3740
DI 10.1109/TMM.2023.3314971
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200013
DA 2024-08-05
ER

PT J
AU Zhu, YX
   Zhao, WL
   Tang, YS
   Rao, YM
   Zhou, J
   Lu, JW
AF Zhu, Yixuan
   Zhao, Wenliang
   Tang, Yansong
   Rao, Yongming
   Zhou, Jie
   Lu, Jiwen
TI StableSwap: Stable Face Swapping in a Shared and Controllable Latent
   Space
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face swapping; latent manipulation; controllable editing
ID IMAGE; MANIPULATION
AB Person-agnostic face swapping has gained significant attention in recent years, as it offers the potential to enhance various real-world applications by combining high fidelity and identity consistency. However, conventional face swapping methods often rely on intricate adjustments of different loss functions, leading to instability during both the training and inference stages. In this work, we propose a simple yet effective framework named StableSwap with a reversible autoencoder to modify the face in a shared latent space. Our approach capitalizes on the information-rich image latent codes to tackle the challenges of complex editing tasks, utilizing the abundant details present in both the source and target faces. To ensure an expressive and robust latent space, we employ a latent alignment approach with perceptual and adversarial losses to optimize the autoencoder. Additionally, we devise a multi-stage identity injection module that samples multiple features with different facial priors and incorporates them to guide the latent image manipulation. By leveraging attention-based blocks, we fuse these futures and update the latent code in a mask-conditioned manner. Both quantitative and qualitative results on the mainstream benchmarks demonstrate that our StableSwap generates competitive identity-consistent swapped faces compared with state-of-the-art methods. Our method outperforms previous approaches in terms of ID Retrieval (98.68) and FID (2.49), while also exhibiting enhanced stability during model training. Beyond this, our model achieves region-controllable face swapping with the capability to perform more fine-grained operations in latent space.
C1 [Zhu, Yixuan; Zhao, Wenliang; Rao, Yongming; Zhou, Jie; Lu, Jiwen] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Tang, Yansong] Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Tang, YS (corresponding author), Tsinghua Univ, Shenzhen Int Grad Sch, Shenzhen 518055, Peoples R China.
EM zhuyx22@mails.tsinghua.edu.cn; zhaowl20@mails.tsinghua.edu.cn;
   tang.yansong@sz.tsinghua.edu.cn; raoyongming95@gmail.com;
   jzhou@tsinghua.edu.cn; lujiwen@tsinghua.edu.cn
RI ; Lu, Jiwen/C-5291-2009
OI Rao, Yongming/0000-0003-3952-8753; Lu, Jiwen/0000-0002-6121-5529
FU National Natural Science Foundation of China
FX No Statement Available
CR Avrahami O, 2022, PROC CVPR IEEE, P18187, DOI 10.1109/CVPR52688.2022.01767
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Blanz V, 2004, COMPUT GRAPH FORUM, V23, P669, DOI 10.1111/j.1467-8659.2004.00799.x
   Brock A., 2018, P INT C LEARN REPR
   Chen RW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2003, DOI 10.1145/3394171.3413630
   Cui Kaiwen, 2023, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P668, DOI 10.1109/CVPRW59228.2023.00074
   DeepFake, 2019, About us
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng Y, 2019, IEEE COMPUT SOC CONF, P285, DOI 10.1109/CVPRW.2019.00038
   Ding F, 2021, IEEE T MULTIMEDIA, V24, P3429, DOI 10.1109/TMM.2021.3098422
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   FaceSwap, 2019, About us
   Gao GG, 2021, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR46437.2021.00341
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3409, DOI 10.1109/TMM.2022.3160360
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, ADV NEUR IN, V34
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Kim J, 2022, PROC CVPR IEEE, P10769, DOI 10.1109/CVPR52688.2022.01051
   Kim K, 2022, Arxiv, DOI arXiv:2212.13344
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Li J, 2021, PROC CVPR IEEE, P5085, DOI 10.1109/CVPR46437.2021.00505
   Li LZ, 2020, PROC CVPR IEEE, P5073, DOI 10.1109/CVPR42600.2020.00512
   Liu MY, 2019, IEEE I CONF COMP VIS, P10550, DOI 10.1109/ICCV.2019.01065
   Liu ZA, 2023, PROC CVPR IEEE, P8578, DOI 10.1109/CVPR52729.2023.00829
   Loshchilov I., 2018, INT C LEARN REPR
   Luo YC, 2022, LECT NOTES COMPUT SC, V13676, P297, DOI 10.1007/978-3-031-19787-1_17
   Meng C., 2021, PROC INT C LEARN REP
   Natsume R., 2018, P ACM SIGGRAPH, P1
   Natsume R, 2019, LECT NOTES COMPUT SC, V11366, P117, DOI 10.1007/978-3-030-20876-9_8
   Nirkin Y, 2019, IEEE I CONF COMP VIS, P7183, DOI 10.1109/ICCV.2019.00728
   Nirkin Y, 2018, IEEE INT CONF AUTOMA, P98, DOI 10.1109/FG.2018.00024
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Paszke A, 2019, ADV NEUR IN, V32
   Ren XH, 2023, IEEE I CONF COMP VIS, P20608, DOI 10.1109/ICCV51070.2023.01889
   Rössler A, 2019, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2019.00009
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Shiohara K, 2023, IEEE I CONF COMP VIS, P7600, DOI 10.1109/ICCV51070.2023.00702
   Song Y., 2020, PROC INT C LEARN REP
   Nguyen TT, 2022, COMPUT VIS IMAGE UND, V223, DOI 10.1016/j.cviu.2022.103525
   van den Oord A, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Vemulapalli R, 2019, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2019.00583
   Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang YH, 2021, Arxiv, DOI arXiv:2106.09965
   Xu C, 2022, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR52688.2022.00748
   Xu YY, 2022, PROC CVPR IEEE, P7632, DOI 10.1109/CVPR52688.2022.00749
   Xu ZL, 2022, LECT NOTES COMPUT SC, V13674, P661, DOI 10.1007/978-3-031-19781-9_38
   Zhao WL, 2023, PROC CVPR IEEE, P8568, DOI 10.1109/CVPR52729.2023.00828
   Zhu YH, 2021, PROC CVPR IEEE, P4832, DOI 10.1109/CVPR46437.2021.00480
NR 57
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7594
EP 7607
DI 10.1109/TMM.2024.3369853
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000048
DA 2024-08-05
ER

PT J
AU Chen, DH
   Zhang, RT
AF Chen, Donghua
   Zhang, Runtong
TI Building Multimodal Knowledge Bases With Multimodal Computational
   Sequences and Generative Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognition; Generative adversarial networks; Data models; Visualization;
   Feature extraction; Databases; Computational modeling; Decision support
   systems; deep learning; generative adversarial networks; knowledge
   representation; multimodal data
ID INFORMATION FUSION
AB Conventional knowledge graphs (KGs) are composed solely of entities, attributes, and relationships, which poses challenges for enhancing multimodal knowledge representation and reasoning. To address the issue, this article proposes a multimodal deep learning-based approach to build a multimodal knowledge base (MMKB) for better multimodal feature (MMF) utilization. First, we construct a multimodal computation sequence (MCS) model for structured multimodal data storage. Then, we propose multimodal node, relationship, and dictionary models to enhance multimodal knowledge representation. Various feature extractors are used to extract MMFs from text, audio, image, and video data. Finally, we leverage generative adversarial networks (GANs) to facilitate MMF representation and update the MMKB dynamically. We examine the performance of the proposed method by using three multimodal datasets. BOW-, LBP-, Volume-, and VGGish-based feature extractors outperform the other methods by reducing at least 1.13%, 22.14%, 39.87, and 5.65% of the time cost, respectively. The average time costs of creating multimodal indexes improve by approximately 55.07% and 68.60% exact matching rates compared with the baseline method, respectively. The deep learning-based autoencoder method reduces the search time cost by 98.90% after using the trained model, outperforming the state-of-the-art methods. In terms of multimodal data representation, the GAN-CNN models achieve an average correct rate of 82.70%. Our open-source work highlights the importance of flexible MMF utilization in multimodal KGs, leading to more powerful and diverse applications that can leverage different types of data.
C1 [Chen, Donghua; Zhang, Runtong] Univ Int Business & Econ, Sch Informat Technol & Management, Dept Artificial Intelligence, Beijing 100029, Peoples R China.
C3 University of International Business & Economics
RP Zhang, RT (corresponding author), Univ Int Business & Econ, Sch Informat Technol & Management, Dept Artificial Intelligence, Beijing 100029, Peoples R China.
EM dhchen@uibe.edu.cn; rtzhang@bjtu.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Armstrong H. L., 2021, Encyclopedia of Sex and Sexuality: Understanding Biology, Psychology, and Culture, P343
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Balabin H, 2022, BIOINFORMATICS, V38, P1648, DOI 10.1093/bioinformatics/btac001
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bellandi V, 2022, BIG DATA, V10, P408, DOI 10.1089/big.2021.0326
   Vásquez-Correa JC, 2019, IEEE J BIOMED HEALTH, V23, P1618, DOI 10.1109/JBHI.2018.2866873
   Cao WP, 2023, WIRES DATA MIN KNOWL, V13, DOI 10.1002/widm.1488
   Cao Y, 2014, CANCER INFORM, V13, P125, DOI 10.4137/CIN.S14053
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chandak P, 2023, SCI DATA, V10, DOI 10.1038/s41597-023-01960-3
   Chen JY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1295, DOI 10.1145/3394486.3403182
   Chen Q, 2021, EXPERT SYST APPL, V177, DOI 10.1016/j.eswa.2021.114939
   Chen YF, 2021, APPL SOFT COMPUT, V112, DOI 10.1016/j.asoc.2021.107788
   Cheng B, 2022, NEUROCOMPUTING, V500, P581, DOI 10.1016/j.neucom.2022.05.058
   Cheng ZY, 2016, MULTIMEDIA SYST, V22, P509, DOI 10.1007/s00530-014-0432-7
   Chiou MJ, 2021, IEEE ACCESS, V9, P50441, DOI 10.1109/ACCESS.2021.3069041
   Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202
   Curry E, 2022, IEEE INTERNET THINGS, V9, P13705, DOI 10.1109/JIOT.2022.3143171
   Dai YL, 2023, INFORM SCIENCES, V623, P164, DOI 10.1016/j.ins.2022.12.014
   Ding Y, 2022, PROC CVPR IEEE, P5079, DOI 10.1109/CVPR52688.2022.00503
   Fei NY, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-30761-2
   Feng DD, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3580501
   Gao N, 2022, ACM T INTEL SYST TEC, V13, DOI 10.1145/3474838
   Garau N, 2022, EXPERT SYST APPL, V202, DOI 10.1016/j.eswa.2022.117172
   Golovanevsky M, 2022, J AM MED INFORM ASSN, V29, P2014, DOI 10.1093/jamia/ocac168
   Gou FF, 2022, HEALTHCARE-BASEL, V10, DOI 10.3390/healthcare10112189
   Han C., 2020, P 28 INT C COMP LING, P3107, DOI [DOI 10.18653/V1/2020.COLINGMAIN.277, 10.18653/v1/2020.coling-main.277]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Holzinger A, 2021, INFORM FUSION, V71, P28, DOI 10.1016/j.inffus.2021.01.008
   Hossain E, 2022, IEEE ACCESS, V10, P46538, DOI 10.1109/ACCESS.2022.3170897
   Hu He, 2022, 2022 7th International Conference on Intelligent Computing and Signal Processing (ICSP), P1097, DOI 10.1109/ICSP54964.2022.9778820
   Huang Yan, 2022, ADV NEURAL INFO PROC, V35, P7892
   Huang ZC, 2023, APPL INTELL, V53, P3652, DOI 10.1007/s10489-022-03667-1
   Hussain A., 2022, P 16 INT C SIGN IM T, P224
   Jabeen S, 2023, ACM T MULTIM COMPUT, V19, DOI 10.1145/3545572
   Ji Z, 2020, KNOWL-BASED SYST, V197, DOI 10.1016/j.knosys.2020.105847
   Jiang YY, 2020, INFORM FUSION, V53, P209, DOI 10.1016/j.inffus.2019.06.019
   Johannesson P., 2014, An Introduction to Design Science, DOI [10.1007/978-3-319-10632-8, DOI 10.1007/978-3-319-10632-8]
   Kannan AV, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P3417, DOI 10.1145/3340531.3417439
   Kesorn K, 2012, IEEE T MULTIMEDIA, V14, P211, DOI 10.1109/TMM.2011.2170665
   Kim Taeksoo, 2017, P 34 INT C MACHINE L, P1857, DOI DOI 10.1109/WPT.2017.7953894
   Lee SI, 2020, J SUPERCOMPUT, V76, P8294, DOI 10.1007/s11227-019-03101-3
   Li WB, 2022, SCI DATA, V9, DOI 10.1038/s41597-022-01557-2
   Liang K, 2022, Arxiv, DOI [arXiv:2212.05767, DOI 10.48550/ARXIV.2212.05767]
   Liu L, 2022, EXPERT SYST APPL, V204, DOI 10.1016/j.eswa.2022.117361
   Lu JH, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3323214
   Middya AI, 2022, KNOWL-BASED SYST, V244, DOI 10.1016/j.knosys.2022.108580
   Naik B, 2020, VIS COMPUT IND BIOME, V3, DOI 10.1186/s42492-020-00062-w
   Pandeya YR, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144927
   Park Sung-Soo, 2022, Personal and Ubiquitous Computing, V26, P355, DOI 10.1007/s00779-019-01261-w
   Pelka O, 2018, LECT NOTES COMPUT SC, V11043, P180, DOI 10.1007/978-3-030-01364-6_20
   Peng J, 2022, IEEE T MULTIMEDIA, V24, P4356, DOI 10.1109/TMM.2021.3116416
   Pezeshkpour P, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P3208
   Qi MS, 2019, PROC CVPR IEEE, P5232, DOI 10.1109/CVPR.2019.00538
   Qiao YY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2085, DOI 10.1145/3474085.3475363
   Qin PD, 2020, AAAI CONF ARTIF INTE, V34, P8673
   Rahate A, 2022, INFORM FUSION, V81, P203, DOI 10.1016/j.inffus.2021.12.003
   Sapinski T, 2019, LECT NOTES COMPUT SC, V11188, P153, DOI 10.1007/978-3-030-05792-3_15
   Sergieh H. M., 2018, P 7 JOINT C LEX COMP, P225, DOI DOI 10.18653/V1/S18-2027
   Speggiorin A, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P3240, DOI 10.1145/3477495.3531679
   Srivastava N, 2014, J MACH LEARN RES, V15, P2949
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Teed Z., 2020, EUR C COMP VIS, P402, DOI DOI 10.1007/978-3-030-58536-524
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Velana Maria., 2016, IAPR Workshop on Multimodal Pattern Recognition of Social Signals in Human-Computer Interaction, P127
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang FJ, 2022, INT CONF SIGN PROCES, P81, DOI 10.1109/ICSP56322.2022.9965282
   Wang HW, 2021, IEEE T KNOWL DATA EN, V33, P3090, DOI 10.1109/TKDE.2019.2961882
   Wang J., 2022, P IEEE INT C BIOINF, P567
   Wang M, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2735, DOI 10.1145/3474085.3475470
   Wang Q, 2020, KNOWL-BASED SYST, V209, DOI 10.1016/j.knosys.2020.106421
   Wang X., 2022, P IEEE INT C MULT EX, P1
   Wang ZK, 2019, IEEE IJCNN, DOI 10.1109/ijcnn.2019.8852079
   Wang ZH, 2021, IEEE J BIOMED HEALTH, V25, P2193, DOI 10.1109/JBHI.2020.3037027
   Wu F, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107335
   Wu YX, 2023, IEEE T MULTIMEDIA, V25, P3113, DOI 10.1109/TMM.2022.3155900
   Xiao SG, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22207918
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xu DR, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3857, DOI 10.1145/3503161.3548388
   Xu W, 2020, BRIT J EDUC TECHNOL, V51, P1734, DOI 10.1111/bjet.12951
   Yan YF, 2021, NEUROCOMPUTING, V429, P69, DOI 10.1016/j.neucom.2020.10.108
   Yang S., 2021, IJCAI, P3978, DOI 10.24963/ijcai
   Zadeh A, 2020, INFORM FUSION, V64, P188, DOI 10.1016/j.inffus.2020.06.001
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5642
   Zeb Adnan, 2022, Knowledge-Based Systems, DOI 10.1016/j.knosys.2022.109451
   Zhang C, 2020, IEEE J-STSP, V14, P478, DOI 10.1109/JSTSP.2020.2987728
   Zhang HK, 2023, IEEE T MULTIMEDIA, V25, P2430, DOI 10.1109/TMM.2022.3147064
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhang L, 2022, IEEE T MULTIMEDIA, V24, P1830, DOI 10.1109/TMM.2021.3073267
   Zhang NX, 2023, DATA MIN KNOWL DISC, V37, P521, DOI 10.1007/s10618-022-00888-3
   Zhang SQ, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12157434
   Zhang WQ, 2022, AAAI CONF ARTIF INTE, P3335
   Zhang YQ, 2019, PROC INT CONF DATA, P614, DOI 10.1109/ICDE.2019.00061
   Zhou SP, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1322, DOI 10.1145/3343031.3351059
   Zia T, 2021, NEUROCOMPUTING, V461, P543, DOI 10.1016/j.neucom.2021.04.128
NR 96
TC 1
Z9 1
U1 27
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2027
EP 2040
DI 10.1109/TMM.2023.3291503
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IM2J4
UT WOS:001166674800038
DA 2024-08-05
ER

PT J
AU Chen, S
   Atapour-Abarghouei, A
   Shum, HPH
AF Chen, Shuang
   Atapour-Abarghouei, Amir
   Shum, Hubert P. H.
TI HINT: High-Quality INpainting Transformer With Mask-Aware Encoding and
   Enhanced Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image inpainting; transformer; representation learning
AB Existing image inpainting methods leverage convolution-based downsampling approaches to reduce spatial dimensions. This may result in information loss from corrupted images where the available information is inherently sparse, especially for the scenario of large missing regions. Recent advances in self-attention mechanisms within transformers have led to significant improvements in many computer vision tasks including inpainting. However, limited by the computational costs, existing methods cannot fully exploit the efficacy of long-range modelling capabilities of such models. In this paper, we propose an end-to-end High-quality INpainting Transformer, abbreviated as HINT, which consists of a novel mask-aware pixel-shuffle downsampling module (MPD) to preserve the visible information extracted from the corrupted image while maintaining the integrity of the information available for high-level inferences made within the model. Moreover, we propose a Spatially-activated Channel Attention Layer (SCAL), an efficient self-attention mechanism interpreting spatial awareness to model the corrupted image at multiple scales. To further enhance the effectiveness of SCAL, motivated by recent advanced in speech recognition, we introduce a sandwich structure that places feed-forward networks before and after the SCAL module. We demonstrate the superior performance of HINT compared to contemporary state-of-the-art models on four datasets, CelebA, CelebA-HQ, Places2, and Dunhuang.
C1 [Chen, Shuang; Atapour-Abarghouei, Amir; Shum, Hubert P. H.] Univ Durham, Durham DH1 3LE, England.
C3 Durham University
RP Shum, HPH (corresponding author), Univ Durham, Durham DH1 3LE, England.
EM shuang.chen@durham.ac.uk; amir.atapour-abarghouei@durham.ac.uk;
   hubert.shum@durham.ac.uk
RI ; Shum, Hubert P. H./E-8060-2015
OI Chen, Shuang/0000-0002-6879-7285; Shum, Hubert P. H./0000-0001-5651-6039
FU EPSRC NortHFutures
FX No Statement Available
CR Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   Atapour-Abarghouei A., 2019, RGB-D Image Analysis and Processing, P15
   Atapour-Abarghouei A, 2016, INT C PATT RECOG, P2813, DOI 10.1109/ICPR.2016.7900062
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Cao CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14489, DOI 10.1109/ICCV48922.2021.01424
   Chang ZY, 2023, Arxiv, DOI [arXiv:2306.04542, 10.48550/arXiv.2306.04542, DOI 10.48550/ARXIV.2306.04542]
   Chen S, 2022, 2022 IEEE-EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS (BHI) JOINTLY ORGANISED WITH THE IEEE-EMBS INTERNATIONAL CONFERENCE ON WEARABLE AND IMPLANTABLE BODY SENSOR NETWORKS (BSN'22), DOI 10.1109/BHI56158.2022.9926917
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deng Y, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P6559, DOI 10.1145/3503161.3548446
   Deng Y, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2529, DOI 10.1145/3474085.3475426
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Gulati A, 2020, INTERSPEECH, P5036, DOI 10.21437/Interspeech.2020-3015
   Guo Q, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P386, DOI 10.1145/3474085.3475170
   Guo XF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14114, DOI 10.1109/ICCV48922.2021.01387
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Jang SI, 2024, IEEE T MED IMAGING, V43, P2036, DOI 10.1109/TMI.2023.3336237
   Jingyuan Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7757, DOI 10.1109/CVPR42600.2020.00778
   Jo Y, 2019, IEEE I CONF COMP VIS, P1745, DOI 10.1109/ICCV.2019.00183
   Karras T., 2018, INT C LEARNING REPRE
   Kingma D., 2014, P INT C LEARN REPR, P1
   Li M., 2023, P AAAI C ART INT, P1368
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li XG, 2022, PROC CVPR IEEE, P1859, DOI 10.1109/CVPR52688.2022.00191
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lugmayr A, 2022, PROC CVPR IEEE, P11451, DOI 10.1109/CVPR52688.2022.01117
   Nazeri K, 2019, IEEE INT CONF COMP V, P3265, DOI 10.1109/ICCVW.2019.00408
   Ni MH, 2023, PROC CVPR IEEE, P14183, DOI 10.1109/CVPR52729.2023.01363
   Park DS, 2020, INT CONF ACOUST SPEE, P6879, DOI [10.1109/ICASSP40776.2020.9053205, 10.1109/icassp40776.2020.9053205]
   Park DS, 2019, INTERSPEECH, P2613, DOI 10.21437/Interspeech.2019-2680
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Peng JL, 2021, PROC CVPR IEEE, P10770, DOI 10.1109/CVPR46437.2021.01063
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Rombach R, 2022, PROC CVPR IEEE, P10674, DOI 10.1109/CVPR52688.2022.01042
   Sharif SMA, 2021, IEEE COMPUT SOC CONF, P233, DOI 10.1109/CVPRW53098.2021.00032
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Sridevi G, 2019, CIRC SYST SIGNAL PR, V38, P3802, DOI 10.1007/s00034-019-01029-w
   Sun HY, 2023, IEEE T MULTIMEDIA, V25, P4240, DOI 10.1109/TMM.2022.3174413
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Uddin SMN, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20113204
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4672, DOI 10.1109/ICCV48922.2021.00465
   Wang LS, 2023, IEEE T PATTERN ANAL, V45, P9072, DOI 10.1109/TPAMI.2022.3225382
   Wang LG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4781, DOI 10.1109/ICCV48922.2021.00476
   Wang N, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3748
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei JJ, 2019, COMPUT GRAPH FORUM, V38, P381, DOI 10.1111/cgf.13845
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Xiao T, 2021, ADV NEUR IN, V34
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu T, 2020, AAAI CONF ARTIF INTE, V34, P12733
   Yu TX, 2019, Arxiv, DOI arXiv:1907.04589
   Yu YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P69, DOI 10.1145/3474085.3475436
   Yu YC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14094, DOI 10.1109/ICCV48922.2021.01385
   Yue ZS, 2021, PROC CVPR IEEE, P642, DOI 10.1109/CVPR46437.2021.00070
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zeng YH, 2019, PROC CVPR IEEE, P1486, DOI 10.1109/CVPR.2019.00158
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang RS, 2023, IEEE T MULTIMEDIA, V25, P7299, DOI 10.1109/TMM.2022.3219728
   Zhang Y, 2023, IEEE T MULTIMEDIA, V25, P7967, DOI 10.1109/TMM.2022.3229968
   Zhang YL, 2024, IEEE T MULTIMEDIA, V26, P1539, DOI 10.1109/TMM.2023.3282892
   Zhao GM, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3476
   Zheng CAX, 2022, PROC CVPR IEEE, P11502, DOI 10.1109/CVPR52688.2022.01122
   Zheng HT, 2022, LECT NOTES COMPUT SC, V13676, P277, DOI 10.1007/978-3-031-19787-1_16
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou YQ, 2020, AAAI CONF ARTIF INTE, V34, P13074
NR 72
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7649
EP 7660
DI 10.1109/TMM.2024.3369897
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000051
OA Green Accepted, Green Submitted
DA 2024-08-05
ER

PT J
AU Ding, BS
   Zhang, RH
   Xu, LX
   Liu, GY
   Yang, S
   Liu, YM
   Zhang, Q
AF Ding, Bosheng
   Zhang, Ruiheng
   Xu, Lixin
   Liu, Guanyu
   Yang, Shuo
   Liu, Yumeng
   Zhang, Qi
TI U<SUP>2</SUP>D<SUP>2</SUP>Net: Unsupervised Unified Image Dehazing and
   Denoising Network for Single Hazy Image Enhancement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Haze removal; noise suppression; unsupervised learning
AB Hazy images captured under ill-posed scenarios with scattering medium (i.e. haze, fog, or smoke) are contaminated in visibility. Inevitably, these images are further degraded by noises owing to real-world imaging. Most existing hazy image enhancement methods perform image dehazing and denoising stage by stage, with the undesirable result that the estimation error of the former stage has to be propagated and amplified in the latter stage, e.g., noise amplification after dehazing. To address this inconsistent degradation, we present an Unsupervised Unified Image Dehazing and Denoising Network, U(2)D(2)Net, to remove the haze and suppress the noise simultaneously for a single hazy image. U(2)D(2)Net is mainly comprised of an unsupervised dehazing module, an unsupervised denoising module, and a region-similarity fusion strategy. Specifically, we propose an unsupervised transmission-aware dehazing module to restore visibility and suppress depth-dependent noise propagation in the dehazing module. Besides, we design an unsupervised network with a Mean/Max Sub-Sampler in the denoising module. To exploit the correlation and complementary between the previous outputs, a region-similarity fusion strategy is developed to compute the final qualified result. Extensive experiments on both synthetic and real-world datasets illustrate that U(2)D(2)Net outperforms other state-of-the-art dehazing and denoising methods in terms of PSNR, SSIM, and subjective visual effects.
C1 [Ding, Bosheng; Zhang, Ruiheng; Xu, Lixin; Liu, Guanyu] Beijing Inst Technol, Sch Mechatron Engn, State Key Lab Electromech Dynam Control, Beijing 100081, Peoples R China.
   [Liu, Yumeng] Chinese Acad Sci, Inst Software, Beijing Key Lab Human Comp Interact, Beijing 100190, Peoples R China.
   [Yang, Shuo] Univ Technol Sydney, Fac Engn & Informat, Sydney 2007, Australia.
   [Zhang, Qi] Tech Univ Munich, Sch Engn & Design, D-80333 Munich, Germany.
C3 Beijing Institute of Technology; Chinese Academy of Sciences; Institute
   of Software, CAS; University of Technology Sydney; Technical University
   of Munich
RP Zhang, RH (corresponding author), Beijing Inst Technol, Sch Mechatron Engn, State Key Lab Electromech Dynam Control, Beijing 100081, Peoples R China.
EM dbs2007@126.com; ruiheng.zhang@bit.edu.cn; lxxu@bit.edu.cn;
   3120220221@bit.edu.cn; shuo.yang@student.uts.edu.au; yumeng@iscas.ac.cn;
   rachelqi.zhang@tum.de
OI Liu, Yumeng/0000-0003-0927-9039; Zhang, Ruiheng/0000-0002-5460-7196;
   Yang, Shuo/0000-0001-6145-0150
FU STI 2030#x2014;Major Projects
FX No Statement Available
CR Batson J, 2019, PR MACH LEARN RES, V97
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Chen SM, 2022, PROC CVPR IEEE, P7602, DOI 10.1109/CVPR52688.2022.00746
   Cho W, 2014, INT CONF BIG DATA, P139, DOI 10.1109/BIGCOMP.2014.6741424
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Engin D, 2018, IEEE COMPUT SOC CONF, P938, DOI 10.1109/CVPRW.2018.00127
   Gu SH, 2019, IEEE I CONF COMP VIS, P2511, DOI 10.1109/ICCV.2019.00260
   Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   Huang T, 2021, PROC CVPR IEEE, P14776, DOI 10.1109/CVPR46437.2021.01454
   Jia XX, 2019, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2019.00621
   Jidesh P., 2014, J. Comput. Eng., V2014
   Joshi N., 2010, Proc. IEEE International Conference on Computational Photography (ICCP), P1
   Kaftory R., 2007, P IEEE CVF C COMP VI, P1
   Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223
   Kuanar S, 2022, SIGNAL PROCESS-IMAGE, V109, DOI 10.1016/j.image.2022.116839
   Kuanar S, 2022, VISUAL COMPUT, V38, P1121, DOI 10.1007/s00371-021-02071-z
   Lan X, 2013, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2013-86
   Lehtinen J, 2018, PR MACH LEARN RES, V80
   Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li LRH, 2020, IEEE T IMAGE PROCESS, V29, P2766, DOI 10.1109/TIP.2019.2952690
   Li PL, 2022, PATTERN RECOGN, V125, DOI 10.1016/j.patcog.2021.108506
   Liu Q, 2018, IEEE T IMAGE PROCESS, V27, P5178, DOI 10.1109/TIP.2018.2849928
   Liu XN, 2022, IEEE T MULTIMEDIA, V24, P3934, DOI 10.1109/TMM.2021.3110483
   Liu X, 2017, COMPUT VIS IMAGE UND, V162, P23, DOI 10.1016/j.cviu.2017.08.002
   Liu Y, 2019, IEEE I CONF COMP VIS, P2492, DOI 10.1109/ICCV.2019.00258
   Matlin E, 2012, PROC SPIE, V8296, DOI 10.1117/12.906773
   Nagaraja Kumar N., 2022, J. Inf. Knowl. Manage., V22
   Nie J., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.2022.3199553, DOI 10.1109/TMM.2022.3199553]
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qin X, 2020, AAAI CONF ARTIF INTE, V34, P11908
   Quan YH, 2020, PROC CVPR IEEE, P1887, DOI 10.1109/CVPR42600.2020.00196
   Riya, 2021, COMPUT MATH APPL, V93, P106, DOI 10.1016/j.camwa.2021.03.029
   Salazar-Colores S, 2019, IEEE T IMAGE PROCESS, V28, P2357, DOI 10.1109/TIP.2018.2885490
   Schechner YY, 2007, IEEE T PATTERN ANAL, V29, P1655, DOI 10.1109/TPAMI.2007.1141
   Shen LH, 2019, IEEE T MULTIMEDIA, V21, P1093, DOI 10.1109/TMM.2018.2871955
   Tian CW, 2023, PATTERN RECOGN, V134, DOI 10.1016/j.patcog.2022.109050
   Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang AN, 2019, IEEE T IMAGE PROCESS, V28, P381, DOI 10.1109/TIP.2018.2868567
   Wei YY, 2021, IEEE T IMAGE PROCESS, V30, P4788, DOI 10.1109/TIP.2021.3074804
   Wu QB, 2020, IEEE T IMAGE PROCESS, V29, P2583, DOI 10.1109/TIP.2019.2949392
   Wu QB, 2020, IEEE T IMAGE PROCESS, V29, P1788, DOI 10.1109/TIP.2019.2942504
   Yang MM, 2020, SIGNAL PROCESS-IMAGE, V83, DOI 10.1016/j.image.2019.115777
   Yang XH, 2020, IEEE T IMAGE PROCESS, V29, P5038, DOI 10.1109/TIP.2020.2978645
   Ye J, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102197
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837
   Zhang Q, 2021, REMOTE SENS ENVIRON, V264, DOI 10.1016/j.rse.2021.112575
   Zhang RH, 2022, NEUROCOMPUTING, V470, P247, DOI 10.1016/j.neucom.2021.10.110
   Zhang RH, 2022, IEEE T MULTIMEDIA, V24, P1735, DOI 10.1109/TMM.2021.3070138
   Zhang RH, 2020, PATTERN RECOGN, V102, DOI 10.1016/j.patcog.2020.107260
   Zhao SY, 2021, IEEE T IMAGE PROCESS, V30, P3391, DOI 10.1109/TIP.2021.3060873
   Zhao SY, 2019, IEEE INT CON MULTI, P1840, DOI 10.1109/ICME.2019.00316
   Zheng LR, 2023, IEEE T MULTIMEDIA, V25, P6794, DOI 10.1109/TMM.2022.3214780
   Zhou H, 2022, DISPLAYS, V72, DOI 10.1016/j.displa.2021.102137
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 58
TC 18
Z9 18
U1 57
U2 57
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 202
EP 217
DI 10.1109/TMM.2023.3263078
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500008
HC Y
HP N
DA 2024-08-05
ER

PT J
AU Du, YC
   Wang, M
   Zhou, WA
   Li, HQ
AF Du, Yongchao
   Wang, Min
   Zhou, Wengang
   Li, Houqiang
TI Progressive Similarity Preservation Learning for Deep Scalable Product
   Quantization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Quantization (signal); Feature extraction; Training; Semantics;
   Visualization; Costs; Image retrieval; product quantization; scalable
   code length
AB Product quantization is an effective strategy for compact feature learning in image retrieval, which generates compact quantization codes of different lengths for varying scenarios. However, existing deep quantization methods obtain quantization codes with different lengths by training multiple models separately for each code length, which brings about large training time cost and degrades deployment flexibility. To this end, we propose a new deep scalable Progressive Similarity Preservation Product Quantization (PSPPQ) framework, which enables us to train the quantized features in different code lengths simultaneously and imposes no additional cost during inference. By progressively approximating the ground truth similarity of image pairs, we achieve direct optimization of similarity ranking, which improves the retrieval accuracy and generates sequential quantization codes with more efficiency. Besides, by combining the advantages of classification loss and hinge loss, we design a semantic ArcFace loss to optimize our network architecture. Experiments on three datasets demonstrate the effectiveness of our proposed method with variable code lengths for scalable image retrieval.
C1 [Du, Yongchao; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Wang, Min] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WA (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.; Wang, M (corresponding author), Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei 230030, Peoples R China.
EM ycdu2020@mail.ustc.edu.cn; wangmin@iai.ustc.edu.cn; zhwg@ustc.edu.cn;
   lihq@ustc.edu.cn
OI Wang, Min/0000-0003-3048-6980
FU National Natural Science Foundation of China
FX No Statement Available
CR Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao Y, 2017, PROC CVPR IEEE, P916, DOI 10.1109/CVPR.2017.104
   Cao Y, 2016, AAAI CONF ARTIF INTE, P3457
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chua TS, 2009, P ACM INT C IM VID R, P1
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Doan KD, 2022, PROC CVPR IEEE, P9437, DOI 10.1109/CVPR52688.2022.00923
   Gao LL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P723
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hoe JT, 2021, ADV NEUR IN, V34
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li JY, 2020, INT J MACH LEARN CYB, V11, P883, DOI 10.1007/s13042-019-01026-0
   Li J, 2017, IEEE T MULTIMEDIA, V19, P559, DOI 10.1109/TMM.2016.2617089
   Li Wu-Jun, 2016, P INT JOINT C ART IN, P1711
   Li YQ, 2021, AAAI CONF ARTIF INTE, V35, P2002
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu B, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P755, DOI 10.1145/3240508.3240543
   Ma C, 2021, IEEE T MULTIMEDIA, V23, P3943, DOI 10.1109/TMM.2020.3034534
   Matsui Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1725, DOI 10.1145/3123266.3123430
   Matsui Y, 2018, ITE TRANS MEDIA TECH, V6, P2, DOI 10.3169/mta.6.2
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, DOI 10.48550/ARXIV.1301.3781]
   Ning QQ, 2017, IEEE T MULTIMEDIA, V19, P586, DOI 10.1109/TMM.2016.2625260
   Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song JK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P912
   Tu RC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3409, DOI 10.1145/3474085.3475498
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang JP, 2021, AAAI CONF ARTIF INTE, V35, P2755
   Wang M, 2020, IEEE T MULTIMEDIA, V22, P1507, DOI 10.1109/TMM.2019.2943778
   Wang M, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1707, DOI 10.1145/3123266.3123415
   Weiss Y., 2008, Neural Information Processing Systems
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang EK, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1064
   Yao X, 2023, IEEE T MULTIMEDIA, V25, P6678, DOI 10.1109/TMM.2022.3213476
   Yu LT, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P861, DOI 10.1145/3240508.3240590
   Yu T, 2018, LECT NOTES COMPUT SC, V11205, P191, DOI 10.1007/978-3-030-01246-5_12
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315
   Zhang T, 2014, PR MACH LEARN RES, V32, P838
NR 43
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3034
EP 3045
DI 10.1109/TMM.2023.3306556
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL6F3
UT WOS:001173355700007
DA 2024-08-05
ER

PT J
AU Feng, H
   Liu, SK
   Deng, JJ
   Zhou, WG
   Li, HQ
AF Feng, Hao
   Liu, Shaokai
   Deng, Jiajun
   Zhou, Wengang
   Li, Houqiang
TI Deep Unrestricted Document Image Rectification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Document image rectification; Unrestricted document images; Transformer
ID BLIND QUALITY ASSESSMENT; SHAPE; RESTORATION
AB In recent years, tremendous efforts have been made on document image rectification, but existing advanced algorithms are limited to processing restricted document images, i.e., the input images must incorporate a complete document. Once the captured image merely involves a local text region, its rectification quality is degraded and unsatisfactory. Our previously proposed DocTr, a transformer-assisted network for document image rectification, also suffers from this limitation. In this work, we present DocTr++, a novel unified framework for document image rectification, without any restrictions on the input distorted images. Our major technical improvements can be concluded in three aspects. Firstly, we upgrade the original architecture by adopting a hierarchical encoder-decoder structure for multi-scale representation extraction and parsing. Secondly, we reformulate the pixel-wise mapping relationship between the unrestricted distorted document images and the distortion-free counterparts. The obtained data is used to train our DocTr++ for unrestricted document image rectification. Thirdly, we contribute a real-world test set and metrics applicable for evaluating the rectification quality. To our best knowledge, this is the first learning-based method for the rectification of unrestricted document images. Extensive experiments are conducted, and the results demonstrate the effectiveness and superiority of our method. We hope our DocTr++ will serve as a strong baseline for generic document image rectification, prompting the further advancement and application of learning-based algorithms.
C1 [Feng, Hao; Liu, Shaokai; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
   [Feng, Hao] Zhangjiang Lab, Shanghai 201210, Peoples R China.
   [Deng, Jiajun] Univ Adelaide, Australian Inst Machine Learning, Adelaide, SA 5000, Australia.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Zhangjiang Laboratory; University of Adelaide
RP Zhou, WA; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230027, Peoples R China.
EM haof@mail.ustc.edu.cn; liushaokai@mail.ustc.edu.cn;
   jiajun.deng@adelaide.edu.au; zhwg@ustc.edu.cn; lihq@ustc.edu.cn
OI Deng, Jiajun/0000-0001-9624-7451; Liu, Shaokai/0009-0004-6095-6132
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2023, Cnstd
   Bai JZ, 2023, Arxiv, DOI arXiv:2308.12966
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Brown MS, 2004, IEEE T PATTERN ANAL, V26, P1295, DOI 10.1109/TPAMI.2004.87
   Brown W. B., 2001, P IEEE INT C COMP VI, P9
   Cao HG, 2003, PROC INT CONF DOC, P71
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Changhua Wu, 2002, Structural, Syntactic, and Statistical Pattern Recognition. Joint IAPR International Workshops SSPR 2002 and SPR 2002 (Lecture Notes in Computer Science Vol. 2396), P348
   Ciardiello G., 1988, P INT C PATT REC, P739
   Das H. M., 2020, BRIT MACH VISCONF
   Das S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4248, DOI 10.1109/ICCV48922.2021.00423
   Das S, 2019, IEEE I CONF COMP VIS, P131, DOI 10.1109/ICCV.2019.00022
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feng H, 2023, Arxiv, DOI arXiv:2308.11592
   Feng H, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P273, DOI 10.1145/3474085.3475388
   Feng H, 2022, LECT NOTES COMPUT SC, V13697, P475, DOI 10.1007/978-3-031-19836-6_27
   Feng H, 2022, Arxiv, DOI arXiv:2110.14968
   Freedman D, 2005, PROC CVPR IEEE, P755
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2013, PROC INT CONF DOC, P403, DOI 10.1109/ICDAR.2013.88
   Jiang XW, 2022, PROC CVPR IEEE, P4533, DOI 10.1109/CVPR52688.2022.00450
   Kim G, 2022, LECT NOTES COMPUT SC, V13688, P498, DOI 10.1007/978-3-031-19815-1_29
   Koo HI, 2009, IEEE T IMAGE PROCESS, V18, P1551, DOI 10.1109/TIP.2009.2019301
   Lat A, 2018, INT C PATT RECOG, P3162, DOI 10.1109/ICPR.2018.8545609
   Lavialle O, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P748, DOI 10.1109/ICIP.2001.958227
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Li XY, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3355089.3356563
   Liang J, 2008, IEEE T PATTERN ANAL, V30, P591, DOI 10.1109/TPAMI.2007.70724
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liu C. Li, 2023, NeuralInf. Process. Syst.
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu G., 2020, Pattern Recognit., V108
   Liu Y. Lu, 2023, Appl. Soft Comput., V136
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Loshchilov I., 2018, INT C LEARN REPR
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo D, 2022, Arxiv, DOI arXiv:2212.08365
   Ma K, 2018, PROC CVPR IEEE, P4700, DOI 10.1109/CVPR.2018.00494
   Ma S., 2022, ACM SIGGRAPH C, P1
   Markovitz Amir, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P208, DOI 10.1007/978-3-030-58610-2_13
   Mathew M, 2021, IEEE WINT CONF APPL, P2199, DOI 10.1109/WACV48630.2021.00225
   Meijering E, 2002, P IEEE, V90, P319, DOI 10.1109/5.993400
   Meng GF, 2018, LECT NOTES COMPUT SC, V11220, P180, DOI 10.1007/978-3-030-01270-0_11
   Meng GF, 2014, PROC CVPR IEEE, P3890, DOI 10.1109/CVPR.2014.497
   Meng GF, 2012, IEEE T PATTERN ANAL, V34, P707, DOI 10.1109/TPAMI.2011.151
   Min XK, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3470970
   Min XK, 2020, IEEE T IMAGE PROCESS, V29, P6054, DOI 10.1109/TIP.2020.2988148
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Nie LQ, 2013, IEEE T MULTIMEDIA, V15, P426, DOI 10.1109/TMM.2012.2229971
   Peng DZ, 2023, IEEE T MULTIMEDIA, V25, P2368, DOI 10.1109/TMM.2022.3146771
   Qin XB, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107404
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   SALTON G, 1991, SCIENCE, V253, P974, DOI 10.1126/science.253.5023.974
   Smith LN, 2019, PROC SPIE, V11006, DOI 10.1117/12.2520589
   Smith R, 2007, PROC INT CONF DOC, P629, DOI 10.1109/icdar.2007.4376991
   Tan CL, 2006, IEEE T PATTERN ANAL, V28, P195, DOI 10.1109/TPAMI.2006.40
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Tsoi M. S., 2004, IEEECONF COMPUT VIS
   Tsoi M. S., 2007, IEEE C COMPUT VIS PA, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Verhoeven F, 2024, Arxiv, DOI arXiv:2302.02887
   Wada T, 1997, INT J COMPUT VISION, V24, P125, DOI 10.1023/A:1007906904009
   Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie GW, 2021, LECT NOTES COMPUT SC, V12821, P466, DOI 10.1007/978-3-030-86549-8_30
   Xie GW, 2020, LECT NOTES COMPUT SC, V12116, P131, DOI 10.1007/978-3-030-57058-3_10
   Xue CH, 2022, PROC CVPR IEEE, P4563, DOI 10.1109/CVPR52688.2022.00453
   Yamashita A, 2004, INT C PATT RECOG, P482, DOI 10.1109/ICPR.2004.1334171
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   You S, 2018, IEEE T PATTERN ANAL, V40, P505, DOI 10.1109/TPAMI.2017.2675980
   Yuan H, 2019, AAAI CONF ARTIF INTE, P5717
   Zhai GT, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2757-1
   Zhang JX, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2805, DOI 10.1145/3503161.3548214
   Zhang L, 2008, IEEE T PATTERN ANAL, V30, P728, DOI 10.1109/TPAMI.2007.70831
   Zhang Zhenrong, 2023, IEEE Transactions on Multimedia, P6743, DOI 10.1109/TMM.2022.3214102
NR 78
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6142
EP 6154
DI 10.1109/TMM.2023.3347094
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600034
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Hui, C
   Zhang, SP
   Cui, WX
   Liu, SH
   Jiang, F
   Zhao, DB
AF Hui, Chen
   Zhang, Shengping
   Cui, Wenxue
   Liu, Shaohui
   Jiang, Feng
   Zhao, Debin
TI Rate-Adaptive Neural Network for Image Compressive Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image compressive sensing; adaptive sampling; sampling matrix; deep
   networks; image compression
ID THRESHOLDING ALGORITHM; MATRICES; BINARY
AB Deep learning-based image compressive sensing (CS) methods have achieved great success in the past few years. However, most of them are content-independent, with a spatially uniform sampling rate allocation for the entire image. Such practises may potentially degrade the performance of image CS with block-based sampling, since the content of different blocks in an image is different. In this article, we propose a novel rate-adaptive image CS neural network (dubbed RACSNet) to achieve adaptive sampling rate allocation based on the content characteristics of the image with a single model. Specifically, a measurement domain-based reconstruction distortion is first used to guide the sampling rate allocation for different blocks in an image without access to the ground truth image. Then, a step-wise training strategy is designed to train a reusable sampling matrix, which is capable of sampling image blocks to generate the compressed measurements under arbitrary sampling rates. Subsequently, a pyramid-shaped initial reconstruction sub-network and a hierarchical deep reconstruction sub-network that fuse the measurement information of different scales are put forward to reconstruct image blocks from the compressed measurements. Finally, a reconstruction distortion map and an improved loss function are developed to eliminate the blocking artifacts and further enhance the CS reconstruction. Experimental results on both objective metrics and subjective visual qualities show that the proposed RACSNet achieves significant improvements over the state-of-the-art methods.
C1 [Hui, Chen; Zhang, Shengping; Cui, Wenxue; Liu, Shaohui; Jiang, Feng; Zhao, Debin] Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
C3 Harbin Institute of Technology
RP Jiang, F (corresponding author), Harbin Inst Technol, Dept Comp Sci & Technol, Harbin 150001, Peoples R China.
EM chenhui@stu.hit.edu.cn; s.zhang@hit.edu.cn; wxcui@hit.edu.cn;
   shliu@hit.edu.cn; fjiang@hit.edu.cn; dbzhao@hit.edu.cn
RI hui, chen/KVX-9809-2024; JIANG, Feng/HTP-2862-2023
OI Jiang, Feng/0000-0001-8342-1211; Liu, Shaohui/0000-0002-1810-5412; Cui,
   Wenxue/0000-0001-8656-0954
FU National Key Research and Development Program of China
FX No Statement Available
CR Adler A, 2016, Arxiv, DOI arXiv:1606.01519
   Akbari A, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574688
   Amini A, 2011, IEEE T INFORM THEORY, V57, P2360, DOI 10.1109/TIT.2011.2111670
   Arbeláez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161
   Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Chen C, 2011, CONF REC ASILOMAR C, P1193, DOI 10.1109/ACSSC.2011.6190204
   Cui WX, 2023, IEEE T MULTIMEDIA, V25, P816, DOI 10.1109/TMM.2021.3132489
   Cui WX, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1748, DOI 10.1109/ICASSP.2018.8461766
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042
   DeVore RA, 2007, J COMPLEXITY, V23, P918, DOI 10.1016/j.jco.2007.04.002
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2009, P NATL ACAD SCI USA, V106, P18914, DOI 10.1073/pnas.0909892106
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Du J, 2019, NEUROCOMPUTING, V328, P105, DOI 10.1016/j.neucom.2018.04.084
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   EVERINGHAM M, 2010, IJCV, V88, P303, DOI DOI 10.1007/S11263-009-0275-4
   Fan ZE, 2022, PROC CVPR IEEE, P8944, DOI 10.1109/CVPR52688.2022.00875
   Fourure D, 2017, Arxiv, DOI arXiv:1707.07958
   Fowler JE, 2011, EUR SIGNAL PR CONF, P564
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Haupt J, 2010, IEEE T INFORM THEORY, V56, P5862, DOI 10.1109/TIT.2010.2070191
   Hui C., 2022, P IEEE INT C MULT EX, P1
   Hui C, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3503160
   Jiwei Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P513, DOI 10.1007/978-3-030-58542-6_31
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kulkarni K, 2016, PROC CVPR IEEE, P449, DOI 10.1109/CVPR.2016.55
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1
   Li R, 2018, MULTIMED TOOLS APPL, V77, P12139, DOI 10.1007/s11042-017-4862-z
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   Lohit S, 2018, IEEE T COMPUT IMAG, V4, P326, DOI 10.1109/TCI.2018.2846413
   Lohit Suhas., 2018, arXiv
   Lu WZ, 2018, IEEE T SIGNAL PROCES, V66, P77, DOI 10.1109/TSP.2017.2757915
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mou C, 2023, IEEE T PATTERN ANAL, V45, P5236, DOI 10.1109/TPAMI.2022.3194001
   Mousavi A, 2015, ANN ALLERTON CONF, P1336, DOI 10.1109/ALLERTON.2015.7447163
   Qiu CX, 2022, Arxiv, DOI arXiv:2203.10779
   Rousset F, 2017, IEEE T COMPUT IMAG, V3, P36, DOI 10.1109/TCI.2016.2637079
   Shi WZ, 2019, PROC CVPR IEEE, P12282, DOI 10.1109/CVPR.2019.01257
   Shi WZ, 2020, IEEE T IMAGE PROCESS, V29, P375, DOI 10.1109/TIP.2019.2928136
   Shi WZ, 2017, IEEE INT CON MULTI, P877, DOI 10.1109/ICME.2017.8019428
   Song JC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4249, DOI 10.1145/3474085.3475562
   Su YM, 2020, SIGNAL PROCESS-IMAGE, V89, DOI 10.1016/j.image.2020.115989
   Svoboda P, 2016, Arxiv, DOI arXiv:1605.00366
   Vehkaperä M, 2016, IEEE T INFORM THEORY, V62, P2100, DOI 10.1109/TIT.2016.2525824
   Wang J, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3448108
   Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892
   Xu YB, 2020, IEEE ACCESS, V8, P217711, DOI 10.1109/ACCESS.2020.3041807
   Yang Y, 2016, ADV NEUR IN, V29
   You D, 2021, IEEE T IMAGE PROCESS, V30, P6066, DOI 10.1109/TIP.2021.3091834
   Yu Y, 2010, IEEE SIGNAL PROC LET, V17, P973, DOI 10.1109/LSP.2010.2080673
   Zhang J, 2023, IEEE SIGNAL PROC MAG, V40, P58, DOI 10.1109/MSP.2022.3208394
   Zhang J, 2020, IEEE J-STSP, V14, P765, DOI 10.1109/JSTSP.2020.2977507
   Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196
   Zhang J, 2014, IEEE T IMAGE PROCESS, V23, P3336, DOI 10.1109/TIP.2014.2323127
   Zhang K, 2022, IEEE T PATTERN ANAL, V44, P6360, DOI 10.1109/TPAMI.2021.3088914
   Zhang KY, 2023, IEEE T MULTIMEDIA, V25, P5676, DOI 10.1109/TMM.2022.3198323
   ZHANG Z H, arXiv
   Zhang ZH, 2021, IEEE T IMAGE PROCESS, V30, P1487, DOI 10.1109/TIP.2020.3044472
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zheng BL, 2020, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR42600.2020.00369
   Zheng S, 2019, IEEE T MULTIMEDIA, V21, P1905, DOI 10.1109/TMM.2019.2891415
   Zhou SW, 2021, IEEE T MULTIMEDIA, V23, P2627, DOI 10.1109/TMM.2020.3014561
   Zhou SW, 2019, MULTIMED TOOLS APPL, V78, P537, DOI 10.1007/s11042-017-5249-x
NR 67
TC 1
Z9 1
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2515
EP 2530
DI 10.1109/TMM.2023.3301213
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IS5I5
UT WOS:001168330100001
DA 2024-08-05
ER

PT J
AU Ji, FF
   Yuan, XT
   Liu, QS
AF Ji, Fanfan
   Yuan, Xiao-Tong
   Liu, Qingshan
TI Soft Weight Pruning for Cross-Domain Few-Shot Learning With Unlabeled
   Target Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-domain few-shot learning; regularized training; self-supervised;
   soft weight pruning
ID MODEL
AB Cross-domain few-shot learning (CDFSL) has received great interest for its effectiveness in solving the problem of the shift between source and target domains in few-shot scenarios. To extract more representative features, recent CDFSL works have exploited small-scale unlabeled samples from the target domain during the feature extraction phase. Existing self-supervised CDFSL methods, however, typically fine-tune the weights of the pre-trained model without taking into account the mismatch between source and target domains. To address this shortcoming, we introduce a self-supervised soft weight pruning strategy for cross-domain few-shot classification tasks with unlabeled target data. Starting from a pre-trained network from the source domain, our approach iterates between pruning out the relatively unimportant connections of the network and reactivating the pruned connections in a joint contrastive and $L<^>{2}$-SP regularized training framework. By combining the soft weight pruning strategy and regularization, our method effectively restricts redundant weights while simultaneously learning crucial features for both source and target tasks. Our approach, in comparison to other methods, does not involve any additional modules in the models; however, it can still achieve remarkable performance. Our approach can be efficiently incorporated into a variety of contrastive learning methods in a plug-and-play fashion. Extensive experimental results on several benchmark datasets demonstrate that our proposed method outperforms existing representative cross-domain few-shot methods by a large margin.
C1 [Ji, Fanfan; Yuan, Xiao-Tong; Liu, Qingshan] Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Yuan, XT (corresponding author), Nanjing Univ Informat Sci & Technol, Engn Res Ctr Digital Forens, Minist Educ, Nanjing 210044, Peoples R China.
EM jiff1995@nuist.edu.cn; xtyuan1980@gmail.com; qsliu@nuist.edu.cn
OI ji, fan fan/0000-0002-9061-0165
FU National Key Research and Development Program of China
FX No Statement Available
CR Abuduweili A, 2021, PROC CVPR IEEE, P6919, DOI 10.1109/CVPR46437.2021.00685
   Cai JH, 2020, Arxiv, DOI arXiv:2005.10544
   Caron Mathilde, 2020, Advances in Neural Information Processing Systems, V33, P9912, DOI DOI 10.5555/3495724.3496555
   Cha S, 2019, IEEE I CONF COMP VIS, P4159, DOI 10.1109/ICCV.2019.00426
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen Y., 2019, INT C LEARN REPRESEN
   Das D., 2021, PROC INT C LEARN REP, P1
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Finn C, 2017, PR MACH LEARN RES, V70
   Fu YQ, 2023, PROC CVPR IEEE, P24575, DOI 10.1109/CVPR52729.2023.02354
   Grill JB, 2020, P 34 INT C NEUR INF, V33, P21271
   Guo YW, 2016, ADV NEUR IN, V29
   Han H., 2016, 4 INT C LEARN REPR I, P51
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Helber P, 2019, IEEE J-STARS, V12, P2217, DOI 10.1109/JSTARS.2019.2918242
   Hoefler T, 2021, J MACH LEARN RES, V23
   Hongduan Tian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P675, DOI 10.1007/978-3-030-58529-7_40
   Islam A, 2021, P ADV NEUR INF PROC, P3584
   Jin XJ, 2016, Arxiv, DOI arXiv:1607.05423
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Krause J, 2015, PROC CVPR IEEE, P5546, DOI 10.1109/CVPR.2015.7299194
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li P, 2022, PROC CVPR IEEE, P9089, DOI 10.1109/CVPR52688.2022.00889
   Li XH, 2018, PR MACH LEARN RES, V80
   Liu BY, 2021, AAAI CONF ARTIF INTE, V35, P8627
   Lodish H., 2000, Molecular cell biology, Vfourth, DOI 10.1016/S1470-8175(01)00023-6
   Lu YW, 2022, IEEE T MULTIMEDIA, V24, P1871, DOI 10.1109/TMM.2021.3073258
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Ma TY, 2023, PROC CVPR IEEE, P19754, DOI 10.1109/CVPR52729.2023.01892
   Mohanty SP, 2016, FRONT PLANT SCI, V7, DOI 10.3389/fpls.2016.01419
   Oh J., 2022, P ADV NEUR INF PROC, V35, P2622
   Oh J, 2022, PROCEEDINGS OF THE 31ST ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, CIKM 2022, P4359, DOI 10.1145/3511808.3557681
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Paszke A, 2019, ADV NEUR IN, V32
   PeterWelinder Pietro, 2010, IEEE COMP SOC C COMP, P25
   Phoo B., 2021, INTCONFLEARNREPRESEN, P1
   Ren M., 2018, 6 INT C LEARN REPR
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Singh A, 2021, Advances in Neural Information Processing Systems, V34
   Snell J, 2017, ADV NEUR IN, V30
   Tian PZ, 2023, IEEE T MULTIMEDIA, V25, P6881, DOI 10.1109/TMM.2022.3215310
   Tschandl P, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.161
   Tseng H.-Y., 2020, INT C LEARN REPRESEN, P1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Horn G, 2018, PROC CVPR IEEE, P8769, DOI 10.1109/CVPR.2018.00914
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Wang Haoqing, 2021, P 30 INT JOINT C ART, P1075
   Wang R, 2023, IEEE T MULTIMEDIA, V25, P1665, DOI 10.1109/TMM.2022.3146744
   Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wu BX, 2021, INT J AUTOM COMPUT, V18, P422, DOI 10.1007/s11633-020-1273-9
   Yang C, 2022, MACH INTELL RES, V19, P24, DOI 10.1007/s11633-022-1320-9
   Yeh JF, 2024, Arxiv, DOI arXiv:2005.09218
   Yosinski J, 2014, ADV NEUR IN, V27
   Yunhui Guo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P124, DOI 10.1007/978-3-030-58583-9_8
   Zhao A, 2021, IEEE WINT CONF APPL, P1389, DOI 10.1109/WACV48630.2021.00143
   Zheng H., 2023, PROC INTCONF LEARN R, P1
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhuang YQ, 2018, INT C PATT RECOG, P1506, DOI 10.1109/ICPR.2018.8545708
NR 64
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6759
EP 6769
DI 10.1109/TMM.2024.3355650
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600050
DA 2024-08-05
ER

PT J
AU Li, AQ
   Hou, SH
   Cai, QY
   Fu, Y
   Huang, YZ
AF Li, Aoqi
   Hou, Saihui
   Cai, Qingyuan
   Fu, Yang
   Huang, Yongzhen
TI Gait Recognition With Drones: A Benchmark
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gait recognition; Drones; high vertical views
AB Gait recognition aims to obtain people's identity through body shape and walking posture. Existing gait recognition studies focus on low vertical view recognition, in which the person and the camera are nearly at the same height. Differently, in this work, we focus on gait recognition at high vertical views. To facilitate the research, we propose a new dataset named DroneGait, where the drones are used to collect the gait data. This dataset contains 22 k sequences of 96 subjects taken at different vertical views, varying from about 0(degrees) to 80(degrees). Furthermore, we evaluate the effectiveness of several state-of-the-art appearance-based and skeleton-based models using our dataset and establish comprehensive baselines. Our results demonstrate that the dataset is challenging and presents significant opportunities to improve existing gait recognition methods. Moreover, we propose a new method called Vertical Distillation, which is based on the feature distillation across different vertical views. Our proposed method substantially outperforms the state-of-the-art models on DroneGait at high vertical views. Cross-vertical-view and cross-domain experiments are also made to explain the importance of gait recognition at high vertical views. Furthermore, we analyze the differences between gait recognition at different vertical views using heatmap visualization techniques. We will make our dataset and code publicly available upon acceptance.
C1 [Li, Aoqi; Hou, Saihui; Cai, Qingyuan; Fu, Yang; Huang, Yongzhen] Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
   [Hou, Saihui; Huang, Yongzhen] Watrix Technol Ltd Co Ltd, Beijing 100088, Peoples R China.
C3 Beijing Normal University
RP Hou, SH; Huang, YZ (corresponding author), Beijing Normal Univ, Sch Artificial Intelligence, Beijing 100875, Peoples R China.
EM rookie@mail.bnu.edu.cn; housaihui@bnu.edu.cn;
   caiqingyuan@mail.bnu.edu.cn; yangfu@mail.bnu.edu.cn;
   huangyongzhen@bnu.edu.cn
RI cai, qingyuan/KIG-4056-2024
OI Hou, Saihui/0000-0003-4689-2860; Cai, Qingyuan/0009-0006-8178-7598
FU National Natural Science Foundation of China
FX No Statement Available
CR Boyi Jiang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P18, DOI 10.1007/978-3-030-58565-5_2
   Chao Fan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14213, DOI 10.1109/CVPR42600.2020.01423
   Chao HQ, 2019, AAAI CONF ARTIF INTE, P8126
   Chen HS, 2022, PROC CVPR IEEE, P2771, DOI 10.1109/CVPR52688.2022.00280
   Collins RT, 2002, FIFTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P366, DOI 10.1109/AFGR.2002.1004181
   Fan C, 2023, PROC CVPR IEEE, P9707, DOI 10.1109/CVPR52729.2023.00936
   Ge Z, 2021, Arxiv, DOI arXiv:2107.08430
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hofmann M, 2014, J VIS COMMUN IMAGE R, V25, P195, DOI 10.1016/j.jvcir.2013.02.006
   Hossain MA, 2010, PATTERN RECOGN, V43, P2281, DOI 10.1016/j.patcog.2009.12.020
   Hsu HM, 2022, IEEE IMAGE PROC, P2546, DOI 10.1109/ICIP46576.2022.9897409
   Li JF, 2021, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR46437.2021.00339
   Li N, 2023, IEEE T MULTIMEDIA, V25, P3046, DOI 10.1109/TMM.2022.3154609
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Li TJ, 2021, PROC CVPR IEEE, P16261, DOI 10.1109/CVPR46437.2021.01600
   Li Xiang, 2022, IEEE Transactions on Biometrics, Behavior, and Identity Science, V4, P234, DOI 10.1109/TBIOM.2022.3174559
   Li ZQ, 2022, LECT NOTES COMPUT SC, V13669, P1, DOI 10.1007/978-3-031-20077-9_1
   Lin BB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14628, DOI 10.1109/ICCV48922.2021.01438
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Ma XX, 2023, PROC CVPR IEEE, P534, DOI 10.1109/CVPR52729.2023.00059
   Pei YL, 2022, MACH LEARN, V111, P519, DOI 10.1007/s10994-021-06044-0
   Perera AG, 2018, COGN COMPUT, V10, P1019, DOI 10.1007/s12559-018-9577-6
   Saihui Hou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P382, DOI 10.1007/978-3-030-58545-7_22
   Shigeki Y., 2018, IPSJ Trans. Comput. Vis. Appl., V1, P1
   Song CF, 2023, IEEE T PATTERN ANAL, V45, P2801, DOI 10.1109/TPAMI.2022.3183288
   Sun K, 2019, Arxiv, DOI [arXiv:1904.04514, DOI 10.48550/ARXIV.1904.04514]
   Takemura Noriko, 2018, IPSJ Transactions on Computer Vision and Applications, V10, DOI 10.1186/s41074-018-0039-6
   Teed Zachary, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P402, DOI 10.1007/978-3-030-58536-5_24
   Teepe T, 2022, IEEE COMPUT SOC CONF, P1568, DOI 10.1109/CVPRW56347.2022.00163
   Teepe T, 2021, IEEE IMAGE PROC, P2314, DOI 10.1109/ICIP42928.2021.9506717
   Weizhi An, 2020, IEEE Transactions on Biometrics, Behavior, and Identity Science, V2, P421, DOI 10.1109/TBIOM.2020.3008862
   Xu K, 2021, IEEE T MULTIMEDIA, V24, P3265, DOI 10.1109/TMM.2021.3095809
   Yan C., 2021, P IEEECVF INT C COMP, P10943
   Yao LX, 2023, IEEE T MULTIMEDIA, V25, P4187, DOI 10.1109/TMM.2022.3171961
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zhang C, 2023, EXPERT SYST, V40, DOI 10.1111/exsy.13244
   Zheng JK, 2022, PROC CVPR IEEE, P20196, DOI 10.1109/CVPR52688.2022.01959
   Zhu Z., 2021, ICCV, p14 789, DOI DOI 10.1109/ICCV48922.2021.01452
NR 40
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3530
EP 3540
DI 10.1109/TMM.2023.3312931
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200029
DA 2024-08-05
ER

PT J
AU Li, JN
   Wang, J
   Xu, TF
AF Li, Jianan
   Wang, Jie
   Xu, Tingfa
TI PointGL: A Simple Global-Local Framework for Efficient Point Cloud
   Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud; feature embedding; graph
AB Efficient analysis of point clouds holds paramount significance in real-world 3D applications. Currently, prevailing point-based models adhere to the PointNet++ methodology, which involves embedding and abstracting point features within a sequence of spatially overlapping local point sets, resulting in noticeable computational redundancy. Drawing inspiration from the streamlined paradigm of pixel embedding followed by regional pooling in Convolutional Neural Networks (CNNs), we introduce a novel, uncomplicated yet potent architecture known as PointGL, crafted to facilitate efficient point cloud analysis. PointGL employs a hierarchical process of feature acquisition through two recursive steps. First, the Global Point Embedding leverages straightforward residual Multilayer Perceptrons (MLPs) to effectuate feature embedding for each individual point. Second, the novel Local Graph Pooling technique characterizes point-to-point relationships and abstracts regional representations through succinct local graphs. The harmonious fusion of one-time point embedding and parameter-free graph pooling contributes to PointGL's defining attributes of minimized model complexity and heightened efficiency. Our PointGL attains state-of-the-art accuracy on the ScanObjectNN dataset while exhibiting a runtime that is more than 5 times faster and utilizing only approximately 4% of the FLOPs and 30% of the parameters compared to the recent PointMLP model.
C1 [Li, Jianan; Wang, Jie; Xu, Tingfa] Beijing Inst Technol, Beijing 100081, Peoples R China.
   [Li, Jianan; Xu, Tingfa] Minist Educ China, Key Lab Photoelect Imaging Technol & Syst, Beijing 100081, Peoples R China.
   [Xu, Tingfa] Beijing Inst Technol, Chongqing Innovat Ctr, Chongqing 401135, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology
RP Li, JN; Xu, TF (corresponding author), Beijing Inst Technol, Beijing 100081, Peoples R China.
EM lijianan@bit.edu.cn; jwang991020@gmail.com; ciom_xtf1@bit.edu.cn
OI xu, tingfa/0000-0001-5452-2662
FU National Natural Science Foundation of China
FX No Statement Available
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Barbu A, 2019, ADV NEUR IN, V32
   Chen XX, 2022, IEEE ROBOT AUTOM LET, V7, P2519, DOI 10.1109/LRA.2022.3143224
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng S, 2022, IEEE INT CONF ROBOT, P9214, DOI 10.1109/ICRA46639.2022.9811904
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Goyal A, 2021, PR MACH LEARN RES, V139
   Guo HY, 2016, IEEE T IMAGE PROCESS, V25, P5526, DOI 10.1109/TIP.2016.2609814
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Hamdi A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1, DOI 10.1109/ICCV48922.2021.00007
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Hendrycks D, 2019, Arxiv, DOI arXiv:1903.12261
   Kingma D. P., 2014, arXiv
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Li YY, 2018, ADV NEUR IN, V31
   Lin ZH, 2022, IEEE T PATTERN ANAL, V44, P4212, DOI 10.1109/TPAMI.2021.3059758
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Loshchilov I, 2017, Sgdr: Stochastic gradient descent with warm restarts, DOI [10.48550/arXiv.1608.03983, DOI 10.48550/ARXIV.1608.03983]
   Ma X., 2022, PROC INT C LEARN REP, P1
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Meraz M, 2022, INT J MULTIMED INF R, V11, P123, DOI 10.1007/s13735-022-00236-7
   Pang Y, 2022, LECT NOTES COMPUT SC, V13662, P604, DOI 10.1007/978-3-031-20086-1_35
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qian G. C., 2022, ADV NEURAL INFORM PR, V35, P23192, DOI [DOI 10.48550/ARXIV.2206.04670, https://doi.org/10.48550/arXiv.2206.04670]
   Qian GC, 2021, ADV NEUR IN, V34
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Ran HX, 2022, PROC CVPR IEEE, P18920, DOI 10.1109/CVPR52688.2022.01837
   Ran HX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15457, DOI 10.1109/ICCV48922.2021.01519
   Recht B, 2019, PR MACH LEARN RES, V97
   Ren J., 2022, Pointcloud-C: Benchmarking and analyzing point cloud perception robustness under corruptions
   Ren JW, 2022, PR MACH LEARN RES
   Saha A, 2022, IEEE INT CONF ROBOT, P9200, DOI 10.1109/ICRA46639.2022.9811901
   Shabbir M, 2021, IEEE ACCESS, V9, P8820, DOI 10.1109/ACCESS.2021.3049564
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Uy MA, 2019, IEEE I CONF COMP VIS, P1588, DOI 10.1109/ICCV.2019.00167
   Wang HC, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9762, DOI 10.1109/ICCV48922.2021.00964
   Wang J., 2021, arXiv
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wiesmann L, 2022, IEEE INT CONF ROBOT, P10925, DOI 10.1109/ICRA46639.2022.9811785
   Wu WX, 2019, PROC CVPR IEEE, P9613, DOI 10.1109/CVPR.2019.00985
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Xu YF, 2018, LECT NOTES COMPUT SC, V11212, P90, DOI 10.1007/978-3-030-01237-3_6
   Yan X, 2020, PROC CVPR IEEE, P5588, DOI 10.1109/CVPR42600.2020.00563
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238
   Yiming Zhao, 2022, 2022 International Conference on Robotics and Automation (ICRA), P7029, DOI 10.1109/ICRA46639.2022.9812058
   Yu XM, 2022, PROC CVPR IEEE, P19291, DOI 10.1109/CVPR52688.2022.01871
   Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
NR 58
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6931
EP 6942
DI 10.1109/TMM.2024.3358695
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000034
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, J
   Wang, XP
   Lv, GQ
   Zeng, ZG
AF Li, Jiang
   Wang, Xiaoping
   Lv, Guoqing
   Zeng, Zhigang
TI GraphCFC: A Directed Graph Based Cross-Modal Feature Complementation
   Approach for Multimodal Conversational Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition in conversation; multimodal fusion; graph neural
   networks; cross-modal feature complementation
AB Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. We extract various types of edges from the constructed graph for encoding, thus enabling GNNs to extract crucial contextual and interactive information more accurately when performing message passing. Furthermore, we design a GNN structure called GAT-MLP, which can provide a new unified network framework for multimodal learning. The experimental results on two benchmark datasets show that our GraphCFC outperforms the state-of-the-art (SOTA) approaches.
C1 [Li, Jiang; Wang, Xiaoping; Lv, Guoqing; Zeng, Zhigang] Huazhong Univ Sci & Technol, Educ Minist China, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.
   [Li, Jiang; Wang, Xiaoping; Lv, Guoqing; Zeng, Zhigang] Huazhong Univ Sci & Technol, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Li, J; Wang, XP (corresponding author), Huazhong Univ Sci & Technol, Educ Minist China, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.; Li, J; Wang, XP (corresponding author), Huazhong Univ Sci & Technol, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
EM lijfrank@hust.edu.cn; wangxiaoping@hust.edu.cn; guoqinglv@hust.edu.cn;
   zgzeng@hust.edu.cn
RI zhu, yujie/KBC-4009-2024; Li, Jiang/HNI-1328-2023; Wang,
   Xiao-Ping/G-7394-2011; Zeng, Zhigang/A-1794-2013
OI Li, Jiang/0000-0002-0116-5662; Guoqing, LV/0009-0004-2966-5724
FU National Natural Science Foundation of China
FX No Statement Available
CR Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Brody S., 2022, PROC INT C LEARN REP, P1
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chatterjee A, 2019, P 13 INT WORKSH SEM, P39, DOI DOI 10.18653/V1/S19-2005
   Chen J., 2018, PROC 6 INT C LEARN R, P1
   D'Mello SK, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682899
   Dosovitskiy A., 2021, PROC INT C LEARNING, DOI DOI 10.48550/ARXIV.2010.11929
   Ghosal D, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2470
   Ghosal D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P154
   Guo WZ, 2019, IEEE ACCESS, V7, P63373, DOI 10.1109/ACCESS.2019.2916887
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hazarika D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1122, DOI 10.1145/3394171.3413678
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu D, 2021, P 59 ANN M ASS COMP, V1, P7042
   Hu J., 2021, P 59 ANN M ASS COMP, P5666
   Huang C., 2018, P C N AM CHAPT ASS C, V2, P49, DOI DOI 10.18653/V1/N18-2008
   Huang ZC, 2019, INT CONF ACOUST SPEE, P5856, DOI 10.1109/ICASSP.2019.8682916
   Ishiwatari T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7360
   Jiao WX, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P397
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Li GH, 2023, IEEE T PATTERN ANAL, V45, P6923, DOI 10.1109/TPAMI.2021.3074057
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Majumder N, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P8968
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Mang Q, 2020, INT CONF ACOUST SPEE, P7829, DOI [10.1109/ICASSP40776.2020.9053896, 10.1109/icassp40776.2020.9053896]
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Sahay S, 2018, FIRST GRAND CHALLENGE AND WORKSHOP ON HUMAN MULTIMODAL LANGUAGE (CHALLENGE-HML), P20
   Shen WZ, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P1551
   Sun ZK, 2020, AAAI CONF ARTIF INTE, V34, P8992
   Tsai Y.-H. H., 2019, PROC INT C LEARN REP
   Tsai YHH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P6558, DOI 10.18653/v1/p19-1656
   Van Kleef GA, 2022, ANNU REV PSYCHOL, V73, P629, DOI 10.1146/annurev-psych-020821-010855
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P., 2017, PREPRINT
   Zhang D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5415
   Zhang XY, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P3465, DOI 10.1145/3442381.3450004
   Zhong PX, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P165
NR 41
TC 5
Z9 5
U1 25
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 77
EP 89
DI 10.1109/TMM.2023.3260635
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500025
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Li, SB
   Zhu, SY
   Ge, Y
   Zeng, B
   Imran, MA
   Abbasi, QH
   Cooper, J
AF Li, Shibo
   Zhu, Shuyuan
   Ge, Yao
   Zeng, Bing
   Imran, Muhammad Ali
   Abbasi, Qammer H.
   Cooper, Jonathan
TI Depth-Guided Deep Video Inpainting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video inpainting; depth completion; depth-guided content reconstruction;
   content enhancement
AB Video inpainting aims to fill in missing regions of a video after any undesired contents are removed from it. This technique can be applied to repair the broken video or edit the video content. In this paper, we propose a depth-guided deep video inpainting network (DGDVI) and demonstrate its effectiveness in processing challenging broken areas crossing multiple depth layers. To achieve our goal, we divide the inpainting into depth completion, content reconstruction, and content enhancement. Three corresponding modules are designed to implement a process-flow. Firstly, we develop a depth completion module based upon the spatio-temporal Transformer which is used to obtain the completed depth information for each video frame. Secondly, we design a content reconstruction module to generate initially inpainted video. With this module, the contents of the missing regions are composed via the depth-guided feature propagation. Thirdly, we construct a content enhancement module to enhance the temporal coherence and texture quality for the inpainted video. All of proposed modules are jointly optimized to guarantee the high inpainting efficiency. The experimental results demonstrate that our proposed method provides better inpainting results, both qualitatively and quantitatively, compared with the previous state-of-the-art.
C1 [Li, Shibo; Zhu, Shuyuan; Zeng, Bing] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Li, Shibo; Ge, Yao; Imran, Muhammad Ali; Abbasi, Qammer H.; Cooper, Jonathan] Univ Glasgow, James Watt Sch Engn, Glasgow G12 8QQ, Scotland.
C3 University of Electronic Science & Technology of China; University of
   Glasgow
RP Zhu, SY (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM 2605222l@student.gla.ac.uk; eezsy@uestc.edu.cn;
   y.ge.2@research.gla.ac.uk; eezeng@uestc.edu.cn;
   muhammad.imran@glasgow.ac.uk; qammer.abbasi@glasgow.ac.uk;
   jon.cooper@glasgow.ac.uk
RI Imran, Muhammad Ali/I-4832-2012; Cooper, Jonathan/E-9000-2010
OI Imran, Muhammad Ali/0000-0003-4743-9136; Cooper,
   Jonathan/0000-0002-2358-1050; Ge, Yao/0000-0001-6314-3891; Abbasi,
   Qammer Hussain/0000-0002-7097-9969
FU National Natural Science Foundation of China
FX No Statement Available
CR Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Chan KCK, 2022, PROC CVPR IEEE, P5962, DOI 10.1109/CVPR52688.2022.00588
   Chang YL, 2019, IEEE I CONF COMP VIS, P9065, DOI 10.1109/ICCV.2019.00916
   Chang Z. Y., 2019, BRIT MACH VIS C, P9066
   Chen Gao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P713, DOI 10.1007/978-3-030-58610-2_42
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dong QL, 2022, PROC CVPR IEEE, P11348, DOI 10.1109/CVPR52688.2022.01107
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Junayed MS, 2023, IEEE T MULTIMEDIA, V25, P7494, DOI 10.1109/TMM.2022.3222932
   Kang J, 2022, LECT NOTES COMPUT SC, V13675, P375, DOI 10.1007/978-3-031-19784-0_22
   Kim D, 2019, PROC CVPR IEEE, P5785, DOI 10.1109/CVPR.2019.00594
   Kingma D. P., 2014, arXiv
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lee S, 2019, IEEE I CONF COMP VIS, P4412, DOI 10.1109/ICCV.2019.00451
   Li WB, 2022, PROC CVPR IEEE, P10748, DOI 10.1109/CVPR52688.2022.01049
   Li Z, 2022, PROC CVPR IEEE, P17541, DOI 10.1109/CVPR52688.2022.01704
   Liang JY, 2022, Arxiv, DOI [arXiv:2201.12288, DOI 10.48550/ARXIV.2201.12288]
   Liu GL, 2018, LECT NOTES COMPUT SC, V11215, P89, DOI 10.1007/978-3-030-01252-6_6
   Liu HY, 2019, IEEE I CONF COMP VIS, P4169, DOI 10.1109/ICCV.2019.00427
   Liu R., 2021, P IEEE CVF INT C COM, P14040
   Liu R, 2021, Arxiv, DOI arXiv:2104.06637
   Miao Liao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P1, DOI 10.1007/978-3-030-58589-1_1
   Nazeri K., 2019, arXiv
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Ranftl R, 2022, IEEE T PATTERN ANAL, V44, P1623, DOI 10.1109/TPAMI.2020.3019967
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Ren YR, 2019, IEEE I CONF COMP VIS, P181, DOI 10.1109/ICCV.2019.00027
   Song Y., 2018, BRIT MACH VIS C
   Sun HY, 2023, IEEE T MULTIMEDIA, V25, P4240, DOI 10.1109/TMM.2022.3174413
   Suvorov R, 2022, IEEE WINT CONF APPL, P3172, DOI 10.1109/WACV51458.2022.00323
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2019, AAAI CONF ARTIF INTE, P5232
   Wang T.-H., 2018, ARXIV
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu HW, 2022, IEEE T MULTIMEDIA, V24, P4016, DOI 10.1109/TMM.2021.3111491
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xiong W, 2019, PROC CVPR IEEE, P5833, DOI 10.1109/CVPR.2019.00599
   Xu N, 2018, LECT NOTES COMPUT SC, V11209, P603, DOI 10.1007/978-3-030-01228-1_36
   Xu R, 2019, PROC CVPR IEEE, P3718, DOI 10.1109/CVPR.2019.00384
   Yanhong Zeng, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P528, DOI 10.1007/978-3-030-58517-4_31
   Yi Z, 2020, P IEEE CVF C COMP VI, P7508, DOI DOI 10.1109/CVPR42600.2020.00753
   Yu JH, 2019, IEEE I CONF COMP VIS, P4470, DOI 10.1109/ICCV.2019.00457
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Yu YC, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P69, DOI 10.1145/3474085.3475436
   Zhang K., 2022, IEEE INT C COMPUT VI, P5982
   Zhang KD, 2022, LECT NOTES COMPUT SC, V13678, P74, DOI 10.1007/978-3-031-19797-0_5
   Zhang Y., 2023, IEEE Trans. Multi-media, early access, DOI [10.1109/TMM.2023.3282892.[24]H., DOI 10.1109/TMM.2023.3282892.[24]H]
   Zhou YQ, 2021, PROC CVPR IEEE, P2266, DOI 10.1109/CVPR46437.2021.00230
   Zou XY, 2021, PROC CVPR IEEE, P16443, DOI 10.1109/CVPR46437.2021.01618
NR 51
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5860
EP 5871
DI 10.1109/TMM.2023.3340089
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100022
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Liu, SY
   Cheng, J
   Xia, ZY
   Xi, ZL
   Hou, Q
   Dong, ZC
AF Liu, Siyu
   Cheng, Jian
   Xia, Ziying
   Xi, Zhilong
   Hou, Qin
   Dong, Zhicheng
TI HCM: Online Action Detection With Hard Video Clip Mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Online action detection; deep metric learning; hard video clip mining;
   intra-class feature compaction
AB Online action detection plays a vital role in video action understanding and can be widely used in various video analysis applications. This task aims to detect actions at the current moment within long untrimmed video streams. However, accurately identifying action-background transitions that are ambiguous in terms of time during detection can be challenging due to the similarity between the action and background clips, adding to the difficulty in finding a suitable division between them. To address this issue, we propose a hard video clip mining method based on deep metric learning for online action detection named HCM. The HCM method first selects video clips that are hard to distinguish to determine the optimization objects. Then, a hard clip mining loss is adopted to push the features toward the centers of the categories to which they belong and away from others. Furthermore, we introduce an intra-class feature compaction loss to constrain the divergence of action features, ensuring the stability of their distribution. We evaluated the proposed method on two challenging online action detection datasets, THUMOS14 and TVSeries. The results show that HCM is effective and efficient in online action detection and action anticipation tasks.
C1 [Liu, Siyu; Cheng, Jian; Xia, Ziying; Xi, Zhilong; Hou, Qin] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Dong, Zhicheng] Tibet Univ, Sch Informat Sci & Technol, Lhasa 850000, Peoples R China.
C3 University of Electronic Science & Technology of China; Tibet University
RP Cheng, J (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM syliu@std.uestc.edu.cn; chengjian@uestc.edu.cn; zyxia@std.uestc.edu.cn;
   xijohnston@gmail.com; uestchq@163.com; dongzc666@163.com
RI Xi, Zhilong/HSG-1432-2023; Xia, Ziying/JUU-4667-2023
OI Xi, Zhilong/0000-0002-1651-0190; Xia, Ziying/0000-0001-7559-1624; Cheng,
   Jian/0000-0001-6966-0531; Dong, Zhicheng/0000-0003-3415-7682
FU NNSFC amp; CAAC
FX No Statement Available
CR Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Junwen, 2022, P IEEECVF C COMPUTER, P19925
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chrungoo A, 2014, LECT NOTES ARTIF INT, V8755, P84, DOI 10.1007/978-3-319-11973-1_9
   De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17
   Deo N, 2017, IEEE INT C INTELL TR
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Eun H, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107695
   Eun H, 2020, PROC CVPR IEEE, P806, DOI 10.1109/CVPR42600.2020.00089
   Gao J., 2017, BMVC, P1
   Gao MF, 2021, PROC CVPR IEEE, P1915, DOI 10.1109/CVPR46437.2021.00195
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Islam A, 2020, IEEE WINT CONF APPL, P536, DOI 10.1109/WACV45572.2020.9093620
   Ji YL, 2020, IEEE T CIRC SYST VID, V30, P2114, DOI 10.1109/TCSVT.2019.2912988
   Jin CB, 2015, LECT NOTES COMPUT SC, V9315, P330, DOI 10.1007/978-3-319-24078-7_33
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
   Keller CG, 2014, IEEE T INTELL TRANSP, V15, P494, DOI 10.1109/TITS.2013.2280766
   Khan MA, 2024, MULTIMED TOOLS APPL, V83, P14885, DOI 10.1007/s11042-020-08806-9
   Kim YH, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107954
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu W, 2017, ADV SOC SCI EDUC HUM, V99, P212
   Lu JW, 2017, IEEE SIGNAL PROC MAG, V34, P76, DOI 10.1109/MSP.2017.2732900
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Martínez F, 2012, COMM COM INF SC, V346, P267
   McInnes L, 2020, Arxiv, DOI [arXiv:1802.03426, 10.21105/joss.00861]
   Merler M, 2019, IEEE T MULTIMEDIA, V21, P1147, DOI 10.1109/TMM.2018.2876046
   Paszke A, 2019, ADV NEUR IN, V32
   Peng WT, 2011, IEEE T MULTIMEDIA, V13, P539, DOI 10.1109/TMM.2011.2131638
   Qu SQ, 2020, Arxiv, DOI arXiv:2011.07915
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi BF, 2020, PROC CVPR IEEE, P1006, DOI 10.1109/CVPR42600.2020.00109
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sohn K, 2016, ADV NEUR IN, V29
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   van den Oord A, 2019, Arxiv, DOI [arXiv:1807.03748, DOI 10.48550/ARXIV.1807.03748]
   Vaswani A, 2017, ADV NEUR IN, V30
   Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813
   Wang F, 2018, IEEE SIGNAL PROC LET, V25, P926, DOI 10.1109/LSP.2018.2822810
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SF, 2019, IEEE T MULTIMEDIA, V21, P314, DOI 10.1109/TMM.2018.2859029
   Wang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7545, DOI 10.1109/ICCV48922.2021.00747
   Wang Z., 2020, P 16 EUR C COMP VIS, P107, DOI [10.1007/978-3-030-58621-8_7, DOI 10.1007/978-3-030-58621-8_7]
   Xiao QQ, 2017, Arxiv, DOI arXiv:1710.00478
   Xu MZ, 2021, ADV NEUR IN, V34
   Xu MZ, 2019, IEEE I CONF COMP VIS, P5531, DOI 10.1109/ICCV.2019.00563
   Yang L, 2022, PROC CVPR IEEE, P3150, DOI 10.1109/CVPR52688.2022.00316
   Yang L, 2022, IEEE T PATTERN ANAL, V44, P9814, DOI 10.1109/TPAMI.2021.3132058
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng RH, 2022, IEEE T PATTERN ANAL, V44, P6209, DOI 10.1109/TPAMI.2021.3090167
   Zhang C, 2021, PROC CVPR IEEE, P16005, DOI 10.1109/CVPR46437.2021.01575
   Zhao PS, 2020, Arxiv, DOI arXiv:2011.09158
NR 63
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 3626
EP 3639
DI 10.1109/TMM.2023.3313258
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IH1O9
UT WOS:001165348200018
DA 2024-08-05
ER

PT J
AU Lu, WQ
   Hu, HM
   Yu, JZ
   Zhou, YB
   Wang, HZ
   Li, B
AF Lu, Wei-Qing
   Hu, Hai-Miao
   Yu, Jinzuo
   Zhou, Yibo
   Wang, Hanzi
   Li, Bo
TI Orientation-Aware Pedestrian Attribute Recognition Based on Graph
   Convolution Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pedestrian attribute recognition; pedestrian-orientation; graph
   convolution network
AB Pedestrian attribute recognition (PAR) aims to generate a structured description of pedestrians and plays an important role in surveillance. Current work focusing on 2D images can achieve decent performance when there is no variation in the captured pedestrian orientation. However, the performance of these works cannot be maintained in scenarios when the orientation of pedestrians is ignored. To mitigate this problem, this paper proposes orientation-aware pedestrian attribute recognition based on graph convolution network (GCN), which is composed of an orientation-aware spatial attention (OSA) module and an orientation-guided attribute-relation learning (OAL) module. Since some attributes can be invisible for certain orientations, OSA is proposed for orientation-aware feature extraction to enhance the learned representation of the visual attributes. Moreover, since different orientations result in different relations among attributes, OAL is proposed to achieve distinguishable and impactful attribute relations by eliminating the confusion of attribute relations in different orientations. Experiments on three challenging datasets (PETA, RAP, and PA100 K) demonstrate that the proposed PAR outperforms the state-of-the-art methods by considerable margins.
C1 [Lu, Wei-Qing; Yu, Jinzuo; Zhou, Yibo; Li, Bo] Beihang Univ, Sch Comp Sci & Engn, Beijing Key Lab Digital Media, Beijing 100191, Peoples R China.
   [Hu, Hai-Miao] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
   [Hu, Hai-Miao] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Hanzi] Xiamen Univ, Sch Informat, Fujian Key Lab Sensing & Comp Smart City, Xiamen 361005, Peoples R China.
C3 Beihang University; Beihang University; Beihang University; Xiamen
   University
RP Hu, HM (corresponding author), Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
EM 574168985@qq.com; frank0139@163.com; 17377133@buaa.edu.cn;
   yiz875@outlook.com; hanzi.wang@xmu.edu.cn
OI Lu, Wei-Qing/0000-0002-1143-6745
FU Pioneer and Leading Goose Ramp;D Program of Zhejiang
FX No Statement Available
CR An HR, 2021, IEEE T MULTIMEDIA, V23, P268, DOI 10.1109/TMM.2020.2975417
   Deng YB, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P789, DOI 10.1145/2647868.2654966
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Fan HN, 2022, IEEE T MULTIMEDIA, V24, P49, DOI 10.1109/TMM.2020.3045286
   Feris R., 2014, P INT C MULT RETR, P153, DOI DOI 10.1145/2578726.2578732
   Gao L, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1340, DOI 10.1145/3343031.3351003
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gu ZX, 2019, IEEE INT CON MULTI, P550, DOI 10.1109/ICME.2019.00101
   Guo H, 2019, PROC CVPR IEEE, P729, DOI 10.1109/CVPR.2019.00082
   Guo H, 2017, PATTERN RECOGN LETT, V94, P38, DOI 10.1016/j.patrec.2017.05.012
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1636, DOI 10.1145/3123266.3123424
   Ji Z, 2023, IEEE T MULTIMEDIA, V25, P7699, DOI 10.1109/TMM.2022.3225754
   Ji Z, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104585
   Ji Z, 2020, PATTERN RECOGN LETT, V138, P170, DOI 10.1016/j.patrec.2020.07.018
   Jia J, 2022, AAAI CONF ARTIF INTE, P1069
   Jia J, 2021, Arxiv, DOI arXiv:2107.03576
   Jia Jian, 2021, P IEEECVF INT C COMP, P962
   Layne R, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.24
   Li DW, 2016, Arxiv, DOI arXiv:1603.07054
   Li DW, 2018, IEEE INT CON MULTI
   Li DW, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P111, DOI 10.1109/ACPR.2015.7486476
   Li WH, 2022, LECT NOTES COMPUT SC, V13672, P562, DOI 10.1007/978-3-031-19775-8_33
   Lin M, 2014, Arxiv, DOI arXiv:1312.4400
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Liu P., 2019, P BRIT MACH VIS C 20
   Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Moghaddam M, 2021, IET IMAGE PROCESS, V15, P2281, DOI 10.1049/ipr2.12195
   Ruby U., 2020, Int. J. Adv. Trends Comput. Sci. Eng., V9, P5393, DOI [10.30534/ijatcse/2020/175942020, DOI 10.30534/IJATCSE/2020/175942020]
   Sarfraz MS, 2017, Arxiv, DOI arXiv:1707.06089
   Sarafianos N, 2018, LECT NOTES COMPUT SC, V11215, P708, DOI 10.1007/978-3-030-01252-6_42
   Siddiquie B, 2011, PROC CVPR IEEE, P801, DOI 10.1109/CVPR.2011.5995329
   Sudowe P, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P329, DOI 10.1109/ICCVW.2015.51
   Tan ZC, 2020, AAAI CONF ARTIF INTE, V34, P12055
   Tang CF, 2019, IEEE I CONF COMP VIS, P4996, DOI 10.1109/ICCV.2019.00510
   Wang JY, 2017, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2017.65
   Wang X, 2022, PATTERN RECOGN, V121, DOI 10.1016/j.patcog.2021.108220
   Wu CY, 2020, PROC CVPR IEEE, P3448, DOI 10.1109/CVPR42600.2020.00351
   Wu MD, 2020, AAAI CONF ARTIF INTE, V34, P12394
   Xu Keyulu, 2018, PREPRINT, DOI DOI 10.48550/ARXIV.1810.00826
   Zagoruyko S., 2017, ICLR, DOI DOI 10.1016/J.CVIU.2019.07.006.ARXIV:1612.0
   Zhang JJ, 2020, Arxiv, DOI arXiv:2011.06798
   Zhang Si, 2019, Comput Soc Netw, V6, P11, DOI 10.1186/s40649-019-0069-y
   Zhao X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3177
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 46
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 28
EP 40
DI 10.1109/TMM.2023.3259686
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500018
DA 2024-08-05
ER

PT J
AU Shao, Z
   Han, JG
   Debattista, K
   Pang, YW
AF Shao, Zhuang
   Han, Jungong
   Debattista, Kurt
   Pang, Yanwei
TI DCMSTRD: End-to-end Dense Captioning via Multi-Scale Transformer
   Decoding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Transformers; Visualization; Feature extraction; Task
   analysis; Computer architecture; Training; Dense captioning;
   artificially designed modules; end-to-end dense captioning framework via
   multi-scale transformer decoding (DCMSTRD); multi-scale language decoder
   (MSLD)
AB Dense captioning creates diverse Region of Interests (RoIs) descriptions for complex visual scenes. While promising results have been obtained, several issues persist. In particular: 1) it is hard to find the optimal parameters for artificially designed modules (e.g., non-maximum suppression (NMS)) causing redundancies and fewer interactions to benefit the two sub-tasks of RoI detection and RoI captioning; 2) the absence of a multi-scale decoder in current methods hinders the acquisition of scale-invariant features, thus leading to poor performance. To tackle these limitations, we bypass the artificially designed modules and present an end-to-end dense captioning framework via multi-scale transformer decoding (DCMSTRD). DCMSTRD solves dense captioning by set matching and prediction instead. To further enhance the discriminative quality of the multi-scale representations during caption generation, we introduce a multi-scale module, termed multi-scale language decoder (MSLD). Our proposed method tested on standard datasets achieves a mean Average Precision (mAP) of 16.7% on the challenging VG-COCO dataset, demonstrating its effectiveness against the current methods.
C1 [Shao, Zhuang] Newcastle Univ, Sch Engn, Newcastle Upon Tyne NE1 7RU, England.
   [Han, Jungong] Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, England.
   [Debattista, Kurt] Univ Warwick, Warwick Mfg Grp, Coventry CV4 7AL, England.
   [Pang, Yanwei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Pang, Yanwei] Shanghai Artificial Intelligence Lab, Shanghai 200232, Peoples R China.
C3 Newcastle University - UK; University of Sheffield; University of
   Warwick; Tianjin University
RP Han, JG (corresponding author), Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, England.
EM zhuang.shao@newcastle.ac.uk; jungonghan77@gmail.com;
   K.Debattista@warwick.ac.uk; pyw@tju.edu.cn
FU National Key Research and Development Program of China
FX No Statement Available
CR ahajan S., 2020, ARXIV201100966, P3613
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bello I, 2019, IEEE I CONF COMP VIS, P3285, DOI 10.1109/ICCV.2019.00338
   Cao JL, 2022, PROC CVPR IEEE, P9448, DOI 10.1109/CVPR52688.2022.00924
   Carion N., 2020, EUR C COMP VIS, P213
   Chang JC, 2023, MULTIMEDIA SYST, V29, P3891, DOI 10.1007/s00530-023-01166-y
   Chen Nuo, 2023, MM '23: Proceedings of the 31st ACM International Conference on Multimedia, P3787, DOI 10.1145/3581783.3613444
   Cho K., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1179
   Chu FC, 2022, LECT NOTES COMPUT SC, V13604, P343, DOI 10.1007/978-3-031-20497-5_28
   Gao AQ, 2024, IEEE T CIRC SYST VID, V34, P2000, DOI 10.1109/TCSVT.2022.3202810
   Gao MQ, 2023, ARTIF INTELL REV, V56, P457, DOI 10.1007/s10462-022-10176-7
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Giudice N.A., 2008, Blind Navigation and the Role of Technology, P479, DOI [10.1002/9780470379424.ch25, DOI 10.1002/9780470379424.CH25]
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Ji Z, 2022, IMAGE VISION COMPUT, V128, DOI 10.1016/j.imavis.2022.104585
   Jiang WT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3460474
   Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li XY, 2019, AAAI CONF ARTIF INTE, P8650
   Li ZH, 2023, IEEE T PATTERN ANAL, V45, P10555, DOI 10.1109/TPAMI.2023.3257546
   Liu AA, 2022, IEEE T NEUR NET LEAR, V33, P7655, DOI 10.1109/TNNLS.2021.3086066
   Liu AA, 2019, MULTIMED TOOLS APPL, V78, P677, DOI 10.1007/s11042-017-5532-x
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu Y, 2023, NEUROCOMPUTING, V517, P213, DOI 10.1016/j.neucom.2022.09.048
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Liu Z, 2024, IEEE T PATTERN ANAL, V46, P543, DOI 10.1109/TPAMI.2023.3323012
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Parmar N, 2018, PR MACH LEARN RES, V80
   Qian TW, 2023, IEEE T MULTIMEDIA, V25, P3950, DOI 10.1109/TMM.2022.3169065
   Qiu S, 2020, IEEE T MULTIMEDIA, V22, P1333, DOI 10.1109/TMM.2019.2942480
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shao Z, 2023, IEEE T MULTIMEDIA, V25, P8753, DOI 10.1109/TMM.2023.3241517
   Shao Zhuang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3152990
   Sharma Himanshu, 2020, 2020 International Conference on Power Electronics & IoT Applications in Renewable Energy and its Control (PARC), P325, DOI 10.1109/PARC49193.2020.236619
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Song D, 2024, COMPUT VIS IMAGE UND, V238, DOI 10.1016/j.cviu.2023.103858
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J., 2020, ECCV, P370
   Wang JB, 2024, PATTERN RECOGN, V147, DOI 10.1016/j.patcog.2023.110047
   Wang T, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6827, DOI 10.1109/ICCV48922.2021.00677
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2021, IEEE T CIRC SYST VID, V31, P1031, DOI 10.1109/TCSVT.2020.2990989
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xu X, 2020, IEEE T NEUR NET LEAR, V31, P5412, DOI 10.1109/TNNLS.2020.2967597
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Yang LJ, 2017, PROC CVPR IEEE, P1978, DOI 10.1109/CVPR.2017.214
   Yin GJ, 2019, PROC CVPR IEEE, P6234, DOI 10.1109/CVPR.2019.00640
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
   Zhu X., 2021, ICLR, P1, DOI DOI 10.48550/ARXIV.2010.04159
NR 59
TC 0
Z9 0
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7581
EP 7593
DI 10.1109/TMM.2024.3369863
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000061
OA Green Accepted
DA 2024-08-05
ER

PT J
AU Su, SL
   Lin, HH
   Hosu, V
   Wiedemann, O
   Sun, JQ
   Zhu, Y
   Liu, HT
   Zhang, YN
   Saupe, D
AF Su, Shaolin
   Lin, Hanhe
   Hosu, Vlad
   Wiedemann, Oliver
   Sun, Jinqiu
   Zhu, Yu
   Liu, Hantao
   Zhang, Yanning
   Saupe, Dietmar
TI Going the Extra Mile in Face Image Quality Assessment: A Novel Database
   and Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Faces; Image quality; Task analysis; Predictive
   models; Databases; Codes; Image quality assessment; face quality;
   subjective study; GAN; generative priors
ID NEURONS
AB An accurate computational model for image quality assessment (IQA) benefits many vision applications, such as image filtering, image processing, and image generation. Although the study of face images is an important subfield in computer vision research, the lack of face IQA data and models limits the precision of current IQA metrics on face image processing tasks such as face superresolution, face enhancement, and face editing. To narrow this gap, in this article, we first introduce the largest annotated IQA database developed to date, which contains 20,000 human faces - an order of magnitude larger than all existing rated datasets of faces - of diverse individuals in highly varied circumstances. Based on the database, we further propose a novel deep learning model to accurately predict face image quality, which, for the first time, explores the use of generative priors for IQA. By taking advantage of rich statistics encoded in well pretrained off-the-shelf generative models, we obtain generative prior information and use it as latent references to facilitate blind IQA. The experimental results demonstrate both the value of the proposed dataset for face IQA and the superior performance of the proposed model.
C1 [Su, Shaolin; Sun, Jinqiu; Zhu, Yu; Zhang, Yanning] Northwestern Polytech Univ, Sch Comp Sci & Engn, Xian 710071, Peoples R China.
   [Su, Shaolin; Lin, Hanhe; Hosu, Vlad; Wiedemann, Oliver; Saupe, Dietmar] Univ Konstanz, Dept Comp & Informat Sci, D-78464 Constance, Germany.
   [Lin, Hanhe] Univ Dundee, Sch Sci & Engn, Dundee DD1 4HN, Scotland.
C3 Northwestern Polytechnical University; University of Konstanz;
   University of Dundee
RP Lin, HH (corresponding author), Univ Dundee, Sch Sci & Engn, Dundee DD1 4HN, Scotland.
EM shaolin_su@mail.nwpu.edu.cn; hlin001@dundee.ac.uk;
   vlad.hosu@uni-konstanz.de; oliver.wiedemann@uni-konstanz.de;
   sunjinqiu@nwpu.edu.cn; yuzhu@nwpu.edu.cn; liuh35@cardiff.ac.uk;
   ynzhang@nwpu.edu.cn; dietmar.saupe@uni-konstanz.de
OI Su, Shaolin/0000-0002-0600-5545; Lin, Hanhe/0000-0002-0297-3549
FU Deutsche Forschungsgemeinschaft
FX No Statement Available
CR Abdal R., 2020, P IEEECVF C COMPUTER, P8296
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alharbi Y, 2020, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR42600.2020.00518
   Best-Rowden L, 2018, IEEE T INF FOREN SEC, V13, P3064, DOI 10.1109/TIFS.2018.2799585
   Bharadwaj S, 2013, IEEE IMAGE PROC, P2792, DOI 10.1109/ICIP.2013.6738575
   Brock A., 2018, P INT C LEARN REPR
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P90, DOI 10.1109/LSP.2014.2347419
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S.A., 2022, P IEEECVF WINTER C A, P1220
   Grother P., 2020, NIST Interagency Rep.
   Gu JJ, 2020, PROC CVPR IEEE, P3009, DOI 10.1109/CVPR42600.2020.00308
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang G. B., 2007, Technical report
   ITUT, 2021, Recommendation itut p.913 (06/2021) methods for the subjective assessment of video quality, audio quality and audiovisual quality of internet video and distribution quality television in any environment, P52
   Kancharla P, 2022, IEEE T IMAGE PROCESS, V31, P263, DOI 10.1109/TIP.2021.3130541
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T., 2018, INT C LEARNING REPRE
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Kingma D. P., 2014, arXiv
   Ko H, 2020, IEEE T IMAGE PROCESS, V29, P5964, DOI 10.1109/TIP.2020.2987180
   Le J., 2010, SIGIR 2010 WORKSH GR, V2126, P22
   Li BW, 2022, IEEE T CIRC SYST VID, V32, P5944, DOI 10.1109/TCSVT.2022.3164467
   Lin KY, 2018, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2018.00083
   Liu LX, 2019, IEEE T MULTIMEDIA, V21, P2305, DOI 10.1109/TMM.2019.2900941
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   OScalaidhe SP, 1997, SCIENCE, V278, P1135, DOI 10.1126/science.278.5340.1135
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   PERRETT DI, 1987, TRENDS NEUROSCI, V10, P358, DOI 10.1016/0166-2236(87)90071-3
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Ro T, 2001, PSYCHOL SCI, V12, P94, DOI 10.1111/1467-9280.00317
   Schlett T, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3507901
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sellahewa H, 2010, IEEE T INSTRUM MEAS, V59, P805, DOI 10.1109/TIM.2009.2037989
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su S., 2021, P 32 BRIT MACH VIS C, P1
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Sun SM, 2023, IEEE T MULTIMEDIA, V25, P2912, DOI 10.1109/TMM.2022.3152942
   Theeuwes J, 2006, VIS COGN, V13, P657, DOI 10.1080/13506280500410949
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Tian Y, 2022, Arxiv, DOI arXiv:2201.11975
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang XT, 2021, PROC CVPR IEEE, P9164, DOI 10.1109/CVPR46437.2021.00905
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Wu YZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14357, DOI 10.1109/ICCV48922.2021.01411
   Xia WH, 2023, IEEE T PATTERN ANAL, V45, P3121, DOI 10.1109/TPAMI.2022.3181070
   Xingang Pan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P262, DOI 10.1007/978-3-030-58536-5_16
   Xu JT, 2016, IEEE T IMAGE PROCESS, V25, P4444, DOI 10.1109/TIP.2016.2585880
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Yang C, 2021, IEEE T MULTIMEDIA, V23, P1557, DOI 10.1109/TMM.2020.3001537
   Yang T, 2021, PROC CVPR IEEE, P672, DOI 10.1109/CVPR46437.2021.00073
   Yang XH, 2021, IEEE T MULTIMEDIA, V23, P4326, DOI 10.1109/TMM.2020.3040529
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zeng H, 2017, Arxiv, DOI arXiv:1708.08190
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhao X., 2019, PROC INT C COMPUT PA, P288
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 74
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2671
EP 2685
DI 10.1109/TMM.2023.3301276
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400018
OA Green Published, Green Submitted, Green Accepted
DA 2024-08-05
ER

PT J
AU Tang, W
   Qing, LB
   Li, LD
   Wang, YC
   Zhu, C
AF Tang, Wang
   Qing, Linbo
   Li, Lindong
   Wang, Yuchen
   Zhu, Ce
TI Progressive Graph Reasoning-Based Social Relation Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Social relation recognition; progressive graph reasoning; social
   relations model; social psychology; transformer
ID INTERPERSONAL-ATTRACTION; PERSONALITY; NETWORK; MODEL
AB Identifying relationships between people from images is essential for studying social activities and interactions, and this has significant potential to further the understanding of human social behaviors. Existing image-based research mainly explores social relationships at the dyadic level, i.e., recognizing pairwise relationships based on visual features of persons, objects, and scenes and their logical constraints. Notably, social relational structures are hierarchically nested, i.e., individuals and dyads are nested within group structures, as indicated in the social relations model (SRM) of social psychology. However, existing computer vision-based studies fail to consider hierarchical nested structures, thus overlooking some of the most important interactions, which leads to poor relation reasoning. To improve the performance of reasoning neural networks, we propose a novel SRM framework for progressive graph reasoning (PGR) to explore social interactions. Specifically, we construct individual-dyad and dyad-group graphs to progressively explore the impact of individuals and groups on recognition of dyadic relationships. A transformer is utilized to fuse visual features and graph reasoning knowledge into a comprehensive representation of social relationships. We demonstrate the effectiveness of the proposed model based on PGR using several public datasets and perform extensive ablation studies to explore the reasons behind its superior performance. Experimental results demonstrate that our proposed model successfully predicts social relationships with higher accuracy than state-of-the-art methods.
C1 [Tang, Wang; Qing, Linbo; Li, Lindong; Wang, Yuchen] Sichuan Univ, Sch Elect Informat, Chengdu 610065, Peoples R China.
   [Zhu, Ce] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610065, Peoples R China.
   [Zhu, Ce] Univ Elect Sci & Technol China, Ctr Robot, Chengdu 610051, Peoples R China.
C3 Sichuan University; University of Electronic Science & Technology of
   China; University of Electronic Science & Technology of China
RP Qing, LB (corresponding author), Sichuan Univ, Sch Elect Informat, Chengdu 610065, Peoples R China.
EM tangwang@stu.scu.edu.cn; qing_lb@scu.edu.cn; lilindong@stu.scu.edu.cn;
   wangyuchen98@stu.scu.edu.cn; eczhu@uestc.edu.cn
OI Tang, Wang/0000-0002-6925-9067
FU National Natural Science Foundation of China
FX No Statement Available
CR Back MD, 2011, EUR J PERSONALITY, V25, P225, DOI 10.1002/per.790
   Becker H, 1942, AM SOCIOL REV, V7, P13, DOI 10.2307/2086253
   BIDDLE BJ, 1986, ANNU REV SOCIOL, V12, P67, DOI 10.1146/annurev.so.12.080186.000435
   Bond CF Jr, 2008, PSYCHOL BULL, V134, P477, DOI 10.1037/0033-2909.134.4.477
   Bugental DB, 2000, PSYCHOL BULL, V126, P187, DOI 10.1037/0033-2909.126.2.187
   Cao L, 2022, IEEE T MULTIMEDIA, V24, P87, DOI 10.1109/TMM.2020.3046867
   Ding YJ, 2022, IEEE T MULTIMEDIA, V24, P2287, DOI 10.1109/TMM.2021.3078907
   Gao JJ, 2021, NEUROCOMPUTING, V456, P243, DOI 10.1016/j.neucom.2021.05.097
   GIFFORD R, 1986, J NONVERBAL BEHAV, V10, P207, DOI 10.1007/BF00987480
   Goel A, 2019, PROC CVPR IEEE, P11178, DOI 10.1109/CVPR.2019.01144
   Guo XM, 2022, CITIES, V121, DOI 10.1016/j.cities.2021.103487
   KENNY DA, 1984, ADV EXP SOC PSYCHOL, V18, P141, DOI 10.1016/S0065-2601(08)60144-6
   KENNY DA, 1988, J NONVERBAL BEHAV, V12, P34, DOI 10.1007/BF00987351
   Kipf T.N., 2017, INT C LEARN REPR, P1
   Krause S, 2014, SOC PSYCHOL PERS SCI, V5, P671, DOI 10.1177/1948550613517723
   Leckelt M, 2015, J PERS SOC PSYCHOL, V109, P856, DOI 10.1037/pspp0000057
   Li C., 2022, IEEE Trans. Multimedia, DOI [10.1109/TMM.2022.3182151, DOI 10.1109/TMM.2022.3182151]
   Li D., 2016, P INT C LEARN REPR, P1
   Li JN, 2017, IEEE I CONF COMP VIS, P2669, DOI 10.1109/ICCV.2017.289
   Li LD, 2022, VISUAL COMPUT, V38, P3979, DOI 10.1007/s00371-021-02244-w
   Malloy T. E., 2018, Social relations modeling of behavior in dyads and groups
   MALLOY TE, 1986, J PERS, V54, P199, DOI 10.1111/j.1467-6494.1986.tb00393.x
   Malloy TE, 2018, EUR J SOC PSYCHOL, V48, P285, DOI 10.1002/ejsp.2324
   Nestler S, 2022, J EDUC BEHAV STAT, V47, P231, DOI 10.3102/10769986211056541
   Niu T, 2022, BUILD ENVIRON, V225, DOI 10.1016/j.buildenv.2022.109563
   Qing LB, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13112038
   Ramanathan V, 2013, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2013.320
   Reis HT, 2011, J PERS SOC PSYCHOL, V101, P557, DOI 10.1037/a0022885
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Shao ZF, 2022, IEEE T MULTIMEDIA, V24, P2069, DOI 10.1109/TMM.2021.3075566
   Sun QR, 2017, PROC CVPR IEEE, P435, DOI 10.1109/CVPR.2017.54
   Tang WJ, 2019, IEEE T MULTIMEDIA, V21, P579, DOI 10.1109/TMM.2018.2889934
   Taylor SE, 2000, PSYCHOL REV, V107, P411, DOI 10.1037/0033-295X.107.3.411
   Wang MY, 2020, PATTERN RECOGN LETT, V138, P410, DOI 10.1016/j.patrec.2020.08.005
   Wang YC, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155749
   Wang ZX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1021
   Wanhua Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P18, DOI 10.1007/978-3-030-58555-6_2
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Yang XM, 2021, IEEE ACCESS, V9, P99398, DOI 10.1109/ACCESS.2021.3096553
   Zhang M, 2019, IEEE INT CON MULTI, P1618, DOI 10.1109/ICME.2019.00279
   Zhang ZP, 2015, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2015.414
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou C, 2019, NAT HUM BEHAV, V3, P847, DOI 10.1038/s41562-019-0618-2
NR 43
TC 0
Z9 0
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6012
EP 6024
DI 10.1109/TMM.2023.3344359
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NB0Y1
UT WOS:001197874100026
DA 2024-08-05
ER

PT J
AU Wang, J
   Li, F
   Zhang, XC
   Sun, HB
AF Wang, Jian
   Li, Fan
   Zhang, Xuchong
   Sun, Hongbin
TI Adversarial Obstacle Generation Against LiDAR-Based 3D Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Laser radar; Point cloud compression;
   Sensors; Detectors; Perturbation methods; Solid modeling; Adversarial
   attack; LiDAR simulation; 3D object detection; point cloud perturbation
AB LiDAR sensors are widely used in many safety-critical applications such as autonomous driving and drone control, and the collected data called point clouds are subsequently processed by 3D object detectors for visual perception. Recent works have shown that attackers can inject virtual points into LiDAR sensors by strategically transmitting laser pulses to them; additionally, deep visual models have been found to be vulnerable to carefully crafted adversarial examples. Therefore, a LiDAR-based perception may be maliciously attacked with serious safety consequences. In this article, we present a highly-deceptive adversarial obstacle generation algorithm against deep 3D detection models, to mimic fake obstacles within the effective detection range of LiDAR using a limited number of points. To achieve this goal, we first perform a physical LiDAR simulation to construct sparse obstacle point clouds. Then, we devise a strong attack strategy to adversarially perturb prototype points along each direction of the ray. Our method achieves a high attack success rate while complying with physical laws at the hardware level. We perform comprehensive experiments on different types of 3D detectors and determine that the voxel-based detectors are more vulnerable to adversarial attacks than the point-based methods. For example, our approach achieves an 89% mean attack success rate against PV-RCNN by using only 20 points to spoof a fake car.
C1 [Wang, Jian; Li, Fan] Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
   [Zhang, Xuchong; Sun, Hongbin] Xi An Jiao Tong Univ, Coll Artificial Intelligence, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Li, F (corresponding author), Xi An Jiao Tong Univ, Sch Informat & Commun Engn, Shaanxi Key Lab Deep Space Explorat Intelligent In, Xian 710049, Peoples R China.
EM wj851329121@stu.xjtu.edu.cn; lifan@mail.xjtu.edu.cn;
   zhangxc0329@mail.xjtu.edu.cn; hsun@mail.xjtu.edu.cn
OI Wang, Jian/0000-0002-4091-2165
FU Natural Science Basic Research Plan in Shaanxi Province of China
FX No Statement Available
CR Behley J, 2019, IEEE I CONF COMP VIS, P9296, DOI 10.1109/ICCV.2019.00939
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   Cao Y., 2019, arXiv
   Cao YL, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2267, DOI 10.1145/3319535.3339815
   Deng JJ, 2021, AAAI CONF ARTIF INTE, V35, P1201
   Duan R., 2021, PROC IEEE C COMPUT V, P16062
   Feng D, 2021, IEEE T INTELL TRANSP, V22, P1341, DOI 10.1109/TITS.2020.2972974
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Hamdi Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P241, DOI 10.1007/978-3-030-58610-2_15
   Han XF, 2023, IEEE T MULTIMEDIA, V25, P5638, DOI 10.1109/TMM.2022.3198318
   Hu JSK, 2022, PROC CVPR IEEE, P8459, DOI 10.1109/CVPR52688.2022.00828
   Goodfellow IJ, 2015, Arxiv, DOI [arXiv:1412.6572, DOI 10.48550/ARXIV.1412.6572]
   Kingma D. P., 2014, arXiv
   Kurakin A., 2018, Artificial Intelligence Safety and Security, P99, DOI DOI 10.1201/9781351251389-8
   Kurakin A, 2017, Arxiv, DOI arXiv:1611.01236
   Lang AH, 2019, PROC CVPR IEEE, P12689, DOI 10.1109/CVPR.2019.01298
   Liu Daniel, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P88, DOI 10.1007/978-3-030-66415-2_6
   Liu D, 2019, IEEE IMAGE PROC, P2279, DOI 10.1109/icip.2019.8803770
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Manivasagam Sivabalan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11164, DOI 10.1109/CVPR42600.2020.01118
   Mumuxin Cai, 2020, 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), P1042, DOI 10.1109/HPCC-SmartCity-DSS50907.2020.00140
   O. D. Team, 2020, "OpenPCDet: An open-source toolbox for 3D object detection from point clouds
   Paden B, 2016, IEEE T INTELL VEHICL, V1, P33, DOI 10.1109/TIV.2016.2578706
   Petit J., 2015, Black Hat Europe
   Qi C. R., 2017, ADV NEURAL INFORM PR, P5099, DOI DOI 10.1109/CVPR.2017.16
   Qi CR, 2018, PROC CVPR IEEE, P918, DOI 10.1109/CVPR.2018.00102
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Rabe M, 2021, IEEE COMPUT SOC CONF, P129, DOI 10.1109/CVPRW53098.2021.00023
   Sanchez-Matilla R, 2020, IEEE T MULTIMEDIA, V22, P1862, DOI 10.1109/TMM.2020.2987694
   Shan JY, 2023, IEEE T MULTIMEDIA, V25, P2339, DOI 10.1109/TMM.2022.3146714
   Shaoshuai Shi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10526, DOI 10.1109/CVPR42600.2020.01054
   Shi SS, 2023, INT J COMPUT VISION, V131, P531, DOI 10.1007/s11263-022-01710-9
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi SS, 2021, IEEE T PATTERN ANAL, V43, P2647, DOI 10.1109/TPAMI.2020.2977026
   Shin H, 2017, LECT NOTES COMPUT SC, V10529, P445, DOI 10.1007/978-3-319-66787-4_22
   Sun JC, 2020, PROCEEDINGS OF THE 29TH USENIX SECURITY SYMPOSIUM, P877
   Szegedy C, 2014, Arxiv, DOI arXiv:1312.6199
   Tsai T, 2020, AAAI CONF ARTIF INTE, V34, P954
   Tu James, 2020, P IEEE CVF C COMP VI, P13716, DOI DOI 10.1109/CVPR42600.2020.01373
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wang XP, 2021, NEUROCOMPUTING, V466, P27, DOI 10.1016/j.neucom.2021.09.027
   Wen YX, 2022, IEEE T PATTERN ANAL, V44, P2984, DOI 10.1109/TPAMI.2020.3044712
   Wicker M, 2019, PROC CVPR IEEE, P11759, DOI 10.1109/CVPR.2019.01204
   Xiang C, 2019, PROC CVPR IEEE, P9128, DOI 10.1109/CVPR.2019.00935
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Yan Y, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18103337
   Yang ZT, 2019, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2019.00204
   Yuan HJ, 2023, IEEE T MULTIMEDIA, V25, P203, DOI 10.1109/TMM.2021.3124083
   Zhang YF, 2022, PROC CVPR IEEE, P18931, DOI 10.1109/CVPR52688.2022.01838
   Zhou QY, 2018, Arxiv, DOI arXiv:1801.09847
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
   Zhu HQ, 2023, IEEE T MULTIMEDIA, V25, P5291, DOI 10.1109/TMM.2022.3189778
NR 53
TC 0
Z9 0
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 2686
EP 2699
DI 10.1109/TMM.2023.3302018
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JL4D3
UT WOS:001173299400009
DA 2024-08-05
ER

PT J
AU Wang, RM
   Zhu, ZL
   Zhu, YB
   Chen, H
   Liao, YZ
   Zhu, ZY
   Ding, YJ
   Gao, CX
   Sang, N
AF Wang, Runmin
   Zhu, Zhenlin
   Zhu, Yanbin
   Chen, Hua
   Liao, Yongzhong
   Zhu, Ziyu
   Ding, Yajun
   Gao, Changxin
   Sang, Nong
TI DIMGNet: A Transformer-Based Network for Pedestrian Reidentification
   With Multi-Granularity Information Mutual Gain
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pedestrians; Feature extraction; Transformers; Computer architecture;
   Task analysis; Cameras; Data mining; Cross-attention mechanism;
   information mutual gain; pedestrian reidentification (ReID); transformer
AB Pedestrian reidentification (ReID) is a challenging task that involves identifying and retrieving specific pedestrians across different cameras and scenes. This problem has significant implications for security surveillance, and has thus received substantial attention in recent years. However, traditional convolutional neural networks (CNNs) have limited receptive fields and cannot capture global information. Moreover, transformer networks, which excel in long-range feature capture, are prone to accuracy degradation due to loss of details. To address these limitations, we propose a transformer-based pedestrian ReID network with double-branch information mutual gain (DIMGNet), which leverages hierarchical parallel levels to support multi-granularity feature information mutual gain. Our model also incorporates an auxiliary camera information (ACI) module to improve feature representation ability. We further embed a cross-attention mechanism into the architecture to enhance mutual gain between multi-granularity features and improve feature discrimination. Finally, we introduce a shuffling technique to increase the robustness of the extracted features. We evaluate the proposed method on several benchmark datasets, including Market-1501 (Zhou et al., 2022), MSMT17 (Wei et al., 2018), DukeMTMC-reID (Ristani et al., 2016), and Occluded-Duke (Miao et al., 2019), achieving mAP values of 90.7%, 68.4%, 83.7%, and 60.6%, respectively. Our method outperforms most state-of-the-art methods, demonstrating the effectiveness of our method.
C1 [Wang, Runmin; Zhu, Zhenlin; Zhu, Yanbin; Chen, Hua; Zhu, Ziyu; Ding, Yajun] Hunan Nor mal Univ, Sch Informat Sci & Engn, Changsha 410081, Peoples R China.
   [Liao, Yongzhong] Hunan First Normal Univ, Sch Elect Informat, Changsha 410205, Peoples R China.
   [Gao, Changxin] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan 430074, Peoples R China.
C3 Hunan First Normal University; Huazhong University of Science &
   Technology
RP Liao, YZ (corresponding author), Hunan First Normal Univ, Sch Elect Informat, Changsha 410205, Peoples R China.
EM runminwang@hunnu.edu.cn; zhu_zhenlin@hunnu.edu.cn;
   yanbinzhu@hunnu.edu.cn; huachen@hunnu.edu.cn; lyz031608@126.com;
   zzyhuhu@163.com; yajunding@hunnu.edu.cn; cgao@hust.edu.cn;
   nsang@hust.edu.cn
RI zhu, ziyu/HPF-8036-2023; Gao, Changxin/L-4841-2016
OI zhu, ziyu/0000-0003-3836-6641; chen, hua/0000-0003-3398-8214; Wang,
   Runmin/0000-0001-9687-9918; sang, nong/0000-0002-9167-1496; zhu,
   yanbin/0009-0006-1159-5769; Gao, Changxin/0000-0003-2736-3920
FU National Natural Science Foundation of China
FX No Statement Available
CR Brown T., 2020, ADV NEURAL INFORM PR, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen CF, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P347, DOI 10.1109/ICCV48922.2021.00041
   Chen H., 2018, P IEEE 4 INT C MULT, P1, DOI DOI 10.1109/BIGMM.2018.8499067
   Chen J, 2022, IEEE T MULTIMEDIA, V24, P4285, DOI 10.1109/TMM.2021.3114539
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Chen YB, 2017, IEEE INT CONF COMP V, P2590, DOI 10.1109/ICCVW.2017.304
   Chen Y, 2022, PATTERN RECOGN LETT, V157, P90, DOI 10.1016/j.patrec.2022.03.020
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Cheng K., 2021, Acta Electronica Sinica, V49
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2021, PROC ICLR
   Fan BY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P655, DOI 10.1145/3394171.3414038
   Fang PF, 2019, IEEE I CONF COMP VIS, P8029, DOI 10.1109/ICCV.2019.00812
   Gao Z, 2021, IEEE T MULTIMEDIA, V23, P3332, DOI 10.1109/TMM.2020.3023784
   Hadsell R., 2006, Computer vision and pattern recognition, 2006 IEEE computer society conference on, V2, P1735, DOI DOI 10.1109/CVPR.2006.100
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang W., 2021, Appl. Sci., V11
   Jiang B, 2021, IEEE T MULTIMEDIA, V24, P3218, DOI 10.1109/TMM.2021.3095789
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li QL, 2022, Arxiv, DOI arXiv:2205.11197
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YY, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107281
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Liang JL, 2019, 2019 IEEE FIFTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM 2019), P366, DOI [10.1109/BigMM.2019.00017, 10.1109/BigMM.2019.00065]
   Liang TF, 2023, IEEE T MULTIMEDIA, V25, P8432, DOI 10.1109/TMM.2023.3237155
   Lingxiao He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P357, DOI 10.1007/978-3-030-58604-1_22
   Liu HJ, 2021, IEEE SIGNAL PROC LET, V28, P653, DOI 10.1109/LSP.2021.3065903
   Liu H, 2017, IEEE T IMAGE PROCESS, V26, P3492, DOI 10.1109/TIP.2017.2700762
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Lu YC, 2022, PROC CVPR IEEE, P7319, DOI 10.1109/CVPR52688.2022.00718
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Matsukawa T, 2016, INT C PATT RECOG, P2428, DOI 10.1109/ICPR.2016.7900000
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Parmar N, 2018, PR MACH LEARN RES, V80
   Peng ZL, 2023, IEEE T PATTERN ANAL, V45, P9454, DOI [10.1109/TPAMI.2023.3243048, 10.1109/ICASSP49357.2023.10094608]
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Tao YS, 2023, IEEE T MULTIMEDIA, V25, P4586, DOI 10.1109/TMM.2022.3178599
   [田永林 Tian Yonglin], 2022, [自动化学报, Acta Automatica Sinica], V48, P957
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C, 2018, LECT NOTES COMPUT SC, V11208, P384, DOI 10.1007/978-3-030-01225-0_23
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang HC, 2022, PROC CVPR IEEE, P7287, DOI 10.1109/CVPR52688.2022.00715
   Wang J, 2018, PATTERN RECOGN, V74, P38, DOI 10.1016/j.patcog.2017.09.014
   Wang ZK, 2022, PROC CVPR IEEE, P4744, DOI 10.1109/CVPR52688.2022.00471
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Yong Z., 2020, Acta Automatica Sinica, V41, P1
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhang ZZ, 2022, IEEE T MULTIMEDIA, V24, P4158, DOI 10.1109/TMM.2021.3115451
   Zhang ZZ, 2019, PROC CVPR IEEE, P667, DOI 10.1109/CVPR.2019.00076
   Zhang Z, 2024, IEEE T MULTIMEDIA, V26, P2303, DOI 10.1109/TMM.2023.3294816
   [赵才荣 Zhao Cairong], 2021, [中国科学. 信息科学, Scientia Sinica Informationis], V51, P1979
   Zhedong Zheng, 2017, ACM Transactions on Multimedia Computing, Communications and Applications, V14, DOI 10.1145/3159171
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
   Zhou QQ, 2022, IEEE T NEUR NET LEAR, V33, P6627, DOI 10.1109/TNNLS.2021.3082701
   Zhu K, 2023, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2023.3301856
NR 67
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6513
EP 6528
DI 10.1109/TMM.2024.3352896
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600043
DA 2024-08-05
ER

PT J
AU Wang, Y
   Kang, HB
   Wu, DD
   Yang, WM
   Zhang, LB
AF Wang, Yong
   Kang, Hongbo
   Wu, Doudou
   Yang, Wenming
   Zhang, Longbin
TI Global and Local Spatio-Temporal Encoder for 3D Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Correlation; Transformers; Pose estimation;
   Videos; Task analysis; Solid modeling; 3D Human pose estimation;
   transformer; spatio-temporal encoder; parallel attention
AB Transformers have been used for 3D human pose estimation with excellent performance; however, most transformers focus on encoding the global spatio-temporal correlation of all joints in the human body and there are few studies on the local Spatio-temporal correlation of each joint in the human body. In this article, we propose a Global and Local Spatio-Temporal Encoder (GLSTE) to model the Spatio-temporal correlation. Specifically, a Global Spatial Encoder (GSE) and a Global Temporal Encoder (GTE) are constructed to capture the global spatial information of all joints in a single frame and the global temporal information of all frames, respectively. A Local Spatio-Temporal Encoder (LSTE) is constructed to capture the spatial and temporal information of each joint in the local N frames. Furthermore, we propose a parallel attention module with weight sharing to better incorporate spatial and temporal information into each node simultaneously. Extensive experiments show that GLSTE outperforms state-of-the-art methods with fewer parameters and less computational overhead on two challenging datasets: Human3.6 M and MPI-INF-3DHP. Especially in the evaluation of Human3.6 M dataset, the results of our method with 27 frames as input are better than the vast majority of recent SOTA methods with 81 and 243 frames as input, which indicates that the model can learn more useful information with smaller inputs.
C1 [Wang, Yong; Kang, Hongbo; Wu, Doudou] Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 400054, Peoples R China.
   [Yang, Wenming] Tsinghua Univ, Shenzhen Int Grad Sch, Dept Elect Engn, Shenzhen 518055, Peoples R China.
   [Zhang, Longbin] Nanyang Technol Univ, Rehabil Res Inst Singapore, Sch Mech & Aerosp Engn, Singapore 639798, Singapore.
C3 Chongqing University of Technology; Tsinghua University; Nanyang
   Technological University
RP Kang, HB (corresponding author), Chongqing Univ Technol, Sch Artificial Intelligence, Chongqing 400054, Peoples R China.
EM ywang@cqut.edu.cn; khb1698@163.com; doudouwu@stu.cqut.edu.cn;
   yang.wenming@sz.tsinghua.edu.cn; longbin.zhang@ntu.edu.sg
RI wang, yong/ISB-5675-2023
OI Zhang, Longbin/0000-0001-8785-5885; Yang, Wenming/0000-0002-2506-1286;
   Kang, Hongbo/0000-0001-5771-3886; wang, yong/0000-0002-7847-3807
FU National Natural Science Foundation of China
FX No Statement Available
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   [Anonymous], 1994, Humancomputer interaction
   Ba L.J., 2016, arXiv, DOI DOI 10.48550/ARXIV.1607.06450
   Bao H., 2022, PROC INT C LEARN REP, P1
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Carion N., 2020, EUR C COMP VIS, P213
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Chu X., 2023, PROC 11 INT C LEARN
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Han K, 2021, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.2103.00112
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hassanin M., 2022, arXiv
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hu WB, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P602, DOI 10.1145/3474085.3475219
   Hua GL, 2023, IEEE T MULTIMEDIA, V25, P1832, DOI 10.1109/TMM.2022.3171102
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kingma J. L., 2015, INT C LEARNING REPRE INT C LEARNING REPRE, P1
   Lai G., 2020, ADV NEUR IN, V33, P4271
   Li C, 2019, PROC CVPR IEEE, P9879, DOI 10.1109/CVPR.2019.01012
   Li WH, 2023, IEEE T MULTIMEDIA, V25, P1282, DOI 10.1109/TMM.2022.3141231
   Li WH, 2022, Arxiv, DOI arXiv:2206.06420
   Li WH, 2022, PROC CVPR IEEE, P13137, DOI 10.1109/CVPR52688.2022.01280
   Lin J., 2019, BMVC
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P494, DOI 10.1109/TPAMI.2019.2894422
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Mehta D, 2017, INT CONF 3D VISION, P506, DOI 10.1109/3DV.2017.00064
   Mehta D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073596
   Paszke A, 2019, ADV NEUR IN, V32
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Shan WK, 2022, LECT NOTES COMPUT SC, V13665, P461, DOI 10.1007/978-3-031-20065-6_27
   Shan WK, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P3446, DOI 10.1145/3474085.3475504
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tay Y, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3530811
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Vaswani A, 2017, ADV NEUR IN, V30
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wu L, 2022, IEEE T IMAGE PROCESS, V31, P4803, DOI 10.1109/TIP.2022.3186746
   Wu L, 2021, IEEE T NEUR NET LEAR, V32, P722, DOI 10.1109/TNNLS.2020.2979190
   Wu L, 2019, IEEE T NEUR NET LEAR, V30, P3347, DOI 10.1109/TNNLS.2019.2891244
   Xu JW, 2020, PROC CVPR IEEE, P896, DOI 10.1109/CVPR42600.2020.00098
   Yuan L, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P538, DOI 10.1109/ICCV48922.2021.00060
   Zeng AL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11416, DOI 10.1109/ICCV48922.2021.01124
   Zhang JL, 2022, PROC CVPR IEEE, P13222, DOI 10.1109/CVPR52688.2022.01288
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
   Zheng C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11636, DOI 10.1109/ICCV48922.2021.01145
NR 55
TC 3
Z9 3
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4039
EP 4049
DI 10.1109/TMM.2023.3321438
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JS4G6
UT WOS:001175134300012
DA 2024-08-05
ER

PT J
AU Wu, JY
   Huang, Y
   Gao, M
   Gao, ZP
   Zhao, JQ
   Zhang, HJ
   Zhang, AG
AF Wu, Junyi
   Huang, Yan
   Gao, Min
   Gao, Zhipeng
   Zhao, Jianqiang
   Zhang, Huiji
   Zhang, Anguo
TI A Two-Stream Hybrid Convolution-Transformer Network Architecture for
   Clothing-Change Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clothing-change person re-identification; ID-unique feature; feature
   supplement module; hierarchical supervision
ID RECOGNITION; ATTENTION
AB Long-term (also called Clothing-Change) person re-identification (CC-reID) aims at confirming the identity of pedestrians captured at diverse locations and/or times. Current CC-reID methods heavily rely on ID features learned by the CNN architecture. However, with limited receptive fields, CNN is hard to effectively explore some unique but discriminative ID features (e.g., hair style, tattoo and accessories) from small body regions. Compared with CNN, Transformer has certain merits in exploring more diverse ID-unique features and retaining more details by the multi-head self-attention design and the removal of down-sampling operation. In this paper, a two-stream hybrid Convolution-Transformer Network (CT-Net) is proposed for CC-reID by combining both CNN and Transformer parallelly in an end-to-end learning scheme. Specifically, CT-Net contains a CNN-based stream (C-Stream) and a Transformer-based stream (T-Stream). Compared with using C-Stream only, T-Stream is used to encourage the C-Stream to explore more detailed ID-unique features when the clothing information is no reliable in CC-reID. Specifically, a Feature Supplement Module (FSM) is proposed to transfer features learned by T-Stream to C-Stream from low-level to high-level for mining more ID-unique feature. In order to further enhance the discriminability and complementary of ID features learned by our CT-Net, we also introduce a hierarchical supervision with bilinear pooling (HSBP). Experimental results demonstrate that CT-Net performs favorably against the state-of-the-art methods over three CC-reID benchmarks. Meanwhile, CT-Net also demonstrates good generalization ability by achieving comparable performance on traditional person re-ID datasets such as Market-1501 and DukeMTMC-reID.
C1 [Wu, Junyi; Gao, Zhipeng; Zhao, Jianqiang; Zhang, Huiji] Xiamen Meiya Pico Informat Co Ltd, Xiamen Meiya Informat Secur Res Inst Co Ltd, AI Res Ctr, Xiamen 361008, Peoples R China.
   [Huang, Yan] Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.
   [Gao, Min] Fuzhou Univ, Coll Phys & Informat Engn, Fujian Key Lab Intelligent Proc & Wireless Transmi, Fuzhou 350025, Peoples R China.
   [Zhang, Anguo] Anhui Univ, Sch Artificial Intelligence, Hefei 230039, Peoples R China.
   [Zhang, Anguo] Minist Educ, Res Ctr Autonomous Unmanned Syst Technol, Hefei 230039, Peoples R China.
   [Zhang, Anguo] Anhui Prov Engn Res Ctr Unmanned Syst & Intelligen, Hefei 230039, Peoples R China.
   [Zhang, Anguo] Univ Macau, Inst Microelect, Taipa 999078, Macao, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Fuzhou
   University; Anhui University; University of Macau
RP Huang, Y (corresponding author), Chinese Acad Sci, Inst Automat, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.; Zhang, AG (corresponding author), Anhui Univ, Sch Artificial Intelligence, Hefei 230039, Peoples R China.; Zhang, AG (corresponding author), Minist Educ, Res Ctr Autonomous Unmanned Syst Technol, Hefei 230039, Peoples R China.; Zhang, AG (corresponding author), Anhui Prov Engn Res Ctr Unmanned Syst & Intelligen, Hefei 230039, Peoples R China.; Zhang, AG (corresponding author), Univ Macau, Inst Microelect, Taipa 999078, Macao, Peoples R China.
EM junyi.wu-1@outlook.com; yan.huang@cripac.ia.ac.cn;
   min.gao-1@outlook.com; zhipeng.gao001015@outlook.com; manet@126.com;
   meiya.ai@outlook.com; anguo.zhang@hotmail.com
RI Huang, Yan/N-3447-2018
OI Huang, Yan/0000-0002-1363-5318; Gao, Min/0009-0000-0208-121X; Zhang,
   Anguo/0000-0002-4825-7054; Wu, Junyi/0000-0002-2509-1223
FU National Natural Science Foundation of China
FX No Statement Available
CR Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225
   Chen BH, 2019, IEEE I CONF COMP VIS, P371, DOI 10.1109/ICCV.2019.00046
   Chen D, 2018, LECT NOTES COMPUT SC, V11211, P764, DOI 10.1007/978-3-030-01234-2_45
   d'Ascoli S, 2021, PR MACH LEARN RES, V139, DOI 10.1088/1742-5468/ac9830
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong HS, 2018, NEUROCOMPUTING, V307, P25, DOI 10.1016/j.neucom.2018.04.013
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Gu XQ, 2022, PROC CVPR IEEE, P1050, DOI 10.1109/CVPR52688.2022.00113
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Q., 2019, INPROC INTJOINT C NE, P1
   Huang Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11875, DOI 10.1109/ICCV48922.2021.01168
   Huang Y, 2022, IEEE T MULTIMEDIA, V24, P1570, DOI 10.1109/TMM.2021.3067760
   Huang Y, 2021, INT J COMPUT VISION, V129, P2244, DOI 10.1007/s11263-021-01474-8
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Huang Y, 2020, IEEE T CIRC SYST VID, V30, P3459, DOI 10.1109/TCSVT.2019.2948093
   Huang Y, 2019, IEEE T IMAGE PROCESS, V28, P1391, DOI 10.1109/TIP.2018.2874715
   Jin X, 2022, PROC CVPR IEEE, P14258, DOI 10.1109/CVPR52688.2022.01388
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Köstinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li HJ, 2021, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR46437.2021.00666
   Li KC, 2023, IEEE T PATTERN ANAL, V45, P12581, DOI 10.1109/TPAMI.2023.3282631
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YJ, 2021, IEEE WINT CONF APPL, P2431, DOI 10.1109/WACV48630.2021.00248
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu XH, 2021, Arxiv, DOI arXiv:2104.01745
   Paszke A., 2017, PROC ADVNEURAL INF P
   Peng ZL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P357, DOI 10.1109/ICCV48922.2021.00042
   Qian XL, 2020, IEEE T PATTERN ANAL, V42, P371, DOI 10.1109/TPAMI.2019.2928294
   Qian Xuelin, 2020, PROC ASIAN C COMPUT, P71
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tang X, 2018, LECT NOTES COMPUT SC, V11213, P812, DOI 10.1007/978-3-030-01240-3_49
   Ustinova E., 2017, AVSS, P1
   Vaswani A, 2017, ADV NEUR IN, V30
   Wan FB, 2020, IEEE COMPUT SOC CONF, P3620, DOI 10.1109/CVPRW50498.2020.00423
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang JH, 2023, COMPUT ELECTRON AGR, V206, DOI 10.1016/j.compag.2023.107682
   Wu HP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P22, DOI 10.1109/ICCV48922.2021.00009
   Wu JY, 2023, IEEE T INF FOREN SEC, V18, P5623, DOI 10.1109/TIFS.2023.3311584
   Wu JY, 2022, PATTERN RECOGN, V131, DOI 10.1016/j.patcog.2022.108865
   Wu JY, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3447715
   Xie XH, 2010, PATTERN RECOGN, V43, P4177, DOI 10.1016/j.patcog.2010.06.019
   Xiu Y., 2018, BMVC
   Xue J, 2018, IEEE COMPUT SOC CONF, P2193, DOI 10.1109/CVPRW.2018.00285
   Yang QZ, 2021, IEEE T PATTERN ANAL, V43, P2029, DOI 10.1109/TPAMI.2019.2960509
   Yu Q, 2018, Arxiv, DOI arXiv:1711.08106
   Yu SJ, 2020, PROC CVPR IEEE, P3397, DOI 10.1109/CVPR42600.2020.00346
   Zhang AG, 2021, PROC CVPR IEEE, P598, DOI 10.1109/CVPR46437.2021.00066
   Zhang GW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P516, DOI 10.1145/3474085.3475202
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zhang P, 2021, IEEE T MULTIMEDIA, V23, P3562, DOI 10.1109/TMM.2020.3028461
   Zhang P, 2018, IEEE WINT CONF APPL, P494, DOI 10.1109/WACV.2018.00060
   Zhang YD, 2021, LECT NOTES COMPUT SC, V12901, P14, DOI 10.1007/978-3-030-87193-2_2
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng M, 2019, PROC CVPR IEEE, P5728, DOI 10.1109/CVPR.2019.00588
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
NR 71
TC 0
Z9 0
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5326
EP 5339
DI 10.1109/TMM.2023.3331569
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600030
DA 2024-08-05
ER

PT J
AU Wu, LF
   Tian, M
   Xiang, Y
   Gu, K
   Shi, G
AF Wu, Lifang
   Tian, Meng
   Xiang, Ye
   Gu, Ke
   Shi, Ge
TI Learning Label Semantics for Weakly Supervised Group Activity
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Weakly Supervised Group Activity Recognition; Label Semantics;
   Multi-Label Classification
AB Weakly supervised group activity recognition deals with the dependence on individual-level annotations during understanding scenes involving multiple individuals, which is a challenging task. Existing methods either take the trained detectors to extract individual features or utilize the attention mechanisms for partial context encoding, followed by integration to form the final group-level representations. However, the detectors require individual-level annotations during the training phase and have a mis-detection issue, and the partial contexts extracted immediately from the whole complex scene are too ambiguous without the guidance of concrete semantics. In this article, we investigate the hierarchical structure inherent in group-level labels to extract the fine-grained semantics without using detectors for weakly supervised group activity recognition. A multi-hot encoding strategy combined with a semantic encoder is first adopted to get the label semantics embeddings. The semantic and visual scene information are then fused through a semantic decoder to obtain activity-specific features. Lastly, we employ the multi-label classification and integrate the scores of hierarchical activity labels. Experimental results show that our proposed method achieves the state-of-the-art performance on three benchmarks, and the accuracy on the Volleyball dataset exceeds the second-best method by 2%.
C1 [Wu, Lifang; Tian, Meng; Xiang, Ye; Gu, Ke; Shi, Ge] Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing University of Technology
RP Xiang, Y (corresponding author), Beijing Univ Technol, Fac Informat Technol, Beijing 100124, Peoples R China.
EM lfwu@bjut.edu.cn; tianmeng@emails.bjut.edu.cn; xiangye@bjut.edu.cn;
   guke@bjut.edu.cn; tinkersxy@gmail.com
OI shi, ge/0000-0002-9296-9905; Xiang, Ye/0000-0003-1945-7433
FU National Natural Science Foundation of China
FX No Statement Available
CR Azar SM, 2019, PROC CVPR IEEE, P7884, DOI 10.1109/CVPR.2019.00808
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Carion N., 2020, EUR C COMP VIS, P213
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen ZM, 2021, IEEE T MULTIMEDIA, V23, P1827, DOI 10.1109/TMM.2020.3003779
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Choi W, 2012, LECT NOTES COMPUT SC, V7575, P215, DOI 10.1007/978-3-642-33765-9_16
   Deng X, 2023, IEEE T MULTIMEDIA, V25, P4013, DOI 10.1109/TMM.2022.3171095
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Duan H., 2022, PROC CVPR IEEE, P2969, DOI DOI 10.1109/CVPR52688.2022.00298
   Ehsanpour Mahsa, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P177, DOI 10.1007/978-3-030-58545-7_11
   Gavrilyuk K, 2020, PROC CVPR IEEE, P836, DOI 10.1109/CVPR42600.2020.00092
   Ge ZY, 2018, Arxiv, DOI arXiv:1807.07247
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Han M., 2022, P IEEE C COMPUTER VI, P2990
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hu GY, 2020, PROC CVPR IEEE, P977, DOI 10.1109/CVPR42600.2020.00106
   Ibrahim MS, 2018, LECT NOTES COMPUT SC, V11207, P742, DOI 10.1007/978-3-030-01219-9_44
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756
   Kim D, 2022, PROC CVPR IEEE, P20051, DOI 10.1109/CVPR52688.2022.01945
   Kingma D.P., 2014, Proc. of ICLR
   Kong LT, 2022, IEEE T CIRC SYST VID, V32, P6086, DOI 10.1109/TCSVT.2022.3156634
   Kong LT, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1328, DOI 10.1109/ICASSP.2018.8461770
   Lan T, 2012, IEEE T PATTERN ANAL, V34, P1549, DOI 10.1109/TPAMI.2011.228
   Li Q, 2016, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2016.325
   Li Shuaicheng, 2021, P IEEE CVF INT C COM, P13668
   Li X, 2017, IEEE I CONF COMP VIS, P2895, DOI 10.1109/ICCV.2017.313
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Lu LH, 2020, IEEE T MULTIMEDIA, V22, P524, DOI 10.1109/TMM.2019.2930344
   Pramono Rizard Renanda Adhi, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P71, DOI 10.1007/978-3-030-58452-8_5
   Pramono RRA, 2021, IEEE T IMAGE PROCESS, V30, P8184, DOI 10.1109/TIP.2021.3113570
   Qi MS, 2018, LECT NOTES COMPUT SC, V11214, P104, DOI 10.1007/978-3-030-01249-6_7
   Qu YR, 2016, IEEE DATA MINING, P1149, DOI [10.1109/ICDM.2016.0151, 10.1109/ICDM.2016.57]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rui Yan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P208, DOI 10.1007/978-3-030-58598-3_13
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Shu TM, 2015, PROC CVPR IEEE, P4576, DOI 10.1109/CVPR.2015.7299088
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tang YS, 2020, IEEE T CIRC SYST VID, V30, P2872, DOI 10.1109/TCSVT.2020.2973301
   Tkalcic M, 2013, IEEE T MULTIMEDIA, V15, P391, DOI 10.1109/TMM.2012.2229970
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang MS, 2017, PROC CVPR IEEE, P7408, DOI 10.1109/CVPR.2017.783
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu JC, 2019, PROC CVPR IEEE, P9956, DOI 10.1109/CVPR.2019.01020
   Wu LF, 2023, IEEE T CIRC SYST VID, V33, P2839, DOI 10.1109/TCSVT.2022.3228731
   Xu DZ, 2020, IEEE ACCESS, V8, P65689, DOI 10.1109/ACCESS.2020.2979742
   Yan R, 2023, IEEE T PATTERN ANAL, V45, P6955, DOI 10.1109/TPAMI.2020.3034233
   Yan R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1292, DOI 10.1145/3240508.3240572
   Yuan HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7456, DOI 10.1109/ICCV48922.2021.00738
   Yuan HJ, 2021, AAAI CONF ARTIF INTE, V35, P3261
   Zappardino F, 2021, INT C PATT RECOG, P10412, DOI 10.1109/ICPR48806.2021.9413195
   Zeng R. T. d., 2022, P 11 INT C LEARN REP
   Zhang PZ, 2020, IEEE T IMAGE PROCESS, V29, P29, DOI 10.1109/TIP.2019.2918725
   Zhang S, 2022, PROC CVPR IEEE, P16639, DOI 10.1109/CVPR52688.2022.01616
   Zhang YY, 2021, PROC CVPR IEEE, P14620, DOI [10.1109/CVPR46437.2021.01439, 10.1109/cvpr46437.2021.01439]
   Zhao DW, 2023, IEEE T MULTIMEDIA, V25, P7235, DOI 10.1109/TMM.2022.3219650
   Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823
   Zhou L., 2023, IEEE Trans. Multimedia, V26, P353
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 65
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 6386
EP 6397
DI 10.1109/TMM.2024.3349923
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NK2D0
UT WOS:001200272600006
DA 2024-08-05
ER

PT J
AU Xiao, LH
   Yang, XS
   Peng, F
   Yan, M
   Wang, YW
   Xu, CS
AF Xiao, Linhui
   Yang, Xiaoshan
   Peng, Fang
   Yan, Ming
   Wang, Yaowei
   Xu, Changsheng
TI CLIP-VG: Self-Paced Curriculum Adapting of CLIP for Visual Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Grounding; Reliability; Adaptation models; Task analysis; Visualization;
   Data models; Annotations; Visual grounding; curriculum learning;
   pseudo-language label; and vision-language models
AB Visual Grounding (VG) is a crucial topic in the field of vision and language, which involves locating a specific region described by expressions within an image. To reduce the reliance on manually labeled data, unsupervised methods have been developed to locate regions using pseudo-labels. However, the performance of existing unsupervised methods is highly dependent on the quality of pseudo-labels and these methods always encounter issues with limited diversity. In order to utilize vision and language pre-trained models to address the grounding problem, and reasonably take advantage of pseudo-labels, we propose CLIP-VG, a novel method that can conduct self-paced curriculum adapting of CLIP with pseudo-language labels. We propose a simple yet efficient end-to-end network architecture to realize the transfer of CLIP to the visual grounding. Based on the CLIP-based architecture, we further propose single-source and multi-source curriculum adapting algorithms, which can progressively find more reliable pseudo-labels to learn an optimal model, thereby achieving a balance between reliability and diversity for the pseudo-language labels. Our method outperforms the current state-of-the-art unsupervised method by a significant margin on RefCOCO/+/g datasets in both single-source and multi-source scenarios, with improvements ranging from 6.78% to 10.67% and 11.39% to 14.87%, respectively. Furthermore, our approach even outperforms existing weakly supervised methods.
C1 [Xiao, Linhui; Yang, Xiaoshan; Peng, Fang; Xu, Changsheng] Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Xiao, Linhui; Yang, Xiaoshan; Peng, Fang; Xu, Changsheng] Peng Cheng Lab PCL, Shenzhen 518066, Peoples R China.
   [Xiao, Linhui; Yang, Xiaoshan; Peng, Fang; Xu, Changsheng] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Yan, Ming] DAMO Acad, Alibaba Grp, Hangzhou 311121, Peoples R China.
   [Wang, Yaowei] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Alibaba
   Group; Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci CASIA, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Peng Cheng Lab PCL, Shenzhen 518066, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM xiaolinhui16@mails.ucas.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn;
   pengfang21@mails.ucas.ac; ym119608@alibaba-inc.com; wangyw@pcl.ac.cn;
   csxu@nlpr.ia.ac.cn
RI Xiao, Linhui/HJO-8602-2023
OI Xiao, Linhui/0000-0003-2592-5264; Yang, Xiaoshan/0000-0001-5453-9755;
   xu, chang sheng/0000-0001-8343-9665; Yan, Ming/0000-0003-4959-8878
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Bengio J., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P41
   Carion N., 2020, EUR C COMP VIS, P213
   Cascante-Bonilla P, 2021, AAAI CONF ARTIF INTE, V35, P6912
   Chen K, 2018, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2018.00425
   Chen XP, 2018, Arxiv, DOI arXiv:1812.03426
   Choi J., 2019, PROC BRIT MACH VIS C
   Chou SH, 2022, 2022 19TH CONFERENCE ON ROBOTS AND VISION (CRV 2022), P48, DOI 10.1109/CRV55824.2022.00015
   Cong Y., 2022, PROC IEEE T PATTERN
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Datta S, 2019, IEEE I CONF COMP VIS, P2601, DOI 10.1109/ICCV.2019.00269
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Dosovitskiy A., 2020, ARXIV, DOI 10.48550/arXiv.2010.11929
   Du YH, 2022, IEEE INT CONF MULTI, DOI 10.1109/ICMEW56448.2022.9859481
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gupta T., 2020, EUR C COMP VIS, P752
   Ho Chih-Hui, 2023, Computer Vision - ECCV 2022 Workshops: Proceedings. Lecture Notes in Computer Science (13808), P3, DOI 10.1007/978-3-031-25085-9_1
   Hong RC, 2022, IEEE T PATTERN ANAL, V44, P684, DOI 10.1109/TPAMI.2019.2911066
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Jiang HJ, 2022, PROC CVPR IEEE, P15492, DOI 10.1109/CVPR52688.2022.01507
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Kazemzadeh S., 2014, EMNLP, DOI DOI 10.3115/V1/D14-1086
   Kumar M, 2010, Advances in Neural Information Processing Systems, V23
   Li J., 2022, arXiv
   Li JH, 2021, ADV NEUR IN, V34
   Li MC, 2021, 35 C NEURAL INFORM P, V34
   Liao Yiyi, 2020, CVPR
   Lin JY, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3251, DOI 10.1145/3447548.3467206
   Liu DQ, 2019, IEEE I CONF COMP VIS, P4672, DOI 10.1109/ICCV.2019.00477
   Liu XJ, 2019, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2019.00270
   Liu XJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P539, DOI 10.1145/3343031.3351074
   Liu YF, 2021, PROC CVPR IEEE, P5608, DOI 10.1109/CVPR46437.2021.00556
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Mokady Ron, 2021, arXiv, DOI DOI 10.48550/ARXIV.2111.09734
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   Radford A, 2021, PR MACH LEARN RES, V139
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Sadhu A, 2019, IEEE I CONF COMP VIS, P4693, DOI 10.1109/ICCV.2019.00479
   Shi HC, 2023, NEUROCOMPUTING, V518, P39, DOI 10.1016/j.neucom.2022.10.079
   Shu Y, 2019, AAAI CONF ARTIF INTE, P4951
   Soviany P, 2022, INT J COMPUT VISION, V130, P1526, DOI 10.1007/s11263-022-01611-x
   Subramanian S, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P5198
   Sun MJ, 2023, IEEE T MULTIMEDIA, V25, P1611, DOI 10.1109/TMM.2021.3139467
   Sun MJ, 2021, IEEE T PATTERN ANAL, V43, P4189, DOI 10.1109/TPAMI.2021.3058684
   Tay Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P4922
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang J, 2019, IEEE I CONF COMP VIS, P4662, DOI 10.1109/ICCV.2019.00476
   Wang LW, 2021, PROC CVPR IEEE, P14085, DOI 10.1109/CVPR46437.2021.01387
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang P, 2019, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2019.00206
   Wang W., 2022, PROC IEEECVF C COMPU, P19175
   Wang X, 2022, IEEE T PATTERN ANAL, V44, P4555, DOI 10.1109/TPAMI.2021.3069908
   Wang YC, 2021, IEEE T MULTIMEDIA, V24, P3276, DOI 10.1109/TMM.2021.3096087
   Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558
   Yao Y, 2022, Arxiv, DOI arXiv:2109.11797
   Ye JB, 2022, PROC CVPR IEEE, P15481, DOI 10.1109/CVPR52688.2022.01506
   Yeh RA, 2018, PROC CVPR IEEE, P6125, DOI 10.1109/CVPR.2018.00641
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang LC, 2021, IEEE-ACM T AUDIO SPE, V29, P3307, DOI 10.1109/TASLP.2021.3121986
   Zhao SY, 2022, LECT NOTES COMPUT SC, V13669, P159, DOI 10.1007/978-3-031-20077-9_10
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
   Zhu HD, 2021, IEEE WINT CONF APPL, P2209, DOI 10.1109/WACV48630.2021.00226
NR 67
TC 0
Z9 0
U1 11
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 4334
EP 4347
DI 10.1109/TMM.2023.3321501
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KQ7M2
UT WOS:001181498100046
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Yang, ZP
   Yang, ZG
   Li, XP
   Yu, Y
   Li, Q
   Liu, WY
AF Yang, Zhuopan
   Yang, Zhenguo
   Li, Xiaoping
   Yu, Yi
   Li, Qing
   Liu, Wenyin
TI A Progressive Placeholder Learning Network for Multimodal Zero-Shot
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Zero-shot learning; Mixers; Prototypes;
   Manifolds; Vectors; Multimodal zero-shot learning; mixup hallucination;
   alternating mixer
AB It is challenging to eliminate the domain shift between seen and unseen classes in multimodal zero-shot learning tasks due to the underlying disparity between the data distributions in the seen and unseen domains. In this paper, we propose a progressive placeholder learning network with mixup hallucination and an alternating mixer, denoted as MHAM, to maintain embedding spaces for unseen classes. Utilizing mixup hallucination (MH) on the visual and textual features obtained by BERT and a vision transformer, MHAM generates visual and textual hallucinated representations with pseudo class embeddings as placeholders for the unseen classes. Furthermore, a number of alternating mixer (AM) blocks are stacked to obtain modality-shared representations for the seen classes and hallucinated representations of progressive placeholders for the unseen classes. In particular, modality-shared representations are obtained by a mixer in an AM block by reversing the dimensionality of the modality-specific and raw representations to model intermodal interactions. MHAM exploits a freezing strategy by fixing the weights over the unseen classes in the last fully connected layer; this step acts as a projection from the raw and modality-shared representations to the embedding space of the seen and unseen classes. Experiments conducted on zero-shot datasets and news event datasets demonstrate the superior performance of the proposed MHAM method.
C1 [Yang, Zhuopan; Yang, Zhenguo; Li, Xiaoping] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510000, Peoples R China.
   [Yu, Yi] Natl Inst Informat, Digital Content & Media Sci Res Div, Tokyo 1018430, Japan.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Liu, Wenyin] Zhongguancun Lab, Beijing 100190, Peoples R China.
C3 Guangdong University of Technology; Research Organization of Information
   & Systems (ROIS); National Institute of Informatics (NII) - Japan; Hong
   Kong Polytechnic University; Zhongguancun Laboratory
RP Yang, ZG (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510000, Peoples R China.
EM zhuopanyang@gmail.com; yzg@gdut.edu.cn; xpli@gdut.edu.cn;
   yiyu@nii.ac.jp; csqli@comp.polyu.edu.hk; liuwy@gdut.edu.cn
RI Li, Qing/JMH-1365-2023
OI Li, Qing/0000-0003-3370-471X; Liu, Wenyin/0000-0002-6237-6607
FU National Key Research and Development Program of China
FX No Statement Available
CR Alamri F., 2021, PROC IMVIP, P1
   Bendre N, 2021, IEEE IMAGE PROC, P1284, DOI 10.1109/ICIP42928.2021.9506108
   Bousmalis K, 2016, ADV NEUR IN, V29
   Chen SM, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P122, DOI 10.1109/ICCV48922.2021.00019
   Chen SZ, 2021, ADV NEUR IN, V34
   Chen XY, 2022, IEEE T MULTIMEDIA, V24, P177, DOI 10.1109/TMM.2020.3047546
   Chen Z, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P844, DOI 10.1145/3474085.3475258
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A., 2021, PROC ICLR
   Feng M, 2022, INT CONF ACOUST SPEE, P481, DOI 10.1109/ICASSP43922.2022.9747472
   Ge JN, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P3244, DOI 10.1145/3503161.3547889
   Guo JC, 2021, IEEE T MULTIMEDIA, V23, P524, DOI 10.1109/TMM.2020.2984091
   Han ZY, 2021, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR46437.2021.00240
   Li ZM, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P1698, DOI 10.1145/3503161.3548128
   Liu M, 2023, PROC CVPR IEEE, P15337, DOI 10.1109/CVPR52729.2023.01472
   Liu SC, 2018, ADV NEUR IN, V31
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu Y, 2019, IEEE I CONF COMP VIS, P6697, DOI 10.1109/ICCV.2019.00680
   Ma PR, 2020, AAAI CONF ARTIF INTE, V34, P11733
   Mall U, 2022, IEEE COMPUT SOC CONF, P3930, DOI 10.1109/CVPRW56347.2022.00438
   Min SB, 2021, IEEE T MULTIMEDIA, V23, P3919, DOI 10.1109/TMM.2020.3033124
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Palatucci M., 2009, Advances in neural information processing systems, V22, P1410
   Radford A, 2021, PR MACH LEARN RES, V139
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Verma V, 2019, PR MACH LEARN RES, V97
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang Q, 2020, IEEE T IMAGE PROCESS, V29, P7549, DOI 10.1109/TIP.2020.3004249
   Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xu BR, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3142181
   Xu W., 2020, Adv. Neural Inf. Process. Syst, P21969
   Xu X, 2017, PROC CVPR IEEE, P2007, DOI 10.1109/CVPR.2017.217
   Yang YH, 2023, IEEE T MULTIMEDIA, V25, P280, DOI 10.1109/TMM.2021.3125134
   Yang Z., 2022, PROC INT JOINT C ART, P1559
   Yang ZG, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2020.102315
   Ye YL, 2022, IEEE T MULTIMEDIA, V24, P1325, DOI 10.1109/TMM.2021.3063616
   Ye ZH, 2022, IEEE T MULTIMEDIA, V24, P2828, DOI 10.1109/TMM.2021.3089017
   Zhang H., 2018, INT C LEARNING REPRE
   Zhang J, 2023, IEEE I CONF COMP VIS, P11507, DOI 10.1109/ICCV51070.2023.01060
   Zhang J, 2023, IEEE T IMAGE PROCESS, V32, P6115, DOI 10.1109/TIP.2023.3328478
   Zhang J, 2022, IEEE T CIRC SYST VID, V32, P5916, DOI 10.1109/TCSVT.2022.3164190
   Zhou KY, 2022, PROC CVPR IEEE, P16795, DOI 10.1109/CVPR52688.2022.01631
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu YZ, 2019, IEEE I CONF COMP VIS, P9843, DOI 10.1109/ICCV.2019.00994
NR 47
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 7933
EP 7945
DI 10.1109/TMM.2024.3373248
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU5P8
UT WOS:001209811000041
DA 2024-08-05
ER

PT J
AU Yu, ZL
   Yang, YY
   Zhu, YB
   Guo, BX
   Li, C
AF Yu, Zilong
   Yang, Yunyun
   Zhu, Yongbin
   Guo, Bixue
   Li, Chun
TI CS-IntroVAE: Cauchy-Schwarz Divergence-Based Introspective Variational
   Autoencoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image reconstruction; image synthesis; hybrid-fusion network; divergence
   learning
ID GENERATIVE ADVERSARIAL NETWORK; GAN
AB Although generative models are still being developed, image reconstruction and generation tasks have evolved dramatically. Since the most popular generative models still have some limitations, it is still challenging. For example, while generative adversarial network (GAN) produces clear images, it is hard to train. The hybrid VAE-GAN incorporates the benefits of both, although it is computationally intensive and prone to drawbacks such as overfitting and gradient disappearance. A novel generative model called the Cauchy-Schwarz Divergence-based Introspective Variational Autoencoder (CS-IntroVAE) is based for this challenge. Extensive experiments show that our model has good performance on both tasks by employing mixed Gaussian distributions as prior distributions and Cauchy-Schwarz divergence as a measure of the distance between prior and posterior distributions.
C1 [Yu, Zilong; Yang, Yunyun; Zhu, Yongbin; Guo, Bixue] Harbin Inst Technol, Sch Sci, Shenzhen 518055, Peoples R China.
   [Li, Chun] Shenzhen MSU BIT Univ, Joint Res Ctr Computat Math & Control, Shenzhen 518172, Peoples R China.
C3 Harbin Institute of Technology; Shenzhen MSU-BIT University
RP Yang, YY (corresponding author), Harbin Inst Technol, Sch Sci, Shenzhen 518055, Peoples R China.; Li, C (corresponding author), Shenzhen MSU BIT Univ, Joint Res Ctr Computat Math & Control, Shenzhen 518172, Peoples R China.
EM 18570419241@163.com; yangyunyun@hit.edu.cn; hit_zhuyb@163.com;
   15565263547@163.com; lichun2020@smbu.edu.cn
OI Li, Chun/0000-0002-2021-751X
FU Shenzhen Higher Education Institutions Stable Support Plan
FX No Statement Available
CR Abrol V, 2021, IEEE T MULTIMEDIA, V23, P2153, DOI 10.1109/TMM.2020.3008053
   Akbari M, 2021, IEEE T MULTIMEDIA, V23, P3013, DOI 10.1109/TMM.2021.3068523
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Azzam M, 2021, IEEE T MULTIMEDIA, V23, P3318, DOI 10.1109/TMM.2020.3023792
   BHATIA R, 1995, LINEAR ALGEBRA APPL, V224, P119
   Bie XY, 2022, Arxiv, DOI arXiv:2204.01565
   Binkowski M., 2018, ARXIV180101401
   Bond-Taylor S, 2022, IEEE T PATTERN ANAL, V44, P7327, DOI 10.1109/TPAMI.2021.3116668
   Boukerche A, 2020, ACM COMPUT SURV, V53, DOI [10.1145/3421763, 10.1145/3381028]
   Choi J, 2022, PROC CVPR IEEE, P11462, DOI 10.1109/CVPR52688.2022.01118
   Dai QQ, 2020, IEEE T MULTIMEDIA, V22, P2564, DOI 10.1109/TMM.2019.2958760
   Daniel T, 2021, PROC CVPR IEEE, P4389, DOI 10.1109/CVPR46437.2021.00437
   DeGroot M.H., 2012, Probability and statistics, V4th ed.
   Demir U, 2022, LECT NOTES COMPUT SC, V13374, P340, DOI 10.1007/978-3-031-13324-4_29
   Deng WX, 2022, IEEE T MULTIMEDIA, V24, P2407, DOI 10.1109/TMM.2021.3080516
   Donahue Jeff, 2017, Adversarial Feature Learning
   Dumoulin V., 2017, INT C LEARN REPR, P1
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Goldberger J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P487
   Goodfellow I. J., 2014, ADV NEURAL INFORM PR
   He KM, 2022, PROC CVPR IEEE, P15979, DOI 10.1109/CVPR52688.2022.01553
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho J., 2020, Adv. Neural. Inf. Process. Syst, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hoffman M.D., 2016, WORKSHOP ADV APPROXI, V1
   Huang HB, 2018, ADV NEUR IN, V31
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jiang K, 2020, IEEE T MULTIMEDIA, V22, P2734, DOI 10.1109/TMM.2019.2960586
   Johnson MJ, 2016, ADV NEUR IN, V29
   Karras T., 2020, Advances in neural information processing systems, V33, P12104, DOI DOI 10.48550/ARXIV.2006.06676
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Khan S, 2022, ACM COMPUT SURV, V54, DOI 10.1145/3505244
   Kingma D. P., 2014, arXiv
   Kingma DP, 2015, ADV NEUR IN, V28
   Krizhevsky A., 2012, Learning multiple layers of features from tiny images, DOI DOI 10.1145/3079856.3080246
   Larsen ABL, 2016, PR MACH LEARN RES, V48
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu CX, 2021, IEEE T MULTIMEDIA, V23, P2843, DOI 10.1109/TMM.2020.3017924
   Liu D, 2022, LECT NOTES COMPUT SC, V13435, P485, DOI 10.1007/978-3-031-16443-9_47
   Liu ZQ, 2022, IEEE T PATTERN ANAL, V44, P3197, DOI 10.1109/TPAMI.2020.3048727
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lu YF, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P469, DOI 10.1145/3357384.3357943
   Ma TF, 2018, ADV NEUR IN, V31
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694
   Miyato T., 2018, INT C LEARN REPR ICL, P1
   Mou C, 2022, IEEE T MULTIMEDIA, V24, P1366, DOI 10.1109/TMM.2021.3063916
   Obukhov Artem, 2020, Software Engineering Perspectives in Intelligent Systems. Proceedings of 4th Computational Methods in Systems and Software 2020. Advances in Intelligent Systems and Computing (AISC 1294), P102, DOI 10.1007/978-3-030-63322-6_8
   Paszke A, 2019, ADV NEUR IN, V32
   Pidhorskyi S., 2020, P IEEE CVF C COMP VI, P14092
   Razavi A, 2019, ADV NEUR IN, V32
   Rybkin O., 2021, PMLR, V139, P9179
   Sara U., 2019, J. Comput. Commun, V7, P8, DOI [DOI 10.4236/JCC.2019.73002, 10.4236/jcc.2019.73002]
   Song J., 2021, P 9 INT C LEARN REPR, P1
   Souibgui MA, 2022, IEEE T PATTERN ANAL, V44, P1180, DOI 10.1109/TPAMI.2020.3022406
   Sun K, 2020, IEEE T MULTIMEDIA, V22, P2246, DOI 10.1109/TMM.2019.2957984
   Takida Y, 2022, PR MACH LEARN RES
   Tang H., 2011, IEEE Comput. Intell. Mag., V6, P60
   Tomczak J., 2018, INT C ARTIFICIAL INT, P1214
   Vahdat A., 2020, Proceedings of the 34th International Conference on Neural Information Processing Systems. NIPS'20, Vvol 33, P19667, DOI [DOI 10.5555/3495724.3497374, 10.48550/arXiv.2007.03898]
   Wang XY, 2022, IEEE T MULTIMEDIA, V24, P4028, DOI 10.1109/TMM.2021.3111485
   Xian YQ, 2019, PROC CVPR IEEE, P10267, DOI 10.1109/CVPR.2019.01052
   Zhang BW, 2022, PROC CVPR IEEE, P11294, DOI 10.1109/CVPR52688.2022.01102
   Zhang C, 2019, IEEE T PATTERN ANAL, V41, P2008, DOI 10.1109/TPAMI.2018.2889774
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang ZY, 2022, IEEE T MULTIMEDIA, V24, P677, DOI 10.1109/TMM.2021.3057989
   Zhou DW, 2021, IEEE T MULTIMEDIA, V24, P3469, DOI 10.1109/TMM.2021.3099297
NR 67
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 663
EP 672
DI 10.1109/TMM.2023.3268870
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000009
DA 2024-08-05
ER

PT J
AU Zhang, XX
   Gui, SP
   Jin, J
   Zhu, ZF
   Zhao, Y
AF Zhang, Xingxing
   Gui, Shupeng
   Jin, Jian
   Zhu, Zhenfeng
   Zhao, Yao
TI ATZSL: Defensive Zero-Shot Recognition in the Presence of Adversaries
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Defensive zero-shot learning; adversarial attacks; min-max optimization;
   relation prediction
AB Zero-shot learning (ZSL) has received extensive attention recently especially in areas of fine-grained object recognition, retrieval, and image captioning. Due to the complete lack of training samples and high requirement of defense transferability, the ZSL model learned is particularly vulnerable against adversarial attacks. Recent work also showed adversarially robust generalization requires more data. This may significantly affect the robustness of ZSL. However, very few efforts have been devoted towards this direction. In this paper, we take an initial attempt, and propose a generic formulation to provide a systematical solution (named <bold>ATZSL</bold>) for learning a defensive ZSL model. It is capable of achieving better generalization on various adversarial objects recognition while only losing a negligible performance on clean images for unseen classes, by casting ZSL into a min-max optimization problem. To address it, we design a defensive relation prediction network, which can bridge the seen and unseen class domains via attributes to generalize prediction and defense strategy. Additionally, our framework can be extended to deal with the poisoned scenario of unseen class attributes. An extensive group of experiments are then presented, demonstrating that ATZSL obtains remarkably more favorable trade-off between model transferability and robustness, over currently available alternatives under various settings.
C1 [Zhang, Xingxing] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Gui, Shupeng] Meta, Menlo Pk, CA 94025 USA.
   [Jin, Jian] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Zhu, Zhenfeng; Zhao, Yao] Beijing Jiaotong Univ, Beijing 100044, Peoples R China.
   [Zhu, Zhenfeng; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Tsinghua University; Nanyang Technological University; Beijing Jiaotong
   University; Beijing Jiaotong University
RP Zhang, XX (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM xxzhang1993@gmail.com; shupenggui@gmail.com; jian.jin@ntu.edu.sg;
   zhfzhu@bjtu.edu.cn; yzhao@bjtu.edu.cn
RI Zhang, Xingxing/HGE-4445-2022
OI Zhang, Xingxing/0000-0002-9838-6962; Zhao, Yao/0000-0002-8581-9554; Jin,
   Jian/0000-0003-4250-1519
FU National Key Ramp;D Program of China
FX No Statement Available
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Akcay S, 2019, LECT NOTES COMPUT SC, V11363, P622, DOI 10.1007/978-3-030-20893-6_39
   [Anonymous], 2018, PROC ANN C NEURAL IN
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bansal A, 2018, LECT NOTES COMPUT SC, V11205, P397, DOI 10.1007/978-3-030-01246-5_24
   Bernstein J, 2018, PR MACH LEARN RES, V80
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Changpinyo S, 2020, INT J COMPUT VISION, V128, P166, DOI 10.1007/s11263-019-01193-1
   Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376
   Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575
   Chen L, 2018, PROC CVPR IEEE, P1043, DOI 10.1109/CVPR.2018.00115
   Chen Pin-Yu, 2017, P 10 ACM WORKSH ART, P15, DOI [DOI 10.1145/3128572.3140448, 10.1145/3128572.3140448]
   Chen Z., 2021, ICCV, P8712
   Cui JQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15701, DOI 10.1109/ICCV48922.2021.01543
   Danskin JM, 1967, The theory of max-min and its application to weapons allocation problems
   Fawzi A., 2018, ARXIV180208686, DOI DOI 10.48550/ARXIV.1802.08686
   Felix R, 2018, LECT NOTES COMPUT SC, V11210, P21, DOI 10.1007/978-3-030-01231-1_2
   Fu ZY, 2018, IEEE T PATTERN ANAL, V40, P2009, DOI 10.1109/TPAMI.2017.2737007
   Gan Z., 2020, Adv. Neural Inf .Process. Syst, V33, P6616
   Gao R, 2023, IEEE T MULTIMEDIA, V25, P1649, DOI 10.1109/TMM.2022.3145666
   Goodfellow IJ, 2015, P 3 INT C LEARNING R
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Ilyas Andrew, 2018, P MACHINE LEARNING R, V80
   Ji Z, 2022, IEEE T CYBERNETICS, V52, P6543, DOI 10.1109/TCYB.2020.3004641
   Kingma D.P., 2014, Proc. of ICLR
   Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473
   Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282
   Kurakin A., 2017, P INT C LEARN REPR
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1966
   Li Y, 2018, PROC CVPR IEEE, P7463, DOI 10.1109/CVPR.2018.00779
   Liu QD, 2022, IEEE T CYBERNETICS, V52, P9634, DOI 10.1109/TCYB.2021.3061771
   Liu SC, 2018, ADV NEUR IN, V31
   Liu ZZ, 2020, IMAGE VISION COMPUT, V98, DOI 10.1016/j.imavis.2020.103924
   Long Y, 2017, PROC CVPR IEEE, P6165, DOI 10.1109/CVPR.2017.653
   Mancini M, 2024, IEEE T PATTERN ANAL, V46, P1545, DOI 10.1109/TPAMI.2022.3163667
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Morgado P, 2017, PROC CVPR IEEE, P2037, DOI 10.1109/CVPR.2017.220
   Radovanovic M, 2010, J MACH LEARN RES, V11, P2487
   Romera-Paredes B., 2015, ICML
   Schmidt L, 2018, ADV NEUR IN, V31
   Sinha Aman, 2018, ICLR
   Socher R, 2013, P 2013 C EMP METH NA, P935
   Song WL, 2023, IEEE T MULTIMEDIA, V25, P2127, DOI 10.1109/TMM.2022.3142958
   Sun B, 2022, PATTERN RECOGN, V126, DOI 10.1016/j.patcog.2022.108563
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tramer Florian, 2019, Advances in Neural Information Processing Systems
   Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386
   Venugopalan S, 2017, PROC CVPR IEEE, P1170, DOI 10.1109/CVPR.2017.130
   Wah C., 2011, Tech. Rep. CNS-TR-2011-001
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang XL, 2018, PROC CVPR IEEE, P6857, DOI 10.1109/CVPR.2018.00717
   Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581
   Xian YQ, 2019, IEEE T PATTERN ANAL, V41, P2251, DOI 10.1109/TPAMI.2018.2857768
   Xu W., 2020, Adv. Neural Inf. Process. Syst, P21969
   Yu YL, 2018, IEEE T CYBERNETICS, V48, P2908, DOI 10.1109/TCYB.2017.2751741
   Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
NR 61
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 15
EP 27
DI 10.1109/TMM.2023.3258624
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ES3V6
UT WOS:001140881500019
OA Green Submitted
DA 2024-08-05
ER

PT J
AU Zhao, WD
   Hu, G
   Wei, F
   Wang, HP
   He, Y
   Lu, HC
AF Zhao, Wenda
   Hu, Guang
   Wei, Fei
   Wang, Haipeng
   He, You
   Lu, Huchuan
TI Attacking Defocus Detection With Blur-Aware Transformation for Defocus
   Deblurring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generative adversarial networks; Task analysis; Feature extraction;
   Convolution; Training; Robustness; Bridges; Blur-aware transfer; defocus
   detection attack; weakly-supervised defocus deblurring
AB Previous fully-supervised defocus deblurring has made significant progress. However, training such deep models requires abundant paired ground truth, which is expensive and error-prone. This paper makes an attempt to train a defocus deblurring model without using paired ground truth and any other unpaired data. Related reblur-to-deblur schemes generally use physics-based reblur or GAN-based reblur, suffering from the robustness of blur kernel and hallucination generated by GAN. Besides, the domain gap between the realistic blurred image and reblurred image hinders deblurring performance. Addressing these challenges, we propose a weakly-supervised defocus deblurring framework via defocus detection attack. On one hand, we build a focused area detection attack (FADA) to enforce the focused area to reblur, thereby reversing its detection result by a pretrained defocus blur detection network. Moreover, we introduce a blur-aware transfer modulated from the defocused region to help FADA render a robust reblurred region. On the other hand, we implement a defocused region detection attack to guide the realistic blurred region to deblur in the process of training deblurring network with simulated-paired areas. Extensive experiments on three widely-used datasets verify the effectiveness of our framework.
C1 [Zhao, Wenda; Hu, Guang; Wei, Fei; Lu, Huchuan] Dalian Univ Technol, Key Lab Intelligent Control & Optimizat Ind Equipm, Minist Educ, Dalian 116024, Peoples R China.
   [Zhao, Wenda; Hu, Guang; Wei, Fei; Lu, Huchuan] Dalian Univ Technol, Sch Informat & Commun Engn, Dalian 116024, Peoples R China.
   [Wang, Haipeng; He, You] Naval Aviat Univ, Res Inst Informat Fus, Yantai 264001, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Wang, HP (corresponding author), Naval Aviat Univ, Res Inst Informat Fus, Yantai 264001, Peoples R China.
EM zhaowenda@dlut.edu.cn; hugester@mail.dlut.edu.cn; fwei@mail.dlut.edu.cn;
   whp5691@163.com; heyou_f@126.com; lhchuan@dlut.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Abuolaim Abdullah, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P111, DOI 10.1007/978-3-030-58607-2_7
   Abuolaim A, 2022, IEEE WINT CONF APPL, P82, DOI 10.1109/WACV51458.2022.00016
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen H, 2018, PROC IEEE INT C COMP, P1
   Diao YF, 2021, PROC CVPR IEEE, P7593, DOI 10.1109/CVPR46437.2021.00751
   Ding JW, 2016, IEEE T CIRC SYST VID, V26, P319, DOI 10.1109/TCSVT.2015.2406231
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672, DOI 10.1145/3422622
   Guo Q, 2021, IEEE T IMAGE PROCESS, V30, P1812, DOI 10.1109/TIP.2020.3045630
   Han JF, 2019, IEEE I CONF COMP VIS, P5157, DOI 10.1109/ICCV.2019.00526
   Hieu Le, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P264, DOI 10.1007/978-3-030-58621-8_16
   Huang R, 2018, NEUROCOMPUTING, V285, P154, DOI 10.1016/j.neucom.2018.01.041
   Karaali A, 2018, IEEE T IMAGE PROCESS, V27, P1126, DOI 10.1109/TIP.2017.2771563
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kim B, 2018, COMPUT GRAPH FORUM, V37, P277, DOI 10.1111/cgf.13567
   Lee J, 2021, PROC CVPR IEEE, P2034, DOI 10.1109/CVPR46437.2021.00207
   Lee J, 2019, PROC CVPR IEEE, P12214, DOI 10.1109/CVPR.2019.01250
   Liang CH, 2022, IEEE T MULTIMEDIA, V24, P61, DOI 10.1109/TMM.2020.3045303
   Liang J., IEEE
   Liu ZH, 2021, PROC CVPR IEEE, P4925, DOI 10.1109/CVPR46437.2021.00489
   Lu YY, 2018, LECT NOTES COMPUT SC, V11216, P293, DOI 10.1007/978-3-030-01258-8_18
   Luo B, 2022, IEEE T CIRC SYST VID, V32, P1467, DOI 10.1109/TCSVT.2021.3074799
   Ma HY, 2022, IEEE T IMAGE PROCESS, V31, P216, DOI 10.1109/TIP.2021.3127850
   Madry A, 2019, Arxiv, DOI arXiv:1706.06083
   Maeda S, 2020, PROC CVPR IEEE, P288, DOI 10.1109/CVPR42600.2020.00037
   Maho T, 2021, PROC CVPR IEEE, P10425, DOI 10.1109/CVPR46437.2021.01029
   Mao XT, 2023, AAAI CONF ARTIF INTE, P1905
   Mopuri KR, 2018, PROC CVPR IEEE, P742, DOI 10.1109/CVPR.2018.00084
   Ning Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P617, DOI 10.1007/978-3-030-58607-2_36
   Pan JS, 2018, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2018.00324
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Ruan LY, 2022, PROC CVPR IEEE, P16283, DOI 10.1109/CVPR52688.2022.01582
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Son H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2622, DOI 10.1109/ICCV48922.2021.00264
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Tang C, 2020, AAAI CONF ARTIF INTE, V34, P12063
   Tang C, 2021, IEEE T MULTIMEDIA, V23, P624, DOI 10.1109/TMM.2020.2985541
   Tang C, 2022, IEEE T PATTERN ANAL, V44, P955, DOI 10.1109/TPAMI.2020.3014629
   Tang C, 2019, PROC CVPR IEEE, P2695, DOI 10.1109/CVPR.2019.00281
   Wang L, 2020, PROC CVPR IEEE, P3773, DOI 10.1109/CVPR42600.2020.00383
   Wang XG, 2021, PROC CVPR IEEE, P16352, DOI 10.1109/CVPR46437.2021.01609
   Wiyatno RR, 2019, IEEE I CONF COMP VIS, P4821, DOI 10.1109/ICCV.2019.00492
   Xiaodong Cun, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P747, DOI 10.1007/978-3-030-58601-0_44
   Xu GD, 2017, IEEE I CONF COMP VIS, P5381, DOI 10.1109/ICCV.2017.574
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhang D., 2022, PROC IEEE INT CONFMU, P1
   Zhang J, 2023, IEEE T NEUR NET LEAR, V34, P8404, DOI 10.1109/TNNLS.2022.3151099
   Zhang XX, 2016, J VIS COMMUN IMAGE R, V35, P257, DOI 10.1016/j.jvcir.2016.01.002
   Zhao F, 2022, IEEE T CIRC SYST VID, V32, P2719, DOI 10.1109/TCSVT.2021.3095347
   Zhao WD, 2023, IEEE T MULTIMEDIA, V25, P9228, DOI 10.1109/TMM.2023.3248162
   Zhao WD, 2022, LECT NOTES COMPUT SC, V13690, P569, DOI 10.1007/978-3-031-20056-4_33
   Zhao WD, 2024, IEEE T NEUR NET LEAR, V35, P9162, DOI 10.1109/TNNLS.2022.3219059
   Zhao WD, 2021, PROC CVPR IEEE, P6929, DOI 10.1109/CVPR46437.2021.00686
   Zhao WD, 2021, IEEE T IMAGE PROCESS, V30, P5426, DOI 10.1109/TIP.2021.3084101
   Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911
   Zhao WD, 2020, IEEE T IMAGE PROCESS, V29, P1356, DOI 10.1109/TIP.2019.2942505
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
NR 59
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 5450
EP 5460
DI 10.1109/TMM.2023.3334023
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU9Q0
UT WOS:001189435600019
DA 2024-08-05
ER

PT J
AU Zhu, J
   Wang, HL
   He, B
AF Zhu, Jian
   Wang, Hanli
   He, Bin
TI Multi-Modal Structure-Embedding Graph Transformer for Visual Commonsense
   Reasoning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Video recording; Transformers; Task analysis;
   Correlation; Linguistics; Commonsense reasoning; Visual commonsense
   reasoning; multi-modal structure embedding; graph transformer; scored
   pooling
AB Visual commonsense reasoning (VCR) is a challenging reasoning task that aims to not only answer the question based on a given image but also provide a rationale justifying for the choice. Graph-based networks are appropriate to represent and extract the correlation between image and language for reasoning, where how to construct and learn graphs based on such multi-modal Euclidean data is a fundamental problem. Most existing graph-based methods view visual regions and linguistic words as identical graph nodes, ignoring inherent characteristics of multi-modal data. In addition, these approaches typically only have one graph-learning layer, and the performance declines as the model goes deeper. To address these issues, a novel method named Multi-modal Structure-embedding Graph Transformer (MSGT) is proposed. Specifically, an answer-vision graph and an answer-question graph are constructed to represent and model intra-modal and inter-modal correlations in VCR simultaneously, where additional multi-modal structure representations are initialized and embedded according to visual region distances and linguistic word orders for more reasonable graph representation. Then, a structure-injecting graph transformer is designed to inject embedded structure priors into the semantic correlation matrix for the evolution of node features and structure representations, which can stack more layers to make model deeper and extract more powerful features with instructive priors. To adaptively fuse graph features, a scored pooling mechanism is further developed to select valuable clues for reasoning from learnt node features. Experiments demonstrate the superiority of the proposed MSGT framework compared with state-of-the-art methods on the VCR benchmark dataset.
C1 [Zhu, Jian; Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
   [He, Bin] Frontiers Sci Ctr Intelligent Autonomous Syst, Shanghai 201210, Peoples R China.
C3 Tongji University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
EM jianzhu@tongji.edu.cn; hanliwang@tongji.edu.cn; hebin@tongji.edu.cn
RI Wang, Hanli/G-5111-2014
OI Wang, Hanli/0000-0002-9999-4871; Zhu, Jian/0000-0002-3835-4627
FU National Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen M, 2020, PR MACH LEARN RES, V119
   Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152
   Cho Jaemin, 2021, INT C MACH LEARN, P1931
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Guan Weili, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P268, DOI 10.1145/3503161.3548020
   Guan WL, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P482, DOI 10.1145/3477495.3532038
   Guo WY, 2021, IEEE T IMAGE PROCESS, V30, P6730, DOI 10.1109/TIP.2021.3097180
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu W., 2021, P NEURIPS D B OCT
   Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44
   Kim J.-H., 2017, P INT C LEARN REPR, P1
   Kingma D.P., 2014, Proc. of ICLR
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li W, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2592
   Li Xiaonan, 2020, ARXIV200411795, DOI DOI 10.18653/V1/2020.ACL-MAIN.611
   Li Z, 2022, Joint answering and explanation for visual commonsense reasoning
   Lin Jingxiang., 2019, Advances in Neural Information Processing Systems, V32, P15615
   Liu JY, 2020, IEEE T IMAGE PROCESS, V29, P5244, DOI 10.1109/TIP.2020.2979010
   Shi HC, 2021, IEEE T MULTIMEDIA, V23, P995, DOI 10.1109/TMM.2020.2991504
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Song DD, 2021, KNOWL-BASED SYST, V230, DOI 10.1016/j.knosys.2021.107408
   Spinelli I, 2021, IEEE T NEUR NET LEAR, V32, P4755, DOI 10.1109/TNNLS.2020.3025110
   Su W., 2020, P INT C LEARN REPR
   Tan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10757, DOI 10.1109/CVPR42600.2020.01077
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HL, 2022, IEEE T MULTIMEDIA, V24, P715, DOI 10.1109/TMM.2021.3058555
   Wen Z, 2021, IEEE T CIRC SYST VID, V31, P1042, DOI 10.1109/TCSVT.2020.2991866
   Wu Aming, 2019, ADV NEUR IN, P5670, DOI DOI 10.5555/3454287.3454796
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Ye KR, 2021, AAAI CONF ARTIF INTE, V35, P3181
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Ying C., 2021, Advances in Neural Information Processing Systems, P28877
   Yu WJ, 2019, ADV NEUR IN, V32
   Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688
   Zhang X, 2022, IEEE T MULTIMEDIA, V24, P2986, DOI 10.1109/TMM.2021.3091882
   Zhao Jianan, 2021, Gophormer: Ego-graph transformer for node classification
   Zhong HS, 2021, IEEE T MULTIMEDIA, V23, P1264, DOI 10.1109/TMM.2020.2995278
   Zhu J, 2022, IEEE T COGN DEV SYST, V14, P752, DOI 10.1109/TCDS.2021.3079278
NR 42
TC 1
Z9 1
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 1295
EP 1305
DI 10.1109/TMM.2023.3279691
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IL9P7
UT WOS:001166602700045
DA 2024-08-05
ER

PT J
AU Zhu, PP
   Wang, X
   Zhu, L
   Sun, ZL
   Zheng, WS
   Wang, YW
   Chen, CW
AF Zhu, Peipei
   Wang, Xiao
   Zhu, Lin
   Sun, Zhenglong
   Zheng, Wei-Shi
   Wang, Yaowei
   Chen, Changwen
TI Prompt-Based Learning for Unpaired Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Metric prompt; prompt-based learning; semantic prompt; unpaired image
   captioning
AB Unpaired Image Captioning (UIC) has been developed to learn image descriptions from unaligned vision-language sample pairs. Existing works usually tackle this task using adversarial learning and visual concept reward based on reinforcement learning. However, these existing works were only able to learn limited cross-domain information in vision and language domains, which restrains the captioning performance of UIC. Inspired by the success of Vision-Language Pre-Trained Models (VL-PTMs) in this research, we attempt to infer the cross-domain cue information about a given image from the large VL-PTMs for the UIC task. This research is also motivated by recent successes of prompt learning in many downstream multi-modal tasks, including image-text retrieval and vision question answering. In this work, a semantic prompt is introduced and aggregated with visual features for more accurate caption prediction under the adversarial learning framework. In addition, a metric prompt is designed to select high-quality pseudo image-caption samples obtained from the basic captioning model and refine the model in an iterative manner. Extensive experiments on the COCO and Flickr30 K datasets validate the promising captioning ability of the proposed model. We expect that the proposed prompt-based UIC model will stimulate a new line of research for the VL-PTMs based captioning.
C1 [Zhu, Peipei; Sun, Zhenglong] Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Zhu, Peipei; Wang, Yaowei] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Wang, Xiao] Anhui Univ, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Zhu, Lin] Beijing Inst Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510275, Peoples R China.
   [Chen, Changwen] Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Peng Cheng Laboratory;
   Anhui University; Beijing Institute of Technology; Sun Yat Sen
   University; Hong Kong Polytechnic University
RP Sun, ZL (corresponding author), Chinese Univ Hong Kong, Sch Sci & Engn, Shenzhen 518172, Peoples R China.; Wang, YW (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.; Chen, CW (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
EM peipeizhu@link.cuhk.edu.cn; wangxiaocvpr@foxmail.com; linzhu@pku.edu.cn;
   sunzhenglong@cuhk.edu.cn; wszheng@ieee.org; wangyw@pcl.ac.cn;
   changwen.chen@polyu.edu.hk
OI , Peipei/0000-0001-7678-0005; Chen, Chang Wen/0000-0002-6720-234X; Zhu,
   Lin/0000-0001-6487-0441
FU Key-Area Research and Development Program of Guangdong Province
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Barraco M, 2022, IEEE COMPUT SOC CONF, P4661, DOI 10.1109/CVPRW56347.2022.00512
   Ben HX, 2022, IEEE T MULTIMEDIA, V24, P904, DOI 10.1109/TMM.2021.3060948
   Brown T., 2020, P ADV NEUR INF PROC, P1877
   Cao S, 2020, NEUROCOMPUTING, V417, P419, DOI 10.1016/j.neucom.2020.08.019
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen TH, 2017, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2017.64
   Chen WH, 2017, Arxiv, DOI arXiv:1611.05321
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Dai B, 2017, ADV NEUR IN, V30
   Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121
   Del Chiaro Riccardo, 2020, ADV NEURAL INFORM PR, P16736
   Denkowski M., 2014, P WMT ACL, P376
   Ding N, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P105
   Donahue J., 2016, Adversarial feature learning
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Feng Y, 2019, PROC CVPR IEEE, P4120, DOI 10.1109/CVPR.2019.00425
   Gu JX, 2019, IEEE I CONF COMP VIS, P10322, DOI 10.1109/ICCV.2019.01042
   Gu YX, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P8410
   Guo D, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P920
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   Gurari Danna, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P417, DOI 10.1007/978-3-030-58520-4_25
   Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LJ, 2022, IEEE INTERNET THINGS, V9, P13965, DOI 10.1109/JIOT.2022.3142759
   He LJ, 2021, UBICOMP/ISWC '21 ADJUNCT: PROCEEDINGS OF THE 2021 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING AND PROCEEDINGS OF THE 2021 ACM INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, P625, DOI 10.1145/3460418.3480405
   Hendricks LA, 2016, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2016.8
   Hong YC, 2021, PROC CVPR IEEE, P1643, DOI 10.1109/CVPR46437.2021.00169
   Hossain MZ, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3295748
   Hu SD, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2225
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jia ML, 2022, LECT NOTES COMPUT SC, V13693, P709, DOI 10.1007/978-3-031-19827-4_41
   Jin W, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2763
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim DJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2012
   Kim J, 2018, LECT NOTES COMPUT SC, V11206, P577, DOI 10.1007/978-3-030-01216-8_35
   Laina I, 2019, IEEE I CONF COMP VIS, P7413, DOI 10.1109/ICCV.2019.00751
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li YA, 2022, PROC CVPR IEEE, P17969, DOI 10.1109/CVPR52688.2022.01746
   LIN C.-Y., 2004, P 4 NTCITR WORKSH, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FL, 2019, IEEE DATA MINING, P439, DOI 10.1109/ICDM.2019.00054
   Liu P., 2021, ACM Computing Surveys, V15, P1
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Narasimhan M, 2021, 35 C NEURAL INFORM P, V34
   Omeiza D, 2022, IEEE T INTELL TRANSP, V23, P10142, DOI 10.1109/TITS.2021.3122865
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parmar N, 2018, PR MACH LEARN RES, V80
   Radford A, 2021, PR MACH LEARN RES, V139
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rubin O., 2021, P C N AM CHAPT ASS C, P2655
   Schick T, 2021, 16TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2021), P255
   Sharma P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2556
   Shen S, 2021, MUCH CAN CLIP BENEFI, P1
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2022, PROC CVPR IEEE, P3825, DOI 10.1109/CVPR52688.2022.00381
   Wang X, 2018, LECT NOTES COMPUT SC, V11220, P38, DOI 10.1007/978-3-030-01270-0_3
   Wang ZF, 2022, PROC CVPR IEEE, P139, DOI 10.1109/CVPR52688.2022.00024
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu SM, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P1180, DOI 10.1145/2998181.2998364
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Yang LY, 2021, IEEE T MULTIMEDIA, V23, P835, DOI 10.1109/TMM.2020.2990074
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Yao T, 2017, PROC CVPR IEEE, P5263, DOI 10.1109/CVPR.2017.559
   Yao Y, 2022, Arxiv, DOI arXiv:2109.11797
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang J, 2021, IEEE T MULTIMEDIA, V23, P92, DOI 10.1109/TMM.2020.2976552
   Zhang RR, 2022, PROC CVPR IEEE, P8542, DOI 10.1109/CVPR52688.2022.00836
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou KY, 2022, INT J COMPUT VISION, V130, P2337, DOI 10.1007/s11263-022-01653-1
   Zhu PP, 2023, IEEE T MULTIMEDIA, V25, P6702, DOI 10.1109/TMM.2022.3214090
NR 81
TC 1
Z9 1
U1 15
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2024
VL 26
BP 379
EP 393
DI 10.1109/TMM.2023.3265842
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HE7C9
UT WOS:001157873000005
OA Green Submitted
DA 2024-08-05
ER

EF