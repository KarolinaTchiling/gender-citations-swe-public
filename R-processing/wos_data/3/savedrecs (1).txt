FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Verma, A
   Subramaniam, LV
   Rajput, N
   Neti, C
   Faruquie, TA
AF Verma, A
   Subramaniam, LV
   Rajput, N
   Neti, C
   Faruquie, TA
TI Animating expressive faces across languages
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo
CY 2001
CL Tokyo, JAPAN
SP IEEE
DE audio to video mapping; facial animation; facial expression synthesis;
   lip synchronization; translingual visual speech synthesis
AB This paper describes a morphing-based audio driven facial animation system. Based on an incoming audio stream, a face image is animated with full lip synchronization and synthesized expressions. A novel scheme to implement a language independent system for audio-driven facial animation given a speech recognition system for just one language, in our case, English, is presented. The method presented here can also be used for text to audio-visual speech synthesis. Visemes in new expressions are synthesized to be able to generate animations with different facial expressions. An animation sequence using optical flow between visemes is constructed, given an incoming audio stream and still pictures of a face representing different visemes. The presented techniques give improved lip synchronization and naturalness to the animated video.
C1 Indian Inst Technol, IBM India Res Lab, New Delhi 110016, India.
   IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
C3 International Business Machines (IBM); Indian Institute of Technology
   System (IIT System); Indian Institute of Technology (IIT) - Delhi;
   International Business Machines (IBM)
RP Indian Inst Technol, IBM India Res Lab, Hauz Khas, New Delhi 110016, India.
EM vermar@in.ibm.com; lvsubram@in.ibm.com; rnitendr@in.ibm.com;
   cneti@us.ibm.com; ftanveer@in.ibm.com
RI Verma, Ashish/L-3943-2019
CR ANDERSON O, P ICASSP 94
   [Anonymous], 1998, Perceiving talking faces: From speech perception to a behavioral principle
   Bahl L. R., 1988, P ICASSP 88 NEW YORK, P40
   BAHL LR, 1989, P ICASSP 89 GLASG SC, P465
   BASU S, 2002, Patent No. 6366885
   BEYMER D, 1993, 1431 MIT AI LAB
   BLACK MJ, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P374, DOI 10.1109/ICCV.1995.466915
   Cohn J., 1998, Proceedings of the sixth ACM international conference on Multimedia: Face/gesture recognition and their applications, P41
   Cosatto E, 2000, IEEE T MULTIMEDIA, V2, P152, DOI 10.1109/6046.865480
   Donovan R. E., 1998, P INT C SPEECH LANG
   Essa IA, 1997, IEEE T PATTERN ANAL, V19, P757, DOI 10.1109/34.598232
   Ezzat T, 1998, COMP ANIM CONF PROC, P96, DOI 10.1109/CA.1998.681913
   EZZAT T, 1996, P 2 IEEE INT C AUT F
   LAVAGETTO F, 1994, P INT S SPEECH IM PR
   NETI C, 2002, NAT C COMM 2002 MUMB
   Parke F., 1996, COMPUTER FACIAL ANIM
   SCOTT SL, 1994, IEEE T PARALL DISTR, V5, P2, DOI 10.1109/71.262584
   Sharma R, 1998, P IEEE, V86, P853, DOI 10.1109/5.664275
   TARTTER VC, 1994, J ACOUST SOC AM, V96, P2101, DOI 10.1121/1.410151
   TSAI RY, 1981, IEEE T ACOUST SPEECH, V29, P1147, DOI 10.1109/TASSP.1981.1163710
   Waters K., 1994, Proceedings ACM Multimedia '94, P149, DOI 10.1145/192593.192644
NR 21
TC 15
Z9 16
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 791
EP 800
DI 10.1109/TMM.2004.837256
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200002
DA 2024-07-18
ER

PT J
AU Huang, CH
   Wu, JL
AF Huang, CH
   Wu, JL
TI Attacking visible watermarking schemes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE attacks; image inpainting; image recovery; visible watermarking
ID IMAGES
AB Visible watermarking schemes are important intellectual property rights (IPR) protection mechanisms for digital images and videos that have to be released for certain purposes but illegal reproductions of them are prohibited. Visible watermarking techniques protect digital contents in a more active manner, which is quite different from the invisible watermarking techniques. Digital data embedded with visible watermarks will contain recognizable but unobtrusive copyright patterns, and the details of the host data should still exist. The embedded pattern of a useful visible watermarking scheme should be difficult or even impossible to be removed unless intensive and expensive human labors are involved. In this paper, we propose an attacking scheme against current visible image watermarking techniques. After manually selecting the watermarked areas, only few human interventions are required. For watermarks purely composed of thin patterns, basic image recovery techniques can completely remove the embedded patterns. For more general watermarks consisting of thick patterns, not only information in surrounding unmarked areas but also information within watermarked areas will be utilized to correctly recover the host image. Although the proposed scheme does not guarantee that the recovered images will be exactly identical to the unmarked originals, the structure of the embedded pattern will be seriously destroyed and a perceptually satisfying recovered image can be obtained. In other words, a general attacking scheme based on the contradictive requirements of current visible watermarking techniques is worked out. Thus, the robustness of current visible watermarking schemes for digital images is doubtful and needs to be improved.
C1 Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Commun & Multimedia Lab, Taipei 106, Taiwan.
C3 National Taiwan University
RP Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Commun & Multimedia Lab, Taipei 106, Taiwan.
EM bh@cmlab.csie.ntu.edu.tw; wjl@cmlab.csie.ntu.edu.tw
OI WU, JA-LING/0000-0002-3631-1551
CR Bertalmio  M., 2000, SIGGRAPH 2000 AUG
   Braudaway GW, 1996, P SOC PHOTO-OPT INS, V2659, P126, DOI 10.1117/12.235469
   CHEN PM, WCCC ICSP 2000
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Haralick R. M., 1992, COMPUTER ROBOT VISIO
   Hsu CT, 1999, IEEE T IMAGE PROCESS, V8, P58, DOI 10.1109/83.736686
   HU Y, 2001, ELECT LETT, V37
   JUDD DB, 1975, COLORS BUSINESS SCI
   KANKANHALLI MS, 1999, IEEE INT C MULT COMP
   KOKARAM AC, 1995, IEEE T IMAGE PROCESS, V11
   MENG J, ICIP 98
   MOHANTY SP, 2000, ICME 2000
   MOHANTY SP, P ACM 1999, P49
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
NR 14
TC 69
Z9 78
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 16
EP 30
DI 10.1109/TMM.2003.819579
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Bharitkar, S
   Kyriakakis, C
AF Bharitkar, S
   Kyriakakis, C
TI Selective signal cancellation for multiple-listener audio applications
   using eigenfilters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE active noise cancellation; eigenfilter; room acoustics; signal
   cancellation
ID SOUND; EIGENVECTORS; MATRICES
AB Selectively canceling signals at specific locations within an acoustical environment with multiple listeners is of significant importance for home theater, automobile, teleconferencing, office, industrial, and other applications. The traditional noise cancellation approach is impractical for such applications because it requires secondary sources to "anti-phase" the primary source, or sensors to be placed on the listeners. In this paper we propose an alternative method for signal cancellation by pre-processing the acoustical signal with a filter known as the eigenfilter [1], [2]. We examine the theoretical properties of such filters, and investigate the performance (gain) and tradeoff issues such as, spectral distortion. Sensitivity of the performance as a function of the room impulse response duration (reverberation) modeled in the eigenfilter is also investigated.
C1 Univ So Calif, Integrated Media Syst Ctr, Immers Audio Lab, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Bharitkar, S (corresponding author), Univ So Calif, Integrated Media Syst Ctr, Immers Audio Lab, Los Angeles, CA 90089 USA.
RI Kyriakakis, Chris/KPA-5752-2024
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   Ash R., 1972, Real Analysis and Probability: Probability and Mathematical Statistics: a Series of Monographs and Textbooks, DOI DOI 10.1016/C2013-0-06164-6
   BHARITKAR S, 2000, P IEEE INT S INT SIG
   BHARITKAR S, 2000, P IEEE C MULT JUL
   BUCKLEY KM, 1987, IEEE T ACOUST SPEECH, V35, P249, DOI 10.1109/TASSP.1987.1165142
   CANTONI A, 1976, IEEE T COMMUN, V24, P804, DOI 10.1109/TCOM.1976.1093391
   Childers D.G., 2000, Speech processing and synthesis toolboxes
   Doak P, 1959, ACUSTICA, V9, P1
   Elliott SJ, 1993, IEEE SIGNAL PROC MAG, V10, P12, DOI 10.1109/79.248551
   GRAY RM, 1980, IEEE T ACOUST SPEECH, V28, P367, DOI 10.1109/TASSP.1980.1163421
   GUICKING D, 1990, J ACOUST SOC AM, V87, P2251, DOI 10.1121/1.399195
   Haykin S. S., 2008, ADAPTIVE FILTER THEO
   Kuttruff H., 1991, ROOM ACOUSTICS
   MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792
   MAKHOUL J, 1981, IEEE T ACOUST SPEECH, V29, P868, DOI 10.1109/TASSP.1981.1163635
   MOURJOPOULOS J, 1985, J SOUND VIB, V102, P217, DOI 10.1016/S0022-460X(85)80054-7
   MOURJOPOULOS J, ROOM ACOUSTICS SIMUL
   NELSON PA, 1987, J SOUND VIB, V117, P1, DOI 10.1016/0022-460X(87)90432-9
   ORFANADIS SJ, 1985, OPTIMUM SIGNAL PROCE
   RABINER LR, 1993, THEORY APPL DIGITAL
   ROBINSON E, 1967, STAT COMMUNICATION D, P269
   ROSS CF, 1981, J SOUND VIB, V74, P411, DOI 10.1016/0022-460X(81)90307-2
   WILLIAMS JEF, 1984, P ROY SOC LOND A MAT, V395, P63, DOI 10.1098/rspa.1984.0090
NR 23
TC 1
Z9 1
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 329
EP 338
DI 10.1109/TMM.2003.811656
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500005
DA 2024-07-18
ER

PT J
AU Papadias, D
   Mantzourogiannis, M
   Ahmad, I
AF Papadias, D
   Mantzourogiannis, M
   Ahmad, I
TI Fast retrieval of similar configurations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based retrieval; local search algorithms; spatial similarity
ID CONSTRAINT SATISFACTION; GENETIC ALGORITHMS; QUERY; REPRESENTATION;
   OPTIMIZATION
AB Configuration similarity is a special form of content-based image retrieval that considers relative, object locations. It can be used as a standalone method, or to complement retrieval based on visual or semantic features. The corresponding queries ask for sets of objects, that. satisfy some spatio-temporal constraints, e.g., "find all,triplets of objects (nu(1), nu(2), nu(3)), such that, nu(1) is northeast of nu(2), which is inside nu(3)." Exhaustive processing (i.e., retrieval of the best solutions) of configuration similarity queries, in general, has exponential complexity and fast search for sub-optimal solutions is the only way. to deal with the vast amounts of multimedia information in several real-time applications. In this paper we first discuss the utilization of nonsystematic search heuristics, based on genetic algorithms, simulated annealing and hill climbing approaches. An extensive experimentation. with real and synthetic datasets. reveals that hill climbing techniques are the best for the current problem; therefore, as a subsequent step we study the search space, and, develop improved variations of hill climbing that take advantage, of the. special structure. of the problem to enhance speed. The, proposed heuristic methods significantly outperform systematic search when them is only limited time for query processing.
C1 Hong Kong Univ Sci & Technol, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Papadias, D (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Clearwater Bay, Hong Kong, Hong Kong, Peoples R China.
EM dimitirs@cs.ust.hk; mantzour@cs.ust.hk
CR ALLEN JF, 1983, COMMUN ACM, V26, P11
   [Anonymous], P ACM SIGMOD INT C M
   CERNY V, 1985, J OPTIMIZ THEORY APP, V45, P41, DOI 10.1007/BF00940812
   Chang S. W., 1989, Potentialities of agricultural engineering in rural development. Proceedings of the international symposium on agricultural engineering (89-ISAE), Beijing, China, 12-15 September 1989., P303
   CHANG SK, 1987, IEEE T PATTERN ANAL, V9, P413, DOI 10.1109/TPAMI.1987.4767923
   Ciaccia P, 1997, PROCEEDINGS OF THE TWENTY-THIRD INTERNATIONAL CONFERENCE ON VERY LARGE DATABASES, P426
   Delis V., 1998, Proceedings ACM Multimedia 98, P333, DOI 10.1145/290747.290797
   Egenhofer MJ, 1997, J VISUAL LANG COMPUT, V8, P403, DOI 10.1006/jvlc.1997.0054
   Ferman AM, 1997, P SOC PHOTO-OPT INS, V3024, P953, DOI 10.1117/12.263307
   FREKSA C, 1992, ARTIF INTELL, V54, P199, DOI 10.1016/0004-3702(92)90090-K
   GALINDOLEGARIA C, 1994, P 20 INT C VER LARG, P85
   Gent IP, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P246
   Goldberg David E, 1989, GENETIC ALGORITHMS S
   GREFENSTETTE JJ, 1986, IEEE T SYST MAN CYB, V16, P122, DOI 10.1109/TSMC.1986.289288
   GUDIVADA VN, 1995, ACM T INFORM SYST, V13, P115, DOI 10.1145/201040.201041
   HARALICK RM, 1980, ARTIF INTELL, V14, P263, DOI 10.1016/0004-3702(80)90051-X
   IOANNIDIS YE, 1990, SIGMOD REC, V19, P312, DOI 10.1145/93605.98740
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   LEE SY, 1992, PATTERN RECOGN, V25, P305, DOI 10.1016/0031-3203(92)90112-V
   LEE SY, 1992, J VISUAL LANG COMPUT, V3, P373
   MINTON S, 1992, ARTIF INTELL, V58, P161, DOI 10.1016/0004-3702(92)90007-K
   Nabil M, 1996, IEEE T KNOWL DATA EN, V8, P533, DOI 10.1109/69.536246
   NAHAR S, 1986, P 23 DES AUT C, P293
   Osman IH, 1996, ANN OPER RES, V63, P513
   Papadias D., 1998, Proceedings of the 1998 ACM CIKM International Conference on Information and Knowledge Management, P289, DOI 10.1145/288627.288669
   PAPADIAS D, 1995, J VISUAL LANG COMPUT, V6, P53, DOI 10.1006/jvlc.1995.1004
   Papadias D, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P168, DOI 10.1145/312624.312673
   Papadias D., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P546
   PETRAKIS E, IN PRESS IEEE T KNOW
   Petrakis EGM, 1997, IEEE T KNOWL DATA EN, V9, P435, DOI 10.1109/69.599932
   PRICE K, 1981, IEEE T PATTERN ANAL, V7, P617
   SHAPIRO LG, 1981, IEEE T PATTERN ANAL, V3, P504, DOI 10.1109/TPAMI.1981.4767144
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Smith JR, 1999, MULTIMEDIA SYST, V7, P129, DOI 10.1007/s005300050116
   SOFFER A, 1996, P SPIE STORAGE RETRI, P144
   SRINIVAS M, 1994, IEEE T SYST MAN CYB, V24, P656, DOI 10.1109/21.286385
   [No title captured]
NR 37
TC 3
Z9 3
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2003
VL 5
IS 2
BP 210
EP 222
DI 10.1109/TMM.2003.811629
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 695HB
UT WOS:000183824100006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, W
   Xu, HY
   Pu, N
   Liu, Y
   Lao, MR
   Wang, WP
   Liu, L
   Lew, MS
AF Chen, Wei
   Xu, Haoyang
   Pu, Nan
   Liu, Yu
   Lao, Mingrui
   Wang, Weiping
   Liu, Li
   Lew, Michael S.
TI Lifelong Fine-Grained Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; lifelong learning; knowledge distillation; catastrophic
   forgetting; data-free image generation
AB Fine-grained image retrieval has been extensively explored in a zero-shot manner. A deep model is trained on the seen part and then evaluated the generalization performance on the unseen part. However, this setting is infeasible for many real-world applications since (1) the retrieval dataset can be non-fixed so that new data are added constantly, and (2) data samples of the seen categories are also common in practice and are important for evaluation. In this paper, we explore lifelong fine-grained image retrieval (LFGIR), which learns continuously on a sequence of new tasks with data from different datasets. We first use knowledge distillation to minimize catastrophic forgetting on old tasks. Training continuously on different datasets causes large domain shifts between the old and new tasks while image retrieval is sensitive to even small shifts in the features. This tends to weaken the effectiveness of knowledge distillation by the frozen teacher. To mitigate the impact of domain shifts, we use the network inversion method to generate images of the old tasks. In addition, we design an on-the-fly teacher which transfers knowledge captured on a new task to the student to improve better generalization performance, thereby achieving a better balance between old and new tasks in the end. We name the whole framework as Dual Knowledge Distillation (DKD), whose efficacy is demonstrated by extensive experimental results on sequential tasks including seven datasets.
C1 [Chen, Wei; Wang, Weiping; Liu, Li] Acad Adv Technol Res Hunan, Changsha 410000, Peoples R China.
   [Xu, Haoyang] Xidian Univ, Coll Commun Engn, Xian 710071, Peoples R China.
   [Pu, Nan; Lao, Mingrui] Leiden Univ, Leiden Inst Adv Comp Sci, NL-2334ES Leiden, Netherlands.
   [Liu, Yu] Dalian Univ Technol, DUT RU Int Sch Informat Sci Engn, Dalian 116000, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90570, Finland.
C3 Xidian University; Leiden University; Leiden University - Excl LUMC;
   Dalian University of Technology; University of Oulu
RP Chen, W (corresponding author), Acad Adv Technol Res Hunan, Changsha 410000, Peoples R China.
EM weichen@nudt.edu.cn; 1397572418@qq.com; n.pu@liacs.leidenuniv.nl;
   liuyu8824@dlut.edu.cn; m.lao@liacs.leidenuniv.nl; wangwp@nudt.edu.cn;
   li.liu@oulu.fi; m.s.lew@liacs.leidenuniv.nl
RI Pu, Nan/JMQ-1195-2023
OI Liu, li/0000-0002-2011-2873; Pu, Nan/0000-0002-2179-8301
FU Leiden University
FX No Statement Available
CR Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Boiman O, 2008, PROC CVPR IEEE, P1992, DOI 10.1109/CVPR.2008.4587598
   Brown Andrew, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P677, DOI 10.1007/978-3-030-58545-7_39
   Chen HT, 2019, IEEE I CONF COMP VIS, P3513, DOI 10.1109/ICCV.2019.00361
   Chen W., 2020, P BRIT MACH VIS C, P1
   Chen W, 2022, IEEE T MULTIMEDIA, V24, P1844, DOI 10.1109/TMM.2021.3073279
   De Lange M, 2022, IEEE T PATTERN ANAL, V44, P3366, DOI 10.1109/TPAMI.2021.3057446
   Dong N, 2021, ADV NEUR IN, V34
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Gautam C, 2021, Arxiv, DOI arXiv:2103.10741
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Haroush Matan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8491, DOI 10.1109/CVPR42600.2020.00852
   Hinton G., 2014, P DEEP LEARN REPR WO, P1
   Hongxu Yin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8712, DOI 10.1109/CVPR42600.2020.00874
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Jung H, 2018, AAAI CONF ARTIF INTE, P3358
   Kaya M, 2019, SYMMETRY-BASEL, V11, DOI 10.3390/sym11091066
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   van de Ven GM, 2019, Arxiv, DOI arXiv:1809.10635
   Maji S, 2013, Arxiv, DOI arXiv:1306.5151
   Mengyao Zhai, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P397, DOI 10.1007/978-3-030-58589-1_24
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Park D, 2019, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2019.00343
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Peng BY, 2019, IEEE I CONF COMP VIS, P5006, DOI 10.1109/ICCV.2019.00511
   Perez-Rua Juan-Manuel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13843, DOI 10.1109/CVPR42600.2020.01386
   Rannen A, 2017, IEEE I CONF COMP VIS, P1329, DOI 10.1109/ICCV.2017.148
   Shin H, 2017, ADV NEUR IN, V30
   Shmelkov K, 2017, IEEE I CONF COMP VIS, P3420, DOI 10.1109/ICCV.2017.368
   Skorokhodov I., 2020, P INT C LEARN REPR, P1
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Sun G, 2022, IEEE T PATTERN ANAL, V44, P3895, DOI 10.1109/TPAMI.2021.3058852
   Sun G, 2021, IEEE T NEUR NET LEAR, V32, P139, DOI 10.1109/TNNLS.2020.2977497
   Tian X, 2021, IEEE T MULTIMEDIA, V23, P1210, DOI 10.1109/TMM.2020.2994509
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wang Y, 2017, AAAI CONF ARTIF INTE, P2739
   Wei K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P551
   Wu C., 2018, NEURIPS, P5962
   Wu DY, 2019, PROC CVPR IEEE, P9061, DOI 10.1109/CVPR.2019.00928
   Xie LX, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P3, DOI 10.1145/2671188.2749289
   Yao X, 2019, NEURAL COMPUT, V31, P2266, DOI 10.1162/neco_a_01232
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yoojin Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW). Proceedings, P3047, DOI 10.1109/CVPRW50498.2020.00363
   You S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1285, DOI 10.1145/3097983.3098135
   Yu L, 2019, PROC CVPR IEEE, P2902, DOI 10.1109/CVPR.2019.00302
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhai MY, 2019, IEEE I CONF COMP VIS, P2759, DOI 10.1109/ICCV.2019.00285
   Zhang B, 2019, IEEE-ACM T AUDIO SPE, V27, P2278, DOI 10.1109/TASLP.2019.2946480
   Zhang JT, 2020, IEEE WINT CONF APPL, P1120, DOI [10.1109/WACV45572.2020.9093365, 10.1109/wacv45572.2020.9093365]
NR 56
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7533
EP 7544
DI 10.1109/TMM.2022.3222934
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Fang, X
   Liu, DZ
   Zhou, P
   Hu, YC
AF Fang, Xiang
   Liu, Daizong
   Zhou, Pan
   Hu, Yuchong
TI Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video moment retrieval; cross-domain learning; unsupervised domain
   adaption
AB As an increasingly popular task in multimedia information retrieval, video moment retrieval (VMR) aims to localize the target moment from an untrimmed video according to a given language query. Most previous methods depend heavily on numerous manual annotations (i.e., moment boundaries), which are extremely expensive to acquire in practice. In addition, due to the domain gap between different datasets, directly applying these pre-trained models to an unseen domain leads to a significant performance drop. In this paper, we focus on a novel task: cross-domain VMR, where fully-annotated datasets are available in one domain ("source domain"), but the domain of interest ("target domain") only contains unannotated datasets. As far as we know, we present the first study on cross-domain VMR. To address this new task, we propose a novel Multi-Modal Cross-Domain Alignment (MMCDA) network to transfer the annotation knowledge from the source domain to the target domain. However, due to the domain discrepancy between the source and target domains and the semantic gap between videos and queries, directly applying trained models to the target domain generally leads to a performance drop. To solve this problem, we develop three novel modules: (i) a domain alignment module is designed to align the feature distributions between different domains of each modality; (ii) a cross-modal alignment module aims to map both video and query features into a joint embedding space and to align the feature distributions between different modalities in the target domain; and (iii) a specific alignment module tries to obtain the fine-grained similarity between a specific frame and the given query for optimal localization. By jointly training these three modules, our MMCDA can learn domain-invariant and semantic-aligned cross-modal representations. Extensive experiments on three challenging benchmarks (ActivityNet Captions, Charades-STA and TACoS) illustrate that our cross-domain method MMCDA outperforms all state-of-the-art single-domain methods. Impressively, MMCDA raises the performance by more than 7% in representative cases, which demonstrates its effectiveness.
C1 [Fang, Xiang; Zhou, Pan] Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Hubei, Peoples R China.
   [Liu, Daizong] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Hu, Yuchong] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Key Lab Informat Storage Syst, Minist Educ China, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Peking University; Huazhong
   University of Science & Technology
RP Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Hubei, Peoples R China.
EM xfang9508@gmail.com; daizongliu1996@gmail.com; zhoupannewton@gmail.com;
   yuchonghu@hust.edu.cn
OI liu, daizong/0000-0001-8179-4508; Fang, Xiang/0000-0003-3231-5771
FU National Natural Science Foundation of China [61972448]
FX This work was supported by National Natural Science Foundation of China
   under Grant 61972448.
CR Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4162, DOI 10.1145/3394171.3413840
   Chen K, 2017, IEEE I CONF COMP VIS, P824, DOI 10.1109/ICCV.2017.95
   Chen QC, 2021, AAAI CONF ARTIF INTE, V35, P1072
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chen ZF, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1884
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan X, 2018, ADV NEUR IN, V31
   Fang X, 2022, Arxiv, DOI arXiv:2208.14882
   Fang X, 2022, IEEE TETCI, V6, P913, DOI 10.1109/TETCI.2021.3077909
   Fang Xiang, 2020, IEEE Trans Artif Intell, V1, P233, DOI 10.1109/TAI.2021.3052425
   Fengda Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10009, DOI 10.1109/CVPR42600.2020.01003
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guan DY, 2022, IEEE T MULTIMEDIA, V24, P2502, DOI 10.1109/TMM.2021.3082687
   Guo ZY, 2022, IEEE T MULTIMEDIA, V24, P2606, DOI 10.1109/TMM.2021.3087001
   Hendricks LA, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P1380
   Hosseini-Asl E., 2018, P INT C LEARN REPR
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P5933, DOI 10.1109/TIP.2021.3090521
   Huang X, 2020, IEEE T CYBERNETICS, V50, P1047, DOI 10.1109/TCYB.2018.2879846
   Jaritz Maximilian, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12602, DOI 10.1109/CVPR42600.2020.01262
   Jie Lei, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P447, DOI 10.1007/978-3-030-58589-1_27
   Jing MM, 2023, IEEE T MULTIMEDIA, V25, P2559, DOI 10.1109/TMM.2022.3148592
   Kim D, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13598, DOI 10.1109/ICCV48922.2021.01336
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li K, 2021, AAAI CONF ARTIF INTE, V35, P1902
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li X, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3051383
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu D., 2022, arXiv
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1683
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu Daizong, 2020, P 28 INT C COMP LING, P1841
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Long FC, 2018, ACM/SIGIR PROCEEDINGS 2018, P725, DOI 10.1145/3209978.3209999
   Long MS, 2018, ADV NEUR IN, V31
   Minuk Ma, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P156, DOI 10.1007/978-3-030-58604-1_10
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Munro J, 2020, PROC CVPR IEEE, P119, DOI 10.1109/CVPR42600.2020.00020
   Na J, 2021, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR46437.2021.00115
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Silva A, 2021, AAAI CONF ARTIF INTE, V35, P557
   Song XL, 2021, PROC CVPR IEEE, P9782, DOI 10.1109/CVPR46437.2021.00966
   Song YJ, 2020, Arxiv, DOI arXiv:2003.07048
   Sun XY, 2021, IEEE T IMAGE PROCESS, V30, P5589, DOI 10.1109/TIP.2021.3086591
   Tan RB, 2021, IEEE WINT CONF APPL, P2082, DOI 10.1109/WACV48630.2021.00213
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Tao YS, 2023, IEEE T MULTIMEDIA, V25, P4586, DOI 10.1109/TMM.2022.3178599
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GM, 2022, IEEE T MULTIMEDIA, V24, P1221, DOI 10.1109/TMM.2022.3142420
   Wang H, 2021, PROC CVPR IEEE, P7022, DOI 10.1109/CVPR46437.2021.00695
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Wang P, 2023, IEEE T MULTIMEDIA, V25, P6026, DOI 10.1109/TMM.2022.3203574
   Wang PF, 2023, IEEE T MULTIMEDIA, V25, P2624, DOI 10.1109/TMM.2022.3149629
   Wang WN, 2019, PROC CVPR IEEE, P334, DOI 10.1109/CVPR.2019.00042
   Wang YX, 2022, IEEE-CAA J AUTOMATIC, V9, P1612, DOI 10.1109/JAS.2022.105515
   Wang ZK, 2022, IEEE T CIRC SYST VID, V32, P8179, DOI 10.1109/TCSVT.2021.3076097
   Xiang Fang, 2022, IEEE Transactions on Artificial Intelligence, V3, P192, DOI 10.1109/TAI.2021.3116546
   Xiao SN, 2021, AAAI CONF ARTIF INTE, V35, P2986
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yang X, 2022, IEEE T IMAGE PROCESS, V31, P1204, DOI 10.1109/TIP.2022.3140611
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng Y., 2022, ACM T MULTIM COMPUT, V18, P1
   Zeng YW, 2021, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR46437.2021.00225
   Zhang AR, 2023, IEEE T MULTIMEDIA, V25, P1773, DOI 10.1109/TMM.2022.3162710
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang H, 2022, IEEE T PATTERN ANAL, V44, P4252, DOI 10.1109/TPAMI.2021.3060449
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang MX, 2021, PROC CVPR IEEE, P12664, DOI 10.1109/CVPR46437.2021.01248
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang WC, 2021, IEEE T IMAGE PROCESS, V30, P3293, DOI 10.1109/TIP.2021.3052083
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang ZJ, 2021, IEEE T MULTIMEDIA, V23, P3306, DOI 10.1109/TMM.2020.3023339
   Zhao WT, 2021, IEEE T IMAGE PROCESS, V30, P1180, DOI 10.1109/TIP.2020.3042086
   Zhao Y, 2021, PROC CVPR IEEE, P4195, DOI 10.1109/CVPR46437.2021.00418
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zolfaghari M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1430, DOI 10.1109/ICCV48922.2021.00148
NR 89
TC 1
Z9 1
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7517
EP 7532
DI 10.1109/TMM.2022.3222965
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000057
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ghafoor, M
   Mahmood, A
AF Ghafoor, Mehwish
   Mahmood, Arif
TI Quantification of Occlusion Handling Capability of a 3D Human Pose
   Estimation Framework
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action classification; human pose estimation; occlusion aware networks;
   occlusion handling quantification; temporal dilated CNN
AB 3D human pose estimation using monocular images is an important yet challenging task. Existing 3D pose detection methods exhibit excellent performance under normal conditions however their performance may degrade due to occlusion. Recently some occlusion aware methods have also been proposed, however, the occlusion handling capability of these networks has not yet been thoroughly investigated. In the current work, we propose an occlusion-guided 3D human pose estimation framework and quantify its occlusion handling capability by using different protocols. The proposed method estimates more accurate 3D human poses using 2D skeletons with missing joints as input. Missing joints are handled by introducing occlusion guidance that provides extra information about the absence or presence of a joint. Temporal information has also been exploited to better estimate the missing joints. A large number of experiments are performed for the quantification of occlusion handling capability of the proposed method on three publicly available datasets in various settings including random missing joints, fixed body parts missing, and complete frames missing, using mean per joint position error criterion. In addition to that, the quality of the predicted 3D poses is also evaluated using action classification performance as a criterion. 3D poses estimated by the proposed method achieved significantly improved action recognition performance in the presence of missing joints. Our experiments demonstrate the effectiveness of the proposed framework for handling the missing joints as well as quantification of the occlusion handling capability of the deep neural networks.
C1 [Ghafoor, Mehwish; Mahmood, Arif] Informat Technol Univ, Dept Comp Sci, Lahore 54600, Pakistan.
RP Mahmood, A (corresponding author), Informat Technol Univ, Dept Comp Sci, Lahore 54600, Pakistan.
EM mehwish.ghafoor@itu.edu.pk; arif.mahmood@itu.edu.pk
RI Mahmood, Arif/R-7949-2019
OI Mahmood, Arif/0000-0001-5986-9876
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Bao Q, 2021, IEEE T MULTIMEDIA, V23, P161, DOI 10.1109/TMM.2020.2980194
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586
   Chen TL, 2022, IEEE T CIRC SYST VID, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Cheng Y., 2021, P AAAI C ART INT
   Cheng Y, 2020, AAAI CONF ARTIF INTE, V34, P10631
   Cheng Y, 2019, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2019.00081
   Das Srijan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P72, DOI 10.1007/978-3-030-58545-7_5
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   Gu RS, 2021, INT C PATT RECOG, P8243, DOI 10.1109/ICPR48806.2021.9412107
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li MP, 2019, IEEE T MULTIMEDIA, V21, P2653, DOI 10.1109/TMM.2019.2903455
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Markovitz Amir, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10536, DOI 10.1109/CVPR42600.2020.01055
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Misra D, 2020, Arxiv, DOI arXiv:1908.08681
   Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P1021, DOI 10.1109/TMM.2020.2991532
   Park S., 2018, PROC BRIT MACH VIS C
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Qammaz A, 2021, INT C PATT RECOG, P6904, DOI 10.1109/ICPR48806.2021.9411956
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Srivastav Vinkle, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12261), P761, DOI 10.1007/978-3-030-59710-8_74
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang, 2020, P IEEE CVF C COMP VI, P7376
   Weng CY, 2019, PROC CVPR IEEE, P5901, DOI 10.1109/CVPR.2019.00606
   Xu YL, 2022, IEEE T PATTERN ANAL, V44, P6327, DOI 10.1109/TPAMI.2021.3087695
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhao L, 2019, PROC CVPR IEEE, P3420, DOI 10.1109/CVPR.2019.00354
NR 33
TC 6
Z9 6
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3311
EP 3318
DI 10.1109/TMM.2022.3158068
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200027
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ji, JY
   Huang, XY
   Sun, XS
   Zhou, YY
   Luo, G
   Cao, LJ
   Liu, JZ
   Shao, L
   Ji, RR
AF Ji, Jiayi
   Huang, Xiaoyang
   Sun, Xiaoshuai
   Zhou, Yiyi
   Luo, Gen
   Cao, Liujuan
   Liu, Jianzhuang
   Shao, Ling
   Ji, Rongrong
TI Multi-Branch Distance-Sensitive Self-Attention Network for Image
   Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Visualization; Head; Feature extraction; Encoding;
   Computer architecture; Computational modeling; Image captioning;
   multi-branch techniques; distance-sensitive positional embedding
AB Self-attention (SA) based networks have achieved great success in image captioning, constantly dominating the leaderboards of online benchmarks. However, existing SA networks still suffer from distance insensitivity and low-rank bottleneck. In this paper, we aim to optimize SA in terms of two aspects, thereby addressing the above issues. First, we introduce a Distance-sensitive Self-Attention (DSA), which considers the raw geometric distances between query-key pairs in the 2D images during SA modeling. Second, we present a simple yet effective approach, named Multi-branch Self-Attention (MSA) to compensate for the low-rank bottleneck. MSA treats a multi-head self-attention layer as a branch and duplicates it multiple times to increase the expressive power of SA. To validate the effectiveness of the two designs, we apply them to the standard self-attention network, and conduct extensive experiments on the highly competitive MS-COCO dataset. We achieve new state-of-the-art performance on both the local and online test sets, i.e., 135.1% CIDEr on the Karpathy split and 135.4% CIDEr on the official online split.
C1 [Ji, Jiayi; Huang, Xiaoyang; Sun, Xiaoshuai; Zhou, Yiyi; Luo, Gen; Cao, Liujuan; Ji, Rongrong] Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
   [Sun, Xiaoshuai] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Sun, Xiaoshuai] Xiamen Univ, Fujian Engn Res Ctr Trusted Artificial Intelligen, Xiamen 361005, Peoples R China.
   [Liu, Jianzhuang] Huawei Technol Co Ltd, Noahs Ark Lab, Shenzhen 518129, Peoples R China.
   [Shao, Ling] Terminus Grp, Beijing 101100, Peoples R China.
   [Ji, Rongrong] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Xiamen University; Xiamen University; Xiamen University; Huawei
   Technologies; Peng Cheng Laboratory
RP Sun, XS (corresponding author), Xiamen Univ, Sch Informat, Dept Artificial Intelligence, Media Analyt & Comp Lab, Xiamen 361005, Peoples R China.
EM jjyxmu@gmail.com; huangxy24@stu.xmu.edu.cn; xssun@xmu.edu.cn;
   zhouyiyi@xmu.edu.cn; luogen@stu.xmu.edu.cn; caoliujuan@xmu.edu.cn;
   liu.jianzhuang@huawei.com; ling.shao@ieee.org; rrji@xmu.edu.cn
RI Shao, Ling/D-3535-2011
OI Cao, Liujuan/0000-0002-7645-9606; Ji, Jiayi/0000-0002-9956-6308; Luo,
   Gen/0000-0001-5334-1843
FU National Science Fund for Distinguished Young Scholars; National Natural
   Science Foundation of China [62025603, U21B2037, 62176222, 62176223,
   62176226, 62072386, 62072387, 62072389]; Guangdong Basic and Applied
   Basic Research Foundation [62002305]; Natural Science Foundation of
   Fujian Province of China [2019B1515120049];  [2021J01002]
FX This work was supported in part by the National Science Fund for
   Distinguished Young Scholars under Grant 62025603, in part by the
   National Natural Science Foundation of China under Grants U21B2037,
   62176222, 62176223, 62176226, 62072386, 62072387, 62072389, and
   62002305, in part by the Guangdong Basic and Applied Basic Research
   Foundation under Grant 2019B1515120049, and in part by the Natural
   Science Foundation of Fujian Province of China under Grant 2021J01002.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Banerjee S., 2005, P ACL WORKSH INTR EX, P65
   Bhojanapalli S, 2020, PR MACH LEARN RES, V119
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cornia Marcella, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10575, DOI 10.1109/CVPR42600.2020.01059
   Fan Y, 2020, Arxiv, DOI arXiv:2006.10270
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Herdade S, 2019, ADV NEUR IN, V32
   Hou JY, 2020, AAAI CONF ARTIF INTE, V34, P10973
   Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Jiang H., 2020, IEEE CVF C COMP VIS, DOI 10.1109/CVPR42600.2020.01028
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Ke L, 2019, IEEE I CONF COMP VIS, P8887, DOI 10.1109/ICCV.2019.00898
   Kim JH, 2018, ADV NEUR IN, V31
   Kingma D.P., 2014, ARXIV14126980
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li G, 2019, IEEE I CONF COMP VIS, P8927, DOI 10.1109/ICCV.2019.00902
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu FL, 2019, ADV NEUR IN, V32
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Longteng Guo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10324, DOI 10.1109/CVPR42600.2020.01034
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo YP, 2021, AAAI CONF ARTIF INTE, V35, P2286
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park CC, 2019, IEEE T PATTERN ANAL, V41, P999, DOI 10.1109/TPAMI.2018.2824816
   Ren S., 2015, NEURAL INFORM PROCES, V28, P91, DOI DOI 10.1109/TPAMI.2016.2577031
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shazeer N, 2020, Arxiv, DOI arXiv:2003.02436
   Smith N. A., 2021, arXiv
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Teney D, 2018, PROC CVPR IEEE, P4223, DOI 10.1109/CVPR.2018.00444
   Vaswani A, 2021, PROC CVPR IEEE, P12889, DOI 10.1109/CVPR46437.2021.01270
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Veit A, 2016, ADV NEUR IN, V29
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wu CH, 2021, 2021 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL-HLT 2021), P2059
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Ye SM, 2018, IEEE T IMAGE PROCESS, V27, P5514, DOI 10.1109/TIP.2018.2855406
   Yingwei Pan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10968, DOI 10.1109/CVPR42600.2020.01098
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2017, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2017.375
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Zha ZJ, 2022, IEEE T PATTERN ANAL, V44, P710, DOI 10.1109/TPAMI.2019.2909864
   Zhang HW, 2018, PROC CVPR IEEE, P4158, DOI 10.1109/CVPR.2018.00437
   Zhang MX, 2019, IEEE T IMAGE PROCESS, V28, P32, DOI 10.1109/TIP.2018.2855415
   Zhang XY, 2021, PROC CVPR IEEE, P15460, DOI 10.1109/CVPR46437.2021.01521
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao WT, 2021, IEEE T IMAGE PROCESS, V30, P1180, DOI 10.1109/TIP.2020.3042086
   Zheng ZL, 2019, PROC CVPR IEEE, P3662, DOI 10.1109/CVPR.2019.00683
   Zhou J., 2016, T ASS COMPUT LING, V4, P371, DOI [DOI 10.1162/TACL_A_00105, 10.1162/tacl_a_00105]
NR 69
TC 6
Z9 6
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3962
EP 3974
DI 10.1109/TMM.2022.3169061
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA S2QI4
UT WOS:001069663600004
DA 2024-07-18
ER

PT J
AU Luo, XQ
   Gao, YH
   Wang, AQ
   Zhang, ZC
   Wu, XJ
AF Luo, Xiaoqing
   Gao, Yuanhao
   Wang, Anqi
   Zhang, Zhancheng
   Wu, Xiao-Jun
TI IFSepR: A General Framework for Image Fusion Based on Separate
   Representation Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Task analysis; Image reconstruction;
   Decoding; Transforms; Knowledge engineering; Autoencoder; contrastive
   learning; disentangled feature learning; fusion rule; image fusion
ID MULTISCALE TRANSFORM; NSCT; EXTRACTION
AB This paper proposes an image fusion framework based on separate representation learning, called IFSepR. We believe that both the co-modal image and the multi-modal image have common and private features based on prior knowledge, exploiting this disentangled representation can help to image fusion, especially to fusion rule design. Inspired by the autoencoder network and contrastive learning, a multi-branch encoder with contrastive constraints is built to learn the common and private features of paired images. In the fusion stage, based on the disentangled features, a general fusion rule is designed to integrate the private features, then combining the fused private features and the common feature are fed into the decoder, reconstructing the fused image. We perform a series of evaluations on three typical image fusion tasks, including multi-focus image fusion, infrared and visible image fusion, medical image fusion. Quantitative and qualitative comparison with five state-of-art image fusion methods demonstrates the advantages of our proposed model.
C1 [Luo, Xiaoqing; Gao, Yuanhao; Wang, Anqi; Wu, Xiao-Jun] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Jiangsu Prov Engn Lab Pattern Recognit & Computat, Wuxi 214122, Peoples R China.
   [Zhang, Zhancheng] Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou 215009, Peoples R China.
C3 Jiangnan University; Suzhou University of Science & Technology
RP Zhang, ZC (corresponding author), Suzhou Univ Sci & Technol, Sch Elect & Informat Engn, Suzhou 215009, Peoples R China.
EM xqluo@jiangnan.edu.cn; 1749338923@qq.com;
   6191910006@stu.jiangnan.edu.cn; zczhang@usts.edu.cn;
   wu_xiaojun@jiangnan.edu.cn
RI Gao, Yuanhao/KFS-3943-2024; Luo, Xiaoqing/AAM-2176-2021
OI Wu, Xiao-Jun/0000-0002-0310-5778
FU National Natural Science Foundation of China [61772237]; Six Talent
   Peaks Project in Jiangsu Province [XYDXX-030]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772237, and in part by the Six Talent
   Peaks Project in Jiangsu Province under Grant XYDXX-030.
CR Aslantas V, 2015, AEU-INT J ELECTRON C, V69, P160, DOI 10.1016/j.aeue.2015.09.004
   Bai XZ, 2015, INFORM FUSION, V22, P105, DOI 10.1016/j.inffus.2014.05.003
   Bai XZ, 2011, OPT EXPRESS, V19, P8444, DOI 10.1364/OE.19.008444
   Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Bousmalis K, 2016, ADV NEUR IN, V29
   Chen T, 2020, PR MACH LEARN RES, V119
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Dou JF, 2019, MULTIMED TOOLS APPL, V78, P12491, DOI 10.1007/s11042-018-6756-0
   Epstein B, 2019, LECT NOTES ARTIF INT, V11051, P494, DOI 10.1007/978-3-030-10925-7_30
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Feng X, 2018, J INF PROCESS SYST, V14, P1405, DOI 10.3745/JIPS.04.0096
   github, about us
   Goshtasby AA, 2007, INFORM FUSION, V8, P114, DOI 10.1016/j.inffus.2006.04.001
   He KJ, 2018, NEUROCOMPUTING, V320, P157, DOI 10.1016/j.neucom.2018.09.018
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Wei H, 2007, PATTERN RECOGN LETT, V28, P493, DOI 10.1016/j.patrec.2006.09.005
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Li ZC, 2015, IEEE T PATTERN ANAL, V37, P2085, DOI 10.1109/TPAMI.2015.2400461
   Liu J, 2015, IEEE T NEUR NET LEAR, V26, P1233, DOI 10.1109/TNNLS.2014.2335234
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2017, INFORM FUSION, V36, P191, DOI 10.1016/j.inffus.2016.12.001
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Luo XQ, 2018, MULTIMED TOOLS APPL, V77, P22407, DOI 10.1007/s11042-018-5985-6
   Luo XY, 2012, SIGNAL PROCESS, V92, P1268, DOI 10.1016/j.sigpro.2011.11.021
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Maglanoc LA, 2020, HUM BRAIN MAPP, V41, P241, DOI 10.1002/hbm.24802
   med.harvard, AANLIB
   Mei KF, 2020, Arxiv, DOI arXiv:2006.13511
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qu Xiao-Bo, 2008, Acta Automatica Sinica, V34, P1508, DOI 10.3724/SP.J.1004.2008.01508
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Savic S., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P604
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang JH, 2015, IEEE T MULTIMEDIA, V17, P1899, DOI 10.1109/TMM.2015.2476660
   Toet A, 2017, DATA BRIEF, V15, P249, DOI 10.1016/j.dib.2017.09.038
   Wang XH, 2015, J OPTICS-UK, V17, DOI 10.1088/2040-8978/17/5/055702
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wencheng Wang, 2011, Journal of Computers, V6, P2559, DOI 10.4304/jcp.6.12.2559-2566
   Zhang Q, 2018, PATTERN RECOGN, V83, P299, DOI 10.1016/j.patcog.2018.06.003
   Zhang Q, 2016, IEEE T IMAGE PROCESS, V25, P2045, DOI 10.1109/TIP.2016.2524212
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhang Y, 2017, INFRARED PHYS TECHN, V83, P227, DOI 10.1016/j.infrared.2017.05.007
   [赵鹏 ZHAO peng], 2008, [光电子·激光, Journal of Optoelectronics·Laser], V19, P814
   Zhou Z, 2014, INFORM FUSION, V20, P60, DOI 10.1016/j.inffus.2013.11.005
NR 48
TC 19
Z9 21
U1 38
U2 143
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 608
EP 623
DI 10.1109/TMM.2021.3129354
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800021
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Jiao, CQ
   Chang, RH
   Qu, L
   Liu, AA
AF Nie, Weizhi
   Jiao, Chuanqi
   Chang, Rihao
   Qu, Lei
   Liu, An-An
TI CPG3D: Cross-Modal Priors Guided 3D Object Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Shape; Image reconstruction; Feature
   extraction; Solid modeling; Task analysis; Data mining; 3D model
   reconstruction; Multimodal learning; Cross-modal retrieval
ID SHAPE; NETWORK
AB Three-dimensional reconstruction is a multimedia technology widely used in computer-aided modeling and 3D animation. Nevertheless, it is still hard for reconstruction methods to overcome the 3D geometry missing and the object occlusion in the single-view images. In this article, we propose a novel method (CPG3D) for reconstructing high-quality 3D shapes from a single image under the guidance of prior knowledge. Using the single-view image as the query, prior knowledge is collected from public 3D datasets, which can compensate for missing 3D geometries and assist the 3D reconstruction network to high fidelity results. Our method consists of three parts: 1) Cross-modal 3D shape retrieval module: This part retrieves related 3D shapes based on 2D images. Here, we apply the pre-trained model to guarantee the correlation between the retrieved 3D shape and the input image. 2) Multimodal information fusion module: We propose a multimodal attention mechanism to handle the information fusing of 2D visual and 3D structural information; 3) Three-dimensional reconstruction module: We propose a novel encoder-decoder network for 3D shape reconstruction. Specifically, we employ the skip connection operation to link the target image's visual information with the 3D model's structural information to enhance the prediction of 3D details. During training, we employ two carefully designed loss functions to lead the multimodal learning to obtain proper modal features. On the ShapeNet and Pix3D datasets, the final experimental results reveal that our method notably increases reconstruction quality and outperforms SOTA methods.
C1 [Nie, Weizhi; Jiao, Chuanqi; Chang, Rihao; Liu, An-An] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Chang, Rihao] Ocean Univ China, Sch Informat Sci & Engn, Qingdao 266100, Shandong, Peoples R China.
   [Qu, Lei] Hisense Grp Holdings Co Ltd, Qingdao 266000, Peoples R China.
C3 Tianjin University; Ocean University of China
RP Chang, RH (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM weizhinie@tju.edu.cn; chuanqi_097@tju.edu.cn; changrihao@tju.edu.cn;
   qulei1@hisense.com; anan0422@gmail.com
OI Chang, Rihao/0000-0001-5384-0212; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China
FX No Statement Available
CR Aliomonos J., 1985, P 9 INT JOINT C ART, P926
   Atick JJ, 1996, NEURAL COMPUT, V8, P1321, DOI 10.1162/neco.1996.8.6.1321
   Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487
   Bane C, 2017, INT CONF 3D VISION, P412, DOI 10.1109/3DV.2017.00054
   Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Dovgard R, 2004, LECT NOTES COMPUT SC, V3022, P99
   Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264
   Fan X, 2021, IEEE T MULTIMEDIA, V23, P1252, DOI 10.1109/TMM.2020.2994506
   Gao JN, 2023, IEEE T MULTIMEDIA, V25, P5248, DOI 10.1109/TMM.2022.3189247
   Guan L., 2007, P IEEE COMP SOC C CO, P1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Hu PP, 2022, IEEE T MULTIMEDIA, V24, P2139, DOI 10.1109/TMM.2021.3076340
   Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Lee T, 2018, INT CONF 3D VISION, P258, DOI 10.1109/3DV.2018.00038
   Li Kejie, 2020, IEEE COMPUT SOC C CO
   Liu CX, 2021, IEEE T MULTIMEDIA, V23, P2843, DOI 10.1109/TMM.2020.3017924
   Loh A. M., 2005, P BRIT MACH VIS C, P69
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lv CL, 2022, IEEE T MULTIMEDIA, V24, P1815, DOI 10.1109/TMM.2021.3073265
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   MEAGHER D, 1982, COMPUT VISION GRAPH, V19, P129, DOI 10.1016/0146-664X(82)90104-6
   Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459
   Michalkiewicz M, 2019, Arxiv, DOI arXiv:1901.06802
   Miller A., 2011, P 4 WORKSH GEN PURP, P1
   Miller A., 2016, P 2016 C EMP METH NA, P1400, DOI DOI 10.18653/V1/D16-1147
   Mu PP, 2018, FRONT INFORM TECH EL, V19, P1397, DOI 10.1631/FITEE.1601764
   Nie WZ, 2022, IEEE T CIRC SYST VID, V32, P992, DOI 10.1109/TCSVT.2021.3070969
   Pan JY, 2019, IEEE I CONF COMP VIS, P9963, DOI 10.1109/ICCV.2019.01006
   Parodi P, 1996, IEEE T PATTERN ANAL, V18, P211, DOI 10.1109/34.481545
   Pinheiro PO, 2019, IEEE I CONF COMP VIS, P7637, DOI 10.1109/ICCV.2019.00773
   Pontes JK, 2019, LECT NOTES COMPUT SC, V11361, P365, DOI 10.1007/978-3-030-20887-5_23
   Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207
   Riegler G, 2017, INT CONF 3D VISION, P57, DOI 10.1109/3DV.2017.00017
   Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Siddique A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22020518
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun XY, 2018, PROC CVPR IEEE, P2974, DOI 10.1109/CVPR.2018.00314
   Tatarchenko M, 2019, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2019.00352
   Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230
   Tu XG, 2021, IEEE T MULTIMEDIA, V23, P1160, DOI 10.1109/TMM.2020.2993962
   Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30
   Vakalopoulou M, 2018, LECT NOTES COMPUT SC, V11073, P658, DOI 10.1007/978-3-030-00937-3_75
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4
   Wang PS, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275050
   Wang WT, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2400, DOI 10.1145/3394171.3413507
   Wang XY, 2022, IEEE T MULTIMEDIA, V24, P4028, DOI 10.1109/TMM.2021.3111485
   Wen C, 2019, IEEE I CONF COMP VIS, P1042, DOI 10.1109/ICCV.2019.00113
   Weston Jason., 2014, ICLR
   Wu J., 2017, PROC ADVNEURAL INF P, V30, P153
   Wu JJ, 2018, LECT NOTES COMPUT SC, V11215, P673, DOI 10.1007/978-3-030-01252-6_40
   Wu JJ, 2016, ADV NEUR IN, V29
   Chang AX, 2015, Arxiv, DOI arXiv:1512.03012
   Xie HZ, 2019, IEEE I CONF COMP VIS, P2690, DOI 10.1109/ICCV.2019.00278
   Xie HZ, 2020, INT J COMPUT VISION, V128, P2919, DOI 10.1007/s11263-020-01347-6
   Xu Q., 2019, Advances in Neural Information Processing Systems (NeurIPS)
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P3014, DOI 10.1109/TMM.2020.2967645
   Yang B, 2020, INT J COMPUT VISION, V128, P53, DOI 10.1007/s11263-019-01217-w
   Yang S, 2021, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR46437.2021.00317
   Yi Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10727, DOI 10.1109/CVPR42600.2020.01074
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284
   Zhou HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1667, DOI 10.1145/3343031.3351011
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P171, DOI 10.1109/TMM.2014.2384396
   Zhou ZW, 2020, IEEE T MED IMAGING, V39, P1856, DOI 10.1109/TMI.2019.2959609
NR 74
TC 3
Z9 3
U1 14
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9383
EP 9396
DI 10.1109/TMM.2023.3251697
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200015
DA 2024-07-18
ER

PT J
AU Tan, HC
   Yin, BC
   Wei, K
   Liu, XP
   Li, X
AF Tan, Hongchen
   Yin, Baocai
   Wei, Kun
   Liu, Xiuping
   Li, Xin
TI ALR-GAN: Adaptive Layout Refinement for Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Semantics; Visualization; Task analysis; Training; Generators;
   Adaptation models; Generative adversarial network; text-to-image
   synthesis; information consistency constraint; object layout refinement
AB We propose a novel Text-to-Image Generation Network, Adaptive Layout Refinement Generative Adversarial Network (ALR-GAN), to adaptively refine the layout of synthesized images without any auxiliary information. The ALR-GAN includes an Adaptive Layout Refinement (ALR) module and a Layout Visual Refinement (LVR) loss. The ALR module aligns the layout structure (which refers to locations of objects and background) of a synthesized image with that of its corresponding real image. In ALR module, we proposed an Adaptive Layout Refinement (ALR) loss to balance the matching of hard and easy features, for more efficient layout structure matching. Based on the refined layout structure, the LVR loss further refines the visual representation within the layout area. Experimental results on two widely-used datasets show that ALR-GAN performs competitively at the Text-to-Image generation task.
C1 [Tan, Hongchen; Yin, Baocai] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Beijing 100124, Peoples R China.
   [Wei, Kun] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Liu, Xiuping] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Li, Xin] Texas A&M Univ, Sch Performance Visualizat & Fine Arts, Sect Visual Comp & Creat Media, College Stn, TX 77843 USA.
C3 Beijing University of Technology; Xidian University; Dalian University
   of Technology; Texas A&M University System; Texas A&M University College
   Station
RP Yin, BC (corresponding author), Beijing Univ Technol, Beijing Inst Artificial Intelligence, Beijing 100124, Peoples R China.
EM tanhongchenphd@bjut.edu.cn; ybc@bjut.edu.cn; weikunsk@gmail.com;
   xpliu@dlut.edu.cn; xinli@tamu.edu
RI Tan, Hongchen/AGZ-4796-2022; Liu, Jinyu/JYQ-6274-2024; Li,
   Shiyue/KFA-3709-2024; Huang, Weilin/AFN-0574-2022; li,
   fei/JYP-3334-2024; Liu, Chenyu/KBQ-8899-2024
OI Tan, Hongchen/0000-0002-3179-8129; Huang, Weilin/0000-0003-1692-4868;
   Li, Xin/0000-0002-0144-9489; Tan, Hongchen/0000-0001-6915-8736
FU National Science Foundation
FX No Statement Available
CR Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Duc Minh Vo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12373), P290, DOI 10.1007/978-3-030-58604-1_18
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Frolov S., 2020, P 2 WORKSH VIS LANGU, P17
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hensel M, 2017, ADV NEUR IN, V30
   Hinz T., 2019, ICLR
   Hinz T, 2022, IEEE T PATTERN ANAL, V44, P1552, DOI 10.1109/TPAMI.2020.3021209
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Jun Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10908, DOI 10.1109/CVPR42600.2020.01092
   Li B., 2019, Advances in Neural Information Processing, P2063
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li YK, 2019, ADV NEUR IN, V32
   Li YT, 2019, PROC CVPR IEEE, P6322, DOI 10.1109/CVPR.2019.00649
   Lin TY, 2020, IEEE T PATTERN ANAL, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1357, DOI 10.1145/3394171.3413505
   Lu XK, 2018, LECT NOTES COMPUT SC, V11218, P369, DOI 10.1007/978-3-030-01264-9_22
   Miyato T., 2018, P INT C LEARN REPR, P842
   Niu T., 2020, P INT C MULT RETR
   Peng J, 2022, IEEE T MULTIMEDIA, V24, P4356, DOI 10.1109/TMM.2021.3116416
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reed S, 2016, PR MACH LEARN RES, V48
   Ruan SL, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13940, DOI 10.1109/ICCV48922.2021.01370
   Salimans T, 2016, ADV NEUR IN, V29
   Sharma S., 2018, P INT C LEARN REPR W, P1
   Sun W, 2022, IEEE T PATTERN ANAL, V44, P5070, DOI 10.1109/TPAMI.2021.3078577
   Sun W, 2019, IEEE I CONF COMP VIS, P10530, DOI 10.1109/ICCV.2019.01063
   Sylvain T, 2021, AAAI CONF ARTIF INTE, V35, P2647
   Tan HC, 2023, IEEE T NEUR NET LEAR, V34, P10309, DOI 10.1109/TNNLS.2022.3165573
   Tan HC, 2022, IEEE T MULTIMEDIA, V24, P832, DOI 10.1109/TMM.2021.3060291
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Tan HC, 2019, IEEE I CONF COMP VIS, P10500, DOI 10.1109/ICCV.2019.01060
   Tao M., 2022, P IEEE CVF C COMP VI, P16494
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2021, PROC CVPR IEEE, P833, DOI 10.1109/CVPR46437.2021.00089
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang LS, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1302, DOI 10.1145/3394171.3414017
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhao B, 2019, PROC CVPR IEEE, P8576, DOI 10.1109/CVPR.2019.00878
   Zheng ZD, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3383184
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 51
TC 7
Z9 7
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8620
EP 8631
DI 10.1109/TMM.2023.3238554
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU van Rensburg, BJ
   Puech, W
   Pedeboy, JP
AF van Rensburg, Bianca Jansen
   Puech, William
   Pedeboy, Jean-Pierre
TI A Format Compliant Encryption Method for 3D Objects Allowing
   Hierarchical Decryption
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Encryption; Security; Visualization; Videos;
   Shape; Streaming media; Multimedia security; 3D object; 3D hierarchical
   decryption; 3D selective encryption; visual security level
AB With the increasing popularity of 3D objects in industry and everyday life, 3D object security has become essential. While there exists methods for 3D selective encryption, where a clear 3D object is encrypted so that the result has the desired level of visual security, to our knowledge, no method exists for decrypting encrypted 3D objects hierarchically. In this paper, we are the first to propose propose a method which allows us to hierarchically decrypt an encrypted 3D object according to a generated ring of keys. This ring consists of a set of keys that allow a stronger or weaker decryption of the encrypted 3D object. Each hierarchically decrypted 3D object has a different visual security level, where the 3D object is more or less visually accessible. Based on a master key, these hierarchical keys are generated using our method during the encryption process. Our method is essential when it comes to preventing trade secrets from being leaked from within a company or by exterior attackers. It is also ecologically friendly and more secure than traditional selective encryption methods.
C1 [van Rensburg, Bianca Jansen; Puech, William] Univ Montpellier, Ctr Natl Rech Sci, Lab Informat Robot & Microelect Montpellier, F-34095 Montpellier, France.
   [Pedeboy, Jean-Pierre] Strategies, F-94510 Rungis, France.
C3 Universite de Montpellier; Centre National de la Recherche Scientifique
   (CNRS)
RP Puech, W (corresponding author), Univ Montpellier, Ctr Natl Rech Sci, Lab Informat Robot & Microelect Montpellier, F-34095 Montpellier, France.
EM bianca.jansen-van-rensburg@lirmm.fr; william.puech@lirmm.fr;
   jp.pedeboy@cadwin.com
OI Jansen van Rensburg, Bianca/0000-0002-8683-1360
CR Abraham AS, 2017, 2017 INTERNATIONAL CONFERENCE ON NETWORKS & ADVANCES IN COMPUTATIONAL TECHNOLOGIES (NETACT), P387, DOI 10.1109/NETACT.2017.8076801
   Abu-Zahra A, 2012, PROCEDIA COMPUT SCI, V10, P240, DOI 10.1016/j.procs.2012.06.033
   Ahmed Fawad, 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P232, DOI 10.1109/PSIVT.2010.46
   Beugnon S, 2022, SIGNAL PROCESS-IMAGE, V108, DOI 10.1016/j.image.2022.116832
   Beugnon S, 2018, IEEE INT CONF MULTI
   Bright P. Jeya, 2019, 2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI). Proceedings, P934, DOI 10.1109/ICOEI.2019.8862525
   Cho MS, 2006, AXMEDIS 2006: SECOND INTERNATIONAL CONFERENCE ON AUTOMATED PRODUCTION OF CROSS MEDIA CONTENT FOR MULTI-CHANNEL DISTRIBUTION, PROCEEDINGS, P121
   Daemen J, 2020, The design of rijndael: the advanced encryption standard (AES), V2nd, DOI DOI 10.1007/978-3-662-60769-53
   Dworkin M.J, 2015, Federal Information Processing Standards, DOI [DOI 10.6028/NIST.FIPS.202, 10.6028/NIST.FIPS.202]
   Éluard M, 2014, IEEE IMAGE PROC, P4787, DOI 10.1109/ICIP.2014.7025970
   Gschwandtner M, 2009, LECT NOTES COMPUT SC, V5876, P35, DOI 10.1007/978-3-642-10520-3_4
   Guo SW, 2020, IEEE T INF FOREN SEC, V15, P1151, DOI 10.1109/TIFS.2019.2935415
   Jenisch S., 2014, IFIP INT C ARTIF INT, P624
   Levoy M, 2005, The Stanford 3D scanning repository
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma CS, 2019, IEEE T MULTIMEDIA, V21, P173, DOI 10.1109/TMM.2018.2851446
   Pommer A., 2002, PROC MULTIMEDIA SECU, P71
   Said A, 2005, IEEE IMAGE PROC, P2401
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sorkine O., 2005, Eurographics 2005-State of the Art Reports, V4, P53, DOI [10.2312/egst.20051044, DOI 10.2312/EGST.20051044]
   Xiang T, 2020, IEEE T CIRC SYST VID, V30, P4129, DOI 10.1109/TCSVT.2019.2955298
   Xiang T, 2016, IEEE T INF FOREN SEC, V11, P951, DOI 10.1109/TIFS.2016.2515503
   Yang K, 2016, IEEE T MULTIMEDIA, V18, P940, DOI 10.1109/TMM.2016.2535728
   Yao Y, 2009, INFORM-J COMPUT INFO, V33, P69
   Zhang XP, 2012, IEEE T IMAGE PROCESS, V21, P3108, DOI 10.1109/TIP.2012.2187671
NR 25
TC 2
Z9 2
U1 3
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7196
EP 7207
DI 10.1109/TMM.2022.3219616
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000035
DA 2024-07-18
ER

PT J
AU Wang, YX
   Liu, M
   Wei, YW
   Cheng, ZY
   Wang, YL
   Nie, LQ
AF Wang, Yunxiao
   Liu, Meng
   Wei, Yinwei
   Cheng, Zhiyong
   Wang, Yinglong
   Nie, Liqiang
TI Siamese Alignment Network for Weakly Supervised Video Moment Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Location awareness; Annotations; Visualization; Task
   analysis; Proposals; Neural networks; Multiple instance learning;
   siamese alignment network; vision-language alignment; weakly-supervised
   video moment retrieval
AB Video moment retrieval, i.e., localizing the specific video moments within a video given a description query, has attracted substantial attention over the past several years. Although great progress has been achieved thus far, most of existing methods are supervised, which require moment-level temporal annotation information. In contrast, weakly-supervised methods which only need video-level annotations remain largely unexplored. In this paper, we propose a novel end-to-end Siamese alignment network for weakly-supervised video moment retrieval. To be specific, we design a multi-scale Siamese module, which could progressively reduce the semantic gap between the visual and textual modality with the Siamese structure. In addition, we present a context-aware multiple instance learning module by considering the influence of adjacent contexts, enhancing the moment-query and video-query alignment simultaneously. By promoting the matching of both moment-level and video-level, our model can effectively improve the retrieval performance, even if only having weak video level annotations. Extensive experiments on two benchmark datasets, i.e., ActivityNet-Captions and Charades-STA, verify the superiority of our model compared with several state-of-the-art baselines.
C1 [Wang, Yunxiao; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.
   [Liu, Meng] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
   [Wei, Yinwei] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Cheng, Zhiyong; Wang, Yinglong] Qilu Univ Technol, Shandong Acad Sci, Shandong Artificial Intelligence Inst, Jinan 250316, Peoples R China.
C3 Shandong University; Shandong Jianzhu University; National University of
   Singapore; Qilu University of Technology
RP Nie, LQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Peoples R China.; Liu, M (corresponding author), Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
EM yunxiao.wang@mail.sdu.edu.cn; mengliu.sdu@gmail.com;
   weiyinwei@hotmail.com; jason.zy.cheng@gmail.com; wangyl@sdas.org;
   nieliqiang@gmail.com
RI Wei, Yinwei/JHX-9398-2023
OI Wei, Yinwei/0000-0003-1791-3159; Liu, Meng/0000-0002-1582-5764; Wang,
   Yunxiao/0000-0003-1897-6557
FU National Natural Science Foundation of China [62006142, 61872270,
   U1936203]; Shandong Provincial Key Research and Development Program
   [2019JZZY010118]; Shandong Provincial Natural Science Foundation for
   Distinguished Young Scholars [ZR2019JQ23, ZR2021JQ26]; Major Basic
   Research Project of Natural Science Foundation of Shandong Province
   [ZR2021ZD15]; Young creative team in universities of Shandong Province
   [2020KJN012]; Science and Technology Innovation Program for
   Distinguished Young Scholars of Shandong Province Higher Education
   Institutions [2021KJ036]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62006142, 61872270, and U1936203, in
   part by the Shandong Provincial Key Research and Development Program
   under Grant 2019JZZY010118, in part by the Shandong Provincial Natural
   Science Foundation for Distinguished Young Scholars under Grants
   ZR2019JQ23 and ZR2021JQ26, in part by the Major Basic Research Project
   of Natural Science Foundation of Shandong Province under Grant
   ZR2021ZD15, in part by the Young creative team in universities of
   Shandong Province under Grant 2020KJN012, and in part by the Science and
   Technology Innovation Program for Distinguished Young Scholars of
   Shandong Province Higher Education Institutions under Grant 2021KJ036.
CR Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P898, DOI 10.1145/3394171.3413841
   Cao D, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4162, DOI 10.1145/3394171.3413840
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen JY, 2019, AAAI CONF ARTIF INTE, P8175
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen Zhiqian, 2020, CoRR
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Gao MF, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P1481
   He DL, 2019, AAAI CONF ARTIF INTE, P8393
   Ilse M, 2018, PR MACH LEARN RES, V80
   Keeler J. D., 1991, Advances in neural information processing systems, P557
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu Y, 2017, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2017.499
   LOSHCHILOV, 2017, P INT C LEARN REPRES
   Mithun NC, 2019, PROC CVPR IEEE, P11584, DOI 10.1109/CVPR.2019.01186
   Mueller J, 2016, AAAI CONF ARTIF INTE, P2786
   OPAZO CR, 2020, P WINTER C APPL COMP, P2453
   Peng B, 2021, NEUROCOMPUTING, V456, P519, DOI 10.1016/j.neucom.2020.05.123
   Peng B, 2020, IEEE T CIRC SYST VID, V30, P131, DOI 10.1109/TCSVT.2018.2889514
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Ryoo MS, 2018, AAAI CONF ARTIF INTE, P7315
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Singh Krishna Kumar, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P3544, DOI 10.1109/ICCV.2017.381
   SONG Y, 2020, CORR
   Tao R, 2016, PROC CVPR IEEE, P1420, DOI 10.1109/CVPR.2016.158
   Varior RR, 2016, LECT NOTES COMPUT SC, V9912, P791, DOI 10.1007/978-3-319-46484-8_48
   Wang H, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4116, DOI 10.1145/3394171.3413975
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang XG, 2018, PATTERN RECOGN, V74, P15, DOI 10.1016/j.patcog.2017.08.026
   WANG Y, IEEE T MULTIMEDIA
   Wu J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1283, DOI 10.1145/3394171.3413862
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Yan Y., 2018, PMLR, P662
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4098, DOI 10.1145/3394171.3413967
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhong JX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P35, DOI 10.1145/3240508.3240511
NR 51
TC 9
Z9 9
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3921
EP 3933
DI 10.1109/TMM.2022.3168424
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500029
DA 2024-07-18
ER

PT J
AU Wang, ZS
   Shao, WY
   Chen, YL
   Xu, JW
   Zhang, XQ
AF Wang, Zhishe
   Shao, Wenyu
   Chen, Yanlin
   Xu, Jiawei
   Zhang, Xiaoqin
TI Infrared and Visible Image Fusion via Interactive Compensatory Attention
   Adversarial Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; attention interaction; attention compensation; dual
   discriminators; adversarial learning
ID NETWORK; NEST
AB The existing generative adversarial fusion methods generally concatenate source images or deep features, and extract local features through convolutional operations without considering their global characteristics, which tends to produce a limited fusion performance. Toward this end, we propose a novel interactive compensatory attention fusion network, termed ICAFusion. In particular, in the generator, we construct a multi-level encoder-decoder network with a triple path, and design infrared and visible paths to provide additional intensity and gradient information for the concatenating path. Moreover, we develop the interactive and compensatory attention modules to communicate their pathwise information, and model their long-range dependencies through a cascading channel-spatial model. The generated attention maps can more focus on infrared target perception and visible detail characterization, and are used to reconstruct the fusion image. Therefore, the generator takes full advantage of local and global features to further increase the representation ability of feature extraction and feature reconstruction. Extensive experiments illustrate that our ICAFusion obtains superior fusion performance and better generalization ability, which precedes other advanced methods in the subjective visual description and objective metric evaluation.
C1 [Wang, Zhishe; Shao, Wenyu; Chen, Yanlin] Taiyuan Univ Sci & Technol, Sch Appl Sci, Taiyuan 030024, Peoples R China.
   [Xu, Jiawei; Zhang, Xiaoqin] Wenzhou Univ, Key Lab Intelligent Informat Safety & Emergency Z, Wenzhou 325035, Peoples R China.
C3 Taiyuan University of Science & Technology; Wenzhou University
RP Xu, JW; Zhang, XQ (corresponding author), Wenzhou Univ, Key Lab Intelligent Informat Safety & Emergency Z, Wenzhou 325035, Peoples R China.
EM wangzs@tyust.edu.cn; wyshaotyust@163.com; chentyust@163.com;
   jxulincoln@gmail.com; zhangxiaoqinnan@gmail.com
OI Shao, Wenyu/0000-0003-3934-1323; Wang, Zhishe/0000-0002-0763-5901
FU Fundamental Research Program of Shanxi Province [201901D111260]; Open
   Foundation of Shanxi Key Laboratory of Signal Capturing Processing
   [ISPT2020-4]; Outstanding Graduate Student Innovation Program of Taiyuan
   University of Science and Technology [SY2022079]; National Natural
   Science Foundation of China [61922064, U2033210]; Zhejiang Provincial
   Natural Science Foundation [LY23F030001, LDT23F02024F02]
FX This work was supported in part by Fundamental Research Program of
   Shanxi Province under Grant 201901D111260, in part by the Open
   Foundation of Shanxi Key Laboratory of Signal Capturing & Processing
   under Grant ISPT2020-4, in part by the Outstanding Graduate Student
   Innovation Program of Taiyuan University of Science and Technology under
   Grant SY2022079, in part by the National Natural Science Foundation of
   China under Grants 61922064 and U2033210, and in part by the Zhejiang
   Provincial Natural Science Foundation under Grants LY23F030001 and
   LDT23F02024F02.
CR Ariffin S., 2016, OTCBVS database
   Eskicioglu AM, 1995, IEEE T COMMUN, V43, P2959, DOI 10.1109/26.477498
   Feng ZX, 2020, IEEE T IMAGE PROCESS, V29, P579, DOI 10.1109/TIP.2019.2928126
   Fu Y, 2021, INFORM FUSION, V72, P110, DOI 10.1016/j.inffus.2021.02.019
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   Hu P, 2019, INFRARED PHYS TECHN, V102, DOI 10.1016/j.infrared.2019.102977
   Jian LH, 2021, IEEE T MULTIMEDIA, V24, P3314, DOI 10.1109/TMM.2021.3096088
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   Kong WW, 2014, INFRARED PHYS TECHN, V67, P161, DOI 10.1016/j.infrared.2014.07.019
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2020, IEEE T INSTRUM MEAS, V69, P9645, DOI 10.1109/TIM.2020.3005230
   Li H, 2020, IEEE T IMAGE PROCESS, V29, P4733, DOI 10.1109/TIP.2020.2975984
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3029360
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li J, 2020, INFORM SCIENCES, V529, P28, DOI 10.1016/j.ins.2020.04.035
   Li Z, 2021, IEEE T MULTIMEDIA, V23, P306, DOI 10.1109/TMM.2020.2978640
   Liu Z, 2012, IEEE T PATTERN ANAL, V34, P94, DOI 10.1109/TPAMI.2011.109
   Long YZ, 2021, INFORM FUSION, V69, P128, DOI 10.1016/j.inffus.2020.11.009
   Luo XQ, 2023, IEEE T MULTIMEDIA, V25, P608, DOI 10.1109/TMM.2021.3129354
   Ma JY, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3038013
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2020, INFORM FUSION, V54, P85, DOI 10.1016/j.inffus.2019.07.005
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Nie RC, 2022, IEEE T MULTIMEDIA, V24, P1460, DOI 10.1109/TMM.2021.3065496
   Piella G, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P173
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Song AY, 2022, NEUROCOMPUTING, V483, P183, DOI 10.1016/j.neucom.2022.02.025
   Toet Alexander, 2014, Figshare
   Wang ZS, 2015, OPTIK, V126, P4184, DOI 10.1016/j.ijleo.2015.08.118
   Wang ZS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3191664
   Wang ZS, 2022, IEEE T CIRC SYST VID, V32, P3360, DOI 10.1109/TCSVT.2021.3109895
   Wang ZS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3139654
   Wang ZS, 2020, OPTIK, V201, DOI 10.1016/j.ijleo.2019.163497
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu H, 2020, Roadscene database
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu Q, 2022, IEEE T MULTIMEDIA, V24, P567, DOI 10.1109/TMM.2021.3055362
   Yang Y, 2021, IEEE T CIRC SYST VID, V31, P4771, DOI 10.1109/TCSVT.2021.3054584
   Zhang H, 2021, INFORM FUSION, V76, P323, DOI 10.1016/j.inffus.2021.06.008
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang XQ, 2022, IEEE T CIRC SYST VID, V32, P510, DOI 10.1109/TCSVT.2021.3067062
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhao F, 2021, IEEE T MULTIMEDIA, V23, P2745, DOI 10.1109/TMM.2020.3016123
   Zhou HB, 2023, IEEE T MULTIMEDIA, V25, P635, DOI 10.1109/TMM.2021.3129609
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
NR 49
TC 7
Z9 7
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7800
EP 7813
DI 10.1109/TMM.2022.3228685
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, YZ
   Wei, RK
   Song, JK
   Liu, Y
   Wang, YT
   Zhou, K
AF Xie, Yanzhao
   Wei, Rukai
   Song, Jingkuan
   Liu, Yu
   Wang, Yangtao
   Zhou, Ke
TI Label-Affinity Self-Adaptive Central Similarity Hashing for Image
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Codes; Semantics; Feature extraction; Generators; Visualization;
   Training; Quantization (signal); Image retrieval; label semantic
   information; hash centers; label-affinity coefficient; hash centroid
ID ITERATIVE QUANTIZATION; PROCRUSTEAN APPROACH
AB Due to the usage of global similarity, the hashing methods based on predefined hash centers have achieved more accurate retrieval results than the pairwise/triplet-based methods. Nevertheless, the fixed hash centers lack the perception of data distribution and are limited by the pre-determined Hadamard matrix, which consider neither the label semantic information nor the object scale size, resulting in sub-optimal retrieval performance and weak generalization ability. In this article, we (1) adopt the label semantic information to generate self-adaptive hash centers and (2) propose the label-affinity coefficient (lac) that considers the scale size of each label/object appearing in the given image to calculate the real hash centroid for this image. Based on this, we propose Label-affinity Self-adaptive Central Similarity Hashing (LSCSH) for image retrieval. LSCSH consists of a hash code generator module and a hash center adapter module. First, we obtain the label word vector (i.e., the word vector representation of each class label) via the Word2Vector technique to generate and update the hash centers that adapt to the distribution of both label word vectors and generated hash codes. Second, we learn lac to indicate the dominance of different labels corresponding to objects in each given image, which considers the unequal scales of each object (corresponding to a label) to calculate a more accurate hash centroid for each image. Last but not least, we design an asynchronous learning mechanism to enable each hash code and its corresponding hash centroid to adapt to each other dynamically. We conduct extensive experiments on 5 image datasets including CIFAR-10, ImageNet, VOC2012, MS-COCO and NUS-WIDE. The experimental results demonstrate that LSCSH can achieve the state-of-the-art visual retrieval performance on both single-label and multi-label image datasets.
C1 [Xie, Yanzhao; Wei, Rukai; Zhou, Ke] Huazhong Univ Sci & Technol, Wuhan Natl Lab Optoelect, Wuhan 430074, Peoples R China.
   [Liu, Yu] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Song, Jingkuan] Univ Elect Sci & Technol China, Chengdu, Peoples R China.
   [Wang, Yangtao] Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 511442, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; University of Electronic Science & Technology of
   China; Guangzhou University
RP Liu, Y (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.; Wang, YT (corresponding author), Guangzhou Univ, Sch Comp Sci & Cyber Engn, Guangzhou 511442, Peoples R China.
EM yzxie@hust.edu.cn; weirukai@hust.edu.cn; jingkuan.song@gmail.com;
   liu_yu@hust.edu.cn; ytaowang@gzhu.edu.cn; zhke@hust.edu.cn
RI XIE, Yan-zhao/B-9619-2011
OI Liu, Yu/0000-0002-1964-9278; Wang, Yangtao/0000-0003-4605-9270; song,
   jingkuan/0000-0002-2549-8322; Wei, Rukai/0000-0003-1164-6360;
   /0000-0002-9274-2807
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2009, NIPS
   Cao Y, 2021, IEEE T MULTIMEDIA, V23, P3907, DOI 10.1109/TMM.2020.3033118
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Chen Y, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1328, DOI 10.1145/3404835.3462888
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dubey S. R., 2022, PROC IEEE INT C MULT, P1
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Joulin A, 2016, Arxiv, DOI [arXiv:1612.03651, DOI 10.48550/ARXIV.1612.03651, 10.48550/arXiv.1612.03651]
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Lai HJ, 2015, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2015.7298947
   Li SY, 2020, IEEE T MULTIMEDIA, V22, P1542, DOI 10.1109/TMM.2019.2946096
   Li WJ, 2016, IJCAI, P1711
   Lin QB, 2021, IEEE T MULTIMEDIA, V23, P550, DOI 10.1109/TMM.2020.2984081
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Ma C, 2020, IEEE T MULTIMEDIA, V22, P760, DOI 10.1109/TMM.2019.2931808
   Mandal D, 2020, IEEE T MULTIMEDIA, V22, P2345, DOI 10.1109/TMM.2019.2954741
   Mikolov T, 2013, Arxiv, DOI [arXiv:1301.3781, 10.48550/arXiv.1301.3781]
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Do TT, 2016, LECT NOTES COMPUT SC, V9909, P219, DOI 10.1007/978-3-319-46454-1_14
   Tu RC, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2869, DOI 10.1145/3442381.3449825
   Wang W., 2013, arXiv
   Wang Z, 2021, P IEEECVF INT C COMP, P10336
   Wu YL, 2020, IEEE T MULTIMEDIA, V22, P1310, DOI 10.1109/TMM.2019.2942494
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xie YZ, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P955
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zhai DM, 2018, PATTERN RECOGN, V75, P250, DOI 10.1016/j.patcog.2017.06.018
   Zhang HF, 2021, IEEE T MULTIMEDIA, V23, P3400, DOI 10.1109/TMM.2020.3025000
   Zhang PF, 2022, IEEE T MULTIMEDIA, V24, P466, DOI 10.1109/TMM.2021.3053766
   Zhou X, 2020, IEEE T CYBERNETICS, V50, P1460, DOI 10.1109/TCYB.2018.2883970
   Zhu H, 2016, AAAI CONF ARTIF INTE, P2415
NR 48
TC 4
Z9 4
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9161
EP 9174
DI 10.1109/TMM.2023.3248170
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200017
DA 2024-07-18
ER

PT J
AU Xiong, J
   Gao, H
   Wang, MH
   Li, HL
   Ngan, KN
   Lin, WS
AF Xiong, Jian
   Gao, Hao
   Wang, Miaohui
   Li, Hongliang
   Ngan, King Ngi
   Lin, Weisi
TI Efficient Geometry Surface Coding in V-PCC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Geometry compression; HEVC; occupancy map; point cloud; rate distortion
   optimization; V-PCC
ID POINT; COMPRESSION; MODEL; MPEG
AB In recent video-based point cloud compression (V-PCC), 3D point clouds are projected onto 2D images and compressed by High-Efficiency Video Coding (HEVC). However, HEVC was originally designed for natural visual signals, which is a suboptimal framework for point clouds. Therefore, there are still problems in geometry information compression in V-PCC: (1) The distortion based on the sum of squared error (SSE) in the existing rate-distortion optimization (RDO) is inconsistent with the geometric quality measurement; (2) The existing prediction cannot explore the fixed relationship between the corresponding far layer and near layer depth, which means that the far layer depth can be always not less than the corresponding near layer depth. In this paper, we present an efficient geometry surface coding (EGSC) method for V-PCC to address the problems. Firstly, an error projection (EP) model is designed to establish the relationship between the SSE-based distortion and the geometry quality metric. Secondly, an EP-based RDO is employed to improve the geometry information compression by estimating the point normals with gradients. Finally, an occupancy-map driven scheme is proposed to improve the prediction accuracy of merge modes. Experimental results show that the proposed method achieves an average of over 10% bit-rate saving compared with the V-PCC reference software.
C1 [Xiong, Jian] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing 210003, Peoples R China.
   [Gao, Hao] Nanjing Univ Posts & Telecommun, Coll Artificial Intelligence, Nanjing 210003, Peoples R China.
   [Wang, Miaohui] Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
   [Li, Hongliang; Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Shenzhen University; University of
   Electronic Science & Technology of China; Nanyang Technological
   University
RP Gao, H (corresponding author), Nanjing Univ Posts & Telecommun, Coll Artificial Intelligence, Nanjing 210003, Peoples R China.; Wang, MH (corresponding author), Shenzhen Univ, Guangdong Key Lab Intelligent Informat Proc, Shenzhen 518060, Peoples R China.
EM jxiong@njupt.edu.cn; tsgaohao@gmail.com; wang.miaohui@gmail.com;
   hlli@uestc.edu.cn; knngan@ee.cuhk.edu.hk; wslin@ntu.edu.sg
RI Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Xiong, Jian/0000-0002-4720-4102; Lin, Weisi/0000-0001-9866-1947
FU National Natural Science Foundation of China [61701258, 61931012,
   61906098]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61701258, 61931012, and 61906098.
CR Akhtar A, 2022, IEEE T MULTIMEDIA, V24, P2866, DOI 10.1109/TMM.2021.3090148
   [Anonymous], 2021, Point Cloud Compression Category 2 Reference SoftwareTMC2-15.0
   [Anonymous], 2021, HIGH EFFICIENCY VIDE
   Bjontegaard G., 2001, CALCULATION AVERAGE
   Budagavi E., 2017, JTC1SC29WG11M41808 I
   Chancel M, 2020, ATTEN PERCEPT PSYCHO, V82, P4058, DOI 10.3758/s13414-020-02107-x
   Cheng WT, 2019, IEEE T IMAGE PROCESS, V28, P4857, DOI 10.1109/TIP.2019.2910662
   Cheng WT, 2017, IEEE T IMAGE PROCESS, V26, DOI 10.1109/TIP.2016.2623488
   de Queiroz RL, 2017, IEEE T IMAGE PROCESS, V26, P3886, DOI 10.1109/TIP.2017.2707807
   Dorea C, 2019, IEEE IMAGE PROC, P3721, DOI [10.1109/icip.2019.8803690, 10.1109/ICIP.2019.8803690]
   Fuchs H, 2014, COMPUTER, V47, P46, DOI 10.1109/MC.2014.185
   Garcia-Hernando G, 2018, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2018.00050
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   He LY, 2017, ASIA-PAC CONF COMMUN, P345
   Jang ES, 2019, IEEE SIGNAL PROC MAG, V36, P118, DOI 10.1109/MSP.2019.2900721
   Jia W, 2022, IEEE T MULTIMEDIA, V24, P2352, DOI 10.1109/TMM.2021.3079698
   Lasserre S., 2017, JTC1SC29WG11M41822 I
   Li CK, 2022, IEEE T COGN DEV SYST, V14, P1594, DOI 10.1109/TCDS.2021.3126637
   Li L, 2021, IEEE T MULTIMEDIA, V23, P2806, DOI 10.1109/TMM.2020.3016894
   Li L, 2021, IEEE T CIRC SYST VID, V31, P326, DOI 10.1109/TCSVT.2020.2966118
   Li L, 2020, IEEE T IMAGE PROCESS, V29, P289, DOI 10.1109/TIP.2019.2931621
   Li Z., 2016, JTC1SC29WG11N16332 I
   Liu H, 2022, IEEE T CIRC SYST VID, V32, P1564, DOI 10.1109/TCSVT.2021.3069838
   Liu JQ, 2019, IEEE INT CON MULTI, P904, DOI 10.1109/ICME.2019.00160
   Liu Q, 2021, IEEE T CIRC SYST VID, V31, P4645, DOI 10.1109/TCSVT.2021.3100282
   Liu Q, 2021, IEEE T MULTIMEDIA, V23, P3278, DOI 10.1109/TMM.2020.3023294
   Liu Q, 2021, IEEE T IMAGE PROCESS, V30, P6623, DOI 10.1109/TIP.2021.3096060
   Mammou K., 2017, JTC1SC29WG11M46149 I
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Robertson MA, 2005, IEEE T CIRC SYST VID, V15, P27, DOI 10.1109/TCSVT.2004.839995
   Schwarz S., 2017, JTC1SC29WG11M41779 I
   Schwarz S., 2019, Document ISO/IEC JTC1/SC29/WG11 N18474
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thanou D, 2016, IEEE T IMAGE PROCESS, V25, P1765, DOI 10.1109/TIP.2016.2529506
   Tian D, 2017, IEEE IMAGE PROC, P3460, DOI 10.1109/ICIP.2017.8296925
   Tulvan C., 2016, JTC1SC29WG11N16331 I
   Wang MH, 2022, IEEE T IND INFORM, V18, P6865, DOI 10.1109/TII.2021.3139895
   Wu XJ, 2021, IEEE T CIRC SYST VID, V31, P4630, DOI 10.1109/TCSVT.2021.3101484
   Xiong J, 2022, IEEE T CIRC SYST VID, V32, P813, DOI 10.1109/TCSVT.2021.3063501
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Xu XZ, 2016, IEEE J EM SEL TOP C, V6, P409, DOI 10.1109/JETCAS.2016.2597645
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Xu ZY, 2021, IEEE T MULTIMEDIA, V23, P1542, DOI 10.1109/TMM.2020.3001540
   Yang Q, 2021, IEEE T MULTIMEDIA, V23, P3877, DOI 10.1109/TMM.2020.3033117
   Zakharchenko V., 2019, Document ISO/IEC JTC1/SC29/WG11 N18487
   Zhang PP, 2021, IEEE T CIRC SYST VID, V31, P4673, DOI 10.1109/TCSVT.2021.3100134
   Zhu H, 2017, IEEE T CIRC SYST VID, V27, P760, DOI 10.1109/TCSVT.2016.2596118
   Zhu WJ, 2021, IEEE T CIRC SYST VID, V31, P765, DOI 10.1109/TCSVT.2020.2985911
NR 50
TC 8
Z9 8
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3329
EP 3342
DI 10.1109/TMM.2022.3158809
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200029
DA 2024-07-18
ER

PT J
AU Xiong, JJ
   Po, LM
   Yu, WY
   Zhao, YZ
   Cheung, KW
AF Xiong, Jingjing
   Po, Lai-Man
   Yu, Wing-Yin
   Zhao, Yuzhi
   Cheung, Kwok-Wai
TI Distortion Map-Guided Feature Rectification for Efficient Video Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Optical distortion; Semantics; Distortion; Feature extraction; Image
   analysis; Image segmentation; Optical imaging; Video semantic
   segmentation; feature warping and correction; deep neural networks;
   optical flow
ID OBJECT SEGMENTATION; RECOGNITION
AB To leverage the strong cross-frame relations of videos, many video semantic segmentation methods tend to explore feature reuse and feature warping based on motion clues. However, since the video dynamics are too complex to model accurately, some warped feature values may be invalid. Moreover, the warping errors can accumulate across frames, thereby resulting in degraded segmentation performance. To tackle this problem, we present an efficient distortion map-guided feature rectification method for video semantic segmentation, specifically targeting the feature updating and correction on the distorted regions with unreliable optical flow. The updated features for the distorted regions are extracted from a light correction network (CoNet). A distortion map serves as the weighted attention to guide the feature rectification by aggregating the warped features and the updated features. The generation of the distortion map is simple yet effective in predicting the distorted areas in the warped features, i.e., moving boundaries, thin objects, and occlusions. In addition, we propose an auxiliary edge-semantics loss to implement the distorted region supervision with classes. Our network is trained in an end-to-end manner and highly modular. Comprehensive experiments on Cityscapes and CamVid datasets demonstrate that the proposed method has achieved state-of-the-art performance by weighing accuracy, inference speed, and temporal consistency on video semantic segmentation.
C1 [Xiong, Jingjing; Po, Lai-Man; Yu, Wing-Yin; Zhao, Yuzhi] City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
   [Po, Lai-Man] Hang Seng Univ Hong Kong, Sch Commun, Shatin, Hong Kong, Peoples R China.
C3 City University of Hong Kong; Hang Seng University of Hong Kong
RP Xiong, JJ (corresponding author), City Univ Hong Kong, Dept Elect Engn, Kowloon, Hong Kong, Peoples R China.
EM jingxiong9-c@my.cityu.edu.hk; eelmpo@cityu.edu.hk;
   wingyinyu8-c@my.cityu.edu.hk; yzzhao2-c@my.cityu.edu.hk;
   keithcheung@hsu.edu.hk
OI YU, Wing Yin/0000-0002-9559-1055; XIONG, Jingjing/0000-0002-5288-3605
CR [Anonymous], 2017, P IEEE 20 INT C INT
   Awan Mehwish, 2020, 2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC), P129, DOI 10.1109/ICAIIC48513.2020.9065018
   Awan Mehwish, 2020, PROC IEEE INT C CONS, P1
   Badrinarayanan V, 2010, PROC CVPR IEEE, P3265, DOI 10.1109/CVPR.2010.5540054
   Borse S., 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, P5901
   Brostow GJ, 2008, LECT NOTES COMPUT SC, V5302, P44, DOI 10.1007/978-3-540-88682-2_5
   Chandra S, 2018, PROC CVPR IEEE, P8915, DOI 10.1109/CVPR.2018.00929
   Cheng JC, 2021, IEEE T MULTIMEDIA, V23, P3112, DOI 10.1109/TMM.2020.3020698
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding MY, 2020, AAAI CONF ARTIF INTE, V34, P10713
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Feng J., 2020, How to train your dragon: Tamed warping network for semantic video segmentation
   Feng JY, 2022, IEEE T PATTERN ANAL, V44, P1591, DOI 10.1109/TPAMI.2020.3024646
   Gadde R, 2017, IEEE I CONF COMP VIS, P4463, DOI 10.1109/ICCV.2017.477
   Guarino G, 2020, IEEE INT CONF ROBOT, P8545, DOI [10.1109/icra40945.2020.9197204, 10.1109/ICRA40945.2020.9197204]
   Hur J, 2019, PROC CVPR IEEE, P5747, DOI 10.1109/CVPR.2019.00590
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain S., 2018, P 15 EUR C COMP VIS, P3
   Jain S, 2019, PROC CVPR IEEE, P8858, DOI 10.1109/CVPR.2019.00907
   Kingma D. P., 2014, arXiv
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lee SH, 2022, INT J PR ENG MAN-GT, V9, P409, DOI [10.1007/s40684-021-00342-7, 10.1109/ICME51207.2021.9428381]
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li NJ, 2021, IEEE T MULTIMEDIA, V23, P203, DOI 10.1109/TMM.2020.2984093
   Li YL, 2018, PROC CVPR IEEE, P5997, DOI 10.1109/CVPR.2018.00628
   Liu YF, 2019, PROC CVPR IEEE, P2599, DOI 10.1109/CVPR.2019.00271
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   Loshchilov I., 2017, P INT C LEARN REPR
   Lu HC, 2021, INT C PATT RECOG, P5354, DOI 10.1109/ICPR48806.2021.9412821
   Moniruzzaman M., IEEE T MULTIMEDIA, DOI [10.1109/TMM.2021.3058050, DOI 10.1109/TMM.2021.3058050]
   Mustikovela SK, 2016, LECT NOTES COMPUT SC, V9915, P804, DOI 10.1007/978-3-319-49409-8_66
   Nilsson D, 2018, PROC CVPR IEEE, P6819, DOI 10.1109/CVPR.2018.00713
   Orsic M, 2019, PROC CVPR IEEE, P12599, DOI 10.1109/CVPR.2019.01289
   Paul M., 2021, ARXIV210101715
   Paul M, 2020, IEEE WINT CONF APPL, P2862, DOI [10.1109/WACV45572.2020.9093520, 10.1109/wacv45572.2020.9093520]
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Ping Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8815, DOI 10.1109/CVPR42600.2020.00884
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shelhamer E, 2016, LECT NOTES COMPUT SC, V9915, P852, DOI 10.1007/978-3-319-49409-8_69
   Sturgess P., 2009, British Machine Vision Conference, P1, DOI [10.5244/C.23.62, DOI 10.5244/C.23.62]
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun K, 2019, Arxiv, DOI arXiv:1904.04514
   Tan ZT, 2021, IEEE T CIRC SYST VID, V31, P175, DOI 10.1109/TCSVT.2020.2971641
   Tanujaya S., 2020, 2020 IEEE INT S CIRC, P1
   Tao AD, 2020, Arxiv, DOI [arXiv:2005.10821, DOI 10.48550/ARXIV.2005.10821]
   Wang H, 2021, Arxiv, DOI arXiv:2102.08643
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang WH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P548, DOI 10.1109/ICCV48922.2021.00061
   Wong CC, 2020, IEEE T IND INFORM, V16, P5128, DOI 10.1109/TII.2019.2950031
   Wu JR, 2020, PATTERN RECOGN, V104, DOI 10.1016/j.patcog.2020.107268
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xiong JJ, 2022, Arxiv, DOI arXiv:2106.04400
   Xu YS, 2018, PROC CVPR IEEE, P6556, DOI 10.1109/CVPR.2018.00686
   Yifan Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P352, DOI 10.1007/978-3-030-58607-2_21
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zhang H, 2014, I C VIRTUAL REALITY, P321, DOI 10.1109/ICVRV.2014.65
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao YY, 2022, IEEE T MULTIMEDIA, V24, P2150, DOI 10.1109/TMM.2021.3076612
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
   Zhu Y, 2019, PROC CVPR IEEE, P8848, DOI 10.1109/CVPR.2019.00906
   Zhuang JF, 2021, IEEE T CIRC SYST VID, V31, P3128, DOI 10.1109/TCSVT.2020.3037234
NR 65
TC 3
Z9 3
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1019
EP 1032
DI 10.1109/TMM.2021.3136085
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900026
DA 2024-07-18
ER

PT J
AU Yang, BX
   Chen, XJ
   Wang, CQ
   Zhang, C
   Chen, ZH
   Sun, XY
AF Yang, Binxin
   Chen, Xuejin
   Wang, Chaoqun
   Zhang, Chi
   Chen, Zihan
   Sun, Xiaoyan
TI Semantics-Preserving Sketch Embedding for Face Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Faces; Aerospace electronics; Codes; Image synthesis; Task
   analysis; Generators; Sketch-based generation; face generation;
   image-to-image translation; semantics-preserving
AB With recent advances in image-to-image translation tasks, remarkable progress has been witnessed in generating face images from sketches. However, existing methods frequently fail to generate images with details that are semantically and geometrically consistent with the input sketch, especially when various decoration strokes are drawn. To address this issue, we introduce a novel W-W+ encoder architecture to take advantage of the high expressive power of W+ space and semantic controllability of W space. We introduce an explicit intermediate representation for sketch semantic embedding. With a semantic feature matching loss for effective semantic supervision, our sketch embedding precisely conveys the semantics in the input sketches to the synthesized images. Moreover, a novel sketch semantic interpretation approach is designed to automatically extract semantics from vectorized sketches. We conduct extensive experiments on both synthesized sketches and hand-drawn sketches, and the results demonstrate the superiority of our method over existing approaches on both semantics-preserving and generalization ability.
C1 [Yang, Binxin; Chen, Xuejin; Wang, Chaoqun; Zhang, Chi; Chen, Zihan; Sun, Xiaoyan] Univ Sci & Technol China, Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chen, XJ (corresponding author), Univ Sci & Technol China, Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
EM tennyson@mail.ustc.edu.cn; xjchen99@ustc.edu.cn; cq14@mail.ustc.edu.cn;
   chih@mail.ustc.edu.cn; zhchen25@mail.ustc.edu.cn; sunxiaoyan@ustc.edu.cn
RI Sun, Xiaoyan/GWZ-5462-2022
OI Sun, Xiaoyan/0000-0002-1386-6853; Wang, Chaoqun/0000-0002-4649-5518;
   Chen, Xuejin/0000-0003-0478-7018
FU National Key Ramp;D Program of China
FX No Statement Available
CR Abdal R, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3447648
   Abdal R, 2019, IEEE I CONF COMP VIS, P4431, DOI 10.1109/ICCV.2019.00453
   Alaluf Y, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6691, DOI 10.1109/ICCV48922.2021.00664
   Alaluf Yuval, 2022, P IEEE CVF C COMP VI, P18511
   Barrow H. G., 1977, P IMAGE UNDERSTANDIN, P659
   Chen SY, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459760
   Chen SY, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392386
   Chen WL, 2018, PROC CVPR IEEE, P9416, DOI 10.1109/CVPR.2018.00981
   Chung J., 2014, NIPS 2014 WORKSH DEE, Vabs/1412.3555, P1, DOI DOI 10.48550/ARXIV.1412.3555
   Du J, 2018, Arxiv, DOI [arXiv:1710.10370, 10.48550/arXiv.1710.10370]
   Gao CY, 2020, PROC CVPR IEEE, P5173, DOI 10.1109/CVPR42600.2020.00522
   Ghosh A, 2019, IEEE I CONF COMP VIS, P1171, DOI 10.1109/ICCV.2019.00126
   Hensel M, 2017, ADV NEUR IN, V30
   Huang Z, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2661229.2661280, 10.1145/2661228.2661280]
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim T, 2017, PR MACH LEARN RES, V70
   Kingma D. P., 2014, arXiv
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Levoy M., 1981, Two-Dimensional Computer Animation, Course Notes 9 for SIGGRAPH, V82
   Li K, 2019, IEEE T IMAGE PROCESS, V28, P3219, DOI 10.1109/TIP.2019.2895155
   Li L, 2019, IEEE COMPUT GRAPH, V39, P38, DOI 10.1109/MCG.2018.2884192
   Li YH, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P991, DOI 10.1145/3394171.3413684
   Li YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2323, DOI 10.1145/3343031.3350854
   Liu FL, 2022, ACM T GRAPHIC, V41, DOI 10.1145/3528223.3530056
   Liu L., 2022, arXiv
   Liu T., 2017, P INT C ADV NEUR INF, V30, P700, DOI DOI 10.48550/ARXIV.1703.00848
   Qi YG, 2019, IEEE ACCESS, V7, P102717, DOI 10.1109/ACCESS.2019.2929804
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schneider RG, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2898351
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Su WC, 2023, IEEE T VIS COMPUT GR, V29, P4074, DOI 10.1109/TVCG.2022.3178734
   Tov O, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459838
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wright L., 2019, Ranger-a synergistic optimizer
   Wu XY, 2018, IEEE INT WORKS MACH
   Xiao CF, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3478513.3480502
   Yang LM, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450284
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu PH, 2021, Arxiv, DOI arXiv:2012.09036
   Zhu XY, 2020, MULTIMED TOOLS APPL, V79, P1585, DOI 10.1007/s11042-019-08158-z
NR 46
TC 0
Z9 0
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8657
EP 8671
DI 10.1109/TMM.2023.3239182
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000051
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, C
   Chen, ML
   Yuan, Y
   Wang, Q
AF Yang, Chuang
   Chen, Mulin
   Yuan, Yuan
   Wang, Qi
TI Reinforcement Shrink-Mask for Text Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Real-time systems; Reliability; Merging; Shape;
   Detectors; Computational efficiency; Text detection; arbitrary-shaped
   text; real-time text detector
AB Existing real-time text detectors reconstruct text contours by shrink-masks only. Though they simplify the framework and can make the model run fast, the strong dependence on shrink-masks leads to unreliable detection results (e.g., miss detection and overdetection). Moreover, these methods ignore the information from surrounding pixels, which causes sensitive shrink-masks and accelerates the reliability decline of detection results. Considering the above problems, we construct an effective and efficient text detection network, termed as Reinforcement Shrink-Mask for Text Detection (RSMTD), which strengthens the model's ability to recognize texts while enjoying a high detection speed. Specifically, an effective text representation strategy (Reinforcement Shrink-Mask, RSM) is designed to decouple texts and shrink-masks. RSM builds texts through shrink-masks and reinforcement offsets to ensure stable detection results encountering shrink-masks that deviate from the ground-truth. It is worth noting that reinforcement offsets can force our method to focus on the foreground shapes to bring precise shrink-mask edges. For the robustness improvement of shrink-masks, Super-pixel Window (SPW) is proposed to encourage RSMTD to utilize the surroundings of each pixel to predict shrink-masks. Particularly, SPW treats the interval regions between texts and shrink-masks as background, which helps to suppress interval regions and to avoid text adhesion. Moreover, a lightweight feature merging branch is constructed to further accelerate the inference process. As demonstrated in the experiments, our method is superior to existing state-of-the-art (SOTA) methods in both detection accuracy and speed on multiple benchmarks.
C1 [Yang, Chuang] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Yang, Chuang; Chen, Mulin; Yuan, Yuan; Wang, Qi] Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence Opt & Elect iOPEN, Xian 710072, Peoples R China.
EM cyang113@mail.nwpu.edu.cn; chenmulin@mail.nwpu.edu.cn;
   y.yuan.ieee@gmail.com; crabwq@gmail.com
RI Yang, Chuang/KHW-4673-2024; Yu, Yue/JWP-9103-2024; liu,
   xinyi/KFB-4466-2024; Liu, Yilin/JWP-9153-2024; zhang,
   yueqi/JXM-4287-2024
OI Liu, Yilin/0000-0002-7581-3933; Wang, Qi/0000-0002-7028-4956
FU National Natural Science Foundation of China [U21B2041, U1864204,
   61825603, 62106182]
FX This work was supported by the National Natural Science Foundation of
   China under Grants U21B2041, U1864204, 61825603, and 62106182. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Palaiahnakote Shivakumara.
CR Baek J, 2019, IEEE I CONF COMP VIS, P4714, DOI 10.1109/ICCV.2019.00481
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Feng W, 2021, PROC CVPR IEEE, P1695, DOI 10.1109/CVPR46437.2021.00174
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ingle R. Reeve, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P17, DOI 10.1109/ICDAR.2019.00013
   Karthick K., 2019, Int. J. Recent Technol. Eng, V8, P2277
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li M, 2023, IEEE T MULTIMEDIA, V25, P649, DOI 10.1109/TMM.2021.3129651
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Minghui Liao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P706, DOI 10.1007/978-3-030-58621-8_41
   Peng DZ, 2023, IEEE T MULTIMEDIA, V25, P2368, DOI 10.1109/TMM.2022.3146771
   Shi-Xue Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9696, DOI 10.1109/CVPR42600.2020.00972
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang WH, 2022, IEEE T PATTERN ANAL, V44, P5349, DOI 10.1109/TPAMI.2021.3077555
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang YX, 2021, IEEE T MULTIMEDIA, V23, P1316, DOI 10.1109/TMM.2020.2995290
   Wu LT, 2023, IEEE T MULTIMEDIA, V25, P2404, DOI 10.1109/TMM.2022.3146779
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue ML, 2021, IEEE T MULTIMEDIA, V23, P2706, DOI 10.1109/TMM.2020.3015037
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yuliang L, 2017, Arxiv, DOI arXiv:1712.02170
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang WQ, 2021, LECT NOTES COMPUT SC, V12824, P79, DOI 10.1007/978-3-030-86337-1_6
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 46
TC 0
Z9 0
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6458
EP 6470
DI 10.1109/TMM.2022.3209022
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500058
DA 2024-07-18
ER

PT J
AU Zhang, CY
   Lin, GS
   Wang, Q
   Shen, FM
   Yao, YZ
   Tang, ZM
AF Zhang, Chuanyi
   Lin, Guosheng
   Wang, Qiong
   Shen, Fumin
   Yao, Yazhou
   Tang, Zhenmin
TI Guided by Meta-Set: A Data-Driven Method for Fine-Grained Visual
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fine-grained; label noise; web images
ID EXPLOITING WEB IMAGES; DEEP; CATEGORY; NOISE
AB The lack of sufficient training data has been one obstacle to fine-grained visual classification research because labeling subcategories generally requires specialist knowledge. As one optional approach to alleviating the data-hunger problem, leveraging web images as training data is drawing increasing attention. Nevertheless, web images potentially have false labels, which can misguide the training process. Although several works have been proposed to deal with label noise, it still can be difficult for the network to tackle complex real-world noisy labels without any prior knowledge. In the literature, we propose to leverage a small and clean meta-set to provide reliable prior knowledge for tackling noisy web images. Specifically, our method trains a network with two peer predicting heads, which learn from noisy web images (web head) and meta ones (meta head), respectively. The meta head produces pseudo soft labels for web images to revise their training loss, which can overcome the high noise ratio problem. Furthermore, a selection net is trained in a meta-learning strategy to identify in- and out-of-distribution noisy images. Then in-distribution ones are reused for training with pseudo soft labels produced by the meta head as supervision, while out-of-distribution ones are discarded. In this manner, the misguidance caused by label noise is remarkably alleviated and in-distribution noisy samples are properly exploited to boost model performance. The superiority of our proposed approach is demonstrated by mathematical theory with great interpretability as well as extensive experimental results on the real-world dataset WebFG-496.
C1 [Zhang, Chuanyi; Wang, Qiong; Yao, Yazhou; Tang, Zhenmin] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Lin, Guosheng] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Shen, Fumin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610056, Peoples R China.
C3 Nanjing University of Science & Technology; Nanyang Technological
   University; University of Electronic Science & Technology of China
RP Wang, Q (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
EM zhangchuanyi@njust.edu.cn; gslin@ntu.edu.sg; wangq@njust.edu.cn;
   fumin.shen@gmail.com; yazhou.yao@njust.edu.cn; tzm.cs@njust.edu.cn
RI Shen, Fumin/R-2121-2016
OI Wang, Qiong/0000-0003-4193-0960; Zhang, Chuanyi/0000-0001-8724-5796;
   Yao, Yazhou/0000-0002-0337-9410
FU National Natural Science Foundation of China [202106840002]; Natural
   Science Foundation of Jiangsu Province [62102182, 61976116, 61905114];
   Fundamental Research Funds for the Central Universities [BK20210327];
   National Key R&D Program of China [30920021135];  [2021YFF0602101]
FX This work was supported by in part by the State Scholarship under Grant
   202106840002, in part by the National Natural Science Foundation of
   China under Grants 62102182, 61976116, and 61905114, in part by the
   Natural Science Foundation of Jiangsu Province under Grant BK20210327,
   in part by the Fundamental Research Funds for the Central Universities
   under Grant 30920021135, and in part by the National Key R&D Program of
   China under Grant 2021YFF0602101.
CR [Anonymous], 2014, P BRIT MACH VIS C, DOI 10.5244/C.28.87
   Arpit D, 2017, PR MACH LEARN RES, V70
   Bai YB, 2021, ADV NEUR IN
   Chen Y, 2019, PROC CVPR IEEE, P5152, DOI 10.1109/CVPR.2019.00530
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Ge WF, 2019, PROC CVPR IEEE, P3029, DOI 10.1109/CVPR.2019.00315
   Ghosh A, 2017, AAAI CONF ARTIF INTE, P1919
   Goldberger J., 2017, P INT C LEARN REPR
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hendrycks D, 2018, ADV NEUR IN, V31
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Korsch D, 2019, LECT NOTES COMPUT SC, V11824, P62, DOI 10.1007/978-3-030-33676-9_5
   Krause J, 2016, LECT NOTES COMPUT SC, V9907, P301, DOI 10.1007/978-3-319-46487-9_19
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Lam M, 2017, PROC CVPR IEEE, P6497, DOI 10.1109/CVPR.2017.688
   Li SK, 2022, PROC CVPR IEEE, P316, DOI 10.1109/CVPR52688.2022.00041
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Liu HF, 2022, IEEE T MULTIMEDIA, V24, P546, DOI 10.1109/TMM.2021.3055024
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Loshchilov I., 2017, P INT C LEARN REPR
   Luo HN, 2019, IEEE I CONF COMP VIS, P9666, DOI 10.1109/ICCV.2019.00976
   Malach E, 2017, ADV NEUR IN, V30
   Nie L., 2012, P 20 ACM INT C MULTI, P59, DOI DOI 10.1145/2393347.2393363
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P2107, DOI 10.1109/TKDE.2015.2399298
   Nie LQ, 2015, IEEE T KNOWL DATA EN, V27, P396, DOI 10.1109/TKDE.2014.2330813
   Niu L, 2018, PROC CVPR IEEE, P7171, DOI 10.1109/CVPR.2018.00749
   Niu L, 2015, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2015.7298894
   Peng YX, 2018, IEEE T IMAGE PROCESS, V27, P1487, DOI 10.1109/TIP.2017.2774041
   Reed S, 2014, Training deep neural networks on noisy labels with bootstrapping
   Ren MY, 2018, PR MACH LEARN RES, V80
   Shu Jun, 2019, ADV NEURAL INFORM PR, P1917
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2018, IEEE T PATTERN ANAL, V40, P905, DOI 10.1109/TPAMI.2017.2705122
   Song H, 2019, PR MACH LEARN RES, V97
   Sun ZR, 2022, PROC CVPR IEEE, P5301, DOI 10.1109/CVPR52688.2022.00524
   Sun ZR, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P92, DOI 10.1145/3394171.3413978
   Sun ZR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10582, DOI 10.1109/ICCV48922.2021.01043
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Vedaldi A., 2013, Technical report
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wei Hongxin, 2020, P IEEE C COMP VIS PA
   Wei XS, 2018, PATTERN RECOGN, V76, P704, DOI 10.1016/j.patcog.2017.10.002
   Xia X, 2020, P INT C LEARN REPR, P1
   Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885
   Xie GS, 2019, PROC CVPR IEEE, P9376, DOI 10.1109/CVPR.2019.00961
   Xie SN, 2015, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2015.7298880
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Xu Z, 2015, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2015.290
   Yao HT, 2016, IEEE T IMAGE PROCESS, V25, P4858, DOI 10.1109/TIP.2016.2599102
   Yao Y., 2016, P 24 ACM INT C MULT, P212
   Yao YZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1735, DOI 10.1145/3394171.3413851
   Yao YZ, 2021, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR46437.2021.00515
   Yao YZ, 2021, PROC CVPR IEEE, P2623, DOI 10.1109/CVPR46437.2021.00265
   Yao YZ, 2020, IEEE T NEUR NET LEAR, V31, P2348, DOI 10.1109/TNNLS.2020.2966644
   Yao YZ, 2020, IEEE T KNOWL DATA EN, V32, P1199, DOI 10.1109/TKDE.2019.2903036
   Yao YZ, 2019, IEEE T MULTIMEDIA, V21, P184, DOI 10.1109/TMM.2018.2847248
   Yao YZ, 2019, IEEE T IMAGE PROCESS, V28, P436, DOI 10.1109/TIP.2018.2869721
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Yu XR, 2019, PR MACH LEARN RES, V97
   Zhang C., 2016, P INT C LEARN REPR, P1
   Zhang CB, 2021, IEEE T IMAGE PROCESS, V30, P5984, DOI 10.1109/TIP.2021.3089942
   Zhang CY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2372, DOI 10.1145/3394171.3414044
   Zhang CY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P4063, DOI 10.1145/3474085.3475536
   Zhang CY, 2020, AAAI CONF ARTIF INTE, V34, P12781
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
   Zhang XP, 2016, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR.2016.128
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P1713, DOI 10.1109/TIP.2016.2531289
   Zheng HL, 2019, PROC CVPR IEEE, P5007, DOI 10.1109/CVPR.2019.00515
   Zheng HL, 2017, IEEE I CONF COMP VIS, P5219, DOI 10.1109/ICCV.2017.557
NR 78
TC 3
Z9 3
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4691
EP 4703
DI 10.1109/TMM.2022.3181439
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300007
DA 2024-07-18
ER

PT J
AU Zhao, SP
   Fei, LK
   Wen, J
   Wu, JG
   Zhang, BB
AF Zhao, Shuping
   Fei, Lunke
   Wen, Jie
   Wu, Jigang
   Zhang, Bob
TI Intrinsic and Complete Structure Learning Based Incomplete Multiview
   Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Complete structure reconstruction; consensus representation learning;
   incomplete multiview clustering; intrinsic structure learning
ID REPRESENTATION
AB In the real-world, some views of samples are often missing for the collected multiview data. Faced with the incomplete multiview data, most of the existing clustering methods tended to learn a common graph from the available views, where the hidden information of the absent views was ignored. Furthermore, some methods filled the absent instances with the average vector of the available samples for each view, which could not reflect a real distribution of the data. To solve these problems, in this paper an intrinsic and complete structure learning based incomplete multiview clustering method (ICSL_IMC) is proposed. Firstly, we calculate the initial complete graphs for all views by exploring the available incomplete graphs, which are further taken as the constraints for the reconstruction of the absent data integrating the self-representation method. Afterwards, encouraged by the complete multiview data, a complete structure inferring strategy is proposed to learn the intrinsic and complete structures for all views, such that the real distribution of the absent instances can be reflected in the completed structure of each view. We integrate these three learning phases into a joint optimization model, which can promote each other in the iterative learning procedure, simultaneously. Comparing with the other state-of-the-art methods, the proposed ICSL_IMC can achieve the best performances on different databases.
C1 [Zhao, Shuping; Fei, Lunke; Wu, Jigang] Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510006, Peoples R China.
   [Wen, Jie] Nanchang Inst Technol, Nanchang 518055, Peoples R China.
   [Wen, Jie] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
   [Zhang, Bob] Univ Macau, Dept Comp & Informat Sci, PAMI Res Grp, Taipa 999078, Macau, Peoples R China.
C3 Guangdong University of Technology; Nanchang Institute Technology;
   Harbin Institute of Technology; University of Macau
RP Wu, JG (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou 510006, Peoples R China.
EM yb77458@connect.um.edu.mo; flksxm@126.com; jiewen_pr@126.com;
   asjgwucn@outlook.com; bobzhang@um.edu.mo
RI Wen, Jie/AAH-8083-2020; Zhang, Bob/HIR-3656-2022; Wen, Jie/G-7235-2015
OI Zhang, Bob/0000-0001-6512-0474; Zhang, Bob/0000-0003-2497-9519; Fei,
   Lunke/0000-0001-6072-7875; Wen, Jie/0000-0001-9554-2379
FU National Natural Science Foundation of China [62106052, 62176066]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62106052 and 62176066. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Vladan Velisavljevic.
CR Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Chao Guoqing, 2021, IEEE Trans Artif Intell, V2, P146, DOI 10.1109/tai.2021.3065894
   cs, COIL 20 DATABASE
   Deng WY, 2020, IEEE ACCESS, V8, P138752, DOI 10.1109/ACCESS.2020.3012500
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fei LK, 2021, IEEE T MULTIMEDIA, V23, P2930, DOI 10.1109/TMM.2020.3019701
   Gao H, 2016, IFIP ADV INF COMM TE, V486, P245, DOI 10.1007/978-3-319-48390-0_25
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Greene D., 2006, P 23 INT C MACH LEAR, V148, P377
   Guo J, 2018, AAAI CONF ARTIF INTE, P298
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Huang D., 2012, Tech. Rep. IRIP-TR-12-FR-001, V3, P3
   Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569
   Huang Z., 2020, P NEURIPS
   ics, HAND WRITTEN DATASET
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li SZ, 2013, IEEE COMPUT SOC CONF, P348, DOI 10.1109/CVPRW.2013.59
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Nie F, 2020, PROC 23 INT C ADV NE, P1813
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P5509, DOI 10.1109/TNNLS.2020.2968848
   Peng X, 2019, PR MACH LEARN RES, V97
   Peng X, 2018, IEEE T IMAGE PROCESS, V27, P5076, DOI 10.1109/TIP.2018.2848470
   Rai N, 2016, INT C PATT RECOG, P2192, DOI 10.1109/ICPR.2016.7899961
   Shao WX, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P1012, DOI 10.1109/BigData.2016.7840701
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang C, 2019, AAAI CONF ARTIF INTE, P5101
   Tao H, 2020, IEEE T CYBERNETICS, V50, P2124, DOI 10.1109/TCYB.2018.2881474
   Trivedi A., 2010, NIPS WORKSH, P1
   ucd, 3 SOURCES DATASET
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang Y., 2016, P 25 INT JOINT C ART, P2153
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wen J, 2021, IEEE T CYBERNETICS, V51, P101, DOI 10.1109/TCYB.2020.2987164
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Wen J, 2020, IEEE T CIRC SYST VID, V30, P75, DOI 10.1109/TCSVT.2018.2889727
   Wen J, 2020, IEEE T CYBERNETICS, V50, P1418, DOI 10.1109/TCYB.2018.2884715
   Wen J, 2018, NEURAL NETWORKS, V108, P83, DOI 10.1016/j.neunet.2018.08.007
   Wen J, 2018, PATTERN RECOGN, V81, P326, DOI 10.1016/j.patcog.2018.04.004
   Wright TG, 2001, SIAM J SCI COMPUT, V23, P591, DOI 10.1137/S1064827500373012
   Wu J, 2018, LECT NOTES ARTIF INT, V11012, P98, DOI 10.1007/978-3-319-97304-3_8
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xu C, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2490539
   Yang MX, 2021, PROC CVPR IEEE, P1134, DOI 10.1109/CVPR46437.2021.00119
   Yang Y, 2018, BIG DATA MIN ANAL, V1, P83, DOI 10.26599/BDMA.2018.9020003
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zhao H., 2016, IJCAI, P2392
   Zhao SP, 2020, INT CONF ACOUST SPEE, P1504, DOI 10.1109/ICASSP40776.2020.9054291
NR 54
TC 12
Z9 12
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1098
EP 1110
DI 10.1109/TMM.2021.3138638
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100006
DA 2024-07-18
ER

PT J
AU Chen, CQ
   Qing, CM
   Xu, XM
   Dickinson, P
AF Chen, Canqiang
   Qing, Chunmei
   Xu, Xiangmin
   Dickinson, Patrick
TI Cross Parallax Attention Network for Stereo Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Superresolution; Three-dimensional displays; Cameras;
   Visualization; Spatial resolution; Estimation; Stereo super-resolution;
   single model for multiple scaling factors; attention mechanism;
   convolutional neural network
ID DEEP
AB Stereo super-resolution (SR) aims to enhance the spatial resolution of one camera view using additional information from the other. Previous deep-learning-based stereo SR methods indeed improved the SR performance effectively by employing additional information, but they are unable to super-resolve stereo images where there are large disparities, or different types of epipolar lines. Moreover, in these methods, one model can only super-solve images of a particular view, and for one specific scale factor. This paper proposes a cross parallax attention stereo super-resolution network (CPASSRnet) which can perform stereo SR of multiple scale factors for both views, with a single model. To overcome the difficulties of large disparity and different types of epipolar lines, a cross parallax attention module (CPAM) is presented, which captures the global correspondence of additional information for each view, relative to the other. CPAM allows the two views to exchange additional information with each other according to the generated attention maps. Quantitative and qualitative results compared with the state of the arts illustrate the superiority of CPASSRnet. Ablation experiments demonstrate that the proposed components are effective and noise tests verify the robustness of CPASSRnet.
C1 [Chen, Canqiang; Qing, Chunmei; Xu, Xiangmin] South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Xu, Xiangmin] Inst Modern Ind Technol SCUT Zhongshan, Zhongshan 528400, Peoples R China.
   [Dickinson, Patrick] Univ Lincoln, Sch Comp Sci, Lincoln LN6 7TS, England.
C3 South China University of Technology; University of Lincoln
RP Qing, CM (corresponding author), South China Univ Technol, Sch Elect & Informat Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM canq.chen@gmail.com; qchm@scut.edu.cn; xmxu@scut.edu.cn;
   pdickinson@lincoln.ac.uk
FU National Natural Science Foundation of China [61972163, U1801262,
   61702192, U1636218, 61802131]; Science and Technology Project of
   Zhongshan [2019AG024]; Research and Development Project in Key Areas of
   Guangdong Province [2018B010109004]
FX This work was supported in part by the National Natural Science
   Foundation ofChina underGrants 61972163, U1801262, 61702192, and
   U1636218, 61802131, in part by Science and Technology Project of
   Zhongshan (2019AG024), and in part by the Research and Development
   Project in Key Areas of Guangdong Province under Grant 2018B010109004.
   The associate editor coordinating the reviewof this manuscript and
   approving it for publication was Prof. Chi-Chun Lee.
CR Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   [Anonymous], 2008, P IEEE C COMP VIS PA
   [Anonymous], 2015, PROC 3 INT C LEARN R
   Baker S, 2007, IEEE I CONF COMP VIS, P588, DOI 10.1109/cvpr.2007.383191
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Bhavsar AV, 2008, INT C PATT RECOG, P1102
   Bo Yan, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13176, DOI 10.1109/CVPR42600.2020.01319
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chen DD, 2018, PROC CVPR IEEE, P6654, DOI 10.1109/CVPR.2018.00696
   Chen JZ, 2016, PROCEEDINGS OF 2016 12TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P551, DOI [10.1109/CIS.2016.133, 10.1109/CIS.2016.0134]
   Cheng H, 2019, IEEE T MULTIMEDIA, V21, P678, DOI 10.1109/TMM.2018.2864613
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Glasner D, 2009, IEEE I CONF COMP VIS, P349, DOI 10.1109/ICCV.2009.5459271
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J., 2020, P 28 ACM INT C MULT, P2728
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jeon DS, 2018, PROC CVPR IEEE, P1721, DOI 10.1109/CVPR.2018.00185
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1646, DOI 10.1109/CVPR.2016.182
   Kim KI, 2010, IEEE T PATTERN ANAL, V32, P1127, DOI 10.1109/TPAMI.2010.25
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li J, 2018, PROC EUR C COMPUT VI, V11212, P517, DOI 10.1007/978-3-030-01237-3_32
   Li J, 2019, IEEE T MULTIMEDIA, V21, P2531, DOI 10.1109/TMM.2019.2908350
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li PL, 2019, PROC CVPR IEEE, P7636, DOI 10.1109/CVPR.2019.00783
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu J, 2020, PROC CVPR IEEE, P2356, DOI 10.1109/CVPR42600.2020.00243
   Mei YQ, 2020, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR42600.2020.00573
   Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925
   Minghao Yin, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P191, DOI 10.1007/978-3-030-58555-6_12
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Parikh AP., 2016, EMNLP
   Park Haesol., 2012, P 2012 ASIA PACIFIC, P1
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2014, LECT NOTES COMPUT SC, V8753, P31, DOI 10.1007/978-3-319-11752-2_3
   Schöps T, 2017, PROC CVPR IEEE, P2538, DOI 10.1109/CVPR.2017.272
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Simonyan K., 2014, 14091556 ARXIV
   Song W, 2020, AAAI CONF ARTIF INTE, V34, P12031
   Sun J, 2008, PROC CVPR IEEE, P2471, DOI 10.1109/CVPR.2008.4587659
   Tian CW, 2021, IEEE T MULTIMEDIA, V23, P1489, DOI 10.1109/TMM.2020.2999182
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang LG, 2019, PROC CVPR IEEE, P12242, DOI 10.1109/CVPR.2019.01253
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiong ZW, 2010, IEEE T IMAGE PROCESS, V19, P2017, DOI 10.1109/TIP.2010.2045707
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Ying XY, 2020, IEEE SIGNAL PROC LET, V27, P496, DOI 10.1109/LSP.2020.2973813
   Yu Qiao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13673, DOI 10.1109/CVPR42600.2020.01369
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhou SC, 2019, PROC CVPR IEEE, P10988, DOI 10.1109/CVPR.2019.01125
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
NR 64
TC 21
Z9 23
U1 3
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 202
EP 216
DI 10.1109/TMM.2021.3050092
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300016
DA 2024-07-18
ER

PT J
AU Ding, YJ
   Ma, YS
   Liao, LZ
   Wong, WK
   Chua, TS
AF Ding, Yujuan
   Ma, Yunshan
   Liao, Lizi
   Wong, Wai Keung
   Chua, Tat-Seng
TI Leveraging Multiple Relations for Fashion Trend Forecasting Based on
   Social Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Market research; Forecasting; Predictive models; Task analysis; Social
   networking (online); Urban areas; Image color analysis; Fashion trend
   forecasting; time series forecasting; fashion analysis; social media
AB Fashion trend forecasting is of great research significance in providing useful suggestions for both fashion companies and fashion lovers. Although various studies have been devoted to tackling this challenging task, they only studied limited fashion elements with highly seasonal or simple patterns, which could hardly reveal the real complex fashion trends. Moreover, the mainstream solutions for this task are still statistical-based and solely focus on time-series data modeling, which limit the forecast accuracy. Towards insightful fashion trend forecasting, previous work [1] proposed to analyze more fine-grained fashion elements which can informatively reveal fashion trends. Specifically, it focused on detailed fashion element trend forecasting for specific user groups based on social media data. In addition, it proposed a neural network-based method, namely KERN, to address the problem of fashion trend modeling and forecasting. In this work, to extend the previous work [1], we propose an improved model named Relation Enhanced Attention Recurrent (REAR) network. Compared to KERN, the REAR model leverages not only the relations among fashion elements, but also those among user groups, thus capturing more types of correlations among various fashion trends. To further improve the performance of long-range trend forecasting, the REAR method devises a sliding temporal attention mechanism, which is able to capture temporal patterns on future horizons better. Extensive experiments and more analysis have been conducted on the FIT [1] and GeoStyle [2] datasets to evaluate the performance of REAR. Experimental and analytical results demonstrate the effectiveness of the proposed REAR model in fashion trend forecasting, which also show the improvement of REAR compared to the KERN.
C1 [Ding, Yujuan] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Wong, Wai Keung] Hong Kong Polytech Univ, Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Ma, Yunshan; Liao, Lizi; Chua, Tat-Seng] Natl Univ Singapore, Singapore 119077, Singapore.
C3 Shenzhen University; Hong Kong Polytechnic University; National
   University of Singapore
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
EM dingyujuan385@gmail.com; yunshan.ma@u.nus.edu; liaolizi.llz@gmail.com;
   calvin.wong@polyu.edu.hk; dcscts@nus.edu.sg
OI Wong, Wai Keung/0000-0002-5214-7114; Liao, Lizi/0000-0002-9973-3305;
   Lai, Zhihui/0000-0002-4388-3080
FU National Research Foundation, Singapore under its International Research
   Centres in Singapore Funding Initiative; Laboratory for Artificial
   Intelligence in Design, Hong Kong [RP3-1]
FX This work was partly supported by National Research Foundation,
   Singapore under its International Research Centres in Singapore Funding
   Initiative. It was also partly supported by the Laboratory for
   Artificial Intelligence in Design (Project Code: RP3-1), Hong Kong.
CR Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2005, Fashion forecasting
   Bandara K, 2019, LECT NOTES COMPUT SC, V11955, P462, DOI 10.1007/978-3-030-36718-3_39
   BOX GEP, 1968, ROY STAT SOC C-APP, V17, P91
   Cinar YG, 2017, LECT NOTES COMPUT SC, V10638, P533, DOI 10.1007/978-3-319-70139-4_54
   Ding Y., 2018, PROC INT C ARTIF INT, P187
   Ding YJ, 2020, IEEE T MULTIMEDIA, V22, P1298, DOI 10.1109/TMM.2019.2940875
   Ding YJ, 2020, IEEE T CIRC SYST VID, V30, P590, DOI 10.1109/TCSVT.2019.2891246
   DuBreuil M, 2020, INT J FASH DES TECHN, V13, P68, DOI 10.1080/17543266.2020.1732482
   Fan CY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2527, DOI 10.1145/3292500.3330662
   Feng FL, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3309547
   Grauman K, 2020, ARXIV200401316
   Gu XL, 2019, IEEE T MULTIMEDIA, V21, P1524, DOI 10.1109/TMM.2018.2876822
   Holt CC, 2004, INT J FORECASTING, V20, P5, DOI 10.1016/j.ijforecast.2004.09.015
   Israeli A., 2018, Harvard Business School Case, V517, P115
   Kim E., 2021, FASHION TRENDS ANAL
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang YX, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3428
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma YS, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P257, DOI 10.1145/3343031.3350889
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   Matzen Kevin, 2017, ARXIV170601869
   Qin Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2627
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Slutzky E, 1937, ECONOMETRICA, V5, P105, DOI 10.2307/1907241
   Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802
   Walker G, 1931, P R SOC LOND A-CONTA, V131, P518, DOI 10.1098/rspa.1931.0069
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   WINTERS PR, 1960, MANAGE SCI, V6, P324, DOI 10.1287/mnsc.6.3.324
   Yang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P775, DOI 10.1145/3331184.3331242
   Yunshan Ma, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P82, DOI 10.1145/3372278.3390677
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
NR 34
TC 6
Z9 6
U1 16
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2287
EP 2299
DI 10.1109/TMM.2021.3078907
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600006
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Fan, BJ
   Tian, JD
   Peng, Y
   Tang, YD
AF Fan, Baojie
   Tian, Jiandong
   Peng, Yan
   Tang, Yandong
TI Discriminative Siamese Complementary Tracker With Flexible Update
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Correlation; Feature extraction; Proposals;
   Convolution; Biological system modeling; State estimation; Siamese
   complementary tracking; online discriminative learning; classifier
   guided template update network; multi-peak suppression
ID VISUAL TRACKING; OBJECT
AB The offline generative Siamese trackers are equipped with the pre-defined anchors and the fixed target template. They overlook the target-background discriminative information, and lack the flexible target-specific update strategy. To overcome above drawbacks, we propose an adaptive and discriminative Siamese complementary tracking network with flexible update scheme. It consists of three collaborate subnetworks: anchor-free Siamese attention classification and regression subnetwork, online discriminative learning with multi-attention and multi-peak suppression, classifier guided template update subnetwork. All of them are interdependent and complementary to enhance each other for accurate target location. Specifically, an anchor-free multi-attention Siamese tracking subnetwork directly classifies the corresponding image patches with reliability assessment, and cascaded regresses the bounding boxes to progressively refine the predicting accuracy. Its evaluation is flexible and general with both proposal and anchor free in per-pixel prediction manner. Then, we integrate an online discriminative classifier optimizing module as a complementary subnetwork. It introduces spatial-temporal attention mechanism to fully explore multi-view multi-scale target-specific features, and evaluates multi-peak suppression to obtain a single centered peak response map. Its classified results can be fused with Siamese classification branch for accurate target location. Finally, the template update subnetwork is guided by the online discriminative classification scores. Extensive experiments on recent tracking datasets verify its top-ranked tracking accuracy and robustness against some state-of-the-art trackers.
C1 [Fan, Baojie] Nanjing Univ Posts & Telecommun, Automat & AI Coll, Shenyang, Peoples R China.
   [Fan, Baojie] Nanjing Univ Posts & Telecommun, State Key Lab Integrated Serv Networks, Shenyang, Peoples R China.
   [Tian, Jiandong; Tang, Yandong] Chinese Acad Sci, Shenyang Inst Automat, Beijing, Peoples R China.
   [Peng, Yan] Shanghai Univ, Res Inst USV Engn, Baoshan, Shanghai, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Nanjing University of
   Posts & Telecommunications; Chinese Academy of Sciences; Shenyang
   Institute of Automation, CAS; Shanghai University
RP Peng, Y (corresponding author), Shanghai Univ, Res Inst USV Engn, Baoshan, Shanghai, Peoples R China.
EM jobfbj@gmail.com; tianjd@sia.cn; pengyan@shu.edu.cn; ytang@sia.cn
OI Tang, Yandong/0000-0003-3805-7654
FU Ministry of Science and Technology of the People's Republic of China
   [2019YFB1310300]; National Natural Science Foundation of China
   [61876092, U2013210]; State key Laboratory of Robotics [2019-O07]; State
   key Laboratory of Integrated Service Network [ISN20-08]
FX This work was supported in part by the Ministry of Science and
   Technology of the People's Republic of China (2019YFB1310300), National
   Natural Science Foundation of China under Grants 61876092 and U2013210,
   in part by the State key Laboratory of Robotics under Grant 2019-O07,
   and in part by the State key Laboratory of Integrated Service Network
   (ISN20-08).
CR Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Choi J, 2019, IEEE I CONF COMP VIS, P911, DOI 10.1109/ICCV.2019.00100
   Dai KN, 2020, PROC CVPR IEEE, P6297, DOI 10.1109/CVPR42600.2020.00633
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE I CONF COMP VIS, P5487, DOI 10.1109/ICCV.2017.585
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gao SH, 2021, IEEE T PATTERN ANAL, V43, P652, DOI 10.1109/TPAMI.2019.2938758
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   Hao Chen, 2019, 2019 IEEE International Conference on Robotics and Biomimetics (ROBIO), P456, DOI 10.1109/ROBIO49542.2019.8961438
   He AF, 2018, PROC CVPR IEEE, P4834, DOI 10.1109/CVPR.2018.00508
   Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21
   Huang L., 2018, PROC IEEE C COMPUT V, P658
   Huang LH, 2020, AAAI CONF ARTIF INTE, V34, P11037
   Jung I., 2018, P ECCV, P83
   Kristan M, 2018, PROC EUR C COMPUT VI, P1
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Li YM, 2021, IEEE T MULTIMEDIA, V23, P810, DOI 10.1109/TMM.2020.2990064
   Lukezic A, 2019, LECT NOTES COMPUT SC, V11362, P595, DOI 10.1007/978-3-030-20890-5_38
   Ma C, 2019, IEEE T PATTERN ANAL, V41, P2709, DOI [10.1109/TPAMI.2018.2865311, 10.1109/INTMAG.2018.8508195]
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Marvasti-Zadeh S. M., 2019, IEEE Trans. Intell. Transp. Syst.
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Park E., 2018, EUR C COMPUT VIS, P569
   Qi YW, 2020, IEEE T SYST MAN CY-S, V50, P1442, DOI 10.1109/TSMC.2018.2801284
   Ren LL, 2018, LECT NOTES COMPUT SC, V11213, P697, DOI 10.1007/978-3-030-01240-3_42
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Sun YX, 2021, IEEE ROBOT AUTOM LET, V6, P510, DOI 10.1109/LRA.2020.3047783
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wang Qiang, 2018, ARXIV181205050
   Wang SK, 2020, IEEE ROBOT AUTOM LET, V5, P3206, DOI 10.1109/LRA.2020.2974392
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2019, IEEE I CONF COMP VIS, P2385, DOI 10.1109/ICCV.2019.00247
   Yunhua Zhang, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11213), P355, DOI 10.1007/978-3-030-01240-3_22
   Zhang KH, 2019, IEEE T IMAGE PROCESS, V28, P479, DOI 10.1109/TIP.2018.2868561
   Zhang LC, 2019, IEEE I CONF COMP VIS, P4009, DOI 10.1109/ICCV.2019.00411
   Zhang TZ, 2017, PROC CVPR IEEE, P4819, DOI [10.1109/CVPR.2017.512, 10.1109/ICCV.2017.469]
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng LY, 2019, IEEE I CONF COMP VIS, P4019, DOI 10.1109/ICCV.2019.00412
   Zhou J., 2019, ARXIV190902959
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 67
TC 4
Z9 4
U1 4
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2766
EP 2778
DI 10.1109/TMM.2021.3087347
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000006
DA 2024-07-18
ER

PT J
AU Guo, ZY
   Zhao, Z
   Jin, WK
   Wang, DZ
   Liu, RT
   Yu, J
AF Guo, Zhaoyu
   Zhao, Zhou
   Jin, Weike
   Wang, Dazhou
   Liu, Ruitao
   Yu, Jun
TI TaoHighlight: Commodity-Aware Multi-Modal Video Highlight Detection in
   E-Commerce
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Feature extraction; Data models; Streaming
   media; Linguistics; Convolution; Video highlight detection; electronic
   Commerce; multi-modal learning
AB In e-commerce, product related video is important content to introduce product characteristics and attract consumers. Especially in the recommendation system of e-commerce platform, video highlight detection methods are usually adopted to capture the most attractive clips for showing to consumers, so as to improve the click through rate of products. However, the effect of the current research methods applied to the actual scene is not satisfactory. Compared with other video understanding tasks, video highlight detection is relatively abstract and subjective, and it is difficult to make accurate judgment only by using visual information. Consequently, we put forward multi-modal video highlight detection task, which introduces video related linguistic information as supervised information. And we propose a graph-based commodity-aware model to solve multi-modal video highlight detection in e-commerce scene. Our model consists of multi-modal highlight detection stage and graph-based fine-tuning stage, in which we adopt graph aggregation method to fuse multi-source natural language information and introduce effective visual feature composition method for graph convolution network based highlight detection. Besides, we release the largest e-commerce video highlight detection dataset, TaoHighlight, in which the videos and related data are collected from Taobao e-commerce platform. Our model achieves state-of-art in all separate categories and overall dataset of TaoHighlight, which shows the superiority of our model.
C1 [Guo, Zhaoyu; Zhao, Zhou; Jin, Weike] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Wang, Dazhou; Liu, Ruitao] Alibaba Grp, Hangzhou 311121, Peoples R China.
   [Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
C3 Zhejiang University; Alibaba Group; Hangzhou Dianzi University
RP Zhao, Z (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
EM joeyguo@zju.edu.cn; zhaozhou@zju.edu.cn; weikejin@zju.edu.cn;
   dazhou.wangdz@alibaba-inc.com; ruitao.liurt@alibaba-inc.com;
   yujun@hdu.edu.cn
RI Zhao, zhuo/JYO-7894-2024; Wang, Dazhou/AAU-3376-2020; zhao,
   zhao/JAC-1686-2023
OI Jin, Weike/0000-0002-3132-5567
FU National Key R&D Program of China [2018AAA0100603]; National Natural
   Science Foundation of China [61836002, 62072397]; Zhejiang Natural
   Science Foundation [LR19F020006]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0100603, in part by the National Natural Science
   Foundation of China under Grants 61836002, 62072397, and Zhejiang
   Natural Science Foundation under Grant LR19F020006. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Prof. Jinhui Tang.
CR Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2019, AAAI CONF ARTIF INTE, P8175
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Doshi N., 2019, INT C COMPUT VIS IMA, P1524
   Gabeur Valentin, 2020, P EUR C COMP VIS, V12349, P214, DOI [10.48550/arXiv.2007.10639, DOI 10.48550/ARXIV.2007.10639]
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Guo DY, 2019, IEEE INT CONF MULTI, P687, DOI 10.1109/ICMEW.2019.00134
   Gygli M, 2016, PROC CVPR IEEE, P1001, DOI 10.1109/CVPR.2016.114
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hirsch S, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P1319, DOI 10.1145/3397271.3401065
   Jiao YF, 2018, IEEE T MULTIMEDIA, V20, P2693, DOI 10.1109/TMM.2018.2815998
   Jin W., 2021, PROC 44 INT ACM SIGI
   Jin WK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1193, DOI 10.1145/3343031.3351065
   Junyeong Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10103, DOI 10.1109/CVPR42600.2020.01012
   Li HR, 2020, AAAI CONF ARTIF INTE, V34, P8188
   Li JT, 2020, AAAI CONF ARTIF INTE, V34, P8212
   Li ZC, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3063-0
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin YL, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P443, DOI 10.1109/ICCVW.2015.65
   Lin ZJ, 2020, AAAI CONF ARTIF INTE, V34, P11539
   Lin ZJ, 2020, IEEE T IMAGE PROCESS, V29, P3750, DOI 10.1109/TIP.2020.2965987
   Liu M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P843, DOI 10.1145/3240508.3240549
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu Yinhan, 2019, ARXIV190711692
   Lu JS, 2019, ADV NEUR IN, V32
   Mandal A., 2019, CEUR Workshop Proceedings, P2410
   Mendi E, 2013, COMPUT ELECTR ENG, V39, P790, DOI 10.1016/j.compeleceng.2012.11.020
   Sahu Saurabh, 2020, MM '20: Proceedings of the 28th ACM International Conference on Multimedia, P313, DOI 10.1145/3394171.3413756
   Sun M, 2014, LECT NOTES COMPUT SC, V8689, P787, DOI 10.1007/978-3-319-10590-1_51
   Vashishth S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3308
   Vaswani A, 2017, ADV NEUR IN, V30
   Xiong B, 2019, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2019.00135
   Xu X., 2019, P PACIS
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeman Daniel, 2018, P CONLL 2018 SHAR TA, P1
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang SY, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2744, DOI 10.1145/3394486.3403325
   Zhang YY, 2020, AAAI CONF ARTIF INTE, V34, P12902
   Zhang Z., 2021, PROC INT C MACH LEAR
   Zhang Z., 2020, Advances in NIPS, P18123
   Zhang Z., 2021, PROC IEEECVF C COMPU
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang Zhu, 2020, P IEEECVF C COMPUTER, P10668
   Zhao B, 2018, PROC CVPR IEEE, P7405, DOI 10.1109/CVPR.2018.00773
NR 49
TC 4
Z9 4
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2606
EP 2616
DI 10.1109/TMM.2021.3087001
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600028
DA 2024-07-18
ER

PT J
AU Huang, TC
   Zhang, RX
   Sun, LF
AF Huang, Tianchi
   Zhang, Rui-Xiao
   Sun, Lifeng
TI Zwei: A Self-Play Reinforcement Learning Framework for Video
   Transmission Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video transmission; self-play; reinforcement learning
AB Video transmission services adopt adaptive algorithms to ensure users' demands. Existing techniques are often optimized and evaluated by a function that linearly combines several weighted metrics. Nevertheless, we observe that the given function often fails to describe the requirement accurately, resulting in the violation of generating the required methods. We propose Zwei, a self-play reinforcement learning framework that updates the policy by straightforwardly utilizing the actual requirement. Technically, Zwei effectively rolls out the trajectories from the same initial state, and instantly estimate the win rate w.r.t the competition outcome, where the outcome represents which trajectory is closer to the assigned requirement. We evaluate Zwei with different requirements on various video transmission tasks, including adaptive bitrate streaming, crowd-sourced live streaming scheduling, and real-time communication. Results indicate that Zwei optimizes itself according to the assigned requirement faithfully, outperforming the state-of-the-art methods under all considered scenarios. Moreover, we further propose Zwei(+), which enables Zwei to learn the policies in the vanilla no-regret reinforcement learning scenario. We validate Zwei(+) in the adaptive Nitrate streaming task and show the superiority of the proposed method over existing state-of-the-art approaches.
C1 [Huang, Tianchi; Zhang, Rui-Xiao; Sun, Lifeng] Tsinghua Univ, Dept Comp Sci & Technol, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
   [Huang, Tianchi; Sun, Lifeng] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 10084, Peoples R China.
   [Sun, Lifeng] Tsinghua Univ, Minist Educ, Key Lab Pervas Comp, Beijing, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University
RP Sun, LF (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.; Sun, LF (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 10084, Peoples R China.
EM htc17@mails.tsinghua.edu.cn; zhangrx17@mails.tsinghua.edu.cn;
   sunlf@tsinghua.edu.cn
OI Huang, Tianchi/0000-0001-9378-6329
FU National Key R&D Program of China [2018YFB1003703]; NSFC [61936011,
   61521002]; Beijing Key Lab of NetworkedMultimedia
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018YFB1003703, in part by NSFC under Grants 61936011,
   61521002, and in part by the Beijing Key Lab of NetworkedMultimedia. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Ali C. Begen.
CR Adhikari VK, 2012, IEEE INFOCOM SER, P1620, DOI 10.1109/INFCOM.2012.6195531
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2017, Cisco Visual Networking Index: Global Mobile Data Traffic Forecast Update, 2016-2021 White Paper
   Balduzzi D, 2019, PR MACH LEARN RES, V97
   Bentaleb A, 2021, IEEE T MULTIMEDIA, V23, P2588, DOI 10.1109/TMM.2020.3013387
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Carlucci G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P133, DOI 10.1145/2910017.2910605
   Coulom R, 2008, LECT NOTES COMPUT SC, V5131, P113, DOI 10.1007/978-3-540-87608-3_11
   Elo A. E., 1978, The Rating of Chessplayers, Past and Present
   Gadaleta M, 2017, IEEE T COGN COMMUN, V3, P703, DOI 10.1109/TCCN.2017.2755007
   Huang T., 2020, ZWEI
   Huang T., 2019, ARXIV190802270
   Huang T., 2018, ARXIV181106166
   Huang TY, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P86, DOI 10.1145/3304109.3306219
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huang TC, 2020, NOSSDAV '20: PROCEEDINGS OF THE 2020 WORKSHOP ON NETWORK AND OPERATING SYSTEM SUPPORT FOR DIGITAL AUDIO AND VIDEO, P7, DOI 10.1145/3386290.3396930
   Huang TC, 2020, IEEE INFOCOM SER, P1967, DOI [10.1109/infocom41043.2020.9155411, 10.1109/INFOCOM41043.2020.9155411]
   Huang TC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1208, DOI 10.1145/3240508.3240545
   Jang E., 2016, ARXIV161101144
   Jiang J., 2020, ARXIV200804128
   Jiang JC, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P393
   Jiang JC, 2016, 13TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI '16), P137
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Kim B, 2020, AAAI CONF ARTIF INTE, V34, P9916
   Lefelhocz C, 1996, IEEE NETWORK, V10, P10, DOI 10.1109/65.484227
   Li Junjie, 2020, ARXIV200313590
   Liu X, 2012, ACM SIGCOMM COMP COM, V42, P359, DOI 10.1145/2377677.2377752
   Lu Z, 2018, IEEE T MULTIMEDIA, V20, P1848, DOI 10.1109/TMM.2017.2772802
   Mao HZ, 2019, ADV NEUR IN, V32
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   McDonald, 2001, PARALLEL PROGRAMMING
   Mnih V, 2016, PR MACH LEARN RES, V48
   Mo J, 1999, IEEE INFOCOM SER, P1556, DOI 10.1109/INFCOM.1999.752178
   Rassool R, 2017, IEEE INT SYM BROADB, P351
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Rossi Dario, 2010, ICCCN, P1
   Sato N, 2017, IEEE SYMP COMP COMMU, P339, DOI 10.1109/ISCC.2017.8024553
   Schulman J., 2017, ARXIV
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spiteri K, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P123, DOI 10.1145/3204949.3204953
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   Torres R, 2011, INT CON DISTR COMP S, P248, DOI 10.1109/ICDCS.2011.43
   Winstein Keith, 2013, 10 USENIX S NETW SYS, P459
   Xiao XD, 2020, IEEE T MULTIMEDIA, V22, P1567, DOI 10.1109/TMM.2019.2945167
   Ye, ARXIV191209729, V2019
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zhang H., 2020, P ACM MOBICOM, P1
   Zhang RX, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P55, DOI 10.1145/3304112.3325607
   Zhang RX, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3397226
   Zhang X., 2020, ARXIV200804687
NR 51
TC 9
Z9 10
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1350
EP 1365
DI 10.1109/TMM.2021.3063620
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200009
DA 2024-07-18
ER

PT J
AU Shi, ZH
   Liu, XH
   Shi, KD
   Dai, LH
   Chen, J
AF Shi, Zhihao
   Liu, Xiaohong
   Shi, Kangdi
   Dai, Linhui
   Chen, Jun
TI Video Frame Interpolation via Generalized Deformable Convolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Interpolation; Kernel; Shape; Streaming media; Estimation;
   Task analysis; Generalized deformable convolution; video frame
   interpolation
AB Video frame interpolation aims at synthesizing intermediate frames from nearby source frames while maintaining spatial and temporal consistencies. The existing deep-learning-based video frame interpolation methods can be roughly divided into two categories: flow-based methods and kernel-based methods. The performance of flow-based methods is often jeopardized by the inaccuracy of flow map estimation due to oversimplified motion models, while that of kernel-based methods tends to be constrained by the rigidity of kernel shape. To address these performance-limiting issues, a novel mechanism named generalized deformable convolution is proposed, which can effectively learn motion information in a data-driven manner and freely select sampling points in space-time. We further develop a new video frame interpolation method based on this mechanism. Our extensive experiments demonstrate that the new method performs favorably against the state-of-the-art, especially when dealing with complex motions. Code is available at https://github.com/zhshi0816/GDConvNet.
C1 [Shi, Zhihao; Liu, Xiaohong; Shi, Kangdi; Dai, Linhui; Chen, Jun] McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
C3 McMaster University
RP Liu, XH; Chen, J (corresponding author), McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8S 4K1, Canada.
EM shiz31@mcmaster.ca; liux173@mcmaster.ca; shik9@mcmaster.ca;
   dail5@mcmaster.ca; junchen@mail.ece.mcmaster.ca
OI Shi, Kangdi/0000-0002-9949-4252; Liu, Xiaohong/0000-0001-6377-4730
FU Natural Sciences and Engineering Research Council of Canada
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada through a Discovery Grant. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Ling-Yu Duan.
CR Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   Bao WB, 2019, PROC CVPR IEEE, P3698, DOI 10.1109/CVPR.2019.00382
   Bao WB, 2021, IEEE T PATTERN ANAL, V43, P933, DOI 10.1109/TPAMI.2019.2941941
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He ZW, 2020, IEEE T MULTIMEDIA, V22, P1042, DOI 10.1109/TMM.2019.2937688
   Holschneider M., 1990, WAVELETS, P286, DOI [10.1007/978-3-642-75988-828, DOI 10.1007/978-3-642-75988-8_28]
   Horn B. K., 1981, P SOC PHOTO-OPT INST, V281, P319
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200
   Jia X., 2016, ADV NEURAL INFORM PR, P667
   Jiang HZ, 2018, PROC CVPR IEEE, P9000, DOI 10.1109/CVPR.2018.00938
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee H, 2020, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR42600.2020.00536
   Li CY, 2020, IEEE T MULTIMEDIA, V22, P704, DOI 10.1109/TMM.2019.2933334
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu XH, 2021, IEEE T IMAGE PROCESS, V30, P2127, DOI 10.1109/TIP.2021.3049974
   Liu XH, 2019, IEEE I CONF COMP VIS, P7313, DOI 10.1109/ICCV.2019.00741
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Long GC, 2016, LECT NOTES COMPUT SC, V9910, P434, DOI 10.1007/978-3-319-46466-4_26
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Ma M, 2008, IEEE T MULTIMEDIA, V10, P1638, DOI 10.1109/TMM.2008.2007282
   Meyer S, 2015, PROC CVPR IEEE, P1410, DOI 10.1109/CVPR.2015.7298747
   Nah S, 2017, PROC CVPR IEEE, P257, DOI 10.1109/CVPR.2017.35
   Niklaus S, 2018, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2018.00183
   Niklaus S, 2017, IEEE I CONF COMP VIS, P261, DOI 10.1109/ICCV.2017.37
   Niklaus S, 2017, PROC CVPR IEEE, P2270, DOI 10.1109/CVPR.2017.244
   Park J., 2018, BRIT MACH VIS C, P147
   Qi GJ, 2015, IEEE T MULTIMEDIA, V17, P1873, DOI 10.1109/TMM.2015.2485538
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Simonyan K., 2014, 14091556 ARXIV
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Soomro K., 2012, ARXIV12120402CS
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Usman M, 2016, IEEE T MULTIMEDIA, V18, P831, DOI 10.1109/TMM.2016.2537200
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang TY, 2019, PROC CVPR IEEE, P12262, DOI 10.1109/CVPR.2019.01255
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xu Xiangyu, 2019, 33 INT C NEURAL INF
   Xue TF, 2019, INT J COMPUT VISION, V127, P1106, DOI 10.1007/s11263-018-01144-2
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zhang CX, 2020, IEEE T MULTIMEDIA, V22, P349, DOI 10.1109/TMM.2019.2929934
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
   Zhu XZ, 2019, PROC CVPR IEEE, P9300, DOI 10.1109/CVPR.2019.00953
NR 55
TC 33
Z9 33
U1 4
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 426
EP 439
DI 10.1109/TMM.2021.3052419
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300033
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, HC
   Liu, XP
   Yin, BC
   Li, X
AF Tan, Hongchen
   Liu, Xiuping
   Yin, Baocai
   Li, Xin
TI Cross-Modal Semantic Matching Generative Adversarial Networks for
   Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Task analysis; Generative adversarial networks; Generators;
   Gallium nitride; Feature extraction; Visualization; Cross-modal semantic
   matching; generative adversarial network (GAN); text-to-image synthesis;
   text _CNNs
AB Synthesizing photo-realistic images based on text descriptions is a challenging image generation problem. Although many recent approaches have significantly advanced the performance of text-to-image generation, to guarantee semantic matchings between the text description and synthesized image remains very challenging. In this paper, we propose a new model, Cross-modal Semantic Matching Generative Adversarial Networks (CSM-GAN), to improve the semantic consistency between text description and synthesized image for a fine-grained text-to-image generation. Two new modules are proposed in CSM-GAN: Text Encoder Module (TEM) and Textual-Visual Semantic Matching Module (TVSMM). TVSMM is aimed at making the distance of the pairs of synthesized image and its corresponding text description closer, in global semantic embedding space, than those of mismatched pairs. This improves the semantic consistency and consequently, the generalizability of CSM-GAN. In TEM, we introduce Text Convolutional Neural Networks (Text_CNNs) to capture and highlight local visual features in textual descriptions. Thorough experiments on two public benchmark datasets demonstrated the superiority of CSM-GAN over other representative state-of-the-art methods.
C1 [Tan, Hongchen; Liu, Xiuping] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Yin, Baocai] Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Li, Xin] Louisiana State Univ, Sch Elect Engn & Comp Sci, Baton Rouge, LA 70803 USA.
   [Li, Xin] Louisiana State Univ, Ctr Computat & Technol, Baton Rouge, LA 70803 USA.
C3 Dalian University of Technology; Dalian University of Technology;
   Louisiana State University System; Louisiana State University; Louisiana
   State University System; Louisiana State University
RP Liu, XP (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM tanhongchenphd@mail.dlut.edu.cn; xpliu@dlut.edu.cn; ybc@dlut.edu.cn;
   xinli@cct.lsu.edu
RI Tan, Hongchen/AGZ-4796-2022; Liu, Xiu/IYJ-9134-2023; Liu,
   Xiufang/I-8003-2015; Li, Xin/HGE-2316-2022
OI Tan, Hongchen/0000-0002-3179-8129; Li, Xin/0000-0002-0144-9489; Tan,
   Hongchen/0000-0001-6915-8736
FU Ministry of Science and Technology of the People's Republic of China
   [2018AAA0102003]; National Natural Science Foundation of China
   [61976040, U19B2039]; National Science Foundation of USA [OIA-1946231];
   Science and Technology Foundation of Dalian [2018J11CY010]
FX This work was supported in part by Ministry of Science and Technology of
   the People's Republic of China under Grant 2018AAA0102003, in part by
   the National Natural Science Foundation of China under Grant 61976040,
   in part by National Natural Science Foundation of China under Grant
   U19B2039, in part by National Science Foundation of USA OIA-1946231, and
   in part by Science and Technology Foundation of Dalian under Grant
   2018J11CY010.
CR [Anonymous], 2018, P 34 INT C MACH LEAR
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen YB, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P167
   Conneau A, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P1107
   David B., 2017, ARXIV170310717
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diederik K., 2013, AUTOENCODING VARIATI
   Faghri F., VSE IMPROVED VISUAL
   Gehring J., 2017, P MACHINE LEARNING R, P1243
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He H., 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, P1576, DOI 10.18653/v1/D15-1181
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Hu BT, 2014, ADV NEUR IN, V27
   Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jason W., 2015, P INT C LEARN REPR
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Kim Y., 2014, P 2014 C EMPIRICAL M
   Kim Y, 2016, AAAI CONF ARTIF INTE, P2741
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu WY, 2017, PROC CVPR IEEE, P6738, DOI 10.1109/CVPR.2017.713
   Liu WY, 2016, PR MACH LEARN RES, V48
   Mehdi M., 2014, CONDITIONAL GENERATI
   Nam S, 2018, ADV NEUR IN, V31
   Odena A, 2017, PR MACH LEARN RES, V70
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T, 2016, ADV NEUR IN, V29
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tobias H., 2019, P INT C LEARN REPR
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiang Z., 2015, TEXT UNDERSTANDING S
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang X, 2015, ADV NEUR IN, V28
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zheng Z., DUAL PATH CONVOLUTIO
   Zhu FD, 2019, PROC CVPR IEEE, P11380, DOI 10.1109/CVPR.2019.01165
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 49
TC 16
Z9 21
U1 6
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 832
EP 845
DI 10.1109/TMM.2021.3060291
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100025
DA 2024-07-18
ER

PT J
AU Wang, YM
   He, WG
AF Wang, Yaomin
   He, Wenguang
TI High Capacity Reversible Data Hiding in Encrypted Image Based on
   Adaptive MSB Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cryptography; Encryption; Data mining; Correlation; Distortion;
   Biomedical imaging; Error analysis; Adaptive most significant bit
   prediction; image encryption; multimedia security; real reversibility;
   reversible data hiding
ID HISTOGRAM-MODIFICATION; DIFFERENCE
AB Reversible data hiding in encrypted image (RDHEI) is a technique that can be adopted by cloud sever to embed additional data into the encrypted image with no permanent distortion. For RDHEI, it remains a challenging task to improve the embedding capacity under the premise of real reversibility. In this paper, a novel RDHEI method based on adaptive most significant bit (MSB) prediction is proposed. The cover image is first encrypted in block-wise manner such that the correlation of pixels within block is preserved. Next, all blocks are permuted to fulfill the final encryption. During data embedding, the upper-left pixel within block is used to predict others such that the embedding room is vacated. Then, available blocks are selected and all blocks are rearranged so as to ensure reversibility. By fully exploiting the correlation of pixels within block via adaptive MSB prediction, the proposed method successes to achieve desirable improvement in capacity. Experimental results show that the proposed method significantly outperforms previous methods. Moreover, real reversibility and real separability are also guaranteed. In a word, the proposed method is a practical method that can be adopted by cloud storage.
C1 [Wang, Yaomin; He, Wenguang] Guangdong Med Univ, Sch Biomed Engn, Guangzhou 524023, Guangdong, Peoples R China.
C3 Guangdong Medical University
RP He, WG (corresponding author), Guangdong Med Univ, Sch Biomed Engn, Guangzhou 524023, Guangdong, Peoples R China.
EM 249668530@qq.com; 56207403@qq.com
OI , Yaomin/0000-0003-4696-0907; he, wenguang/0000-0003-1051-389X
FU National Natural Science Foundation of China [61802074]; Guangdong Basic
   and Applied Basic Research Foundation [2020A1515010760]; Medical
   Scientific Research Fund of Guangdong Province, China [A2018186]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 61802074, in part by Guangdong Basic and Applied
   Basic Research Foundation under Grant 2020A1515010760, and in part
   byMedical Scientific Research Fund of Guangdong Province, China under
   Grant A2018186. The associate editor coordinating the review of this
   manuscript and approving it for publication was Pro. Marco Grangetto.
CR Bhardwaj R, 2020, PATTERN RECOGN LETT, V139, P60, DOI 10.1016/j.patrec.2018.01.014
   He WG, 2018, INFORM SCIENCES, V467, P784, DOI 10.1016/j.ins.2018.04.088
   He WG, 2016, J VIS COMMUN IMAGE R, V40, P459, DOI 10.1016/j.jvcir.2016.07.014
   Hong W, 2012, IEEE SIGNAL PROC LET, V19, P199, DOI 10.1109/LSP.2012.2187334
   Hu YJ, 2008, IEEE T MULTIMEDIA, V10, P1500, DOI 10.1109/TMM.2008.2007341
   Jiang RQ, 2018, IEEE T MULTIMEDIA, V20, P55, DOI 10.1109/TMM.2017.2723244
   Karim MSA, 2014, SIGNAL PROCESS, V94, P174, DOI 10.1016/j.sigpro.2013.06.014
   Khanam FTZ, 2016, INT CONF UBIQ FUTUR, P869, DOI 10.1109/ICUFN.2016.7537160
   Li XL, 2013, IEEE T IMAGE PROCESS, V22, P2181, DOI 10.1109/TIP.2013.2246179
   Liao X, 2015, J VIS COMMUN IMAGE R, V28, P21, DOI 10.1016/j.jvcir.2014.12.007
   Lin CC, 2015, MULTIMED TOOLS APPL, V74, P3823, DOI 10.1007/s11042-013-1801-5
   Lin CC, 2015, INFORM SCIENCES, V293, P314, DOI 10.1016/j.ins.2014.08.057
   Ma KD, 2013, IEEE T INF FOREN SEC, V8, P553, DOI 10.1109/TIFS.2013.2248725
   Mathews LR, 2014, 2014 INTERNATIONAL CONFERENCE ON CONTROL, INSTRUMENTATION, COMMUNICATION AND COMPUTATIONAL TECHNOLOGIES (ICCICCT), P937, DOI 10.1109/ICCICCT.2014.6993092
   Ou B, 2016, J VIS COMMUN IMAGE R, V39, P12, DOI 10.1016/j.jvcir.2016.05.005
   Puech W, 2008, PROC SPIE, V6819, DOI 10.1117/12.766754
   Puteaux P, 2018, IEEE T INF FOREN SEC, V13, P1670, DOI 10.1109/TIFS.2018.2799381
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Tai WL, 2009, IEEE T CIRC SYST VID, V19, P904, DOI 10.1109/TCSVT.2009.2017409
   Nguyen TS, 2016, SIGNAL PROCESS-IMAGE, V44, P84, DOI 10.1016/j.image.2016.03.010
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wu XT, 2014, SIGNAL PROCESS, V104, P387, DOI 10.1016/j.sigpro.2014.04.032
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zhang WM, 2013, IEEE T IMAGE PROCESS, V22, P2775, DOI 10.1109/TIP.2013.2257814
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
   Zhang XP, 2011, IEEE SIGNAL PROC LET, V18, P255, DOI 10.1109/LSP.2011.2114651
   Zhao ZF, 2011, AEU-INT J ELECTRON C, V65, P814, DOI 10.1016/j.aeue.2011.01.014
NR 28
TC 26
Z9 29
U1 8
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1288
EP 1298
DI 10.1109/TMM.2021.3062699
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200004
DA 2024-07-18
ER

PT J
AU Yan, ZY
   Zhang, RM
   Zhang, HZ
   Zhang, QF
   Zuo, WM
AF Yan, Zhaoyi
   Zhang, Ruimao
   Zhang, Hongzhi
   Zhang, Qingfu
   Zuo, Wangmeng
TI Crowd Counting Via Perspective-Guided Fractional-Dilation Convolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolution; Computer architecture; Estimation; Feature extraction;
   Kernel; Computer science; Annotations; Surveillance; neural network;
   supervised learning
AB Crowd counting is critical for numerous video surveillance scenarios. One of the main issues in this task is how to handle the dramatic scale variations of pedestrians caused by the perspective effect. To address this issue, this paper proposes a novel convolution neural network-based crowd counting method, termed Perspective-guided Fractional-Dilation Network (PFDNet). By modeling the continuous scale variations, the proposed PFDNet is able to select the proper fractional-dilation kernels for adapting to different spatial locations. It significantly improves the flexibility of the state-of-the-arts that only consider the discrete representative scales. In addition, by avoiding the multi-scale or multi-column architecture that used in other methods, it is computationally more efficient. In practice, the proposed PFDNet is constructed by stacking multiple Perspective-guided Fractional-Dilation Convolutions (PFC) on a VGG16-BN backbone. By introducing a novel generalized dilation convolution operation, the PFC can handle fractional dilation ratios in the spatial domain under the guidance of perspective annotations, achieving continuous scales modeling of pedestrians. To deal with the problem of unavailable perspective information in some cases, we further introduce an effective perspective estimation branch to the proposed PFDNet, which can be trained in either supervised or weakly-supervised setting once the branch has been pre-trained. Extensive experiments show that the proposed PFDNet outperforms state-of-the-art methods on ShanghaiTech A, ShanghaiTech B, WorldExpo'10, UCF-QNRF, UCF_CC_50 and TRANCOS dataset, achieving MAE 53.8, 6.5, 6.8, 84.3205.8, and 3.06 respectively.
C1 [Yan, Zhaoyi; Zhang, Hongzhi; Zhang, Qingfu; Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Zhang, Ruimao] Chinese Univ Hong Kong, Sch Data Sci, Hong Kong, Peoples R China.
   [Zhang, Ruimao] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.
   [Zhang, Qingfu] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Zhang, Qingfu] City Univ Hong Kong, Shenzhen Res Inst, Shenzhen 518057, Peoples R China.
C3 Harbin Institute of Technology; Chinese University of Hong Kong;
   Shenzhen Research Institute of Big Data; City University of Hong Kong;
   City University of Hong Kong; Shenzhen Research Institute, City
   University of Hong Kong
RP Zhang, HZ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM yanzhaoyi@outloolc.xm; ruimao.zhang@ieee.org; zhanghz@hit.edu.cn;
   qingfu.zhang@cityu.edu.hk; cswmzuo@hit.edu.cn
RI Zuo, Wangmeng/B-3701-2008; Zhang, Hongzhi/HHS-0345-2022
OI Zuo, Wangmeng/0000-0002-3330-783X; Zhang, Ruimao/0000-0001-9511-7532
FU National Natural Scientific Foundation of China (NSFC) [U19A2073,
   61872118]
FX This work was supported by the National Natural Scientific Foundation of
   China (NSFC) under Grants U19A2073 and 61872118.
CR Cao XK, 2018, LECT NOTES COMPUT SC, V11209, P757, DOI 10.1007/978-3-030-01228-1_45
   Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569
   Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191
   Chen K, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.21
   Cheng ZQ, 2019, IEEE I CONF COMP VIS, P6151, DOI 10.1109/ICCV.2019.00625
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   DAVIES AC, 1995, ELECTRON COMMUN ENG, V7, P37, DOI 10.1049/ecej:19950106
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Guerrero-Gómez-Olmedo R, 2015, LECT NOTES COMPUT SC, V9117, P423, DOI 10.1007/978-3-319-19390-8_48
   HE DC, 1990, IEEE T GEOSCI REMOTE, V28, P509
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang SY, 2018, IEEE T IMAGE PROCESS, V27, P1049, DOI 10.1109/TIP.2017.2740160
   Idrees H, 2018, LECT NOTES COMPUT SC, V11206, P544, DOI 10.1007/978-3-030-01216-8_33
   Idrees H, 2013, PROC CVPR IEEE, P2547, DOI 10.1109/CVPR.2013.329
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Jiang XH, 2020, PROC CVPR IEEE, P4705, DOI 10.1109/CVPR42600.2020.00476
   Jiang XH, 2021, IEEE T MULTIMEDIA, V23, P443, DOI 10.1109/TMM.2020.2980945
   Jiang XL, 2019, PROC CVPR IEEE, P6126, DOI 10.1109/CVPR.2019.00629
   Kingma D. P., 2014, arXiv
   Lempitsky V., 2010, P ADV NEUR INF PROC, V23, P1, DOI DOI 10.5555/2997189.2997337
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Liang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P164, DOI 10.1007/978-3-030-58607-2_10
   Liu J, 2018, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2018.00545
   Liu L., IEEE T MULTIMEDIA, V23, P2021
   Liu LB, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P849
   Liu N, 2019, PROC CVPR IEEE, P3220, DOI 10.1109/CVPR.2019.00334
   Liu WZ, 2019, PROC CVPR IEEE, P5094, DOI 10.1109/CVPR.2019.00524
   Liu XL, 2018, PROC CVPR IEEE, P7661, DOI 10.1109/CVPR.2018.00799
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu YT, 2019, PROC CVPR IEEE, P6462, DOI 10.1109/CVPR.2019.00663
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Luo A., 2020, AAAI CONF ARTIF INTE, P11693
   Ma ZH, 2019, IEEE I CONF COMP VIS, P6141, DOI 10.1109/ICCV.2019.00624
   Marana AN, 1998, SIBGRAPI '98 - INTERNATIONAL SYMPOSIUM ON COMPUTER GRAPHICS, IMAGE PROCESSING, AND VISION, PROCEEDINGS, P354, DOI 10.1109/SIBGRA.1998.722773
   Meng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3401, DOI 10.1109/CVPR.2011.5995698
   Mostajabi M, 2018, PROC CVPR IEEE, P5629, DOI 10.1109/CVPR.2018.00590
   Paszke A, 2019, ADV NEUR IN, V32
   Ranjan V, 2018, LECT NOTES COMPUT SC, V11211, P278, DOI 10.1007/978-3-030-01234-2_17
   Ryan D, 2009, 2009 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA 2009), P81, DOI 10.1109/DICTA.2009.22
   Sam DB, 2021, IEEE T PATTERN ANAL, V43, P2739, DOI 10.1109/TPAMI.2020.2974830
   Sam DB, 2017, PROC CVPR IEEE, P4031, DOI 10.1109/CVPR.2017.429
   Shen Z, 2018, PROC CVPR IEEE, P5245, DOI 10.1109/CVPR.2018.00550
   Shi ML, 2019, 2019 1ST INTERNATIONAL CONFERENCE ON INDUSTRIAL ARTIFICIAL INTELLIGENCE (IAI 2019), DOI 10.1109/iciai.2019.8850794
   Shi ZL, 2019, IEEE I CONF COMP VIS, P4199, DOI 10.1109/ICCV.2019.00430
   Shi ZL, 2018, PROC CVPR IEEE, P5382, DOI 10.1109/CVPR.2018.00564
   Simonyan K., 2014, CORR
   Sindagi VA, 2019, IEEE I CONF COMP VIS, P1002, DOI 10.1109/ICCV.2019.00109
   Sindagi VA, 2020, IEEE T IMAGE PROCESS, V29, P323, DOI 10.1109/TIP.2019.2928634
   Sindagi VA, 2017, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2017.206
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wan J, 2022, IEEE T PATTERN ANAL, V44, P1357, DOI 10.1109/TPAMI.2020.3022878
   Wan J, 2019, IEEE I CONF COMP VIS, P1130, DOI 10.1109/ICCV.2019.00122
   Wan Jia, 2020, ADV NEURAL INF PROCE, V33
   Wang B., 2020, ADV NEURAL INFORM PR
   Wang C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1299, DOI 10.1145/2733373.28063370-12345-67-8/90/01
   Wang G, 2018, IEEE T MULTIMEDIA, V20, P2921, DOI 10.1109/TMM.2018.2829163
   Wang M, 2018, IEEE T MULTIMEDIA, V20, P620, DOI 10.1109/TMM.2017.2748459
   Wang Q, 2019, PROC CVPR IEEE, P8190, DOI 10.1109/CVPR.2019.00839
   Wu B, 2005, IEEE I CONF COMP VIS, P90
   Xiong HP, 2019, IEEE I CONF COMP VIS, P8361, DOI 10.1109/ICCV.2019.00845
   Xiyang Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P241, DOI 10.1007/978-3-030-58586-0_15
   Xu CF, 2019, IEEE I CONF COMP VIS, P8381, DOI 10.1109/ICCV.2019.00847
   Yan ZY, 2019, IEEE I CONF COMP VIS, P952, DOI 10.1109/ICCV.2019.00104
   Yang YF, 2021, IEEE T IMAGE PROCESS, V30, P1395, DOI 10.1109/TIP.2020.3043122
   Yutao Hu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P747, DOI 10.1007/978-3-030-58542-6_45
   Zhang AR, 2019, IEEE I CONF COMP VIS, P6787, DOI 10.1109/ICCV.2019.00689
   Zhang A, 2019, IEEE I CONF COMP VIS, P5713, DOI 10.1109/ICCV.2019.00581
   Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684
   Zhang L, 2018, IEEE WINT CONF APPL, P1113, DOI 10.1109/WACV.2018.00127
   Zhang YY, 2016, PROC CVPR IEEE, P589, DOI 10.1109/CVPR.2016.70
NR 71
TC 25
Z9 25
U1 2
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2633
EP 2647
DI 10.1109/TMM.2021.3086709
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1F3XL
UT WOS:000795103700003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yuan, ZK
   Cheng, K
   Tang, JH
   Yang, X
AF Yuan, Zikang
   Cheng, Ken
   Tang, Jinhui
   Yang, Xin
TI RGB-D DSO: Direct Sparse Odometry With RGB-D Cameras for Indoor Scenes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Optimization; Simultaneous localization and mapping; Feature
   extraction; Three-dimensional displays; Real-time systems; Calibration;
   Robot sensing systems; robotics and automation; robots; simultaneous
   localization and mapping
ID SLAM
AB Visual odometry (VO) is a fundamental technique for many robotics and augmented reality (AR) applications. However, most existing RGB-D VO systems suffer from large performance degradation when large occlusions are present and/or a large portion of depth values are invalid due to the limited range of an RGB-D camera, prohibiting the usage of most systems in practical applications. To address above two problems, we present RGB-D DSO, an RGB-D direct sparse odometry with the core part being sliding-window optimization with occlusion removal and a depth refinement module. Occlusion removal excludes negative effects arising from occluded objects when minimizing the final energy function for camera pose tracking. Depth refinement ensures sufficient valid depth values uniformly distributed for the depth map of a keyframe. Experimental results on three public datasets demonstrate that our method achieves smaller tracking error than most existing state-of-the-art methods. Meanwhile, our system takes only 21.93 ms to track a frame, which is faster than most existing methods.
C1 [Yuan, Zikang] Huazhong Univ Sci & Technol, Inst Artificial Intelligence, Wuhan 430074, Peoples R China.
   [Cheng, Ken; Yang, Xin] Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Tang, Jinhui] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210093, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology; Nanjing University of Science & Technology
RP Yang, X (corresponding author), Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM yzk2020@hust.edu.cn; kencheng@hust.edu.cn; jinhuitang@njust.edu.cn;
   xinyang2014@hust.edu.cn
RI Tang, Jinhui/KBR-0891-2024; Yuan, Zikang/IXN-0401-2023
FU National Natural Science Foundation of China [62122029, 61872417,
   6201101073, 62061160490]; project of Wuhan Science and Technology Bureau
   [2020010601012167]
FX of publication 22 September 2021; date of current version 9 August 2022.
   This work was supported in part by the National Natural Science
   Foundation of China under Grants 62122029, 61872417, 6201101073, and
   62061160490, and in part by the project of Wuhan Science and Technology
   Bureau under Grant 2020010601012167. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Federica Battisti.
CR Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Engel J, 2013, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2013.183
   Gálvez-López D, 2012, IEEE T ROBOT, V28, P1188, DOI 10.1109/TRO.2012.2197158
   Ghaffari M, ARXIV190402266, V2019
   Handa A, 2014, IEEE INT CONF ROBOT, P1524, DOI 10.1109/ICRA.2014.6907054
   Kerl C, 2013, IEEE INT C INT ROBOT, P2100, DOI 10.1109/IROS.2013.6696650
   Kerl C, 2013, IEEE INT CONF ROBOT, P3748, DOI 10.1109/ICRA.2013.6631104
   Lai K, 2014, IEEE INT CONF ROBOT, P3050, DOI 10.1109/ICRA.2014.6907298
   Lin T.-Y., 2019, ARXIV
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P664, DOI 10.1109/TMM.2018.2863604
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Piao JC, 2019, IEEE T MULTIMEDIA, V21, P2827, DOI 10.1109/TMM.2019.2913324
   Schöps T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Whelan T, 2016, INT J ROBOT RES, V35, P1697, DOI 10.1177/0278364916669237
   Whelan T, 2015, INT J ROBOT RES, V34, P598, DOI 10.1177/0278364914551008
   Yuan ZK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1410, DOI 10.1145/3343031.3351079
NR 19
TC 6
Z9 6
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4092
EP 4101
DI 10.1109/TMM.2021.3114546
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400030
DA 2024-07-18
ER

PT J
AU Zhao, J
   Qi, WZ
   Zhou, WG
   Duan, N
   Zhou, M
   Li, HQ
AF Zhao, Jian
   Qi, Weizhen
   Zhou, Wengang
   Duan, Nan
   Zhou, Ming
   Li, Houqiang
TI Conditional Sentence Generation and Cross-Modal Reranking for Sign
   Language Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Assistive technology; Videos; Gesture recognition; Feature extraction;
   Task analysis; Linguistics; Training; Sign language translation;
   conditional sentence generation; cross-modal reranking
ID RECOGNITION; FRAMEWORK
AB Sign Language Translation (SLT) aims to generate spoken language translations from sign language videos. Currently, the available sign language datasets are relatively too small to learn the linguistic properties of spoken language. In this paper, towards effective SLT, we propose a novel framework which takes the advantage of the spoken language grammar learnt from a large corpus of text sentences. Our framework consists of three key modules: word existence verification, conditional sentence generation and cross-modal re-ranking. We first check the existence of words in the vocabulary by a series of binary classification in parallel. After that, the appearing words are assembled and guided by a pretrained spoken language generator to produce multiple candidate sentences in spoken language manner. Last but not least, we select the sentence most semantically similar to the input sign video as the translation result with a crossmodal re-ranking model. We evaluate our framework on two large scale continuous SLT benchmarks, i.e., CSL and RWTHPHOENIX-Weather 2014 T. Experimental results demonstrate that the proposed framework achieves promising performance on both datasets.
C1 [Zhao, Jian; Qi, Weizhen; Zhou, Wengang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Duan, Nan; Zhou, Ming] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; Microsoft Research Asia
RP Zhou, WG; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM zj140@mail.ustc.edu.cn; weizheu@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   nanduan@microsoft.com; mingzhou@microsoft.com; lihq@ustc.edu.cn
RI Duan, Nan/AAR-2231-2020; ZHOU, MING/JVP-2920-2024; Li, Houqiang
   Li/B-6259-2013; Zhao, Jian/GXW-1743-2022
OI Duan, Nan/0000-0002-3241-0907; Zhao, Jian/0000-0003-4895-990X; Duan,
   Nan/0000-0002-3387-4674
FU National Key R&D Program of China [2017YFB1002202]; National Natural
   Science Foundation of China [U20A20183, 62021001]; Youth Innovation
   Promotion Association CAS [2018497]; GPU cluster built by MCC Laboratory
   of Information Science and Technology Institution, USTC
FX This work was supported in part by the National Key R&D Program of China
   under contract 2017YFB1002202, in part by theNational Natural Science
   Foundation of China under Contracts U20A20183 and 62021001, and in part
   by the Youth Innovation Promotion Association CAS under Grant 2018497.
   It was also supported by the GPU cluster built by MCC Laboratory of
   Information Science and Technology Institution, USTC.
CR [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   ausl, US
   awhamburg, about us
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Buehler P, 2009, PROC CVPR IEEE, P2953, DOI 10.1109/CVPRW.2009.5206523
   Camgoz N. C., 2020, P IEEE C COMP VIS PA, P100
   Camgoz NC, 2017, IEEE I CONF COMP VIS, P3075, DOI 10.1109/ICCV.2017.332
   Camgoz NC, 2018, PROC CVPR IEEE, P7784, DOI 10.1109/CVPR.2018.00812
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chai X., 2013, IEEE C AFGR, V655, P4
   Chen SZ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1838, DOI 10.1145/3123266.3123420
   Cheng W-F, 2018, ARXIV PREPRINT ARXIV
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho Kyunghyun, 2014, SYNTAX SEMANTICS STR, P5, DOI [10.3115/v1/w14-4012, 10.3115 /v1/D14-1179, DOI 10.3115/V1/D14-1179]
   Cooper H, 2009, PROC CVPR IEEE, P2560
   Cui RP, 2019, IEEE T MULTIMEDIA, V21, P1880, DOI 10.1109/TMM.2018.2889563
   Cui RP, 2017, PROC CVPR IEEE, P1610, DOI 10.1109/CVPR.2017.175
   Dan W., 2017, LANG DOCUMENTATION C, V1, P17
   De-An Huang, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9908, P137, DOI 10.1007/978-3-319-46493-0_9
   Devlin J., 2018, BERT PRE TRAINING DE
   Dong JF, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1082, DOI 10.1145/2964284.2984064
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P751
   Guo D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P744
   Guo D, 2018, AAAI CONF ARTIF INTE, P6845
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hori T, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P518, DOI 10.18653/v1/P17-1048
   Huang J, 2018, AAAI CONF ARTIF INTE, P2257
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Koller O, 2020, IEEE T PATTERN ANAL, V42, P2306, DOI 10.1109/TPAMI.2019.2911077
   Koller O, 2018, INT J COMPUT VISION, V126, P1311, DOI 10.1007/s11263-018-1121-3
   Koller O, 2017, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2017.364
   Koller O, 2016, PROC CVPR IEEE, P3793, DOI 10.1109/CVPR.2016.412
   Koller O, 2015, COMPUT VIS IMAGE UND, V141, P108, DOI 10.1016/j.cviu.2015.09.013
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lewis M, 2020, P 58 ANN M ASS COMP, P7871, DOI DOI 10.18653/V1/2020.ACL-MAIN.703
   Li JN, 2019, AAAI CONF ARTIF INTE, P8618
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Liu MY, 2018, IEEE T MULTIMEDIA, V20, P1932, DOI 10.1109/TMM.2017.2786868
   Liu ZW, 2017, IEEE I CONF COMP VIS, P4473, DOI [10.1109/ICCVW.2017.361, 10.1109/ICCV.2017.478]
   Orbay A, 2020, IEEE INT CONF AUTOMA, P222, DOI 10.1109/FG47880.2020.00002
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7
   Pu JF, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P885
   Pu JF, 2019, PROC CVPR IEEE, P4160, DOI 10.1109/CVPR.2019.00429
   Qi WZ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2401
   Raffel C., 2019, J Mach Learn Res, V21, P1, DOI DOI 10.48550/ARXIV.1910.10683
   Schembri A, 2013, LANG DOC CONSERV, V7, P136
   Simonyan K., 2014, CORR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Teichmann M, 2019, PROC CVPR IEEE, P5104, DOI 10.1109/CVPR.2019.00525
   Vaswani A, 2017, ADV NEUR IN, V30
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Wang H., 2016, ACM T ACCESS COMPUT, V8, P1, DOI DOI 10.1145/2897735
   Wang HJ, 2019, IEEE T MULTIMEDIA, V21, P2806, DOI 10.1109/TMM.2019.2915032
   Wang LC, 2014, IEEE T MULTIMEDIA, V16, P751, DOI 10.1109/TMM.2014.2298382
   Wang S, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1483, DOI 10.1145/3240508.3240671
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Yang ZL, 2019, ADV NEUR IN, V32
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yin Kayo, 2020, ARXIV200400588
   Zhang J, 2019, IEEE T MULTIMEDIA, V21, P221, DOI 10.1109/TMM.2018.2844689
   Zhang ZH, 2019, IEEE IMAGE PROC, P285, DOI [10.1109/ICIP.2019.8802972, 10.1109/icip.2019.8802972]
   Zhou H, 2020, AAAI CONF ARTIF INTE, V34, P13009
NR 67
TC 11
Z9 11
U1 2
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2662
EP 2672
DI 10.1109/TMM.2021.3087006
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1F3XL
UT WOS:000795103700004
DA 2024-07-18
ER

PT J
AU Chen, HF
   Jiang, DM
   Sahli, H
AF Chen, Haifeng
   Jiang, Dongmei
   Sahli, Hichem
TI Transformer Encoder With Multi-Modal Multi-Head Attention for Continuous
   Affect Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Context modeling; Feature extraction; Correlation;
   Computational modeling; Visualization; Redundancy; Multi modal affective
   state recognition; self-attention; temporal dependency; multi-modal
   multi-head attention; inter-modality interaction
ID EMOTION RECOGNITION; NETWORKS; MEMORY
AB Continuous affect recognition is becoming an increasingly attractive research topic in affective computing. Previous works mainly focused on modelling the temporal dependency within a sensor modality, or adopting early or late fusion for multi-modal affective state recognition. However, early fusion suffers from the curse of dimensionality, and late fusion ignores the complementarity and redundancy between multiple modal streams. In this paper, we first introduce the transformer-encoder with a self-attention mechanism and propose a Convolutional Neural Network-Transformer Encoder (CNN-TE) framework to model the temporal dependency for single modal affect recognition. Further, to effectively consider the complementarity and redundancy between multiple streams we propose a Transformer Encoder with Multi-modal Multi-head Attention (TEMMA) for multi-modal affect recognition. TEMMA allows to progressively and simultaneously refine the inter-modality interactions and intra-modality temporal dependency. The learned multi-modal representations are fed to an Inference Sub-network with fully connected layers to estimate the affective state. The proposed framework is trained in a nutshell and demonstrates its effectiveness on the AVEC2016 and AVEC2019 datasets. Compared to state-of-the-art models, our approach obtains remarkable improvements on both arousal and valence in terms of concordance correlation coefficient (CCC) reaching 0.583 for arousal and 0.564 for valence on the AVEC2019 test set.
C1 [Chen, Haifeng; Jiang, Dongmei] Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big, NPU VUB Joint AVSP Res Lab, Sch Comp Sci,Shaanxi Key Lab Speech & Image Infor, Xian 710072, Peoples R China.
   [Chen, Haifeng; Jiang, Dongmei] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Sahli, Hichem] Vrije Univ Brussel, Dept Elect & Informat, VUB NPU Joint AVSP Res Lab, B-1050 Brussels, Belgium.
   [Sahli, Hichem] Interuniv Microelect Ctr, B-3001 Heverlee, Belgium.
C3 Northwestern Polytechnical University; Peng Cheng Laboratory; Vrije
   Universiteit Brussel; Interuniversity Microelectronics Centre
RP Jiang, DM (corresponding author), Northwestern Polytech Univ, Natl Engn Lab Integrated Aerosp Ground Ocean Big, NPU VUB Joint AVSP Res Lab, Sch Comp Sci,Shaanxi Key Lab Speech & Image Infor, Xian 710072, Peoples R China.
EM chenhaifeng2018@mail.nwpu.edu.cn; jiangdm@nwpu.edu.cn; hsahli@etrovub.be
RI 陈, 海丰/IUQ-4329-2023
OI Chen, Haifeng/0000-0001-7164-5840; Sahli, Hichem/0000-0002-1774-2970;
   Jiang, Dongmei/0000-0002-6238-8499
FU Shaanxi Provincial Key RD Program [2017KW-ZD-14]; 111 Project [B16039];
   Flemish Government (AI Research Program)
FX This work was supported in part by the Shaanxi Provincial Key R&D
   Program under Grant 2017KW-ZD-14, in part by 111 Project under Grant
   B16039, and in part by Flemish Government (AI Research Program). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Elisa Ricci.
CR [Anonymous], 2014, AVEC 14 P 4 INT WORK
   [Anonymous], 2015, P 5 INT WORKSH AUD V, DOI DOI 10.1145/2808196.2811642
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI DOI 10.18653/V1/D15-1166
   [Anonymous], 2019, P 9 INT AUD VIS EM C, P3, DOI DOI 10.1145/3347320.3357688
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Bai S., 2018, EMPIRICAL EVALUATION
   Brady K, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P97, DOI 10.1145/2988257.2988264
   Chao L, 2015, P 5 INT WORKSH AUD V, P65, DOI [DOI 10.1145/2808196.2811634, 10.1145/2808196.2811634]
   Chen H., 2019, P 9 INT AUD VIS EM C, P19
   Chen S., 2017, P 7 ANN WORKSH AUD V, P19
   Chen T., 2020, ARXIV200205709
   Dang T., 2017, P 7 ANN WORKSHOP AUD, P27
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Du ZY, 2021, IEEE T AFFECT COMPUT, V12, P565, DOI 10.1109/TAFFC.2019.2940224
   Ekman P., 2005, SER SERIES AFFECTIVE, V2nd, DOI [10.1093/acprof:oso/9780195179644.001.0001, DOI 10.1093/ACPROF:OSO/9780195179644.001.0001]
   Gamage KW, 2018, PROCEEDINGS OF THE 2018 AUDIO/VISUAL EMOTION CHALLENGE AND WORKSHOP (AVEC'18), P47, DOI 10.1145/3266302.3266314
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Han J, 2017, IMAGE VISION COMPUT, V65, P76, DOI 10.1016/j.imavis.2016.11.020
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Lang, 2015, P 5 INT WORKSH AUD V
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang J., 2017, P 7 ANN WORKSH AUD V, P11
   Huang J, 2020, INT CONF ACOUST SPEE, P3507, DOI [10.1109/icassp40776.2020.9053762, 10.1109/ICASSP40776.2020.9053762]
   Huang J, 2018, PROCEEDINGS OF THE 2018 AUDIO/VISUAL EMOTION CHALLENGE AND WORKSHOP (AVEC'18), P57, DOI 10.1145/3266302.3266304
   Huang Z, 2015, P 5 INT WORKSH AUD V, P41, DOI 10.1145/2808196.2811640
   Le H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5612
   Kaya H., 2019, P 9 INT AUDIOVISUAL, P27
   Khorram S, 2017, INTERSPEECH, P1253, DOI 10.21437/Interspeech.2017-548
   Kolen JF, 2009, A Field Guide to Dynamical Recurrent Networks, DOI [DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037]
   Kossaifi J, 2021, IEEE T PATTERN ANAL, V43, P1022, DOI 10.1109/TPAMI.2019.2944808
   Liang PP, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/3242969.3243019
   McKeown G, 2012, IEEE T AFFECT COMPUT, V3, P5, DOI 10.1109/T-AFFC.2011.20
   Mirsamadi S, 2017, INT CONF ACOUST SPEE, P2227, DOI 10.1109/ICASSP.2017.7952552
   Mishra N., 2017, META LEARNING TEMPOR
   Nicolaou MA, 2011, IEEE T AFFECT COMPUT, V2, P92, DOI 10.1109/T-AFFC.2011.9
   Pantic M, 2003, P IEEE, V91, P1370, DOI 10.1109/JPROC.2003.817122
   Pantic M, 2007, LECT NOTES COMPUT SC, V4451, P47
   Pei EC, 2019, MULTIMED TOOLS APPL, V78, P19387, DOI 10.1007/s11042-019-7313-1
   Poria S, 2017, INFORM FUSION, V37, P98, DOI 10.1016/j.inffus.2017.02.003
   Ringeval F, 2013, IEEE INT CONF AUTOMA
   Ringeval F, 2018, PROCEEDINGS OF THE 2018 AUDIO/VISUAL EMOTION CHALLENGE AND WORKSHOP (AVEC'18), P3, DOI 10.1145/3266302.3266316
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Russell JA, 2003, ANNU REV PSYCHOL, V54, P329, DOI 10.1146/annurev.psych.54.101601.145102
   Sander D, 2005, NEURAL NETWORKS, V18, P317, DOI 10.1016/j.neunet.2005.03.001
   Scherer K., 2000, Neuropsychology of Emotion, V137, P137
   Schuller B, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P361
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P211, DOI 10.1109/T-AFFC.2011.37
   Stappen L, 2019, INT CONF ACOUST SPEE, P6680, DOI 10.1109/ICASSP.2019.8683801
   Sutskever I., 2014, ADV NEURAL INFORM PR, V4, P3104, DOI DOI 10.5555/2969033.2969173
   Tsai Yao-Hung Hubert, 2019, MULTIMODAL TRANSFORM
   Tzirakis P, 2017, IEEE J-STSP, V11, P1301, DOI 10.1109/JSTSP.2017.2764438
   Valstar M., 2013, P 3 ACM INT WORKSHOP, DOI [10.1145/2512530.2512533, DOI 10.1145/2512530.2512533]
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Vaswani A, 2017, ADV NEUR IN, V30
   Wöllmer M, 2013, IMAGE VISION COMPUT, V31, P153, DOI 10.1016/j.imavis.2012.03.001
   Wöllmer M, 2010, IEEE J-STSP, V4, P867, DOI 10.1109/JSTSP.2010.2057200
   Zadeh A., 2019, ARXIV191109826
   Zadeh A., 2017, P 2017 C EMP METH NA, P1103, DOI 10.18653/v1/D17-1115
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zhang ZX, 2019, INT CONF ACOUST SPEE, P6705, DOI 10.1109/ICASSP.2019.8682896
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhao J., 2019, P INT AUD VIS EM CHA, P37
   Zhao JM, 2018, PROCEEDINGS OF THE 2018 AUDIO/VISUAL EMOTION CHALLENGE AND WORKSHOP (AVEC'18), P65, DOI 10.1145/3266302.3266313
NR 67
TC 33
Z9 35
U1 9
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4171
EP 4183
DI 10.1109/TMM.2020.3037496
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900020
DA 2024-07-18
ER

PT J
AU Chen, JY
   Luo, XZ
   Hu, M
   Wu, D
   Zhou, YP
AF Chen, Jinyu
   Luo, Xianzhuo
   Hu, Miao
   Wu, Di
   Zhou, Yipeng
TI Sparkle: User-Aware Viewport Prediction in 360-Degree Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360-degree video; viewport prediction; user behavior analysis
AB In 360-degree video streaming, users commonly watch a video scene within a Field of View (FoV). Such observation provides an opportunity to save bandwidth consumption by predicting and then prefetching video tiles within the FoV. However, existing FoV prediction methods seldom consider the diversity among user behaviors and the impact of different video genres. Thus, previous one-size-fits-all models cannot make accurate prediction for users with different behavior patterns. In this paper, we propose a user-aware viewport prediction algorithm called Sparkle, which is a practical whitebox approach for FoV prediction. Instead of training a single learning model to predict the behaviors for all users, our proposed algorithm is tailored to fit each individual user. In particular, unlike other learning models, our prediction model is completely explainable and all the parameters have their physical meanings. We first conduct a measurement study to analyze real user behaviors and observe that there exists sharp fluctuation of view orientation and user posture has significant impact on the viewport movement of users. Moreover, cross-user similarity is diverse across different video genres. Inspired by these insights, we further design a user-aware viewport prediction algorithm by mimicking a user's viewport movement on the tile map, and determine how a user will change the viewport angle based on his (or her) trajectory and other similar users' behaviors in the past time window. Extensive evaluations with real datasets demonstrate that, our proposed algorithm significantly outperforms the state-of-the-art benchmark methods (e.g., LSTM-based methods) by over 5%, and the prediction accuracy is much more stable on various types of 360-degree videos than previous methods.
C1 [Chen, Jinyu; Luo, Xianzhuo; Hu, Miao; Wu, Di] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
   [Chen, Jinyu; Luo, Xianzhuo; Hu, Miao; Wu, Di] Guangdong Key Lab Big Data Anal & Proc, Guangzhou 510006, Peoples R China.
   [Zhou, Yipeng] Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2112, Australia.
   [Zhou, Yipeng] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Sun Yat Sen University; Macquarie University; Peng Cheng Laboratory
RP Wu, D (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Guangdong, Peoples R China.
EM chenjy585@mail2.sysu.edu.cn; luoxzh3@mail2.sysu.edu.cn;
   humiao@outlook.com; wudi27@mail.sysu.edu.cn; yipeng.zhou@mq.edu.au
RI Wu, Di/HNP-3772-2023; wu, di/IYS-9217-2023
OI Chen, Jinyu/0000-0002-3502-0146; Zhou, Yipeng/0000-0003-1533-0865
FU National Natural Science Foundation of China [U1911201, 61872420,
   61802452, 62072486]; Guangdong Special Support Program [2017TX04X148];
   Project "FANet: PCL Future Greater-Bay Area Network Facilities for
   Large-scale Experiments and Applications" [LZC0019]; Science and
   Technology Program of Guangzhou [202007040006]; ARC [DE180100950]
FX This work was supported by the National Natural Science Foundation of
   China under Grants U1911201, 61872420, 61802452, and 62072486, Guangdong
   Special Support Program under Grant 2017TX04X148, the Project "FANet:
   PCL Future Greater-Bay Area Network Facilities for Large-scale
   Experiments and Applications (LZC0019)", Science and Technology Program
   of Guangzhou under Grants 202007040006 and ARC DE180100950. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sanjeev Mehrotra.
CR Nguyen A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1190, DOI 10.1145/3240508.3240669
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   Ban Y, 2018, SA'18: SIGGRAPH ASIA 2018 EMERGING TECHNOLOGIES, DOI 10.1145/3275476.3275492
   Chakareski J., 2018, 2018 IEEE INT C COMM, P1, DOI DOI 10.1109/ICC.2018.8422859
   Corbillon X, 2017, IEEE ICC, DOI 10.1109/ICC.2017.7996611
   Duanmu F, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P13, DOI 10.1145/3097895.3097898
   Eltobgy O., 2020, IEEE T MULTIMEDIA
   Fan CL, 2020, IEEE T MULTIMEDIA, V22, P744, DOI 10.1109/TMM.2019.2931807
   Fan CL, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329119
   Guan Y, 2019, SIGCOMM '19 - PROCEEDINGS OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P394, DOI 10.1145/3341302.3342063
   Kammachi-Sreedhar K, 2016, IEEE INT SYM MULTIM, P583, DOI [10.1109/ISM.2016.0126, 10.1109/ISM.2016.143]
   Kan NW, 2019, INT CONF ACOUST SPEE, P4030, DOI 10.1109/ICASSP.2019.8683779
   Lai ZQ, 2020, IEEE T MOBILE COMPUT, V19, P1586, DOI 10.1109/TMC.2019.2913364
   Le Feuvre J, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P329, DOI 10.1145/2910017.2910641
   Lo WC, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P211, DOI 10.1145/3083187.3083219
   Mahzari A, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P173, DOI 10.1145/3240508.3240680
   Maniotis P, 2020, IEEE T MULTIMEDIA, V22, P2382, DOI 10.1109/TMM.2019.2957993
   Pang HT, 2019, IEEE INFOCOM SER, P991, DOI 10.1109/INFOCOM.2019.8737395
   Papaioannou G, 2019, PROCEEDINGS OF THE 2019 THE TWENTIETH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING (MOBIHOC '19), P171, DOI 10.1145/3323679.3326515
   Petrangeli S, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P306, DOI 10.1145/3123266.3123453
   Qian F, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P99, DOI 10.1145/3241539.3241565
   Shi Shu., 2019, Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services, P130
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Song JR, 2020, IEEE T MULTIMEDIA, V22, P2366, DOI 10.1109/TMM.2019.2957976
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tzavidas S, 2005, IEEE T MULTIMEDIA, V7, P880, DOI 10.1109/TMM.2005.854430
   Wang DS, 2019, IEEE T SERV COMPUT, V12, P685, DOI 10.1109/TSC.2018.2828426
   Wang G, 2018, IEEE T MULTIMEDIA, V20, P2921, DOI 10.1109/TMM.2018.2829163
   Xianglong Feng, 2019, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V3, DOI 10.1145/3328914
   Xiao MB, 2018, IEEE INFOCOM SER, P953, DOI 10.1109/INFOCOM.2018.8486390
   Xie L, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P564, DOI 10.1145/3240508.3240556
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu YY, 2018, PROC CVPR IEEE, P5333, DOI 10.1109/CVPR.2018.00559
   Zhang C, 2017, IEEE T MULTIMEDIA, V19, P2333, DOI 10.1109/TMM.2017.2743987
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhao PY, 2019, INT CONF ACOUST SPEE, P1657, DOI 10.1109/ICASSP.2019.8682776
   Zhou ML, 2021, IEEE T MULTIMEDIA, V23, P1106, DOI 10.1109/TMM.2020.2992968
   Zhou ML, 2020, IEEE T IMAGE PROCESS, V29, P7603, DOI 10.1109/TIP.2020.3004714
   Zhou ML, 2019, IEEE T MULTIMEDIA, V21, P1921, DOI 10.1109/TMM.2019.2895281
NR 39
TC 17
Z9 19
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3853
EP 3866
DI 10.1109/TMM.2020.3033127
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100033
DA 2024-07-18
ER

PT J
AU Gao, PC
   Lu, K
   Xue, J
   Shao, L
   Lyu, JY
AF Gao, Pengcheng
   Lu, Ke
   Xue, Jian
   Shao, Ling
   Lyu, Jiayi
TI A Coarse-to-Fine Facial Landmark Detection Method Based on
   Self-attention Mechanism
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Feature extraction; Heating systems; Face recognition; Neural
   networks; Computer vision; Training; Convolutional neural network;
   facial landmark detection; self-attention mechanism
ID NETWORK
AB Facial landmark detection in the wild remains a challenging problem in computer vision. Deep learning-based methods currently play a leading role in solving this. However, these approaches generally focus on local feature learning and ignore global relationships. Therefore, in this study, a self-attention mechanism is introduced into facial landmark detection. Specifically, a coarse-to-fine facial landmark detection method is proposed that uses two stacked hourglasses as the backbone, with a new landmark-guided self-attention (LGSA) block inserted between them. The LGSA block learns the global relationships between different positions on the feature map and allows feature learning to focus on the locations of landmarks with the help of a landmark-specific attention map, which is generated in the first-stage hourglass model. A novel attentional consistency loss is also proposed to ensure the generation of an accurate landmark-specific attention map. A new channel transformation block is used as the building block of the hourglass model to improve the model's capacity. The coarse-to-fine strategy is adopted during and between phases to reduce complexity. Extensive experimental results on public datasets demonstrate the superiority of our proposed method against state-of-the-art models.
C1 [Gao, Pengcheng; Lu, Ke; Xue, Jian] Univ Chinese Acad Sci, Sch Engn Sci, Beijing 100049, Peoples R China.
   [Lu, Ke] Foshan Univ, Chancheng Dist 528100, Peoples R China.
   [Shao, Ling] Mohamed Bin Zayed Univ, Incept Inst Artificial Intelligence, Abu Dhabi 144534, U Arab Emirates.
   [Lyu, Jiayi] Capital Normal Univ, Informat Engn Coll, Beijing 100048, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Foshan University; Capital Normal University
RP Xue, J (corresponding author), Univ Chinese Acad Sci, Sch Engn Sci, Beijing 100049, Peoples R China.
EM gaopengcheng15@mails.ucas.ac.cn; luk@ucas.ac.cn; xuejian@ucas.ac.cn;
   ling.shao@ieee.org; lyujiayi_cnu@163.com
RI Shao, Ling/D-3535-2011; pengcheng, gao/HKO-0415-2023; Xue,
   Jian/W-1980-2019
OI Xue, Jian/0000-0002-9460-802X
FU National Key R&D Program of China [2017YFB1002203]; National Natural
   Science Foundation of China [61671426, 61871258, 61929104, 61972375];
   Beijing Natural Science Foundation [4182071]; University of Chinese
   Academy of Sciences [Y95401YXX2]; Key Project of Education Commission of
   Beijing Municipal [KZ201911417048]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002203, in part by the National Natural Science
   Foundation of China under Grants 61671426, 61871258, 61929104, and
   61972375, in part by the Beijing Natural Science Foundation (4182071),
   in part by the University of Chinese Academy of Sciences (Y95401YXX2)
   and in part by the Key Project of Education Commission of Beijing
   Municipal (KZ201911417048).
CR [Anonymous], 2018, BRIT MACH VIS C
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191
   Cao XD, 2012, PROC CVPR IEEE, P2887, DOI 10.1109/CVPR.2012.6248015
   Dapogny A, 2019, IEEE I CONF COMP VIS, P6892, DOI 10.1109/ICCV.2019.00699
   Deng JK, 2019, IEEE T IMAGE PROCESS, V28, P3636, DOI 10.1109/TIP.2019.2899267
   Devito Z., 2017, NEURIPS WORKSHOPS, P1
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dong XY, 2019, IEEE I CONF COMP VIS, P783, DOI 10.1109/ICCV.2019.00087
   Dong XY, 2018, PROC CVPR IEEE, P360, DOI 10.1109/CVPR.2018.00045
   Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Feng ZH, 2017, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2017.392
   Feng ZH, 2015, IEEE SIGNAL PROC LET, V22, P76, DOI 10.1109/LSP.2014.2347011
   Ghiasi G, 2014, PROC CVPR IEEE, P1899, DOI 10.1109/CVPR.2014.306
   Guo J., 2018, ARXIV181201936
   Guo X., 2019, ARXIV190210859
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619
   Hu J., 2018, PROC CVPR IEEE, P7132
   KAZEMI V, 2014, PROC CVPR IEEE, P1867, DOI [DOI 10.1109/CVPR.2014.241, 10.1109/CVPR.2014.241]
   Kowalski M, 2017, IEEE COMPUT SOC CONF, P2034, DOI 10.1109/CVPRW.2017.254
   Kumar A, 2018, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2018.00052
   Li YX, 2019, IEEE INT CON MULTI, P820, DOI 10.1109/ICME.2019.00146
   Liu LB, 2019, IEEE T MULTIMEDIA, V21, P2248, DOI 10.1109/TMM.2019.2902096
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Lv JJ, 2017, PROC CVPR IEEE, P3691, DOI 10.1109/CVPR.2017.393
   Mao QR, 2017, IEEE T MULTIMEDIA, V19, P861, DOI 10.1109/TMM.2016.2629282
   Miao X, 2018, PROC CVPR IEEE, P5040, DOI 10.1109/CVPR.2018.00529
   Mo HY, 2019, IEEE T MULTIMEDIA, V21, P943, DOI 10.1109/TMM.2018.2867262
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Ro Y.M., 2019, IEEE T CIRC SYST VID
   Robinson JP, 2019, IEEE I CONF COMP VIS, P10102, DOI 10.1109/ICCV.2019.01020
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sun K., 2019, P IEEE C COMPUTER VI, DOI DOI 10.48550/ARXIV.1904.04514
   Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446
   Szegedy C., 2015, IEEE T MULTIMEDIA, P1
   Tang ZQ, 2018, LECT NOTES COMPUT SC, V11207, P348, DOI 10.1007/978-3-030-01219-9_21
   Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453
   Valle R, 2018, LECT NOTES COMPUT SC, V11218, P609, DOI 10.1007/978-3-030-01264-9_36
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weng RL, 2016, IEEE T MULTIMEDIA, V18, P2066, DOI 10.1109/TMM.2016.2591508
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Wu WY, 2017, IEEE COMPUT SOC CONF, P2096, DOI 10.1109/CVPRW.2017.261
   Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227
   Wu Y, 2019, INT J COMPUT VISION, V127, P115, DOI 10.1007/s11263-018-1097-z
   Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang J, 2017, IEEE COMPUT SOC CONF, P2025, DOI 10.1109/CVPRW.2017.253
   Zhang J, 2016, PROC CVPR IEEE, P3428, DOI 10.1109/CVPR.2016.373
   Zhang J, 2014, LECT NOTES COMPUT SC, V8690, P1, DOI 10.1007/978-3-319-10605-2_1
   Zhang YT, 2018, PROC CVPR IEEE, P2694, DOI 10.1109/CVPR.2018.00285
   Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7
   Zhiwei L., 2019, ARXIV PREPRINT ARXIV
   Zhu ML, 2019, PROC CVPR IEEE, P3481, DOI 10.1109/CVPR.2019.00360
   Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 60
TC 27
Z9 28
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 926
EP 938
DI 10.1109/TMM.2020.2991507
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300008
DA 2024-07-18
ER

PT J
AU Lebreton, P
   Yamagishi, K
AF Lebreton, Pierre
   Yamagishi, Kazuhisa
TI Predicting User Quitting Ratio in Adaptive Bitrate Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Delays; Loading; Quality of experience; Bit rate;
   Monitoring; Correlation; Adaptive bitrate video streaming; monitoring;
   quitting ratio; user engagement
ID WATCHING TIME; QUALITY
AB To improve user engagement such as viewing time, this paper addresses the understanding and prediction of the user quitting ratio for users watching videos using adaptive bit rate video streaming. The user quitting ratio is defined as the percentage of users still watching videos at a given time. To perform this study, five subjective experiments involving up to 264 participants were conducted in a laboratory setting. Results indicated the effects of coding quality, initial buffering, and midway stalling on user quitting ratio. Then, a framework was defined to predict the user quitting ratio as a function of time. This framework achieves good prediction accuracy and can be used in multiple scenarios including when quality adaptation and stalling occur. Finally, it is suitable for monitoring applications where bitstream are encrypted and low processing cost is required.
C1 [Lebreton, Pierre; Yamagishi, Kazuhisa] NTT Corp, NTT Network Technol Labs, Tokyo 1808585, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Lebreton, P (corresponding author), NTT Corp, NTT Network Technol Labs, Tokyo 1808585, Japan.
EM lebreton.pierre.mz@hco.ntt.co.jp; kazuhisa.yamagishi.vf@hco.ntt.co.jp
RI Yamagishi, Kazuhisa/JXY-6414-2024
OI Yamagishi, Kazuhisa/0000-0001-9219-6351
CR Ahmed Abdelsalam A., 2017, 2017 IEEE Vehicle Power and Propulsion Conference (VPPC). Proceedings, DOI 10.1109/VPPC.2017.8330901
   Akamai, 2012, CISC VIS NETW IND GL
   [Anonymous], 2015, 2015 IEEE INT C DAT
   [Anonymous], 2014, P IEEE INT C MULT EX
   Ayad I, 2018, COMPUT NETW, V133, P90, DOI 10.1016/j.comnet.2018.01.019
   Balachandran A, 2013, ACM SIGCOMM COMP COM, V43, P339, DOI 10.1145/2534169.2486025
   Bampis C. G., 2017, arXiv:1703.00633
   Bin Pan, 2012, 2012 IEEE 12th International Conference on Computer and Information Technology (CIT), P398, DOI 10.1109/CIT.2012.94
   Brunnstrom, 2012, EUR NETW QUAL EXPERI
   CHEN Y, 2015, CHINA J COMMERCE, P101
   Chen YS, 2014, IEEE ICC, P1825, DOI 10.1109/ICC.2014.6883588
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Dobrian F, 2013, COMMUN ACM, V56, P91, DOI 10.1145/2428556.2428577
   Garcia M.-N, 2014, 2014 Sixth International Workshop on Quality of Multimedia Experience (QoMEX), P141, DOI 10.1109/QoMEX.2014.6982310
   Gopalakrishnan V., 2011, P 2011 ACM SIGCOMM C, P225, DOI DOI 10.1145/2068816.2068838
   HE Y, 2015, P INT C CYB EN DISTR, P261
   HWANG KW, 2012, P INT C RES NETW, P45
   ITU-T, 2008, ITU T P910 SUBJECTIV
   ITU-T Recommendation P.1203, 2017, P1203 ITUT, P1
   Krishnan SS, 2013, IEEE ACM T NETWORK, V21, P2001, DOI 10.1109/TNET.2013.2281542
   Lebreton P., 2018, P IEEE INT C MULT EX, P1
   Lebreton P., 2019, P IEEE 21 INT WORKSH, P1
   Lebreton P, 2019, IEICE T COMMUN, VE102B, P2226, DOI 10.1587/transcom.2019EBP3045
   Li Y., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P209, DOI [10.1145/2068816, DOI 10.1145/2068816]
   Li YH, 2012, IEEE ICC, P2093, DOI 10.1109/ICC.2012.6363915
   Li ZG, 2012, PLANT SCI, V185, P185, DOI 10.1016/j.plantsci.2011.10.006
   Miller R.G., 1997, Survival analysis
   Mok R. K. P., 2011, 2011 IFIP/IEEE International Symposium on Integrated Network Management (IM 2011), P485, DOI 10.1109/INM.2011.5990550
   Moldovan C, 2016, 2016 28TH INTERNATIONAL TELETRAFFIC CONGRESS (ITC 28), VOL 1, P103, DOI 10.1109/ITC-28.2016.122
   Nam H, 2016, IEEE INFOCOM SER
   Pinson MH, 2011, IEEE SIGNAL PROC MAG, V28, P60, DOI 10.1109/MSP.2011.942470
   PLISSONNEAU L, 2012, ITC P 24 INT TEL C, P1
   Raca D, 2018, PROCEEDINGS OF THE 28TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'18), P49, DOI 10.1145/3210445.3210457
   Reichl P, 2015, INT WORK QUAL MULTIM, DOI 10.1109/QoMEX.2015.7148138
   Robitza W., 2016, 5 ISCADEGA WORKSHOP, P39
   Robitza W, 2017, BUFFERER INSERTS FAK
   Robitza W., 2016, QOE NET REPORT, P1
   Robitza W, 2018, INT WORK QUAL MULTIM, P324
   Robitza Werner., 2017, PROC 9 INT C QUAL MU, P1
   SACKL A, 2012, P GC 12 WORKSH QUAL
   Sahoo S., 2017, 2017 4 INT C POWER C, P1, DOI [10.1109/ICPCES.2017.8117627, DOI 10.1109/ICPCES.2017.8117627]
   Schulzrinne H, 2014, YOUSLOW WHAT INFLUEN, V44, P1
   Seufert M, 2017, QOMEX, P1
   Seufert M, 2015, IEEE COMMUN SURV TUT, V17, P469, DOI 10.1109/COMST.2014.2360940
   Shafiq M. Zubair, 2014, ACM SIGMETRICS Performance Evaluation Review, V42, P367, DOI 10.1145/2591971.2591975
   Song JK, 2019, IEEE T NEUR NET LEAR, V30, P3047, DOI 10.1109/TNNLS.2018.2851077
   TAKAHASHI S, 2019, QUAL MULTIMEDIA EXPE
   TAN X, 2018, WIREL COMMUN MOB COM, V2018, P1
   Wang XR, 2015, 2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD), P2086, DOI 10.1109/FSKD.2015.7382273
   Wu Siqi, 2018, 12 INT AAAI C WEB SO, P434
   Yamagishi K, 2017, IEEE T MULTIMEDIA, V19, P1545, DOI 10.1109/TMM.2017.2669859
   Yan H, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1109, DOI 10.1145/3132847.3132884
   Yang L, 2017, COMPUT NETW, V126, P256, DOI 10.1016/j.comnet.2017.07.012
   Yeo K, 2018, IEEE DATA MINING, P697, DOI 10.1109/ICDM.2018.00085
   Zhangyan N, 2019, SG12C370R2 ITUI
NR 55
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4526
EP 4540
DI 10.1109/TMM.2020.3044452
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800011
OA hybrid
DA 2024-07-18
ER

PT J
AU Li, JX
   Pan, ZF
   Liu, QS
   Wang, ZY
AF Li, Junxia
   Pan, Zefeng
   Liu, Qingshan
   Wang, Ziyang
TI Stacked U-Shape Network With Channel-Wise Attention for Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Convolution; Saliency detection; Semantics; Task
   analysis; Object detection; Visualization; Saliency detection; feature
   integration; channel attention; cascaded feedback; cross-layer
   cross-channel complements
AB This paper addresses the core issue of how to learn powerful features for saliency. We have two major observations. First, feature maps of different layers in convolutional neural networks play different roles in saliency detection. Second, different feature channels in the same layer are not of equal importance to saliency, and they often have different response to foreground or background. To address these problems, a stacked U-shape network with channel-wise attention is presented to effectively utilize these features, which mainly consists of a parallel dilated convolution (PDC) module and a multi-level attention cascaded feedback (MACF) module. More specifically, PDC aims to enlarge the receptive field without increasing the computation and effectively avoid the gridding problem. MACF is innovatively designed to adaptively select the cross-layer complementary information, and the inter-dependencies between different channel maps in the same layer can be depicted well. Finally, we adopt a multi-layer loss function to improve the commonly used binary cross entropy loss which treats all pixels equally. The extensive experiments on five saliency detection datasets demonstrate that the proposed method outperforms the state-of-the-art approaches.
C1 [Li, Junxia; Pan, Zefeng; Liu, Qingshan; Wang, Ziyang] Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.
   [Li, Junxia; Pan, Zefeng; Liu, Qingshan; Wang, Ziyang] Nanjing Univ Informat Sci & Technol, Jiangsu Collaborat Innovat Ctr Atmospher Environm, Sch Automat, Nanjing 210044, Peoples R China.
C3 Nanjing University of Information Science & Technology
RP Li, JX (corresponding author), Jiangsu Key Lab Big Data Anal Technol, Nanjing 210044, Peoples R China.
EM junxiali99@163.com; cxpanzefeng@163.com; qsliu@nuist.edu.cn;
   1585072156@163.com
RI liu, qingqing/HHD-0360-2022; Chen, Rainie/ISS-6016-2023; Liu,
   Qing/GWC-9222-2022; zhang, ying/JQX-1479-2023; Liu,
   Qingqing/HMV-4816-2023
OI Pan, Zefeng/0000-0002-8909-8619
FU National Major Project of China for New Generation of AI
   [2018AAA0100400]; National Science Fund of China [61532009, 61702272,
   61773219, 61802199]; Startup Foundation for Introducing Talent of NUIST
   [2243141701034]
FX This work was supported in part by the National Major Project of China
   for New Generation of AI under Grant 2018AAA0100400, in part by the
   National Science Fund of China under Grants 61532009, 61702272,
   61773219, and 61802199, and in part by the Startup Foundation for
   Introducing Talent of NUIST under Grant 2243141701034.
CR Cai Z., 2017, P IEEE C COMP VIS PA
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Gidaris S, 2016, PROC CVPR IEEE, P789, DOI 10.1109/CVPR.2016.92
   Han JW, 2015, IEEE T CIRC SYST VID, V25, P1309, DOI 10.1109/TCSVT.2014.2381471
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu H., 2017, Proceedings_of_the_34th_International_Conference_on_Machine_Learning-Volume, P1568
   Islam MA, 2018, PROC CVPR IEEE, P7142, DOI 10.1109/CVPR.2018.00746
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   Kingma D. P., 2014, arXiv
   Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li JX, 2022, IEEE T CYBERNETICS, V52, P873, DOI 10.1109/TCYB.2020.2988093
   Li JX, 2016, IEEE T IMAGE PROCESS, V25, P4421, DOI 10.1109/TIP.2016.2588331
   Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306
   Li X, 2018, LECT NOTES COMPUT SC, V11219, P370, DOI 10.1007/978-3-030-01267-0_22
   Li YW, 2019, PROC CVPR IEEE, P7019, DOI 10.1109/CVPR.2019.00719
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Mahadevan V, 2013, IEEE T PATTERN ANAL, V35, P541, DOI 10.1109/TPAMI.2012.98
   Qi GJ, 2016, PROC CVPR IEEE, P2267, DOI 10.1109/CVPR.2016.249
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shen C, 2019, IEEE T CIRC SYST VID, V29, P3016, DOI 10.1109/TCSVT.2018.2872503
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu RM, 2019, PROC CVPR IEEE, P8142, DOI 10.1109/CVPR.2019.00834
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang J, 2013, IEEE T NEUR NET LEAR, V24, P1023, DOI 10.1109/TNNLS.2013.2249088
   Yang J, 2012, PATTERN RECOGN, V45, P1104, DOI 10.1016/j.patcog.2011.08.022
   Yoo D, 2015, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2015.305
   Zhang DW, 2020, IEEE T PATTERN ANAL, V42, P1755, DOI 10.1109/TPAMI.2019.2900649
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang LH, 2019, IEEE WINT CONF APPL, P91, DOI 10.1109/WACV.2019.00017
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang TZ, 2015, INT J COMPUT VISION, V111, P171, DOI 10.1007/s11263-014-0738-0
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 57
TC 78
Z9 79
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1397
EP 1409
DI 10.1109/TMM.2020.2997192
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200017
DA 2024-07-18
ER

PT J
AU Liu, JX
   Tan, R
   Han, G
   Sun, N
   Kwong, S
AF Liu, Jixin
   Tan, Rong
   Han, Guang
   Sun, Ning
   Kwong, Sam
TI Privacy-Preserving In-Home Fall Detection Using Visual Shielding Sensing
   and Private Information-Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Privacy; Senior citizens; Nonhomogeneous media;
   Visualization; Cameras; Fall detection; privacy-preserving; multilayer
   compressed sensing; LBP-TOP; information embedded
ID DETECTION SYSTEM; VIDEO SURVEILLANCE; RECOGNITION; PROTECTION; RISK
AB Falls are the main cause of accidental injuries, and even death among elderly people, especially those who live alone in their homes. The absence of a reliable fall detection system has long been a serious problem for home health monitoring. A video surveillance system can be used to monitor elderly people at home to detect falls, but the traditional implementation of such intelligent detection falls short of personal privacy-related considerations; additionally, many people do not want to be watched in their homes. To solve this problem, we propose a fall detection system with visual shielding that can ensure the safety of elderly people in their homes while preserving their personal privacy. Multilayer compressed sensing is first used to achieve visually shielded video frames. By combining low-rank sparse decomposition theory with the improved local binary pattern on the three orthogonal planes, the object features are extracted from the shielded video frames. Finally, to compensate for the information lost in the compressed video to a certain extent, a private information-embedded classification model is proposed to identify fall-related behavior. The experimental results on two public fall datasets show that the proposed method delivers impressive accuracy and a low error rate while effectively distinguishing between fall- and nonfall-related behaviors in videos.
C1 [Liu, Jixin; Tan, Rong; Han, Guang; Sun, Ning] Nanjing Univ Posts & Telecommun, Minist Educ, Engn Res Ctr Wideband Wireless Commun Technol, Nanjing 210003, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong 999077, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; City University of
   Hong Kong
RP Liu, JX (corresponding author), Nanjing Univ Posts & Telecommun, Minist Educ, Engn Res Ctr Wideband Wireless Commun Technol, Nanjing 210003, Peoples R China.
EM liujixin@njupt.edu.cn; 1018010619@njupt.edu.cn;
   hanguang8848@njupt.edu.cn; sunning@njupt.edu.cn; cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; LIU, Jixin/AHC-0596-2022; Sun,
   Ning/AAJ-8901-2020
OI Kwong, Sam/0000-0001-7484-7261; Sun, Ning/0000-0002-6907-3756
FU Provincial Natural Science Foundation of the Science and Technology
   Bureau of Jiangsu Province [BK20180088]; China Postdoctoral Science
   Foundation [2019M651916]; Scientific Research Foundation of Nanjing
   University of Posts and Telecommunications [NY218066]; Natural Science
   Foundation of China [61871445]
FX Manuscript received January 19, 2020; revisedMay 27, 2020 and August 28,
   2020; accepted October 2, 2020. Date of publication October 12, 2020;
   date of current version October 19, 2021. This work was supported by
   funds from the Provincial Natural Science Foundation of the Science and
   Technology Bureau of Jiangsu Province under Grant BK20180088, the China
   Postdoctoral Science Foundation under Grant 2019M651916, the Scientific
   Research Foundation of Nanjing University of Posts and
   Telecommunications under Grant NY218066 and the Natural Science
   Foundation of China under Grant 61871445. (Corresponding author: Jixin
   Liu.)
CR Agrawal R., 1998, SIGMOD Record, V27, P94, DOI 10.1145/276305.276314
   Alhimale L, 2014, APPL SOFT COMPUT, V18, P59, DOI 10.1016/j.asoc.2014.01.024
   Ambrose AF, 2013, MATURITAS, V75, P51, DOI 10.1016/j.maturitas.2013.02.009
   [Anonymous], 2017, P 32 AAAI C ART INT
   [Anonymous], 2011, VISUAL COMMUN-US, DOI DOI 10.1109/VCIP.2011.6115917
   Cai X, 2017, IEEE T BIO-MED ENG, V64, P2618, DOI 10.1109/TBME.2017.2653246
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Charfi I, 2012, 8TH INTERNATIONAL CONFERENCE ON SIGNAL IMAGE TECHNOLOGY & INTERNET BASED SYSTEMS (SITIS 2012), P218, DOI 10.1109/SITIS.2012.155
   Chelli A, 2019, IEEE ACCESS, V7, P38670, DOI 10.1109/ACCESS.2019.2906693
   Çiftçi S, 2018, IEEE T MULTIMEDIA, V20, P68, DOI 10.1109/TMM.2017.2728479
   Cossalter M, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P436, DOI 10.1109/AVSS.2009.13
   Costantini L, 2011, LECT NOTES COMPUT SC, V6979, P199, DOI 10.1007/978-3-642-24088-1_21
   Dai J, 2015, IEEE IMAGE PROC, P4238, DOI 10.1109/ICIP.2015.7351605
   de Quadros T, 2018, IEEE SENS J, V18, P5082, DOI 10.1109/JSEN.2018.2829815
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Dufaux F, 2008, IEEE T CIRC SYST VID, V18, P1168, DOI 10.1109/TCSVT.2008.928225
   Ejupi A, 2017, IEEE T BIO-MED ENG, V64, P1602, DOI 10.1109/TBME.2016.2614230
   Fan KB, 2019, MULTIMED TOOLS APPL, V78, P9101, DOI 10.1007/s11042-018-5638-9
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Goodfellow I., 2014, ADV NEURAL INF PROCE, V27, DOI DOI 10.1145/3422622
   Harrou F, 2019, IEEE ACCESS, V7, P114966, DOI 10.1109/ACCESS.2019.2936320
   Jia XT, 2018, J COMPUT SCI-NETH, V25, P289, DOI 10.1016/j.jocs.2017.03.016
   Jiang HB, 2018, IEEE ACCESS, V6, P13317, DOI 10.1109/ACCESS.2018.2812887
   Kalache A., 2007, World health organisation global report on falls prevention in older age
   Khalique F, 2019, IEEE ACCESS, V7, P101309, DOI 10.1109/ACCESS.2019.2930730
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Li YN, 2016, INT C PATT RECOG, P25, DOI 10.1109/ICPR.2016.7899602
   Liong ST, 2016, SIGNAL PROCESS-IMAGE, V47, P170, DOI 10.1016/j.image.2016.06.004
   Liu JX, 2019, 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA (ICAIBD 2019), P423, DOI [10.1109/icaibd.2019.8837037, 10.1109/ICAIBD.2019.8837037]
   Liu JX, 2017, IET SIGNAL PROCESS, V11, P115, DOI 10.1049/iet-spr.2016.0026
   Liu XM, 2016, IEEE T IMAGE PROCESS, V25, P2844, DOI 10.1109/TIP.2016.2554320
   Liu XFS, 2014, MEAS PHYS EDUC EXERC, V18, P91, DOI 10.1080/1091367X.2013.864657
   Lotfi A, 2018, IEEE ACCESS, V6, P70272, DOI 10.1109/ACCESS.2018.2881237
   Lu ZJ, 2018, IEEE ACCESS, V6, DOI [10.1109/ACCESS.2018.2864189, 10.1109/LSP.2018.2810121]
   Ma X, 2014, IEEE J BIOMED HEALTH, V18, P1915, DOI 10.1109/JBHI.2014.2304357
   Mubashir M, 2013, NEUROCOMPUTING, V100, P144, DOI 10.1016/j.neucom.2011.09.037
   Olivieri DN, 2012, EXPERT SYST APPL, V39, P5935, DOI 10.1016/j.eswa.2011.11.109
   Núñez-Marcos A, 2017, WIREL COMMUN MOB COM, DOI 10.1155/2017/9474806
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Paneerselvam A, 2018, IEEE GLOB CONF CONSU, P351, DOI 10.1109/GCCE.2018.8574617
   Poonsri A, 2017, 2017 9TH INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY AND ELECTRICAL ENGINEERING (ICITEE)
   Qin Z, 2016, IEEE T MULTIMEDIA, V18, P929, DOI 10.1109/TMM.2016.2535729
   Rahman S., 2016, P 9 INT C ROB VIS SI, V398, P237
   Padilla-López JR, 2015, EXPERT SYST APPL, V42, P4177, DOI 10.1016/j.eswa.2015.01.041
   Ribaric S, 2016, SIGNAL PROCESS-IMAGE, V47, P131, DOI 10.1016/j.image.2016.05.020
   Rougier C, 2011, IEEE T CIRC SYST VID, V21, P611, DOI 10.1109/TCSVT.2011.2129370
   Saadeh W, 2019, IEEE T NEUR SYS REH, V27, P995, DOI 10.1109/TNSRE.2019.2911602
   Saini M, 2014, MULTIMED TOOLS APPL, V68, P135, DOI 10.1007/s11042-012-1207-9
   Wang L, 2019, SIGNAL PROCESS-IMAGE, V78, P246, DOI 10.1016/j.image.2019.07.011
   Wang YX, 2017, IEEE T MOBILE COMPUT, V16, P581, DOI 10.1109/TMC.2016.2557792
   Wang ZY, 2016, PROC CVPR IEEE, P4792, DOI 10.1109/CVPR.2016.518
   Wright J., 2008, PROC 8 IEEE INT C AU, P1
   Xu MZ, 2018, IEEE WINT CONF APPL, P1597, DOI 10.1109/WACV.2018.00178
   Yu M, 2012, IEEE T INF TECHNOL B, V16, P1274, DOI 10.1109/TITB.2012.2214786
   Yun YX, 2015, IEEE IMAGE PROC, P3280, DOI 10.1109/ICIP.2015.7351410
   Zerrouki N, 2018, MULTIMED TOOLS APPL, V77, P6405, DOI 10.1007/s11042-017-4549-5
   Zhao GY, 2007, IEEE T PATTERN ANAL, V29, P915, DOI 10.1109/TPAMI.2007.1110
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 60
TC 7
Z9 7
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3684
EP 3699
DI 10.1109/TMM.2020.3029904
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100020
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Ren, MJ
   Nie, J
   Zhao, SC
AF Nie, Weizhi
   Ren, Minjie
   Nie, Jie
   Zhao, Sicheng
TI C-GCN: Correlation Based Graph Convolutional Network for Audio-Video
   Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Feature extraction; Correlation; Task analysis;
   Visualization; Face recognition; Convolution; Emotion recognition;
   multi-head attention; multiple graphs
ID MODEL
AB With the development of both hardware and deep neural network technologies, tremendous improvements have been achieved in the performance of automatic emotion recognition (AER) based on the video data. However, AER is still a challenging task due to subtle expression, abstract concept of emotion and the representation of multi-modal information. Most proposed approaches focus on the multi-modal feature learning and fusion strategy, which pay more attention to the characteristic of a single video and ignore the correlation among the videos. To explore this correlation, in this paper, we propose a novel correlation-based graph convolutional network (C-GCN) for AER, which can comprehensively consider the correlation of the intra-class and inter-class videos for feature learning and information fusion. More specifically, we introduce the graph model to represent the correlation among the videos. This correlated information can help to improve the discrimination of node features in the progress of graph convolutional network. Meanwhile, the multi-head attention mechanism is applied to predict the hidden relationship among the videos, which can strengthen the inter-class correlation to improve the performance of classifiers. The C-GCN is evaluated on the AFEW datasets and eNTERFACE 05 dataset. The final experimental results demonstrate the superiority of our proposed method over the state-of-the-art methods.
C1 [Nie, Weizhi; Ren, Minjie] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Shandong, Peoples R China.
   [Zhao, Sicheng] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 Tianjin University; Ocean University of China; University of California
   System; University of California Berkeley
RP Ren, MJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Shandong, Peoples R China.
EM weizhinie@tju.edu.cn; renminjie@tju.edu.cn; niejie@ouc.edu.cn;
   schzhao@gmail.com
RI Nie, Jie/ABG-9228-2021; lu, lala/GQQ-3784-2022; Nie,
   Weizhi/ABF-5316-2021
OI Nie, Jie/0000-0003-4952-7666; lu, lala/0000-0002-6080-8074; nie,
   weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61702471,
   61772359]; Open Project Program of the State Key Laboratory of CAD AMP;
   CG, Zhejiang University [A2005, A2012]; Tianjin Science Foundation for
   Young Scientists of China [19JCQNJC00500]; 2019 Tianjin New Generation
   Artificial Intelligence Major Program [18ZXZNGX00150, 19ZXZNGX00110]
FX Manuscript received May 8, 2020; revised September 5, 2020; accepted
   October 12, 2020. Date of publication October 21, 2020; date of current
   version October 19, 2021. This work was supported in part by the
   National Natural Science Foundation of China (61872267, 61702471, and
   61772359), in part by the grant of 2019 Tianjin New Generation
   Artificial Intelligence Major Program (18ZXZNGX00150 and 19ZXZNGX00110),
   in part by the Open Project Program of the State Key Laboratory of CAD &
   CG, Zhejiang University under Grant A2005 and A2012, and in part by the
   Tianjin Science Foundation for Young Scientists of China
   (19JCQNJC00500). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Ye Wang.
   (Corresponding author: Weizhi Nie.)
CR Abdolrashidi A, 2019, DEEP EMOTION FACIAL, V1902
   AGUILAR G, 2019, P 57 C ASS COMP LING, V1, P991
   Aldeneh Z, 2017, INT CONF ACOUST SPEE, P2741, DOI 10.1109/ICASSP.2017.7952655
   Bengio Y., 2013, Fer-2013 face database
   Cai J, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P443, DOI 10.1109/MIPR.2019.00089
   Chen ZQ, 2018, ACM/SIGIR PROCEEDINGS 2018, P225, DOI 10.1145/3209978.3209997
   Darwin C., 1872, P374
   Dhall A., 2019, ICMI 19 P 2019 INT C, P546, DOI DOI 10.1145/3340555.3355710
   Dhall A, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P653
   Dhall A, 2012, IEEE MULTIMEDIA, V19, P34, DOI 10.1109/MMUL.2012.26
   Dong YZ, 2019, IEEE T MULTIMEDIA, V21, P3150, DOI 10.1109/TMM.2019.2918739
   Ebesu T, 2018, ACM/SIGIR PROCEEDINGS 2018, P515, DOI 10.1145/3209978.3209991
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754
   Gu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P157, DOI 10.1145/3343031.3351039
   Guha T, 2020, LEARNABLE GRAPH INCE
   Guo X, 2020, IEEE WINT CONF APPL, P2910, DOI [10.1109/WACV45572.2020.9093547, 10.1109/wacv45572.2020.9093547]
   Guo ZJ, 2019, T ASSOC COMPUT LING, V7, P297, DOI 10.1162/tacl_a_00269
   Han SZ, 2018, PROC CVPR IEEE, P5070, DOI 10.1109/CVPR.2018.00532
   Han WJ, 2018, INTERSPEECH, P932, DOI 10.21437/Interspeech.2018-1858
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hu P, 2017, P 19 ACM INT C MULT, P553, DOI DOI 10.1145/3136755.3143009
   Jia J, 2019, IEEE T MULTIMEDIA, V21, P1853, DOI 10.1109/TMM.2018.2887016
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Knyazev B, 2018, IEEE INT CONF AUTOMA, P692, DOI 10.1109/FG.2018.00109
   Ko BC, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18020401
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee G, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P1017, DOI 10.1145/3331184.3331325
   Li SN, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P589, DOI 10.1145/3340555.3355719
   Liu CH, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P630, DOI 10.1145/3242969.3264989
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Ma YX, 2019, INFORM FUSION, V46, P184, DOI 10.1016/j.inffus.2018.06.003
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Mansouri-Benssassi E, 2018, LECT NOTES COMPUT SC, V11324, P426, DOI 10.1007/978-3-030-04070-3_33
   MARREROFERNANDE.PD, 2019, IEEE C COMP VIS PATT, P837
   Martin O., 2006, 22 INT C DATA ENG WO, P8, DOI [DOI 10.1109/ICDEW.2006.145, 10.1109/ICDEW.2006.145]
   Mirsamadi S, 2017, INT CONF ACOUST SPEE, P2227, DOI 10.1109/ICASSP.2017.7952552
   Noroozi F, 2019, IEEE T AFFECT COMPUT, V10, P60, DOI 10.1109/TAFFC.2017.2713783
   Perozzi B, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P701, DOI 10.1145/2623330.2623732
   Sahoo S, 2016, IEEE STUDENT TECHNOL, P7, DOI 10.1109/TechSym.2016.7872646
   Satt A, 2017, INTERSPEECH, P1089, DOI 10.21437/Interspeech.2017-200
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Trigeorgis G, 2016, INT CONF ACOUST SPEE, P5200, DOI 10.1109/ICASSP.2016.7472669
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Tzinis E, 2017, INT CONF AFFECT, P190, DOI 10.1109/ACII.2017.8273599
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic Petar, 2018, INT C LEARN REPR
   Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753
   Wu M, 2021, IEEE T SYST MAN CY-S, V51, P1473, DOI 10.1109/TSMC.2019.2897330
   Xu HY, 2019, INTERSPEECH, P3569, DOI 10.21437/Interspeech.2019-3247
   Xu QD, 2018, ACM/SIGIR PROCEEDINGS 2018, P981, DOI 10.1145/3209978.3210117
   Yao L, 2019, AAAI CONF ARTIF INTE, P7370
   Ye J, 2020, P 34 AAAI C ART INT
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
   Zhang T, 2022, IEEE T AFFECT COMPUT, V13, P379, DOI 10.1109/TAFFC.2019.2937768
   Zhang Y, 2019, IEEE IJCNN, P1, DOI DOI 10.1109/ijcnn.2019.8851942
   Zhang YY, 2018, ASIAPAC SIGN INFO PR, P1771, DOI 10.23919/APSIPA.2018.8659587
   Zhang ZX, 2019, IEEE T MULTIMEDIA, V21, P1289, DOI 10.1109/TMM.2018.2871949
   Zhou HS, 2019, ICMI'19: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P562, DOI 10.1145/3340555.3355713
NR 62
TC 35
Z9 35
U1 7
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3793
EP 3804
DI 10.1109/TMM.2020.3032037
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100028
DA 2024-07-18
ER

PT J
AU Wang, B
   Niu, HF
   Zeng, JC
   Bai, GF
   Lin, SZ
   Wang, YB
AF Wang, Bin
   Niu, Huifang
   Zeng, Jianchao
   Bai, Guifeng
   Lin, Suzhen
   Wang, Yanbo
TI Latent Representation Learning Model for Multi-Band Images Fusion via
   Low-Rank and Sparse Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Feature extraction; Dictionaries; Sparse matrices; Machine
   learning; Neural networks; Tensile stress; Image fusion; multi-band
   images; low-rank; sparse; representation learning
ID FOCUS IMAGE; DECOMPOSITION; ENHANCEMENT; NETWORK
AB The fusion of multi-band images including far-infrared image (FIRI), near-infrared image (NIRI), and visible image (VISI) primarily suffers from four challenges. One is the problem of simultaneous fusion for multiple images. Most existing methods are oriented towards the fusion of two objects, which is generally achieved with a sequential fusion method. This means that intermediate fusion results are repeatedly integrated with the unprocessed images until all images have been fused. However, this may amplify the blurring effect, and even engender artifacts. Second, consistent training labels for image fusion cannot currently be obtained for some types of images (e.g., medical images, and multi-band images), which may lead to the failed application of supervised learning methods. Third, the existing methods often do not directly focus on the potential mapping relationship between the original, and resulting images, which usually increases the unpredictability of the fusion results. Fourth, redundant features or singularities are often not eliminated in the general fusion process, and both may interfere with or even obscure significant features in the source images. To address the abovementioned problems, this paper proposes a latent representation learning model that can synchronously integrate multi-band images without samples. Specifically, the model can capture the clean, and distinctive features of the originals via latent low-rank, and sparse embedding. The extracted intrinsic features are projected onto the target fusion space through an assumed mapping relationship. The final results were obtained through the designed optimization algorithm. In addition, numerous experiments were implemented to prove the rationality, and feasibility of the proposed fusion model with subjective evaluation, objective indexes, and convergence analysis.
C1 [Wang, Bin; Niu, Huifang; Zeng, Jianchao; Bai, Guifeng; Lin, Suzhen; Wang, Yanbo] North Univ China, Dept Data Sci & Technol, Taiyuan 030051, Peoples R China.
   [Wang, Bin] Shanxi Key Lab Signal Capturing & Proc, Taiyuan 030051, Peoples R China.
C3 North University of China
RP Wang, B (corresponding author), North Univ China, Dept Data Sci & Technol, Taiyuan 030051, Peoples R China.
EM binwang_nuc@163.com; b1907070@st.nuc.edu.cn; zjc@nuc.edu.cn;
   b1705008@st.nuc.edu.cn; lsz@nuc.edu.cn; yanbowang@nuc.edu.cn
RI niu, hui/GQP-6490-2022
OI Wang, Yanbo/0000-0001-7997-434X
FU Graduate Education Innovation Project of Shanxi Province, China
   [2019BY115, 2019BY107, 2020SY382]; Basic Applied Research Projects of
   Shanxi Province, China [201801D121126, 201801D121155, 201701D121062];
   ShanxiKey Research, and Development Projects [201603D321128]
FX This work was supported in part by the Graduate Education Innovation
   Project of Shanxi Province, China, in 2019 under Grants 2019BY115,
   2019BY107, and 2020SY382, in part by the Basic Applied Research Projects
   of Shanxi Province, China under Grants 201801D121126, 201801D121155, and
   201701D121062, and in part by the ShanxiKey Research, and Development
   Projects under Grant 201603D321128.
CR Bhatnagar G, 2013, IEEE T MULTIMEDIA, V15, P1014, DOI 10.1109/TMM.2013.2244870
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Candès EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395
   Cheng BY, 2018, INFRARED PHYS TECHN, V92, P68, DOI 10.1016/j.infrared.2018.05.006
   Cheng BY, 2018, NEUROCOMPUTING, V310, P135, DOI 10.1016/j.neucom.2018.05.028
   Fang XZ, 2018, IEEE T NEUR NET LEAR, V29, P5228, DOI 10.1109/TNNLS.2018.2796133
   Feng L, 2020, KNOWL-BASED SYST, V191, DOI 10.1016/j.knosys.2019.105172
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   He GQ, 2019, IEEE GEOSCI REMOTE S, V16, P1796, DOI 10.1109/LGRS.2019.2907721
   Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Jing PG, 2021, IEEE T MULTIMEDIA, V23, P2259, DOI 10.1109/TMM.2020.3009485
   Jing PG, 2020, IEEE T MULTIMEDIA, V22, P1555, DOI 10.1109/TMM.2019.2944749
   Li H.C., 2018, PREPRINT
   Li HF, 2019, IEEE T INTELL TRANSP, V20, P2025, DOI 10.1109/TITS.2018.2856928
   Li HF, 2018, PATTERN RECOGN, V79, P130, DOI 10.1016/j.patcog.2018.02.005
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2019, IEEE T GEOSCI REMOTE, V57, P7832, DOI 10.1109/TGRS.2019.2916654
   Li J, 2021, IEEE T MULTIMEDIA, V23, P1383, DOI 10.1109/TMM.2020.2997127
   Li ST, 2018, IEEE T IMAGE PROCESS, V27, P4118, DOI 10.1109/TIP.2018.2836307
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Lin, 2010, ARXIV10095055
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu GC, 2011, IEEE I CONF COMP VIS, P1615, DOI 10.1109/ICCV.2011.6126422
   Liu PF, 2018, IEEE T GEOSCI REMOTE, V56, P1788, DOI 10.1109/TGRS.2017.2768386
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Meng LY, 2019, BIOMED SIGNAL PROCES, V53, DOI 10.1016/j.bspc.2019.04.013
   Naidu VPS, 2011, DEFENCE SCI J, V61, P479, DOI 10.14429/dsj.61.705
   Ren ZW, 2020, IEEE T IMAGE PROCESS, V29, P2094, DOI 10.1109/TIP.2019.2938859
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Ulfarsson MO, 2019, IEEE T GEOSCI REMOTE, V57, P6408, DOI 10.1109/TGRS.2019.2906048
   Vargas E, 2019, IEEE T GEOSCI REMOTE, V57, P5043, DOI 10.1109/TGRS.2019.2895822
   Wang ML, 2020, IEEE T MED IMAGING, V39, P644, DOI 10.1109/TMI.2019.2933160
   Wang QT, 2020, IEEE T CIRC SYST VID, V30, P2418, DOI 10.1109/TCSVT.2019.2919310
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, IN PRESS
   Wei Q, 2015, IEEE T GEOSCI REMOTE, V53, P3658, DOI 10.1109/TGRS.2014.2381272
   Wong WK, 2017, IEEE T IMAGE PROCESS, V26, P2905, DOI 10.1109/TIP.2017.2691543
   Wu YL, 2020, IEEE T MULTIMEDIA, V22, P1310, DOI 10.1109/TMM.2019.2942494
   Xiao B, 2020, IEEE T MULTIMEDIA, V22, P285, DOI 10.1109/TMM.2019.2928516
   Xu Y, 2020, IEEE T GEOSCI REMOTE, V58, P348, DOI 10.1109/TGRS.2019.2936486
   Xydeas CS, 2000, ELECTRON LETT, V36, P308, DOI 10.1049/el:20000267
   Yang SY, 2018, IEEE T NEUR NET LEAR, V29, P3647, DOI 10.1109/TNNLS.2017.2736011
   Yang Y, 2020, IEEE T INSTRUM MEAS, V69, P4753, DOI 10.1109/TIM.2019.2951864
   Yang Y, 2019, IEEE T COMPUT IMAG, V5, P262, DOI 10.1109/TCI.2018.2889959
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Zhang K, 2018, IEEE J-STARS, V11, P1030, DOI 10.1109/JSTARS.2017.2785411
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zhou P, 2016, IEEE T NEUR NET LEAR, V27, P1080, DOI 10.1109/TNNLS.2015.2436951
   Zhou T, 2019, IEEE T MED IMAGING, V38, P2411, DOI 10.1109/TMI.2019.2913158
   Zhou T, 2019, IEEE T BIO-MED ENG, V66, P165, DOI 10.1109/TBME.2018.2824725
NR 54
TC 15
Z9 16
U1 4
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3137
EP 3152
DI 10.1109/TMM.2020.3020695
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000015
DA 2024-07-18
ER

PT J
AU Wang, HB
   Wang, Y
   Zhang, Z
   Fu, XP
   Zhuo, L
   Xu, ML
   Wang, M
AF Wang, Huibing
   Wang, Yang
   Zhang, Zhao
   Fu, Xianping
   Zhuo, Li
   Xu, Mingliang
   Wang, Meng
TI Kernelized Multiview Subspace Analysis By Self-Weighted Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Dimensionality reduction; Sparse matrices; Correlation;
   Optimization; Laplace equations; Image retrieval; Co-regularized; kernel
   space; kernelized multiview subspace analysis; multiview learning;
   self-weighted
ID IMAGE; REPRESENTATION
AB With the popularity of multimedia technology, information is always represented from multiple views. Even though multiview data can reflect the same sample from different perspectives, multiple views are consistent to some extent because they are representations of the same sample. Most of the existing algorithms are graph-based ones to learn the complex structures within multiview data but overlook the information within data representations. Furthermore, many existing works treat multiple views discriminatively by introducing some hyperparameters, which is undesirable in practice. To this end, abundant multiview-based methods have been proposed for dimension reduction. However, there is still no research that leverages the existing work into a unified framework. In this paper, we propose a general framework for multiview data dimension reduction, named kernelized multiview subspace analysis (KMSA) to handle multiview feature representation in the kernel space, providing a feasible channel for multiview data with different dimensions. Compared with the graph-based methods, KMSA can fully exploit information from multiview data with nothing to lose. Since different views have different influences on KMSA, we propose a self-weighted strategy to treat different views discriminatively. A co-regularized term is proposed to promote the mutual learning from multiviews. KMSA combines self-weighted learning with the co-regularized term to learn the appropriate weights for all views. We evaluate our proposed framework on 6 multiview datasets for classification and image retrieval. The experimental results validate the advantages of our proposed method.
C1 [Wang, Huibing; Fu, Xianping] Dalian Maritime Univ, Coll Informat & Sci Technol, Dalian 116021, Liaoning, Peoples R China.
   [Wang, Yang; Zhang, Zhao; Wang, Meng] Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Anhui, Peoples R China.
   [Zhuo, Li] Beijing Univ Technol, Fac Informat Technol, Beijing 100000, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
C3 Dalian Maritime University; Hefei University of Technology; Beijing
   University of Technology; Zhengzhou University
RP Wang, Y; Zhang, Z (corresponding author), Hefei Univ Technol, Minist Educ, Key Lab Knowledge Engn Big Data, Hefei, Anhui, Peoples R China.
EM huibing.wang@dlmu.edu.cn; wangy@cse.unsw.edu.au; cszzhang@gmail.com;
   fxp@dlmu.edu.cn; zhuoli@bjtu.edu.cn; iexumingliang@zzu.edu.cn;
   eric.mengwang@gmail.com
RI Zhang, Zhao/B-5136-2010; zhang, zhang/GQZ-6804-2022; Wang,
   Meng/ITR-8699-2023
OI Zhang, Zhao/0000-0002-5703-7969; 
FU National Key Research and Development Program of China [2018YFB0804203];
   National Natural Science Foundation of China [61806035, U1936217,
   61672365, 61732008, 61725203, 61370142, 62002041]; Key Research and
   Technology Development Projects of Anhui Province [202004a05020043];
   Postdoctoral Science Foundation [3620080307]; Dalian Science and
   Technology Innovation Fund [2019J11CY001]; Liaoning Revitalization
   Talents Program [XLYC1908007]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB0804203, in part by the
   National Natural Science Foundation of China under Grants 61806035,
   U1936217, 61672365, 61732008, 61725203, 61370142, and 62002041, in part
   by The Key Research and Technology Development Projects of Anhui
   Province (No. 202004a05020043), in part by the Postdoctoral Science
   Foundation (3620080307), in part by the Dalian Science and Technology
   Innovation Fund (2019J11CY001), and in part by the Liaoning
   Revitalization Talents Program (XLYC1908007). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lu Fang.
CR [Anonymous], 2011, INT C NEURAL INF PRO
   [Anonymous], 1976, PRINCIPLES MATH ANAL
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Bisson G, 2012, IEEE DATA MINING, P828, DOI 10.1109/ICDM.2012.93
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Fan JC, 2020, IEEE T NEUR NET LEAR, V31, P749, DOI 10.1109/TNNLS.2019.2909686
   Gao XB, 2008, PATTERN RECOGN, V41, P3179, DOI 10.1016/j.patcog.2008.03.025
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J, 2018, AAAI CONF ARTIF INTE, P298
   He XF, 2004, ADV NEUR IN, V16, P153
   Izenman AJ, 2008, SPRINGER TEXTS STAT, P237, DOI 10.1007/978-0-387-78189-1_8
   Jolliffe I.T., 1986, Principal component analysis, DOI DOI 10.1016/0169-7439(87)80084-9
   Kan MN, 2016, IEEE T PATTERN ANAL, V38, P188, DOI 10.1109/TPAMI.2015.2435740
   Kang Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2312
   Li GX, 2012, IEEE T KNOWL DATA EN, V24, P2040, DOI 10.1109/TKDE.2011.160
   Li SZ, 2002, LECT NOTES COMPUT SC, V2353, P67
   Lisanti G, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3038916
   Liu GH, 2011, PATTERN RECOGN, V44, P2123, DOI 10.1016/j.patcog.2011.02.003
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Luo Y, 2015, IEEE T KNOWL DATA EN, V27, P3111, DOI 10.1109/TKDE.2015.2445757
   Michaeli T., 2016, INT C MACHINE LEARNI, P1967, DOI DOI 10.48550/ARXIV.1511.04839
   Mika S., 1999, Neural Networks for Signal Processing IX: Proceedings of the 1999 IEEE Signal Processing Society Workshop (Cat. No.98TH8468), P41, DOI 10.1109/NNSP.1999.788121
   Nie FP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2022, DOI 10.1145/3219819.3220049
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qiao LS, 2010, PATTERN RECOGN, V43, P331, DOI 10.1016/j.patcog.2009.05.005
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang M, 2022, IEEE T KNOWL DATA EN, V34, P2574, DOI 10.1109/TKDE.2020.3015777
   Wang Y., 2020, ACM T MULTIMEDIA COM
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2017, IEEE T NEUR NET LEAR, V28, P57, DOI 10.1109/TNNLS.2015.2498149
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wu L, 2019, IEEE T MULTIMEDIA, V21, P1412, DOI 10.1109/TMM.2018.2877886
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Xu WT, 2018, IEEE T MOBILE COMPUT, V17, P197, DOI 10.1109/TMC.2017.2702634
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2017, IEEE T IMAGE PROCESS, V26, P648, DOI 10.1109/TIP.2016.2627806
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhang W., 2016, P INT JOINT C ART IN, P2153
   Zhu Jun-Yan, 2018, Advances in Neural Information Processing Systems
NR 50
TC 75
Z9 75
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3828
EP 3840
DI 10.1109/TMM.2020.3032023
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100031
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, QQ
   Cheng, JF
   Gao, QX
   Zhao, GS
   Jiao, LC
AF Wang, Qianqian
   Cheng, Jiafeng
   Gao, Quanxue
   Zhao, Guoshuai
   Jiao, Licheng
TI Deep Multi-View Subspace Clustering With Unified and Discriminative
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Clustering methods; Correlation; Decoding; Feature extraction; Intserv
   networks; Convolution; Databases; Multi-view clustering; local
   structure; discrimi- native learning
ID SPARSE
AB Deep multi-view subspace clustering has achieved promising performance compared with other multi-view clustering. However, existing deep multi-view subspace clustering only considers the global structure for all views, and they ignore the local geometric structure among each view. In addition, they cannot learn discriminative feature on different clusters of different views, i.e., inter-cluster difference. To solve these problems, in this paper, we propose a novel Deep Multi-view Subspace Clustering with Unified and Discriminative Learning (DMSC-UDL). DMSC-UDL combines global and local structures with self-expression layer. The global and local structures help each other forward and achieve small distance between samples of the same cluster. To make samples in different clusters of different views farther, DMSC-UDL uses a discriminative constraint between different views. In this way, DMSC-UDL makes the same cluster's samples have large weights, while different clusters' samples have small weights. Thus, it can learn a better shared connection matrix for multi-view clustering. Extensive experimental results reveal that the proposed multi-view clustering method is superior to several state-of-the-art multi-view clustering methods in terms of performance.
C1 [Wang, Qianqian] Xidian Univ, Sch Telecommun Engn, Minist Educ Intellisense & Image Understanding, State Key Lab Integrated Serv Networks,Key Lab, Xian 710071, Peoples R China.
   [Cheng, Jiafeng; Gao, Quanxue] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Jiao, Licheng] Xidian Univ, Sch Artificial Intelligence, Minist Educ Intellisense & Image Understanding, Key Lab, Xian 710071, Peoples R China.
C3 Xidian University; Xidian University; Xidian University
RP Wang, QQ (corresponding author), Xidian Univ, Sch Telecommun Engn, Minist Educ Intellisense & Image Understanding, State Key Lab Integrated Serv Networks,Key Lab, Xian 710071, Peoples R China.
EM qianqian174@foxmail.com; chengjf1208@163.com; qxgao@xidian.edu.cn;
   guoshuai.zhao@xjtu.edu.cn; lchjiao@mail.xidian.edu.cn
RI Jiao, Licheng/JOZ-0842-2023; Wang, Qianqian/AHC-6753-2022
OI Jiao, Licheng/0000-0003-3354-9617; Wang, Qianqian/0000-0001-8011-171X
FU National Natural Science Foundation of China [61773302, 61906141];
   Initiative Postdocs Supporting Program [BX20190262]; China Postdoctoral
   Science Foundation [2019M653564, 2019M663642]; National Natural Science
   Foundation of Shaanxi Province [2020JZ-19, 2020JQ-317, 2020JQ-327];
   Innovation Fund of Xidian University
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61773302 and 61906141, in part by Initiative Postdocs
   Supporting Program BX20190262, in part by China Postdoctoral Science
   Foundation under Grants 2019M653564 and 2019M663642, in part by the
   National Natural Science Foundation of Shaanxi Province under Grants
   2020JZ-19, 2020JQ-317, and 2020JQ-327 and in part by the Innovation Fund
   of Xidian University. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. ShaoenWu.
CR Abavisani M, 2018, IEEE J-STSP, V12, P1601, DOI 10.1109/JSTSP.2018.2875385
   Abavisani M, 2018, INFORM FUSION, V39, P168, DOI 10.1016/j.inffus.2017.05.002
   Akaho S., 2006, CoRR
   Andrew G., 2013, P ICML, P1247
   [Anonymous], 2017, ARXIV170202519
   [Anonymous], 2020, IEEE T NEUR NET LEAR, DOI [DOI 10.1109/TKDE.2019.2903810, DOI 10.1109/TNNLS.2019.2912082]
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Cai X., 2013, 23 INT JOINT C ART I, P2598
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chaudhuri K., 2009, P 26 ANN INT C MACH, P129
   Chen C, 2019, NEUROCOMPUTING, V366, P1, DOI 10.1016/j.neucom.2019.06.098
   Cui GS, 2018, NEUROCOMPUTING, V292, P38, DOI 10.1016/j.neucom.2018.02.067
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Gao QX, 2020, AAAI CONF ARTIF INTE, V34, P3938
   Guo DY, 2014, INT C PATT RECOG, P3774, DOI 10.1109/ICPR.2014.648
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang SD, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107015
   Huang ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2563
   Kakade SM, 2007, LECT NOTES COMPUT SC, V4539, P82, DOI 10.1007/978-3-540-72927-3_8
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kang Z, 2020, KNOWL-BASED SYST, V189, DOI 10.1016/j.knosys.2019.105102
   Kang Z, 2020, NEURAL NETWORKS, V122, P279, DOI 10.1016/j.neunet.2019.10.010
   King DB, 2015, ACS SYM SER, V1214, P1
   Kumar A., 2011, P 28 INT C MACHINE L, P393
   Li FF, 2018, PATTERN RECOGN, V83, P161, DOI 10.1016/j.patcog.2018.05.019
   Li ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2952
   Lin Bingqian, 2018, ARXIV180806220
   Liu J., 2013, P 2013 SIAM INT C DA, P252, DOI DOI 10.1137/1.9781611972832.28
   Luo P, 2018, NEUROCOMPUTING, V294, P1, DOI 10.1016/j.neucom.2017.10.023
   Sun G, 2021, IEEE T NEUR NET LEAR, V32, P139, DOI 10.1109/TNNLS.2020.2977497
   Sun X., 2019, P ACML, P1001
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tolic D, 2018, PATTERN RECOGN, V82, P40, DOI 10.1016/j.patcog.2018.04.029
   Tzortzis G, 2012, IEEE DATA MINING, P675, DOI 10.1109/ICDM.2012.43
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang QQ, 2018, IEEE DATA MINING, P1290, DOI 10.1109/ICDM.2018.00174
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P3939, DOI 10.1109/TIP.2015.2457339
   Wolf L., 2011, Face Recognition in Unconstrained Videos with Matched Background Similarity
   Xiao H., 2017, arXiv
   Xie JY, 2016, PR MACH LEARN RES, V48
   Xie Y, 2020, IEEE T KNOWL DATA EN
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yin Q., 2015, PROC CIKM 15, P383
   Yin QY, 2015, NEUROCOMPUTING, V156, P12, DOI 10.1016/j.neucom.2015.01.017
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang Z, 2019, IEEE T PATTERN ANAL, V41, P1774, DOI 10.1109/TPAMI.2018.2847335
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhu Pengfei, 2019, ARXIV190801978
NR 54
TC 82
Z9 85
U1 6
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3483
EP 3493
DI 10.1109/TMM.2020.3025666
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100004
DA 2024-07-18
ER

PT J
AU Wu, DP
   Bao, RL
   Li, ZD
   Wang, HG
   Zhang, H
   Wang, RY
AF Wu, Dapeng
   Bao, Ruili
   Li, Zhidu
   Wang, Honggang
   Zhang, Hong
   Wang, Ruyan
TI Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid
   Human-Artificial Intelligence Scheme
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Delays; Servers; Resource management; Optimization;
   Quality of service; Video coding; Edge-cloud collaboration; hybrid
   human-artificial intelligence; statistical delay guarantee; video
   caching and delivery; video coding rate
ID DELIVERY; PLACEMENT
AB In this paper, a video service enhancement strategy is investigated under an edge-cloud collaboration framework, where video caching and delivery decisions are made at the cloud and edge respectively. We aim to guarantee the user fairness in terms of video coding rate under statistical delay constraint and edge caching capacity constraint. A hybrid human-artificial intelligence approach is developed to improve the user hit rate for video caching. Specifically, individual user interest is first characterized by merging factorization machine (FM) model and multi-layer perceptron (MLP) model, where both low-order and high-order features can be well learned simultaneously. Thereafter, a social aware similarity model is constructed to transfer individual user interest to group interest, based on which, videos can be selected to cache at the network edge. Furthermore, a dual bisection exploration scheme is proposed to optimize wireless resource allocation and video coding rate. The effectiveness of the proposed video caching and delivery scheme is finally validated by extensive experiments with a real-world dataset.
C1 [Wu, Dapeng; Bao, Ruili; Li, Zhidu; Zhang, Hong; Wang, Ruyan] Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
   [Wu, Dapeng; Bao, Ruili; Li, Zhidu; Zhang, Hong; Wang, Ruyan] Key Lab Opt Commun & Networks Chongqing, Chongqing 400065, Peoples R China.
   [Wu, Dapeng; Bao, Ruili; Li, Zhidu; Zhang, Hong; Wang, Ruyan] Key Lab Ubiquitous Sensing & Networking Chongqing, Chongqing 400065, Peoples R China.
   [Wang, Honggang] Univ Massachusetts Dartmouth, Elect & Comp Engn Dept, N Dartmouth, MA 02747 USA.
C3 Chongqing University of Posts & Telecommunications; University of
   Massachusetts System; University Massachusetts Dartmouth
RP Li, ZD (corresponding author), Chongqing Univ Posts & Telecommun, Sch Commun & Informat Engn, Chongqing 400065, Peoples R China.
EM wudp@cqupt.edu.cn; S190131250@stu.cqupt.edu.cn; lizd@cqupt.edu.cn;
   hwang1@umassd.edu; hongzhang@cqupt.edu.cn; wangry@cqupt.edu.cn
RI Zhang, Hong/AHC-9408-2022; Wu, Dapeng/IWE-0674-2023
OI Zhang, Hong/0000-0002-3406-5713; Wu, Dapeng/0000-0003-2105-9418; Bao,
   Ruili/0000-0003-2897-773X
FU IEEE ICC 2021; National Natural Science Foundation of China [61901078,
   61771082, 61871062]; Science, Technology Research Program of Chongqing
   Municipal Education Commission [KJQN201900609, KJQN202000626]; Natural
   Science Foundation of Chongqing [cstc2020jcyj-zdxmX0024]; University
   Innovation Research Group of Chongqing [CXQT20017]
FX This work was supported in part by IEEE ICC 2021, in part by the
   National Natural Science Foundation of China under Grants 61901078,
   61771082, and 61871062, and in part by the Science, Technology Research
   Program of Chongqing Municipal Education Commission under Grants
   KJQN201900609 and KJQN202000626, in part by the Natural Science
   Foundation of Chongqing under Grant cstc2020jcyj-zdxmX0024, and in part
   by University Innovation Research Group of Chongqing under Grant
   CXQT20017.
CR [Anonymous], 2016, P 1 WORKSH DEEP LEAR, DOI [DOI 10.1145/2988450.2988454, 10.1145/2988450.2988454]
   Chatzieleftheriou LE, 2019, IEEE T MOBILE COMPUT, V18, P125, DOI 10.1109/TMC.2018.2831690
   Choi M, 2019, IEEE T WIREL COMMUN, V18, P5705, DOI 10.1109/TWC.2019.2938755
   Costa MD, 2020, PSYCHOTHER PSYCHOSOM, DOI 10.1159/000511880
   Dara S, 2020, J INTELL INF SYST, V54, P271, DOI 10.1007/s10844-018-0542-3
   Fan CS, 2020, IEEE T COMMUN, V68, P7007, DOI 10.1109/TCOMM.2020.3013633
   Gao J, 2019, IEEE T WIREL COMMUN, V18, P5938, DOI 10.1109/TWC.2019.2940667
   Goian HS, 2019, IEEE ACCESS, V7, P27699, DOI 10.1109/ACCESS.2019.2898734
   Guo HF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1725
   Jiang Y., 2008, Stochastic Network Calculus
   Jiang YX, 2020, IEEE T COMMUN, V68, P1567, DOI 10.1109/TCOMM.2019.2961081
   Khalek AA, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2332304
   Li N, 2019, IEEE T CIRC SYST VID, V29, P1795, DOI 10.1109/TCSVT.2018.2850445
   Li ZD, 2019, IEEE J SEL AREA COMM, V37, P283, DOI 10.1109/JSAC.2018.2872374
   Liu HF, 2019, RECSYS 2019: 13TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P34, DOI 10.1145/3298689.3347012
   Mehrizi S, 2020, IEEE T COMMUN, V68, P7068, DOI 10.1109/TCOMM.2020.3015478
   Navarro-Ortiz J, 2020, IEEE COMMUN SURV TUT, V22, P905, DOI 10.1109/COMST.2020.2971781
   Pu LJ, 2018, IEEE J SEL AREA COMM, V36, P1751, DOI 10.1109/JSAC.2018.2844624
   Qin D, 2020, IEEE T KNOWL DATA EN, V32, P453, DOI 10.1109/TKDE.2018.2879658
   Sheng M, 2016, IEEE COMMUN MAG, V54, P70, DOI 10.1109/MCOM.2016.7537179
   Wang Li-cai, 2011, Acta Electronica Sinica, V39, P2547
   Wu HQ, 2020, IEEE T WIREL COMMUN, V19, P6409, DOI 10.1109/TWC.2020.3003339
   Wu J, 2019, IEEE WIREL COMMUN LE, V8, P1722, DOI 10.1109/LWC.2019.2939129
   YAN J, 2020, IEEE INT CONF COMM
   Zhang S, 2018, IEEE T MOBILE COMPUT, V17, P1791, DOI 10.1109/TMC.2017.2780834
   Zhang W, 2019, IEEE T VEH TECHNOL, V68, P2958, DOI 10.1109/TVT.2019.2895682
   Zhang X, 2019, IEEE J SEL AREA COMM, V37, P1721, DOI 10.1109/JSAC.2019.2927088
   Zhang XW, 2018, IEEE T VEH TECHNOL, V67, P9047, DOI 10.1109/TVT.2018.2849703
   Zhang Z, 2020, IEEE T VEH TECHNOL, V69, P7955, DOI 10.1109/TVT.2020.2994181
   Zhang ZF, 2018, MOBILE NETW APPL, V23, P639, DOI 10.1007/s11036-017-0973-z
   Zhou G., 2020, IEEE T VEH TECHNOL, V69, P116
   Zhou L, 2020, IEEE WIREL COMMUN, V27, P112, DOI 10.1109/MWC.001.1900201
NR 32
TC 39
Z9 39
U1 0
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2208
EP 2221
DI 10.1109/TMM.2021.3066050
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, LB
   Du, DW
   Li, CC
   Wu, YJ
   Luo, TJ
AF Zhang, Libo
   Du, Dawei
   Li, Congcong
   Wu, Yanjun
   Luo, Tiejian
TI Iterative Knowledge Distillation for Automatic Check-Out
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Testing; Training; Adaptation models; Reliability; Feature extraction;
   Training data; Task analysis; Automatic check-out; domain adaptation;
   data augmentation; dual pyramid scale network; iterative knowledge
   distillation
ID SALIENT OBJECT DETECTION; NETWORKS
AB Automatic Check-Out (ACO) provides an object detection based mechanism for retailers to process the purchases of customers automatically. However, it suffers a lot from the domain shift problem because of different data distribution between the single item in training exemplar images and mixed items in testing checkout images. In this paper, we propose a new iterative knowledge distillation method to solve the domain adaptation problem for this task. First, we develop a new augmentation data strategy to generate synthesized checkout images. It can extract segmented items from the training images by the coarse-to-fine strategy and filter items with unrealistic poses by pose pruning. Second, we propose a dual pyramid scale network (DPSNet) to exploit the multi-scale feature representation in joint detection and counting views. Third, the iterative knowledge distillation training strategy is developed to make full use of both image-level and instance-level samples to narrow the semantic gap between source domain and target domain. Extensive experiments on the large-scale Retail Product Checkout (RPC) dataset show the proposed DPSNet can achieve state-of-the-art performance compared with existing methods. The source codes can be found at https://isrc.iscas.ac.cn/gitlab/research/dpsnet.
C1 [Zhang, Libo; Wu, Yanjun] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Du, Dawei] SUNY Albany, Comp Sci Dept, Albany, NY 12222 USA.
   [Li, Congcong; Luo, Tiejian] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101400, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; State
   University of New York (SUNY) System; State University of New York
   (SUNY) Albany; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Du, DW (corresponding author), SUNY Albany, Comp Sci Dept, Albany, NY 12222 USA.
EM libo@iscas.ac.cn; cvdaviddo@gmail.com; licongcong18@mails.ucas.edu.cn;
   yanjun@iscas.ac.cn; tjluo@ucas.ac.cn
RI Yu, Chongxiu/KDM-7354-2024; Li, Chun/KBC-9591-2024; liu,
   xinyi/KFB-4466-2024; yuan, lin/JDW-7387-2023
OI Yu, Chongxiu/0000-0002-8221-6221; 
FU National Natural Science Foundation of China [61807033]; Key Research
   Program of Frontier Sciences, CAS [ZDBS-LY-JSC038]; Youth Innovation
   Promotion Association of the Chinese Academy of Sciences [2020111];
   Outstanding Youth Scientist Project of ISCAS
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grant 61807033, and in part by the Key
   Research Program of Frontier Sciences, CAS, under Grant ZDBS-LY-JSC038.
   Thework ofL. Zhangwas supported by theYouth Innovation Promotion
   Association of the Chinese Academy of Sciences (2020111), and
   Outstanding Youth Scientist Project of ISCAS.
CR Adriana R., 2015, Fitnets: Hints for thin deep nets, P2
   Bagherinezhad H., 2018, CORR
   Chen GB, 2017, ADV NEUR IN, V30
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Dean J., 2015, NIPS DEEP LEARNING R
   Devito Z., 2017, NEURIPS WORKSHOPS, P1
   Dollár P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231
   Follmann P, 2018, LECT NOTES COMPUT SC, V11214, P581, DOI 10.1007/978-3-030-01249-6_35
   Fukuda T, 2017, INTERSPEECH, P3697, DOI 10.21437/Interspeech.2017-614
   Furlanello T., 2018, P MACHINE LEARNING R, V80, P1607, DOI DOI 10.48550/ARXIV.1805.04770
   Gaoyuan Mu, 2018, 2018 IEEE Industrial Cyber-Physical Systems (ICPS). Proceedings, P79, DOI 10.1109/ICPHYS.2018.8387641
   George M, 2014, LECT NOTES COMPUT SC, V8690, P440, DOI 10.1007/978-3-319-10605-2_29
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Heo B, 2019, AAAI CONF ARTIF INTE, P3771, DOI 10.1609/aaai.v33i01.33013771
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Jund P., 2016, The freiburg groceries dataset
   Klasson M, 2019, IEEE WINT CONF APPL, P491, DOI 10.1109/WACV.2019.00058
   Koubaroulis D, 2002, AS C COMP VIS
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Li CC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2152, DOI 10.1145/3343031.3350989
   Li QQ, 2017, PROC CVPR IEEE, P7341, DOI 10.1109/CVPR.2017.776
   Li YH, 2018, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2018.00120
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Merler M., 2007, PROC IEEE C COMPUT V, P1
   Ning GH, 2018, IEEE T MULTIMEDIA, V20, P1246, DOI 10.1109/TMM.2017.2762010
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Qi F, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P429, DOI 10.1145/3240508.3240633
   Qian SS, 2018, IEEE T MULTIMEDIA, V20, P2086, DOI 10.1109/TMM.2017.2785227
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Rocha A, 2010, COMPUT ELECTRON AGR, V70, P96, DOI 10.1016/j.compag.2009.09.002
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saito K, 2017, PR MACH LEARN RES, V70
   Song GC, 2018, ADV NEUR IN, V31
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang Shitao, 2019, ARXIV190100366
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang JD, 2017, INT J COMPUT VISION, V123, P251, DOI 10.1007/s11263-016-0977-3
   Wei X-S, 2019, ARXIV190107249
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 47
TC 10
Z9 10
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4158
EP 4170
DI 10.1109/TMM.2020.3037502
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900019
DA 2024-07-18
ER

PT J
AU Zhang, ZW
   Xu, D
   Ouyang, WL
   Zhou, LP
AF Zhang, Zhiwang
   Xu, Dong
   Ouyang, Wanli
   Zhou, Luping
TI Dense Video Captioning Using Graph-Based Sentence Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Visualization; Semantics; Decoding; Computer architecture;
   Microprocessors; Feature extraction; Dense video captioning; sentence
   summarization; graph convolutional network
AB Recently, dense video captioning has made attractive progress in detecting and captioning all events in a long untrimmed video. Despite promising results were achieved, most existing methods do not sufficiently explore the scene evolution within an event temporal proposal for captioning, and therefore perform less satisfactorily when the scenes and objects change over a relatively long proposal. To address this problem, we propose a graph-based partition-and-summarization (GPaS) framework for dense video captioning within two stages. For the "partition" stage, a whole event proposal is split into short video segments for captioning at a finer level. For the "summarization" stage, the generated sentences carrying rich description information for each segment are summarized into one sentence to describe the whole event. We particularly focus on the "summarization" stage, and propose a framework that effectively exploits the relationship between semantic words for summarization. We achieve this goal by treating semantic words as the nodes in a graph and learning their interactions by coupling Graph Convolutional Network (GCN) and Long Short Term Memory (LSTM), with the aid of visual cues. Two schemes of GCN-LSTM Interaction (GLI) modules are proposed for seamless integration of GCN and LSTM. The effectiveness of our approach is demonstrated via an extensive comparison with the state-of-the-arts methods on the two benchmarks ActivityNet Captions dataset and YouCook II dataset.
C1 [Zhang, Zhiwang; Xu, Dong; Ouyang, Wanli; Zhou, Luping] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2008, Australia.
C3 University of Sydney
RP Xu, D (corresponding author), Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2008, Australia.
EM zhiwang.zhang@sydney.edu.au; dong.xu@sydney.edu.au;
   wanli.ouyang@sydney.edu.au; luping.zhou@sydney.edu.au
RI Xu, Dong/A-3694-2011; Zhang, Zhiwang/HII-9869-2022
OI Zhang, Zhiwang/0000-0002-8867-3888; Zhou, Luping/0000-0001-8762-2424
FU Australian Research Council Future Fellowship [FT180100116]
FX This work was supported by the Australian Research Council Future
   Fellowship under Grant FT180100116.
CR [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2015, 3 INT C LEARNING REP
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Chen XP, 2018, PROC CVPR IEEE, P7995, DOI 10.1109/CVPR.2018.00834
   Denkowski Michael, 2014, P 9 WORKSH STAT MACH, DOI [10.3115/v1/W14-3348, DOI 10.3115/V1/W14-3348]
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duchenne O, 2009, IEEE I CONF COMP VIS, P1491, DOI 10.1109/ICCV.2009.5459279
   Erkan G, 2004, J ARTIF INTELL RES, V22, P457, DOI 10.1613/jair.1523
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kipf Thomas N., 2017, ICLR
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Li G, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1191, DOI 10.1145/2733373.2806314
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Li Y., 2018, EUR C COMP VIS ECCV, P684
   Li Y, 2018, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2018.00782
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lio P., 2018, P INT C LEARN REPR I
   Liu Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P631, DOI 10.1145/2964284.2967298
   Narasimhan M, 2018, ADV NEUR IN, V31
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shen ZQ, 2017, PROC CVPR IEEE, P5159, DOI 10.1109/CVPR.2017.548
   Shu X., 2019, ARXIV190913245
   Shu XB, 2021, IEEE T PATTERN ANAL, V43, P1110, DOI 10.1109/TPAMI.2019.2942030
   Shu XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P35, DOI 10.1145/2733373.2806216
   Su R, 2019, PROC CVPR IEEE, P12008, DOI 10.1109/CVPR.2019.01229
   Sutskever I, 2014, ADV NEUR IN, V27
   Tan JH, 2019, IEEE T MULTIMEDIA, V21, P2686, DOI 10.1109/TMM.2019.2904878
   Tang JH, 2022, IEEE T PATTERN ANAL, V44, P636, DOI 10.1109/TPAMI.2019.2928540
   Tang JH, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2998574
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Venugopalan S., 2015, P ANN C N AM CHAPT A, P1494, DOI 10.3115/v1/N15-1173
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang JW, 2018, PROC CVPR IEEE, P7190, DOI 10.1109/CVPR.2018.00751
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wu LX, 2020, IEEE T MULTIMEDIA, V22, P808, DOI 10.1109/TMM.2019.2931815
   Wu X, 2018, PROC CVPR IEEE, P6829, DOI 10.1109/CVPR.2018.00714
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yang ZL, 2016, ADV NEUR IN, V29
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Zhang ZW, 2020, IEEE T CIRC SYST VID, V30, P3130, DOI 10.1109/TCSVT.2019.2936526
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhou LW, 2018, PROC CVPR IEEE, P8739, DOI 10.1109/CVPR.2018.00911
NR 59
TC 24
Z9 24
U1 2
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1799
EP 1810
DI 10.1109/TMM.2020.3003592
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300025
DA 2024-07-18
ER

PT J
AU Li, RF
   Wang, N
   Feng, FX
   Zhang, GW
   Wang, XJ
AF Li, Ruifan
   Wang, Ning
   Feng, Fangxiang
   Zhang, Guangwei
   Wang, Xiaojie
TI Exploring Global and Local Linguistic Representations for Text-to-Image
   Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gallium nitride; Task analysis; Linguistics; Generators; Generative
   adversarial networks; Training; Correlation; Text-to-image synthesis;
   generative adversarial network (GAN); linguistic representation;
   cross-modal
AB The task of text-to-image synthesis is to generate photographic images conditioned on given textual descriptions. This challenging task has recently attracted considerable attention from the multimedia community due to its potential applications. Most of the up-to-date approaches are built based on generative adversarial network (GAN) models, and they synthesize images conditioned on the global linguistic representation. However, the sparsity of the global representation results in training difficulties on GANs and a shortage of fine-grained information in the generated images. To address this problem, we propose cross-modal global and local linguistic representations-based generative adversarial networks (CGL-GAN) by incorporating the local linguistic representation into the GAN. In our CGL-GAN, we construct a generator to synthesize the target images and a discriminator to judge whether the generated images conform with the text description. In the discriminator, we construct the cross-modal correlation by projecting the image representations at high and low levels onto the global and local linguistic representations, respectively. We design the hinge loss function to train our CGL-GAN model. We evaluate the proposed CGL-GAN on two publicly available datasets, the CUB and the MS-COCO. The extensive experiments demonstrate that incorporating fine-grained local linguistic information with cross-modal correlation can greatly improve the performance of text-to-image synthesis, even when generating high-resolution images.
C1 [Li, Ruifan; Wang, Xiaojie] Beijing Univ Posts & Telecommun, Minist Educ, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Li, Ruifan; Zhang, Guangwei; Wang, Xiaojie] Beijing Univ Posts & Telecommun, Minist Educ, Engn Res Ctr Informat Networks, Beijing 100876, Peoples R China.
   [Wang, Ning; Feng, Fangxiang] Beijing Univ Posts & Telecommun, Sch Comp Sci, Beijing 100876, Peoples R China.
   [Zhang, Guangwei] Beijing Univ Posts & Telecommun, Minist Educ, Inst Network Technol, Beijing 100876, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Beijing University of
   Posts & Telecommunications; Beijing University of Posts &
   Telecommunications; Beijing University of Posts & Telecommunications
RP Li, RF (corresponding author), Beijing Univ Posts & Telecommun, Minist Educ, Sch Comp Sci, Beijing 100876, Peoples R China.; Li, RF (corresponding author), Beijing Univ Posts & Telecommun, Minist Educ, Engn Res Ctr Informat Networks, Beijing 100876, Peoples R China.
EM rfli@bupt.edu.cn; niwang@bupt.edu.cn; f.fangxiang@gmail.com;
   gwzhang@bupt.edu.cn; xjwang@bupt.edu.cn
RI LI, Ruifan/AFM-1702-2022; Wang, Ning/JEO-4770-2023
OI Feng, Fangxiang/0000-0002-4798-4233
FU National Key R&D Program of China [2019YFF0303300, 2019YFF0303302];
   National Natural Science Foundation of China [61802026, 61906018];
   Science and Technology Program of the Headquarters of State Grid
   Corporation of China [5200-201918255A-0-0-00]; 111 Project [B08004]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2019YFF0303300 and Subject II under Grant 2019YFF0303302, in
   part by the National Natural Science Foundation of China under Grants
   61802026 and 61906018, and in part by the Science and Technology Program
   of the Headquarters of State Grid Corporation of China under Grant
   5200-201918255A-0-0-00, and in part by the 111 Project under Grant
   B08004. The associate editor coordinating the review of this manuscript
   and approving it for publication was Professor Chi-Chun Lee.
CR [Anonymous], 2017, GEOMETRIC GAN
   Arjovsky M., 2017, ARXIV170107875
   Arjovsky Martin, 2017, P INT C LEARN REPR
   Berthelot D, 2017, ARXIV
   Brock AM, 2018, PROCEEDINGS PERVASIVE DISPLAYS 2018: THE 7TH ACM INTERNATIONAL SYMPOSIUM ON PERVASIVE DISPLAYS, DOI 10.1145/3205873.3205877
   De Vries H., 2017, ADV NEURAL INFORM PR, P6594
   Feng FX, 2015, NEUROCOMPUTING, V154, P50, DOI 10.1016/j.neucom.2014.12.020
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gulrajani Ishaan, 2017, P ADV NEUR INF PROC, P5767
   Guo Y, 2019, IEEE T MULTIMEDIA, V21, P2726, DOI 10.1109/TMM.2019.2908352
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Huang X, 2019, IEEE T MULTIMEDIA, V21, P2850, DOI 10.1109/TMM.2019.2911456
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mao YZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4258
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Miyato T., 2018, P INT C LEARN REPR
   Miyato Takeru, 2018, CGANS PROJECTION DIS, DOI DOI 10.48550/ARXIV.1802.05637
   Odena A, 2017, PR MACH LEARN RES, V70
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Qiu ZF, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P225, DOI 10.1145/3077136.3080842
   Radford A, 2016, 4 INT C LEARNING REP
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shen CY, 2017, 2017 14TH CHINA INTERNATIONAL FORUM ON SOLID STATE LIGHTING (SSLCHINA) : INTERNATIONAL FORUM ON WIDE BANDGAP SEMICONDUCTORS (IFWS), P9, DOI 10.1109/IFWS.2017.8245963
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Xiao XY, 2019, IEEE T MULTIMEDIA, V21, P2942, DOI 10.1109/TMM.2019.2915033
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
NR 42
TC 25
Z9 26
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3075
EP 3087
DI 10.1109/TMM.2020.2972856
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700005
DA 2024-07-18
ER

PT J
AU Lu, X
   Zhu, L
   Li, JJ
   Zhang, HX
   Shen, HT
AF Lu, Xu
   Zhu, Lei
   Li, Jingjing
   Zhang, Huaxiang
   Shen, Heng Tao
TI Efficient Supervised Discrete Multi-View Hashing for Large-Scale
   Multimedia Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Semantics; Optimization; Training; Quantization (signal);
   Kernel; Search problems; Hashing; multi-view; multimedia search
ID CODES
AB Hashing has recently received substantial attention in large-scale multimedia search for its extremely low-cost storage cost and high retrieval efficiency. However, most existing hashing techniques focus on learning hash codes for single-view or cross-view retrieval. It is still an unsolved problem that how to efficiently learn discriminative binary codes for multi-view data that is common in real world multimedia search. In this paper, we propose an efficient Supervised Discrete Multi-view Hashing (SDMH) to solve the problem. SDMH first properly detects the shared binary hash codes, with an integrated multi-view feature mapping and latent hash coding, by exploiting the complementarity of different view-specific features and removing the involved inter-view redundancy. To further enhance the discriminative capability of hash codes, SDMH directly represses the explicit semantic labels of data samples with their corresponding binary codes. Different from most existing multi-view hashing methods that adopt "relaxing+rounding" hash optimization strategy or the discrete optimization method based on discrete cyclic coordinate descent, an efficient augmented Lagrangian multiplier (ALM) based discrete hash optimization method is developed in this paper to optimize the hash codes within a single step. Experimental results on four benchmark datasets demonstrate the superior performance of the proposed approach over state-of-the-art hashing techniques, in terms of both learning efficiency and retrieval accuracy.
C1 [Lu, Xu; Zhu, Lei; Zhang, Huaxiang] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
   [Li, Jingjing; Shen, Heng Tao] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
C3 Shandong Normal University; University of Electronic Science &
   Technology of China
RP Zhu, L; Zhang, HX (corresponding author), Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Peoples R China.
EM lxuu306@hotmail.com; leizhu0608@gmail.com; lijin117@yeah.net;
   huaxzhang@hotmail.com; shenhengtao@hotmail.com
RI li, jy/HTT-1535-2023; LI, Jing/HNB-5575-2023; Zhu, Lei/GQQ-1130-2022;
   Lu, Xu/ABF-9543-2021; li, jian/IAQ-2794-2023; Li, Jing/GYU-5036-2022;
   Shen, Heng Tao/ABD-5331-2021
OI Zhu, Lei/0000-0002-5348-7532; Lu, Xu/0000-0002-8459-3186; Zhu,
   Lei/0000-0002-2993-7142; zhang, hua xiang/0000-0001-6259-7533
FU National Natural Science Foundation of China [61772322, 61802236,
   61572298, U1836216]; Natural Science Foundation of Shandong, China
   [ZR2019QF002]; Taishan Scholar Project of Shandong Province, China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61772322, 61802236, 61572298, and
   U1836216, in part by the Natural Science Foundation of Shandong, China,
   under Grant ZR2019QF002, and in part by Taishan Scholar Project of
   Shandong Province, China. The associate editor coordinating the reviewof
   this manuscript and approving it for publicationwas Prof. Benoit HUET.
CR Alex K., 2009, THESIS
   [Anonymous], 2010, CORR
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Hao YB, 2017, IEEE T MULTIMEDIA, V19, P1, DOI 10.1109/TMM.2016.2610324
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Irie G, 2015, IEEE I CONF COMP VIS, P1886, DOI 10.1109/ICCV.2015.219
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Kim S, 2013, INT CONF ACOUST SPEE, P3123, DOI 10.1109/ICASSP.2013.6638233
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liu Hong, 2016, IJCAI, P1767, DOI DOI 10.1109/TIP.2016.2564638
   Liu L, 2015, IEEE T IMAGE PROCESS, V24, P956, DOI 10.1109/TIP.2015.2390975
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1606, DOI 10.1109/TIP.2014.2305072
   Murty KL, 2013, WOODHEAD PUBL SER EN, P3, DOI 10.1533/9780857097453.1.3
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen XB, 2016, IEEE SIGNAL PROC LET, V23, P893, DOI 10.1109/LSP.2016.2517093
   Shen XB, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3178119
   Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Shen XB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P831, DOI 10.1145/2733373.2806342
   Sheng Q, 2013, 2013 FIFTH INTERNATIONAL CONFERENCE ON GEO-INFORMATION TECHNOLOGIES FOR NATURAL DISASTER MANAGEMENT (GIT4NDM), P143, DOI 10.1109/GIT4NDM.2013.27
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Wang DX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2291
   Wang D, 2018, IEEE T CIRC SYST VID, V28, P2703, DOI 10.1109/TCSVT.2017.2723302
   Wang D, 2019, IEEE T PATTERN ANAL, V41, P2466, DOI 10.1109/TPAMI.2018.2861000
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3890
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xie L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3133
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu D, 2012, IEEE INFOCOM SER, P2881, DOI 10.1109/INFCOM.2012.6195720
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhang CH, 2017, IEEE T IMAGE PROCESS, V26, P2604, DOI 10.1109/TIP.2017.2675205
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang JY, 2018, LECT NOTES COMPUT SC, V11206, P304, DOI 10.1007/978-3-030-01216-8_19
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zheng HW, 2017, 2017 14TH CHINA INTERNATIONAL FORUM ON SOLID STATE LIGHTING (SSLCHINA) : INTERNATIONAL FORUM ON WIDE BANDGAP SEMICONDUCTORS (IFWS), P175, DOI 10.1109/IFWS.2017.8246003
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
   Zhu L, 2017, IEEE T CYBERNETICS, V47, P3941, DOI 10.1109/TCYB.2016.2591068
   Zhu L, 2017, IEEE T MULTIMEDIA, V19, P2066, DOI 10.1109/TMM.2017.2729025
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
NR 50
TC 49
Z9 51
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 2048
EP 2060
DI 10.1109/TMM.2019.2947358
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500011
DA 2024-07-18
ER

PT J
AU Yuan, MK
   Peng, YX
AF Yuan, Mingkuan
   Peng, Yuxin
TI CKD: Cross-Task Knowledge Distillation for Text-to-Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Visualization; Task analysis; Image synthesis; Generative
   adversarial networks; Neural networks; Image color analysis;
   Text-to-image synthesis; knowledge distillation; transfer learning;
   image semantic understanding
ID REPRESENTATION
AB Text-to-image synthesis (T2IS) has drawn increasing interest recently, which can automatically generate images conditioned on text descriptions. It is a highly challenging task that learns a mapping from a semantic space of text description to a complex RGB pixel space of image. The main issues of T2IS lie in two aspects: semantic consistency and visual quality. The distributions between text descriptions and image contents are inconsistent since they belong to different modalities. So it is ambitious to generate images containing consistent semantic contents with the text descriptions, which is the semantic consistency issue. Moreover, due to the discrepancy of data distributions between real and synthetic images in huge pixel space, it is hard to approximate the real data distribution for synthesizing photo-realistic images, which is the visual quality issue. For addressing the above issues, we propose a cross-task knowledge distillation (CKD) approach to transfer knowledge from multiple image semantic understanding tasks into T2IS task. There is amount of knowledge in image semantic understanding tasks to translate image contents into semantic representation, which is advantageous to address the issues of semantic consistency and visual quality for T2IS. Moreover, we design a multi-stage knowledge distillation paradigm to decompose the distillation process into multiple stages. By this paradigm, it is effective to approximate the distributions of real image and understand textual information for T2IS, which can improve the visual quality and semantic consistency of synthetic images. Comprehensive experiments on widely-used datasets show the effectiveness of our proposed CKD approach.
C1 [Yuan, Mingkuan; Peng, Yuxin] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
C3 Peking University
RP Peng, YX (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM yuanmingkuan@pku.edu.cn; pengyuxin@pku.edu.cn
FU National Natural Science Foundation of China [61925201, 61771025]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61925201 and Grant 61771025. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. VasileiosMezaris.
CR [Anonymous], 2016, TECH REP
   [Anonymous], 2016, P INT C MACH LEARN
   Arjovsky, 2017, ARXIV170104862
   Ba Jimmy, 2014, ADV NEURAL INFORM PR
   Berthelot David, 2017, CoRR
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Dean J., 2015, NIPS DEEP LEARNING R
   Deng CH, 2018, PROC INT CONF DATA, P1220, DOI 10.1109/ICDE.2018.00115
   Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregor K., 2015, P INT C MACH LEARN, P1747
   Gulrajani I., 2017, Advances in neural information processing systems, P5769
   Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309
   GUZMANRIVERA A, 2012, ADV NEURAL INFORM PR, P1799
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   King DB, 2015, ACS SYM SER, V1214, P1
   Kingma D. P., 2014, arXiv
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee DH, 2018, INT C PATT RECOG, P3390, DOI 10.1109/ICPR.2018.8545121
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Liu YC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P700, DOI 10.1145/3240508.3240567
   Lovay M, 2015, PROCEEDINGS OF THE 2015 ARGENTINE SCHOOL OF MICRO-NANOELECTRONICS, TECHNOLOGY AND APPLICATIONS (EAMTA), P1, DOI 10.1109/EAMTA.2015.7237369
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Mansimov E., 2016, P INT C LEARN REPR
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Peng CL, 2016, IEEE T NEUR NET LEAR, V27, P2201, DOI 10.1109/TNNLS.2015.2464681
   Peng YX, 2019, IEEE T MULTIMEDIA, V21, P1538, DOI 10.1109/TMM.2018.2877885
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Ranzato M., 2010, Advances in Neural Information Processing Systems 22 (NIPS'09), P2002
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2016, PR MACH LEARN RES, V48
   Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy Christian, 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.308
   Thrun S, 1998, LEARNING TO LEARN, P181
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   van den Oord A, 2016, PR MACH LEARN RES, V48
   van den Oord Aaron, 2016, ARXIV160605328
   Vinyals O, 2017, IEEE T PATTERN ANAL, V39, P652, DOI 10.1109/TPAMI.2016.2587640
   Wah C, 2011, CALTECH UCSD BIRDS 2
   Wang XL, 2017, PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATIONS (ICCC), P1174, DOI 10.1109/CompComm.2017.8322728
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Yuan MK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1407, DOI 10.1145/3240508.3240559
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang SC, 2016, IEEE T IMAGE PROCESS, V25, P220, DOI 10.1109/TIP.2015.2501755
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
NR 64
TC 51
Z9 54
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1955
EP 1968
DI 10.1109/TMM.2019.2951463
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500004
DA 2024-07-18
ER

PT J
AU Gu, DH
   Li, YW
   Jiang, F
   Wen, ZJ
   Liu, SH
   Shi, WZ
   Lu, GM
   Zhou, CS
AF Gu, Donghao
   Li, Yaowei
   Jiang, Feng
   Wen, Zhaojing
   Liu, Shaohui
   Shi, Wuzhen
   Lu, Guangming
   Zhou, Changsheng
TI VINet: A Visually Interpretable Image Diagnosis Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Medical services; Biomedical imaging; Estimation;
   Computational modeling; Solid modeling; Task analysis; Machine learning;
   neural network; image classification; medical diagnostic imaging
ID PULMONARY NODULES; CT; CLASSIFICATION
AB Recently, due to the black box characteristics of deep learning techniques, the deep network-based computer-aided diagnosis (CADx) systems have encountered many difficulties in practical applications. The crux of the problem is that these models should be explainable the model should give doctors rationales that can explain the diagnosis. In this paper, we propose a visually interpretable network (VINet) which can generate diagnostic visual interpretations while making accurate diagnoses. VINet is an end-to-end model consisting of an importance estimation network and a classification network. The former produces a diagnostic visual interpretation for each case, and the classifier diagnoses the case. In the classifier, by exploring the information in the diagnostic visual interpretation, the irrelevant information in the feature maps is eliminated by our proposed feature destruction process. This allows the classification network to concentrate on the important features and use them as the primary references for classification. Through a joint optimization of higher classification accuracy and eliminating as many irrelevant features as possible, a precise, fine-grained diagnostic visual interpretation, along with an accurate diagnosis, can be produced by our proposed network simultaneously. Based on a computed tomography image dataset (LUNA16) on pulmonary nodule, extensive experiments have been conducted, demonstrating that the proposed VINet can produce state-of-the-art diagnostic visual interpretations compared with all baseline methods.
C1 [Gu, Donghao; Li, Yaowei; Jiang, Feng; Wen, Zhaojing; Liu, Shaohui; Shi, Wuzhen] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Jiang, Feng; Liu, Shaohui; Shi, Wuzhen] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Lu, Guangming; Zhou, Changsheng] Nanjing Jinling Hosp, Dept Med Imaging, Nanjing 210002, Peoples R China.
C3 Harbin Institute of Technology; Peng Cheng Laboratory
RP Jiang, F (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM 18s103170@stu.hit.edu.cn; ywli@hit.edu.cn; fjiang@hitedu.cn;
   18s103172@stu.hit.edu.cn; shliu@hit.edu.cn; wzhshi@hit.edu.cn;
   cjr.luguangming@vip.163.com; 66368823@qq.com
RI JIANG, Feng/HTP-2862-2023; Liu, shaohui/HKE-1383-2023
OI Jiang, Feng/0000-0001-8342-1211; Li, Yaowei/0000-0002-8595-7930; Liu,
   Shaohui/0000-0002-1810-5412
FU National Key Research and Development Program of China [2018YFC0806802,
   2018YFC0832105]
FX This work was supported by the National Key Research and Development
   Program of China under Grants 2018YFC0806802 and 2018YFC0832105. The
   code of this work is released at https://github.com/plantabrick/VINet.
CR [Anonymous], 2014, WORKSHOP INT C LEARN
   [Anonymous], 2012, P BIOSIGNALS
   Armato III S. G., 2015, DATA LIDC IDRI CANC, V9, P7
   Armato SG, 2011, MED PHYS, V38, P915, DOI 10.1118/1.3528204
   Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Baehrens D, 2010, J MACH LEARN RES, V11, P1803
   Bojarski M, 2018, IEEE INT CONF ROBOT, P4701
   Chen DL, 2018, THER CLIN RISK MANAG, V14, P203, DOI 10.2147/TCRM.S152127
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51
   Dosovitskiy A, 2016, PROC CVPR IEEE, P4829, DOI 10.1109/CVPR.2016.522
   Ecker C, 2010, J NEUROSCI, V30, P10612, DOI 10.1523/JNEUROSCI.5413-09.2010
   Erhan D, 2009, Univ Montr, V1341, P1
   Fourure Damien, 2017, BRIT MACH VIS C
   Girshick R., 2014, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Haufe S, 2014, NEUROIMAGE, V87, P96, DOI 10.1016/j.neuroimage.2013.10.067
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Kim M., 2018, MACH LEARN HLTH WORK
   Klöppel S, 2008, BRAIN, V131, P681, DOI 10.1093/brain/awm319
   Kumar D, 2019, IEEE ACCESS, V7, P25891, DOI 10.1109/ACCESS.2019.2893635
   KURIYAMA K, 1987, AM J ROENTGENOL, V149, P1139, DOI 10.2214/ajr.149.6.1139
   Lahav O., 2018, MACH LEARN HLTH WORK
   Lee HY, 2011, J THORAC IMAG, V26, P106, DOI 10.1097/RTI.0b013e3181fbaa64
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Liao FZ, 2019, IEEE T NEUR NET LEAR, V30, P3484, DOI 10.1109/TNNLS.2019.2892409
   Massa F., 2015, ADV NEURAL INFORM PR, P8024
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Paszke A., 2019, Adv. Neural Inf. Process. Syst. (NeurIPS), P8024
   Robnik-Sikonja M, 2008, IEEE T KNOWL DATA EN, V20, P589, DOI 10.1109/TKDE.2007.190734
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Setio AAA, 2017, MED IMAGE ANAL, V42, P1, DOI 10.1016/j.media.2017.06.015
   SIEGELMAN SS, 1986, RADIOLOGY, V160, P307, DOI 10.1148/radiology.160.2.3726105
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Springenberg J. T., 2015, ARXIV PREPRINT ARXIV
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Swensen SJ, 2005, RADIOLOGY, V235, P259, DOI 10.1148/radiol.2351041662
   Wang Z, 2007, NEUROIMAGE, V36, P1139, DOI 10.1016/j.neuroimage.2007.03.072
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
   Zintgraf L. M., 2017, ARXIV170204595, P1
   ZWIREWICH CV, 1991, RADIOLOGY, V179, P469, DOI 10.1148/radiology.179.2.2014294
NR 48
TC 19
Z9 19
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1720
EP 1729
DI 10.1109/TMM.2020.2971170
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500006
DA 2024-07-18
ER

PT J
AU Xiao, WH
   He, HG
   Wang, TT
   Chao, HY
AF Xiao, Wenhui
   He, Huiguo
   Wang, Tingting
   Chao, Hongyang
TI The Interpretable Fast Multi-Scale Deep Decoder for the Standard HEVC
   Bitstreams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Encoding; Decoding; Standards; Redundancy; Computational
   efficiency; Neural networks; HEVC; multi-scale similarity; compression
   efficiency; deep learning; interpretability
ID MOTION ESTIMATION; PREDICTION; MODEL
AB It is a research hotspot to restore decoded videos with existing bitstreams by applying deep neural network to improve compression efficiency at decoder-end. Existing research has verified that the utilization of redundancy at decoder-end, which is underused by the encoder, can bring an increase of compression efficiency. However, most existing research neglects the abundant multi-scale information among video frames as a typical type of such redundancy. It remains an interesting yet challenging topic how to build an effective, interpretable and fast deep neural network for the purpose of using the multi-scale similarity at decoder-end and further enhancing compression efficiency. To this end, this paper considers the use of underused inter multi-scale information and proposes the Fast Multi-Scale Deep Decoder (Fast MSDD) for the state-of-the-art video coding standard HEVC. The advantages of Fast MSDD are three-fold. First, it achieves a higher coding efficiency without modifying any encoding algorithm. Second, Fast MSDD is interpretable based on the framework of using the underused redundancy. Third, it guarantees the model's inference speed while fully using the multi-scale similarity among video frames. Extensive experimental results verify Fast MSDD's effectiveness, interpretability, and computational efficiency. Fast MSDD obtains averagely 14.3%, 10.8%, 8.5% and 7.6% BD gains for AI, LP, LB and RA respectively. Compared with our previous work MSDD, Fast MSDD achieves increases of 59.3%, 49.1%, 61.0% and 29.3%. Meanwhile, 16.9%, 11.2%, 9.2% and 8.3% BD gains are observed on videos with scale changes, which validate the interpretability of the proposed method. Furthermore, Fast MSDD can save at most 56.3% time compared to MSDD.
C1 [Xiao, Wenhui; He, Huiguo; Chao, Hongyang] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
   [Xiao, Wenhui] Tencent Media Lab, Shenzhen 518057, Peoples R China.
   [Wang, Tingting] Microsoft Corp, Redmond, WA 98052 USA.
C3 Sun Yat Sen University; Microsoft
RP Chao, HY (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.; Wang, TT (corresponding author), Microsoft Corp, Redmond, WA 98052 USA.
EM xiaowh3@mail2.sysu.edu.cn; hehg3@mail2.sysu.edu.cn;
   tiwang@microsoft.com; isschhy@mail.sysu.edu.cn
OI Xiao, Wenhui/0000-0002-1933-0451
FU National Science Foundation of China [61672548, U1611461, 61173081];
   Guangzhou Science and Technology Program, China [201510010165]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61672548, U1611461, and 61173081 and in part by
   Guangzhou Science and Technology Program, China, under Grant
   201510010165.
CR Abadi Martin, 2016, arXiv
   Al-Regib G, 2003, IEEE T CIRC SYST VID, V13, P1000, DOI 10.1109/TCSVT.2003.816520
   Bjontegaard G, 2001, VCEGM33
   Bossen F., 2013, JCTVCL1100
   Bossen F., 2014, HM SOFTWARE MANUAL
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C., 2016, ADV NEURAL INFORM PR, P3468
   Guan ZY, 2021, IEEE T PATTERN ANAL, V43, P949, DOI 10.1109/TPAMI.2019.2944806
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang H, 2013, IEEE T CIRC SYST VID, V23, P1651, DOI 10.1109/TCSVT.2013.2254977
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jin ZP, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1368, DOI 10.1109/ICASSP.2018.8461356
   Kim HS, 2012, IEEE T CIRC SYST VID, V22, P1280, DOI 10.1109/TCSVT.2012.2198137
   Kingma D. P., 2014, arXiv
   Kordasiewicz RC, 2007, IEEE T CIRC SYST VID, V17, P1388, DOI 10.1109/TCSVT.2007.903777
   Li C, 2017, IEEE IMAGE PROC, P4577, DOI 10.1109/ICIP.2017.8297149
   Li DP, 2015, PROC CVPR IEEE, P213, DOI 10.1109/CVPR.2015.7298617
   Li L, 2018, IEEE T CIRC SYST VID, V28, P1934, DOI 10.1109/TCSVT.2017.2699919
   Li L, 2015, IEEE INT SYMP CIRC S, P525, DOI 10.1109/ISCAS.2015.7168686
   Ma L, 2018, 2018 IEEE FOURTH INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM)
   Narroschke M, 2013, PICT COD SYMP, P321, DOI 10.1109/PCS.2013.6737748
   Po LM, 2010, IEEE T CIRC SYST VID, V20, P1625, DOI 10.1109/TCSVT.2010.2087474
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song XD, 2018, IEEE IMAGE PROC, P1133, DOI 10.1109/ICIP.2018.8451589
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Wang TT, 2018, IEEE DATA COMPR CONF, P197, DOI 10.1109/DCC.2018.00028
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wiegand T, 2005, IEEE T CIRC SYST VID, V15, P197, DOI 10.1109/TCSVT.2004.841690
   Xiph.org, XIPH ORG DERFS TEST
   Yang R, 2019, IEEE T CIRC SYST VID, V29, P2039, DOI 10.1109/TCSVT.2018.2867568
   Yang R, 2018, PROC CVPR IEEE, P6664, DOI 10.1109/CVPR.2018.00697
   Yang R, 2017, IEEE INT CON MULTI, P817, DOI 10.1109/ICME.2017.8019299
   Yu HT, 2009, IEEE INT SYMP CIRC S, P629, DOI 10.1109/ISCAS.2009.5117827
   Yu LW, 2019, IEEE SIGNAL PROC LET, V26, P557, DOI 10.1109/LSP.2019.2899253
   Yuan H, 2012, IEEE T MULTIMEDIA, V14, P1370, DOI 10.1109/TMM.2012.2190393
   Yuan H, 2010, IEEE SIGNAL PROC LET, V17, P787, DOI 10.1109/LSP.2010.2055051
NR 39
TC 8
Z9 9
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1680
EP 1691
DI 10.1109/TMM.2020.2978664
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500003
DA 2024-07-18
ER

PT J
AU Tan, JH
   Chan, CS
   Chuah, JH
AF Tan, Jia Huei
   Chan, Chee Seng
   Chuah, Joon Huang
TI COMIC: Toward A Compact Image Captioning Model With Attention
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image captioning; deep compression network; deep learning
AB Recent works in image captioning have shown very promising raw performance. However, we realize that most of these encoder-decoder style networks with attention do not scale naturally to large vocabulary size, making them difficult to deploy on embedded systems with limited hardware resources. This is because the size of word and output embedding matrices grow proportionally with the size of vocabulary, adversely affecting the compactness of these networks. To address this limitation, this paper introduces a brand new idea in the domain of image captioning. That is, we tackle the problem of compactness of image captioning models which is hitherto unexplored. We showed that our proposed model, named COMIC for compact image captioning, achieves comparable results in five common evaluation metrics with state-of-the-art approaches on both MS-COCO and InstaPIC-1.1M datasets despite having an embedded vocabulary size that is 39x-99x smaller.
C1 [Tan, Jia Huei; Chan, Chee Seng] Univ Malaya, Ctr Image & Signal Proc, Dept Artificial Intelligence, Fac Comp Sci & Informat Technol, Kuala Lumpur 50603, Malaysia.
   [Chuah, Joon Huang] Univ Malaya, Dept Elect Engn, Fac Engn, Kuala Lumpur 50603, Malaysia.
C3 Universiti Malaya; Universiti Malaya
RP Chan, CS (corresponding author), Univ Malaya, Ctr Image & Signal Proc, Dept Artificial Intelligence, Fac Comp Sci & Informat Technol, Kuala Lumpur 50603, Malaysia.
EM tanjiahuei@siswa.um.edu.my; cs.chan@um.edu.my; jhchuah@um.edu.my
RI Chuah, Joon Huang/F-9990-2010; Chan, Chee Seng/B-9754-2011
OI Chuah, Joon Huang/0000-0001-9058-3497; Chan, Chee
   Seng/0000-0001-7677-2865; Tan, Jia Huei/0000-0001-7743-848X
FU University of Malaya [FG002-17AFR]
FX This work was supported by the UMFrontier Research under Grant
   FG002-17AFR, from the University of Malaya. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Abdulmotaleb El Saddik.
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], ARXIV170803312
   [Anonymous], 2017, P IEEE C COMPUTER VI, DOI DOI 10.48550/ARXIV.1704.03899
   [Anonymous], 2017, P 2017 C EMP METH NA, DOI [DOI 10.18653/V1/D17-1151, 10.18653/v1/D17-1151]
   [Anonymous], 2015, EMPIRICAL METHODS NA
   [Anonymous], 2015, ARXIV150501809
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], P 20 INT C COMP LING
   [Anonymous], 2016, P C N AM CHAPT ASS C, DOI DOI 10.18653/V1/N16-1155
   [Anonymous], 2016, ARXIV160207360
   [Anonymous], 2016, ARXIV161101646
   [Anonymous], 2016, ARXIV160805859
   [Anonymous], 2016, ADV NEURAL INFORM PR
   [Anonymous], 2017, Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2004, 42 ANN M ASS COMP LI
   Bahdanau Dzmitry, 2014, NEURAL MACHINE TRANS
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754
   Fu K, 2017, IEEE T PATTERN ANAL, V39, P2321, DOI 10.1109/TPAMI.2016.2642953
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Glorot X., 2010, P INT C ART INT STAT, P249
   Han S., 2015, ARXIV151000149
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kaiser Lukasz, 2017, arXiv
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kingma D. P., 2014, arXiv
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Li X., 2016, Advances in Neural Information Processing Systems, P4385
   Liao WS, 2016, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING, 2016, VOL 1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Sennrich R., 2015, ARXIV150807909
   Sutskever I, 2014, ADV NEUR IN, V27
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tan YH, 2019, NEUROCOMPUTING, V333, P86, DOI 10.1016/j.neucom.2018.12.026
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3115432
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhang X, 2017, ARXIV170802657
   Zhao N, 2016, PROCEEDINGS OF 2016 8TH INTERNATIONAL CONFERENCE ON MODELLING, IDENTIFICATION & CONTROL (ICMIC 2016), P989, DOI 10.1109/ICMIC.2016.7804258
NR 52
TC 36
Z9 37
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2686
EP 2696
DI 10.1109/TMM.2019.2904878
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, X
   Li, H
   Fan, YL
   Chen, R
AF Yang, Xi
   Li, Hui
   Fan, Yu-Long
   Chen, Rong
TI Single Image Haze Removal via Region Detection Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Haze removal; convolutional neural network; regional detection network;
   detail enhancement
ID ALGORITHM; FRAMEWORK; VISION
AB Haze removal typically works on a physical model to estimate how light is transmitted and lost due to absorption and scattering through the atmosphere. In this paper, a region detection network is proposed to learn the relationship between the hazy image and the medium transmission map in a patchwise manner; the transmission map is then used to remove haze via an atmospheric scattering model and enhance the detail of dehazed images. To this end, we design a simple yet powerful deep convolutional neural network, which mainly consists of two types of network units and can be trained in an end-to-end manner. One network unit is a module with the residual structure that facilitates the learning process of the deep network. The other is a novel module with a cascaded cross channel pool, which fuses multi-level haze-relevant features and boosts the abstraction ability of the model on a nonlinear manifold. Moreover, an evolutionary-based enhancement method is developed to improve the level of detail of over-smoothed results. Several comparative experiments have been conducted on synthetic and real images, through which we conclude that the proposed method achieves state-of-the-art haze removal results, qualitatively and quantitatively. Supplementary experiments further indicate that our method works better against other adverse effects on vision quality (e.g., the mist formed by heavy rain and the veil met underwater). Moreover, we present a lightweight version of the proposed network, which achieves an impressive haze removal performance even on low-power devices.
C1 [Yang, Xi; Li, Hui; Fan, Yu-Long; Chen, Rong] Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Peoples R China.
C3 Dalian Maritime University
RP Chen, R (corresponding author), Dalian Maritime Univ, Coll Informat Sci & Technol, Dalian 116026, Peoples R China.
EM yokiqust@dlmu.edu.cn; li_hui@dlmu.edu.cn; sumihui@dlmu.edu.cn;
   rchen@dlmu.edu.cn
RI Chen, Rong/AAM-2436-2020
OI Chen, Rong/0000-0001-5848-6398; Fan, Yulong/0000-0003-3870-3425
FU National Natural Science Foundation of China [61602077, 61672122,
   61402070]; Natural Science Foundation of Liaoning Province of China
   [20170540097]; Fundamental Research Funds for the Central Universities
   [3132016348]; Next-Generation Internet Innovation Project of CERNET
   [NGII20181205]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61602077, Grant 61672122, and Grant 61402070, in part
   by the Natural Science Foundation of Liaoning Province of China under
   Grant 20170540097, in part by the Fundamental Research Funds for the
   Central Universities under Grant 3132016348, and in part by the
   Next-Generation Internet Innovation Project of CERNET under Grant
   NGII20181205.
CR Angitha PV, 2017, 2017 INTERNATIONAL CONFERENCE OF ELECTRONICS, COMMUNICATION AND AEROSPACE TECHNOLOGY (ICECA), VOL 2, P425, DOI 10.1109/ICECA.2017.8212850
   [Anonymous], 2013, ICML
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, PROC 10 INT S COMMUN
   [Anonymous], 2018, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2018.2807593
   [Anonymous], 2014, IEEE International Conference on Computational Photography (ICCP)
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2014, APPL REGRESSION ANAL
   [Anonymous], 2017, Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications
   Berman D, 2017, IEEE INT CONF COMPUT, P115
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gao YY, 2019, IEEE T MULTIMEDIA, V21, P351, DOI 10.1109/TMM.2018.2856095
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Hautiere Nicolas, 2008, Image Analysis & Stereology, V27, P87, DOI 10.5566/ias.v27.p87-95
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Hong Y, 2017, IEEE INT C BIOINFORM, P1357, DOI 10.1109/BIBM.2017.8217860
   Karaboga D., 2004, Turkish Journal Electrical Engineering and Computer Sciences, Elektrik, V12, P53
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   Lin M., 2013, P 2 INT C LEARNING R
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   McCartney E.J., 1976, Optics of the atmosphere: Scattering by molecules and particles, P421
   MCDONALD JE, 1963, J ATMOS SCI, V20, P476, DOI 10.1175/1520-0469(1963)020<0476:TSAINM>2.0.CO;2
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321
   Ren WQ, 2018, PROC CVPR IEEE, P3253, DOI 10.1109/CVPR.2018.00343
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Song YF, 2018, IEEE T MULTIMEDIA, V20, P1548, DOI 10.1109/TMM.2017.2771472
   Stark JA, 2000, IEEE T IMAGE PROCESS, V9, P889, DOI 10.1109/83.841534
   Tahir MA, 2013, IEEE T MULTIMEDIA, V15, P1653, DOI 10.1109/TMM.2013.2264927
   Tan R. T., 2008, P IEEE C COMP VIS PA, P1
   Tan SC, 2015, IEEE T NEUR NET LEAR, V26, P933, DOI 10.1109/TNNLS.2014.2329097
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Tsai TH, 2012, IEEE T MULTIMEDIA, V14, P669, DOI 10.1109/TMM.2011.2180705
   Wang DX, 2015, IEEE T MULTIMEDIA, V17, P1404, DOI 10.1109/TMM.2015.2455415
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yang MM, 2018, IEEE T MULTIMEDIA, V20, P3008, DOI 10.1109/TMM.2018.2820327
   Yu J, 2010, INT CONF SIGN PROCES, P1048, DOI 10.1109/ICOSP.2010.5655901
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang H, 2018, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2018.00337
   Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 60
TC 26
Z9 26
U1 2
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2019
VL 21
IS 10
BP 2545
EP 2560
DI 10.1109/TMM.2019.2908375
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JD1IM
UT WOS:000489728400010
DA 2024-07-18
ER

PT J
AU Ruan, WJ
   Chen, J
   Wu, Y
   Wang, JQ
   Liang, C
   Hu, RM
   Jiang, JJ
AF Ruan, Weijian
   Chen, Jun
   Wu, Yi
   Wang, Jinqiao
   Liang, Chao
   Hu, Ruimin
   Jiang, Junjun
TI Multi-Correlation Filters With Triangle-Structure Constraints for Object
   Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation filters; partial occlusions; triangle structure; high
   energy; high integrity
ID VISUAL TRACKING; NONRIGID OBJECT
AB Correlation filters (CFs) have been extensively used in tracking tasks due to their high efficiency although most of them regard the tracked target as a whole and are minimally effective in handling partial occlusion. In this study, we incorporate a part-based strategy into the framework of CFs and propose a novel multipart correlation tracker with triangle-structure constraints. Specifically, we train multiple CFs for the global object and local parts, which are then jointly applied to obtain the correlation response of any candidate during tracking. The tracker is robust in handling partial occlusion because of the use of part-based representation. The remaining global representation can contribute reliable cues in cases wherein several local filters drift away in a specific scene. We further propose a triangle-structure model to measure the structural similarity of candidates. The model employs multiple triangles to determine the spatial relationship among parts and helps constrain the location of the target. Moreover, we introduce an effective part selection scheme based on energy and integrity, which is generally applicable to part-tracking models. Extensive experiments on two public benchmarks demonstrate the superiority of the proposed method over the state-of-the-art approaches.
C1 [Ruan, Weijian; Chen, Jun; Liang, Chao; Hu, Ruimin] Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Hubei, Peoples R China.
   [Ruan, Weijian; Chen, Jun; Liang, Chao; Hu, Ruimin] Collaborat Innovat Ctr Geospatial Technol, Wuhan 430072, Hubei, Peoples R China.
   [Wu, Yi] CuraCloud Corp, Seattle, WA 98104 USA.
   [Wu, Yi] Nanjing Audit Univ, Nanjing 211815, Jiangsu, Peoples R China.
   [Wang, Jinqiao] Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Jiang, Junjun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Jiang, Junjun] Peng Cheng Lab, Shenzhen, Peoples R China.
C3 Wuhan University; Nanjing Audit University; Chinese Academy of Sciences;
   Institute of Automation, CAS; Harbin Institute of Technology; Peng Cheng
   Laboratory
RP Chen, J (corresponding author), Wuhan Univ, Sch Comp, Natl Engn Res Ctr Multimedia Software, Wuhan 430072, Hubei, Peoples R China.
EM rweij@whu.edu.cn; chenj.whu@gmail.com; ywu.china@gmail.com;
   jqwang@nlpr.ia.ac.cn; cliang@whu.edu.cn; hrm1964@gmail.com;
   jiangjunjun@hit.edu.cn
RI Chen, Jun/AAD-8167-2022; Jiang, Junjun/L-7087-2019
OI Jiang, Junjun/0000-0002-5694-505X; wang, jin qiao/0000-0002-9118-2780;
   Hu, Ruimin/0000-0002-0290-5757; Ruan, Weijian/0000-0003-3710-8739
FU National Nature Science Foundation of China [U1611461, 61876135,
   61876086]; National Key R&D Program of China [2017YFC0803700]; Hubei
   Province Technological Innovation Major Project [2017AAA123,
   2018AAA062]; Nature Science Foundation of Jiangsu Province [BK20160386]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants U1611461, 61876135, and 61876086, in
   part by the National Key R&D Program of China under Grant
   2017YFC0803700, in part by Hubei Province Technological Innovation Major
   Project under Grants 2017AAA123 and 2018AAA062, and the Nature Science
   Foundation of Jiangsu Province under Grant BK20160386.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2015, PROC CVPR IEEE
   Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao J, 2014, LECT NOTES COMPUT SC, V8691, P188, DOI 10.1007/978-3-319-10578-9_13
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Jia X, 2012, PROC CVPR IEEE, P1822, DOI 10.1109/CVPR.2012.6247880
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kristan M, 2015, LECT NOTES COMPUT SC, V8926, P191, DOI 10.1007/978-3-319-16181-5_14
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P2427, DOI 10.1109/TPAMI.2013.32
   Kwon J, 2009, PROC CVPR IEEE, P1208, DOI 10.1109/CVPRW.2009.5206502
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Li Y, 2015, PROC CVPR IEEE, P353, DOI 10.1109/CVPR.2015.7298632
   Liu T, 2015, PROC CVPR IEEE, P4902, DOI 10.1109/CVPR.2015.7299124
   Lu Y, 2014, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2014.443
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Ma JY, 2014, IEEE T IMAGE PROCESS, V23, P1706, DOI 10.1109/TIP.2014.2307478
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Mei X, 2011, PROC CVPR IEEE, P1257
   Nam Hyeonseob., 2015, CoRR
   Nejhum S.M. Shahed., 2008, Proceedings IEEE Conference on Computer Vision and Pattern Recognition, P1
   Oron S, 2015, INT J COMPUT VISION, V111, P213, DOI 10.1007/s11263-014-0740-6
   Ruan WJ, 2017, IEEE INT CON MULTI, P1231, DOI 10.1109/ICME.2017.8019504
   Ruan Weijian, 2016, ICME, P1
   Shao ZF, 2018, IEEE T BIG DATA, V4, P105, DOI 10.1109/TBDATA.2017.2715815
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Wang Z, 2018, IEEE T CYBERNETICS, V48, P3006, DOI 10.1109/TCYB.2017.2755044
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yao R, 2013, PROC CVPR IEEE, P2363, DOI 10.1109/CVPR.2013.306
   Zhai DM, 2018, PATTERN RECOGN, V75, P250, DOI 10.1016/j.patcog.2017.06.018
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang SL, 2015, IEEE T MULTIMEDIA, V17, P265, DOI 10.1109/TMM.2015.2390044
   Zhang TZ, 2014, PROC CVPR IEEE, P1258, DOI 10.1109/CVPR.2014.164
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zhou Y, 2017, IEEE T MULTIMEDIA, V19, P1798, DOI 10.1109/TMM.2017.2689918
   Zhu GB, 2015, IEEE T IMAGE PROCESS, V24, P5140, DOI 10.1109/TIP.2015.2479460
   2010, IEEE T PATTERN ANAL, V32, P1627, DOI DOI 10.1109/TPAMI.2009.167
   2008, IEEE T PATTERN ANAL, V30, P1186, DOI DOI 10.1109/TPAMI.2007.70771
   2010, PROC CVPR IEEE, P1269
   2008, INT J COMPUT VISION, V77, P125, DOI DOI 10.1007/S11263-007-0075-7
NR 58
TC 49
Z9 50
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1122
EP 1134
DI 10.1109/TMM.2018.2872897
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600004
DA 2024-07-18
ER

PT J
AU Du, YL
   Fang, M
   Yi, JF
   Xu, C
   Cheng, J
   Tao, DC
AF Du, Yali
   Fang, Meng
   Yi, Jinfeng
   Xu, Chang
   Cheng, Jun
   Tao, Dacheng
TI Enhancing the Robustness of Neural Collaborative Filtering Systems Under
   Malicious Attacks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Recommendation systems; adversarial learning; collaborative filtering;
   malicious attacks
ID MATRIX FACTORIZATION; RECOMMENDATION
AB Recommendation systems have become ubiquitous in online shopping in recent decades due to their power in reducing excessive choices of customers and industries. Recent collaborative filtering methods based on the deep neural network are studied and introduce promising results due to their power in learning hidden representations for users and items. However, it has revealed its vulnerabilities under malicious user attacks. With the knowledge of a collaborative filtering algorithm and its parameters, the performance of this recommendation system can be easily downgraded. Unfortunately, this problem is not addressed well, and the study on defending recommendation systems is insufficient. In this paper, we aim to improve the robustness of recommendation systems based on two concepts-stage-wise hints training and randomness. To protect a target model, we introduce noise layers in the training of a target model to increase its resistance to adversarial perturbations. To reduce the noise layers' influence on model performance, we introduce intermediate layer outputs as hints from a teacher model to regularize the intermediate layers of a student target model. We consider white box attacks under which attackers have the knowledge of the target model. The generalizability and robustness properties of our method have been analytically inspected in experiments and discussions, and the computational cost is comparable to training a standard neural network-based collaborative filtering model. Through our investigation, the proposed defensive method can reduce the success rate of malicious user attacks and keep the prediction accuracy comparable to standard neural recommendation systems.
C1 [Du, Yali] Univ Technol Sydney, Fac Engn & Informat Technol, Ultimo, NSW 2007, Australia.
   [Du, Yali; Xu, Chang; Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, UBTECH Sydney Artificial Intelligence Ctr, Darlington, NSW 2008, Australia.
   [Du, Yali; Xu, Chang; Tao, Dacheng] Univ Sydney, Fac Engn & Informat Technol, Sch Informat Technol, Darlington, NSW 2008, Australia.
   [Du, Yali; Cheng, Jun] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Shenzhen 518055, Peoples R China.
   [Fang, Meng] Tencent AI Lab, Shenzhen 518057, Peoples R China.
   [Yi, Jinfeng] JD AI Res, Beijing 100020, Peoples R China.
   [Cheng, Jun] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 University of Technology Sydney; University of Sydney; University of
   Sydney; Chinese Academy of Sciences; Shenzhen Institute of Advanced
   Technology, CAS; Tencent; Chinese University of Hong Kong
RP Cheng, J (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab Virtual Real & Human Interact Te, Shenzhen 518055, Peoples R China.
EM yali.du@student.uts.edu.au; mfang@tencent.com; yijinfeng@jd.com;
   c.xu@sydney.edu.au; jun.cheng@siat.ac.cn; dacheng.tao@sydney.edu.au
RI Tao, Dacheng/A-5449-2012; Xu, Chang/AAG-9337-2019
OI Tao, Dacheng/0000-0001-7225-5449; Xu, Chang/0000-0002-4756-0609; Du,
   Yali/0000-0001-5683-2621
FU Australian Research Council [FL-170100117, DP-180103424, LP-150100671];
   Shenzhen Key Laboratory [ZDSYS201605101739178]; National Natural Science
   Foundation of China [61772508, U1713213]; Guangdong Technology
   [2016B010108010, 2016B010125003]; Shenzhen Technology Project
   [JSGG20160331185256983]; CAS Key Technology Talent Program, Shenzhen
   Engineering Laboratory for 3D Content Generating Technologies
   [[2017]476]; Key Laboratory of Human-Machine Intelligence-Synergy
   Systems, Shenzhen Institutes of Advanced Technology, Chinese Academy of
   Sciences [2014DP173025]
FX This work was supported in part by the Australian Research Council
   Projects under FL-170100117, DP-180103424, LP-150100671, in part by
   Shenzhen Key Laboratory under Grant ZDSYS201605101739178, in part by the
   National Natural Science Foundation of China under Grants 61772508 and
   U1713213, in part by Guangdong Technology under Grants 2016B010108010
   and 2016B010125003, in part by the Shenzhen Technology Project
   JSGG20160331185256983, in part by the CAS Key Technology Talent Program,
   Shenzhen Engineering Laboratory for 3D Content Generating Technologies
   under Grant [2017]476, and in part by the Key Laboratory of
   Human-Machine Intelligence-Synergy Systems, Shenzhen Institutes of
   Advanced Technology, Chinese Academy of Sciences under Grant
   2014DP173025. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Honggang Wang.
   (Yali Du and Meng Fang contributed equally to this work.) (Corresponding
   author: Jun Cheng.)
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2018, P 35 INT C MACH LEAR
   [Anonymous], P IEEE INT C BIG DAT
   [Anonymous], 2015, ADV NEURAL INFORM PR
   [Anonymous], 2013, ARXIV13060626
   [Anonymous], P FLOR ART INT RES S
   [Anonymous], J MACHINE LEARNING R
   [Anonymous], 2018, PROC INT C LEARN REP
   [Anonymous], P IEEE S SECUR PRIV
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bennett J., 2007, P KDD CUP WORKSH, P1
   Chen PY, 2018, AAAI CONF ARTIF INTE, P10
   Chen YD, 2013, IEEE T INFORM THEORY, V59, P4324, DOI 10.1109/TIT.2013.2249572
   Chen Yudong, 2011, Proceedings of the 28th International Conference on Machine Learning, P873
   Chollet F, 2015, KERAS
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Du Y, 2018, IEEE T AUTOM SCI ENG, V15, P1471, DOI 10.1109/TASE.2018.2858290
   Du YL, 2018, AISEC'18: PROCEEDINGS OF THE 11TH ACM WORKSHOP ON ARTIFICIAL INTELLIGENCE AND SECURITY, P13, DOI 10.1145/3270101.3270106
   Du YL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1617
   Du YL, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1610
   Fang M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1106, DOI 10.1145/2623330.2623672
   Gong C., 2018, IJCAI, P2156
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Goodfellow I.J., 2014, ARXIV 14126572
   Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He Xiangnan, 2015, P 24 ACM INT C INF K, P1661, DOI DOI 10.1145/2806416.2806504
   Hinton G., 2015, arXiv preprint arXiv:1503.02531
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lam S. K., 2004, P 13 INT C WORLD WID, P393, DOI DOI 10.1145/988672.988726
   Li B, 2016, ADV NEURAL INFORM PR, P1893
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Liu SF, 2017, PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON GREY SYSTEMS AND INTELLIGENT SERVICES (GSIS), P1, DOI [10.1109/PLASMA.2017.8496132, 10.1109/PESGM.2017.8273885, 10.1109/GSIS.2017.8077658]
   Liu Xuanqing, 2017, Towards robust neural networks via random self-ensemble
   McAuley Julian, 2013, RECSYS
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Mobasher B., 2005, P WEBKDD WORKSH CHIC
   Nie FP, 2012, IEEE DATA MINING, P566, DOI 10.1109/ICDM.2012.160
   O'Mahony M. P., 2002, Database and Expert Systems Applications. 13th International Conference, DEXA 2002. Proceedings (Lecture Notes in Computer Science Vol.2453), P494
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rennie Jasson D. M., 2005, P 22 INT C MACH LEAR, P713, DOI DOI 10.1145/1102351.1102441
   Romero A., 2014, 3 INT C LEARN REPRES
   Salakhutdinov R., 2007, P 24 INT C MACHINE L, P791
   Salakhutdinov Ruslan, 2008, P INT C MACH LEARN, P880, DOI [10.1145/1390156.1390267, DOI 10.1145/1390156.1390267]
   Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Szegedy C., 2014, ICLR, P1
   van den Berg R., 2018, P 24 ACM SIGKDD INT
   Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273
   Wang Y, 2017, IEEE T IMAGE PROCESS, V26, P1393, DOI 10.1109/TIP.2017.2655449
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Yang Q, 2015, IEEE COMMUN MAG, V53, P42, DOI 10.1109/MCOM.2015.7180506
   Yi Jinfeng, 2013, Int. Conf. on Mach. Learning, P1400
   Yu K, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P211, DOI 10.1145/1571941.1571979
   Zheng Y, 2016, INT C MACHINE LEARNI, P764
   Zhou YP, 2018, IEEE T MULTIMEDIA, V20, P2153, DOI 10.1109/TMM.2017.2781364
NR 59
TC 30
Z9 30
U1 8
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 555
EP 565
DI 10.1109/TMM.2018.2887018
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800003
OA Green Published
DA 2024-07-18
ER

PT J
AU Dong, JF
   Li, XR
   Snoek, CGM
AF Dong, Jianfeng
   Li, Xirong
   Snoek, Cees G. M.
TI Predicting Visual Features From Text for Image and Video Caption
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image and video; caption retrieval
AB This paper strives to find amidst a set of sentences the one best describing the content of a given image or video. Different from existing works, which rely on a joint subspace for their image and video caption retrieval, we propose to do so in a visual space exclusively. Apart from this conceptual novelty, we contribute Word2VisualVec, a deep neural network architecture that learns to predict a visual feature representation from textual input. Example captions are encoded into a textual embedding based on multiscale sentence vectorization and further transferred into a deep visual feature of choice via a simple multilayer perceptron. We further generalize Word2VisualVec for video caption retrieval, by predicting from text both three-dimensional convolutional neural network features as well as a visual-audio representation. Experiments on Flickr8k, Flickr30k, the Microsoft Video Description dataset, and the very recent NIST TrecVid challenge for video caption retrieval detail Word2VisualVec's properties, its benefit over textual embeddings, the potential for multimodal query composition, and its state-of-the-art results.
C1 [Dong, Jianfeng] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Li, Xirong] Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
   [Snoek, Cees G. M.] Univ Amsterdam, Informat Inst, NL-1098 XH Amsterdam, Netherlands.
C3 Zhejiang University; Renmin University of China; University of Amsterdam
RP Li, XR (corresponding author), Renmin Univ China, Key Lab Data Engn & Knowledge Engn, Sch Informat, Beijing 100872, Peoples R China.
EM danieljf24@zju.edu.cn; xirong@ruc.edu.cn; cgmsnoek@uva.nl
RI ARSLAN, Okan/AAA-3232-2020; Li, Xirong/AAD-3347-2019
OI Li, Xirong/0000-0002-0220-8310; Snoek, Cees/0000-0001-9092-1556
FU National Natural Science Foundation of China [61672523]; Fundamental
   Research Funds for the Central Universities; Research Funds of Renmin
   University of China [18XNLG19]; STW STORY project
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 61672523, in part by the Fundamental Research Funds
   for the Central Universities, in part by the Research Funds of Renmin
   University of China (18XNLG19), and in part by the STW STORY project.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Benoit Huet. (Corresponding
   author: Xirong Li.)
CR [Anonymous], COMPUT VIS PATTERN R
   Awad G., 2016, Trecvid 2016: Evaluating video search, video event detection, localization, and hyperlinking
   Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483
   Bai B., 2009, NIPS, P64
   Cappallo S, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1311, DOI 10.1145/2733373.2806335
   Chami I, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P352, DOI 10.1145/3078971.3078993
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cho K., 2014, ARXIV14061078
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Devlin J., ARXIV150504467
   Dong J., 2016, P ACM MULT C, P988
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gong YC, 2014, LECT NOTES COMPUT SC, V8692, P529, DOI 10.1007/978-3-319-10593-2_35
   Grangier D, 2008, IEEE T PATTERN ANAL, V30, P1371, DOI 10.1109/TPAMI.2007.70791
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Jain M, 2015, IEEE I CONF COMP VIS, P4588, DOI 10.1109/ICCV.2015.521
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang L, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P49, DOI 10.1145/2733373.2806237
   Jiang XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P69, DOI 10.1145/2733373.2806240
   Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kiros Ryan., 2015, Transactions of the Association for Computational Linguistics (TACL)
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuznetsova P., 2012, Long Papers, P359
   Le D.-D., 2016, P INT WORKSH VID RET
   Lev G, 2016, LECT NOTES COMPUT SC, V9910, P833, DOI 10.1007/978-3-319-46466-4_50
   Li XR, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P879, DOI 10.1145/2766462.2767773
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Mansimov E., 2016, P INT C LEARN REPR
   Mettes P, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P175, DOI 10.1145/2911996.2912036
   Mordvintsev Alexander, 2015, GOOGLE RES BLOG, V2015, P3
   Ordonez V., 2011, ADV NEURAL INFORM PR, P1143
   Ordonez V, 2016, INT J COMPUT VISION, V119, P46, DOI 10.1007/s11263-015-0840-y
   Otani Mayu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P651, DOI 10.1007/978-3-319-46604-0_46
   Pan YW, 2016, PROC CVPR IEEE, P4594, DOI 10.1109/CVPR.2016.497
   Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Reed S, 2016, PR MACH LEARN RES, V48
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vendrov I., 2016, ICLR, P1
   Venugopalan S., 2015, P N AM ASS COMP LING, P1724
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang C., 2016, P ACM MULT C
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xindi Shang, 2016, MultiMedia Modeling. 22nd International Conference, MMM 2016. Proceedings, P264, DOI 10.1007/978-3-319-27671-7_22
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu R, 2015, AAAI CONF ARTIF INTE, P2346
   Yagcioglu S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P106
   Yao T, 2015, IEEE I CONF COMP VIS, P28, DOI 10.1109/ICCV.2015.12
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   You QZ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1008, DOI 10.1145/2964284.2964288
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Yu HN, 2016, PROC CVPR IEEE, P4584, DOI 10.1109/CVPR.2016.496
   Yu Y, 2017, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2017.347
   Zhang H., 2016, P INT WORKSH VID RET
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
NR 72
TC 139
Z9 149
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3377
EP 3388
DI 10.1109/TMM.2018.2832602
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600016
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Shao, F
   Gao, Y
   Jiang, QP
   Jiang, GY
   Ho, YS
AF Shao, Feng
   Gao, Ying
   Jiang, Qiuping
   Jiang, Gangyi
   Ho, Yo-Sung
TI Multistage Pooling for Blind Quality Prediction of Asymmetric
   Multiply-Distorted Stereoscopic Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind quality prediction; multiply-distorted stereoscopic image; sparse
   representation; multi-stage pooling
ID BINOCULAR COMBINATION; COMPRESSION; FEATURES; SCORES
AB Quality prediction for asymmetric multiply-distorted stereoscopic images (MDSIs) confronts more challenges than previous stereoscopic image quality assessment (SIQA) issues, whereas the existing no-reference SIQA methods have been limited to understand the asymmetric distortions and multiple distortions simultaneously for general-purpose blind quality prediction. In this paper, we propose a multistage pooling (MUSP) model for quality prediction of asymmetric MDSIs. In the training stage, we establish multimodal sparse representation framework for phase and amplitude components, respectively. In the testing stage, we use an MUSP strategy to simulate the pooling procedure undergoing multimodal quality pooling, feature pooling, binocular pooling, and phase-amplitude quality pooling in order. Experimental results on our new established database (NBU-MDSID Phase-II) demonstrate the effectiveness of our blind metric.
C1 [Shao, Feng; Gao, Ying; Jiang, Qiuping; Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Ho, Yo-Sung] Gwangju Inst Sci & Technol, Sch Informat & Commun, Gwangju 500712, South Korea.
C3 Ningbo University; Gwangju Institute of Science & Technology (GIST)
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM shaofeng@nbu.edu.cn; ying1181245026@qq.com; jqp910707@126.com;
   jianggangyi@nbu.edu.cn; hoyo@gist.ac.kr
RI jiang, gang/KII-8233-2024; Jiang, Qiuping/AAL-8273-2020; Qiuping,
   Jiang/AAO-2830-2021
OI Qiuping, Jiang/0000-0002-6025-9343; HO, YO-SUNG/0000-0002-7220-1034
FU Natural Science Foundation of China [61622109]; Zhejiang Natural Science
   Foundation of China [R18F010008]; Natural Science Foundation of Ningbo
   [2017A610112]; K. C. Wong Magna Fund in Ningbo University
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61622109, in part by the Zhejiang Natural Science
   Foundation of China under Grant R18F010008, in part by the Natural
   Science Foundation of Ningbo under Grant 2017A610112, in part by the
   Natural Science Foundation of Ningbo under Grant 2017A610112, and in
   part by the K. C. Wong Magna Fund in Ningbo University. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Zixiang Xiong.
CR [Anonymous], 2013, International Scholarly Research Notices., DOI DOI 10.1155/2013/905685
   [Anonymous], 1999, P910 ITUT
   [Anonymous], P911 ITUT
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Chen ZB, 2018, IEEE T IMAGE PROCESS, V27, P721, DOI 10.1109/TIP.2017.2766780
   Ding J, 2006, P NATL ACAD SCI USA, V103, P1141, DOI 10.1073/pnas.0509629103
   Fang Y., 2017, P INT C QUAL MULT EX
   Fu J, 2016, INT CONF ACOUST SPEE, P1075, DOI 10.1109/ICASSP.2016.7471841
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Gu K, 2013, IEEE WORKSHOP SIG, P241, DOI 10.1109/SiPS.2013.6674512
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Karimi M, 2017, IEEE T MULTIMEDIA, V19, P2475, DOI 10.1109/TMM.2017.2699082
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Lei JJ, 2018, IEEE T CIRC SYST VID, V28, P3333, DOI 10.1109/TCSVT.2017.2749146
   LEVELT WJM, 1965, BRIT J PSYCHOL, V56, P1, DOI 10.1111/j.2044-8295.1965.tb00939.x
   Li CF, 2015, IEEE IMAGE PROC, P4883, DOI 10.1109/ICIP.2015.7351735
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YM, 2014, SIGNAL PROCESS-IMAGE, V29, P748, DOI 10.1016/j.image.2014.05.007
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Liu Y, 2016, SIGNAL PROCESS, V125, P237, DOI 10.1016/j.sigpro.2016.01.019
   Lu YA, 2015, IEEE SIGNAL PROC LET, V22, P1811, DOI 10.1109/LSP.2015.2436908
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Meegan DV, 2001, J EXP PSYCHOL-APPL, V7, P143, DOI 10.1037//1076-898X.7.2.143
   Min XK, 2017, IEEE T IMAGE PROCESS, V26, P5462, DOI 10.1109/TIP.2017.2735192
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Oh H, 2017, IEEE T IMAGE PROCESS, V26, P4923, DOI 10.1109/TIP.2017.2725584
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127
   Shao F, 2018, IEEE T CIRC SYST VID, V28, P573, DOI 10.1109/TCSVT.2016.2628082
   Shao F, 2017, IEEE T IMAGE PROCESS, V26, P4790, DOI 10.1109/TIP.2017.2721546
   Shao F, 2017, IEEE T MULTIMEDIA, V19, P1821, DOI 10.1109/TMM.2017.2685240
   Shao F, 2016, IEEE T COMPUT IMAG, V2, P123, DOI 10.1109/TCI.2016.2538720
   Shao F, 2016, IEEE T IMAGE PROCESS, V25, P2059, DOI 10.1109/TIP.2016.2538462
   Shao F, 2016, IEEE T CYBERNETICS, V46, P730, DOI 10.1109/TCYB.2015.2414479
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Song R, 2015, J INF SCI ENG, V31, P1593
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Sun W, 2017, PATTERN RECOGN, V61, P153, DOI 10.1016/j.patcog.2016.07.033
   Wang JH, 2017, IEEE T IMAGE PROCESS, V26, P1330, DOI 10.1109/TIP.2017.2651387
   Wang JH, 2017, IEEE T IMAGE PROCESS, V26, P1202, DOI 10.1109/TIP.2016.2642791
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang J, 2017, ACSR ADV COMPUT, V74, P70
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540
   Zhang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2379, DOI 10.1109/TIP.2012.2183879
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P3810, DOI 10.1109/TIP.2015.2456414
NR 68
TC 9
Z9 10
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2605
EP 2619
DI 10.1109/TMM.2018.2817072
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000006
DA 2024-07-18
ER

PT J
AU Chen, ZX
   Xu, Z
   Zhang, Y
   Gu, X
AF Chen, Zhuoxiang
   Xu, Zhe
   Zhang, Ya
   Gu, Xiao
TI Query-Free Clothing Retrieval via Implicit Relevance Feedback
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mental image retrieval; attribute learning
ID IMAGE RETRIEVAL; SYSTEM; PHOTO; COLOR; MODEL
AB Image-based clothing retrieval is receiving increasing interest with the growth of online shopping. In practice, users may often have a desired piece of clothing in mind (e.g., either having seen it before on the street or requiring certain specific clothing attributes), but may be unable to supply an image as a query. We model this problem as a new type of image retrieval task, in which the target image resides only in the user's mind (called "mental image retrieval" hereafter). Because of the absence of an explicit query image, we propose solving this problem through relevance feedback. Specifically, a new Bayesian formulation is proposed that simultaneously models the retrieval target and its high-level representation in the mind of the user (called the "user metric" hereafter) as posterior distributions of prefetched shop images and heterogeneous features extracted from multiple clothing attributes, respectively. Requiring only clicks as user feedback, the proposed algorithm is able to account for the variability in human decision-making. Experiments with real users demonstrate the effectiveness of the proposed algorithm.
C1 [Chen, Zhuoxiang; Xu, Zhe; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200000, Peoples R China.
   [Gu, Xiao] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200000, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200000, Peoples R China.
EM chenzhuoxiang@sjtu.edu.cn; xz3030@sjtu.edu.cn; ya_zhang@sjtu.edu.cn;
   gugu97@sjtu.edu.cn
RI Zhang, Ya/Y-8255-2019
OI Zhang, Ya/0000-0002-5390-9053; Xu, Zhe/0000-0002-3902-2595
FU High Technology Research and Development Program of China
   [2015AA015801]; National Nature Science Foundation of China [61521062];
   Science and Technology Commission of Shanghai Municipality [12DZ2272600]
FX This work was supported in part by the High Technology Research and
   Development Program of China under Grant 2015AA015801, in part by
   National Nature Science Foundation of China under Grant 61521062, and in
   part by Science and Technology Commission of Shanghai Municipality
   (12DZ2272600). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Zhu Li.
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   [Anonymous], P 11 AS C COMP VIS
   [Anonymous], 2007, International Conference on Computer Vision (ICCV)
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], 2013, P 3 ACM C INT C MULT
   Auer P, 2010, JMLR WORKSH CONF PRO, V11, P51
   Auer P, 2011, STUD COMPUT INTELL, V346, P59
   Bian W, 2010, IEEE T IMAGE PROCESS, V19, P545, DOI 10.1109/TIP.2009.2035223
   Biswas A, 2013, PROC CVPR IEEE, P644, DOI 10.1109/CVPR.2013.89
   Broilo M, 2010, IEEE T MULTIMEDIA, V12, P267, DOI 10.1109/TMM.2010.2046269
   Chen BC, 2013, IEEE T MULTIMEDIA, V15, P1163, DOI 10.1109/TMM.2013.2242460
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen Q, 2015, PROC CVPR IEEE, P5315, DOI 10.1109/CVPR.2015.7299169
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Cox IJ, 2000, IEEE T IMAGE PROCESS, V9, P20, DOI 10.1109/83.817596
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Escorcia V, 2015, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2015.7298730
   Fang YC, 2005, LECT NOTES COMPUT SC, V3546, P637
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Hong RC, 2015, IEEE T MULTIMEDIA, V17, P1980, DOI 10.1109/TMM.2015.2476657
   Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127
   Huang SL, 2016, PROC CVPR IEEE, P1173, DOI 10.1109/CVPR.2016.132
   Jagadeesh V, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1925, DOI 10.1145/2623330.2623332
   Kherfi ML, 2006, IEEE T IMAGE PROCESS, V15, P1017, DOI 10.1109/TIP.2005.863969
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kiapour MH, 2014, LECT NOTES COMPUT SC, V8689, P472, DOI 10.1007/978-3-319-10590-1_31
   Kovashka A, 2012, PROC CVPR IEEE, P2973, DOI 10.1109/CVPR.2012.6248026
   Kurita T., 1993, Proceedings of the Second International Conference on Document Analysis and Recognition (Cat. No.93TH0578-5), P547, DOI 10.1109/ICDAR.1993.395676
   Kwak IS, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.14
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liu J, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P297, DOI 10.1109/ROBIO.2014.7090346
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Lu Y, 2003, IEEE T MULTIMEDIA, V5, P339, DOI 10.1109/TMM.2003.813280
   Murala S, 2012, IEEE T IMAGE PROCESS, V21, P2874, DOI 10.1109/TIP.2012.2188809
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rocchio J., 1966, Document retrieval systems optimization and evaluation
   Rui Y, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P815, DOI 10.1109/ICIP.1997.638621
   Shankar S, 2015, PROC CVPR IEEE, P3403, DOI 10.1109/CVPR.2015.7298962
   Shao M, 2013, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2013.451
   Simo-Serra E, 2015, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2015.7298688
   Song Z, 2011, IEEE I CONF COMP VIS, P1084, DOI 10.1109/ICCV.2011.6126355
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Su JH, 2011, IEEE T KNOWL DATA EN, V23, P360, DOI 10.1109/TKDE.2010.124
   Suditu N, 2012, P 21 ACM INT C INF K, P1323, DOI DOI 10.1145/2396761.2398435
   Suditu N, 2011, IEEE I CONF COMP VIS, P2118, DOI 10.1109/ICCV.2011.6126487
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Sun Y, 2010, IEEE IMAGE PROC, P3209, DOI 10.1109/ICIP.2010.5651984
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tao DC, 2006, IEEE T MULTIMEDIA, V8, P716, DOI 10.1109/TMM.2005.861375
   Wang XW, 2013, IEEE T MULTIMEDIA, V15, P2035, DOI 10.1109/TMM.2013.2279658
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Zhang N, 2014, PROC CVPR IEEE, P1637, DOI 10.1109/CVPR.2014.212
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
   Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3
NR 58
TC 13
Z9 14
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2126
EP 2137
DI 10.1109/TMM.2017.2785253
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tejero-de-Pablos, A
   Nakashima, Y
   Sato, T
   Yokoya, N
   Linna, M
   Rahtu, E
AF Tejero-de-Pablos, Antonio
   Nakashima, Yuta
   Sato, Tomokazu
   Yokoya, Naokazu
   Linna, Marko
   Rahtu, Esa
TI Summarization of User-Generated Sports Video by Using Deep Action
   Recognition Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sports video summarization; user-generated video; action recognition;
   deep learning; 3D convolutional neural networks; long short-term memory
ID HIGHLIGHTS
AB Automatically generating a summary of a sports video poses the challenge of detecting interesting moments, or highlights, of a game. Traditional sports video summarization methods leverage editing conventions of broadcast sports video that facilitate the extraction of high-level semantics. However, user-generated videos are not edited and, thus, traditional methods are not suitable to generate a summary. In order to solve this problem, this paper proposes a novel video summarization method that uses players' actions as a cue to determine the highlights of the original video. A deep neural-network-based approach is used to extract two types of action-related features and to classify video segments into interesting or uninteresting parts. The proposed method can be applied to any sports in which games consist of a succession of actions. Especially, this paper considers the case of Kendo (Japanese fencing) as an example of a sport to evaluate the proposed method. The method is trained using Kendo videos with ground truth labels that indicate the video highlights. The labels are provided by annotators possessing a different experience with respect to Kendo to demonstrate how the proposed method adapts to different needs. The performance of the proposed method is compared with several combinations of different features, and the results show that it outperforms previous summarization methods.
C1 [Tejero-de-Pablos, Antonio; Nakashima, Yuta; Sato, Tomokazu; Yokoya, Naokazu] Nara Inst Sci & Technol, Grad Sch Informat Sci, Nara 6300101, Japan.
   [Tejero-de-Pablos, Antonio] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138654, Japan.
   [Nakashima, Yuta] Osaka Univ, Inst Databil Sci, Suita, Osaka 5650871, Japan.
   [Sato, Tomokazu] Shiga Univ, Fac Data Sci, Shiga 5228522, Japan.
   [Linna, Marko] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
   [Rahtu, Esa] Tampere Univ Technol, Dept Signal Proc, FIN-33101 Tampere, Finland.
C3 Nara Institute of Science & Technology; University of Tokyo; Osaka
   University; Shiga University; University of Oulu; Tampere University
RP Tejero-de-Pablos, A (corresponding author), Nara Inst Sci & Technol, Grad Sch Informat Sci, Nara 6300101, Japan.; Tejero-de-Pablos, A (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138654, Japan.
EM antonio-t@mi.t.u-tokyo.ac.jp; n-yuta@ids.osaka-u.ac.jp;
   t-sato@biwako.shiga-u.ac.jp; yokoya@is.naist.jp; marko.linna@gmail.com;
   esa.rahtu@tut.fi
RI Nakashima, Yuta/Y-6218-2019
OI Nakashima, Yuta/0000-0001-8000-3567; Rahtu, Esa/0000-0001-8767-0864
FU Japan Society for the Promotion of Science KAKENHI [16K16086];
   Grants-in-Aid for Scientific Research [16K16086] Funding Source: KAKEN
FX This work was supported in part by the Japan Society for the Promotion
   of Science KAKENHI under Grant 16K16086. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei.
CR [Anonymous], ARXIVCORRABS16090742
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2014, ADV COMPUTER SCI INT
   [Anonymous], IEEE INT CON MULTI
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2011, P 5 IEEE INT C AUTOM
   [Anonymous], P WORKSH MACH LEARN
   [Anonymous], 2009, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.23.124
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2016, VIDEO SUMMARIZATION
   [Anonymous], P INT WORKSH MULT IN
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Calderara Simone, 2008, 2008 IEEE Fifth International Conference on Advanced Video and Signal Based Surveillance, P121, DOI 10.1109/AVSS.2008.32
   Chen CY, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/460913
   Chen DY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P667
   Chen M, 2006, IEEE SIGNAL PROC MAG, V23, P38, DOI 10.1109/MSP.2006.1621447
   Choi J., 2008, ACM ICMR, P291
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   del Molino AG, 2017, AAAI CONF ARTIF INTE, P4046
   Divakaran A, 2003, INT SER VIDEO COMPUT, V6, P91
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Hua XS, 2004, IEEE T CIRC SYST VID, V14, P572, DOI 10.1109/TCSVT.2004.826750
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2015, IEEE T MULTIMEDIA, V17, P1174, DOI 10.1109/TMM.2015.2436813
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   King DB, 2015, ACS SYM SER, V1214, P1
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Li BX, 2001, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P132, DOI 10.1109/IVL.2001.990867
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Liang CH, 2004, 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL I, CONFERENCE PROCEEDINGS, P297
   Lienhart R, 2000, PROC SPIE, V3972, P378
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Mendi E, 2013, COMPUT ELECTR ENG, V39, P790, DOI 10.1016/j.compeleceng.2012.11.020
   Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118
   Mills M., 1992, CHI '92 Conference Proceedings. ACM Conference on Human Factors in Computing Systems. Striking a Balance, P93, DOI 10.1145/142750.142764
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Nitta N, 2009, MULTIMED TOOLS APPL, V41, P1, DOI 10.1007/s11042-008-0217-0
   Otani M, 2017, MULTIMED TOOLS APPL, V76, P12097, DOI 10.1007/s11042-016-4061-3
   Pan H, 2001, INT CONF ACOUST SPEE, P1649, DOI 10.1109/ICASSP.2001.941253
   Peng WT, 2011, IEEE T MULTIMEDIA, V13, P539, DOI 10.1109/TMM.2011.2131638
   Rodriguez M. D., 2008, PROC IEEE C COMPUT V, P1
   Simonyan K., 2014, 14091556 ARXIV
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sun XH, 2009, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2009.5204255
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tejero-de-Pablos A, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0120-y
   Tjondronegoro D, 2004, IEEE MULTIMEDIA, V11, P22, DOI 10.1109/MMUL.2004.28
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Xia L., 2012, CVPR 2012 HAU3D Workshop, P20
   Xu G, 2005, IEEE T CIRC SYST VID, V15, P1422, DOI 10.1109/TCSVT.2005.856903
   Yang H, 2015, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2015.526
   Zeng KH, 2016, LECT NOTES COMPUT SC, V9906, P609, DOI 10.1007/978-3-319-46475-6_38
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhang ZY, 2012, IEEE MULTIMEDIA, V19, P4, DOI 10.1109/MMUL.2012.24
NR 60
TC 56
Z9 59
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2018
VL 20
IS 8
BP 2000
EP 2011
DI 10.1109/TMM.2018.2794265
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GN8BR
UT WOS:000439378600007
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Song, YF
   Li, J
   Wang, XG
   Chen, XW
AF Song, Yafei
   Li, Jia
   Wang, Xiaogang
   Chen, Xiaowu
TI Single Image Dehazing Using Ranking Convolutional Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image dehazing; haze-relevant features; convolutional neural
   network; ranking layer
ID FRAMEWORK; ALGORITHM; FEATURES
AB Single image dehazing, which aims to recover the clear image solely from an input hazy or foggy image, is a challenging ill-posed problem. Analyzing existing approaches, the common key step is to estimate the haze density of each pixel. To this end, various approaches often heuristically designed haze-relevant features. Several recent works also automatically learn the features via directly exploiting convolutional neural networks (CNN). However, it may be insufficient to fully capture the intrinsic attributes of hazy images. To obtain effective features for single image dehazing, this paper presents a novel ranking convolutional neural network (Ranking-CNN). In Ranking-CNN, a novel ranking layer is proposed to extend the structure of CNN so that the statistical and structural attributes of hazy images can be simultaneously captured. By training Ranking-CNN in a well-designed manner, powerful haze-relevant features can be automatically learned from massive hazy image patches. Based on these features, haze can be effectively removed by using a haze density prediction model trained through the random forest regression. Experimental results show that our approach outperforms several previous dehazing approaches on synthetic and real-world benchmark images. Comprehensive analyses are also conducted to interpret the proposed Ranking-CNN from both the theoretical and experimental aspects.
C1 [Song, Yafei; Li, Jia; Wang, Xiaogang; Chen, Xiaowu] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Li, Jia] Beihang Univ, Int Res Inst Multidisciplinary Sci, Beijing 100191, Peoples R China.
C3 Beihang University; Beihang University
RP Chen, XW (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
EM songyf@buaa.edu.cn; jiali@buaa.edu.cn; wangxiaogang@buaa.edu.cn;
   chen@buaa.edu.cn
RI Wang, Xiaogang/B-2439-2013; Li, Jia/AAB-6431-2019; yang,
   qing/JBR-8440-2023; zhang, weijie/JQX-1450-2023
OI Wang, Xiaogang/0000-0002-7929-5889; Li, Jia/0000-0002-4346-8696; Song,
   Yafei/0000-0003-3537-1015
FU National Natural Science Foundation of China [61532003, 61672072,
   61325011, 61421003]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61532003, 61672072, 61325011, and 61421003.
CR Agostinelli F., 2013, ADV NEURAL INFORM PR, V26, P1493
   [Anonymous], PROC CVPR IEEE
   [Anonymous], P SIGGRAPH AS TECH B
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2008, P IEEE COMPUTER VISI
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2014, IEEE International Conference on Computational Photography (ICCP)
   [Anonymous], IEEE T PATT IN PRESS
   [Anonymous], P 22 ACM INT C MULT
   [Anonymous], 1957, Vision through the atmosphere
   Berman D, 2016, PROC CVPR IEEE, P1674, DOI 10.1109/CVPR.2016.185
   Breiman L., 2001, Mach. Learn., V45, P5
   Cai BL, 2016, IEEE T IMAGE PROCESS, V25, P5187, DOI 10.1109/TIP.2016.2598681
   Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39
   Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Fei Wu, 2015, IEEE Transactions on Big Data, V1, P109, DOI 10.1109/TBDATA.2015.2497270
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gibson KB, 2013, IEEE IMAGE PROC, P714, DOI 10.1109/ICIP.2013.6738147
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   He KM, 2009, PROC CVPR IEEE, P1956, DOI [10.1109/CVPRW.2009.5206515, 10.1109/CVPR.2009.5206515]
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177
   McCartney E.J., 1976, OPTICS ATMOSPHERE
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Oakley JP, 2007, IEEE T IMAGE PROCESS, V16, P511, DOI 10.1109/TIP.2006.887736
   Ren WQ, 2016, LECT NOTES COMPUT SC, V9906, P154, DOI 10.1007/978-3-319-46475-6_10
   Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   Scharstein D., 2007, PROC IEEE C COMPUT V, P1
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142
   Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tan R. T., 2008, P IEEE C COMP VIS PA, P1
   Tang KT, 2014, PROC CVPR IEEE, P2995, DOI 10.1109/CVPR.2014.383
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Timofeev Y.M., 2008, THEORETICAL FUNDAMEN
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Wang YK, 2014, IEEE T IMAGE PROCESS, V23, P4826, DOI 10.1109/TIP.2014.2358076
   Wen HC, 2013, IEEE INT SYMP CIRC S, P753, DOI 10.1109/ISCAS.2013.6571956
   Xie J., 2012, ADV NEURAL INFORM PR, P341
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 57
TC 97
Z9 104
U1 2
U2 56
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1548
EP 1560
DI 10.1109/TMM.2017.2771472
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Rawat, YS
   Song, ML
   Kankanhalli, MS
AF Rawat, Yogesh Singh
   Song, Mingli
   Kankanhalli, Mohan S.
TI A Spring-Electric Graph Model for Socialized Group Photography
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Art; layout; machine learning; photography; real-time systems; social
   computing; visual balance
ID RECOMMENDATION; SYSTEM
AB Visual balance is considered as one of the important factors in defining the aesthetic quality of visual arts. In this paper, we propose a novel method to obtain visual balance in a layout with dynamic visual elements. We use the idea of a spring-electric graph model and augment it with the concept of color energy from the literature of visual arts. We also present an interesting application of the proposed model in photography assistance. We focus on group photography and utilize social media images along with the proposed spring-electric model for providing a recommendation to the user. The proposed method can provide real-time feedback to the user regarding the arrangement of people, their position, and relative size on the image frame. We conducted qualitative experiments along with user studies to evaluate the proposed method. Experimental results and user studies show the effectiveness of the proposed model in obtaining visual balance and group photography recommendation.
C1 [Rawat, Yogesh Singh; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Song, Mingli] Zhejiang Univ, Coll Comp Sci, Alibaba Zhejiang Univ Joint Res Inst Frontier Tec, Hangzhou 310027, Zhejiang, Peoples R China.
C3 National University of Singapore; Zhejiang University
RP Rawat, YS (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM yogesh@u.nus.edu; brooksong@zju.edu.cn; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019
OI Kankanhalli, Mohan/0000-0002-4846-2015; Rawat,
   Yogesh/0000-0003-4052-6798
FU National Natural Science Foundation of China [61572428, U1509206];
   National Key Research and Development Program [2016YFB1200203]; National
   Research Foundation, Prime Ministers Office, Singapore, under its
   International Research Centre in Singapore Funding Initiative
FX This work was supported in part by the National Research Foundation,
   Prime Ministers Office, Singapore, under its International Research
   Centre in Singapore Funding Initiative and National Natural Science
   Foundation of China (61572428, U1509206), and in part by National Key
   Research and Development Program (2016YFB1200203).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2010, IEEE IMAGE PROC, P2653, DOI 10.1109/ICIP.2010.5652636
   [Anonymous], 1991, Color Image Scale
   [Anonymous], 2004, P ACM C INT US INT
   [Anonymous], SMART GRAPHICS
   [Anonymous], P ACM SIGGRAPH AS
   [Anonymous], 2010, ACM MULTIMEDIA 2010
   [Anonymous], 1927, STUDIES OPTICS
   [Anonymous], COMPOSING YOUR PAINT
   Arnheim R., 2009, The power of the center: A study of composition in the visual arts (20th anniversary edition)
   Arnheim Rudolf., 2004, Art and Visual Perception: A Psychology of the Creative Eye
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   DIBATTISTA G, 1994, COMP GEOM-THEOR APPL, V4, P235, DOI 10.1016/0925-7721(94)00014-X
   Eades Peter., 1984, Congressus numerantium, P146
   FRUCHTERMAN TMJ, 1991, SOFTWARE PRACT EXPER, V21, P1129, DOI 10.1002/spe.4380211102
   Jahanian Ali, 2013, P 2013 INT C INTELLI, P95
   Kalayeh MM, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P923, DOI 10.1145/2733373.2806365
   KAMADA T, 1989, INFORM PROCESS LETT, V31, P7, DOI 10.1016/0020-0190(89)90102-6
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Ma S, 2014, IEEE IMAGE PROC, P556, DOI 10.1109/ICIP.2014.7025111
   Ma S, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1053, DOI 10.1145/2647868.2655053
   Ni BB, 2013, IEEE T MULTIMEDIA, V15, P1138, DOI 10.1109/TMM.2013.2241042
   Obrador P, 2010, IEEE IMAGE PROC, P3185, DOI 10.1109/ICIP.2010.5654231
   Purchase H. C., 2011, P 12 AUSTR US INT C, V117, P19
   Rawat YS, 2017, IEEE T CIRC SYST VID, V27, P149, DOI 10.1109/TCSVT.2016.2555658
   Rawat YS, 2015, ACM T MULTIM COMPUT, V12, DOI 10.1145/2808199
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   Simond Florian, 2015, 2015 IEEE International Conference on Image Processing (ICIP). Proceedings, P3788, DOI 10.1109/ICIP.2015.7351513
   Su HH, 2012, IEEE T MULTIMEDIA, V14, P833, DOI 10.1109/TMM.2012.2186123
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang YT, 2015, ACM T INTEL SYST TEC, V7, DOI 10.1145/2770879
   Xu PF, 2014, MULTIMED TOOLS APPL, V69, P3, DOI 10.1007/s11042-012-1343-2
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Yan JZ, 2013, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2013.130
   Yin WY, 2014, IEEE T MULTIMEDIA, V16, P184, DOI 10.1109/TMM.2013.2283468
   Zettl H., 2010, Sight, sound, motion: Applied media aesthetics
   Zhang X., 2008, PROC IEEE C COMPUT V, P1
   Zhang YH, 2012, IEEE IMAGE PROC, P2753
NR 41
TC 9
Z9 11
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 754
EP 766
DI 10.1109/TMM.2017.2750420
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500019
DA 2024-07-18
ER

PT J
AU Zhai, DM
   Liu, XM
   Ji, XY
   Zhao, DB
   Satoh, S
   Gao, W
AF Zhai, Deming
   Liu, Xianming
   Ji, Xiangyang
   Zhao, Debin
   Satoh, Shin'ichi
   Gao, Wen
TI Supervised Distributed Hashing for Large-Scale Multimedia Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hash function learning; large-scale distributed data; multimedia
   retrieval; supervised distributed hashing
ID COMPLEXITY; SCENE
AB Recent years have witnessed the growing popularity of hashing for large-scale multimedia retrieval. Extensive hashing methods have been designed for data stored in a single machine, that is, centralized hashing. In many real-world applications, however, the large-scale data are often distributed across different locations, servers, or sites. Although hashing for distributed data can be implemented by assembling all distributed data together as a whole dataset in theory, it usually leads to prohibitive computation, communication, and storage casts in practice. Up to now, only a few methods were tailored for distributed hashing, which are all unsupervised approaches. In this paper, we propose an efficient and effective method called supervised distributed hashing (SupDisH), which learns discriminative hash functions by leveraging the semantic label information in a distributed manner. Specifically, we cast the distributed hashing problem into the framework of classification, where the learned binary codes are expected to be distinct enough for semantic retrieval. By introducing auxiliary variables, the distributed model is then separated into a set of decentralized subproblems with consistency constraints, which can he solved in parallel on each vertex of the distributed network. As such, we can obtain high-quality distinctive unbiased binary' codes and consistent hash functions with low computational complexity, which facilitate tackling large-scale multimedia retrieval tasks involving distributed datasets. Experimental evaluations on three large-scale datasets show that SupDisH is competitive to centralized hashing methods and outperforms the state-of-the-art unsupervised distributed method significantly.
C1 [Zhai, Deming; Liu, Xianming; Zhao, Debin] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Ji, Xiangyang] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Satoh, Shin'ichi] Natl Inst Informat, Tokyo 1018430, Japan.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Key Lab Machine Percept MoE, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Tsinghua University; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan; Peking University; Peking University
RP Liu, XM (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM zhaideming@hit.edu.cn; csxm@hit.edu.cn; xyji@tsinghua.edu.cn;
   dbzhao@hit.edu.cn; satoh@nii.ac.jp; wgao@pku.edu.cn
RI Zhao, Debin/JEP-0204-2023
FU Major State Basis Research Development Program of China (973 Program)
   [2015CB351804]; Natural Science Foundation of China [61502122, 61672193,
   61671188, 61571164]; Fundamental Research Funds for the Central
   Universities [HIT.NSRIF.201653]
FX This work was supported in part by the Major State Basis Research
   Development Program of China (973 Program) under Grant 2015CB351804; in
   part by the Natural Science Foundation of China under Grants 61502122,
   61672193, 61671188, and 61571164; and in part by the Fundamental
   Research Funds for the Central Universities under Grant
   HIT.NSRIF.201653.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2009, NIPS
   [Anonymous], PATTERN REC IN PRESS
   Bahmani Bahman., 2012, P 21 ACM INT C INFOR, P2174, DOI DOI 10.1145/2396761.2398596
   Bertsekas D. P., 1999, NONLINEAR PROGRAMMIN
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734
   CASTELLI V, 1995, PATTERN RECOGN LETT, V16, P105, DOI 10.1016/0167-8655(94)00074-D
   Chen Q, 2016, INT SYM COMPUT INTEL, P118, DOI [10.1109/ISCID.2016.141, 10.1109/ISCID.2016.2036]
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   COPPERSMITH D, 1982, SIAM J COMPUT, V11, P472, DOI 10.1137/0211038
   Corbett JC, 2013, ACM T COMPUT SYST, V31, DOI 10.1145/2491245
   Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Greenhill S., 2007, P 15 ACM INT C MULT, P413
   Hanke M., 1993, Surveys on Mathematics for Industry, V3, P253
   He J., 2010, ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P1129
   Hong MY, 2015, INT CONF ACOUST SPEE, P3836, DOI 10.1109/ICASSP.2015.7178689
   Howard A, 2002, DISTRIBUTED AUTONOMOUS ROBOTIC SYSTEMS 5, P299
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Leng C, 2015, PR MACH LEARN RES, V37, P1642
   Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XM, 2017, IEEE T IMAGE PROCESS, V26, P509, DOI 10.1109/TIP.2016.2627807
   Loosli G., 2007, LARGE SCALE KERNEL M, P301
   Lv Q., 2007, P 33 INT C VER LARG, P950
   Min KR, 2010, PROC CVPR IEEE, P3477, DOI 10.1109/CVPR.2010.5539973
   Norouzi M.E., 2011, ICML
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003
   Parikh N, 2014, MATH PROGRAM COMPUT, V6, P77, DOI 10.1007/s12532-013-0061-8
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Song JK, 2014, IEEE T CYBERNETICS, V44, P1225, DOI 10.1109/TCYB.2013.2289351
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Song Jingkuan, ARXIV170107901
   Tang JH, 2015, IEEE T IMAGE PROCESS, V24, P2827, DOI 10.1109/TIP.2015.2421443
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang J., IEEE T PATTERN ANAL, P1
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang Jun., 2010, ICML, P1127
   Weiss Y., 2008, NIPS, V21, P1753
   Yun S, 2014, SIAM J OPTIMIZ, V24, P1567, DOI 10.1137/130937755
   Zhai D., 2017, PATTERN REC IN PRESS
   Zhai Deming., 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, P2754
   Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P3025, DOI 10.1109/TIP.2014.2326010
   Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165
NR 53
TC 32
Z9 32
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 675
EP 686
DI 10.1109/TMM.2017.2749160
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500013
DA 2024-07-18
ER

PT J
AU Yao, JC
   Wang, YF
   Zhang, Y
   Sun, J
   Zhou, J
AF Yao, Jiangchao
   Wang, Yanfeng
   Zhang, Ya
   Sun, Jun
   Zhou, Jun
TI Joint Latent Dirichlet Allocation for Social Tags
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Joint latent dirichlet allocation; non-IID learning; social tags;
   structure mining; topic models
ID IMAGE; RETRIEVAL; SEGMENTATION; RELEVANCE; FEATURES; MODEL
AB Social tags, serving as a textual source of simple but useful semantic metadata to reflect the user preference or describe the web objects, has been widely used in many applications. However, social tags have several unique characteristics, i.e., sparseness and data coupling (i.e., non-IIDness), which makes existing text analysis methods such as LDA not directly applicable. In this paper, we propose a new generative algorithm for social tag analysis named joint latent Dirichlet allocation, which models the generation of tags based on both the users and the objects, and thus accounts for the coupling relationships among social tags. The model introduces two latent factors that jointly influence tag generation: the user's latent interest factor and the object's latent topic factor, formulated as user-topic distribution matrix and object-topic distribution matrix, respectively. A Gibbs sampling approach is adopted to simultaneously infer the above two matrices as well as a topic-word distribution matrix. Experimental results on four social tagging datasets have shown that our model is able to capture more reasonable topics and achieves better performance than five state-of-the-art topic models in terms of the widely used point-wise mutual information metric. In addition, we analyze the learnt topics showing that our model recovers more themes from social tags while LDA may lead the topic vanishing problems, and demonstrate its advantages in the social recommendation by evaluating the retrieval results with mean reciprocal rank metric. Finally, we explore the joint procedure of our model in depth to show the non-IID characteristic of social tagging process.
C1 [Yao, Jiangchao; Wang, Yanfeng; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Sun, Jun; Zhou, Jun] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Wang, YF; Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM sunarker@sjtu.edu.cn; wangyanfeng@sjtu.edu.cn; ya_zhang@sjtu.edu.cn;
   junsun@sjtu.edu.cn; zhoujun@sjtu.edu.cn
RI Zhang, Ya/Y-8255-2019; Zhou, Jun/W-2233-2019; Yao,
   Jiangchao/JOZ-1621-2023; Wang, Yan-Feng/ABB-8063-2020
OI Zhang, Ya/0000-0002-5390-9053; Zhou, Jun/0000-0001-5822-8233; 
FU High Technology Research and Development Program of China
   [2015AA015801]; National Natural Science Foundation of China [61221001];
   Science and Technology Commission of Shanghai Municipality [12DZ2272600]
FX This work was supported in part by the High Technology Research and
   Development Program of China under Grant 2015AA015801, in part by the
   National Natural Science Foundation of China under Grant 61221001, and
   in part by the Science and Technology Commission of Shanghai
   Municipality under Grant 12DZ2272600. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Zhen Wen.
CR Abel F. T., 2013, USER MODEL USER-ADAP, P1
   Aliakbary Sadegh, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P588, DOI 10.1109/CSE.2009.411
   [Anonymous], CORR
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], CORR
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   Blei D., 2006, ADV NEURAL INFORM PR
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Chen L, 2012, IEEE T MULTIMEDIA, V14, P1057, DOI 10.1109/TMM.2012.2187435
   Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795
   DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
   Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128
   Gan Z, 2015, PR MACH LEARN RES, V37, P1823
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   Hinton GE, 2009, INT C NEURAL INF PRO, P1607
   Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649
   Hu Yuheng., 2012, AAAI, V12, P59
   Hwang SJ, 2012, IEEE T PATTERN ANAL, V34, P1145, DOI 10.1109/TPAMI.2011.190
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lan T, 2013, PROC CVPR IEEE, P3103, DOI 10.1109/CVPR.2013.399
   Larochelle H., 2012, NIPS
   Levy M, 2009, IEEE T MULTIMEDIA, V11, P383, DOI 10.1109/TMM.2009.2012913
   Li LJ, 2009, PROC CVPR IEEE, P2036, DOI 10.1109/CVPRW.2009.5206718
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liao S, 2015, IEEE T MULTIMEDIA, V17, P1058, DOI 10.1109/TMM.2015.2436057
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Mao YQ, 2011, LECT NOTES ARTIF INT, V7120, P109
   Mimno D, 2011, EMNLP, P262, DOI DOI 10.5555/2145432.2145462
   Min WQ, 2015, IEEE T MULTIMEDIA, V17, P1787, DOI 10.1109/TMM.2015.2463226
   Nallapati R. M., 2008, P 14 ACM SIGKDD INT, DOI DOI 10.1145/1401890.1401957
   Negoescu RA, 2010, IEEE T MULTIMEDIA, V12, P399, DOI 10.1109/TMM.2010.2050649
   Newman D, 2011, Advances in Neural Information Processing Systems, P496, DOI DOI 10.5555/2986459.2986515
   Newman D., 2010, AUTOMATIC EVALUATION
   Paisley J, 2015, IEEE T PATTERN ANAL, V37, P256, DOI 10.1109/TPAMI.2014.2318728
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Rosen-Zvi Michal., 2004, UAI
   Sang JT, 2012, IEEE T MULTIMEDIA, V14, P883, DOI 10.1109/TMM.2012.2188782
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Song Y, 2011, ACM T WEB, V5, DOI 10.1145/1921591.1921595
   Tricomi FG, 1951, PAC J MATH, V1, P133, DOI DOI 10.2140/PJM.1951.1.133
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wu L, 2013, IEEE T PATTERN ANAL, V35, P716, DOI 10.1109/TPAMI.2012.124
   Xu Z, 2014, IEEE T MULTIMEDIA, V16, P1986, DOI 10.1109/TMM.2014.2342658
   Yin ZJ, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P957
   Zhao B, 2010, LECT NOTES COMPUT SC, V6315, P785, DOI 10.1007/978-3-642-15555-0_57
   Zhou M., 2012, Artificial Intelligence and Statistics, P1462
NR 52
TC 19
Z9 21
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 224
EP 237
DI 10.1109/TMM.2017.2716829
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700018
DA 2024-07-18
ER

PT J
AU Yu, L
   Tillo, T
   Xiao, JM
   Grangetto, M
AF Yu, Li
   Tillo, Tamtnam
   Xiao, Jimin
   Grangetto, Marco
TI Convolutional Neural Network for Intermediate View Enhancement in
   Multiview Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network (ConvNet); multiview navigation; multiview
   video representation; multiview video streaming
ID MPEG-DASH; VIDEO; REPRESENTATION; OPTIMIZATION; TEXTURE; FRAME
AB Multiview video streaming continues to gain popularity clue to the great viewing experience it offers, as well as its availability that has been enabled by increased network throughput and other recent technical developments. User demand for interactive multiview video streaming that provides seamless view switching upon request is also increasing. However, it is a highly challenging task to stream stable and high quality videos that allow real-time scene navigation within the bandwidth constraint. In this paper, a convolutional neural network (ConvNet)-assisted seamless multiview video streaming system is proposed to tackle the challenge. The proposed method solves the problem from two perspectives. First, a ConvNet-assisted multiview representation method is proposed, which provides flexible interactivity without compromising on multiview video compression efficiency. Second, a bit allocation mechanism guided by a navigation model is developed to provide seamless navigation and adapt to network bandwidth fluctuations at the same time. These two blocks work closely to provide an optimized viewing experience to users. They can be integrated into any existing multiview video streaming framework to enhance overall performance. Experimental results demonstrate the effectiveness of the proposed method for seamless multiview streaming.
C1 [Yu, Li; Xiao, Jimin] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
   [Tillo, Tamtnam] Libera Univ Bolzano Bozen, Fac Comp Sci, I-39100 Bolzano, Italy.
   [Grangetto, Marco] Univ Torino, Dept Comp Sci, I-10149 Turin, Italy.
C3 Xi'an Jiaotong-Liverpool University; Free University of Bozen-Bolzano;
   University of Turin
RP Xiao, JM (corresponding author), Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Peoples R China.
EM li.yu12@xjtlu.edu.cn; ttillo@unibz.it; jimin.xiao@xjtlu.edu.cn;
   marco.grangetto@unito.it
RI Grangetto, Marco/D-1222-2010
FU National Natural Science Foundation of China [61210006, 61501379];
   Jiangsu Science and Technology Programme [BK20150375]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61210006 and Grant 61501379 and by the Jiangsu Science
   and Technology Programme (BK20150375). The associate editor coordinating
   the review of tins manuscript and approving it for publication was Prof.
   Liang Zhou.
CR [Anonymous], NAG U SEQ
   [Anonymous], 2014, P INT C SIGN PROC CO
   Chakareski J, 2013, IEEE T IMAGE PROCESS, V22, P3473, DOI 10.1109/TIP.2013.2269801
   Chen Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/786015
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P744, DOI 10.1109/TIP.2010.2070074
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Fang L, 2014, IEEE T IMAGE PROCESS, V23, P185, DOI 10.1109/TIP.2013.2287608
   Gu B, 2015, IEEE T NEUR NET LEAR, V26, P1403, DOI 10.1109/TNNLS.2014.2342533
   Hannuksela MM, 2013, IEEE T IMAGE PROCESS, V22, P3449, DOI 10.1109/TIP.2013.2269274
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Le L, 2016, 2016 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING, APPLICATIONS AND TECHNOLOGIES (BDCAT), P1, DOI 10.1145/3006299.3006312
   Li B, 2013, IEEE INT SYMP CIRC S, P477, DOI 10.1109/ISCAS.2013.6571884
   Liao RJ, 2015, IEEE I CONF COMP VIS, P531, DOI 10.1109/ICCV.2015.68
   Mao XJ, 2016, ADV NEUR IN, V29
   Maugey T, 2013, IEEE T MULTIMEDIA, V15, P1070, DOI 10.1109/TMM.2013.2246147
   Maugey T, 2013, IEEE T IMAGE PROCESS, V22, P3459, DOI 10.1109/TIP.2013.2270183
   Maugey T, 2011, IEEE IMAGE PROC, P589, DOI 10.1109/ICIP.2011.6116618
   Merkle P, 2007, IEEE IMAGE PROC, P201
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Ndjiki-Nya P, 2011, IEEE T MULTIMEDIA, V13, P453, DOI 10.1109/TMM.2011.2128862
   Nunome T, 2015, ASIA-PAC CONF COMMUN, P575, DOI 10.1109/APCC.2015.7412577
   Pan ZQ, 2015, IEEE T BROADCAST, V61, P166, DOI 10.1109/TBC.2015.2419824
   Ren DN, 2014, IEEE T MULTIMEDIA, V16, P1874, DOI 10.1109/TMM.2014.2332139
   Sexton I, 1999, IEEE SIGNAL PROC MAG, V16, P85, DOI 10.1109/79.768575
   Smolic A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2161, DOI 10.1109/ICME.2006.262683
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Wang C, 2013, SIGNAL PROCESS-IMAGE, V28, P522, DOI 10.1016/j.image.2013.01.006
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Wen XZ, 2015, INFORM SCIENCES, V295, P395, DOI 10.1016/j.ins.2014.10.040
   Xiao JM, 2015, LECT NOTES COMPUT SC, V9315, P410, DOI 10.1007/978-3-319-24078-7_41
   Xiao JM, 2015, IEEE T CIRC SYST VID, V25, P139, DOI 10.1109/TCSVT.2014.2334011
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
   YANG YY, 1995, IEEE T IMAGE PROCESS, V4, P896, DOI 10.1109/83.392332
   Yu L., IEEE T BROA IN PRESS
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang YMD, 2012, EURASIP J ADV SIG PR, DOI 10.1186/1687-6180-2012-102
   Zhang YB, 2012, IEEE T CIRC SYST VID, V22, P12, DOI 10.1109/TCSVT.2011.2130450
   Zhang ZB, 2014, IEEE T IMAGE PROCESS, V23, P2343, DOI 10.1109/TIP.2014.2315958
   Zhang ZB, 2012, IEEE DATA COMPR CONF, P189, DOI 10.1109/DCC.2012.26
   Zhao MC, 2015, 2015 PICTURE CODING SYMPOSIUM (PCS) WITH 2015 PACKET VIDEO WORKSHOP (PV), P221, DOI 10.1109/PCS.2015.7170079
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhou L, 2017, IEEE T CIRC SYST VID, V27, P84, DOI 10.1109/TCSVT.2016.2539698
   Zhou L, 2016, IEEE T MULTIMEDIA, V18, P905, DOI 10.1109/TMM.2016.2537782
   Zhou YP, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P9, DOI 10.1109/VCIP.2014.7051491
NR 47
TC 11
Z9 11
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2018
VL 20
IS 1
BP 15
EP 28
DI 10.1109/TMM.2017.2726900
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FQ3WS
UT WOS:000418289700002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Trzcinski, T
   Rokita, P
AF Trzcinski, Tomasz
   Rokita, Przemyslaw
TI Predicting Popularity of Online Videos Using Support Vector Regression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computer vision; popularity prediction; support vector regression; video
   analysis
AB In this work, we propose a regression method to predict the popularity of an online video measured by its number of views. Our method uses Support Vector Regression with Gaussian radial basis functions. We show that predicting popularity patterns with this approach provides more precise and more stable prediction results, mainly thanks to the nonlinear character of the proposed method as well as its robustness. We prove the superiority of our method against the state of the art using datasets containing almost 24 000 videos from YouTube and Facebook. We also show that using visual features, such as the outputs of deep neural networks or scene dynamics' metrics, can be useful for popularity prediction before content publication. Furthermore, we show that popularity prediction accuracy can be improved by combining early distribution patterns with social and visual features and that social features represent a much stronger signal in terms of video popularity prediction than the visual ones.
C1 [Trzcinski, Tomasz; Rokita, Przemyslaw] Warsaw Univ Technol, Inst Comp Sci, PL-00665 Warsaw, Poland.
C3 Warsaw University of Technology
RP Trzcinski, T (corresponding author), Warsaw Univ Technol, Inst Comp Sci, PL-00665 Warsaw, Poland.
EM t.trzcinski@ii.pw.edu.pl; pro@ii.pw.edu.p
RI Rokita, Przemyslaw/JJF-1446-2023; Trzcinski, Tomasz/JJC-1873-2023
OI Rokita, Przemyslaw/0000-0002-4433-2133; Trzcinski,
   Tomasz/0000-0002-1486-8906
FU Faculty of Electronics and Information Technology at Warsaw University
   of Technology [II/2015/GD/1]
FX This work was supported in part by the Dean of the Faculty of
   Electronics and Information Technology at Warsaw University of
   Technology under project II/2015/GD/1. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Tao Mei. (Corresponding author: Tomasz Trzcinski.)
CR Adage.com, 2015, FAC 85 US CREAT CONT
   Almeida V, 1996, PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON PARALLEL AND DISTRIBUTED INFORMATION SYSTEMS, P92, DOI 10.1109/PDIS.1996.568672
   Amari S, 1999, NEURAL NETWORKS, V12, P783, DOI 10.1016/S0893-6080(99)00032-5
   [Anonymous], CORR
   [Anonymous], 2011, P 20 INT C COMPANION, DOI [10.1145/1963192.1963222, DOI 10.1145/1963192.1963222]
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], 2015, Deep Residual Learning for Image Recognition
   [Anonymous], 2016, P 24 ACM INT C MULT
   Bandari Roja., 2012, CORR
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Boujelbene Z. B., 2010, Int.J. Digit. Cont. Technol. Appl, V4, P100, DOI [10.4156/jdcta.vol4.issue6.12, DOI 10.4156/JDCTA.VOL4.ISSUE6.12]
   Bundesamt fur Verbraucherschutz und Lebens-mittelsicherheit Paul-Ehrlich-Gesellschaft fur Chemotherapie e. V, 2016, INTELLIGENCE
   Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Castillo C., 2014, P 17 ACM C COMP SUPP, P211, DOI [10.1145/2531602.2531623, DOI 10.1145/2531602.2531623]
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Chesire M, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P1
   Crane R, 2008, P NATL ACAD SCI USA, V105, P15649, DOI 10.1073/pnas.0803685105
   Drucker H, 1997, ADV NEUR IN, V9, P155
   Figueiredo F, 2014, ACM T INTERNET TECHN, V14, DOI 10.1145/2665065
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Gelli F, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P907, DOI 10.1145/2733373.2806361
   Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677
   Jacobs A., 2004, TRECVID, V2004, P197
   Jiang L., 2014, P INT C MULTIMEDIA R, P193
   Khosla A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P867, DOI 10.1145/2566486.2567996
   Min RQ, 2007, ICMLA 2007: SIXTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P13, DOI 10.1109/ICMLA.2007.84
   N. Techblog, 2016, ITS ALL TEST NETFL E
   Osborne M., 2011, P INT C WEB SOC, P586
   Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tatar A, 2014, J INTERNET SERV APPL, V5, DOI 10.1186/s13174-014-0008-y
   TechCrunch, 2015, 2015 SPEND RISES 187
   Tsagkias M, 2010, LECT NOTES COMPUT SC, V5993, P191, DOI 10.1007/978-3-642-12275-0_19
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wu B, 2016, AAAI CONF ARTIF INTE, P272
   Wu B, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1336, DOI 10.1145/2964284.2964335
NR 39
TC 104
Z9 110
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2017
VL 19
IS 11
BP 2561
EP 2570
DI 10.1109/TMM.2017.2695439
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FJ9CT
UT WOS:000413068200016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Panda, R
   Roy-Chowdhury, AK
AF Panda, Rameswar
   Roy-Chowdhury, Amit K.
TI Multi-View Surveillance Video Summarization via Joint Embedding and
   Sparse Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera network; multi-view video; sparse optimization; video
   summarization
ID CORRENTROPY; FRAMEWORK
AB Most traditional video summarization methods are designed to generate effective summaries for single-view videos, and thus, they cannot fully exploit the complicated intra-and inter-view correlations in summarizing multi-view videos in a camera network. In this paper, with the aim of summarizing multi-view videos, we introduce a novel unsupervised framework via joint embedding and sparse representative selection. The objective function is twofold. The first is to capture the multiview correlations via an embedding, which helps in extracting a diverse set of representatives. The second is to use a l(2,1)-norm to model the sparsity while selecting representative shots for the summary. We propose to jointly optimize both of the objectives, such that embedding can not only characterize the correlations, but also indicate the requirements of sparse representative selection. We present an efficient alternating algorithm based on half-quadratic minimization to solve the proposed non-smooth and non-convex objective with convergence analysis. A key advantage of the proposed approach with respect to the state-of-the-art is that it can summarize multi-view videos without assuming any prior correspondences/alignment between them, e.g., uncalibrated camera networks. Rigorous experiments on several multi-view datasets demonstrate that our approach clearly outperforms the state-of-the-art methods.
C1 [Panda, Rameswar; Roy-Chowdhury, Amit K.] Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA.
C3 University of California System; University of California Riverside
RP Panda, R (corresponding author), Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA.
EM rpand002@ucr.edu; amitrc@ece.ucr.edu
RI Panda, Rameswar/AAY-9834-2020
OI Panda, Rameswar/0000-0003-4359-2475; Roy-Chowdhury,
   Amit/0000-0001-6690-9725
FU NSF [1544969]; NVIDIA
FX This work was supported in part by the NSF under Grant 1544969, and in
   part by the NVIDIA with the donation of the Tesla K40 GPU. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Cees Snoek. (Corresponding author: Rameswar Panda.)
CR Almeida J, 2012, PATTERN RECOGN LETT, V33, P397, DOI 10.1016/j.patrec.2011.08.007
   [Anonymous], P INT C AC SPEECH SI
   [Anonymous], 2014, ADV NEURAL INFORM PR
   [Anonymous], 2014, COMPUT NOW
   [Anonymous], P 27 INT C SYST
   [Anonymous], P 2 INT WORKSH STAT
   [Anonymous], 2007, PROC IEEE INT C COMP
   [Anonymous], P COMP VIS PATT REC
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Caicedo JC, 2015, IEEE I CONF COMP VIS, P2488, DOI 10.1109/ICCV.2015.286
   Chakraborty A, 2016, IEEE T PATTERN ANAL, V38, P1859, DOI 10.1109/TPAMI.2015.2491922
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   De Leo C, 2011, LECT NOTES COMPUT SC, V6468, P94
   De Leo C, 2014, ACM T SENSOR NETWORK, V10, DOI 10.1145/2530285
   Dornaika F, 2015, PATTERN RECOGN, V48, P3714, DOI 10.1016/j.patcog.2015.05.018
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Feng SK, 2012, PROC CVPR IEEE, P2082, DOI 10.1109/CVPR.2012.6247913
   de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gao SH, 2010, PROC CVPR IEEE, P3555, DOI 10.1109/CVPR.2010.5539943
   GEMAN D, 1992, IEEE T PATTERN ANAL, V14, P367, DOI 10.1109/34.120331
   Guan GL, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2632267
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   He R, 2014, IEEE T PATTERN ANAL, V36, P261, DOI 10.1109/TPAMI.2013.102
   He R, 2012, PROC CVPR IEEE, P2504, DOI 10.1109/CVPR.2012.6247966
   Herranz L, 2010, IEEE T CIRC SYST VID, V20, P1265, DOI 10.1109/TCSVT.2010.2057020
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khosla A, 2013, PROC CVPR IEEE, P2698, DOI 10.1109/CVPR.2013.348
   Kim G, 2014, PROC CVPR IEEE, P4225, DOI 10.1109/CVPR.2014.538
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Kuanar SK, 2013, J VIS COMMUN IMAGE R, V24, P1212, DOI 10.1016/j.jvcir.2013.08.003
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Lie WN, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P413, DOI 10.1109/ICME.2008.4607459
   Lu CY, 2013, IEEE I CONF COMP VIS, P1801, DOI 10.1109/ICCV.2013.226
   Lu XQ, 2013, IEEE T CIRC SYST VID, V23, P2022, DOI 10.1109/TCSVT.2013.2244798
   Ma YF, 2005, IEEE T MULTIMEDIA, V7, P907, DOI 10.1109/TMM.2005.854410
   Mei SH, 2015, PATTERN RECOGN, V48, P522, DOI 10.1016/j.patcog.2014.08.002
   Meng JJ, 2016, PROC CVPR IEEE, P1039, DOI 10.1109/CVPR.2016.118
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Nguyen HV, 2012, LECT NOTES COMPUT SC, V7577, P414, DOI 10.1007/978-3-642-33783-3_30
   Nie FP, 2011, IEEE I CONF COMP VIS, P2268
   Ou SH, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331916
   Panda R, 2016, INT C PATT RECOG, P2971, DOI 10.1109/ICPR.2016.7900089
   Panda R, 2016, IEEE IMAGE PROC, P191, DOI 10.1109/ICIP.2016.7532345
   Panda R, 2014, INT C PATT RECOG, P3481, DOI 10.1109/ICPR.2014.599
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Peng Y, 2017, MULTIMED TOOLS APPL, V76, P8859, DOI 10.1007/s11042-016-3510-3
   Peng YX, 2006, IEEE T CIRC SYST VID, V16, P612, DOI 10.1109/TCSVT.2006.873157
   Ping Li, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2473, DOI 10.1109/ICIP.2011.6116162
   Poleg Y, 2014, PROC CVPR IEEE, P2537, DOI 10.1109/CVPR.2014.325
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Pritch Y., 2007, PROC IEEE INT C COMP, P1
   Roy-Chowdhury AmitK., 2012, Synthesis Lectures on Computer Vision
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salehin Md.Musfequs., 2016, Fusion of Foreground Object, Spatial and Frequency Domain Motion Information for Video Summarization
   Simonyan K., 2014, CORR
   Song YL, 2015, PROC CVPR IEEE, P5179, DOI 10.1109/CVPR.2015.7299154
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Wang M, 2013, ACM T INTERNET TECHN, V12, DOI 10.1145/2492690
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang Y, 2015, IEEE T IMAGE PROCESS, V24, P4027, DOI 10.1109/TIP.2015.2456508
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   Zhang K, 2016, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2016.120
   Zhang K, 2016, LECT NOTES COMPUT SC, V9911, P766, DOI 10.1007/978-3-319-46478-7_47
   Zhao B, 2014, PROC CVPR IEEE, P2513, DOI 10.1109/CVPR.2014.322
   Zhu XT, 2016, INT J COMPUT VISION, V117, P247, DOI 10.1007/s11263-015-0864-3
NR 71
TC 45
Z9 47
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2017
VL 19
IS 9
BP 2010
EP 2021
DI 10.1109/TMM.2017.2708981
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5XC
UT WOS:000411244200005
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Cheng, ZQ
   Wu, X
   Liu, Y
   Hua, XS
AF Cheng, Zhi-Qi
   Wu, Xiao
   Liu, Yang
   Hua, Xian-Sheng
TI Video eCommerce plus plus : Toward Large Scale Online Video Advertising
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE E-commerce; online advertising; recommendation; video analysis
ID RECOMMENDATION
AB The prevalence of online videos provides an opportunity for e-commerce companies to recommend their products in videos. In this paper, we propose an online video advertising system named Video eCommerce++, to exhibit appropriate product ads to particular users at proper time stamps of videos, which takes into account video semantics, user shopping preference, and viewing behavior feedback. First, an incremental co-relation regression (ICRR) model is novelly proposed to construct the semantic association between videos and products. To meet the requirement of online advertising, ICRR is implemented in an incremental way to reduce the time complexity. User preference diffusion (UPD) is induced under the framework of heterogeneous information network to construct user-product association from two different e-commerce platforms, Tmall and MagicBox, which alleviates the problems of data sparsity and cold start. A video scene importance model (VSIM) is proposed to model the scene importance by utilizing the user viewing behavior, so that ads can be embedded at the most attractive positions in the video stream. To combine the outputs of ICRR, UPD, and VSIM, a unified distributed heterogeneous relationmatrix factorization (D-HRMF) is applied for online video advertising, which is efficiently conducted in parallel to address the real-time update problem, so that the whole system can be performed in real time. Extensive experiments conducted on a variety of online videos from Tmall MagicBox demonstrate that Video eCommerce++ significantly outperforms the state-of-the-art advertising methods, and can handle large-scale data in real time.
C1 [Cheng, Zhi-Qi; Wu, Xiao] Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
   [Liu, Yang; Hua, Xian-Sheng] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
C3 Southwest Jiaotong University; Alibaba Group
RP Wu, X (corresponding author), Southwest Jiaotong Univ, Sch Informat Sci & Technol, Chengdu 610031, Peoples R China.
EM zhiqicheng@gmail.com; wuxiaohk@home.swjtu.edu.cn;
   panjun.ly@alibaba-inc.com; xiansheng.hxs@alibaba-inc.com
RI Cheng, Zhi-Qi/ABD-1657-2020
OI Cheng, Zhi-Qi/0000-0002-1720-2085
FU National Natural Science Foundation of China [61373121]; Program for
   Sichuan Provincial Science Fund for Distinguished Young Scholars
   [13QNJJ0149]; Fundamental Research Funds for the Central Universities;
   Graduate Innovation Fund of Southwest Jiaotong University [YC201504110]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61373121, in part by the Program for
   Sichuan Provincial Science Fund for Distinguished Young Scholars under
   Grant 13QNJJ0149, in part by the Fundamental Research Funds for the
   Central Universities, and in part by the Graduate Innovation Fund of
   Southwest Jiaotong University under Grant YC201504110. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Shu-Ching Chen. (Correpsonding author: Xiao Wu.)
CR [Anonymous], 2014, ACMMM
   Cheng ZQ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1365, DOI 10.1145/2964284.2964326
   Corrado G., 2012, P 25 INT C NEUR INF, P1223
   Cui P, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P597, DOI 10.1145/2647868.2654946
   Elkahky A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P278, DOI 10.1145/2736277.2741667
   Eyal K, 2003, J BROADCAST ELECTRON, V47, P77, DOI 10.1207/s15506878jobem4701_5
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Hoffman J., 2014, NIPS (News Physiol. Sci.), P3536
   Hu Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P129, DOI 10.1145/2733373.2806239
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Jiang P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1879, DOI 10.1145/2783258.2788562
   Kar Wreetabrata., 2015, Proceedings of the 9th ACM Conference on Recommender Systems, RecSys'15, P203, DOI DOI 10.1145/2792838.2800194
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   McAuley J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2783258.2783381
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Mei T., 2008, P 16 ACM INT C MULT, P439, DOI [https://doi.org/10.1145/1459359.1459418, DOI 10.1145/1459359.1459418]
   Mei T, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071402
   Mei T, 2010, P IEEE, V98, P1416, DOI 10.1109/JPROC.2009.2039841
   Mei T, 2009, IEEE T CIRC SYST VID, V19, P1866, DOI 10.1109/TCSVT.2009.2026949
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Ronen I, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P243, DOI 10.1145/2600428.2609596
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Shalom OS, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P63, DOI 10.1145/2872427.2883057
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Sunt YZ, 2011, PROC VLDB ENDOW, V4, P992
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Wang J., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P501, DOI 10.1145/1148170.1148257
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wu A. G., 2007, P ACM MM, P218
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yi Zhang, 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P81
   Yu B., 2003, Proceedings of the eleventh ACM international conference on Multimedia, P382
   Yu X, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P283, DOI 10.1145/2556195.2556259
   Zhang S, 2006, SIAM PROC S, P549
   Zhang YF, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1373, DOI 10.1145/2736277.2741087
   Zhong WL, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2287, DOI 10.1145/2783258.2788565
   Zhou P, 2016, IEEE T MULTIMEDIA, V18, P1217, DOI 10.1109/TMM.2016.2537216
   Zhu T, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P657, DOI 10.1145/2600428.2609603
NR 38
TC 22
Z9 22
U1 2
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1170
EP 1183
DI 10.1109/TMM.2016.2647386
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400005
DA 2024-07-18
ER

PT J
AU Song, XD
   Peng, XL
   Xu, JZ
   Shi, GM
   Wu, F
AF Song, Xiaodan
   Peng, Xiulian
   Xu, Jizheng
   Shi, Guangming
   Wu, Feng
TI Distributed Compressive Sensing for Cloud-Based Wireless Image
   Transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed compressive sensing (CS); graceful degradation (GD); image
   transmission; side information (SI)
ID VIDEO; RECONSTRUCTION; INFORMATION
AB We consider efficient image transmission via timevarying channels. To improve the performance, we propose a new distributed compressive sensing (CS) scheme that can leverage similar images in the cloud. It is featured by channel SNR and bandwidth scalability, high efficiency, and low encoding complexity. For each image, a compressed thumbnail is first transmitted after forward error correction (FEC) and modulation to retrieve similar images and generate a side information (SI) in the cloud. The residual image after subtracting the decompressed thumbnail is then coded and transmitted by CS through a very dense constellation without FEC. The linearly and ratelessly generated CS measurements make it capable of achieving both graceful quality degradation (GD) with the channel SNR and bandwidth scalability in a universal scheme. A mode decision and transform-domain power allocation are introduced for better bandwidth usage and protection against channel errors. At the decoder, a two-step CS decoding is performed to recover the residual signal, where both the local and nonlocal correlations within the image and that with the SI are exploited. Simulations on landmark images and an AWGN channel show that the received image quality gracefully increases with the channel SNR and bandwidth. Furthermore, it outperforms existing schemes both subjectively and objectively by up to 11 dB gains compared with the state-of-the-art transmission scheme with GD, i.e. SoftCast.
C1 [Song, Xiaodan; Shi, Guangming] Xidian Univ, Xian 710071, Peoples R China.
   [Peng, Xiulian; Xu, Jizheng] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Wu, Feng] Univ Sci & Technol China, Hefei 230026, Peoples R China.
C3 Xidian University; Microsoft; Microsoft Research Asia; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS
RP Song, XD (corresponding author), Xidian Univ, Xian 710071, Peoples R China.
EM xiaodansong@outlook.com; xipe@microsoft.com; jzxu@microsoft.com;
   gmshi@xidian.edu.cn; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; Xu, Jizheng/JDD-5152-2023; Song,
   Xiaodan/GSE-2144-2022
OI Song, Xiaodan/0000-0002-8049-1828
FU Distinguished Young Scholars Program [61425026]; Major Project from the
   National Natural Science Foundation of China [61631017]; Major State
   Basic Research Development Program of China (973 Program)
   [2013CB329402]; National Natural Science Foundation of China [61472301,
   61632019, 61372131, 61672404]; Foundation for Innovative Research Groups
   of the National Natural Science Foundation of China [61621005]
FX This work was supported in part by the Distinguished Young Scholars
   Program under Grant 61425026, in part by a Major Project from the
   National Natural Science Foundation of China under Grant 61631017, in
   part by the Major State Basic Research Development Program of China (973
   Program, 2013CB329402), in part by the National Natural Science
   Foundation of China under Grant 61472301, Grant 61632019, Grant
   61372131, and Grant 61672404, and in part by the Foundation for
   Innovative Research Groups of the National Natural Science Foundation of
   China under Grant 61621005. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Honggang
   Wang.
CR [Anonymous], 2014, J CHONGQING U, DOI DOI 10.11835/J.ISSN.1000-582X.2014.05.014
   [Anonymous], 2014, P IEEE INT C MULT EX
   [Anonymous], 2015, Math. Prob. Eng.
   [Anonymous], HEVC TEST MOD HM14 0
   Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P118, DOI 10.1109/MSP.2007.4286571
   Baron D., 2009, DISTRIBUTED COMPRESS
   Candès EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083
   Chang K, 2013, IEEE INT SYMP CIRC S, P221, DOI 10.1109/ISCAS.2013.6571822
   Cui Hao, 2013, P ACM INT C MOD AN S, P273
   DAUBECHIES I, 2003, ITERATIVE THRESHOLDI
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   Dong WS, 2014, IEEE T IMAGE PROCESS, V23, P3618, DOI 10.1109/TIP.2014.2329449
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   Fan XM, 2012, IEEE NUCL SCI CONF R, P1
   Gan L., 2008, 2008 16th European Signal Processing Conference, P1
   Gan ZL, 2010, INT CONF SIGN PROCES, P1129, DOI 10.1109/ICOSP.2010.5655867
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   Hung-Wei Chen, 2010, 2010 28th Picture Coding Symposium (PCS 2010), P210, DOI 10.1109/PCS.2010.5702466
   Jakubczak S., 2011, PROC MOBICOM, P289
   Katabi D., 2009, TECH REP
   Kratochvll T., 2009, LECT NOTES ELECT ENG
   LEE KH, 1976, IEEE T COMMUN, V24, P1283
   Li CB, 2013, IEEE T BROADCAST, V59, P197, DOI 10.1109/TBC.2012.2226509
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2014, SIGNAL PROCESS-IMAGE, V29, P361, DOI 10.1016/j.image.2014.01.005
   Petrazzuoli G, 2014, IEEE T MULTIMEDIA, V16, P1834, DOI 10.1109/TMM.2014.2342201
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   SLEPIAN D, 1973, IEEE T INFORM THEORY, V19, P471, DOI 10.1109/TIT.1973.1055037
   Song XD, 2015, IEEE T CIRC SYST VID, V25, P1926, DOI 10.1109/TCSVT.2015.2416562
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Thirumalai V, 2012, IEEE T IMAGE PROCESS, V21, P3206, DOI 10.1109/TIP.2012.2188035
   Trocan M, 2014, MULTIMED TOOLS APPL, V72, P95, DOI 10.1007/s11042-012-1330-7
   Wang AH, 2014, SIGNAL PROCESS-IMAGE, V29, P599, DOI 10.1016/j.image.2014.03.002
   Wen YG, 2014, IEEE T MULTIMEDIA, V16, P885, DOI 10.1109/TMM.2014.2315596
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu F, 2014, IEEE T IMAGE PROCESS, V23, P1015, DOI 10.1109/TIP.2014.2298972
   WYNER AD, 1976, IEEE T INFORM THEORY, V22, P1, DOI 10.1109/TIT.1976.1055508
   Xiong RG, 2014, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2014.55
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Yue HJ, 2013, IEEE T MULTIMEDIA, V15, P845, DOI 10.1109/TMM.2013.2239629
   Zhang YX, 2008, IEEE T MULTIMEDIA, V10, P1648, DOI 10.1109/TMM.2008.2007324
NR 43
TC 24
Z9 24
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1351
EP 1364
DI 10.1109/TMM.2017.2654123
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400019
DA 2024-07-18
ER

PT J
AU Dong, XP
   Shen, JB
   Yu, DJ
   Wang, WG
   Liu, JH
   Huang, H
AF Dong, Xingping
   Shen, Jianbing
   Yu, Dajiang
   Wang, Wenguan
   Liu, Jianhong
   Huang, Hua
TI Occlusion-Aware Real-Time Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Circulant structure; classifier-pool; entropy minimization; occlusion;
   real-time; visual tracking
ID VISUAL TRACKING
AB The online learning methods are popular for visual tracking because of their robust performance for most video sequences. However, the drifting problem caused by noisy updates is still a challenge for most highly adaptive online classifiers. In visual tracking, target object appearance variation, such as deformation and long-term occlusion, easily causes noisy updates. To overcome this problem, a new real-time occlusion-aware visual tracking algorithm is introduced. First, we learn a novel two-stage classifier with circulant structure with kernel, named integrated circulant structure kernels (ICSK). The first stage is applied for transition estimation and the second is used for scale estimation. The circulant structure makes our algorithm realize fast learning and detection. Then, the ICSK is used to detect the target without occlusion and build a classifier pool to save these classifiers with noisy updates. When the target is in heavy occlusion or after longterm occlusion, we redetect it using an optimal classifier selected from the classifier-pool according to an entropy minimization criterion. Extensive experimental results on the full benchmark demonstrate our real-time algorithm achieves better performance than state-of-the-art methods.
C1 [Dong, Xingping; Shen, Jianbing; Yu, Dajiang; Wang, Wenguan; Liu, Jianhong; Huang, Hua] Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Shen, JB (corresponding author), Beijing Inst Technol, Beijing Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM shenjianbing@bit.edu.cn
RI XIONG, LIU/JOK-5886-2023; Huang, Hua/M-9684-2013; Dong,
   Xingping/AAA-8107-2019; Shen, Jianbing/U-8796-2019; Wang,
   Wenguan/AAA-5782-2022; Lu, Jianhong/IYT-3322-2023
OI Huang, Hua/0000-0003-2587-1702; Dong, Xingping/0000-0003-1613-9288;
   Wang, Wenguan/0000-0002-0802-9567; Shen, Jianbing/0000-0002-4109-8353
FU National Basic Research Program of China (973 Program) [2013CB328805];
   National Natural Science Foundation of China [61272359]; Fok Ying-Tong
   Education Foundation for Young Teachers, Specialized Fund for Joint
   Building Program of Beijing Municipal Education Commission
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2013CB328805, in part by the National
   Natural Science Foundation of China under Grant 61272359, and in part by
   the Fok Ying-Tong Education Foundation for Young Teachers, Specialized
   Fund for Joint Building Program of Beijing Municipal Education
   Commission. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhen Wen.
   (Corresponding author: Jianbing Shen.)
CR [Anonymous], 2014, LECT NOTES COMPUT SC, DOI DOI 10.1007/978-3-319-10599-4_13
   [Anonymous], 2012, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2012.6247881
   Avidan S, 2004, IEEE T PATTERN ANAL, V26, P1064, DOI 10.1109/TPAMI.2004.53
   Babenko B, 2011, IEEE T PATTERN ANAL, V33, P1619, DOI 10.1109/TPAMI.2010.226
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Chen DP, 2013, IEEE I CONF COMP VIS, P1113, DOI 10.1109/ICCV.2013.142
   Chen DP, 2014, LECT NOTES COMPUT SC, V8689, P345, DOI 10.1007/978-3-319-10590-1_23
   Danelljan M., 2014, BRIT MACH VIS C, P1, DOI DOI 10.5244/C.28.65
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Dong XP, 2016, IEEE T IMAGE PROCESS, V25, P516, DOI 10.1109/TIP.2015.2505184
   Grandvalet Y, 2004, Advances in neural information processing systems, V17
   Hare S, 2016, IEEE T PATTERN ANAL, V38, P2096, DOI 10.1109/TPAMI.2015.2509974
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Henriques J, 2012, LNCS, P702, DOI DOI 10.1007/978-3-642-33765-9_50
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Holzer S, 2013, IEEE T PATTERN ANAL, V35, P105, DOI 10.1109/TPAMI.2012.86
   Jianbing Shen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3481, DOI 10.1109/CVPR.2011.5995507
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kwak S, 2011, IEEE I CONF COMP VIS, P1551, DOI 10.1109/ICCV.2011.6126414
   Lee DY, 2014, PROC CVPR IEEE, P3486, DOI 10.1109/CVPR.2014.446
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P5867, DOI 10.1109/TIP.2016.2615812
   Ma B, 2016, IEEE T CYBERNETICS, V46, P2411, DOI 10.1109/TCYB.2015.2477879
   Ma B, 2016, IEEE T IMAGE PROCESS, V25, P4199, DOI 10.1109/TIP.2016.2588329
   Ma B, 2015, IEEE I CONF COMP VIS, P4400, DOI 10.1109/ICCV.2015.500
   Ma B, 2015, IEEE T MULTIMEDIA, V17, P1818, DOI 10.1109/TMM.2015.2463221
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Saffari Amir, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1393, DOI 10.1109/ICCVW.2009.5457447
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Shen JB, 2014, IEEE T CIRC SYST VID, V24, P1088, DOI 10.1109/TCSVT.2014.2302545
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Shen JB, 2013, IEEE T CYBERNETICS, V43, P1453, DOI 10.1109/TCYB.2013.2273270
   Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230
   Wang D, 2014, PROC CVPR IEEE, P3478, DOI 10.1109/CVPR.2014.445
   Wang D, 2015, IEEE T IMAGE PROCESS, V24, P2646, DOI 10.1109/TIP.2015.2427518
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xie LX, 2013, IEEE T MULTIMEDIA, V15, P1244, DOI 10.1109/TMM.2013.2264929
   Zhang KH, 2014, LECT NOTES COMPUT SC, V8693, P127, DOI 10.1007/978-3-319-10602-1_9
   Zhang KH, 2012, LECT NOTES COMPUT SC, V7574, P864, DOI 10.1007/978-3-642-33712-3_62
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
   Zuo Z, 2015, PATTERN RECOGN, V48, P3004, DOI 10.1016/j.patcog.2015.02.003
NR 44
TC 199
Z9 207
U1 1
U2 65
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2017
VL 19
IS 4
BP 763
EP 771
DI 10.1109/TMM.2016.2631884
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EO0NT
UT WOS:000396395500008
DA 2024-07-18
ER

PT J
AU Xie, WC
   Shen, LL
   Jiang, JM
AF Xie, Weicheng
   Shen, Linlin
   Jiang, Jianmin
TI A Novel Transient Wrinkle Detection Algorithm and Its Application for
   Expression Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Edge detection; expression synthesis; transient wrinkle; wrinkle
   structure
ID RECOGNITION; FEATURES; SCALE
AB Because facial wrinkle is a representative feature of facial expression, automatic wrinkle detection has been an important and challenging topic for expression simulation, recognition, and animation. Recently, most works about wrinkle detection have focused on permanent wrinkles (e.g., age wrinkles), which are usually linear shapes, whereas the detection of transient wrinkles (e.g., expression wrinkles) has not been sufficiently studied because of their shape diversity and complexity. In this work, a novel algorithm for automatic detection of transient wrinkles with linear, fixed, and chaotic shapes is proposed, which largely consists of edge pair matching, active-appearance-model-based wrinkle structure location, and support-vector-machine-based wrinkle classification. The proposed wrinkle detector is applied for expression synthesis and an improved Poisson wrinkle mapping approach is proposed. Experimental results illustrate the competitiveness of the proposed wrinkle detector in detecting different transient wrinkles. Compared with state-of-the-art algorithms, the proposed approach yields complete and accurate wrinkle centers. The expression synthesized by the improved wrinkle mapping is also much more realistic.
C1 [Xie, Weicheng; Shen, Linlin] Shenzhen Univ, Comp Vis Inst, Sch Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Xie, Weicheng; Shen, Linlin] Shenzhen Univ, Shenzhen Key Lab Spatial Informat Smart Sensing &, Shenzhen 518060, Peoples R China.
   [Xie, Weicheng] Univ Nottingham, Sch Comp Sci, Nottingham NG8 1BB, England.
   [Jiang, Jianmin] Shenzhen Univ, Res Inst Future Media Comp, Shenzhen 518060, Peoples R China.
C3 Shenzhen University; Shenzhen University; University of Nottingham;
   Shenzhen University
RP Shen, LL (corresponding author), Shenzhen Univ, Comp Vis Inst, Sch Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.; Shen, LL (corresponding author), Shenzhen Univ, Shenzhen Key Lab Spatial Informat Smart Sensing &, Shenzhen 518060, Peoples R China.
EM wcxie@szu.edu.cn; llshen@szu.edu.cn; jianmin.jiang@szu.edu.cn
RI Shen, Linlin/AEX-9392-2022
OI Shen, Linlin/0000-0003-1420-0815
FU Natural Science Foundation of China [61272050, 61672357, 61602315];
   Science Foundation of Guangdong Province [2014A030313556]; China
   Postdoctoral Science Foundation [2015M572363]; Science and Technology
   Innovation Commission of Shenzhen [JCYJ20160422144110140]; Open Research
   Fund of China-U.K. Visual Information Processing Laboratory
FX The work was supported by the Natural Science Foundation of China under
   Grant 61272050, Grant 61672357, and Grant 61602315, by the Science
   Foundation of Guangdong Province under Grant 2014A030313556, by the
   China Postdoctoral Science Foundation under Grant 2015M572363, by the
   Science and Technology Innovation Commission of Shenzhen under Grant
   JCYJ20160422144110140, and by the Open Research Fund of China-U.K.
   Visual Information Processing Laboratory.
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], P INT C MACH VIS APP
   Azazi A, 2015, EXPERT SYST APPL, V42, P3056, DOI 10.1016/j.eswa.2014.10.042
   Batool N, 2015, PATTERN RECOGN, V48, P642, DOI 10.1016/j.patcog.2014.08.003
   Batool N, 2014, IEEE T IMAGE PROCESS, V23, P3773, DOI 10.1109/TIP.2014.2332401
   Batool N, 2012, LECT NOTES COMPUT SC, V7584, P178, DOI 10.1007/978-3-642-33868-7_18
   BEIER T, 1992, COMP GRAPH, V26, P35, DOI 10.1145/142920.134003
   Boissieux L, 2000, SPRING COMP SCI, P15
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cao C, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766943
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Cula GO, 2013, SKIN RES TECHNOL, V19, pE243, DOI 10.1111/j.1600-0846.2012.00635.x
   Cula OG, 2005, INT J COMPUT VISION, V62, P97, DOI 10.1007/s11263-005-4637-2
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Haibin Ling, 2007, 2007 11th IEEE International Conference on Computer Vision, P1
   Huang YH, 2012, IMAGE VISION COMPUT, V30, P750, DOI 10.1016/j.imavis.2011.12.008
   Huang YZ, 2010, IEEE T MULTIMEDIA, V12, P536, DOI 10.1109/TMM.2010.2052792
   Jain AK, 2009, IEEE IMAGE PROC, P37, DOI 10.1109/ICIP.2009.5413921
   Jian MW, 2015, IEEE T CYBERNETICS, V45, P1575, DOI 10.1109/TCYB.2014.2356200
   KWON YH, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P762, DOI 10.1109/CVPR.1994.323894
   Li K, 2014, IEEE T MULTIMEDIA, V16, P299, DOI 10.1109/TMM.2013.2293064
   Li L, 2011, COMPUT GRAPH-UK, V35, P175, DOI 10.1016/j.cag.2010.08.003
   Liu F, 2015, NEUROCOMPUTING, V168, P599, DOI 10.1016/j.neucom.2015.05.065
   Liu ZC, 2001, COMP GRAPH, P271
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Milborrow S, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P380
   Ng CC, 2015, IEEE ACCESS, V3, P1079, DOI 10.1109/ACCESS.2015.2455871
   Ng CC, 2015, LECT NOTES COMPUT SC, V9005, P609, DOI 10.1007/978-3-319-16811-1_40
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Popa T, 2009, COMPUT GRAPH FORUM, V28, P427, DOI 10.1111/j.1467-8659.2009.01382.x
   Scherbaum K, 2011, COMPUT GRAPH FORUM, V30, P485, DOI 10.1111/j.1467-8659.2011.01874.x
   Shen LL, 2008, MED IMAGE ANAL, V12, P375, DOI 10.1016/j.media.2007.12.004
   Sofka M, 2006, IEEE T MED IMAGING, V25, P1531, DOI 10.1109/TMI.2006.884190
   Song ML, 2007, IEEE T MULTIMEDIA, V9, P1384, DOI 10.1109/TMM.2007.906591
   Tian YI, 2001, IEEE T PATTERN ANAL, V23, P97, DOI 10.1109/34.908962
   Tsalakanidou F, 2010, PATTERN RECOGN, V43, P1763, DOI 10.1016/j.patcog.2009.12.009
   Tzimiropoulos G, 2013, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2013.79
   Wu Y, 1999, VISUAL COMPUT, V15, P183, DOI 10.1007/s003710050171
   Yin LJ, 2001, COMPUT VIS IMAGE UND, V84, P201, DOI 10.1006/cviu.2001.0949
   Zhang YH, 2014, J VIS COMMUN IMAGE R, V25, P916, DOI 10.1016/j.jvcir.2014.02.010
   Zhang YM, 2005, IEEE T PATTERN ANAL, V27, P699, DOI 10.1109/TPAMI.2005.93
NR 45
TC 17
Z9 18
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2017
VL 19
IS 2
BP 279
EP 292
DI 10.1109/TMM.2016.2614429
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN1UN
UT WOS:000395795800005
DA 2024-07-18
ER

PT J
AU Shao, F
   Li, KM
   Lin, WS
   Jiang, GY
   Dai, QH
AF Shao, Feng
   Li, Kemeng
   Lin, Weisi
   Jiang, Gangyi
   Dai, Qionghai
TI Learning Blind Quality Evaluator for Stereoscopic Images Using Joint
   Sparse Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; feature-distribution; feature-prior;
   joint sparse representation; stereoscopic image
ID BINOCULAR COMBINATION; GRADIENT MAGNITUDE; RECEPTIVE-FIELDS; VIDEO
   QUALITY; GAIN-CONTROL; COMPRESSION; STATISTICS; SIMILARITY; MODEL
AB Perceptual quality prediction for stereoscopic images is of fundamental importance in determining the level of quality perceived by humans in terms of the 3D viewing experience. However, the existing no-reference quality assessment (NR-IQA) framework has its limitation in addressing binocular combination for stereoscopic images. In this paper, we propose a new NR-IQA for stereoscopic images using joint sparse representation. We analyze the relationship between left and right quality predictors, and formulate stereoscopic quality prediction as a combination of feature-prior and feature-distribution. Based on this finding, we extract feature vector that handles different features to be interacted by joint sparse representation, and use support vector regression to characterize feature-prior. Meanwhile, we implement feature-distribution using sparsity regularization as the basis of weights for binocular combination to derive the overall quality score. Experimental results on five public 3D IQA databases demonstrate that in comparison with the existing methods, the devised algorithm achieves high consistent alignment with subjective assessment.
C1 [Shao, Feng; Li, Kemeng; Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Ctr Multimedia & Network Technol, Singapore 639798, Singapore.
   [Dai, Qionghai] Tsinghua Univ, Broadband Networks & Digital Media Lab, Beijing 100084, Peoples R China.
C3 Ningbo University; Nanyang Technological University; Tsinghua University
RP Shao, F (corresponding author), Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Zhejiang, Peoples R China.
EM shaofeng@nbu.edu.cn; likemengnbu@139.com; wslin@ntu.edu.sg;
   jianggangyi@nbu.edu.cn; qhdai@tsinghua.edu.cn
RI Lin, Weisi/A-3696-2011; Lin, Wei/D-3353-2012; Lin, Weisi/A-8011-2012
OI Lin, Weisi/0000-0001-9866-1947; 
FU Natural Science Foundation of China [61271021]; K.C. Wong Magna Fund,
   Ningbo University, Ningbo, China
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61271021, and in part by the K.C. Wong Magna Fund,
   Ningbo University, Ningbo, China. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Christian Timmerer.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2010, P 5 INT WORKSH VID P
   [Anonymous], 2011, 2011 IEEE INT INSTRU, DOI DOI 10.1109/IMTC.2011.5944170
   [Anonymous], 2013, P IVMSP JUN
   Baker DH, 2012, VISION RES, V56, P1, DOI 10.1016/j.visres.2012.01.008
   Banitalebi-Dehkordi A, 2016, MULTIMED TOOLS APPL, V75, P4187, DOI 10.1007/s11042-015-2466-z
   Battisti F, 2015, SIGNAL PROCESS-IMAGE, V30, P78, DOI 10.1016/j.image.2014.10.005
   Belhumeur PN, 1996, INT J COMPUT VISION, V19, P237, DOI 10.1007/BF00055146
   Benoit A, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/659024
   Bensalma R, 2013, MULTIDIM SYST SIGN P, V24, P281, DOI 10.1007/s11045-012-0178-3
   Boev A, 2006, 7TH IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, P218
   Boureau Y. L., 2010, P 27 INT C MACH LEAR, P111
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Cong LJ, 2014, J VISION, V14, DOI 10.1167/14.13.1
   COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   De Silva V, 2013, IEEE T IMAGE PROCESS, V22, P3392, DOI 10.1109/TIP.2013.2268422
   Ding J, 2006, P NATL ACAD SCI USA, V103, P1141, DOI 10.1073/pnas.0509629103
   Ding J, 2013, J VISION, V13, DOI 10.1167/13.2.13
   Gorley P., 2008, SPIE STER DISPL APPL
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Gu K, 2012, J ELECTR COMPUT ENG, V2012, DOI 10.1155/2012/436031
   Guha T, 2014, SIGNAL PROCESS-IMAGE, V29, P1138, DOI 10.1016/j.image.2014.09.010
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Hwang JJ, 2011, KSII T INTERNET INF, V5, P1613, DOI 10.3837/tiis.2011.09.007
   Jin L., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2521, DOI 10.1109/ICIP.2011.6116175
   Jin LN, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-6
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   Julesz B., 1971, Foundation of Cyclopean Perception
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim T, 2014, IEEE T MULTIMEDIA, V16, P387, DOI 10.1109/TMM.2013.2292592
   Kim YJ, 2013, J VISION, V13, DOI 10.1167/13.6.15
   Lebreton P, 2012, IEEE J-STSP, V6, P710, DOI 10.1109/JSTSP.2012.2213236
   Lee K, 2015, IEEE J-STSP, V9, P533, DOI 10.1109/JSTSP.2015.2393296
   Lin YH, 2014, IEEE T IMAGE PROCESS, V23, P1527, DOI 10.1109/TIP.2014.2302686
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P494, DOI 10.1016/j.image.2014.02.004
   Liu XA, 2015, MULTIMED TOOLS APPL, V74, P2803, DOI 10.1007/s11042-013-1698-z
   Maalouf A, 2011, INT CONF ACOUST SPEE, P1161
   Mather G., 2008, Foundations of sensation and perception
   Meegan DV, 2001, J EXP PSYCHOL-APPL, V7, P143, DOI 10.1037//1076-898X.7.2.143
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mittal A, 2012, IEEE SIGNAL PROC LET, V19, P75, DOI 10.1109/LSP.2011.2179293
   Moorthy A. K., 2013, SPIE HUM VIS EL IM 1
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ryu S, 2014, IEEE T CIRC SYST VID, V24, P591, DOI 10.1109/TCSVT.2013.2279971
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sazzad ZMP, 2009, INT WORK QUAL MULTIM, P180, DOI 10.1109/QOMEX.2009.5246956
   Shao F, 2016, IEEE T CYBERNETICS, V46, P730, DOI 10.1109/TCYB.2015.2414479
   Shao F, 2015, IEEE T BROADCAST, V61, P154, DOI 10.1109/TBC.2015.2402491
   Shao F, 2015, IEEE T IMAGE PROCESS, V24, P2971, DOI 10.1109/TIP.2015.2436332
   Shao F, 2015, IEEE SIGNAL PROC LET, V22, P1548, DOI 10.1109/LSP.2015.2413946
   Shao F, 2013, IEEE T IMAGE PROCESS, V22, P1940, DOI 10.1109/TIP.2013.2240003
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Shekhar S, 2014, IEEE T PATTERN ANAL, V36, P113, DOI 10.1109/TPAMI.2013.109
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Tam WJ, 2011, IEEE T BROADCAST, V57, P335, DOI 10.1109/TBC.2011.2125070
   Wang JJ, 2014, TRANS MOBIL SER, P1
   Wang XM, 2011, J BIOMED BIOTECHNOL, P1, DOI 10.1155/2011/419343
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wilcox LM, 1998, VISION RES, V38, P3671, DOI 10.1016/S0042-6989(98)00066-2
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Xing LY, 2012, IEEE T MULTIMEDIA, V14, P326, DOI 10.1109/TMM.2011.2172402
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yasakethu SLP, 2008, IEEE T CONSUM ELECTR, V54, P1969, DOI 10.1109/TCE.2008.4711260
   Zhang LJ, 2012, IEEE T IMAGE PROCESS, V21, P2379, DOI 10.1109/TIP.2012.2183879
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhou JW, 2014, J VISION, V14, DOI 10.1167/14.13.24
NR 77
TC 37
Z9 37
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 2104
EP 2114
DI 10.1109/TMM.2016.2594142
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800016
DA 2024-07-18
ER

PT J
AU Li, LD
   Wu, D
   Wu, JJ
   Li, HL
   Lin, WS
   Kot, AC
AF Li, Leida
   Wu, Dong
   Wu, Jinjian
   Li, Haoliang
   Lin, Weisi
   Kot, Alex C.
TI Image Sharpness Assessment by Sparse Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dictionary learning; image quality assessment; image sharpness;
   no-reference; sparse representation
ID QUALITY ASSESSMENT; BLUR; ARTIFACTS; ALGORITHM
AB Recent advances in sparse representation show that overcomplete dictionaries learned from natural images can capture high-level features for image analysis. Since atoms in the dictionaries are typically edge patterns and image blur is characterized by the spread of edges, an overcomplete dictionary can be used to measure the extent of blur. Motivated by this, this paper presents a no-reference sparse representation-based image sharpness index. An overcomplete dictionary is first learned using natural images. The blurred image is then represented using the dictionary in a block manner, and block energy is computed using the sparse coefficients. The sharpness score is defined as the variance-normalized energy over a set of selected high-variance blocks, which is achieved by normalizing the total block energy using the sum of block variances. The proposed method is not sensitive to training images, so a universal dictionary can be used to evaluate the sharpness of images. Experiments on six public image quality databases demonstrate the advantages of the proposed method.
C1 [Li, Leida; Wu, Dong] China Univ Min & Technol, Sch Informat & Elect Engn, Xuzhou 221116, Peoples R China.
   [Wu, Jinjian] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Li, Haoliang; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 China University of Mining & Technology; Xidian University; Nanyang
   Technological University; Nanyang Technological University
RP Li, LD; Wu, D (corresponding author), China Univ Min & Technol, Sch Informat & Elect Engn, Xuzhou 221116, Peoples R China.; Wu, JJ (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.; Li, HL; Kot, AC (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.; Lin, WS (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM reader1104@hotmail.com; starrysky_wd@163.com;
   jinjian.wu@mail.xidian.edu.cn; hli016@e.ntu.edu.sg; wslin@ntu.edu.sg;
   eackot@ntu.edu.sg
RI Wu, Jinjian/GQH-0222-2022; Li, Li/AEM-3636-2022; Lin, Weisi/A-3696-2011;
   li, li/HII-4157-2022; Lin, Weisi/A-8011-2012
OI Lin, Weisi/0000-0001-9866-1947; Li, Haoliang/0000-0002-8723-8112
FU Fundamental Research Funds for the Central Universities [2015XKMS032]
FX This work was supported by the Fundamental Research Funds for the
   Central Universities under Grant 2015XKMS032. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Maria Martini.
CR Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], IEEE SIGNAL PROC LET
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 2013, International Scholarly Research Notices., DOI DOI 10.1155/2013/905685
   [Anonymous], 2012, P 29 INT COFERENCE I
   Bahrami K., 2014, SOURCE CODES MLV IND
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Chang HW, 2013, IEEE T IMAGE PROCESS, V22, P4007, DOI 10.1109/TIP.2013.2266579
   Ding Y, 2014, ELECTRON LETT, V50, P509, DOI 10.1049/el.2013.4298
   Evans M., 1993, STAT DISTRIBUTIONS
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Guha T, 2014, SIGNAL PROCESS-IMAGE, V29, P1138, DOI 10.1016/j.image.2014.09.010
   Hassen R., 2013, SOURCE CODES LPC IND
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Li LJ, 2014, INT J COMPUT VISION, V107, P20, DOI 10.1007/s11263-013-0660-x
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu HT, 2010, IEEE T CIRC SYST VID, V20, P529, DOI 10.1109/TCSVT.2009.2035848
   Ma L, 2013, CHINA COMMUN, V10, P62, DOI 10.1109/CC.2013.6520939
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Murthy A. V., 2010, VBQUEST VISUAL BLUR
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ng, 2007, ADV NEURAL INF PROCE, P801
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pati YC, 1993, SIGN SYST COMP 1993, P40, DOI DOI 10.1109/ACSSC.1993.342465
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sang QB, 2014, J VIS COMMUN IMAGE R, V25, P1625, DOI 10.1016/j.jvcir.2014.08.002
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Virtanen T., 2013, CAMERA IMAGE DATABAS
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Vu C. T., 2012, SOURCE CODES S3 FISH
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu SQ, 2009, J VIS COMMUN IMAGE R, V20, P231, DOI 10.1016/j.jvcir.2009.03.002
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 50
TC 102
Z9 112
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1085
EP 1097
DI 10.1109/TMM.2016.2545398
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100012
DA 2024-07-18
ER

PT J
AU Qin, Z
   Ren, K
   Yu, T
   Weng, J
AF Qin, Zhan
   Ren, Kui
   Yu, Ting
   Weng, Jian
TI DPcode: Privacy-Preserving Frequent Visual Patterns Publication on Cloud
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud; data publication; differential privacy; frequent pattern mining
   (FPM)
ID RETRIEVAL; DATABASE
AB Nowadays, cloud has become a promising multimedia data processing and sharing platform. Many institutes and companies plan to outsource and share their large-scale video and image datasets on cloud for scientific research and public interest. Among various video applications, the discovery of frequent visual patterns over graphical data is an exploratory and important technique. However, the privacy concerns over the leakage of sensitive information contained in the videos/images impedes the further implementation. Although the frequent visual patterns mining (FVPM) algorithm aggregates summary over individual frames and seems not to pose privacy threat, the private information contained in individual frames still may be leaked from the statistical result. In this paper, we study the problem of privacy-preserving publishing of graphical data FVPM on cloud. We propose the first differentially private frequent visual patterns mining algorithm for graphical data, named DPcode. We propose a novel mechanism that integrates the privacy-preserving visual word conversion with the differentially private mechanism under the noise allocation strategy of the sparse vector technique. The optimized algorithms properly allocate the privacy budgets among different phases in FPM algorithm over images and reduce the corresponding data distortion. Extensive experiments are conducted based on datasets commonly used in visual mining algorithms. The results show that our approach achieves high utility while satisfying a practical privacy requirement.
C1 [Qin, Zhan; Ren, Kui] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Yu, Ting] Cyber Secur Grp, Qatar Comp Res Inst, Doha, Qatar.
   [Weng, Jian] Jinan Univ, Sch Informat Technol, Guangzhou 510632, Guangdong, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; Qatar Foundation (QF); Hamad Bin Khalifa
   University-Qatar; Qatar Computing Research Institute; Jinan University
RP Qin, Z; Ren, K (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.; Yu, T (corresponding author), Cyber Secur Grp, Qatar Comp Res Inst, Doha, Qatar.; Weng, J (corresponding author), Jinan Univ, Sch Informat Technol, Guangzhou 510632, Guangdong, Peoples R China.
EM zhanqin@buffalo.edu; kuiren@buffalo.edu; tyu@qf.org.qa;
   cryptjweng@gmail.com
RI Ren, Kui/AGE-3662-2022
OI Ren, Kui/0000-0003-3441-6277; Weng, Jian/0000-0003-4067-8230
CR Agrawal R., 1993, SIGMOD Record, V22, P207, DOI 10.1145/170036.170072
   Akgül CB, 2011, J DIGIT IMAGING, V24, P208, DOI 10.1007/s10278-010-9290-9
   Andrs Miguel E., 2013, P 2013 ACM SIGSAC C, P901, DOI DOI 10.1145/2508859.2516735
   [Anonymous], 2000, P 5 INT WORKSH DIG M
   Bhaskar R., 2010, P 16 ACM SIGKDD INT, P503, DOI DOI 10.1145/1835804.1835869
   Calders T., 2005, SIAM SDM NEWP BEACH
   Chaudhuri Kamalika, 2009, ADV NEURAL INFORM PR, P289
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1
   Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14
   Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12
   Goryczka S, 2013, P JOINT EDBT ICDT WO, P155, DOI [10.1145/2457317.2457343, 10.1145/2457317, DOI 10.1145/2457317]
   Han J., 2006, DATA MINING SE AS ED
   Han JW, 2007, DATA MIN KNOWL DISC, V15, P55, DOI 10.1007/s10618-006-0059-1
   Han JW, 2000, SIGMOD RECORD, V29, P1
   Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85
   Kantarcioˇglu Murat., 2004, Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), P599, DOI DOI 10.1145/1014052.1014126
   Ke Y, 2004, PROC CVPR IEEE, P506
   Kotz S., 2001, The Laplace Distribution and Generalizations: A Revisit with Applications to Communications, Economics, Engineering, and Finance, P22
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lee J, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P931, DOI 10.1145/2623330.2623723
   Lehmann TM, 2005, COMPUT MED IMAG GRAP, V29, P143, DOI 10.1016/j.compmedimag.2004.09.010
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li N, 2012, PROC VLDB ENDOW, V5, P1340, DOI 10.14778/2350229.2350251
   Liu J., 2006, SIAM SDM BETH MD US
   Nister David, 2006, CVPR
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Punitha P, 2006, IEEE T KNOWL DATA EN, V18, P1368, DOI 10.1109/TKDE.2006.154
   Qin Z., 2015, DPCODE MINING FREQUE
   Sabater J., 2002, Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems, P475
   Shen ET, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P545
   Stein S, 2013, UBICOMP'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING, P729, DOI 10.1145/2493432.2493482
   Sweeney L, 2002, INT J UNCERTAIN FUZZ, V10, P557, DOI 10.1142/S0218488502001648
   Turdukulov U, 2014, INT J GEOGR INF SCI, V28, P2013, DOI 10.1080/13658816.2014.889834
   van Ginneken B, 2006, MED IMAGE ANAL, V10, P19, DOI 10.1016/j.media.2005.02.002
   Washio T., 2003, ACM SIGKDD EXPLORATI, V5, P59, DOI [DOI 10.1145/959242.959249, 10.1145/959242.959249]
   Zeng C, 2012, PROC VLDB ENDOW, V6, P25, DOI 10.14778/2428536.2428539
NR 37
TC 4
Z9 4
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2016
VL 18
IS 5
SI SI
BP 929
EP 939
DI 10.1109/TMM.2016.2535729
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DK5YD
UT WOS:000374996200012
DA 2024-07-18
ER

PT J
AU Almowuena, S
   Rahman, MM
   Hsu, CH
   Hassan, AA
   Hefeeda, M
AF Almowuena, Saleh
   Rahman, Md. Mahfuzur
   Hsu, Cheng-Hsin
   Hassan, Ahmad AbdAllah
   Hefeeda, Mohamed
TI Energy-Aware and Bandwidth-Efficient Hybrid Video Streaming Over Mobile
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hybrid unicast-multicast; mobile multimedia; single frequency networks;
   video streaming
ID ADAPTIVE RESOURCE-ALLOCATION; BROADCAST MULTICAST SERVICE; SCALABLE
   VIDEO; WIRELESS; LTE; PERFORMANCE; DELIVERY; QUALITY
AB Current cellular networks support video streaming over unicast or multicast. However, there exists a tradeoff between utilizing the two: i) unicast leads to higher network load, but lower energy consumption of mobile devices, and ii) multicast results in lower network load, but higher energy consumption. To make the best out of both, we propose to concurrently utilize unicast and multicast for minimizing the energy consumption of mobile devices and minimizing the load on cellular networks. Cellular networks support two multicast schemes: i) independent cell networks and ii) multi-cell single frequency networks, where multiple adjacent base stations operate on the same frequency. We first consider the less-complicated independent cell networks, and then extend our solution to single frequency networks for better performance. We formulate the resource allocation in hybrid multicast-unicast streaming systems as a binary integer programming problem. We describe optimal algorithms for the two multicast schemes. We then propose two efficient, heuristic, algorithms that run faster and provide close to optimal results. While our solution is general, for concreteness, we conduct detailed LTE packet-level simulations using OPNET. Our simulation results show the proposed algorithms i) scale to many more mobile devices than the state-of-the-art unicast-only approaches and ii) result in lower energy consumption than the latest multicast-only approaches. In addition, the algorithms designed for multi-cell single frequency networks outperform the algorithms designed for independent cell networks in all aspects, such as service ratio, spectral efficiency, energy saving, video quality, frame loss rate, initial buffering time, and number of re-buffering events.
C1 [Almowuena, Saleh; Rahman, Md. Mahfuzur] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
   [Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30013, Taiwan.
   [Hassan, Ahmad AbdAllah] Taibah Univ, Dept Comp Sci & Informat, Al Madina 344, Saudi Arabia.
   [Hassan, Ahmad AbdAllah] Mansoura Univ, Dept Elect & Commun Engn, Mansoura 35516, Egypt.
   [Hefeeda, Mohamed] Hamad Bin Khalifa Univ, Qatar Comp Res Inst, Doha 5825, Qatar.
C3 Simon Fraser University; National Tsing Hua University; Taibah
   University; Egyptian Knowledge Bank (EKB); Mansoura University; Qatar
   Foundation (QF); Hamad Bin Khalifa University-Qatar; Qatar Computing
   Research Institute
RP Almowuena, S (corresponding author), Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
FU National Science, Technology and Innovation Plan (NSTIP) of the Kingdom
   of Saudi Arabia; Natural Sciences and Engineering Research Council
   (NSERC) of Canada
FX This work was supported in part by the National Science, Technology and
   Innovation Plan (NSTIP) of the Kingdom of Saudi Arabia, and by the
   Natural Sciences and Engineering Research Council (NSERC) of Canada.
CR 3GPP, 2010, 26903 3GPP TR
   Alexiou A, 2012, WIREL NETW, V18, P227, DOI 10.1007/s11276-011-0341-z
   Almowuena Saleh, 2015, P ACM MULT SYST C, P153
   [Anonymous], 2013, 4G: LTE/LTE-advanced for mobile broadband
   [Anonymous], 2012, LTE MOD US GUID OPNE
   [Anonymous], 2014, NOK NETW 1 TRIAL LTE
   [Anonymous], 2014, 36300 3GPP TS
   [Anonymous], 2009, PAK DEV REV, DOI DOI 10.1109/ICC.2009.5198850
   [Anonymous], 2014, CISC VIS NETW IND GL
   Araniti G, 2015, IEEE WIREL COMMUN LE, V4, P149, DOI 10.1109/LWC.2014.2387824
   Araniti G, 2014, IEEE T BROADCAST, V60, P358, DOI 10.1109/TBC.2014.2321678
   Araniti G, 2013, IEEE T BROADCAST, V59, P658, DOI 10.1109/TBC.2013.2271387
   Chen J., 2013, Proceedings of the 19th Annual International Conference on Mobile Computing Networking, P389
   Cheng X, 2008, INT WORKSH QUAL SERV, P249
   Chuah SP, 2012, IEEE T MULTIMEDIA, V14, P1324, DOI 10.1109/TMM.2012.2193560
   Cicconetti C, 2006, IEEE NETWORK, V20, P50, DOI 10.1109/MNET.2006.1607896
   Deng H, 2012, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2012-195
   Eager D, 2001, IEEE T KNOWL DATA EN, V13, P742, DOI 10.1109/69.956098
   Erman J., 2013, Proceedings of the 2013 conference on Internet measurement conference, P353
   Gruber M, 2011, IEEE COMMUN MAG, V49, P176, DOI 10.1109/MCOM.2011.6094023
   Haghani E, 2009, IEEE T MULTIMEDIA, V11, P1140, DOI 10.1109/TMM.2009.2026099
   Hartung F, 2007, IEEE T BROADCAST, V53, P188, DOI 10.1109/TBC.2007.891711
   Hefeeda M, 2010, IEEE ACM T NETWORK, V18, P610, DOI 10.1109/TNET.2009.2030326
   Hlavacs H, 2008, ACM T MULTIM COMPUT, V4, DOI 10.1145/1324287.1324295
   Hoque M., 2014, ACM T MULTIM COMPUT, V10, P1
   Hsu CH, 2011, IEEE T MOBILE COMPUT, V10, P406, DOI 10.1109/TMC.2010.173
   Hsu CH, 2010, IEEE ACM T NETWORK, V18, P681, DOI 10.1109/TNET.2009.2033058
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   Huang J., 2012, P 10 INT C MOB SYST, P225, DOI DOI 10.1145/2307636.2307658
   Kambhatla KKR, 2012, IEEE T MULTIMEDIA, V14, P1480, DOI 10.1109/TMM.2012.2196508
   Kapoor A., 2009, RECOMMENDED BIT RATE
   Khan S, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/94918
   Kim J, 2007, IEEE COMMUN LETT, V11, P486, DOI 10.1109/LCOMM.2007.070007
   Kuo WH, 2011, IEEE T MULTIMEDIA, V13, P116, DOI 10.1109/TMM.2010.2082350
   Lecompte D, 2012, IEEE COMMUN MAG, V50, P68, DOI 10.1109/MCOM.2012.6353684
   Lederer S., 2012, P 3 MULT SYST C, P89
   Lee SJ, 2011, IEEE T COMMUN, V59, P1264, DOI 10.1109/TCOMM.2011.020811.090200
   Lewis D., 2014, VERIZON DELIVERS LTE
   Luna CE, 2003, IEEE J SEL AREA COMM, V21, P1710, DOI 10.1109/JSAC.2003.815394
   Monserrat JF, 2012, IEEE T BROADCAST, V58, P157, DOI 10.1109/TBC.2012.2191030
   Rahman M.H., 2014, INT MECH ENG C EXP, P1, DOI [10.1115/IMECE2014-36484, DOI 10.1115/IMECE2014-36484]
   Rong L., 2008, Proceedings of the IEEE Global Telecommunications Conference, P1, DOI [10.1109/GLOCOM.2008.ECP.459 rsity, DOI 10.1109/GLOCOM.2008.ECP.459RSITY]
   Samberg D., 2014, CUSTOMERS USE 1 9 TE
   Schulman A, 2010, MOBICOM 10 & MOBIHOC 10: PROCEEDINGS OF THE 16TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING AND THE 11TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P85
   Seeling P, 2012, IEEE COMMUN SURV TUT, V14, P1142, DOI 10.1109/SURV.2011.082911.00067
   Sharangi S, 2011, IEEE T MULTIMEDIA, V13, P102, DOI 10.1109/TMM.2010.2076799
   Singhal C, 2014, IEEE T MOBILE COMPUT, V13, P1522, DOI 10.1109/TMC.2013.138
   Suh CH, 2008, IEEE T WIREL COMMUN, V7, P27, DOI 10.1109/TWC.2008.060467
   Talarico Salvatore, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P6994, DOI 10.1109/ICASSP.2014.6854956
   Urie A, 2013, BELL LABS TECH J, V18, P57, DOI 10.1002/bltj.21605
   Wang S., 2013, PROC OF ACM INTERNAT, P525
   Won H, 2009, IEEE T WIREL COMMUN, V8, P4540, DOI 10.1109/TWC.2009.080330
   Xu J, 2010, IEEE T BROADCAST, V56, P98, DOI 10.1109/TBC.2009.2039691
   Ye Zhao, 2013, Middleware 2013. ACM/IFIP/USENIX 14th International Middleware Conference. Proceedings: LNCS 8275, P445, DOI 10.1007/978-3-642-45065-5_23
   Yu YJ, 2012, IEEE T MOBILE COMPUT, V11, P1508, DOI 10.1109/TMC.2011.186
   Zaki Yasir, 2011, OPNET WORKSH, P1
   Zhu H, 2005, IEEE T MOBILE COMPUT, V4, P391, DOI 10.1109/TMC.2005.58
   Zorzi M, 1997, IEEE PERS COMMUN, V4, P27, DOI 10.1109/98.637380
NR 59
TC 34
Z9 40
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 102
EP 115
DI 10.1109/TMM.2015.2502067
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700010
DA 2024-07-18
ER

PT J
AU Zahálka, J
   Rudinac, S
   Worring, M
AF Zahalka, Jan
   Rudinac, Stevan
   Worring, Marcel
TI Interactive Multimodal Learning for Venue Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep nets; interactive city exploration; location-based social networks;
   semantic concept detectors; topic models; user-centered design
AB In this paper, we propose City Melange, an interactive and multimodal content-based venue explorer. Our framework matches the interacting user to the users of social media platforms exhibiting similar taste. The data collection integrates location-based social networks such as Foursquare with general multimedia sharing platforms such as Flickr or Picasa. In City Melange, the user interacts with a set of images and thus implicitly with the underlying semantics. The semantic information is captured through convolutional deep net features in the visual domain and latent topics extracted using Latent Dirichlet allocation in the text domain. These are further clustered to provide representative user and venue topics. A linear SVM model learns the interacting user's preferences and determines similar users. The experiments show that our content-based approach outperforms the user-activity-based and popular vote baselines even from the early phases of interaction, while also being able to recommend mainstream venues to mainstream users and off-the-beaten-track venues to afficionados. City Melange is shown to be a well-performing venue exploration approach.
C1 [Zahalka, Jan; Rudinac, Stevan; Worring, Marcel] Univ Amsterdam, Inst Informat, NL-1098 Amsterdam, Netherlands.
C3 University of Amsterdam
RP Zahálka, J (corresponding author), Univ Amsterdam, NL-1098 Amsterdam, Netherlands.
EM j.zahalka@uva.nl; s.rudinac@uva.nl; m.worring@uva.nl
RI Worring, Marcel/JRW-7059-2023; Zahálka, Jan/AAR-5242-2020
OI Worring, Marcel/0000-0003-4097-4136; Zahalka, Jan/0000-0002-6743-3607
FU Dutch Technology Foundation STW; Netherlands Organisation for Scientific
   Research (NWO) - Ministry of Economic Affairs
FX This work was supported in part by the Dutch Technology Foundation STW,
   Netherlands Organisation for Scientific Research (NWO), which is funded
   in part by the Ministry of Economic Affairs. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Tao Mei.
CR Adomavicius G, 2011, RECOMMENDER SYSTEMS HANDBOOK, P217, DOI 10.1007/978-0-387-85820-3_7
   [Anonymous], ACM TIST
   [Anonymous], WHAT IS JUST YOU FEA
   [Anonymous], 2013, PROC THE 21 ACM INT
   [Anonymous], ACM TOMMCAP
   [Anonymous], TAPPING BIG DATA MAK
   [Anonymous], 2011, P 3 ACM SIGMM INT WO
   [Anonymous], ACM TIST
   Bird Steven, 2009, NATURAL LANGUAGE PRO, DOI DOI 10.1007/S10579-010-9124-X
   Cheng A.-J., 2011, P 19 ACM INT C MULTI, P83
   Cremonesi P., 2010, P 4 ACM C REC SYST, P39, DOI [DOI 10.1145/1864708.1864721, 10.1145/1864708.1864721]
   Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]
   Gantner Z., 2011, P 5 ACM C REC SYST, P305, DOI DOI 10.1145/2043932.2043989
   Hoffman M., 2010, P ADV NEUR INF PROC, V23:856-864
   Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22
   Jannach Dietmar, 2013, User Modeling, Adaptation, and Personalization. 21th International Conference, UMAP 2013. Proceedings., P25, DOI 10.1007/978-3-642-38844-6_3
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurashima Takeshi., 2013, P 6 ACM INT C WEB SE, P375
   Levandoski JJ, 2012, PROC INT CONF DATA, P450, DOI 10.1109/ICDE.2012.54
   Lian DF, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P831, DOI 10.1145/2623330.2623638
   Liu B, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1043
   Liu H., 2012, Proceedings_of_the_20th_ACM_ International_Conference_on_Multimedia, MM'12, P9
   Noulas A, 2012, PROCEEDINGS OF 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON PRIVACY, SECURITY, RISK AND TRUST AND 2012 ASE/IEEE INTERNATIONAL CONFERENCE ON SOCIAL COMPUTING (SOCIALCOM/PASSAT 2012), P144, DOI 10.1109/SocialCom-PASSAT.2012.70
   Pan R, 2008, IEEE DATA MINING, P502, DOI 10.1109/ICDM.2008.16
   Pang YW, 2011, COMPUT VIS IMAGE UND, V115, P352, DOI 10.1016/j.cviu.2010.10.010
   Popescu A., 2009, ACM International Conference on Information and Knowledge Management, P1713, DOI DOI 10.1145/1645953.1646211
   Quercia D., 2014, P 25 ACM C HYPERTEXT, P116, DOI [DOI 10.1145/2631775.2631799, 10/gfvsn2, 10 .1145/2631775.2631799]
   Rehurek R., 2010, LREC, DOI DOI 10.13140/2.1.2393.1847
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi Y, 2014, ACM COMPUT SURV, V47, DOI 10.1145/2556270
   Steck H., 2011, P 5 ACM C REC SYST, P125
   Yin HZ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P221
   Zahálka J, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P205, DOI 10.1145/2647868.2656403
NR 35
TC 16
Z9 16
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2235
EP 2244
DI 10.1109/TMM.2015.2480007
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500011
DA 2024-07-18
ER

PT J
AU Hasan, M
   Roy-Chowdhury, AK
AF Hasan, Mahmudul
   Roy-Chowdhury, Amit K.
TI A Continuous Learning Framework for Activity Recognition Using Deep
   Hybrid Feature Models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Active learning; activity recognition; autoencoder; hybrid feature
   model; incremental learning
ID HISTOGRAMS
AB Most of the research on human activity recognition has focused on learning a static model, considering that all the training instances are labeled and present in advance, while in streaming videos new instances continuously arrive and are not labeled. Moreover, these methods generally use application-specific hand-engineered and static feature models, which are not suitable for continuous learning. Some recent approaches on activity recognition use deep-learning-based hierarchical feature models, but the large size of these networks constrain them from being used in continuous learning scenarios. In this work, we propose a continuous activity learning framework for streaming videos by intricately tying together deep hybrid feature models and active learning. This allows us to automatically select the most suitable features and take the advantage of incoming unlabeled instances to improve the existing model incrementally. Given the segmented activities from streaming videos, we learn features in an unsupervised manner using deep hybrid networks, which have the ability to take the advantage of both the local hand-engineered features and the deep model in an efficient way. Additionally, we use active learning to train the activity classifier using a reduced amount of manually labeled instances. Retraining the models with a huge amount of accumulated examples is computationally expensive and not suitable for continuous learning. Hence, we propose a method to select the best subset of these examples to update the models incrementally. We conduct rigorous experiments on four challenging human activity datasets to demonstrate the effectiveness of our framework.
C1 [Hasan, Mahmudul] Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA.
   [Roy-Chowdhury, Amit K.] Univ Calif Riverside, Dept Elect & Comp Engn, Riverside, CA 92521 USA.
C3 University of California System; University of California Riverside;
   University of California System; University of California Riverside
RP Hasan, M (corresponding author), Univ Calif Riverside, Dept Comp Sci & Engn, Riverside, CA 92521 USA.
EM mhasa004@ucr.edu; amitrc@ece.ucr.edu
OI Roy-Chowdhury, Amit/0000-0001-6690-9725
FU NSF [IIS-1316934]; ONR [N00014-12-1-1026]; Google; Direct For Computer &
   Info Scie & Enginr [1316934] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems [1316934] Funding
   Source: National Science Foundation
FX This work was supported in part by the NSF under Grant IIS-1316934, by
   the ONR Grant N00014-12-1-1026, and by Google. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Hugo Larochelle.
CR Albanese M, 2008, IEEE T MULTIMEDIA, V10, P982, DOI 10.1109/TMM.2008.2001369
   Angelova A, 2005, PROC CVPR IEEE, P494
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], 1987, CLUSTERING MEANS MED
   [Anonymous], 2011, P ICML
   [Anonymous], 2007, CIVR '07
   [Anonymous], CORR
   [Anonymous], 2012, Synthesis Lectures on Artificial Intelligence and Machine Learning
   [Anonymous], 2012, P TRECVID
   [Anonymous], 2009, P BRIT MACH VIS C BM, DOI DOI 10.5244/C.23.124
   [Anonymous], 2008, P IEEE INT C COMP VI
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5
   Chaudhry R, 2009, PROC CVPR IEEE, P1932, DOI 10.1109/CVPRW.2009.5206821
   Coates A., 2011, P ADV NEUR INF PROC, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Druck B., 2009, P C EMP METH NAT LAN, P81, DOI DOI 10.3115/1699510.1699522
   Elhamifar E, 2012, PROC CVPR IEEE, P1600, DOI 10.1109/CVPR.2012.6247852
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb P., 2011, Discriminatively trained deformable part models, release 4
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Hasan M, 2014, PROC CVPR IEEE, P796, DOI 10.1109/CVPR.2014.107
   Hasan M, 2014, LECT NOTES COMPUT SC, V8691, P705, DOI 10.1007/978-3-319-10578-9_46
   He HB, 2011, IEEE T NEURAL NETWOR, V22, P1901, DOI 10.1109/TNN.2011.2171713
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Le Q. V., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3361, DOI 10.1109/CVPR.2011.5995496
   Lee H., 2007, NIPS
   Li X, 2013, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2013.116
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Loy CC, 2012, PROC CVPR IEEE, P1560, DOI 10.1109/CVPR.2012.6247847
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Minhas R, 2012, IEEE T CIRC SYST VID, V22, P1529, DOI 10.1109/TCSVT.2011.2177182
   Nayak NM, 2013, IEEE T INF FOREN SEC, V8, P1610, DOI 10.1109/TIFS.2013.2277669
   Oh SM, 2011, PROC CVPR IEEE
   Polikar R, 2001, IEEE T SYST MAN CY C, V31, P497, DOI 10.1109/5326.983933
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Reddy KK, 2013, MACH VISION APPL, V24, P971, DOI 10.1007/s00138-012-0450-4
   Reddy KK, 2009, IEEE I CONF COMP VIS, P1010, DOI 10.1109/ICCV.2009.5459374
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Salakhutdinov R., 2009, AISTATS
   Sarawagi S., 2004, ADV NEURAL INFORM PR, V17, P1185
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Settles B, 2008, ADV NEURAL INFORM PR, V21, P1289
   Solmaz B, 2013, MACH VISION APPL, V24, P1473, DOI 10.1007/s00138-012-0449-x
   Song B, 2010, LECT NOTES COMPUT SC, V6311, P605, DOI 10.1007/978-3-642-15549-9_44
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Vezhnevets A, 2012, PROC CVPR IEEE, P3162, DOI 10.1109/CVPR.2012.6248050
   Vijayanarasimhan S, 2010, PROC CVPR IEEE, P3035, DOI 10.1109/CVPR.2010.5540055
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wilson DR, 2003, NEURAL NETWORKS, V16, P1429, DOI 10.1016/S0893-6080(03)00138-2
   Wongun Choi, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3273, DOI 10.1109/CVPR.2011.5995707
   Xianghang Liu, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3253, DOI 10.1109/ICIP.2011.6116363
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zhang C, 2002, IEEE T MULTIMEDIA, V4, P260, DOI 10.1109/TMM.2002.1017738
   Zhu YY, 2013, PROC CVPR IEEE, P2491, DOI 10.1109/CVPR.2013.322
   Zivkovic Z, 2004, INT C PATT RECOG, P28, DOI 10.1109/ICPR.2004.1333992
NR 62
TC 62
Z9 67
U1 1
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1909
EP 1922
DI 10.1109/TMM.2015.2477242
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400005
DA 2024-07-18
ER

PT J
AU Comminiello, D
   Cecchi, S
   Scarpiniti, M
   Gasparini, M
   Romoli, L
   Piazza, F
   Uncini, A
AF Comminiello, Danilo
   Cecchi, Stefania
   Scarpiniti, Michele
   Gasparini, Michele
   Romoli, Laura
   Piazza, Francesco
   Uncini, Aurelio
TI Intelligent Acoustic Interfaces With Multisensor Acquisition for
   Immersive Reproduction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Immersive communication; intelligent acoustic interface; kinect sensor;
   multichannel reproduction; multimodal interaction; noise reduction
ID COMBINATION; REDUCTION; DESIGN
AB Immersive speech communication systems have been gaining increasing attention due to their ability to reproduce enhanced acoustic images, and thus achieving good performance in terms of sound quality and accuracy. In this context, a fundamental role is played by intelligent acoustic interfaces (IAIs), which aim at acquiring and/or reproducing desired acoustic information with enhanced perception. The recent widespread availability of multimedia devices, equipped with different kind of sensors, has broadened the range of data processing methods, thus giving a chance for developing advanced IAIs. In this paper, we propose an immersive communication system composed of two IAIs: the first one exploits microphones and cameras, together with a signal processing system, to reduce unwanted noise and enhance the speech quality of the desired information in the transmitting room; the second one is an advanced reproduction system based on a loudspeaker array and on an effective wave field synthesis technique capable of reproducing the spatial perception of the desired speech source in the receiving room. The whole system has been assessed in simulated and real immersive communication scenarios: objective and subjective evaluations have been shown the effectiveness of the proposed system.
C1 [Comminiello, Danilo; Scarpiniti, Michele; Uncini, Aurelio] Univ Roma La Sapienza, Dept Informat Engn Elect & Telecommun, I-00184 Rome, Italy.
   [Cecchi, Stefania; Gasparini, Michele; Romoli, Laura; Piazza, Francesco] Univ Politecn Marche, Dept Informat Engn, I-60131 Ancona, Italy.
C3 Sapienza University Rome; Marche Polytechnic University
RP Comminiello, D (corresponding author), Univ Roma La Sapienza, Dept Informat Engn Elect & Telecommun, I-00184 Rome, Italy.
EM danilo.comminiello@uniroma1.it; s.cecchi@univpm.it
RI Comminiello, Danilo/AAA-6026-2020; Scarpiniti, Michele/K-5383-2015;
   Cecchi, Stefania/M-4604-2013
OI Comminiello, Danilo/0000-0003-4067-4504; Scarpiniti,
   Michele/0000-0002-3164-6256; Cecchi, Stefania/0000-0002-9625-814X;
   Uncini, Aurelio/0000-0002-5793-0917
FU bdSound
FX The work of D. Comminiello was supported in part by bdSound. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Yonggang Wen.
CR Ahrens J., 2007, IEEE WORKSHOP APPL S, P66
   Ahrens J., 2008, P 124 AUD ENG SOC CO
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Apostolopoulos JG, 2012, P IEEE, V100, P974, DOI 10.1109/JPROC.2011.2182069
   Arenas-García J, 2006, IEEE T SIGNAL PROCES, V54, P1078, DOI 10.1109/TSP.2005.863126
   Arenas-García J, 2006, SIGNAL PROCESS, V86, P2430, DOI 10.1016/j.sigpro.2005.11.008
   BERKHOUT AJ, 1993, J ACOUST SOC AM, V93, P2764, DOI 10.1121/1.405852
   Brandstein M., 2001, MICROPHONE ARRAYS SI
   Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Comminiello D, 2014, IEEE IJCNN, P3577, DOI 10.1109/IJCNN.2014.6889598
   Comminiello D, 2013, SIGNAL PROCESS, V93, P3306, DOI 10.1016/j.sigpro.2013.05.014
   Comminiello D, 2010, IEEE INT SYMP CIRC S, P2127, DOI 10.1109/ISCAS.2010.5536944
   DeVries D, 1996, J AUDIO ENG SOC, V44, P1120
   Franck A., 2008, P 125 AUD ENG SOC CO
   Franck A., 2007, P 32 AUD ENG SOC C H
   FROST OL, 1972, PR INST ELECTR ELECT, V60, P926, DOI 10.1109/PROC.1972.8817
   Fu JJ, 2013, IEEE T MULTIMEDIA, V15, P1340, DOI 10.1109/TMM.2013.2247584
   Gasparini M., 2011, P 130 AUD ENG SOC CO
   Habets EAP, 2013, IEEE T AUDIO SPEECH, V21, P945, DOI 10.1109/TASL.2013.2239292
   Hefley W.E., 1993, P 1 INT C INTELLIGEN, P3
   Hori T, 2012, IEEE T AUDIO SPEECH, V20, P499, DOI 10.1109/TASL.2011.2164527
   Huang YT, 2011, IEEE SIGNAL PROC MAG, V28, P20, DOI 10.1109/MSP.2010.938754
   *ITU, 2003, BS1284 ITUR
   ITU, 1997, ITU-R BS.1116-1 Recommendation
   John P.W.M., 1998, Statistical Design and Analysis of Experiments
   Koivuniemi K., 2001, P 111 AUD ENG SOC CO
   Lattanzi A., 2008, P 124 AUDIO ENG SOC, P1
   Lázaro-Gredilla M, 2010, IEEE T SIGNAL PROCES, V58, P3890, DOI 10.1109/TSP.2010.2047501
   Liu QJ, 2014, IEEE T MULTIMEDIA, V16, P1610, DOI 10.1109/TMM.2014.2322824
   Markovich-Golan S, 2015, SIGNAL PROCESS, V107, P4, DOI 10.1016/j.sigpro.2014.07.014
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Morse P. M., 1986, Theoretical Acoustics
   Mouchtaris A, 2000, IEEE T MULTIMEDIA, V2, P77, DOI 10.1109/6046.845012
   Peled Y, 2013, IEEE T AUDIO SPEECH, V21, P2532, DOI 10.1109/TASL.2013.2277939
   Peretti P, 2010, EUR SIGNAL PR CONF, P1939
   Quackenbush S.R., 1988, Objective Measures of Speech Quality
   Reindl Klaus, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P283, DOI 10.1109/ChinaSIP.2013.6625345
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Ruan H, 2014, IEEE SIGNAL PROC LET, V21, P60, DOI 10.1109/LSP.2013.2290948
   Theodoropoulos D, 2011, IEEE T MULTIMEDIA, V13, P235, DOI 10.1109/TMM.2010.2098397
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
   Zhao H, 2006, IEEE T CIRCUITS-II, V53, P157, DOI 10.1109/TCSII.2005.856673
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 58
TC 10
Z9 10
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1262
EP 1272
DI 10.1109/TMM.2015.2442151
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000012
DA 2024-07-18
ER

PT J
AU Shi, CJA
   Ruan, QQ
   An, GY
   Zhao, RZ
AF Shi, Caijuan
   Ruan, Qiuiqi
   An, Gaoyun
   Zhao, Ruizhen
TI Hessian Semi-Supervised Sparse Feature Selection Based on
   <i>L</i><sub>2,1/2</sub>-Matrix Norm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hessian regularization; l(2,1/2)-matrix norm; semi-supervised learning;
   sparse feature selection; web image annotation
ID IMAGE ANNOTATION; REGULARIZATION
AB Semi-supervised sparse feature selection, which can exploit the small number labeled data and large number unlabeled data simultaneously, has become an important technique in many applications on large-scale web image owing to its high efficiency and effectiveness. Recently, graph Laplacian-based semi-supervised sparse feature selection has obtained considerable attention, but it suffers with only few labeled data because Laplacian regularization is short of extrapolating power. In this paper we propose a novel semi-supervised sparse feature selection framework based on Hessian regularization and l(2,1/2)-matrix norm, namely Hessian sparse feature selection based on L-2,L-1/2-matrix norm (HFSL). Hessian regularization favors functions whose values vary linearly with respect to geodesic distance and preserves the local manifold structure well, leading to good extrapolating power to boost semi-supervised learning, and then to enhance HFSL performance. The l(2,1/2)-matrix norm model makes HFSL select the most discriminative sparse features with good robustness. An efficient iterative algorithm is designed to optimize the objective function. We apply our algorithm into the image annotation task and conduct extensive experiments on two web image datasets. The results demonstrate that our algorithm outperforms state-of-the-art sparse feature selection methods and is promising for large-scale web image applications.
C1 [Shi, Caijuan; Ruan, Qiuiqi; An, Gaoyun; Zhao, Ruizhen] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Shi, Caijuan] Hebei United Univ, Coll Informat Engn, Tangshan 063009, Peoples R China.
   [Shi, Caijuan; Ruan, Qiuiqi; An, Gaoyun; Zhao, Ruizhen] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
C3 Beijing Jiaotong University; North China University of Science &
   Technology; Beijing Jiaotong University
RP Shi, CJA (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM shicaijuan2011@gmail.com; qqruan@center.njtu.edu.cn
FU National Key Basic Research Program of China [2012CB316304]; New Century
   Excellent Talents in University [NCET-12-0768]; Youth Foundation Project
   of Hebei Colleges and University Scientific and Technology Research
   [QN2014026]; National Natural Science Foundation of China [61172128];
   Fundamental Research Funds for the Central Universities [2013JBM020,
   2013JBZ003]; Program for Innovative Research Team in University of
   Ministry of Education of China [IRT201206]; Beijing Higher Education
   Young Elite Teacher Project [YETP0544]; Research Fund for the Doctoral
   Program of Higher Education of China [20120009110008, 20120009120009]
FX This work was supported in part by the National Key Basic Research
   Program of China under Grant 2012CB316304, the New Century Excellent
   Talents in University under Grant NCET-12-0768, the Youth Foundation
   Project of Hebei Colleges and University Scientific and Technology
   Research under Grant QN2014026, the National Natural Science Foundation
   of China under Grant 61172128, the Fundamental Research Funds for the
   Central Universities under Grants 2013JBM020 and 2013JBZ003, the Program
   for Innovative Research Team in University of Ministry of Education of
   China under Grant IRT201206, the Beijing Higher Education Young Elite
   Teacher Project under Grant YETP0544, and the Research Fund for the
   Doctoral Program of Higher Education of China under Grants
   20120009110008 and 20120009120009. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Tommaso Melodia.
CR [Anonymous], 2009, ICIVR
   [Anonymous], P ICWMNN
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Cai D., 2010, KDD, P333
   Cawley G.C., 2006, Advances in Neural Information Processing Systems, P209
   Chartrand R, 2007, IEEE SIGNAL PROC LET, V14, P707, DOI 10.1109/LSP.2007.898300
   Chartrand R, 2009, I S BIOMED IMAGING, P262, DOI 10.1109/ISBI.2009.5193034
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Eells J., 1983, Selected Topics in Harmonic Maps, V50, DOI [10.1090/cbms/050, DOI 10.1090/CBMS/050]
   Foucart S, 2009, APPL COMPUT HARMON A, V26, P395, DOI 10.1016/j.acha.2008.09.001
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Kim F., 2009, P 22 INT C NEUR INF, P979
   Kononenko I., 1994, Machine Learning: ECML-94. European Conference on Machine Learning. Proceedings, P171
   Lafferty J. D., 2003, P INT C MACH LEARN, P912, DOI DOI 10.5555/3041838.3041953
   Li H, 2009, INT CONF DAT MIN WOR, P164, DOI 10.1109/ICDMW.2009.46
   Li Z., 2012, P AAAI C ART INT, P1026
   Liu HW, 2009, PATTERN RECOGN, V42, P1330, DOI 10.1016/j.patcog.2008.10.028
   Liu WF, 2014, COMPUT VIS IMAGE UND, V118, P50, DOI 10.1016/j.cviu.2013.03.007
   Liu WF, 2013, IEEE T IMAGE PROCESS, V22, P2676, DOI 10.1109/TIP.2013.2255302
   Lu K, 2011, PATTERN RECOGN, V44, P1155, DOI 10.1016/j.patcog.2010.11.009
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1021, DOI 10.1109/TMM.2012.2187179
   Sotoca JM, 2010, PATTERN RECOGN, V43, P2068, DOI 10.1016/j.patcog.2009.12.013
   Nguyen C, 2013, P 23 INT JOINT C ART, P1558
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Nie FP, 2010, IEEE T IMAGE PROCESS, V19, P1921, DOI 10.1109/TIP.2010.2044958
   Song Caifeng, 2013, P 5 INT C INT MULT C, P264
   Steinke Florian, 2008, ADV NEURAL INFORM PR, V21
   Tang J., 2012, P 18 ACM SIGKDD INT, P904, DOI DOI 10.1145/2339530.2339673
   Tao DP, 2013, IEEE T MULTIMEDIA, V15, P833, DOI 10.1109/TMM.2013.2238909
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Wang L. P., 2013, CORR, Vabs/1303.3987
   Xu ZL, 2010, IEEE T NEURAL NETWOR, V21, P1033, DOI 10.1109/TNN.2010.2047114
   Xu ZB, 2012, IEEE T NEUR NET LEAR, V23, P1013, DOI 10.1109/TNNLS.2012.2197412
   Xu ZB, 2010, SCI CHINA INFORM SCI, V53, P1159, DOI 10.1007/s11432-010-0090-0
   Yun Liu, 2010, Proceedings of the 2010 International Conference on Computer and Information Application (ICCIA 2010), P293, DOI 10.1109/ICCIA.2010.6141595
   Zheng M, 2014, NEUROCOMPUTING, V123, P247, DOI 10.1016/j.neucom.2013.08.001
   Zhu X, 2003, ICML
   Zhu Xiaojin, 2007, 1530 U WISC
NR 41
TC 39
Z9 41
U1 3
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2015
VL 17
IS 1
BP 16
EP 28
DI 10.1109/TMM.2014.2375792
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AX6QW
UT WOS:000347047400003
DA 2024-07-18
ER

PT J
AU Mazloom, M
   Gavves, E
   Snoek, CGM
AF Mazloom, Masoud
   Gavves, Efstratios
   Snoek, Cees G. M.
TI <i>Conceptlets</i>: Selective Semantics for Classifying Video Events
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Concept detection; cross-entropy optimization; event recognition
ID RECOGNITION; INFORMATION; DETECTORS; ONTOLOGY; MODEL; GAP
AB An emerging trend in video event classification is to learn an event from a bank of concept detector scores. Different from existing work, which simply relies on a bank containing all available detectors, we propose in this paper an algorithm that learns from examples what concepts in a bank are most informative per event, which we call the conceptlet. We model finding the conceptlet out of a large set of concept detectors as an importance sampling problem. Our proposed approximate algorithm finds the optimal conceptlet using a cross-entropy optimization. We study the behavior of video event classification based on conceptlets by performing four experiments on challenging internet video from the 2010 and 2012 TRECVID multimedia event detection tasks and Columbia's consumer video dataset. Starting from a concept bank of more than thousand precomputed detectors, our experiments establish there are (sets of) individual concept detectors that are more discriminative and appear to be more descriptive for a particular event than others, event classification using an automatically obtained conceptlet is more robust than using all available concepts, and conceptlets obtained with our cross-entropy algorithm are better than conceptlets from state-of-the-art feature selection algorithms. What is more, the conceptlets make sense for the events of interest, without being programmed to do so.
C1 [Mazloom, Masoud; Gavves, Efstratios; Snoek, Cees G. M.] Univ Amsterdam, Inst Informat, Intelligent Syst Lab, NL-1098 XH Amsterdam, Netherlands.
C3 University of Amsterdam
RP Mazloom, M (corresponding author), Univ Amsterdam, Inst Informat, Intelligent Syst Lab, NL-1098 XH Amsterdam, Netherlands.
EM m.mazloom@uva.nl; efstratios.gavves@gmail.com; cgmsnoek@uva.nl
RI Gavves, Efstratios/AAA-6992-2019
OI Gavves, Efstratios/0000-0001-8947-1332
FU STW STORY project; Dutch national program COMMIT; Intelligence Advanced
   Research Projects Activity (IARPA) via Department of Interior National
   Business Center [D11PC20067]
FX This work was supported in part by the STW STORY project, the Dutch
   national program COMMIT, and by the Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior National Business
   Center under Contract D11PC20067. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Chong- Wah Ngo.
CR [Anonymous], Imagenet: A large-scale hierarchical image database
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], 2009, P CVPR
   [Anonymous], 2010, P NIPS
   [Anonymous], 2004, Introduction to Rare Event Simulation
   [Anonymous], CVIU
   [Anonymous], P NIST TRECVID WORKS
   [Anonymous], MM
   [Anonymous], P ICMR
   [Anonymous], 2007, P CIVR
   Ballan L, 2011, MULTIMED TOOLS APPL, V51, P279, DOI 10.1007/s11042-010-0643-7
   Ballan L, 2010, MULTIMED TOOLS APPL, V48, P69, DOI 10.1007/s11042-009-0351-3
   Bankert R.L., 1995, P 5 INT WORKSH ART I, P199
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Ebadollahi S., 2006, P ICME
   Gkalelis N., 2011, P CBMI
   Guyon I., 2003, Journal of Machine Learning Research, V3, P1157, DOI 10.1162/153244303322753616
   Haering N, 2000, IEEE T CIRC SYST VID, V10, P857, DOI 10.1109/76.867923
   Hall M. A., 1999, Proceedings of the Twelfth International Florida AI Research Society Conference, P235
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Huurnink B., 2008, P MIR
   Inoue N., 2011, P NIST TRECVID WORKS
   Inza I, 2000, ARTIF INTELL, V123, P157, DOI 10.1016/S0004-3702(00)00052-7
   Ivanov YA, 2000, IEEE T PATTERN ANAL, V22, P852, DOI 10.1109/34.868686
   Jiang Y.-G., 2012, P ICMR
   Jiang YG, 2013, INT J MULTIMED INF R, V2, P73, DOI 10.1007/s13735-012-0024-2
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Lavee G, 2009, IEEE T SYST MAN CY C, V39, P489, DOI 10.1109/TSMCC.2009.2023380
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li Xirong., 2011, Proceedings of the 19th ACM International Conference on Multimedia, MM '11, P233
   Lin DH, 2006, LECT NOTES COMPUT SC, V3951, P68
   Liu H, 2008, CH CRC DATA MIN KNOW, P3
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Ma Z., 2013, THESIS U TRENTO TREN
   Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339
   Mannor S., 2005, P ICML
   Mazloom M., 2013, P ICMR
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Meyer PE, 2008, IEEE J-STSP, V2, P261, DOI 10.1109/JSTSP.2008.923858
   Myers G. K., 2014, MACH VIS APPL, V25
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Natarajan P, 2012, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2012.6247814
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Neo Shi-Yong, 2006, P CIVR
   Ng A. Y., 2004, P ICML
   Oh S., 2014, MACH VIS APPL, V25
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Rasiwasia N, 2007, IEEE T MULTIMEDIA, V9, P923, DOI 10.1109/TMM.2007.900138
   Rubinstein R. Y., 2004, The cross-entropy method: A unified approach to combinatorial optimization, monte-carlo simulation and machine learning
   Rudinac S, 2012, INT J MULTIMED INF R, V1, P263, DOI 10.1007/s13735-012-0018-0
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Saeys Y, 2007, BIOINFORMATICS, V23, P2507, DOI 10.1093/bioinformatics/btm344
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   SANCHEZ J, 2013, P IJCV, V105, P222
   Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4
   Siedlecki W., 1988, P INT J PATT REC
   Smeaton A. F., 2006, P MIR
   Snoek Cees G. M., 2008, Foundations and Trends in Information Retrieval, V2, P215, DOI 10.1561/1500000014
   Snoek CGM, 2007, IEEE T MULTIMEDIA, V9, P975, DOI 10.1109/TMM.2007.900156
   Tamrakar A, 2012, PROC CVPR IEEE, P3681, DOI 10.1109/CVPR.2012.6248114
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Torresani L, 2010, LECT NOTES COMPUT SC, V6311, P776, DOI 10.1007/978-3-642-15549-9_56
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Wei XY, 2008, IEEE T MULTIMEDIA, V10, P1085, DOI 10.1109/TMM.2008.2001382
   Xie LX, 2008, P IEEE, V96, P623, DOI 10.1109/JPROC.2008.916362
   Xu D, 2008, IEEE T PATTERN ANAL, V30, P1985, DOI 10.1109/TPAMI.2008.129
   Younessian E., 2012, P ICMR
NR 68
TC 28
Z9 29
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2214
EP 2228
DI 10.1109/TMM.2014.2359771
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300012
DA 2024-07-18
ER

PT J
AU Chiu, CY
   Tsai, TH
   Liou, YC
   Han, GW
   Chang, HS
AF Chiu, Chih-Yi
   Tsai, Tsung-Han
   Liou, Yu-Cyuan
   Han, Guei-Wun
   Chang, Hung-Shuo
TI Near-Duplicate Subsequence Matching Between the Continuous Stream and
   Large Video Dataset
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based retrieval; near-duplicate detection; video copy detection
ID COPY-DETECTION
AB In this paper, we study the subsequence matching problem of near-duplicate video detection. In particular, we address the application of monitoring a continuous stream with a large video dataset. To achieve real-time response and high accuracy, we propose a novel framework containing two characteristics. First, the subsequence matching is transformed to a 2-D Hough space projection of pairwise frame similarities between two subsequences. We present an approximate Hough transform that replaces the 2-D Hough space with a 1-D Hough space. The near-duplicate subsequence detection can be deemed to be the voting and searching in the 1-D Hough space with a lower time complexity. Second, a coarse-to-fine matching strategy is incorporated in the proposed framework. The coarse level matching selects the candidate videos based on the time-decay hit frequency between the query stream and dataset videos. The fine level matching applies the approximate Hough transform to detect the near-duplicate subsequences coexisting within the query stream and candidate videos. Several state-of-the-art methods are implemented for comparison. Experimental results show our framework outperforms in terms of accuracy and efficiency.
C1 [Chiu, Chih-Yi; Tsai, Tsung-Han; Liou, Yu-Cyuan; Han, Guei-Wun; Chang, Hung-Shuo] Natl Chiayi Univ, Dept Comp Sci & Informat Engn, Chiayi 60004, Taiwan.
C3 National Chiayi University
RP Chiu, CY (corresponding author), Natl Chiayi Univ, Dept Comp Sci & Informat Engn, Chiayi 60004, Taiwan.
EM cychiu@mail.ncyu.edu.tw; s0962575@mail.ncyu.edu.tw;
   s0992976@mail.ncyu.edu.tw; s1020435@mail.ncyu.edu.tw;
   s1010425@mail.ncyu.edu.tw
RI Chiu, Chih-Yi/AAN-2961-2020
OI Chiu, Chih-Yi/0000-0002-2859-6120
FU National Science Council of Taiwan [MOST 103-2221-E-415-009-MY3]
FX This work was supported in part by the National Science Council of
   Taiwan under Grant MOST 103-2221-E-415-009-MY3. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang.
CR Agrawal R., 1993, P INT C FDN DAT ORG
   Cai Y, 2013, IEEE MULTIMEDIA, V20, P42, DOI 10.1109/MMUL.2013.23
   Chen YG, 2007, PROC INT CONF DATA, P761
   Chiu CY, 2008, IEEE T CIRC SYST VID, V18, P412, DOI 10.1109/TCSVT.2008.918447
   Chiu CY, 2010, ACM T MULTIM COMPUT, V6, DOI 10.1145/1671962.1671966
   Dong W., 2008, P ACM INT C MULT MM
   Douze M., 2008, P TRECVID WORKSH GAI
   FALOUTSOS C, 1994, P ACM INT C MAN DAT
   Fu A. W. C., 2005, P INT C VER LARG DAT
   Hoad TC, 2006, ACM T INFORM SYST, V24, P1, DOI 10.1145/1125857.1125858
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Huang Z, 2010, IEEE T MULTIMEDIA, V12, P386, DOI 10.1109/TMM.2010.2050737
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Kim H., 2008, P ACM C IM VID RETR
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   LAWTO J, 2007, P ACM INT C IM VID R
   Lei YQ, 2012, IEEE T CIRC SYST VID, V22, P1332, DOI 10.1109/TCSVT.2012.2201670
   Liu B, 2011, IEEE MULTIMEDIA, V18, P22, DOI 10.1109/MMUL.2011.37
   Liu H, 2013, IEEE T KNOWL DATA EN, V25, P1706, DOI 10.1109/TKDE.2012.92
   Liu JJ, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2501654.2501658
   Liu JJ, 2011, ACM T INFORM SYST, V29, DOI 10.1145/2037661.2037666
   Liu Yang, 2010, P ACM INT C IM VID R
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Moon Y. S., 2002, P ACM INT C MAN DAT
   Moon YS, 2001, PROC INT CONF DATA, P263, DOI 10.1109/ICDE.2001.914837
   Ren J., 2012, P ACM INT C MULT RET
   Shang L., 2010, P ACM INT C MULT MM
   Shao J, 2007, PROC INT CONF DATA, P1370
   Tan H. K., 2009, P ACM INT C MULT MM
   Tian YH, 2013, IEEE MULTIMEDIA, V20, P72, DOI 10.1109/MMUL.2012.62
   Wang A, 2003, ISMIR
   Wei SK, 2011, IEEE T CIRC SYST VID, V21, P15, DOI 10.1109/TCSVT.2011.2105554
   Wu H., 2005, P ACM INT C MAN DAT
   Wu JX, 2009, IEEE I CONF COMP VIS, P630, DOI 10.1109/ICCV.2009.5459178
   Yan Y, 2008, PROC INT CONF DATA, P853, DOI 10.1109/ICDE.2008.4497494
   Yeh MC, 2009, P ACM INT C IM VID R
   Zhou X., 2010, P ACM INT C MULT MM
   Zhou XM, 2012, VLDB J, V21, P489, DOI 10.1007/s00778-011-0255-5
   Zhou XM, 2010, IEEE T KNOWL DATA EN, V22, P1372, DOI 10.1109/TKDE.2009.171
NR 40
TC 9
Z9 11
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1952
EP 1962
DI 10.1109/TMM.2014.2342668
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300013
DA 2024-07-18
ER

PT J
AU Zhu, XS
   Ding, J
   Dong, HH
   Hu, KF
   Zhang, XB
AF Zhu, Xinshan
   Ding, Jie
   Dong, Honghui
   Hu, Kongfa
   Zhang, Xiaobin
TI Normalized Correlation-Based Quantization Modulation for Robust
   Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding error probability; normalized correlation; quantization
   watermarking; spread transform; valumetric scaling
ID DIGITAL WATERMARKING; PERFORMANCE ANALYSIS; IMAGE WATERMARKING; INDEX
   MODULATION; INFORMATION; SCHEME
AB A novel quantization watermarking method is presented in this paper, which is developed following the established feature modulation watermarking model. In this method, a feature signal is obtained by computing the normalized correlation (NC) between the host signal and a random signal. Information modulation is carried out on the generated NC by selecting a codeword from the codebook associated with the embedded information. In a simple case, the structured codebooks are designed using uniform quantizers for modulation. The watermarked signal is produced to provide the modulated NC in the sense of minimizing the embedding distortion. The performance of the NC-based quantization modulation (NCQM) is analytically investigated, in terms of the embedding distortion and the decoding error probability in the presence of valumetric scaling and additive noise attacks. Numerical simulations on artificial signals confirm the validity of our analyses and exhibit the performance advantage of NCQM over other modulation techniques. The proposed method is also simulated on real images by using the wavelet-based implementations, where the host signal is constructed by the detail coefficients of wavelet decomposition at the third level and transformed into the NC feature signal for the information modulation. Experimental results show that the proposed NCQM not only achieves the improved watermark imperceptibility and a higher embedding capacity in high-noise regimes, but also is more robust to a wide range of attacks, e. g., valumetric scaling, Gaussian filtering, additive noise, Gamma correction, and Gray-level transformations, as compared with the state-of-the-art watermarking methods.
C1 [Zhu, Xinshan; Dong, Honghui] Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
   [Zhu, Xinshan; Ding, Jie; Zhang, Xiaobin] Yangzhou Univ, Sch Informat Engn, Yangzhou 225127, Peoples R China.
   [Dong, Honghui] Beijing Jiaotong Univ, State Key Lab Rail Traff Control & Safety, Beijing 100044, Peoples R China.
   [Hu, Kongfa] Nanjing Univ Chinese Med, Sch Informat Technol, Nanjing 210023, Jiangsu, Peoples R China.
C3 Tianjin University; Yangzhou University; Beijing Jiaotong University;
   Nanjing University of Chinese Medicine
RP Dong, HH (corresponding author), Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
EM xszhu_hm@hotmail.com; jieding@yzu.edu.cn; hhdong@bjtu.edu.cn;
   kfhu@njutcm.edu.cn; zxb@yzu.edu.cn
RI Zhu, Xinshan/AAD-6338-2022; Zhang, Xiaobin/K-5406-2012
FU National Natural Science Foundation of China [60803122, 61103018,
   61100120, 61172014]; National Program of International ST Cooperation
   [2013DFA11040]; Natural Science Foundation of Jiangsu Province
   [BK2012683]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 60803122, 61103018, 61100120, and 61172014, by the
   National Program of International S&T Cooperation under Grant
   2013DFA11040, and by the Natural Science Foundation of Jiangsu Province
   under Grant BK2012683. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Shahram
   Shirani. (Corresponding author: Honghui Dong.)
CR Akhaee MA, 2011, IEEE T INF FOREN SEC, V6, P883, DOI 10.1109/TIFS.2011.2146250
   Balado F, 2005, LECT NOTES COMPUT SC, V3710, P336
   Barni M., 2004, WATERMARKING SYSTEMS, V1st
   Bartolini F, 2004, IEEE T SIGNAL PROCES, V52, P2965, DOI 10.1109/TSP.2004.833868
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Comesana P., 2007, P INT C IMAGE PROCES, P145, DOI [10.1109/ICIP.2007.4379113, DOI 10.1109/ICIP.2007.4379113]
   COSTA MHM, 1983, IEEE T INFORM THEORY, V29, P439, DOI 10.1109/TIT.1983.1056659
   Cox IJ, 2008, MKS MULTIMED INFORM, P1
   Cox IJ, 1999, P IEEE, V87, P1127, DOI 10.1109/5.771068
   Devroye L., 1986, Non-Uniform Random Variate Generation
   Eggers JJ, 2003, IEEE T SIGNAL PROCES, V51, P1003, DOI 10.1109/TSP.2003.809366
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Guerrini F, 2011, IEEE T INF FOREN SEC, V6, P283, DOI 10.1109/TIFS.2011.2109383
   Kalantari NK, 2010, IEEE T IMAGE PROCESS, V19, P1504, DOI 10.1109/TIP.2010.2042646
   Kalantari NK, 2010, IEEE T CIRC SYST VID, V20, P396, DOI 10.1109/TCSVT.2009.2035842
   Lewis AS, 1992, IEEE T IMAGE PROCESS, V1, P244, DOI 10.1109/83.136601
   Li Q, 2007, IEEE T INF FOREN SEC, V2, P127, DOI 10.1109/TIFS.2007.897266
   Mankar VH, 2009, COMM COM INF SC, V40, P400, DOI 10.1007/978-3-642-03547-0_38
   Miller ML, 2000, LECT NOTES COMPUT SC, V1768, P146
   Nezhadarya E, 2011, IEEE T INF FOREN SEC, V6, P1200, DOI 10.1109/TIFS.2011.2163627
   Oostveen J, 2004, P SOC PHOTO-OPT INS, V5306, P296, DOI 10.1117/12.526586
   Ourique F, 2005, INT CONF ACOUST SPEE, P797
   Pacitti MD, 2005, LECT NOTES COMPUT SC, V3710, P403
   Papulis A., 1991, Probability, Random Variables, and Stochastic Processes, VThird
   Pérez-Freire L, 2008, IEEE T INF FOREN SEC, V3, P593, DOI 10.1109/TIFS.2008.2002938
   Pérez-González F, 2005, IEEE T SIGNAL PROCES, V53, P3960, DOI 10.1109/TSP.2005.855407
   Pérez-González F, 2003, IEEE T SIGNAL PROCES, V51, P960, DOI 10.1109/TSP.2003.809368
   Poor H. Vincent, 1994, An introduction to signal detection and estimation
   Teolis A., 1998, APPL NUM HARM ANAL, DOI 10.1007/978-1-4612-1664-3
   Voloshynovskiy S, 2001, SIGNAL PROCESS, V81, P1177, DOI 10.1016/S0165-1684(01)00039-1
   Wang YW, 2002, IEEE T IMAGE PROCESS, V11, P77, DOI 10.1109/83.982816
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P685, DOI 10.1109/TIP.2003.810588
   Zareian M, 2013, INT CONF ACOUST SPEE, P2945, DOI 10.1109/ICASSP.2013.6638197
   Zhu XS, 2012, INT CONF ACOUST SPEE, P1765, DOI 10.1109/ICASSP.2012.6288241
   [朱新山 Zhu XinShan], 2012, [计算机学报, Chinese Journal of Computers], V35, P1959
NR 36
TC 36
Z9 38
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1888
EP 1904
DI 10.1109/TMM.2014.2340695
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300008
OA Bronze
DA 2024-07-18
ER

PT J
AU Spyromitros-Xioufis, E
   Papadopoulos, S
   Kompatsiaris, I
   Tsoumakas, G
   Vlahavas, I
AF Spyromitros-Xioufis, Eleftherios
   Papadopoulos, Symeon
   Kompatsiaris, Ioannis (Yiannis)
   Tsoumakas, Grigorios
   Vlahavas, Ioannis
TI A Comprehensive Study Over VLAD and Product Quantization in Large-Scale
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image classification; image retrieval; indexing
ID CONSISTENCY; LIBRARY
AB This paper deals with content-based large-scale image retrieval using the state-of-the-art framework of VLAD and Product Quantization proposed by Jegou et al. [1] as a starting point. Demonstrating an excellent accuracy-efficiency trade-off, this framework has attracted increased attention from the community and numerous extensions have been proposed. In this work, we make an in-depth analysis of the framework that aims at increasing our understanding of its different processing steps and boosting its overall performance. Our analysis involves the evaluation of numerous extensions (both existing and novel) as well as the study of the effects of several unexplored parameters. We specifically focus on: a) employing more efficient and discriminative local features; b) improving the quality of the aggregated representation; and c) optimizing the indexing scheme. Our thorough experimental evaluation provides new insights into extensions that consistently contribute, and others that do not, to performance improvement, and sheds light onto the effects of previously unexplored parameters of the framework. As a result, we develop an enhanced framework that significantly outperforms the previous best reported accuracy results on standard benchmarks and is more efficient.
C1 [Spyromitros-Xioufis, Eleftherios; Papadopoulos, Symeon; Kompatsiaris, Ioannis (Yiannis)] Ctr Res & Technol Hellas ITI CERTH, Inst Informat Technol, Thessaloniki, Greece.
   [Spyromitros-Xioufis, Eleftherios; Tsoumakas, Grigorios; Vlahavas, Ioannis] Aristotle Univ Thessaloniki AUTH, Dept Informat, Thessaloniki, Greece.
C3 Centre for Research & Technology Hellas; Aristotle University of
   Thessaloniki
RP Spyromitros-Xioufis, E (corresponding author), Ctr Res & Technol Hellas ITI CERTH, Inst Informat Technol, Thessaloniki, Greece.
EM espyromi@iti.gr; papadop@iti.gr; ikom@iti.gr; greg@csd.auth.gr;
   vlahavas@csd.auth.gr
RI Kompatsiaris, Ioannis/P-8594-2015; Papadopoulos, Symeon/AET-0683-2022;
   Tsoumakas, Grigorios/B-4718-2008; Vlahavas, Ioannis/Q-1779-2017
OI Kompatsiaris, Ioannis/0000-0001-6447-9020; Spyromitros-Xioufis,
   Eleftherios/0000-0001-9178-8603; Tsoumakas,
   Grigorios/0000-0002-7879-669X; Vlahavas, Ioannis/0000-0003-3477-8825;
   Papadopoulos, Symeon/0000-0002-5441-7341
FU SocialSensor FP7 project - EC [287975]
FX This work was supported by the SocialSensor FP7 project, partially
   funded by the EC under Contract Number 287975. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Chong-Wah Ngo.
CR Abeles P, 2013, LECT NOTES COMPUT SC, V8034, P454, DOI 10.1007/978-3-642-41939-3_44
   [Anonymous], 2013, P 21 ACM INT C MULT, DOI 10.1145/2502081.2502171
   ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018
   Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cai H., 2011, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2011.6064624
   Chen D., 2011, P IEEE 2011 45 AS C, P850
   Chen D, 2013, SIGNAL PROCESS, V93, P2316, DOI 10.1016/j.sigpro.2012.06.005
   Chu LY, 2013, IEEE T MULTIMEDIA, V15, P1982, DOI 10.1109/TMM.2013.2270455
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Chum O., 2008, BMVC, P812, DOI DOI 10.5244/C.22.50
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2010, PROC CVPR IEEE, P3416, DOI 10.1109/CVPR.2010.5539997
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong W., 2012, P 2 ACM INT C MULT R, DOI DOI 10.1145/2324796.2324798
   Fan P, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P726, DOI 10.1109/ICNIDC.2009.5360809
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Huiskes M., 2010, Proceedings of the international conference on Multimedia information retrieval
   Jegou H, 2008, LECT NOTES COMPUT SC, V5302, P304, DOI 10.1007/978-3-540-88682-2_24
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2012, LECT NOTES COMPUT SC, V7573, P774, DOI 10.1007/978-3-642-33709-3_55
   Jégou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2009, IEEE I CONF COMP VIS, P2357, DOI 10.1109/ICCV.2009.5459419
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Law-To Julien., 2007, P 6 ACM INT C IMAGE, P371
   Li LA, 2010, IEEE IMAGE PROC, P2361, DOI 10.1109/ICIP.2010.5652210
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Picard D, 2011, IEEE IMAGE PROC, P669, DOI 10.1109/ICIP.2011.6116641
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E., 2012, P 2012 13 INT WORKSH, P1
   Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Weiss Y., 2008, PROC ADV NEURAL INFO, V21, P1753
   Xie L, 2011, P 19 ACM INT C MULT, P53, DOI DOI 10.1145/2072298.2072307
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 43
TC 83
Z9 91
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1713
EP 1728
DI 10.1109/TMM.2014.2329648
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200019
DA 2024-07-18
ER

PT J
AU Grachten, M
   Krebs, F
AF Grachten, Maarten
   Krebs, Florian
TI An Assessment of Learned Score Features for Modeling Expressive Dynamics
   in Music
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Computational Musicology; Unsupervised Learning; Expressive Music
   Performance
ID ALGORITHM; RULES
AB The study of musical expression is an ongoing and increasingly data-intensive endeavor, in which machine learning techniques can play an important role. The purpose of this paper is to evaluate the utility of unsupervised feature learning in the context of modeling expressive dynamics, in particular note intensities of performed music. We use a note centric representation of musical contexts, which avoids shortcomings of existing musical representations. With that representation, we perform experiments in which learned features are used to predict note intensities. The experiments are done using a data set comprising professional performances of Chopin's complete piano repertoire. For feature learning we use Restricted Boltzmann machines, and contrast this with features learned using matrix decomposition methods. We evaluate the results both quantitatively and qualitatively, identifying salient learned features, and discussing their musical relevance.
C1 [Grachten, Maarten] Austrian Res Inst Artificial Intelligence OFAI, Vienna, Austria.
   [Krebs, Florian] Johannes Kepler Univ Linz, Dept Computat Percept, A-4040 Linz, Austria.
C3 Johannes Kepler University Linz
RP Grachten, M (corresponding author), Austrian Res Inst Artificial Intelligence OFAI, Vienna, Austria.
RI Krebs, Florian/HTQ-7013-2023
FU Austrian Science Fund (FWF) [Z159, TRP-109]; European Union Seventh
   Framework Programme FP7 through the PHENICX project [601166]; Austrian
   Science Fund (FWF) [Z159] Funding Source: Austrian Science Fund (FWF)
FX This work was supported in part by the Austrian Science Fund (FWF) in
   the context of the projects Z159 "Wittgenstein Award" and TRP-109, and
   in part by the European Union Seventh Framework Programme FP7 through
   the PHENICX project (grant agreement no. 601166). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Mitsunori Ogihara.
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   [Anonymous], 1938, The Psychology of Music
   [Anonymous], 2012, P 13 INT SOC MUS INF
   [Anonymous], 2012, P 29 INT C MACHINE L
   [Anonymous], P AISTATS
   Arzt A, 2008, FRONT ARTIF INTEL AP, V178, P241, DOI 10.3233/978-1-58603-891-5-241
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Berkes P, 2005, J VISION, V5, P579, DOI 10.1167/5.6.9
   Binet A., 1896, ANN PSYCHOL, P201
   Erhan D, 2009, VISUALIZING HIGHER L
   Flossmann S, 2010, J NEW MUSIC RES, V39, P363, DOI 10.1080/09298215.2010.523469
   Friberg A., 2006, Advances in Cognitive Psychology, V2, P145, DOI [10.2478/v10053-008-0052-x, DOI 10.2478/V10053-008-0052-X]
   Goebl W, 2003, J ACOUST SOC AM, V114, P2273, DOI 10.1121/1.1605387
   Grachten M., 2004, LECT NOTES COMPUTER
   Grachten M, 2012, J NEW MUSIC RES, V41, P311, DOI 10.1080/09298215.2012.731071
   Halko N., 2009, APPL COMPUTAT MATH C
   Hazan A, 2006, LECT NOTES COMPUT SC, V3907, P676
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kivy P., 1980, The Corded Shell Reflections on Musical Expression
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756
   Lockett A. J., 2009, AI0904 U TX AUST
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mohamed A., 2011, P ICASSP 2011
   MOOG RA, 1990, COMPUT MUSIC J, V14, P52, DOI 10.2307/3679712
   Schluter J., 2011, P 10 INT C MACH LEAR
   Schluter J., 2011, THESIS TU MUNCHEN MU
   Spiliopoulou A, 2011, LECT NOTES ARTIF INT, V6913, P289, DOI 10.1007/978-3-642-23808-6_19
   SUNDBERG J, 1980, J ACOUST SOC AM, V68, P772, DOI 10.1121/1.384816
   SUNDBERG J, 1991, MUSIC PERCEPT, V9, P71
   TODD NPM, 1992, J ACOUST SOC AM, V91, P3540, DOI 10.1121/1.402843
   Widmer G, 2003, ARTIF INTELL, V146, P129, DOI 10.1016/S0004-3702(03)00016-X
   Widmer G, 2009, AI MAG, V30, P35, DOI 10.1609/aimag.v30i3.2249
NR 35
TC 9
Z9 9
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1211
EP 1218
DI 10.1109/TMM.2014.2311013
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600004
DA 2024-07-18
ER

PT J
AU de Fez, I
   Guerri, JC
AF de Fez, Ismael
   Carlos Guerri, Juan
TI An Adaptive Mechanism for Optimal Content Download in Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive codes; application layer-forward error correction (AL-FEC);
   file delivery over unidirectional transport (FLUTE); low density parity
   check (LDPC); multicast wireless networks
AB This paper presents an adaptive mechanism for improving the content download in wireless environments. The solution is based on the use of the file delivery over unidirectional transport (FLUTE) protocol in multicast networks, which reduce considerably the bandwidth when there are many users interested in the same contents. Specifically, the system proposed reduces the average download time of clients within the coverage area, thus improving the Quality of Experience. To that extent, clients send periodically feedback messages to the server reporting the losses they are experiencing. With this information, the server decides which is the optimum application layer-forward error correction (AL-FEC) code rate that minimizes the average download time, taking into account the channel bandwidth, and starts sending data with that code rate. The system proposed is evaluated in various scenarios, considering different distributions of losses in the coverage area. Results show that the adaptive solution proposed is very suitable in wireless networks with limited bandwidth.
C1 [de Fez, Ismael; Carlos Guerri, Juan] Univ Politecn Valencia, Inst Telecommun & Multimedia Applicat ITEAM, Valencia 46022, Spain.
C3 Universitat Politecnica de Valencia
RP de Fez, I (corresponding author), Univ Politecn Valencia, Inst Telecommun & Multimedia Applicat ITEAM, Valencia 46022, Spain.
EM isdefez@iteam.upv.es; jcguerri@dcom.upv.es
RI de+Fez+Lava, Ismael/AAJ-1048-2020; Guerri, Juan Carlos/K-9659-2014
OI de Fez, Ismael/0000-0002-1337-1973; Guerri, Juan
   Carlos/0000-0002-5807-1923
FU Ministerio de Economia y Competitividad of the Government of Spain under
   project COMINN [IPT-2012-0883-430000]
FX This work is supported in part by the Ministerio de Economia y
   Competitividad of the Government of Spain under project COMINN
   (IPT-2012-0883-430000). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Wenwu Zhu.
CR Akhshabi S, 2012, SIGNAL PROCESS-IMAGE, V27, P271, DOI 10.1016/j.image.2011.10.003
   [Anonymous], 2013, 126346 ETSI TS
   [Anonymous], 2012, 230091 ISOIEC
   [Anonymous], P 2005 ACM C EM NETW
   [Anonymous], 2004, 300744V151 ETSI EN
   [Anonymous], 102472 ETSI TS
   Busse I, 1996, COMPUT COMMUN, V19, P49, DOI 10.1016/0140-3664(95)01038-6
   Chiao HT, 2012, IEEE INT SYM BROADB
   de Fez I, 2013, COMPUT COMMUN, V36, P1298, DOI 10.1016/j.comcom.2013.04.008
   de Fez I, 2012, MULTIMED TOOLS APPL, V60, P669, DOI 10.1007/s11042-011-0841-y
   de Fez I, 2012, IEEE T MULTIMEDIA, V14, P641, DOI 10.1109/TMM.2012.2190392
   Digital Video Broadcasting (DVB), 2009, 102034 ETSI TS
   Dujovne D., 2006, P ACM INT S MSWIM MA
   GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683
   Guerri JC, 2001, MULTIMED TOOLS APPL, V13, P307, DOI 10.1023/A:1009633314583
   *IEEE COMP SOC, 1999, 80211B IEEE COMP SOC
   Kobayashi M, 2009, IEEE T MULTIMEDIA, V11, P1466, DOI 10.1109/TMM.2009.2032692
   Lacan J., 2006, INT S MOD OPT MOB AD
   Lecompte D, 2012, IEEE COMMUN MAG, V50, P68, DOI 10.1109/MCOM.2012.6353684
   Liang B, 1999, IEEE INFOCOM SER, P1377, DOI 10.1109/INFCOM.1999.752157
   Ott J., 2006, IETF RFC, V4585
   Paila T., 2012, IETF RFC, V6726
   Paolini Enrico, 2008, 2008 4th Advanced Satellite Mobile Systems (ASMS), P274, DOI 10.1109/ASMS.2008.54
   Peltotalo J, 2007, INT J COMMUN SYST, V20, P633, DOI 10.1002/dac.835
   Rappaport T. S., 1996, COMMUN ENG EMERGING
   ROCA V, 2004, RR5225 INRIA
   Roca V., 2008, IETF RFC, V5170
   Stockhammer T., 2012, IEEE VIS COMM IM PRO
   World Bank Grp, 2012, INFORMATION AND COMMUNICATIONS FOR DEVELOPMENT 2012: MAXIMIZING MOBILE, P1, DOI 10.1596/978-0-8213-8991-1
   Zamalloa MZ, 2007, ACM T SENSOR NETWORK, V3, DOI 10.1145/1240226.1240227
NR 30
TC 9
Z9 9
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1140
EP 1155
DI 10.1109/TMM.2014.2307155
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800020
OA Green Published
DA 2024-07-18
ER

PT J
AU Yuan, ML
   Khan, IR
   Farbiz, F
   Yao, SS
   Niswar, A
   Foo, MH
AF Yuan, Miaolong
   Khan, Ishtiaq Rasool
   Farbiz, Farzam
   Yao, Susu
   Niswar, Arthur
   Foo, Min-Hui
TI A Mixed Reality Virtual Clothes Try-On System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D-2D alignment; body customization; mixed reality; skin color matching
   and real-time cloth simulation; virtual try-on.
ID ACCEPTANCE
AB Virtual try-on of clothes has received much attention recently due to its commercial potential. It can be used for online shopping or intelligent recommendation to narrow down the selections to a few designs and sizes. In this paper, we present a mixed reality system for 3D virtual clothes try-on that enables a user to see herself wearing virtual clothes while looking at a mirror display, without taking off her actual clothes. The user can select various virtual clothes for trying-on. The system physically simulates the selected virtual clothes on the user's body in real-time and the user can see virtual clothes fitting on the her mirror image from various angles as she moves. The major contribution of this paper is that we automatically customize an invisible (or partially visible) avatar based on the user's body size and the skin color and use it for proper clothes fitting, alignment and clothes simulation in our virtual try-on system. We present three scenarios: i) virtual clothes on the avatar, ii) virtual clothes on the user's image and iii) virtual clothes on the avatar blended with the user's face image. We have conducted a user study to evaluate the effectiveness of these three solutions from the end user's perception of quality attributes, cognitive attributes and attitude towards using. The user study shows that among these three scenarios, the second one is most preferred by the users and for 50% of them the experience they had with our system was sufficient to make the purchase decision for the outfits they virtually tried-on.
C1 [Yuan, Miaolong; Farbiz, Farzam; Yao, Susu; Niswar, Arthur; Foo, Min-Hui] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
   [Khan, Ishtiaq Rasool] King Abdulaziz Univ, Fac Comp & Informat Technol, Jeddah 21413, Saudi Arabia.
C3 Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R); King Abdulaziz University
RP Yuan, ML (corresponding author), ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
EM myuan@i2r.a-star.edu.sg; irkhan@kau.edu.sa; farbizf@i2r.a-star.edu.sg;
   ssyao@i2r.a-star.edu.sg; aniswar@i2r.a-star.edu.sg;
   mhnfoog@i2r.a-star.edu.sg
RI Khan, Ishtiaq Rasool/AAI-9556-2020; Khan, Ishtiaq R/M-3942-2013; Farbiz,
   Farzam/AAC-4088-2020
OI Khan, Ishtiaq Rasool/0000-0002-3887-9052; Khan, Ishtiaq
   R/0000-0002-3887-9052; Farbiz, Farzam/0000-0001-8387-6507
CR Allen B, 2002, ACM T GRAPHIC, V21, P612, DOI 10.1145/566570.566626
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   Conover W. J., 1998, PRACTICAL NONPARAMET, V3rd
   Cordier F., 2001, P VIRT REAL INT C LA
   Cui Y., 2012, P ACCV WORKSH COL DE
   DAVIS FD, 1989, MIS QUART, V13, P319, DOI 10.2307/249008
   Eisert P., 2011, IEEE COMSOC MMTC E L, V6, P37
   Fiala M, 2007, IEEE VIRTUAL REALITY 2007, PROCEEDINGS, P251
   Hauswiesner Stefan, 2011, P 10 INT C VIRTUAL R, P23
   Hilsmann A., 2009, P MIR 2009 COMP VIS
   Iskdogan F., 2012, COMPUT VIS COURSE PR
   Kasap M, 2009, VRST 09 P 16 ACM S V, P123, DOI [10.1145/1643928.1643956, DOI 10.1145/1643928.1643956]
   Le Thanh T, 2009, LECT NOTES COMPUT SC, V5496, P182, DOI 10.1007/978-3-642-01811-4_17
   Legris P, 2003, INFORM MANAGE-AMSTER, V40, P191, DOI 10.1016/S0378-7206(01)00143-4
   Lepetit V, 2006, IEEE T PATTERN ANAL, V28, P1465, DOI 10.1109/TPAMI.2006.188
   MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030
   McLellan H., 1998, J VISUAL LITERACY, V18, P175, DOI DOI 10.1080/23796529.1998.11674538
   Meng YW, 2010, COMPUT AIDED DESIGN, V42, P310, DOI 10.1016/j.cad.2009.12.004
   Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37
   Özuysal M, 2006, LECT NOTES COMPUT SC, V3953, P592, DOI 10.1007/11744078_46
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Richter M., 2012, P INT C 3D IM DAT PR
   Rong Li, 2011, 2011 4th International Symposium on Computational Intelligence and Design, P32, DOI 10.1109/ISCID.2011.17
   Seo Hyewon., 2003, Proceedings of the 2003 Symposium on Interactive 3D Graphics, P19, DOI [10.1145/641480.641487, DOI 10.1145/641480.641487]
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Spanlang B., 2005, RES J TEXTILE APPARE, V9, P74
   Spanlang B., 2004, International Conference on Computer Systems and Technologies, P1
   STANNEY K, 1995, VIRTUAL REALITY ANNUAL INTERNATIONAL SYMPOSIUM '95, PROCEEDINGS, P28
   Theng YL, 2007, LECT NOTES COMPUT SC, V4563, P728
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Volino P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559762
   Wacker M., 2005, RES J TEXT APPAR, V9, P37, DOI [https://doi.org/10.1108/RJTA-09-01-2005-B005, DOI 10.1108/RJTA-09-01-2005-B005]
   Wei Zhang, 2008, 13th International Conference on Intelligent User Interfaces. IUI 2008, P60, DOI 10.1145/1378773.1378782
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
NR 35
TC 55
Z9 75
U1 7
U2 90
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1958
EP 1968
DI 10.1109/TMM.2013.2280560
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900019
DA 2024-07-18
ER

PT J
AU Wu, JJ
   Shi, GM
   Lin, WS
   Liu, AM
   Qi, F
AF Wu, Jinjian
   Shi, Guangming
   Lin, Weisi
   Liu, Anmin
   Qi, Fei
TI Just Noticeable Difference Estimation for Images With Free-Energy
   Principle
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Autoregressive (AR) model; disorder; free energy; internal generative
   mechanism (IGM); just noticeable difference (JND)
ID BRAIN; MODEL
AB In this paper, we introduce a novel just noticeable difference (JND) estimation model based on the unified brain theory, namely the free-energy principle. The existing pixel-based JND models mainly consider the orderly factors and always underestimate the JND threshold of the disorderly region. Recent research indicates that the human visual system (HVS) actively predicts the orderly information and avoids the residual disorderly uncertainty for image perception and understanding. Thus, we suggest that there exists disorderly concealment effect which results in high JND threshold of the disorderly region. Beginning with the Bayesian inference, we deduce an autoregressive model to imitate the active prediction of the HVS. Then, we estimate the disorderly concealment effect for the novel JND model. Experimental results confirm that the proposed JND model outperforms the relevant existing ones. Furthermore, we apply the proposed JND model in image compression, and around 15% of bit rate can be reduced without jeopardizing the perceptual quality.
C1 [Wu, Jinjian; Shi, Guangming; Qi, Fei] Xidian Univ, Sch Elect Engn, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
   [Lin, Weisi; Liu, Anmin] Nanyang Technol Univ, Sch Comp Engn, Nanyang 639798, Singapore.
C3 Xidian University; Nanyang Technological University
RP Wu, JJ (corresponding author), Xidian Univ, Sch Elect Engn, Minist Educ China, Key Lab Intelligent Percept & Image Understanding, Xian 710071, Peoples R China.
EM jinjian.wu@mail.xidian.edu.cn; gmshi@xidian.edu.cn; wslin@ntu.edu.sg;
   liua0002@ntu.edu.sg; fred.qi@ieee.org
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011; Qi, Fei/G-3978-2013; Wu,
   Jinjian/GQH-0222-2022
OI Lin, Weisi/0000-0001-9866-1947; Qi, Fei/0000-0002-2161-1551; 
FU Major State Basic Research Development Program of China (973 Program)
   [2013CB329402]; National Science Foundation of China [61033004,
   61070138, 61072104, 61227004]; Fundamental Research Funds for the
   Central Universities [K50513100005]
FX This work was supported in part by the Major State Basic Research
   Development Program of China (973 Program) under Grant 2013CB329402, the
   National Science Foundation of China under Grants 61033004, 61070138,
   61072104, and 61227004, and the Fundamental Research Funds for the
   Central Universities under Grant K50513100005. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopolous.
CR Chou CH, 2008, IET IMAGE PROCESS, V2, P304, DOI 10.1049/iet-ipr:20080034
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Friston KJ, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006421
   Gao DS, 2009, IEEE T PATTERN ANAL, V31, P989, DOI 10.1109/TPAMI.2009.27
   GREGORY RL, 1980, PHILOS T ROY SOC B, V290, P181, DOI 10.1098/rstb.1980.0090
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Lin WS, 2005, IEEE T CIRC SYST VID, V15, P900, DOI 10.1109/TCSVT.2005.848345
   Liu AM, 2010, IEEE T CIRC SYST VID, V20, P1648, DOI 10.1109/TCSVT.2010.2087432
   Lubin J., 1995, VISION MODELS TARGET, P245
   Netravali A.N., 1988, DIGITAL PICTURES REP
   NETRAVALI AN, 1977, P IEEE, V65, P536, DOI 10.1109/PROC.1977.10515
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   RECOMMENDATION ITU-R BT, 2002, METH SUBJ ASS QUAL T
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Sternberg R. J., 2003, Cognitive Psychology
   Vasconcelos M, 2009, IEEE T PATTERN ANAL, V31, P228, DOI 10.1109/TPAMI.2008.77
   Wei ZY, 2009, IEEE T CIRC SYST VID, V19, P337, DOI 10.1109/TCSVT.2009.2013518
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Yang XK, 2005, SIGNAL PROCESS-IMAGE, V20, P662, DOI 10.1016/j.image.2005.04.001
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P742, DOI 10.1109/TCSVT.2005.848313
   Zhai GT, 2012, IEEE T IMAGE PROCESS, V21, P41, DOI 10.1109/TIP.2011.2161092
   Zhang XH, 2008, J VIS COMMUN IMAGE R, V19, P30, DOI 10.1016/j.jvcir.2007.06.001
NR 24
TC 103
Z9 113
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1705
EP 1710
DI 10.1109/TMM.2013.2268053
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800021
DA 2024-07-18
ER

PT J
AU Chen, X
   Chen, X
   Ward, RK
   Wang, ZJ
AF Chen, Xun
   Chen, Xiang
   Ward, Rabab Kreidieh
   Wang, Z. Jane
TI A Joint Multimodal Group Analysis Framework for Modeling Corticomuscular
   Activity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Corticomuscular activity analysis; data fusion; EEG; EMG; group
   analysis; multimodal; Parkinson's disease
ID PARKINSONS-DISEASE; OSCILLATIONS; COHERENCE; CORTEX; MUSCLE
AB Corticomuscular coupling analysis based on multiple data sets such as electroencephalography (EEG) and electromyography (EMG) signals provides a useful tool for understanding human motor control systems. Two probably most popular methods are the pair-wise magnitude-squared coherence (MSC) between EEG and simultaneously-recorded EMG signals, and partial least square (PLS). Unfortunately, MSC and PLS generally deal with only two types of data sets at the same time, while we may need to analyze more than two types of data sets. Moreover, it is not straightforward to extend MSC to the group level for combining results across subjects. Also, PLS can have the information mixing problem since only the variations in one data set are used to predict the other data set. To address these concerns, we propose a joint multimodal analysis framework for corticomuscular coupling analysis. The proposed framework models multiple data spaces simultaneously in a multidirectional fashion. Furthermore, to address the inter-subject variability concern in real-world medical applications, we extend the proposed framework from the individual subject level to the group level to obtain common corticomuscular coupling patterns across subjects. We apply the proposed framework to concurrent EEG, EMG and behavior data collected in a Parkinson's disease (PD) study. The results reveal several highly correlated temporal patterns among the three types of signals and their corresponding spatial activation patterns. In PD subjects, there are enhanced connections between occipital region and other regions, which is consistent with the previous medical finding. The proposed framework is a promising technique for performing multi-subject and multi-modal data analysis.
C1 [Chen, Xun; Ward, Rabab Kreidieh; Wang, Z. Jane] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Chen, Xiang] Univ Sci & Technol China, Dept Elect Sci & Technol, Hefei 230027, Anhui, Peoples R China.
C3 University of British Columbia; Chinese Academy of Sciences; University
   of Science & Technology of China, CAS
RP Chen, X (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM xunchen@ece.ubc.ca; xch@ustc.edu.cn; rababw@ece.ubc.ca;
   zjanew@ece.ubc.ca
RI Chen, Xun/H-7693-2016
FU Canadian Natural Sciences and Engineering Research Council (NSERC) STPGP
   [365208-08]; Qatar National Research Fund (QNRF) [NPRP 09-310-1-058];
   Michael Smith Foundation for Health Research (MSFHR)
FX This work was supported by the Canadian Natural Sciences and Engineering
   Research Council (NSERC) STPGP 365208-08, Qatar National Research Fund
   (QNRF) No. NPRP 09-310-1-058, and the Michael Smith Foundation for
   Health Research (MSFHR). The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Dimitri Van
   De Ville.
CR Baker SN, 1997, J PHYSIOL-LONDON, V501, P225, DOI 10.1111/j.1469-7793.1997.225bo.x
   Bortel R, 2006, SIGNAL PROCESS, V86, P1737, DOI 10.1016/j.sigpro.2005.09.011
   Brown P, 2001, NEUROREPORT, V12, P2577, DOI 10.1097/00001756-200108080-00057
   Calhoun VD, 2001, HUM BRAIN MAPP, V14, P140, DOI 10.1002/hbm.1048
   Clancy EA, 2002, J ELECTROMYOGR KINES, V12, P1, DOI 10.1016/S1050-6411(01)00033-5
   Fearn T, 2000, CHEMOMETR INTELL LAB, V50, P47, DOI 10.1016/S0169-7439(99)00045-3
   GELADI P, 1986, ANAL CHIM ACTA, V185, P1, DOI 10.1016/0003-2670(86)80028-9
   Good P., 2013, Permutation tests: A practical guide to resampling methods for testing hypotheses
   Kim T, 2007, IEEE T AUDIO SPEECH, V15, P70, DOI 10.1109/TASL.2006.872618
   Krzanowski W., 2000, PRINCIPLES MULTIVARI, V23
   Li YO, 2009, IEEE T SIGNAL PROCES, V57, P3918, DOI 10.1109/TSP.2009.2021636
   Martínez-Montes E, 2004, NEUROIMAGE, V22, P1023, DOI 10.1016/j.neuroimage.2004.03.038
   McIntosh AR, 1996, NEUROIMAGE, V3, P143, DOI 10.1006/nimg.1996.0016
   MURTHY VN, 1992, P NATL ACAD SCI USA, V89, P5670, DOI 10.1073/pnas.89.12.5670
   Pollok B, 2012, J PHYSIOL-LONDON, V590, P3203, DOI 10.1113/jphysiol.2012.231316
   Praamstra P, 1998, BRAIN, V121, P167, DOI 10.1093/brain/121.1.167
   Salenius S, 2002, BRAIN, V125, P491, DOI 10.1093/brain/awf042
   Salenius S, 1997, J NEUROPHYSIOL, V77, P3401, DOI 10.1152/jn.1997.77.6.3401
   Stevenson JKR, 2011, EUR J NEUROSCI, V33, P298, DOI 10.1111/j.1460-9568.2010.07501.x
   Stoffers D, 2008, NEUROIMAGE, V41, P212, DOI 10.1016/j.neuroimage.2008.02.027
   Wang ZJ, 2009, BIOMED ENG ONLINE, V8, DOI 10.1186/1475-925X-8-9
   Weiss S, 2000, COGNITIVE BRAIN RES, V9, P299, DOI 10.1016/S0926-6410(00)00011-2
   Wold S, 1996, J CHEMOMETR, V10, P463
   Yu HL, 2004, CHEMOMETR INTELL LAB, V73, P199, DOI 10.1016/j.chemolab.2004.04.006
   Zhao CH, 2011, AICHE J, V57, P1233, DOI 10.1002/aic.12339
NR 25
TC 12
Z9 12
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1049
EP 1059
DI 10.1109/TMM.2013.2245319
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600009
DA 2024-07-18
ER

PT J
AU Liu, YH
AF Liu, Yonghuai
TI Message Passing Matching Dynamics for Overlapping Point Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Free form shapes; matching dynamics; message passing; overlapping point
   identification; underlying transformation; weight share
ID REGISTRATION
AB Existing registration algorithms usually converge to a local minimum due to inaccurate evaluation of the tentative correspondences established. In this paper, we move a step further and instead estimate the extent to which a point lies in the overlapping area. To this end, we regard the registration problem as an exchange network and develop a matching dynamics to characterize the interaction inside. Then we propose a novel algorithm based on the powerful message passing scheme derived from the matching dynamics for the optimization of the overlapping point weight. The novel algorithm penalizes in the process of deterministic annealing those tentative correspondences that violate the properties of the matching dynamics. The rigid transformation that brings the two overlapping shapes into alignment is finally estimated in the weighted least squares sense. Our experiments use both synthetic and real data to show that our proposed algorithm is more likely to converge to the global minimum than four selected state of the art ones for more accurate and robust results.
C1 [Liu, Yonghuai] Aberystwyth Univ, Dept Comp Sci, Ceredigion SY23 3DB, Dyfed, Wales.
C3 Aberystwyth University
RP Liu, YH (corresponding author), Aberystwyth Univ, Dept Comp Sci, Ceredigion SY23 3DB, Dyfed, Wales.
EM yyl@aber.ac.uk
RI Liu, Yonghuai/ABF-3794-2020
FU Higher Education Funding Council for Wales (HEFCW)
FX The Research Institute of Visual Computing is funded by Higher Education
   Funding Council for Wales (HEFCW). This work was supported by HEFCW. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris.
CR Albarelli A, 2010, PROC CVPR IEEE, P430, DOI 10.1109/CVPR.2010.5540183
   [Anonymous], P EUR C COMP VIS HER
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Bayati M, 2008, IEEE T INFORM THEORY, V54, P1241, DOI 10.1109/TIT.2007.915695
   BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791
   Bican J, 2009, PATTERN RECOGN LETT, V30, P914, DOI 10.1016/j.patrec.2009.03.015
   Chetverikov D, 2005, IMAGE VISION COMPUT, V23, P299, DOI 10.1016/j.imavis.2004.05.007
   Chua CS, 1997, INT J COMPUT VISION, V25, P63, DOI 10.1023/A:1007981719186
   Chui HL, 2003, COMPUT VIS IMAGE UND, V89, P114, DOI 10.1016/S1077-3142(03)00009-2
   Cordón O, 2006, PATTERN RECOGN LETT, V27, P1191, DOI 10.1016/j.patrec.2005.07.017
   Domokos C, 2010, PATTERN RECOGN, V43, P569, DOI 10.1016/j.patcog.2009.08.013
   Jian B, 2011, IEEE T PATTERN ANAL, V33, P1633, DOI 10.1109/TPAMI.2010.223
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kanoria Y, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1518
   Kleinberg J, 2008, ACM S THEORY COMPUT, P295
   Liu YG, 2005, PATTERN RECOGN, V38, P1615, DOI 10.1016/j.patcog.2005.01.008
   Myronenko A, 2010, IEEE T PATTERN ANAL, V32, P2262, DOI 10.1109/TPAMI.2010.46
   Phillips JM, 2007, 3DIM 2007: SIXTH INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P427
   Sandhu R, 2010, IEEE T PATTERN ANAL, V32, P1459, DOI 10.1109/TPAMI.2009.142
   Silva L, 2005, IEEE T PATTERN ANAL, V27, P762, DOI 10.1109/TPAMI.2005.108
   Toldo R., 2010, P 3DPVT
   Zitová B, 2003, IMAGE VISION COMPUT, V21, P977, DOI 10.1016/S0262-8856(03)00137-9
NR 22
TC 0
Z9 0
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2013
VL 15
IS 5
BP 1152
EP 1162
DI 10.1109/TMM.2013.2247034
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222WT
UT WOS:000324763600017
DA 2024-07-18
ER

PT J
AU Wang, SX
   Dey, S
AF Wang, Shaoxuan
   Dey, Sujit
TI Adaptive Mobile Cloud Computing to Enable Rich Mobile Multimedia
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile Computing; Cloud Computing; Cloud Gaming; Multimedia Applications
AB With worldwide shipments of smartphones (487.7 million) exceeding PCs (414.6 million including tablets) in 2011 [1], and in the US alone, more users predicted to access the Internet from mobile devices than from PCs by 2015 [2], clearly there is a desire to be able to use mobile devices and networks like we use PCs and wireline networks today. However, in spite of advances in the capabilities of mobile devices, a gap will continue to exist, and may even widen, with the requirements of rich multimedia applications. Mobile cloud computing can help bridge this gap, providing mobile applications the capabilities of cloud servers and storage together with the benefits of mobile devices and mobile connectivity, possibly enabling a new generation of truly ubiquitous multimedia applications on mobile devices: Cloud Mobile Media (CMM) applications. In this paper, we look at early trends, and opportunities and benefits for new CMM applications and services. We analyze the challenges imposed by mobile cloud computing that need to be addressed to make CMM applications viable, including response time, user experience, cloud computing cost, mobile network bandwidth, and scalability to large number of CMM users, besides other important cloud computing issues like energy consumption, privacy, and security. We illustrate the challenges using Cloud Mobile Gaming (CMG), an approach that enables rich multiplayer Internet games on mobile devices, where compute intensive tasks like graphic rendering are executed on cloud servers in response to gaming commands on a mobile device, and the resulting video has to be streamed back to the mobile device in near real time, making it one of the most challenging CMM applications. Subsequently, we focus in this paper on developing adaptive mobile cloud computing techniques to address the CMG challenges. Specifically, we propose a rendering adaptation technique, which can dynamically vary the richness and complexity of graphic rendering depending on the network and cloud computing constraints, thereby impacting both the bit rate of the rendered video that needs to be streamed back from the cloud server to the mobile device, and the computation load on the CMG servers. Experiments conducted on a cellular network demonstrate that our proposed technique can significantly improve user experience, and ensure scalability of the CMG approach in terms of both network bandwidth and server computational need.
C1 [Wang, Shaoxuan; Dey, Sujit] Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Wang, SX (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, Mobile Syst Design Lab, La Jolla, CA 92093 USA.
EM shaoxuan@ece.ucsd.edu; dey@ece.ucsd.edu
CR Ahlehagh H., 2012, P IEEE ICC WORKSH RE
   Ahlehagh H., 2012, P IEEE WCNC PAR FRAN
   [Anonymous], CISC VIS NETW IND FO
   [Anonymous], MOB CLOUD SMART DEV
   ARCchart, 2011, MOB CLOUD MARK AN FO
   Asia Times, 2010, ASIA TIMES
   Bossche R., 2010, P IEEE INT C CLOUD C
   Canalys, 2011, SMART PHON OV CLIENT
   IDC, 2011, MOR MOB INT US WIR U
   Lee G., 2011, HOTCLOUD
   Lee Y.-T., 2011, P ACM MULT SYST
   MarketsAndMarkets, 2010, WORLD MOB APPL MARK
   Wang S., 2010, P IEEE WCNC SYDN AUS
   Wang S., 2012, P IEEE ICC
   Wang S., 2010, P IEEE GLOBECOM
   Wang S., 2009, P IEEE GLOBECOM
   Wang Shaoxuan., 2012, Proceedings of ACM SIGMOBILE Mobile Computing and Communications Review, V16, P10
NR 17
TC 131
Z9 142
U1 1
U2 76
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 870
EP 883
DI 10.1109/TMM.2013.2240674
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500015
DA 2024-07-18
ER

PT J
AU Wang, XF
   Chen, M
   Kwon, TT
   Yang, LT
   Leung, VCM
AF Wang, Xiaofei
   Chen, Min
   Kwon, Ted Taekyoung
   Yang, Laurence T.
   Leung, Victor C. M.
TI AMES-Cloud: A Framework of Adaptive Mobile Video Streaming and Efficient
   Social Video Sharing in the Clouds
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; cloud computing; mobile networks; scalable
   video coding; social video sharing
ID CODING EXTENSION; PERFORMANCE; ADAPTATION
AB While demands on video traffic over mobile networks have been souring, the wireless link capacity cannot keep up with the traffic demand. The gap between the traffic demand and the link capacity, along with time-varying link conditions, results in poor service quality of video streaming over mobile networks such as long buffering time and intermittent disruptions. Leveraging the cloud computing technology, we propose a new mobile video streaming framework, dubbed AMES-Cloud, which has two main parts: adaptive mobile video streaming (AMoV) and efficient social video sharing (ESoV). AMoV and ESoV construct a private agent to provide video streaming services efficiently for each mobile user. For a given user, AMoV lets her private agent adaptively adjust her streaming flow with a scalable video coding technique based on the feedback of link quality. Likewise, ESoV monitors the social network interactions among mobile users, and their private agents try to prefetch video content in advance. We implement a prototype of the AMES-Cloud framework to demonstrate its performance. It is shown that the private agents in the clouds can effectively provide the adaptive streaming, and perform video sharing (i.e., prefetching) based on the social network analysis.
C1 [Wang, Xiaofei; Kwon, Ted Taekyoung] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 151742, South Korea.
   [Chen, Min; Yang, Laurence T.] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan 430074, Peoples R China.
   [Yang, Laurence T.] St Francis Xavier Univ, Dept Comp Sci, Antigonish, NS B2G 2W5, Canada.
   [Leung, Victor C. M.] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 Seoul National University (SNU); Huazhong University of Science &
   Technology; Saint Francis Xavier University - Canada; University of
   British Columbia
RP Wang, XF (corresponding author), Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 151742, South Korea.
EM dob-bymmlab@gmail.com; minchen@ieee.org; tkkwon@snu.ac.kr;
   ltyang@gmail.com; vleung@ece.ubc.ca
RI Chen, Min/N-9350-2015; Wang, Xiaofei/HJG-7044-2022; Laurence T. Yang,
   FCAE/AAA-1898-2019; Leung, Victor C. M./AGU-2462-2022; Leung, Victor
   C.M./X-6823-2019
OI Chen, Min/0000-0002-0960-4447; Wang, Xiaofei/0000-0001-7783-4204;
   Laurence T. Yang, FCAE/0000-0002-7986-4244; Leung, Victor C.
   M./0000-0003-3529-2640; Leung, Victor C.M./0000-0003-3529-2640
FU Korea Communications Commission, Korea [KCA-2012-11-911-05-002]; Youth
   1000 Talent Program; Program for New Century Excellent Talents in
   University
FX This work was supported by the Korea Communications Commission, Korea,
   under the R&D program supervised by the Korea Communications Agency
   under Grant KCA-2012-11-911-05-002. The work of M. Chen was supported in
   part by Youth 1000 Talent Program and the Program for New Century
   Excellent Talents in University. The associate editor coordinating the
   review of this manuscript and approving it for publication was Yonggang
   Wen.
CR Aggarwal B, 2010, ACM SIGCOMM COMP COM, V40, P477, DOI 10.1145/1851275.1851272
   [Anonymous], CISC VIS NETW IND GL
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], P IEEE GLOBECOM
   BALASUBRAMANIAN A., 2010, Proceedings of MobiSys, P209, DOI DOI 10.1145/1814433.1814456
   Benevenuto F, 2009, ACM T MULTIM COMPUT, V5, DOI 10.1145/1596990.1596994
   Calyam P, 2011, J COMMUN NETW-S KOR, V13, P591, DOI 10.1109/JCN.2011.6157475
   Cha M, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P1
   Chen Y, 2002, 10TH IEEE INTERNATIONAL CONFERENCE ON NETWORK PROTOCOLS, PROCEEDINGS, P165, DOI 10.1109/ICNP.2002.1181397
   Chetan S., 2010, CLOUD COMPUTING MOBI
   Dinh H. T., 2011, WILEY J WIRELESS OCT
   Fernandez JC, 2009, IEEE T MULTIMEDIA, V11, P1082, DOI 10.1109/TMM.2009.2026086
   Fu Y, 2006, Progress of Green Oxidation/Reduction Technologies, P1
   Huang ZX, 2011, IEEE INFOCOM SER, P201, DOI 10.1109/INFCOM.2011.5935009
   Joon-Myung Kang, 2011, Journal of Computing Science and Engineering, V5, P338, DOI 10.5626/JCSE.2011.5.4.338
   Li JY, 2012, J PARALLEL DISTR COM, V72, P666, DOI 10.1016/j.jpdc.2012.02.002
   Li Y., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P209, DOI [10.1145/2068816, DOI 10.1145/2068816]
   McDonagh P., 2011, P WPMC
   Nafaa A, 2008, IEEE COMMUN MAG, V46, P72, DOI 10.1109/MCOM.2008.4427233
   Niu D, 2012, IEEE INFOCOM SER, P460, DOI 10.1109/INFCOM.2012.6195785
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Schwarz H, 2008, IEEE SIGNAL PROC MAG, V25, P135, DOI 10.1109/MSP.2007.914712
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Singh V, 2009, IEEE INFOCOM 2009 - IEEE CONFERENCE ON COMPUTER COMMUNICATIONS WORKSHOPS, P205
   Taleb T, 2008, IEEE T VEH TECHNOL, V57, P3801, DOI 10.1109/TVT.2008.918727
   Taleb T, 2011, IEEE T BROADCAST, V57, P662, DOI 10.1109/TBC.2011.2159529
   Tappayuthpijarn K., 2009, Proceedings of the 2009 International Conference on Wireless Communications and Mobile Computing: Connecting the World Wirelessly, P1325
   Wang F, 2012, IEEE INFOCOM SER, P199, DOI 10.1109/INFCOM.2012.6195578
   Wang G, 2010, ADV INTEL SYS RES, V12, P1
   Wang X, 2011, LECT NOTES COMPUT SC, V6579, P184, DOI 10.1007/978-3-642-19260-9_19
   Wang Z, 2012, IEEE INFOCOM SER, P2901, DOI 10.1109/INFCOM.2012.6195726
   Wen Y., 2011, ENERGY OPTIMAL EXECU ENERGY OPTIMAL EXECU
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1227, DOI 10.1109/TCSVT.2007.905519
   Zambelli A., 2009, IIS Smooth Streaming Technical Overview
   Zhang K, 2005, MULTIMEDIA SYST, V10, P245, DOI 10.1007/s00530-004-0155-2
   Zhang Q, 2010, J INTERNET SERV APPL, V1, P7, DOI 10.1007/s13174-010-0007-6
   Zhang W. W., 2012, IEEE T MULT IN PRESS
   Zhang W. W., 2013, P INFOCOM MIN
   Zhang Y., 2010, HDB OPTIMIZATION COM
NR 39
TC 84
Z9 90
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2013
VL 15
IS 4
BP 811
EP 820
DI 10.1109/TMM.2013.2239630
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148FK
UT WOS:000319228500010
DA 2024-07-18
ER

PT J
AU Meng, FM
   Li, HL
   Liu, GH
   Ngan, KN
AF Meng, Fanman
   Li, Hongliang
   Liu, Guanghui
   Ngan, King Ngi
TI Object Co-Segmentation Based on Shortest Path Algorithm and Saliency
   Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-saliency; co-segmentation; shortest path algorithm
ID IMAGE; COSEGMENTATION; CONTOURS
AB Segmenting common objects that have variations in color, texture and shape is a challenging problem. In this paper, we propose a new model that efficiently segments common objects from multiple images. We first segment each original image into a number of local regions. Then, we construct a digraph based on local region similarities and saliency maps. Finally, we formulate the co-segmentation problem as the shortest path problem, and we use the dynamic programming method to solve the problem. The experimental results demonstrate that the proposed model can efficiently segment the common objects from a group of images with generally lower error rate than many existing and conventional co-segmentation methods.
C1 [Meng, Fanman; Li, Hongliang; Liu, Guanghui] Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610073, Peoples R China.
   [Ngan, King Ngi] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese
   University of Hong Kong
RP Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610073, Peoples R China.
EM fanmanmeng@yahoo.com; hlli@uestc.edu.cn; guanghuiliu@uestc.edu.cn;
   knngan@ee.cuhk.edu.hk
RI Liu, Guanghui/C-3658-2012; Ngan, N/E-8240-2014
OI Liu, Guanghui/0000-0002-4170-4552; Ngan, N/0000-0003-1946-3235; Li,
   Hongliang/0000-0002-7481-095X
FU NSFC [60972109, 61101091]; Program for New Century Excellent Talents in
   University [NCET-08-0090]; Ph.D. Programs Foundation of Ministry of
   Education of China [20110185110002]; Fundamental Research Funds for the
   Central Universities [E022050205]
FX This work was supported in part by NSFC (No.60972109 and 61101091), the
   Program for New Century Excellent Talents in University (NCET-08-0090),
   the Ph.D. Programs Foundation of Ministry of Education of China (No.
   20110185110002), and the Fundamental Research Funds for the Central
   Universities (E022050205).
CR Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   Arbeláez P, 2009, PROC CVPR IEEE, P2294, DOI 10.1109/CVPRW.2009.5206707
   Batra D, 2009, IEEE IMAGE PROC, P2393, DOI 10.1109/ICIP.2009.5414482
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Brox T, 2010, LECT NOTES COMPUT SC, V6315, P282, DOI 10.1007/978-3-642-15555-0_21
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Han JW, 2006, IEEE T CIRC SYST VID, V16, P141, DOI 10.1109/TCSVT.2005.859028
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   Hochbaum DS, 2009, IEEE I CONF COMP VIS, P269, DOI 10.1109/ICCV.2009.5459261
   Jacobson N, 2010, IEEE T IMAGE PROCESS, V19, P2924, DOI 10.1109/TIP.2010.2050928
   Jing F, 2004, IEEE T CIRC SYST VID, V14, P672, DOI 10.1109/TCSVT.2004.826775
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kai-Yueh Chang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2129, DOI 10.1109/CVPR.2011.5995415
   Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239
   Kolmogorov V., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91
   Li HL, 2007, IEEE T CIRC SYST VID, V17, P1742, DOI 10.1109/TCSVT.2007.903326
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li HL, 2009, IEEE T MULTIMEDIA, V11, P77, DOI 10.1109/TMM.2008.2008922
   Ling HB, 2007, IEEE T PATTERN ANAL, V29, P286, DOI 10.1109/TPAMI.2007.41
   Mukherjee Lopamudra, 2011, Proc IEEE Comput Soc Conf Comput Vis Pattern Recognit, P1881, DOI 10.1109/CVPR.2011.5995420
   Mukherjee L, 2009, PROC CVPR IEEE, P2028, DOI 10.1109/CVPRW.2009.5206652
   Vicente S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2217, DOI 10.1109/CVPR.2011.5995530
   Vicente S, 2010, LECT NOTES COMPUT SC, V6312, P465, DOI 10.1007/978-3-642-15552-9_34
   Zhang JY, 2010, PROC CVPR IEEE, P2125, DOI 10.1109/CVPR.2010.5539891
NR 29
TC 129
Z9 137
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1429
EP 1441
DI 10.1109/TMM.2012.2197741
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600005
DA 2024-07-18
ER

PT J
AU Gritti, T
   Monaci, G
AF Gritti, Tommaso
   Monaci, Gianluca
TI Automatic Light Scene Setting Through Image-Based Sparse Light Effect
   Approximation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light atmosphere creation; lighting design; scene understanding; sparse
   image approximation
AB Recently a series of key factors are deeply transforming lighting systems. Single lights can be controlled individually and with advanced rendering capabilities. Furthermore, a shift is underway from independent light sources to integrated lighting installations. These factors fuel the need for intuitive, flexible control techniques that can fully exploit the rendering capabilities of a lighting infrastructure. In this paper, we present a novel framework to automatically create lighting atmospheres in any type of environment. The desired light settings are derived by approximation of user-defined input images or videos. This input modality gives an immediate and intuitive representation of a lighting atmosphere and allows the user to translate into a light setting any visual content. The effectiveness and versatility of the proposed solution is demonstrated and discussed by deploying the system in several challenging, real-life application scenarios.
C1 [Gritti, Tommaso; Monaci, Gianluca] Philips Res Europe, Video & Image Proc Grp, NL-5656 AE Eindhoven, Netherlands.
C3 Philips; Philips Research
RP Gritti, T (corresponding author), Philips Res Europe, Video & Image Proc Grp, NL-5656 AE Eindhoven, Netherlands.
EM tommaso.gritti@philips.com; gianluca.monaci@philips.com
RI Monaci, Gianluca/HLH-8132-2023
OI Monaci, Gianluca/0000-0001-5514-8457
CR [Anonymous], 2008, A wavelet tour of signal processing: The sparse way
   [Anonymous], 2011, WIRED
   [Anonymous], 2007, INERT1989 ROC
   Debevec P., 1997, P ACM SIGGRAPH, P722
   Gritti T., 2011, P ACM MULT
   Hanson RJ., 1974, SOLVING LEAST SQUARE
   Leppamaki S., 2006, THESIS U HELSINKI HE
   Monaci G., 2011, P ACM MULT, P1285
   Newsham GR, 2009, LIGHTING RES TECHNOL, V41, P143, DOI 10.1177/1477153508099889
   Oberfeld D, 2009, J SENS STUD, V24, P797, DOI 10.1111/j.1745-459X.2009.00239.x
   Quartier K., 2009, P EUR AC DES INT C, P352
   Sarkar A., 2008, P SPIE SENS CAM SYST
   Sarkar A, 2006, LEUKOS, V2, P307, DOI 10.1080/15502724.2006.10747642
   Schanda J., 1997, OSA AIP HDB APPL PHO
   van Bommel W. J. M., 2004, Lighting Research & Technology, V36, P255, DOI 10.1191/1365782804li122oa
NR 15
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1351
EP 1358
DI 10.1109/TMM.2012.2191271
PN 2
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400019
DA 2024-07-18
ER

PT J
AU Zhu, SA
   Ngo, CW
   Jiang, YG
AF Zhu, Shiai
   Chong-Wah Ngo
   Jiang, Yu-Gang
TI Sampling and Ontologically Pooling Web Images for Visual Concept
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training set construction; visual concept learning; web images
ID OBJECT CATEGORIES
AB Sufficient training examples are essential for effective learning of semantic visual concepts. In practice, however, acquiring noise-free training examples has always been expensive. Recently the rapid popularization of social media websites, such as Flickr, has made it possible to collect training exemplars without human assistance. This paper proposes a novel and efficient approach to collect training samples from the noisily tagged Web images for visual concept learning, where we try to maximize two important criteria, relevancy and coverage, of the automatically generated training sets. For the former, a simple method named semantic field is introduced to handle the imprecise and incomplete image tags. Specifically, the relevancy of an image to a target concept is predicted by collectively analyzing the associated tag list of the image using two knowledge sources: WordNet corpus and statistics from Flickr.com. To boost the coverage or diversity of the training sets, we further propose an ontology-based hierarchical pooling method to collect samples not only based on the target concept alone, but also from ontologically neighboring concepts. Extensive experiments on three different datasets (NUS-WIDE, PASCAL VOC, and ImageNet) demonstrate the effectiveness of our proposed approach, producing competitive performance even when comparing with concept classifiers learned using expert-labeled training examples.
C1 [Zhu, Shiai; Chong-Wah Ngo] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
   [Jiang, Yu-Gang] Fudan Univ, Sch Comp Sci, Shanghai 201203, Peoples R China.
C3 City University of Hong Kong; Fudan University
RP Zhu, SA (corresponding author), City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.
EM shiaizhu2@student.cityu.edu.hk; cwngo@cs.cityu.edu.hk; ygj@fudan.edu.cn
OI Ngo, Chong Wah/0000-0003-4182-8261
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 119709]
FX This work was supported by the Research Grants Council of the Hong Kong
   Special Administrative Region, China under Grant CityU 119709. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Marcel Worring.
CR [Anonymous], NIST TRECVID WORKSH
   [Anonymous], P ACM MULT
   [Anonymous], P CIVR
   [Anonymous], ICCV
   [Anonymous], P INT C CONT BAS IM
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI [10.1145/1899412.1899418, DOI 10.1145/1899412.1899418]
   [Anonymous], 2010, PASCAL VISUAL OBJECT
   [Anonymous], P ICMR
   Borth D., 2010, P MIR, P25
   Chua T.-S., 2009, CIVR NEW YORK NY
   Cilibrasi RL, 2007, IEEE T KNOWL DATA EN, V19, P370, DOI 10.1109/TKDE.2007.48
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan JP, 2010, PROC CVPR IEEE, P802, DOI 10.1109/CVPR.2010.5540135
   Fergus R, 2005, IEEE I CONF COMP VIS, P1816
   FERGUS R, 2010, P ECCV, V6311, P762
   Griffin G., 2007, CALTECH 256 OBJECT C
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jiang Y.-G., 2009, Proc. ACM MM, P155, DOI DOI 10.1145/1631272.1631296
   Jiang YG, 2010, IEEE T MULTIMEDIA, V12, P42, DOI 10.1109/TMM.2009.2036235
   Jurafsky D., 2000, Speech and language processing: An introduction to natural language processing, computational linguistics, and speech recognition
   KENNEDY L, 2006, 21720063 ADVENT COL
   Kennedy L., 2009, Proc. Workshop on Web-scale Multimedia Corpus, P17
   Kennedy L.S., 2006, Proc. ACM Multimedia Information Retrieval, P249, DOI DOI 10.1145/1178677.1178712
   Li LJ, 2010, INT J COMPUT VISION, V88, P147, DOI 10.1007/s11263-009-0265-6
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Liu Dong., 2009, P 18 INT C WORLD WID, P351
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Quenot G., 2011, TRECVID 2011 COLLABO
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schroff F, 2011, IEEE T PATTERN ANAL, V33, P754, DOI 10.1109/TPAMI.2010.133
   Setz AT, 2009, IEEE INT CON MULTI, P1460, DOI 10.1109/ICME.2009.5202778
   Snoek C.G.M., 2010, P NIST TRECVID WORKS, pIV
   Tian Q., 2004, P ICME JUN 30, P1022
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347
   Ulges A., 2010, VIDEO SEARCH MINING
   Ulges A, 2010, COMPUT VIS IMAGE UND, V114, P429, DOI 10.1016/j.cviu.2009.08.002
   Wang J., 2009, P CVPR, P1390
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133
   Yan R, 2005, PROC CVPR IEEE, P657
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 43
TC 25
Z9 27
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
SI SI
BP 1068
EP 1078
DI 10.1109/TMM.2012.2190387
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QL
UT WOS:000306599300012
OA Green Published
DA 2024-07-18
ER

PT J
AU Niu, YZ
   Liu, F
   Feng, WC
   Jin, HL
AF Niu, Yuzhen
   Liu, Feng
   Feng, Wu-Chi
   Jin, Hailin
TI Aesthetics-Based Stereoscopic Photo Cropping for Heterogeneous Displays
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetics; multimedia authoring; photo cropping; stereoscopic
   photography
ID ADAPTATION; IMAGES; VIDEO; SHIFT
AB Stereoscopic displays are becoming ubiquitous, ranging from large 3-D TVs to small mobile phones. Stereoscopic photos need to be carefully adapted to be effectively viewed on the displays other than originally intended. In this paper, we present a method that can automatically crop and scale an existing stereoscopic photo to a variety of displays while preserving its aesthetic value. We formulate stereoscopic photo adaptation as an optimization problem that aims to preserve the aesthetic value of the input photo. We define a wide range of energy terms to preserve the stereoscopic photo aesthetics by borrowing rules from stereoscopic photography. Our experiments on a wide variety of stereoscopic photos demonstrate that our method can robustly produce display-dependent stereoscopic photos that deliver pleasant viewing experiences.
C1 [Niu, Yuzhen; Liu, Feng; Feng, Wu-Chi] Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
   [Jin, Hailin] Adobe Syst Inc, Adv Technol Labs, San Jose, CA 95110 USA.
C3 Portland State University; Adobe Systems Inc.
RP Niu, YZ (corresponding author), Portland State Univ, Dept Comp Sci, Portland, OR 97207 USA.
EM yuzhen@cs.pdx.edu; fliu@cs.pdx.edu; wuchi@cs.pdx.edu; hljin@adobe.com
FU Portland State University
FX This work was supported by the Portland State University Faculty
   Enhancement Grant. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Pascal Frossard.
CR [Anonymous], DIGITAL 3D STEREO GU
   [Anonymous], 2007, ACM T GRAPH
   [Anonymous], P ACM INT C MULT
   [Anonymous], 2007, EMPIR STUD ARTS, DOI DOI 10.2190/10T0-2378-0583-73Q4
   [Anonymous], 2006, P 14 ACM INT C MULT
   [Anonymous], 2015, Design, Color, and Composition in Photography
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   [Anonymous], 1991, Grammar of the film language
   Arnheim R., 1988, POWER CTR STUDY COMP
   Chang CH, 2011, IEEE T MULTIMEDIA, V13, P589, DOI 10.1109/TMM.2011.2116775
   Chen LQ, 2003, MULTIMEDIA SYST, V9, P353, DOI 10.1007/s00530-003-0105-4
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Crow F. C., 1984, Computers & Graphics, V18, P207
   DATTA R., 2006, European Conference on Computer Vision, P288, DOI DOI 10.1007/11744078_23
   Dong WM, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618471
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fan Xin., 2003, P ACM MULTIMEDIA, P247
   Gallagher AC, 2005, 2ND CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, PROCEEDINGS, P460, DOI 10.1109/CRV.2005.84
   Girgensohn A., 2004, PROC ACM S USER INTE, P13
   Gooch B, 2001, SPRING EUROGRAP, P83
   Harel J., 2006, ADV NEURAL INF PROCE
   Hartley R, 2000, MULTIPLE VIEW GEOMET
   Hu Y., 2010, P IEEE C COMP VIS PA
   Jiang W, 2010, IEEE INT CON MULTI, P920, DOI 10.1109/ICME.2010.5582588
   Ke Y., 2006, P IEEE C COMP VIS PA
   KOPF S, 2006, P 14 ACM INT C MULT, P957
   Kopf Stephan., 2009, MM 09, P321
   Krages B., 2005, The Art of Composition
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Li CC, 2010, IEEE IMAGE PROC, P3221, DOI 10.1109/ICIP.2010.5651833
   Liu Hao., 2003, P ACM INT C MULTIMED, P148
   Liu LG, 2010, COMPUT GRAPH FORUM, V29, P469, DOI 10.1111/j.1467-8659.2009.01616.x
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Nack F., 2001, IEEE Multimedia, V8, P10, DOI 10.1109/93.959093
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Peng W.-T., 2008, P INT C ADV MULT MOD
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Rankin R., 1993, WILDERNESS LIGHT PHO
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Sandhaus P., 2010, P 18 ACM INT C MULTI, P1555, DOI DOI 10.1145/1873951.1874283
   Sandhaus P, 2011, LECT NOTES COMPUT SC, V6523, P84
   Santella A., 2006, Conference on Human Factors in Computing Systems. CHI2006, P771
   SAVAKIS AE, 2000, P SPIE HUMAN VISION, V5
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Suh Bongwon, 2003, P ACM S US INT SOFTW, P95, DOI DOI 10.1145/964696.964707
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   WANG C., 2008, P SPIE, V6803
   Wang J, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1711
   Wang J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1330511.1330512
NR 53
TC 35
Z9 41
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 783
EP 796
DI 10.1109/TMM.2012.2186122
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700011
DA 2024-07-18
ER

PT J
AU Subramanyam, AV
   Emmanuel, S
   Kankanhalli, MS
AF Subramanyam, A. V.
   Emmanuel, Sabu
   Kankanhalli, Mohan S.
TI Robust Watermarking of Compressed and Encrypted JPEG2000 Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compressed and encrypted domain watermarking; JPEG2000
ID INFORMATION; SCHEME; RC4
AB Digital asset management systems (DAMS) generally handle media data in a compressed and encrypted form. It is sometimes necessary to watermark these compressed encrypted media items in the compressed-encrypted domain itself for tamper detection or ownership declaration or copyright management purposes. It is a challenge to watermark these compressed encrypted streams as the compression process would have packed the information of raw media into a low number of bits and encryption would have randomized the compressed bit stream. Attempting to watermark such a randomized bit stream can cause a dramatic degradation of the media quality. Thus it is necessary to choose an encryption scheme that is both secure and will allow watermarking in a predictable manner in the compressed encrypted domain. In this paper, we propose a robust watermarking algorithm to watermark JPEG2000 compressed and encrypted images. The encryption algorithm we propose to use is a stream cipher. While the proposed technique embeds watermark in the compressed-encrypted domain, the extraction of watermark can be done in the decrypted domain. We investigate in detail the embedding capacity, robustness, perceptual quality and security of the proposed algorithm, using these watermarking schemes: Spread Spectrum (SS), Scalar Costa Scheme Quantization Index Modulation (SCS-QIM), and Rational Dither Modulation (RDM).
C1 [Subramanyam, A. V.; Emmanuel, Sabu] Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
   [Kankanhalli, Mohan S.] Nanyang Technol Univ, Sch Comp, Singapore, Singapore.
C3 Nanyang Technological University; Nanyang Technological University
RP Subramanyam, AV (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore, Singapore.
EM subr0021@ntu.edu.sg; asemmanuel@ntu.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019; Emmanuel, Sabu/A-3690-2011
OI Kankanhalli, Mohan/0000-0002-4846-2015; 
FU Agency for Science, Technology and Research (A*STAR), Singapore
   [0721010022]
FX This work was supported by the Agency for Science, Technology and
   Research (A*STAR), Singapore, under the project "Digital Rights
   Violation Detection for Digital Asset Management" (Project No:
   0721010022). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Oscar C. Au.
CR Abrardo A., 2006, IEE Proceedings-Information Security, V153, P107, DOI 10.1049/ip-ifs:20055152
   [Anonymous], NETW C CONS COMM
   Battisti F, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/938515
   Bianchi T, 2010, IEEE T INF FOREN SEC, V5, P180, DOI 10.1109/TIFS.2009.2036230
   Cancellaro M., 2008, P SOC PHOTO-OPT INS, V6819
   Castelluccia C, 2005, PROCEEDINGS OF MOBIQUITOUS 2005, P109
   Deng MN, 2009, MM&SEC'09: PROCEEDINGS OF THE 2009 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, P9
   Eggers JJ, 2003, IEEE T SIGNAL PROCES, V51, P1003, DOI 10.1109/TSP.2003.809366
   ELGAMAL T, 1985, IEEE T INFORM THEORY, V31, P469, DOI 10.1109/TIT.1985.1057074
   Engel D, 2009, MULTIMEDIA SYST, V15, P243, DOI 10.1007/s00530-008-0150-0
   ETSI/SAGE, 2006, SPEC 3GPP CONF INT A
   Fluhrer S. R., 2001, Fast Software Encryption. 7th International Workshop, FSE 2000. Proceedings (Lecture Notes in Computer Science Vol.1978), P19
   GOLDWASSER S, 1984, J COMPUT SYST SCI, V28, P270, DOI 10.1016/0022-0000(84)90070-9
   Hartung F, 1998, SIGNAL PROCESS, V66, P283, DOI 10.1016/S0165-1684(98)00011-5
   Hartung F, 1999, P SOC PHOTO-OPT INS, V3657, P147, DOI 10.1117/12.344665
   Hwang SO, 2004, J SYST SOFTWARE, V73, P533, DOI 10.1016/j.jss.2003.10.016
   Klein A, 2008, DESIGN CODE CRYPTOGR, V48, P269, DOI 10.1007/s10623-008-9206-6
   Lian SG, 2006, OPT ENG, V45, DOI 10.1117/1.2333510
   Mantin I, 2002, LECT NOTES COMPUT SC, V2355, P152
   Memon N, 2001, IEEE T IMAGE PROCESS, V10, P643, DOI 10.1109/83.913598
   Paillier P, 1999, LECT NOTES COMPUT SC, V1592, P223
   Papoulis A., 1965, Probability, random variables, and stochastic processes, V196
   Paul G, 2008, DESIGN CODE CRYPTOGR, V49, P123, DOI 10.1007/s10623-008-9177-7
   Pérez-González F, 2005, IEEE T SIGNAL PROCES, V53, P3960, DOI 10.1109/TSP.2005.855407
   Pérez-González F, 2003, IEEE T SIGNAL PROCES, V51, P960, DOI 10.1109/TSP.2003.809368
   Prins JP, 2007, EURASIP J INF SECUR, DOI 10.1155/2007/31340
   Rabbani M, 2002, SIGNAL PROCESS-IMAGE, V17, P3, DOI 10.1016/S0923-5965(01)00024-8
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   Schaefer G., 2009, MULTIMEDIA SYST, V15, P243
   Schneier B., 2015, APPL CRYPTOGRAPHY, VSecond
   Shannon CE, 1998, M D COMPUT, V15, P57
   Subramanyam AV, 2010, IEEE INT CON MULTI, P1315, DOI 10.1109/ICME.2010.5583571
   Sun QB, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P921
   Thomas T, 2009, IEEE T INF FOREN SEC, V4, P758, DOI 10.1109/TIFS.2009.2033229
   Trappe W, 2003, IEEE T SIGNAL PROCES, V51, P1069, DOI 10.1109/TSP.2003.809378
   Weber A. G., 1997, USC-SIPI Report, V315
   Wu HJ, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P869
   Zhi Li, 2007, 2007 International Conference on Multimedia & Expo, P627
NR 38
TC 73
Z9 82
U1 0
U2 40
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 703
EP 716
DI 10.1109/TMM.2011.2181342
PN 2
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700005
DA 2024-07-18
ER

PT J
AU Wu, J
   Worring, M
AF Wu, Jun
   Worring, Marcel
TI Efficient Genre-Specific Semantic Video Indexing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Efficiency; genre classification; genre-specific concept detection;
   semantic indexing
ID IMAGE CLASSIFICATION; CONCEPT ONTOLOGY; FEATURES
AB Large video collections such as YouTube contain many different video genres, while in many applications the user might be interested in one or two specific video genres only. Thus, when users are querying the system with a specific semantic concept like AnchorPerson, and MovieStars, they are likely aiming a genre specific instantiation of this concept. Existing methods treat this problem as a classical learning problem leading to unnecessarily complex models. We propose a framework to detect visual-based genre-specific concepts in a more efficient and accurate way. We do so by using a two-step framework distinguishing two different levels. Genre-specific concept models are trained based on a training set with data labeled at video level for genres and at shot level for semantic concepts. In the classification stage, video genre classification is applied first to reduce the entire data set to a relatively small subset. Then, the genre-specific concept models are applied to this subset only. Experiments have been conducted on a small 28-h data set for genre-specific concept detection and a 4168-h (80 031 videos) benchmark data set for genre-specific topic search. Experimental results show that our proposed two-step method is more efficient and effective than existing methods which do not consider the different semantic levels between video genres and semantic concepts for both the indexing and the search tasks. When filtering out 80% of the data set, the average performance loss is about 11.3% for genre-specific concept detection and 31.5% for genre-specific topic search, while the processing speed increases hundreds of times for different video genres.
C1 [Wu, Jun; Worring, Marcel] Univ Amsterdam, Inst Informat, Intelligent Syst Lab Amsterdam, NL-1098 XG Amsterdam, Netherlands.
C3 University of Amsterdam
RP Wu, J (corresponding author), Northwestern Polytech Univ, Sch Elect & Informat, Xian 710072, Peoples R China.
EM junwu@nwpu.edu.cn; m.worring@uva.nl
RI Worring, Marcel/JRW-7059-2023
OI Worring, Marcel/0000-0003-4097-4136
FU European Union [SIP-2007-TP-1317033]; National Science Foundation of
   China [61103062]
FX Manuscript received December 27, 2010; revised October 08, 2011;
   accepted October 19, 2011. Date of publication November 07, 2011; date
   of current version March 21, 2012. This was supported in part by the
   European Union Project "The Investigator's Dashboard (through the Safer
   Internet plus program SIP-2007-TP-1317033) and the National Science
   Foundation of China under Grant 61103062. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ketan Mayer-Patel.
CR [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P 17 ACM INT C MULT
   [Anonymous], 2007, CIVR '07
   [Anonymous], COMPUT VIS PATT RECO, DOI DOI 10.1109/CVPR.2009.5206557
   [Anonymous], 2009, C LAS EL PAC RIM OPT
   Baeza-Yates Ricardo, 1999, MODERN INFORM RETRIE, V463
   Borth D., 2009, P ACM MULT, P1111
   Cao J., 2009, ICTMCG09001 CHIN I C
   Carmel D., 2001, SIGIR Forum, P43, DOI 10.1145/383952.383958
   Fan J, 2007, IEEE T MULTIMEDIA, V9, P939, DOI 10.1109/TMM.2007.900143
   Fischer Stephan., 1995, ACM MULTIMEDIA, V95, P295, DOI DOI 10.1145/217279.215283
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Jiang YG, 2009, IEEE I CONF COMP VIS, P1420, DOI 10.1109/ICCV.2009.5459295
   Li H., 2010, P TRECVID WORKSH
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   NAPHADE MR, 2000, P NEUR INF PROC SYST, V13, P967
   Natsev A., 2009, P TRECVID WORKSH
   Natsev A., 2008, P TRECVID WORKSH
   Natsev Apostol., 2007, MULTIMEDIA 07, P991
   Persin M., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P339
   Rabinovich A, 2007, IEEE I CONF COMP VIS, P1237, DOI 10.1109/iccv.2007.4408986
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Smeaton AF, 2009, SIGNALS COMMUN TECHN, P151, DOI 10.1007/978-0-387-76569-3_6
   Smith JR, 1997, IEEE MULTIMEDIA, V4, P12, DOI 10.1109/93.621578
   Snoek C. G. M., 2010, P TRECVID WORKSH
   Snoek CGM, 2005, MULTIMED TOOLS APPL, V25, P5, DOI 10.1023/B:MTAP.0000046380.27575.a5
   Song Y, 2009, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON INNOVATION AND MANAGEMENT, VOLS I AND II, P1113, DOI 10.1145/1631272.1631524
   Sun C., 2010, P TRECVID WORKSH
   Uijlings JRR, 2010, IEEE T MULTIMEDIA, V12, P665, DOI 10.1109/TMM.2010.2052027
   Ulges A, 2011, IEEE T MULTIMEDIA, V13, P330, DOI 10.1109/TMM.2010.2101051
   Vailaya A, 2001, IEEE T IMAGE PROCESS, V10, P117, DOI 10.1109/83.892448
   Vailaya A, 1998, PATTERN RECOGN, V31, P1921, DOI 10.1016/S0031-3203(98)00079-X
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Wang B, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P353, DOI 10.1109/ICME.2006.262509
   Wu J., 2010, P CIVR, P266
   Xu LQ, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P485
   Yanagawa A., 2007, 22220068 COL U
   YANG L., 2007, MIR, P265
   Yu H., 2003, In International Conference on Image Processing, P24
   Yuan X, 2006, IEEE IMAGE PROC, P2905, DOI 10.1109/ICIP.2006.313037
   Zhang J, 2007, INT J COMPUT VISION, V73, P213, DOI 10.1007/s11263-006-9794-4
NR 45
TC 18
Z9 18
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2012
VL 14
IS 2
BP 291
EP 302
DI 10.1109/TMM.2011.2174969
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OZ
UT WOS:000302702500005
DA 2024-07-18
ER

PT J
AU Cui, P
   Wang, F
   Sun, LF
   Zhang, JW
   Yang, SQ
AF Cui, Peng
   Wang, Fei
   Sun, Li-Feng
   Zhang, Jian-Wei
   Yang, Shi-Qiang
TI A Matrix-Based Approach to Unsupervised Human Action Categorization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action categorization; joint matrix factorization; tensor
   representation; video analysis
ID DISCRIMINANT-ANALYSIS; TENSOR
AB Human action, as the basic unit of most human-relevant video content, bridges the gap between low-level visual features and high-level semantics. Human action recognition is of great significance in the applications of human-computer interaction, intelligent video surveillance, video retrieval and search. In this paper, we propose a novel unsupervised approach to mining categories from action video sequences, which consists of two modules: action representation for video data structurization and learning model for unsupervised categorization. In action representation, a novel view of video decomposition is presented. Videos are regarded as spatially distributed dynamic pixel time series, and these dynamic pixels are first quantized into pixel prototypes. After replacing the pixel time series with their corresponding prototype labels, the video sequences are compressed into two-dimensional action matrices. In the learning model, we put these matrices together to form an multi-action tensor, and propose the joint matrix factorization method to simultaneously cluster the pixel prototypes into pixel signatures, and matrices into action classes with the consideration of the duality between pixel clustering and action clustering. The approach is tested on public and popular Weizmann, and KTH datasets, and promising results are achieved.
C1 [Cui, Peng; Sun, Li-Feng; Yang, Shi-Qiang] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Wang, Fei] IBM TJ Watson Res Lab, Healthcare Transformat Grp, Hawthorne, NY 10532 USA.
   [Zhang, Jian-Wei] Univ Hamburg, Dept Informat, Hamburg, Germany.
C3 Tsinghua University; University of Hamburg
RP Cui, P (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM cuip@tsinghua.edu.cn; feiwang03@gmail.com; sunlf@mail.tsinghua.edu.cn;
   zhang@informatik.uni-hamburg.de; yangshq@mail.ts-inghua.edu.cn
RI Wang, Fei/HRA-7319-2023; yang, shiqiang/AAH-5484-2019; zhang,
   jian/HPD-1712-2023
OI Wang, Fei/0000-0001-9459-9461; 
FU National Natural Science Foundation of China [60933013, 60833009,
   61003097]; National Basic Research Program of China [2011CB302206]; Hong
   Kong Scholar Program; National Key Project Series [2011ZX01042-001-002]
FX This work was supported in part by the National Natural Science
   Foundation of China, No. 60933013, No. 60833009, and No. 61003097;
   National Basic Research Program of China, No. 2011CB302206; Hong Kong
   Scholar Program, and National Key Project Series, No.
   2011ZX01042-001-002. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Shuicheng Yan.
CR [Anonymous], P IEEE INT WORKSH VI
   [Anonymous], 2003, TIME SERIES FEATURE
   [Anonymous], P INT C COMP VIS
   [Anonymous], P INT C PATT REC
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Boiman O., 2005, P INT C COMP VIS
   CHEN X, 2006, P IEEE INT C DAT MIN
   Choi KH, 2005, IEEE T MULTIMEDIA, V7, P628, DOI 10.1109/TMM.2005.850964
   CHRIS D, 2006, P INT C KNOWL DISC D
   Cui P., 2008, P IEEE INT C DAT MIN
   Tao DC, 2007, KNOWL INF SYST, V13, P1, DOI 10.1007/s10115-006-0050-6
   Haibin Ling, 2007, 2007 11th IEEE International Conference on Computer Vision
   Hoey J, 2001, IEEE WORKSHOP ON DETECTION AND RECOGNITION OF EVENTS IN VIDEO, PROCEEDINGS, P99, DOI 10.1109/EVENT.2001.938872
   JIANG H., 2006, P IEEE C COMP VIS PA
   Ke Y., 2005, P INT C COMP VIS BEI
   Morup M., 2006, Decomposing the time-frequency representation of EEG using nonnegative matrix and multi-way factorization
   Niebles JC, 2008, INT J COMPUT VISION, V79, P299, DOI 10.1007/s11263-007-0122-4
   Tao DC, 2008, IEEE T CIRC SYST VID, V18, P1397, DOI 10.1109/TCSVT.2008.2002825
   Tao DC, 2008, NEUROCOMPUTING, V71, P1866, DOI 10.1016/j.neucom.2007.08.036
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   WANG L, 2007, P ICDM WORKSH KNOWL
   Wang M, 2009, IEEE T MULTIMEDIA, V11, P465, DOI 10.1109/TMM.2009.2012919
   Wang M, 2009, IEEE T CIRC SYST VID, V19, P733, DOI 10.1109/TCSVT.2009.2017400
   Xiang T, 2005, IEEE I CONF COMP VIS, P1238
   Xu W., 2003, P 26 ANN INT ACM SIG, P267
   Yao B., 2009, P IEEE C COMP VIS
   Yeffet L., 2009, P IEEE C COMP VIS
   Yi-Leh Wu, 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P488
NR 28
TC 19
Z9 23
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 102
EP 110
DI 10.1109/TMM.2011.2176110
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100010
DA 2024-07-18
ER

PT J
AU Kang, MK
   Ho, YS
AF Kang, Min-Koo
   Ho, Yo-Sung
TI Depth Video Coding Using Adaptive Geometry Based Intra Prediction for
   3-D Video Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Depth video coding; geometry-based block partitioning; intra prediction;
   3-D video system
AB Depth video coding is an essential part of 3-D video processing systems. Specifically, object boundary regions are important in depth video coding since these regions significantly affect the visual quality of a synthesized view. In this paper, we propose an efficient depth video coding method to determine precise intra prediction modes and thereby reduce the loss of boundary information. To achieve this objective, we analyze and exploit statistical and geometric characteristics of the depth video. Experimental results subsequently show that the proposed method performs better than the original intra prediction of H.264/AVC in terms of bit savings and rendering quality.
C1 [Kang, Min-Koo; Ho, Yo-Sung] Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Kwangju 500712, South Korea.
C3 Gwangju Institute of Science & Technology (GIST)
RP Kang, MK (corresponding author), Gwangju Inst Sci & Technol GIST, Sch Informat & Commun, Kwangju 500712, South Korea.
EM minkoo@gist.ac.kr; hoyo@gist.ac.kr
OI Kang, Min-Koo/0000-0003-1109-4818
FU Samsung Electronics Co., Ltd.; MKE under the ITRC
   [NIPA-2011-(C1090-1111-0003)]
FX This work was supported in part by Samsung Electronics Co., Ltd. and in
   part by MKE under the ITRC support program supervised by NIPA
   (NIPA-2011-(C1090-1111-0003)). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Ming-Ting Sun.
CR [Anonymous], 2010, N11630 ISOIEC JTC1SC
   [Anonymous], 2010, N11631 ISOIEC JTC1SC
   [Anonymous], 2009, M16090 ISOIEC JTC1SC
   [Anonymous], P IEEE ICIP
   Dai C., 2007, P IEEE INT C IM PROC
   DUDA RO, 1972, COMMUN ACM, V15, P11, DOI 10.1145/361237.361242
   Fehn C., 2002, PROC INT BROADCAST C, P357
   ISO/IEC JTC1/SC29/WG11, 2010, N11679 ISOIEC JTC1SC
   ISO/IEC JTC1/SC29/WG11, 2010, N11678 ISOIEC JTC1SC
   *ITU T, 2001, Q616 ITUT
   Joint Video Team, REF SOFTW VERS 14 2
   Kang MK, 2010, IEEE INT CON MULTI, P1230, DOI 10.1109/ICME.2010.5583876
   Kang MK, 2010, PROC SPIE, V7543, DOI 10.1117/12.837349
   Kim WS, 2010, PROC SPIE, V7543, DOI 10.1117/12.839030
   Merkle P., 2008, 2008 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video, P245, DOI 10.1109/3DTV.2008.4547854
   Merkle P., 2006, P INT C IM PROC ICIP, V1, pI201
   Mueller K., 2004, P PICT COD S SAN FRA, P15
   Na ST, 2008, IEEE INT SYMP CIRC S, P1400, DOI 10.1109/ISCAS.2008.4541689
   Oh H, 2006, LECT NOTES COMPUT SC, V4319, P898
   OH KJ, 2009, IET ELECT LETT, V45, P1
   Smolic A, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2161, DOI 10.1109/ICME.2006.262683
NR 21
TC 24
Z9 28
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 121
EP 128
DI 10.1109/TMM.2011.2169238
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100012
DA 2024-07-18
ER

PT J
AU Zhou, HY
   Li, XL
   Sadka, AH
AF Zhou, Huiyu
   Li, Xuelong
   Sadka, Abdul H.
TI Nonrigid Structure-From-Motion From 2-D Images Using Markov Chain Monte
   Carlo
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Markov chain Monte Carlo; nonrigid; structure from motion; uncalibrated
ID SHAPE; RECOVERY; ALGORITHM; TRACKING; MCMC; EM
AB In this paper we present a new method for simultaneously determining 3-D shape and motion of a nonrigid object from uncalibrated 2-D images without assuming the distribution characteristics. A nonrigid motion can be treated as a combination of a rigid rotation and a nonrigid deformation. To seek accurate recovery of deformable structures, we estimate the probability distribution function of the corresponding features through random sampling, incorporating an established probabilistic model. The fitting between the observation and the projection of the estimated 3-D structure will be evaluated using a Markov chain Monte Carlo based expectation maximization algorithm. Applications of the proposed method to both synthetic and real image sequences are demonstrated with promising results.
C1 [Zhou, Huiyu] Queens Univ Belfast, ECIT, Belfast BT3 9DT, Antrim, North Ireland.
   [Li, Xuelong] Chinese Acad Sci, Ctr OPT IMagery Anal & Learning OPTIMAL, State Key Lab Transient Opt & Photon, Xian Inst Opt & Precis Mech, Xian 710119, Peoples R China.
   [Sadka, Abdul H.] Brunel Univ, Sch Engn & Design, Uxbridge UB8 3PH, Middx, England.
C3 Queens University Belfast; State Key Laboratory of Transient Optics &
   Photonics; Chinese Academy of Sciences; Xi'an Institute of Optics &
   Precision Mechanics, CAS; Brunel University
RP Zhou, HY (corresponding author), Queens Univ Belfast, ECIT, Belfast BT3 9DT, Antrim, North Ireland.
EM H.Zhou@ecit.qub.ac.uk; xuelong_li@opt.ac.cn; Abdul.Sadka@brunel.ac.uk
RI Zhou, Huiyu/O-2692-2014; Li, Xuelong/Z-3785-2019; li,
   xiang/GWM-6319-2022; Li, Xuelong/ABF-3381-2020
OI Zhou, Huiyu/0000-0003-1634-9840; 
FU National Basic Research Program of China (973 Program) [2012CB316400];
   National Natural Science Foundation of China [61125106, 61072093]; State
   Key Laboratory of Industrial Control Technology (Zhejiang University)
   [ICT1105]; EPSRC [EP/K004379/1] Funding Source: UKRI
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) (Grant No. 2012CB316400), the National Natural
   Science Foundation of China (Grant Nos. 61125106 and 61072093), and the
   Open Project of State Key Laboratory of Industrial Control Technology
   (Zhejiang University) (Grant No. ICT1105). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Qingshan Liu.
CR [Anonymous], 2007, RUSHES PROJECT DELIV
   AZARBAYEJANI A, 1995, IEEE T PATTERN ANAL, V17, P562, DOI 10.1109/34.387503
   Barth AdamT., 2008, BodyNets'08: Proceedings of the ICST 3rd international conference on Body area networks, P1
   Bartoli A, 2008, J MATH IMAGING VIS, V31, P133, DOI 10.1007/s10851-007-0062-1
   Biernacki C, 2003, COMPUT STAT DATA AN, V41, P561, DOI 10.1016/S0167-9473(02)00163-9
   Bregler C, 2000, PROC CVPR IEEE, P690, DOI 10.1109/CVPR.2000.854941
   Carceroni R. L., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P520, DOI 10.1109/ICCV.1999.791267
   CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568
   Coleman DA, 2000, J COMPUT GRAPH STAT, V9, P672, DOI 10.2307/1391087
   Damen D, 2009, PROC CVPR IEEE, P927, DOI 10.1109/CVPRW.2009.5206636
   Davis M., 1993, Proceedings 1993 IEEE Symposium on Visual Languages (Cat. No.93TH0562-9), P196, DOI 10.1109/VL.1993.269596
   Dellaert F, 2003, MACH LEARN, V50, P45, DOI 10.1023/A:1020245811187
   Farenzena M., 2008, P EUR C COMP VIS, P196
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Forsyth D. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P660, DOI 10.1109/ICCV.1999.791288
   Ghahramani Z., 1996, Tech. Rep.
   Guskov I, 2002, WSCG'2002, VOLS I AND II, CONFERENCE PROCEEDINGS, P203
   Hartley R, 2008, LECT NOTES COMPUT SC, V5302, P276, DOI 10.1007/978-3-540-88682-2_22
   Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650
   Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223
   Meila M, 2001, MACH LEARN, V42, P9, DOI 10.1023/A:1007648401407
   Ohbuchi R., 2006, INT MULTIMEDIA C ACM, P163
   Pollefeys M, 1999, INT J COMPUT VISION, V32, P7, DOI 10.1023/A:1008109111715
   Qian G, 2004, INT J COMPUT VISION, V59, P5, DOI 10.1023/B:VISI.0000020669.68126.4b
   Seo Y, 2001, PROC CVPR IEEE, P1148
   Shaji Appu, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563071
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Szeliski R., 1994, Journal of Visual Communication and Image Representation, V5, P10, DOI 10.1006/jvci.1994.1002
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684
   Tomasi C, 1991, DETECTION TRACKING P
   Torresani L, 2004, ADV NEUR IN, V16, P1555
   Torresani L, 2001, PROC CVPR IEEE, P493
   Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752
   Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239
   VONESH EF, 1987, BIOMETRICS, V43, P617, DOI 10.2307/2531999
   Xiao J, 2004, PROC CVPR IEEE, P668
   Yan JY, 2008, IEEE T PATTERN ANAL, V30, P865, DOI 10.1109/TPAMI.2007.70739
   Zhou HY, 2008, INT J PATTERN RECOGN, V22, P279, DOI 10.1142/S0218001408006259
   Zhou HY, 2009, IEEE IMAGE PROC, P4309, DOI 10.1109/ICIP.2009.5413666
NR 40
TC 11
Z9 12
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2012
VL 14
IS 1
BP 168
EP 177
DI 10.1109/TMM.2011.2170406
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 924OL
UT WOS:000302701100016
DA 2024-07-18
ER

PT J
AU Chang, CH
   Liang, CK
   Chuang, YY
AF Chang, Che-Han
   Liang, Chia-Kai
   Chuang, Yung-Yu
TI Content-Aware Display Adaptation and Interactive Editing for
   Stereoscopic Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-aware image retargeting; depth adaptation; stereoscopic image
   editing
AB We propose a content-aware stereoscopic image display adaptation method which simultaneously resizes a binocular image to the target resolution and adapts its depth to the comfort zone of the display while preserving the perceived shapes of prominent objects. This method does not require depth information or dense correspondences. Given the specification of the target display and a sparse set of correspondences, our method efficiently deforms the input stereoscopic images for display adaptation by solving a least-squares energy minimization problem. This can be used to adjust stereoscopic images to fit displays with different real estates, aspect ratios and comfort zones. In addition, with slight modifications to the energy function, our method allows users to interactively adjust the sizes, locations and depths of the selected objects, giving users aesthetic control for depth perception. User studies show that the method is effective at editing depth and reducing occurrences of diplopia and distortions.
C1 [Chang, Che-Han; Chuang, Yung-Yu] Natl Taiwan Univ, Taipei 10617, Taiwan.
   [Liang, Chia-Kai] Refocus Imaging, Mountain View, CA 94041 USA.
C3 National Taiwan University
RP Chang, CH (corresponding author), Natl Taiwan Univ, Taipei 10617, Taiwan.
EM frank@cmlab.csie.ntu.edu.tw; liangck@gmail.com; cyy@csie.ntu.edu.tw
RI ; Liang, Chia-Kai/L-9973-2019
OI Chuang, Yung-Yu/0000-0002-1383-0017; Liang, Chia-Kai/0000-0003-3649-2505
FU Cyber-Link Technical Elite Fellowship; National Science Council
   [99-2628-E-002-015, 99-2622-E-002-026-CC2]
FX The work of C. H. Chang was supported in part by a Cyber-Link Technical
   Elite Fellowship. This work was supported in part by the National
   Science Council under Grant 99-2628-E-002-015 and Grant
   99-2622-E-002-026-CC2. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Ketan
   Mayer-Patel.
CR [Anonymous], P SPIE
   [Anonymous], 2001, Robotica, DOI DOI 10.1017/S0263574700223217
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Brown M, 2005, PROC CVPR IEEE, P510
   Carroll R, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778864
   Carroll R, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531349
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   KIM WJ, 2009, P STER DISPL APPL 20
   Kooi FL, 2004, DISPLAYS, V25, P99, DOI 10.1016/j.displa.2004.07.004
   Koppal SJ, 2011, IEEE COMPUT GRAPH, V31, P20, DOI 10.1109/MCG.2010.37
   Lambooij M, 2009, J IMAGING SCI TECHN, V53, DOI 10.2352/J.ImagingSci.Technol.2009.53.3.030201
   LAMBOOIJ MTM, 2007, P SOC PHOTO-OPT INS, V6490, P49001
   Lang M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778812
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mendiburu Bernard., 2009, 3D Movie Making: Stereoscopic Digital Cinema From Scrip to Screen
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   SHAMIR A, 2009, SIGGRAPH ASIA 2009 C
   Wang C, 2008, PROC SPIE, V6803, DOI 10.1117/12.767702
   Wang YS, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618473
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wartell Z, 2002, IEEE T VIS COMPUT GR, V8, P129, DOI 10.1109/2945.998666
   Wheatstone Charles., PHILOS T ROYAL SOC L, V128, P371
   Zhang GX, 2009, COMPUT GRAPH FORUM, V28, P1897, DOI 10.1111/j.1467-8659.2009.01568.x
   Zhang Y, 2010, LECT NOTES COMPUT SC, V5916, P314, DOI 10.1007/978-3-642-11301-7_33
NR 27
TC 105
Z9 117
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2011
VL 13
IS 4
BP 589
EP 601
DI 10.1109/TMM.2011.2116775
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 805EX
UT WOS:000293710300001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lo, HY
   Wang, JC
   Wang, HM
   Lin, SD
AF Lo, Hung-Yi
   Wang, Ju-Chiang
   Wang, Hsin-Min
   Lin, Shou-De
TI Cost-Sensitive Multi-Label Learning for Audio Tag Annotation and
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio tag annotation; audio tag retrieval; cost-sensitive learning;
   multi-label; tag count
ID MUSIC
AB Audio tags correspond to keywords that people use to describe different aspects of a music clip. With the explosive growth of digital music available on the Web, automatic audio tagging, which can be used to annotate unknown music or retrieve desirable music, is becoming increasingly important. This can be achieved by training a binary classifier for each tag based on the labeled music data. Our method that won the MIREX 2009 audio tagging competition is one of this kind of methods. However, since social tags are usually assigned by people with different levels of musical knowledge, they inevitably contain noisy information. By treating the tag counts as costs, we can model the audio tagging problem as a cost-sensitive classification problem. In addition, tag correlation information is useful for automatic audio tagging since some tags often co-occur. By considering the co-occurrences of tags, we can model the audio tagging problem as a multi-label classification problem. To exploit the tag count and correlation information jointly, we formulate the audio tagging task as a novel cost-sensitive multi-label (CSML) learning problem and propose two solutions to solve it. The experimental results demonstrate that the new approach outperforms our MIREX 2009 winning method.
C1 [Lo, Hung-Yi; Lin, Shou-De] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Lo, Hung-Yi; Wang, Ju-Chiang; Wang, Hsin-Min] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
   [Wang, Ju-Chiang] Natl Taiwan Univ, Dept Elect Engn, Taipei 106, Taiwan.
C3 National Taiwan University; Academia Sinica - Taiwan; National Taiwan
   University
RP Lo, HY (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM hungyi@iis.sinica.edu.tw; asriver@iis.sinica.edu.tw;
   whm@iis.sinica.edu.tw; sdlin@csie.ntu.edu.tw
RI Wang, Hsin-Min/ABA-8747-2020
OI Wang, Hsin-Min/0000-0003-3599-5071; LIN, SHOU-DE/0000-0001-9970-1250
FU National Science Council of Taiwan [NSC99-2631-H-001-020]
FX This work was supported in part by the Taiwan e-Learning and Digital
   Archives Program (TELDAP) sponsored by the National Science Council of
   Taiwan under Grant: NSC99-2631-H-001-020. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zhu Liu.
CR [Anonymous], SIMPLE COST SENSITIV
   [Anonymous], ADV NEUR INF PROC SY
   Bergstra J, 2006, MACH LEARN, V65, P473, DOI 10.1007/s10994-006-9019-7
   CAO L, 2009, P ACM INT C MULT
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Foote J., 2003, P SPIE STORAGE RETRI
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   HOFFMAN M, 2009, P INT SOC MUS INF RE
   HU X, 2009, P INT SOC MUS INF RE
   Lamere P, 2008, J NEW MUSIC RES, V37, P101, DOI 10.1080/09298210802479284
   Lartillot O., 2007, INT C DIG AU EFF
   Li XR, 2009, IEEE T MULTIMEDIA, V11, P1310, DOI 10.1109/TMM.2009.2030598
   Lin HT, 2007, MACH LEARN, V68, P267, DOI 10.1007/s10994-007-5018-6
   LIN HT, 2010, P WORKSH MACH LEARN
   LO HY, 2010, P IEEE INT C MULT EX
   Mandel M. I., 2007, P INT C MUS INF RETR
   Mandel M, 2008, J NEW MUSIC RES, V37, P151, DOI 10.1080/09298210802479300
   MIOTTO R, 2010, P INT SOC MUS INF RE
   NESS S, 2009, P ACM INT C MULT
   PLATT J, 1998, ADV LARGE MARGIN CLA
   Sun YM, 2007, PATTERN RECOGN, V40, P3358, DOI 10.1016/j.patcog.2007.04.009
   TINGLE D, 2010, P ACM INT C MULT INF
   TSOUMAKAS G, 2007, P EUR C MACH LEARN
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Turnbull D, 2008, IEEE T AUDIO SPEECH, V16, P467, DOI 10.1109/TASL.2007.913750
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   ZADROZNY B, 2003, P IEEE INT C DAT MIN
NR 27
TC 66
Z9 84
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 518
EP 529
DI 10.1109/TMM.2011.2129498
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700011
DA 2024-07-18
ER

PT J
AU Mansour, H
   Nasiopoulos, P
   Krishnamurthy, V
AF Mansour, Hassan
   Nasiopoulos, Panos
   Krishnamurthy, Vikram
TI Rate and Distortion Modeling of CGS Coded Scalable Video Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Coarse grain scalability; rate-distortion modeling; scalable video
   coding
ID BIT ALLOCATION
AB In this paper, we derive single layer and scalable video rate and distortion models for video bitstreams encoded using the coarse grain quality scalability (CGS) feature of the scalable extension of H. 264/AVC. In these models, we assume the source is Laplacian distributed and compensate for errors in the distribution assumption by linearly scaling the Laplacian parameter lambda. Moreover, we present simplified approximations of the derived models that allow for a run-time calculation of sequence dependent model constants. Our models use the mean absolute difference (MAD) of the prediction residual signal and the encoder quantization parameter (QP) as input parameters. Consequently, we are able to estimate the residual MAD, bitrate, and distortion of a future video frame at any QP value and for both base-layer and CGS layer packets. We also present simulation results that demonstrate the accuracy of the proposed models.
C1 [Mansour, Hassan] Univ British Columbia, Dept Math, Vancouver, BC V6T 1Z4, Canada.
   [Mansour, Hassan] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
   [Nasiopoulos, Panos; Krishnamurthy, Vikram] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia; University of British Columbia;
   University of British Columbia
RP Mansour, H (corresponding author), Univ British Columbia, Dept Math, Vancouver, BC V6T 1Z4, Canada.
EM has-sanm@cs.ubc.ca; panosn@ece.ubc.ca; vikramk@ece.ubc.ca
CR Dai M, 2004, IEEE IMAGE PROC, P1093
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P840, DOI 10.1109/TCSVT.2002.804883
   *ISO IEC, 2006, 1449610 ISO IEC JTC
   *ISO IEC, 2004, 144922 ISO IEC JTC
   *ISO IEC JTC, 2007, 1SC29WG11N8964 ISOIE
   Kamaci N, 2005, IEEE T CIRC SYST VID, V15, P994, DOI 10.1109/TCSVT.2005.852400
   Kwon DK, 2007, IEEE T CIRC SYST VID, V17, P517, DOI 10.1109/TCSVT.2007.894053
   Li ZG, 2004, IEEE IMAGE PROC, P745
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   MANSOUR H, 2008, P IEEE PERS IND MOB, P1
   MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   SU Y, 2005, P IEEE INT S CIRC SY, V2, P1234
   Sullivan G., 2003, JTC1SC29WG11 ISOIEC
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tu YK, 2007, IEEE T CIRC SYST VID, V17, P530, DOI 10.1109/TCSVT.2007.894041
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhu X., 2006, P IEEE INT C MULT EX
NR 19
TC 17
Z9 20
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 165
EP 180
DI 10.1109/TMM.2010.2099648
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800001
DA 2024-07-18
ER

PT J
AU Wang, DQ
   Yeo, CK
AF Wang, Danqi
   Yeo, Chai Kiat
TI Superchunk-Based Efficient Search in P2P-VoD System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content discovery; peer-to-peer; video-on-demand
ID OVERLAY; VOD; SERVICE
AB In this paper, we seek to provide reliable and fast content discovery in peer-to-peer (P2P) video-on-demand (VoD) system to enable user interactivity under peer dynamics. We first identify two characteristics of content discovery in P2P-VoD: real-time constraints and limited local cache. Tapping on these properties, we propose a hybrid content discovery mechanism: SUpeRchunk-based eFficient search Network (SURFNet). SURFNet divides movies into superchunks and chunks. Stable peers that are likely to have longer lifespans in the system are used to construct an AVL tree to provide superchunk-level data availability information. Peers storing the same superchunk are grouped into a holder-chain, which is then attached to a node in the AVL tree. This structured overlay is further extended by a gossip-based unstructured network for chunk-level information exchange and data transmission. Since the AVL tree consists of only stable peers, it provides a reliable backbone even in highly dynamic environment. Our analysis and simulation results show that SURFNet supports nearly-constant and logarithmic search time for seeking within a video and jumping to a different video, respectively.
C1 [Wang, Danqi; Yeo, Chai Kiat] Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Wang, DQ (corresponding author), Nanyang Technol Univ, Ctr Multimedia & Network Technol, Sch Comp Engn, Singapore 639798, Singapore.
EM s080004@ntu.edu.sg; asckyeo@ntu.edu.sg
RI Yeo, Chai Kiat/A-3683-2011
OI Yeo, Chai Kiat/0000-0002-7618-1472
CR Annapureddy S, 2007, IEEE INFOCOM SER, P2571, DOI 10.1109/INFCOM.2007.323
   [Anonymous], 2007, WORKSHOP PEER TO PEE
   [Anonymous], NETWORK SIMULATOR NS
   [Anonymous], 2001, Pastry: Scalable, decentralized object location, and routing for large-scale peer-to-peer systems, DOI DOI 10.1007/3-540-45518-3_18
   Aspnes J, 2007, ACM T ALGORITHMS, V3, DOI 10.1145/1290672.1290674
   BISHOP M, 2006, P 25 IEEE INT C COMP, P1
   Cheng B, 2007, IEEE ICC, P1698, DOI 10.1109/ICC.2007.284
   Cheng B, 2008, EUROSYS'08: PROCEEDINGS OF THE EUROSYS 2008 CONFERENCE, P109, DOI 10.1145/1357010.1352605
   Chi HC, 2007, IEEE J SEL AREA COMM, V25, P119, DOI 10.1109/JSAC.2007.070112
   Cui Y, 2004, IEEE J SEL AREA COMM, V22, P91, DOI 10.1109/JSAC.2003.818799
   Do TT, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1467, DOI 10.1109/ICC.2004.1312755
   Guo Y., 2003, Proceedings of the 12th International Conference on World Wide Web, P301, DOI DOI 10.1145/775152.775195
   Guo Y, 2008, CONSUM COMM NETWORK, P452, DOI 10.1109/ccnc08.2007.107
   He YF, 2009, IEEE T MULTIMEDIA, V11, P509, DOI 10.1109/TMM.2009.2012921
   He Y, 2009, IEEE T PARALL DISTR, V20, P528, DOI 10.1109/TPDS.2008.102
   Huang Y, 2008, ACM SIGCOMM COMP COM, V38, P375, DOI 10.1145/1402946.1403001
   JAGADISH HV, 2005, VLDB, P661
   Liu JC, 2006, MULTIMED TOOLS APPL, V29, P211, DOI 10.1007/s11042-006-0013-7
   Maymounkov Petar., 2002, IPTPS 01, P53
   Ratnasamy S, 2001, ACM SIGCOMM COMP COM, V31, P161, DOI 10.1145/964723.383072
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Wang D., 2009, P IEEE GLOBECOM, P1
   Wang D, 2008, IEEE T PARALL DISTR, V19, P503, DOI 10.1109/TPDS.2007.70748
   Wang F, 2008, IEEE INFOCOM SER, P36
   Xu CQ, 2008, IEEE ICC, P1797, DOI 10.1109/ICC.2008.345
   Yang X., 2009, P 8 INT WORKSH PEER
   Yiu WPK, 2007, IEEE J SEL AREA COMM, V25, P1717, DOI 10.1109/JSAC.2007.071210
   Zhao BY, 2004, IEEE J SEL AREA COMM, V22, P41, DOI 10.1109/JSAC.2003.818784
   Zheng C., 2005, PROC ACM WORKSHOP AD, P29
NR 29
TC 14
Z9 14
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2011
VL 13
IS 2
BP 376
EP 387
DI 10.1109/TMM.2011.2106485
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 738SG
UT WOS:000288661800018
DA 2024-07-18
ER

PT J
AU Choi, JY
   De Neve, W
   Plataniotis, KN
   Ro, YM
AF Choi, Jae Young
   De Neve, Wesley
   Plataniotis, Konstantinos N.
   Ro, Yong Man
TI Collaborative Face Recognition for Improved Face Annotation in Personal
   Photo Collections Shared on Online Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaboration; face annotation; face recognition; online social network;
   personal photos; social context
ID DISCRIMINANT-ANALYSIS; ENSEMBLE; SAMPLE
AB Using face annotation for effective management of personal photos in online social networks (OSNs) is currently of considerable practical interest. In this paper, we propose a novel collaborative face recognition (FR) framework, improving the accuracy of face annotation by effectively making use of multiple FR engines available in an OSN. Our collaborative FR framework consists of two major parts: selection of FR engines and merging (or fusion) of multiple FR results. The selection of FR engines aims at determining a set of personalized FR engines that are suitable for recognizing query face images belonging to a particular member of the OSN. For this purpose, we exploit both social network context in an OSN and social context in personal photo collections. In addition, to take advantage of the availability of multiple FR results retrieved from the selected FR engines, we devise two effective solutions for merging FR results, adopting traditional techniques for combining multiple classifier results. Experiments were conducted using 547 991 personal photos collected from an existing OSN. Our results demonstrate that the proposed collaborative FR method is able to significantly improve the accuracy of face annotation, compared to conventional FR approaches that only make use of a single FR engine. Further, we demonstrate that our collaborative FR framework has a low computational cost and comes with a design that is suited for deployment in a decentralized OSN.
C1 [Choi, Jae Young; De Neve, Wesley; Ro, Yong Man] Korea Adv Inst Sci & Technol, Dept Elect Engn, Image & Video Syst Lab, Taejon 305701, South Korea.
   [Plataniotis, Konstantinos N.] Univ Toronto, Dept Elect & Comp Engn, Multimedia Lab, Toronto, ON M5S 3G4, Canada.
C3 Korea Advanced Institute of Science & Technology (KAIST); University of
   Toronto
RP Choi, JY (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Image & Video Syst Lab, Taejon 305701, South Korea.
EM jygchoi@kaist.ac.kr; wesley.deneve@kaist.ac.kr; kostas@comm.utoronto.ca;
   ymro@ee.kaist.ac.kr
RI Ro, Yong Man/ABF-6817-2020; De Neve, Wesley Marcel/C-6480-2008; Ro, Yong
   Man/C-1731-2011; Plataniotis, Konstantinos/E-8471-2014
OI Ro, Yong Man/0000-0001-5306-6853; De Neve, Wesley
   Marcel/0000-0002-8190-3839; Plataniotis,
   Konstantinos/0000-0003-3647-5473
FU National Research Foundation (NRF) of Korea [NRF-D00070]
FX Manuscript received December 03, 2009; revised June 28, 2010 and
   September 10, 2010; accepted September 17, 2010. Date of publication
   October 14, 2010; date of current version January 19, 2011. This work
   was supported by the National Research Foundation (NRF) of Korea under
   grant NRF-D00070). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Alex C. Kot.
CR [Anonymous], P INT ACM IEEE CS JO
   [Anonymous], INT J IMAGE GRAPHICS
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   BENDER M, 2008, IEEE DATA ENG B, V30, P51
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Choi JY, 2010, IEEE T CIRC SYST VID, V20, P1292, DOI 10.1109/TCSVT.2010.2058470
   Choi JY, 2009, IEEE T SYST MAN CY B, V39, P1217, DOI 10.1109/TSMCB.2009.2014245
   CHOI K, 2008, P IEEE INT C AUT FAC
   Clauset A, 2009, SIAM REV, V51, P661, DOI 10.1137/070710111
   GURSEL A, 2009, P INT JOINT C ART IN
   Jain A, 2005, PATTERN RECOGN, V38, P2270, DOI 10.1016/j.patcog.2005.01.012
   Jain AK, 2004, IEEE T CIRC SYST VID, V14, P4, DOI 10.1109/TCSVT.2003.818349
   Jiang XD, 2008, IEEE T PATTERN ANAL, V30, P383, DOI 10.1109/TPAMI.2007.70708
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   LERMAN K, 2007, P INT JOINT C ART IN
   Liu CL, 2005, PATTERN RECOGN, V38, P11, DOI 10.1016/j.patcog.2004.05.013
   LU J, 2009, P IEEE INT C COMP VI
   Lu JW, 2006, IEEE T NEURAL NETWOR, V17, P166, DOI 10.1109/TNN.2005.860853
   Lu JW, 2003, PATTERN RECOGN LETT, V24, P3079, DOI 10.1016/S0167-8655(03)00167-3
   Lu JW, 2003, IEEE T NEURAL NETWOR, V14, P117, DOI 10.1109/TNN.2002.806629
   Moghaddam B, 2000, PATTERN RECOGN, V33, P1771, DOI 10.1016/S0031-3203(99)00179-X
   NAAMAN M, 2005, P ACM INT C ACM IEEE
   O'Hare N, 2009, IEEE T MULTIMEDIA, V11, P220, DOI 10.1109/TMM.2008.2009679
   PERLIBAKAS V, 2004, PATTERN RECOGNIT LET, V25, P1421
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   STONE Z, 2008, P IEEE INT C COMP VI
   Su Y, 2009, IEEE T IMAGE PROCESS, V18, P1885, DOI 10.1109/TIP.2009.2021737
   TRON R, 2008, P 8 WORKSH OMN VIS C
   TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71
   VIOLA P, 2001, P IEEE INT C CIVR
   VLACHOU A, 2008, P LNCS INT C AD MULT
   Wang J, 2006, PATTERN RECOGN, V39, P1746, DOI 10.1016/j.patcog.2006.03.010
   WANG P, 2005, P IEEE INT C COMP VI
   WU P, 2009, P ACM INT C MULT
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883
   YEUNG CA, 2009, P INT JOINT C W3C WO
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zhu JK, 2008, IEEE T MULTIMEDIA, V10, P86, DOI 10.1109/TMM.2007.911245
NR 39
TC 44
Z9 54
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 14
EP 28
DI 10.1109/TMM.2010.2087320
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900003
DA 2024-07-18
ER

PT J
AU Oh, SM
   Kim, JH
AF Oh, Sung-Min
   Kim, Jae-Hyun
TI Application-Aware Design to Enhance System Efficiency for VoIP Services
   in BWA Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer framework; dynamic resource request; resource efficiency;
   voice over Internet protocol (VoIP) service
AB This paper has designed a cross-layer framework for voice over Internet protocol (VoIP) services in IEEE 802.16 systems. It uses the application session information of the session description protocol to generate the quality of service parameters in IEEE 802.16 systems. This feature allows the system to efficiently allocate the radio resource because it can exactly estimate the properties of VoIP services such as packet-size and packet-generation-interval. In other words, the cross-layer framework is expected to achieve a novel resource request scheme for a VoIP service that dynamically assigns the radio resource. This paper has analyzed the maximum number of supportable VoIP users for the resource request schemes in terms of the packet-generation-interval in the silent-period, the duration of the silent-period, and the major VoIP speech codec. The numerical results show that the proposed scheme can efficiently support the VoIP services for the various communication environments. Particularly, it can improve the maximum number of supportable VoIP users by 14 similar to 93% compared to an extended real-time polling service.
C1 [Oh, Sung-Min; Kim, Jae-Hyun] Ajou Univ, Sch Elect & Comp Engn, Suwon 441749, South Korea.
C3 Ajou University
RP Oh, SM (corresponding author), Ajou Univ, Sch Elect & Comp Engn, Suwon 441749, South Korea.
EM smallb01@ajou.ac.kr; jkim@ajou.ac.kr
FU MKE/KEIT [KI001822]; MKE, Korea [NIPA-2010-(C1090-1021-0011)]
FX Manuscript received May 18, 2010; revised September 08, 2010; accepted
   October 29, 2010. Date of publication November 18, 2010; date of current
   version January 19, 2011. This work was supported in part by the IT R&D
   program of MKE/KEIT (KI001822, Research on Ubiquitous Mobility
   Management Methods for Higher Service Availability) and in part by the
   MKE, Korea, under the ITRC support program supervised by the NIPA
   (NIPA-2010-(C1090-1021-0011)). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Paal
   Halvorsen.
CR *3GPP, 2001, 26201 3GPP TS
   *3GPP, 2002, 26092 3GPP TS
   *3GPP2, 2004, CS0014A 3GPP2
   [Anonymous], 80216E2009 IEEE
   Chu GS, 2002, 2002 INTERNATIONAL CONFERENCE ON COMMUNICATIONS, CIRCUITS AND SYSTEMS AND WEST SINO EXPOSITION PROCEEDINGS, VOLS 1-4, P435, DOI 10.1109/ICCCAS.2002.1180654
   HANDLEY M, 1998, 2327 RFC
   HONG SE, 2006, P VEH TECHN C, V3, P1226
   *ITU T, 1996, G7231 ITUT
   *ITU T, 2007, G729 ITUT
   *ITU T, 2000, G711 ITUT
   Kwon T, 2005, IEEE COMMUN MAG, V43, P136, DOI 10.1109/MCOM.2005.1561931
   Lee H, 2005, IEEE COMMUN LETT, V9, P691, DOI 10.1109/LCOMM.2005.08008
   LEE H, 2004, P VTC 2004 FALL SEPT, V5, P3070
   Oh SM, 2005, THIRD IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, WORKSHOPS, P215
   Oh SM, 2008, IEEE COMMUN LETT, V12, P374, DOI 10.1109/LCOMM.2008.072098
   *SP RFI, 2000, V11I04000407 SPRFT
   SRINIVASAN R, 2007, 80216M IEEE
   *WIMAX FOR NETW AR, 2009, T33101R015V01C WIMAX
   Wongthavarawat K, 2003, INT J COMMUN SYST, V16, P81, DOI 10.1002/dac.581
   2006, P WIMAX FOR AUG
NR 20
TC 6
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 143
EP 154
DI 10.1109/TMM.2010.2093512
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900015
DA 2024-07-18
ER

PT J
AU Xiang, Y
   Peng, DZ
   Natgunanathan, I
   Zhou, WL
AF Xiang, Yong
   Peng, Dezhong
   Natgunanathan, Iynkaran
   Zhou, Wanlei
TI Effective Pseudonoise Sequence and Decoding Function for
   Imperceptibility and Robustness Enhancement in Time-Spread Echo-Based
   Audio Watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio watermarking; autocorrelation; echo hiding; echo kernel;
   time-spread echo
ID SECURE
AB This paper proposes an effective pseudonoise (PN) sequence and the corresponding decoding function for time-spread echo-based audio watermarking. Different from the traditional PN sequence used in time-spread echo hiding, the proposed PN sequence has two features. Firstly, the echo kernel resulting from the new PN sequence has frequency characteristics with smaller magnitudes in perceptually significant region. This leads to higher perceptual quality. Secondly, the correlation function of the new PN sequence has three times more large peaks than that of the existing PN sequence. Based on this feature, we propose a new decoding function to improve the robustness of time-spread echo-based audio watermarking. The effectiveness of the proposed PN sequence and decoding function is illustrated by theoretical analysis, simulation examples, and listening test.
C1 [Xiang, Yong; Natgunanathan, Iynkaran] Deakin Univ, Sch Engn, Geelong, Vic 3217, Australia.
   [Peng, Dezhong] Deakin Univ, Sch Engn, Melbourne, Vic 3125, Australia.
   [Zhou, Wanlei] Deakin Univ, Sch Informat Technol, Melbourne, Vic 3125, Australia.
C3 Deakin University; Deakin University; Deakin University
RP Xiang, Y (corresponding author), Deakin Univ, Sch Engn, Waurn Ponds Campus, Geelong, Vic 3217, Australia.
EM yxiang@deakin.edu.au; pengdz@scu.edu.cn; inat@deakin.edu.au;
   wanlei.zhou@deakin.edu.au
RI Zhou, Wanlei/HKV-8022-2023
OI Zhou, Wanlei/0000-0002-1680-2521
FU Australian Research Council [DP0773446, DP1095498]; National Natural
   Science Foundation of China [60802064]; National Basic Research Program
   of China (973 Program) [2011CB302201]; Australian Research Council
   [DP0773446, DP1095498] Funding Source: Australian Research Council
FX Manuscript received January 19, 2010; revised May 06, 2010 and July 28,
   2010; accepted July 31, 2010. Date of publication September 27, 2010;
   date of current version January 19, 2011. This work was supported in
   part by the Australian Research Council under grants DP0773446 and
   DP1095498, the National Natural Science Foundation of China under grant
   60802064, and the National Basic Research Program of China (973 Program)
   under grant 2011CB302201. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Z. Jane
   Wang.
CR [Anonymous], 2016, HDB APPL CRYPTOGRAPH
   [Anonymous], 1997, P 5 EUR C SPEECH COM
   Arnold M, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1013, DOI 10.1109/ICME.2000.871531
   Bassia P, 2001, IEEE T MULTIMEDIA, V3, P232, DOI 10.1109/6046.923822
   Bender W., 1996, IBM SYST J, V35, DOI DOI 10.1147/SJ.353.0313
   Boney L., 1996, P EUR SIGN PROC C TR
   Chen OTC, 2008, IEEE T AUDIO SPEECH, V16, P629, DOI 10.1109/TASL.2007.913022
   Çiloglu T, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1017, DOI 10.1109/ICME.2000.871532
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Cvejic N, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P227, DOI 10.1109/ASPAA.2001.969584
   Erfani Y, 2009, DIGIT SIGNAL PROCESS, V19, P809, DOI 10.1016/j.dsp.2009.04.003
   Greenberg S., 2004, SPEECH PROCESSING AU
   Gruhl D, 1996, PROCEEDING 1 INFOROM, P295
   Kim HJ, 2003, IEEE T CIRC SYST VID, V13, P885, DOI 10.1109/TCSVT.2003.815950
   Kirovski D, 2001, INT CONF ACOUST SPEE, P1345, DOI 10.1109/ICASSP.2001.941177
   Ko BS, 2005, IEEE T MULTIMEDIA, V7, P212, DOI 10.1109/TMM.2005.843366
   Lemma AN, 2003, IEEE T SIGNAL PROCES, V51, P1088, DOI 10.1109/TSP.2003.809372
   Lie WN, 2006, IEEE T MULTIMEDIA, V8, P46, DOI 10.1109/TMM.2005.861292
   Liu QZ, 2009, IEEE T INF FOREN SEC, V4, P359, DOI 10.1109/TIFS.2009.2024718
   Liu Z, 2003, IEEE T CIRC SYST VID, V13, P801, DOI 10.1109/TCSVT.2003.815960
   Malvar HS, 2003, IEEE T SIGNAL PROCES, V51, P898, DOI 10.1109/TSP.2003.809385
   Oh HO, 2001, INT CONF ACOUST SPEE, P1341, DOI 10.1109/ICASSP.2001.941176
   Peng DZ, 2009, IEEE T SIGNAL PROCES, V57, P809, DOI 10.1109/TSP.2008.2007604
   Swanson MD, 1998, SIGNAL PROCESS, V66, P337, DOI 10.1016/S0165-1684(98)00014-0
   Wang HQ, 2008, APPL ACOUST, V69, P868, DOI 10.1016/j.apacoust.2007.06.001
   Xiang Y, 2006, IEEE T CIRCUITS-II, V53, P758, DOI 10.1109/TCSII.2006.875382
   Xiang Y, 2010, IEEE T NEURAL NETWOR, V21, P82, DOI 10.1109/TNN.2009.2034518
   Xu CS, 1999, J AUDIO ENG SOC, V47, P805
   Yeo IK, 2003, IEEE T SPEECH AUDI P, V11, P381, DOI 10.1109/TSA.2003.812145
   Youn D.H., 2001, P IEEE INT C MULT EX, P433
   ZWICKER E, 1980, J ACOUST SOC AM, V68, P1523, DOI 10.1121/1.385079
NR 31
TC 62
Z9 65
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2011
VL 13
IS 1
BP 2
EP 13
DI 10.1109/TMM.2010.2080668
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 708TJ
UT WOS:000286386900002
DA 2024-07-18
ER

PT J
AU Varni, G
   Volpe, G
   Camurri, A
AF Varni, Giovanna
   Volpe, Gualtiero
   Camurri, Antonio
TI A System for Real-Time Multimodal Analysis of Nonverbal Affective Social
   Interaction in User-Centric Media
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affective social behavior of small groups of users; affective sound and
   music processing; multimodal analysis of nonverbal affective social
   interaction; real-time applications of multimodal affective social
   interaction; user-centric media
ID BODY MOVEMENT; PHASE SYNCHRONY; EMOTION; PERFORMANCES; RECOGNITION;
   DYNAMICS; EEG
AB This paper presents a multimodal system for real-time analysis of nonverbal affective social interaction in small groups of users. The focus is on two major aspects of affective social interaction: the synchronization of the affective behavior within a small group and the emergence of functional roles, such as leadership. A small group of users is modeled as a complex system consisting of single interacting components that can auto-organize and show global properties. Techniques are developed for computing quantitative measures of both synchronization and leadership. Music is selected as experimental test-bed since it is a clear example of interactive and social activity, where affective nonverbal communication plays a fundamental role. The system has been implemented as software modules for the EyesWeb XMI platform (http://www.eyesweb.org). It has been used in experimental frameworks (a violin duo and a string quartet) and in real-world applications (in user-centric applications for active music listening). Further application scenarios include entertainment, edutainment, therapy and rehabilitation, cultural heritage, and museum applications. Research has been carried out in the framework of the EU-ICT FP7 Project SAME (http://www.sameproject.eu).
C1 [Varni, Giovanna; Volpe, Gualtiero; Camurri, Antonio] Univ Genoa, DIST, I-16145 Genoa, Italy.
C3 University of Genoa
RP Varni, G (corresponding author), Univ Genoa, DIST, I-16145 Genoa, Italy.
EM giovanna.varni@unige.it; gualtiero.volpe@unige.it;
   antonio.camurri@unige.it
RI Volpe, Gualtiero/C-1308-2013
OI Volpe, Gualtiero/0000-0003-0760-4627; CAMURRI,
   ANTONIO/0000-0003-3378-8685
FU EU [215749 SAME]
FX Manuscript received December 12, 2009; revised March 24, 2010 and May
   20, 2010; accepted May 21, 2010. Date of current version September 15,
   2010. This work was supported in part by the FP7 EU-ICT Project 215749
   SAME (http://www.sameproject.eu). The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Nadia Magnenat-Thalmann.
CR [Anonymous], 1981, LECT NOTES MATH
   Arnhold J, 1999, PHYSICA D, V134, P419, DOI 10.1016/S0167-2789(99)00140-2
   Bhattacharya J, 2005, SIGNAL PROCESS, V85, P2161, DOI 10.1016/j.sigpro.2005.07.007
   Boccaletti S, 2002, PHYS REP, V366, P1, DOI 10.1016/S0370-1573(02)00137-0
   Boone RT, 1998, DEV PSYCHOL, V34, P1007, DOI 10.1037/0012-1649.34.5.1007
   Camurri A, 2005, IEEE MULTIMEDIA, V12, P43, DOI 10.1109/MMUL.2005.2
   Camurri A, 2003, INT J HUM-COMPUT ST, V59, P213, DOI 10.1016/S1071-5819(03)00050-8
   Camurri A., 2008, P 3 INT C DIGITAL IN, P376
   Camurri A., 2004, LNAI, V2915
   Cao LY, 1998, PHYSICA D, V121, P75, DOI 10.1016/S0167-2789(98)00151-1
   Castellano G, 2008, MUSIC PERCEPT, V26, P103, DOI 10.1525/MP.2008.26.2.103
   Davidson J.W., 1994, J HUM MOVEMENT STUD, V6, P279
   DEMEIJER M, 1989, J NONVERBAL BEHAV, V13, P247, DOI 10.1007/BF00990296
   DEROSIS A, 2005, P VIRT SOC CHAR S AI
   DIDUCH L, 2008, P IEEE INT C MULT EX
   Dixon S, 2001, J NEW MUSIC RES, V30, P39, DOI 10.1076/jnmr.30.1.39.7119
   Douglas-Cowie E, 2003, SPEECH COMMUN, V40, P33, DOI 10.1016/S0167-6393(02)00070-5
   Driskell JE, 2006, GROUP DYN-THEOR RES, V10, P249, DOI 10.1037/1089-2699.10.4.249
   Dunbar NE, 2005, J SOC PERS RELAT, V22, P207, DOI 10.1177/0265407505050944
   ECKMANN JP, 1980, EPL-EUROPHYS LETT, V5, P973
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   HASHIMOTO S, 1997, P INT WORKSH KANSEI, P101
   JAIMOVICH J, 2009, P 2009 INT COMP MUS, P461
   KAPUR A, 2005, LNCS, V3784
   Kasap Z, 2009, IEEE COMPUT GRAPH, V29, P20, DOI 10.1109/MCG.2009.26
   Keller P.E., 2008, Enacting intersubjectivity: A cognitive and social perspective to the study of interactions
   Kim J, 2008, IEEE T PATTERN ANAL, V30, P2067, DOI 10.1109/TPAMI.2008.26
   Kivy Peter., 1989, SOUND SENTIMENT
   KLEIN J, 1999, THESIS MIT CAMBRIDGE
   Krumhansl CL, 1997, CAN J EXP PSYCHOL, V51, P336, DOI 10.1037/1196-1961.51.4.336
   Laban R., 1947, EFFORT
   Lachaux JP, 1999, HUM BRAIN MAPP, V8, P194, DOI 10.1002/(SICI)1097-0193(1999)8:4<194::AID-HBM4>3.0.CO;2-C
   LASOBALLESTEROS I, 2008, USER CTR FUTURE MEDI
   LIU K, 2005, P CHI WORKSH HCI CHA
   Magnenat-Thalmann N, 2008, VISUAL COMPUT, V24, P827, DOI 10.1007/s00371-008-0264-6
   Marwan N, 2007, PHYS REP, V438, P237, DOI 10.1016/j.physrep.2006.11.001
   Mast MS, 2002, HUM COMMUN RES, V28, P420, DOI 10.1111/j.1468-2958.2002.tb00814.x
   MCCOWAN I, 2004, AMBIENT INTELLIGENCE, P235
   McQuiggan SW, 2007, INT J HUM-COMPUT ST, V65, P348, DOI 10.1016/j.ijhcs.2006.11.015
   Nawrath J, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.038701
   Néda Z, 2000, NATURE, V403, P849, DOI 10.1038/35002660
   Norton R., 1983, COMMUNICATOR STYLE T
   PENTLAND A, 2004, P 3 IEEE INT C DEV L
   Pentland A, 2007, IEEE SIGNAL PROC MAG, V24, P108, DOI 10.1109/MSP.2007.4286569
   Pianesi F, 2008, PERS UBIQUIT COMPUT, V12, P181, DOI 10.1007/s00779-007-0144-5
   Pikovsky AS, 1996, EUROPHYS LETT, V34, P165, DOI 10.1209/epl/i1996-00433-3
   Quiroga RQ, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.041904
   Richardson DC, 2007, PSYCHOL SCI, V18, P407, DOI 10.1111/j.1467-9280.2007.01914.x
   RIENKS RJ, 2005, P WORKSH MACH LEARN
   Romano MC, 2005, EUROPHYS LETT, V71, P466, DOI 10.1209/epl/i2005-10095-1
   Rosenblum MG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.045202
   RUSSELL JA, 1980, J PERS SOC PSYCHOL, V39, P1161, DOI 10.1037/h0077714
   Thalmann Daniel., 2007, CROWD SIMULATION
   Thiel M, 2006, EUROPHYS LETT, V75, P535, DOI 10.1209/epl/i2006-10147-0
   Timmers Renee., 2006, PSYCHOL MUSIC, V34, P481, DOI [DOI 10.1177/0305735606067165, 10.1177/0305735606067165]
   Trianni V, 2009, IEEE T EVOLUT COMPUT, V13, P722, DOI 10.1109/TEVC.2009.2015577
   VARNI G, 2009, P 2009 IEEE INT C SO
   VARNI G, 2009, P 1 INT ICST C US CT
   Vinciarelli A., 2008, P 10 INT C MULTIMODA, P61, DOI DOI 10.1145/1452392.1452405
   Vinciarelli A, 2009, IMAGE VISION COMPUT, V27, P1743, DOI 10.1016/j.imavis.2008.11.007
   VINYES M, 2006, P 120 AES CONV PAR F
   WAGNER J, 2009, P 2009 INT C AFF COM
   Wallbott HG, 1998, EUR J SOC PSYCHOL, V28, P879, DOI 10.1002/(SICI)1099-0992(1998110)28:6<879::AID-EJSP901>3.0.CO;2-W
   Zbilut JP, 2002, PHYS LETT A, V297, P173, DOI 10.1016/S0375-9601(02)00436-X
NR 64
TC 54
Z9 59
U1 2
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 576
EP 590
DI 10.1109/TMM.2010.2052592
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900011
DA 2024-07-18
ER

PT J
AU Ma, H
   Zhu, JK
   Lyu, MRT
   King, I
AF Ma, Hao
   Zhu, Jianke
   Lyu, Michael Rung-Tsong
   King, Irwin
TI Bridging the Semantic Gap Between Image Contents and Tags
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based image retrieval; image annotation; random walk; text-based
   image retrieval
ID RETRIEVAL
AB With the exponential growth of Web 2.0 applications, tags have been used extensively to describe the image contents on the Web. Due to the noisy and sparse nature in the human generated tags, how to understand and utilize these tags for image retrieval tasks has become an emerging research direction. As the low-level visual features can provide fruitful information, they are employed to improve the image retrieval results. However, it is challenging to bridge the semantic gap between image contents and tags. To attack this critical problem, we propose a unified framework in this paper which stems from a two-level data fusions between the image contents and tags: 1) A unified graph is built to fuse the visual feature-based image similarity graph with the image-tag bipartite graph; 2) A novel random walk model is then proposed, which utilizes a fusion parameter to balance the influences between the image contents and tags. Furthermore, the presented framework not only can naturally incorporate the pseudo relevance feedback process, but also it can be directly applied to applications such as content-based image retrieval, text-based image retrieval, and image annotation. Experimental analysis on a large Flickr dataset shows the effectiveness and efficiency of our proposed framework.
C1 [Ma, Hao; Zhu, Jianke; Lyu, Michael Rung-Tsong; King, Irwin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Ma, H (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM hma@cse.cuhk.edu.hk; jkzhu@cse.cuhk.edu.hk; lyu@cse.cuhk.edu.hk;
   king@cse.cuhk.edu.hk
RI ma, hao/GLS-6300-2022; King, Irwin/C-9681-2015
OI ma, hao/0000-0002-3342-3099; King, Irwin/0000-0001-8106-6447
FU Council of the Hong Kong Special Administrative Region, China [CUHK
   4128/08E, CUHK 4154/09E]
FX Manuscript received October 31, 2009; revised February 28, 2010;
   accepted April 25, 2010. Date of publication May 27, 2010; date of
   current version July 16, 2010. This work was supported by two grants
   from the Research Grants Council of the Hong Kong Special Administrative
   Region, China (Project No. CUHK 4128/08E and CUHK 4154/09E). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Abdulmotaleb El Saddik.
CR Ames M., 2007, P SIGCHI C HUMAN FAC, P971
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], 2009, P 17 ACM INT C MULT
   Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348
   Auchard E., 2007, REUTERS
   Baluja Shumeet, 2008, P 17 INT C WORLD WID, P895, DOI DOI 10.1145/1367497.1367618
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Craswell N., 2007, Proc. 30th Annual Int. ACM SIGIR CRDIR, P239
   Datta AK, 2008, OPT LASER TECHNOL, V40, P1, DOI 10.1016/j.optlastec.2007.04.006
   Djeraba C, 2003, IEEE T KNOWL DATA EN, V15, P118, DOI 10.1109/TKDE.2003.1161586
   Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97
   EIRON N., 2004, P 13 INT C WORLD WID, P309, DOI DOI 10.1145/988672.988714
   FAN J, 2004, P 27 ANN INT ACM SIG, P361
   Fan JP, 2008, IEEE T MULTIMEDIA, V10, P167, DOI 10.1109/TMM.2007.911775
   GHOSHAL A, 2005, P 28 ANN INT ACM SIG, P544
   Grigorova A, 2007, IEEE T MULTIMEDIA, V9, P1183, DOI 10.1109/TMM.2007.902828
   Hsu WinstonH., 2007, ACM MM
   Jeon J., 2003, P 26 ANN INT ACM SIG
   Krishnapuram R, 2004, IEEE T KNOWL DATA EN, V16, P1185, DOI 10.1109/TKDE.2004.53
   Kuo Yin-Hsi., 2009, ACM Multimedia, P65
   LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma H, 2007, P 30 ANN INT ACM SIG, P39, DOI DOI 10.1145/1277741.1277751
   Ma H., 2008, CIKM '08, P931
   Ma H, 2009, PROCEEDINGS 32ND ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P203, DOI 10.1145/1571941.1571978
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   PAGE L, SIDLWP19990120
   Pearson K, 1905, NATURE, V72, P294, DOI 10.1038/072294b0
   Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tang JH, 2010, IEEE T MULTIMEDIA, V12, P131, DOI 10.1109/TMM.2009.2037373
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wang C., 2008, ACM SIGIR, P355, DOI DOI 10.1145/1390334.1390396
   Wu L., 2008, P ACM INT C MULTIMED, P31, DOI DOI 10.1145/1459359.1459364
   Yang H., 2007, P 30 ANN INT ACM SIG, P431, DOI DOI 10.1145/1277741.1277815
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   ZHU J, ACM T MULTI IN PRESS
   Zhu JK, 2008, IEEE T MULTIMEDIA, V10, P86, DOI 10.1109/TMM.2007.911245
   Zhu Jianke., 2008, P 16 ACM INT C MULTI, P41
   Zhu X., 2005, THESIS PITTSBURGH
NR 44
TC 87
Z9 93
U1 0
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 462
EP 473
DI 10.1109/TMM.2010.2051360
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500010
DA 2024-07-18
ER

PT J
AU Wang, W
   Hempel, M
   Peng, DM
   Wang, HG
   Sharif, H
   Chen, HH
AF Wang, Wei
   Hempel, Michael
   Peng, Dongming
   Wang, Honggang
   Sharif, Hamid
   Chen, Hsiao-Hwa
TI On Energy Efficient Encryption for Video Streaming in Wireless Sensor
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross layer approach; resource allocation; video selective encryption;
   wireless sensor network
ID SELECTIVE ENCRYPTION; TRANSMISSION; COMPRESSION
AB Selective encryption for video streaming was proposed for efficient multimedia content protection. However, the issues on joint optimization of video quality, content protection, and communication energy efficiency in a wireless sensor network (WSN) have not been fully addressed in the literature. In this paper, we propose a scheme to optimize the energy, distortion, and encryption performance of video streaming in WSNs. The contribution of this work is twofold. First, a channel-aware selective encryption approach is proposed to minimize the extra encryption dependency overhead at the application layer. Second, an unequal error protection (UEP)-based network resource allocation scheme is proposed to improve the communication efficiency at the lower layers. Simulation experiments demonstrate that the proposed joint selective encryption and resource allocation scheme can improve the video transmission quality significantly with guaranteed content protection and energy efficiency.
C1 [Wang, Wei] S Dakota State Univ, Dept Elect Engn & Comp Sci, Brookings, SD 57006 USA.
   [Hempel, Michael; Peng, Dongming; Sharif, Hamid] Univ Nebraska, Dept Elect & Comp Engn, Lincoln, NE 68508 USA.
   [Wang, Honggang] Univ Massachusetts Dartmouth, Dept Elect & Comp Engn, N Dartmouth, MA 02747 USA.
   [Chen, Hsiao-Hwa] Natl Cheng Kung Univ, Dept Engn Sci, Tainan 70101, Taiwan.
C3 South Dakota State University; University of Nebraska System; University
   of Nebraska Lincoln; University of Massachusetts System; University
   Massachusetts Dartmouth; National Cheng Kung University
RP Wang, W (corresponding author), S Dakota State Univ, Dept Elect Engn & Comp Sci, Brookings, SD 57006 USA.
EM wei.wang@sdstate.edu; mhempel@unlnotes.unl.edu; dpeng@unlnotes.unl.edu;
   hwang1@umassd.edu; hsharif@unlnotes.unl.edu; hshwchen@ieee.org
RI Sharif, Haidar/AAR-6783-2021; Wang, Honggang/D-6079-2013
OI Sharif, Haidar/0000-0001-7235-6004; Hempel, Michael/0000-0002-7091-8349;
   Wang, Honggang/0000-0001-9475-2630; Sharif-Kashani,
   Hamid/0000-0001-6229-2043
FU US NSF [0707944]; Taiwan National Science Council Research
   [NSC98-2219-E-006-011]; Division Of Computer and Network Systems; Direct
   For Computer & Info Scie & Enginr [0707944] Funding Source: National
   Science Foundation
FX Manuscript received May 23, 2009; revised February 09, 2010; accepted
   April 01, 2010. Date of publication May 18, 2010; date of current
   version July 16, 2010. This work was supported in part by US NSF Grant
   No. 0707944 for wireless sensor networks research, and in part by Taiwan
   National Science Council Research Grant NSC98-2219-E-006-011. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Ali N. Akansu.
CR Agi I., 1996, Proceedings of the Symposium on Network and Distributed System Security, P137, DOI 10.1109/NDSS.1996.492420
   Alattar A. M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P256, DOI 10.1109/ICIP.1999.819590
   [Anonymous], 2002, P 5 NORD SIGN PROC S
   AU O, 2007, IEEE SIGNAL PROCESS, V14, P201, DOI DOI 10.1109/LSP.2006.884012
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   Grangetto M, 2006, IEEE T MULTIMEDIA, V8, P905, DOI 10.1109/TMM.2006.879919
   Haykin Simon., 1994, COMMUNICATION SYSTEM, V3-rd, P510
   *ITU T, 2003, H264 ITUT 10
   Jakimoski G, 2008, IEEE T MULTIMEDIA, V10, P330, DOI 10.1109/TMM.2008.917355
   Kankanhalli MS, 2002, IEEE T CONSUM ELECTR, V48, P356, DOI 10.1109/TCE.2002.1010142
   Lian SG, 2006, IEEE T CONSUM ELECTR, V52, P621, DOI 10.1109/TCE.2006.1649688
   Lookabaugh T, 2004, IEEE COMMUN MAG, V42, P124, DOI 10.1109/MCOM.2004.1299355
   Nanjunda C, 2005, IEEE ICC, P1287
   Schurgers C, 2001, ISLPED'01: PROCEEDINGS OF THE 2001 INTERNATIONAL SYMPOSIUM ON LOWPOWER ELECTRONICS AND DESIGN, P96, DOI 10.1109/LPE.2001.945382
   Stallings William., 2000, DATA COMPUTER COMMUN, V7-th, P85
   Sullivan GJ, 2005, P IEEE, V93, P18, DOI 10.1109/JPROC.2004.839617
   Tang L., 1996, P 4 ACM INT MULT C A, P219
   WANG W, 2009, P IEEE GLOB DEC
   Wang W, 2008, IEEE T MULTIMEDIA, V10, P1169, DOI 10.1109/TMM.2008.2001354
   Wang W, 2007, ADV MULTIMED, V2007, DOI 10.1155/2007/25187
   Wang W, 2009, WIREL COMMUN MOB COM, V9, P383, DOI 10.1002/wcm.550
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   Yang M, 2004, IEEE POTENTIALS, V23, P28
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
NR 25
TC 32
Z9 36
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 417
EP 426
DI 10.1109/TMM.2010.2050653
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500006
DA 2024-07-18
ER

PT J
AU Shrestha, P
   Barbieri, M
   Weda, H
   Sekulovski, D
AF Shrestha, Prarthana
   Barbieri, Mauro
   Weda, Hans
   Sekulovski, Dragan
TI Synchronization of Multiple Camera Videos Using Audio-Visual Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content analysis and synthesis; feature extraction and representation;
   joint media and multimodal processing
AB Digital video capturing is getting popular with the decreasing price of camcorders and the increasing availability of devices with embedded video cameras such as digital-still cameras, mobile phones and PDAs. While a raw home video is considered as visually non-appealing, having multiple recordings of the same event provides the opportunity to combine audio and video segments from different cameras for improving quality and aesthetics. Mixing content from different recordings requires precise synchronization among the recordings. In most present applications, synchronization is done manually and considered as a very tedious task. In this paper, we propose a novel automated synchronization approach based on detecting and matching audio and video features extracted from the recorded content. We assess experimentally three realizations of this approach on a common data set and make recommendations on the usability of the different realizations in practical use cases. The realizations have no limitations on the number and movement of the cameras. Moreover, they are robust against various ambient noises and audio-visual artifacts occurring during the recordings.
C1 [Shrestha, Prarthana; Barbieri, Mauro; Weda, Hans; Sekulovski, Dragan] Philips Res Europe, NL-5656 AE Eindhoven, Netherlands.
C3 Philips; Philips Research
RP Shrestha, P (corresponding author), Philips Res Europe, NL-5656 AE Eindhoven, Netherlands.
EM p.shrestha@tue.nl; mauro.barbieri@philips.com; hans.weda@philips.com;
   dragan.sekulovski@philips.com
OI Weda, Hans/0000-0001-6675-2270
FU Dutch BSIK
FX This work was supported in part by the Dutch BSIK research program
   MultimediaN. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Susanto Rahardja.
CR [Anonymous], 1998, BT13591 ITUR
   [Anonymous], P ADV CONC INT VIS S
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Caspi Y, 2000, PROC CVPR IEEE, P682, DOI 10.1109/CVPR.2000.854940
   Caspi Y., 2004, P 2 INT S 3D DAT PRO
   CORMEN HT, 2001, INTRO ALGORITHMS
   Cremer M, 2009, P SPIE IS T ELECT IM, V7254
   Haitsma J., 2002, INT C MUS INF RETR P, P1
   *ISO IEC, 2007, JTC1SC29WG11N9163 IS
   Kennedy Lyndon., 2009, WWW 09, P311, DOI [DOI 10.1145/1526709.1526752, 10.1145/1526709.1526752]
   KLAPURI AP, 2003, P CAMBR MUS PROC C C
   Lei C, 2006, IEEE T IMAGE PROCESS, V15, P2473, DOI 10.1109/TIP.2006.877438
   Lienhart R., 1999, ACM MULTIMEDIA, P37, DOI DOI 10.1145/319878.319888
   Michel M, 2006, P 4 ACM INT WORKSH V, P3, DOI 10.1145/1178782.1178785
   OSHAUGHNESSY D, 1990, J FORENSIC SCI, V35, P69
   SCHRADER JE, 2003, THESIS EINDHOVEN U T
   Shrestha P., 2007, P 15 ANN ACM INT C M, P545
   SHRESTHA P, 2006, P 14 ACM INT C MULT, P137
   SINHA SN, 2000, P 19 IEEE C COMP VIS, V2, P682
   STEIN G, 1998, P DARPA IM UND WORKS, P521
   Tuytelaars T., 2004, P IEEE COMP SOC C CO
   WHITEHEAD A, 2005, P 14 BRAZ S COMP GRA, P132
   Zettl Herbert, 1999, APPL MEDIA AESTHETIC
   TEST DATA SET
NR 24
TC 32
Z9 39
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 79
EP 92
DI 10.1109/TMM.2009.2036285
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800007
DA 2024-07-18
ER

PT J
AU Zhang, YF
   Qin, SY
   He, ZH
AF Zhang, Yongfei
   Qin, Shiyin
   He, Zhihai
TI Fine-Granularity Transmission Distortion Modeling for Video Packet
   Scheduling Over Mesh Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay constraints; dynamic network; priority scheduling; transmission
   distortion model; video streaming
ID MULTIHOP WIRELESS NETWORKS; AWARE RESOURCE-ALLOCATION; COMMUNICATION
AB Packet scheduling is a critical component in multi-session video streaming over mesh networks. Different video packets have different levels of contribution to the overall video presentation quality at the receiver side. In this work, we develop a fine-granularity transmission distortion model for the encoder to predict the quality degradation of decoded videos caused by lost video packets. Based on this packet-level transmission distortion model, we propose a content-and-deadline-aware scheduling (CDAS) scheme for multi-session video streaming over multi-hop mesh networks, where content priority, queuing delays, and dynamic network transmission conditions are jointly considered for each video packet. Our extensive experimental results demonstrate that the proposed transmission distortion model and the CDAS scheme significantly improve the performance of multi-session video streaming over mesh networks.
C1 [Zhang, Yongfei; Qin, Shiyin] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Zhang, Yongfei; He, Zhihai] Univ Missouri, Dept Elect & Comp Engn, Columbia, MO 65211 USA.
C3 Beihang University; University of Missouri System; University of
   Missouri Columbia
RP Zhang, YF (corresponding author), Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
EM zhangyf.ac@gmail.com; qsy@buaa.edu.cn; HeZhi@missouri.edu
RI qin, shi/JNY-1785-2023; Zhang, Yongfei/A-1505-2010; He,
   Zhihai/A-5885-2019
OI Zhang, Yongfei/0000-0002-5080-1733
CR AGRAWAL R, IEEE T INF IN PRESS
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   Chiou HJ, 2005, J VIS COMMUN IMAGE R, V16, P563, DOI 10.1016/j.jvcir.2004.11.009
   Chou PA, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P587, DOI 10.1109/MMSP.2001.962796
   Dua A, 2007, IEEE T MOBILE COMPUT, V6, P1410, DOI 10.1109/TMC.2007.1055
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P1051, DOI 10.1109/TCSVT.2006.881198
   Jensen E. D., 1985, Proceedings of the Real-Time Systems Symposium (Cat. No.85CH2220-2), P112
   Joo C., 2008, IEEE INFOCOM 2008 TH, P1103, DOI DOI 10.1109/INFOCOM.2008.165
   Joo C, 2007, IEEE DECIS CONTR P, P2002
   KANG S, 2002, P PACK VID APR
   Kodialam M., 2003, P 9 ANN INT C MOBILE, P42
   LIU CL, 1973, J ACM, V20, P46, DOI 10.1145/321738.321743
   MIAO Z, 2002, P PACK VID APR
   Nyamweno S, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P268, DOI 10.1109/MMSP.2007.4412869
   OFUJI Y, 2003, P IEEE 58 VEH TECHN
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   Pahalawatta PV, 2007, WIREL COMMUN MOB COM, V7, P131, DOI 10.1002/wcm.469
   Politis I., 2007, PACKET VIDEO 2007, P349
   SETTON E, 2004, P IEEE INT WORKSH MU
   Shiang HP, 2007, IEEE T MULTIMEDIA, V9, P1299, DOI 10.1109/TMM.2007.902845
   Shiang HP, 2007, IEEE J SEL AREA COMM, V25, P770, DOI 10.1109/JSAC.2007.070513
   Sikora T, 1997, IEEE T CIRC SYST VID, V7, P19, DOI 10.1109/76.554415
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tong XL, 2007, IEEE T MOBILE COMPUT, V6, P1343, DOI 10.1109/TMC.2007.1063
   Tupelly R. S., 2003, P C INF SCI SYST
   van der Schaar M, 2006, IEEE T MOBILE COMPUT, V5, P755, DOI 10.1109/TMC.2006.81
   Wang Y, 1998, P IEEE, V86, P974, DOI 10.1109/5.664283
   Xiao L, 2004, IEEE T COMMUN, V52, P1136, DOI 10.1109/TCOMM.2004.831346
   Zhang CY, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P855
   Zhang F, 2006, IEEE T MOBILE COMPUT, V5, P144, DOI 10.1109/TMC.2006.25
   Zhang HH, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P477, DOI 10.1109/ICME.2008.4607475
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
NR 33
TC 25
Z9 32
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2010
VL 12
IS 1
BP 1
EP 12
DI 10.1109/TMM.2009.2036290
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 533SJ
UT WOS:000272844800001
DA 2024-07-18
ER

PT J
AU Mademlis, A
   Daras, P
   Tzovaras, D
   Strintzis, MG
AF Mademlis, Athanasios
   Daras, Petros
   Tzovaras, Dimitrios
   Strintzis, Michael Gerassimos
TI Ellipsoidal Harmonics for 3-D Shape Description and Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D object retrieval; ellipsoidal harmonics; shape description
AB In this paper, a novel approach for 3-D Shape description and retrieval based on the theory of ellipsoidal harmonics is presented. Four novel descriptors are introduced: the surface ellipsoidal harmonics descriptor, which concerns 3-D objects that are described as polygonal surfaces; the volumetric ellipsoidal harmonics descriptor, which is applicable to volumetric 3-D objects; the generalized ellipsoidal harmonics descriptor that is applied to any local 3-D object descriptors; and, finally, the combined ellipsoidal-spherical harmonics descriptor, which leads to a compact and powerful descriptor that inherits the advantages of both approaches: the rotation invariance properties of the spherical harmonics and the directional information enclosed in ellipsoidal harmonics. Experimental results performed using well-known 3-D object databases prove the retrieval efficiency of the proposed approach.
C1 [Mademlis, Athanasios; Strintzis, Michael Gerassimos] Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
   [Daras, Petros; Tzovaras, Dimitrios; Strintzis, Michael Gerassimos] Ctr Res & Technol Hellas, Informat & Telemat Inst, Thessaloniki, Greece.
C3 Aristotle University of Thessaloniki; Centre for Research & Technology
   Hellas
RP Mademlis, A (corresponding author), Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Informat Proc Lab, GR-54006 Thessaloniki, Greece.
EM mademlis@iti.gr; daras@iti.gr; Dimitrios.tzovaras@iti.gr;
   strintzi@eng.auth.gr
RI Tzovaras, Dimitrios/ABB-9576-2021; Daras, Petros/F-5284-2012
OI Tzovaras, Dimitrios/0000-0001-6915-6722; Daras,
   Petros/0000-0003-3814-6710
CR *AIM SHAP, SHAP RETR CONT
   [Anonymous], 1931, The Theory of Spherical and Ellipsoidal Harmonics
   [Anonymous], P 4 EUR S GEOM PROC
   [Anonymous], THESIS U LEIPZIG LEI
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Bustos B, 2006, INT J DIGIT LIBRARIE, V6, P39, DOI 10.1007/s00799-005-0122-3
   Byerly W.E., 1959, An Elementary Treatise on Fourier's Series: and Spherical, Cylindrical, and Ellipsoidal Harmonics, with Applications to Problems in Mathematical Physics
   CANTERAKIS N, 1999, P SCAND C IM AN
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cornea ND, 2005, INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, PROCEEDINGS, P366, DOI 10.1109/SMI.2005.1
   Corney J, 2002, IEEE COMPUT GRAPH, V22, P65, DOI 10.1109/MCG.2002.999789
   Daras P, 2006, IEEE T MULTIMEDIA, V8, P101, DOI 10.1109/TMM.2005.861287
   Hilaga M, 2001, COMP GRAPH, P203, DOI 10.1145/383259.383282
   HORN BKP, 1984, P IEEE, V72, P1671, DOI 10.1109/PROC.1984.13073
   *ISO MPEG, 2001, N3914 ISOMPEG
   Jayanti S, 2006, COMPUT AIDED DESIGN, V38, P939, DOI 10.1016/j.cad.2006.06.007
   Kang S. B., 1991, Proceedings 1991 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (91CH2983-5), P580, DOI 10.1109/CVPR.1991.139757
   KAZHDAN M, 2003, P S GEOM PROC JUN
   Kriegel Hans-Peter., 2003, SIGMOD 03, P587
   Kriegel HP, 2003, EIGHTH INTERNATIONAL CONFERENCE ON DATABASE SYSTEMS FOR ADVANCED APPLICATIONS, PROCEEDINGS, P27, DOI 10.1109/DASFAA.2003.1192365
   Liu SJ, 2007, COMPUT GRAPH-UK, V31, P243, DOI 10.1016/j.cag.2006.12.004
   Mademlis A, 2007, THIRD INTERNATIONAL SYMPOSIUM ON 3D DATA PROCESSING, VISUALIZATION, AND TRANSMISSION, PROCEEDINGS, P743
   Novotni M., 2003, P 8 ACM S SOL MOD AP, P216, DOI DOI 10.1145/781606.781639
   Paquet E, 2000, SIGNAL PROCESS-IMAGE, V16, P103, DOI 10.1016/S0923-5965(00)00020-5
   Ricard J, 2005, PATTERN RECOGN LETT, V26, P2174, DOI 10.1016/j.patrec.2005.03.030
   Romain G, 2001, CELEST MECH DYN ASTR, V79, P235, DOI 10.1023/A:1017555515763
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sundar H, 2003, SMI 2003: SHAPE MODELING INTERNATIONAL 2003, PROCEEDINGS, P130, DOI 10.1109/smi.2003.1199609
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Vranic DV, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P757
   Vranic DV, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P177, DOI 10.1109/ICME.2002.1035747
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   WEYRICH T, 2004, P EUR GRAN SPAIN
   Zhang C., 2001, Proc. ACM international conference on Multimedia, P615
   [No title captured]
NR 35
TC 6
Z9 7
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1422
EP 1433
DI 10.1109/TMM.2009.2032690
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000003
DA 2024-07-18
ER

PT J
AU Sat, B
   Wah, BW
AF Sat, Batu
   Wah, Benjamin W.
TI Statistical Scheduling of Offline Comparative Subjective Evaluations for
   Real-Time Multimedia
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian analysis; Internet; just noticeable difference; real-time
   multimedia; statistical scheduling; subjective tests; voice-over-IP
ID VOICE QUALITY
AB In this paper, we study the statistical scheduling of offline subjective tests for evaluating alternative control schemes in real-time multimedia applications. These applications are characterized by multiple counteracting objective quality metrics (such as delay and signal quality) that can be affected by various control schemes. However, the trade-offs among these metrics with respect to the subjective preferences of users are not defined. As a result, it is difficult to select the proper control schemes that lead to the best subjective quality at run time. Since subjective tests are expensive to conduct and the number of possible control schemes and run-time conditions is prohibitively large, it is important that a minimum number of such tests be conducted offline, and that the results learned can be generalized to unseen conditions with statistical confidence. To this end, we study in this paper efficient algorithms for scheduling a sequence of subjective tests, while leaving the generalization of limited offline subjective tests to guide the operation of the control schemes at run time to a future paper. Using Monte Carlo simulations, we verify the robustness of our algorithms in terms of their accuracy and efficiency.
C1 [Sat, Batu; Wah, Benjamin W.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Sat, B (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.
EM batusat@illinois.edu; wah@illinois.edu
FU National Science Foundation [CNS 08-41336]
FX This work was supported by the National Science Foundation under Grant
   CNS 08-41336.
CR Boutremans C, 2003, IEEE INFOCOM SER, P652
   Huang ZX, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P493, DOI 10.1109/ICME.2008.4607479
   *ITU T, ITU T P SER REC
   *ITU T, ITU T G SER REC
   Sat B., 2007, Proceedings of the 15th international conference on Multimedia, P137
   Sat B, 2007, IEEE INT SYM MULTIM, P3, DOI 10.1109/ISM.2007.48
   Sat B, 2008, IEEE INT SYM MULTIM, P424, DOI 10.1109/ISM.2008.116
   Sat B, 2009, IEEE MULTIMEDIA, V16, P46, DOI 10.1109/MMUL.2009.2
   Sun LF, 2006, IEEE T MULTIMEDIA, V8, P809, DOI 10.1109/TMM.2006.876279
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
NR 10
TC 4
Z9 7
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1114
EP 1130
DI 10.1109/TMM.2009.2026097
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700008
DA 2024-07-18
ER

PT J
AU Meerwald, P
   Koidl, C
   Uhl, A
AF Meerwald, Peter
   Koidl, Christian
   Uhl, Andreas
TI Attack on "Watermarking Method Based on Significant Difference of
   Wavelet Coefficient Quantization"
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attack; copyright protection; quantization; watermarking
ID TREE QUANTIZATION
AB This letter describes an attack on the recently proposed "Watermarking Method Based on Significant Difference of Wavelet Coefficient Quantization" by Lin et al. While the method is shown to be robust against many signal processing operations, security of the watermarking scheme under intentional attack exploiting knowledge of the implementation has been neglected. We demonstrate a straightforward attack which retains the fidelity of the image. The method is therefore not suitable for copyright protection applications. Further, we propose a countermeasure which mitigates the shortcoming.
C1 [Meerwald, Peter; Koidl, Christian; Uhl, Andreas] Salzburg Univ, Dept Comp Sci, A-5020 Salzburg, Austria.
C3 Salzburg University
RP Meerwald, P (corresponding author), Salzburg Univ, Dept Comp Sci, A-5020 Salzburg, Austria.
EM pmeerw@cosy.sbg.ac.at; ckoidl@cosy.sbg.ac.at; uhl@cosy.sbg.ac.at
RI Meerwald-Stadler, Peter/W-8687-2019
OI Meerwald-Stadler, Peter/0000-0002-3025-9393
FU Austrian Science Fund [FWF-P19159-N13]
FX This work was supported by Austrian Science Fund Project FWF-P19159-N13.
CR Cayre F, 2005, IEEE T SIGNAL PROCES, V53, P3976, DOI 10.1109/TSP.2005.855418
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Comesana P., 2006, IEE Proceedings-Information Security, V153, P115, DOI 10.1049/ip-ifs:20055151
   Das TK, 2006, MULTIMEDIA SYST, V12, P151, DOI 10.1007/s00530-006-0046-9
   Kalker T, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P201, DOI 10.1109/MMSP.2001.962734
   Kerckhoffs A., 1883, J. des Sci. militaires, V1, P5
   Lin WH, 2008, IEEE T MULTIMEDIA, V10, P746, DOI 10.1109/TMM.2008.922795
   Petitcolas FAP, 1998, LECT NOTES COMPUT SC, V1525, P218
   Petitcolas FAP, 2001, PROC SPIE, V4314, P575, DOI 10.1117/12.435442
   PIVA A, 2007, P SPIE SECURITY STEG, V6505
   Voloshynovskiy S, 2001, IEEE COMMUN MAG, V39, P118, DOI 10.1109/35.940053
   Wang SH, 2004, IEEE T IMAGE PROCESS, V13, P154, DOI 10.1109/TIP.2004.823822
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
NR 13
TC 26
Z9 27
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 1037
EP 1041
DI 10.1109/TMM.2009.2021793
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300020
DA 2024-07-18
ER

PT J
AU Weng, CY
   Chu, WT
   Wu, JL
AF Weng, Chung-Yi
   Chu, Wei-Ta
   Wu, Ja-Ling
TI RoleNet: Movie Analysis from the Perspective of Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Community analysis; movie understanding; social network analysis; story
   segmentation
ID VIDEO
AB With the idea of social network analysis, we propose a novel way to analyze movie videos from the perspective of social relationships rather than audiovisual features. To appropriately describe role's relationships in movies, we devise a method to quantify relations and construct role's social networks, called RoleNet. Based on RoleNet, we are able to perform semantic analysis that goes beyond conventional feature-based approaches. In this work, social relations between roles are used to be the context information of video scenes, and leading roles and the corresponding communities can be automatically determined. The results of community identification provide new alternatives in media management and browsing. Moreover, by describing video scenes with role's context, social-relation-based story segmentation method is developed to pave a new way for this widely-studied topic. Experimental results show the effectiveness of leading role determination and community identification. We also demonstrate that the social-based story segmentation approach works much better than the conventional tempo-based method. Finally, we give extensive discussions and state that the proposed ideas provide insights into context-based video analysis.
C1 [Weng, Chung-Yi; Wu, Ja-Ling] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Deartment Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Chu, Wei-Ta] Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi 621, Taiwan.
C3 National Taiwan University; National Chung Cheng University
RP Weng, CY (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Deartment Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM chunye@cmlab.csie.ntu.edu.tw; wjl@cmlab.csie.ntu.edu.tw;
   wtchu@cs.ccu.edu.tw
RI Weng, Chung-Yi/JEO-2228-2023; Chu, Wei-Ta/AAE-8471-2022
OI Chu, Wei-Ta/0000-0001-5722-7239; WU, JA-LING/0000-0002-3631-1551
CR Adams B, 2002, IEEE T MULTIMEDIA, V4, P472, DOI 10.1109/TMM.2002.802016
   Albrecht K., 2005, SOCIAL INTELLIGENCE
   [Anonymous], 1995, PROC ICJAI, DOI DOI 10.1145/217279.215068
   [Anonymous], Open Source Computer Vision Library - v2.4.9
   [Anonymous], 1999, UNDERSTANDING MOVIES
   Bordwell David, 1985, Narration in the Fiction Film
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   CHEN HW, 2004, P ACM SIGMM INT WORK, P251
   Chu WT, 2005, MULTIMEDIA SYST, V10, P570, DOI 10.1007/s00530-005-0183-6
   DORAI C, 2001, IEEE MULTIMEDIA, V8, P10
   Garg N. P., 2008, P ACM INT C MULT, P693
   Guimerà R, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.065103
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   HSU WHM, 2003, P 2003 IEEE INT C MU, V2, P413
   HUNG H, 2007, P ACM INT C MULT, P835
   Jung B, 2004, 12 ANN ACM INT C MUL, P828, DOI [10.1145/1027527.1027720, DOI 10.1145/1027527.1027720]
   Krause AE, 2003, NATURE, V426, P282, DOI 10.1038/nature02115
   Li Y, 2006, IEEE SIGNAL PROC MAG, V23, P79
   Li Y, 2004, IEEE T CIRC SYST VID, V14, P1073, DOI 10.1109/TCSVT.2004.831968
   Lienhart R, 1998, PROC SPIE, V3656, P290, DOI 10.1117/12.333848
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Nefian AV, 1999, INT CONF ACOUST SPEE, P3553, DOI 10.1109/ICASSP.1999.757610
   Pentland A, 2007, IEEE SIGNAL PROC MAG, V24, P108, DOI 10.1109/MSP.2007.4286569
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   Rasheed Z, 2003, PROC CVPR IEEE, P343
   Rienks R., 2006, P 8 INT C MULTIMODAL, P257
   SCOTT J., 2017, Social Network Analysis, V4th
   SMEATON AF, 2006, P ACM INT WORKSH MUL, P231
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Truong BT, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198305
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   Vinciarelli A, 2006, P IEEE INT C MULT EX, P779
   Vinciarelli A., 2008, P 16 ACM INT C MULT, P1061, DOI DOI 10.1145/1459359.1459573
   Vinciarelli Alessandro., 2007, Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07, P261, DOI DOI 10.1145/1291233.1291287
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Weng C, 2007, P INT WORKSH MULT IN, P51, DOI DOI 10.1145/1290082.1290092
   WENG CY, 2006, P IEEE INT C MULT EX, P1403
   Yeung M, 1998, COMPUT VIS IMAGE UND, V71, P94, DOI 10.1006/cviu.1997.0628
   Zhu XQ, 2005, IEEE T MULTIMEDIA, V7, P648, DOI 10.1109/TMM.2005.850977
NR 39
TC 102
Z9 117
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 256
EP 271
DI 10.1109/TMM.2008.2009684
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hu, YJ
   Lee, HK
   Chen, KY
   Li, JW
AF Hu, Yongjian
   Lee, Heung-Kyu
   Chen, Kaiying
   Li, Jianwei
TI Difference Expansion Based Reversible Data Hiding Using Two Embedding
   Directions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data hiding; difference expansion embedding; integer Haar wavelet
   transform; reversible watermarking
ID WATERMARKING
AB Current difference-expansion (DE) embedding techniques perform one layer embedding in a difference image. They do not turn to the next difference image for another layer embedding unless the current difference image has no expandable differences left. The obvious disadvantage of these techniques is that image quality may have been severely degraded even before the later layer embedding begins because the previous layer embedding has used up all expandable differences, including those with large magnitude. Based on integer Haar wavelet transform, we propose a new DE embedding algorithm, which utilizes the horizontal as well as vertical difference images for data hiding. We introduce a dynamical expandable difference search and selection mechanism. This mechanism gives even chances to small differences in two difference images and effectively avoids the situation that the largest differences in the first difference image are used up while there is almost no chance to embed in small differences of the second difference image. We also present an improved histogram-based difference selection and shifting scheme, which refines our algorithm and makes it resilient to different types of images. Compared with current algorithms, the proposed algorithm often has better embedding capacity versus image quality performance. The advantage of our algorithm is more obvious near the embedding rate of 0.5 bpp.
C1 [Hu, Yongjian; Lee, Heung-Kyu] Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea.
   [Hu, Yongjian; Chen, Kaiying; Li, Jianwei] S China Univ Technol, Coll Automat Sci & Engn, Guangzhou 510641, Peoples R China.
C3 Korea Advanced Institute of Science & Technology (KAIST); South China
   University of Technology
RP Hu, YJ (corresponding author), Korea Adv Inst Sci & Technol, Dept Comp Sci, Taejon 305701, South Korea.
EM eeyjhu@scut.edu.cn; hklee@mmc.kaist.ac.kr
RI Lee, Heung Kyu/C-1941-2011
FU KAIST; MOST [R0A-2007-000-20023-0]; NSF of China [60772115, 60572140]
FX Manuscript received September 12, 2007: revised July 28. 2008. Current
   version published December 10. 2008. This work was supported in part by
   BK 21 project of KAIST, KOSEF Grant NRL Program R0A-2007-000-20023-0 of
   MOST, and the NSF of China 60772115 and 60572140. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ishfaq Ahmad.
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   Celik MU, 2005, IEEE T IMAGE PROCESS, V12, P157
   Coltuc D, 2007, IEEE SIGNAL PROC LET, V14, P255, DOI 10.1109/LSP.2006.884895
   Fridrich J, 2002, EURASIP J APPL SIG P, V2002, P185, DOI 10.1155/S1110865702000537
   Honsinger C. W., 2001, US Patent, Patent No. [6,278,791, 6278791]
   Hu Y, 2006, IEEE T CIRC SYST VID, V16, P1423, DOI 10.1109/TCSVT.2006.884011
   Kamstra L, 2005, IEEE T IMAGE PROCESS, V14, P2082, DOI 10.1109/TIP.2005.859373
   Li RYM, 2007, IEEE INT SYMP CIRC S, P1273, DOI 10.1109/ISCAS.2007.378403
   Lie W. N., 2005, P IEEE INT C MULT EX, P1174
   MACQ B, 2000, P EUSIPCO TAMP FINL, P533
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Xuan GR, 2006, LECT NOTES COMPUT SC, V4283, P323
NR 14
TC 87
Z9 98
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1500
EP 1512
DI 10.1109/TMM.2008.2007341
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600007
DA 2024-07-18
ER

PT J
AU Kang, XG
   Huang, JW
   Zeng, WJ
AF Kang, Xiangui
   Huang, Jiwu
   Zeng, Wenjun
TI Improving Robustness of Quantization-Based Image Watermarking via
   Adaptive Receiver
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Watermark; adaptive receiver; quantization
ID RESILIENT; DIVERSITY; ATTACKS; CODES
AB In this paper, the watermarking channel is modeled as a generalized channel with fading and nonzero mean additive noise. In order to improve the watermark robustness against the generalized channel, we present an optimized watermark extraction scheme by using an adaptive receiver for quantization-based watermarking. In the proposed extraction scheme, we adaptively estimate the decision zone of the binary data bits and the quantization step size. A training sequence is embedded into the original image together with the informative watermark. The estimation of the decision zone takes advantage of the response function of the training sequence. Compared to those watermarking schemes without receiver adaptation, the main improvement is the enhanced robustness against median filtering, image intensity Direct Current (DC) change, histogram equalization, color reduction, image intensity linear scaling, image intensity nonlinear scaling such as Gamma correction etc.
C1 [Kang, Xiangui; Huang, Jiwu] Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.
   [Zeng, Wenjun] Univ Missouri, Dept Comp Sci, Columbia, MO 65211 USA.
C3 Sun Yat Sen University; University of Missouri System; University of
   Missouri Columbia
RP Kang, XG (corresponding author), Sun Yat Sen Univ, Sch Informat Sci & Technol, Guangzhou 510275, Guangdong, Peoples R China.
EM isskxg@mail.sysu.edu.cn; isshjw@mail.sysu.edu.cn; zengw@missouri.edu
RI Kang, Xiangui/AAO-5527-2020; huang, jw/KVY-9917-2024
CR Berrou C, 1996, IEEE T COMMUN, V44, P1261, DOI 10.1109/26.539767
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Cheng L. L., 2001, Journal of Insect Science (Tucson), V1, P1
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Deguillaume F, 2003, SIGNAL PROCESS, V83, P2133, DOI 10.1016/S0165-1684(03)00172-5
   He DJ, 2004, IEEE IMAGE PROC, P737
   Huang JW, 2002, IEEE T CIRC SYST VID, V12, P916, DOI 10.1109/TCSVT.2002.804897
   Kang XG, 2002, LECT NOTES COMPUT SC, V2613, P212
   Kang XG, 2003, IEEE T CIRC SYST VID, V13, P776, DOI 10.1109/TCSVT.2003.815957
   Kundur D, 2001, IEEE T SIGNAL PROCES, V49, P2383, DOI 10.1109/78.950793
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   Lu CS, 2006, IEEE T MULTIMEDIA, V8, P668, DOI 10.1109/TMM.2006.876300
   Moulin P, 2005, P IEEE, V93, P2083, DOI 10.1109/JPROC.2005.859599
   Petitcolas FAP, 2000, IEEE SIGNAL PROC MAG, V17, P58, DOI 10.1109/79.879339
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Shi YQ, 2002, IEEE T CIRCUITS-I, V49, P779, DOI 10.1109/TCSI.2002.1010033
   SHTEREV ID, 2004, SPIE SECURITY STEGAN, V5306
   Solanki K, 2004, IEEE IMAGE PROC, P39
   Voloshynovskiy S, 2001, PROC SPIE, V4314, P673, DOI 10.1117/12.435452
   WANG J, 2006, SPIE SECURITY STEGAN, V6072, pB1
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P685, DOI 10.1109/TIP.2003.810588
   WU M, 2001, THESIS PRINCETON U P
   Zhao HV, 2005, IEEE T IMAGE PROCESS, V14, P646, DOI 10.1109/TIP.2005.846035
NR 23
TC 21
Z9 23
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 953
EP 959
DI 10.1109/TMM.2008.2001361
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600001
DA 2024-07-18
ER

PT J
AU Wang, YJ
   Guan, L
AF Wang, Yongjin
   Guan, Ling
TI Recognizing human emotional state from audiovisual signals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audiovisual information; emotion recognition; multiclassifier
ID RECOGNITION
AB Machine recognition of human emotional state is an important component for efficient human-computer interaction. The majority of existing works address this problem by utilizing audio signals alone, or visual information only. In this paper, we explore a systematic approach for recognition of human emotional state front audiovisual signals. The audio characteristics of emotional speech are represented by the extracted prosodic, Mel-frequency Cepstral Coefficient (MFCC), and formant frequency features. A face detection scheme based on HSV color model is used to detect the face from the background. The visual information is represented by Gabor wavelet features. We perform feature selection by using a stepwise method based on Mahalanobis distance. The selected audiovisual features are used to classify the data into their corresponding emotions. Based on a comparative study of different classification algorithms and specific characteristics of individual emotion, a novel multiclassifier scheme is proposed to boost the recognition performance. The feasibility of the proposed system is tested over a database that incorporates human subjects front different languages and cultural backgrounds. Experimental results demonstrate the effectiveness of the proposed system. The multiclassifier scheme achieves the best overall recognition rate of 82.14%.
C1 [Wang, Yongjin] Univ Toronto, Edward S Rogers Sr Dept Elect & Elect Engn, Toronto, ON M5S 3G4, Canada.
   [Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 University of Toronto; Toronto Metropolitan University
RP Wang, YJ (corresponding author), Univ Toronto, Edward S Rogers Sr Dept Elect & Elect Engn, Toronto, ON M5S 3G4, Canada.
EM ywang@comm.utoronto.ca; lguan@ee.ryerson.ca
CR Bailly Gerard., 1992, Talking Machines - Theories, Models, and Designs
   Bartlett A., 2004, Digital hearing aids
   Becchetti C., 1999, Speech Recognition, Theory and C++ Implementation
   Chang CL, 2003, J SOLID STATE ELECTR, V7, P125, DOI 10.1007/s10008-002-0304-5
   Chen CY, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P1469
   Chen LS, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P83, DOI 10.1109/MMSP.1998.738917
   Cohen I, 2003, LECT NOTES COMPUT SC, V2728, P184
   De Silva L. C., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P332, DOI 10.1109/AFGR.2000.840655
   De Silva LC, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P1310
   De Silva LC, 1997, ICICS - PROCEEDINGS OF 1997 INTERNATIONAL CONFERENCE ON INFORMATION, COMMUNICATIONS AND SIGNAL PROCESSING, VOLS 1-3, P397, DOI 10.1109/ICICS.1997.647126
   Dellaert F, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1970, DOI 10.1109/ICSLP.1996.608022
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   EKMAN P, 1994, PSYCHOL BULL, V115, P268, DOI 10.1037/0033-2909.115.2.268
   EKMAN P, 1988, J EXP PSYCHOL GEN, V117, P86, DOI 10.1037/0096-3445.117.1.86
   Garcia C, 1999, IEEE T MULTIMEDIA, V1, P264, DOI 10.1109/6046.784465
   Ghosh J, 2002, LECT NOTES COMPUT SC, V2364, P1
   HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871
   Haykin S., 1999, NEURAL NETWORKS COMP, DOI [10.1017/S0269888998214044, DOI 10.1017/S0269888998214044]
   Hoch S., 2005, Proc. Acoustics, Speech, P1085, DOI [10.1109/ICASSP.2005.1415597., DOI 10.1109/ICASSP.2005.1415597]
   Kim DJ, 2003, IEEE INT CONF FUZZY, P908
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Lee CM, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P737, DOI 10.1109/ICME.2002.1035887
   Lyons M. J., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P202, DOI 10.1109/AFGR.2000.840635
   Ma L, 2004, IEEE T SYST MAN CY B, V34, P1588, DOI 10.1109/TSMCB.2004.825930
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Nwe TL, 2001, IEEE REGION 10 INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONIC TECHNOLOGY, VOLS 1 AND 2, P297, DOI 10.1109/TENCON.2001.949600
   NWE TL, 2003, IEICE T INFORMATI ED, V86, P105
   Pantic M, 2004, IEEE T SYST MAN CY B, V34, P1449, DOI 10.1109/TSMCB.2004.825931
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Song ML, 2004, PROC CVPR IEEE, P1020
   Song ML, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P877
   THEODORIDS S, 2003, PATTERN RECOGNITION
   Ververidis D, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P593
   ZENG Z, 2005, IEEE COMP SOC C COMP, V2, P967
   ZENG Z, 2004, P 6 ACM INT C MULT I, P137
   2004, INTO COMPUTER PROGRA
NR 36
TC 43
Z9 49
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 659
EP 668
DI 10.1109/TMM.2008.921734
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jakimoski, G
   Subbalakshmi, KP
AF Jakimoski, Goce
   Subbalakshmi, K. P.
TI Cryptanalysis of some multimedia encryption schemes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE arithmetic coding; cryptanalysis; key-based interval splitting;
   multimedia encryption; multiple Huffman tables; randomized arithmetic
   coding
ID SELECTIVE ENCRYPTION; COMPRESSION; IMAGES
AB Encryption is one of the fundamental technologies that is used in digital rights management. Unlike ordinary computer applications, multimedia applications generate large amounts of data that has to be processed in real time. So, a number of encryption schemes for multimedia applications have been proposed in recent years. We analyze the following proposed methods for multimedia encryption: key-based multiple Huffman tables (MHT), arithmetic coding with key-based interval splitting (KSAC), and randomized arithmetic coding (RAC). Our analysis shows that MHT and KSAC are vulnerable to low complexity known- and/or chosen-plaintext attacks. Although we do not provide any attacks on RAC, we point out some disadvantages of RAC over the classical compress-then-encrypt approach.
C1 [Jakimoski, Goce; Subbalakshmi, K. P.] Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ 07030 USA.
C3 Stevens Institute of Technology
RP Jakimoski, G (corresponding author), Stevens Inst Technol, Dept Elect & Comp Engn, Hoboken, NJ 07030 USA.
EM gjakimos@stevens.edu; ksubbala@stevens.edu
RI Subbalakshmi, Koduvayur/JYO-3634-2024
OI Subbalakshmi, Koduvayur/0000-0002-1670-9378
FU National Science Foundation [NSF 0627688]
FX This work was supported in part by the National Science Foundation under
   Grant NSF 0627688. The associate editor coordinating the review of this
   manuscript and approving it for pubication was Prof. Mohan S.
   Kankanhalli.
CR Alattar A. M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P256, DOI 10.1109/ICIP.1999.819590
   [Anonymous], 2002, P 5 NORD SIGN PROC S
   Bergen H. A., 1993, Computers & Security, V12, P157, DOI 10.1016/0167-4048(93)90099-Q
   BERGEN HA, 1992, COMPUT SECURITY, V11
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   CLEARY JG, 1995, COMPUT SECUR, V14, P167, DOI 10.1016/0167-4048(95)97050-K
   *FIPS PUBS, 2001, 197 FIPS PUBS
   *FIPS PUBS, 1999, 463 FIPS PUBS
   GRANGETTO M, 2000, P IEEE INT WORKSH MU
   Grangetto M, 2006, IEEE T MULTIMEDIA, V8, P905, DOI 10.1109/TMM.2006.879919
   JONES DW, 1988, COMMUN ACM, V31, P996, DOI 10.1145/63030.63036
   Kundur D, 2004, P IEEE, V92, P918, DOI 10.1109/JPROC.2004.827356
   LANGDON GG, 1981, IEEE T COMMUN, V29, P858, DOI 10.1109/TCOM.1981.1095052
   LIU X, 1997, P 6 IMA INT C CRYPT
   Lookabaugh T, 2004, IEEE COMMUN MAG, V42, P124, DOI 10.1109/MCOM.2004.1299355
   Meyer J., 1995, PROJECT DESCRIPTION
   MOO PW, 1999, P IEEE INT C MULT CO
   MOO PW, 1999, P IEEE INT C IM PROC
   Pommer A, 2003, MULTIMEDIA SYST, V9, P279, DOI 10.1007/s00530-003-0099-y
   Qiao L., 1997, Las Vegas : Proceedings of the 1s International Conference on Imaging Science, Systems and Technology, P21
   Servetti A, 2002, IEEE T SPEECH AUDI P, V10, P637, DOI 10.1109/TSA.2002.804300
   SHI C, 1998, P 6 INT MULT C BRIST
   SHI C, 1999, 1999 INT C PAR DISTR
   Spanos G.A., 1995, Las Vegas : Proceedings of 4th International Conference on Computer Communications and Networks, P20
   Tang L., 1996, P 4 ACM INT MULT C A, P219
   Van Droogenbroeck M, 2002, advanced concepts for intelligent vision systems (ACIVS)
   Wen JT, 2006, IEEE SIGNAL PROC LET, V13, P69, DOI 10.1109/LSP.2005.861589
   Wu C.P., 2000, SPIE INT S INFORM TE, V4209, P284
   Wu CP, 2005, IEEE T MULTIMEDIA, V7, P828, DOI 10.1109/TMM.2005.854469
   WU CP, 2001, P SPIE SEC WAT MULT, V4314
   Zeng WJ, 2003, IEEE T MULTIMEDIA, V5, P118, DOI 10.1109/TMM.2003.808817
NR 31
TC 99
Z9 104
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 330
EP 338
DI 10.1109/TMM.2008.917355
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Besson, P
   Popovici, V
   Vesin, JM
   Thiran, JP
   Kunt, M
AF Besson, Patricia
   Popovici, Vlad
   Vesin, Jean-Marc
   Thiran, Jean-Philippe
   Kunt, Murat
TI Extraction of audio features specific to speech production for
   multimodal speaker detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio features; differential evolution; multimodal; mutual information;
   speaker detection; speech
ID GENETIC ALGORITHM; OPTIMIZATION; PROBABILITY
AB A method that exploits an information theoretic framework to extract optimized audio features using video information is presented. A simple measure of mutual information (MI) between the resulting audio and video features allows the detection of the active speaker among different candidates. This method involves the optimization of an MI-based objective function. No approximation is needed to solve this optimization problem, neither for the estimation of the probability density functions (pdfs) of the features, nor for the cost function itself. The pdfs are estimated from the samples using a nonparametric approach. The challenging optimization problem is solved using a global method: the differential evolution algorithm. Two information theoretic optimization criteria are compared and their ability to extract audio features specific to speech production is discussed. Using these specific audio features, candidate video features are then classified as member of the "speaker" or "non-speaker" class, resulting in a speaker detection scheme. As a result, our method achieves a speaker detection rate of 100% on in-house test sequences, and of 85% on most commonly used sequences.
C1 [Besson, Patricia; Vesin, Jean-Marc; Thiran, Jean-Philippe; Kunt, Murat] Swiss Fed Inst Technol EPFL, CH-1015 Lausanne, Switzerland.
   [Popovici, Vlad] Swiss Fed Inst Technol, Bioinformat Core Facil, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Swiss Federal Institutes of Technology Domain;
   Ecole Polytechnique Federale de Lausanne
RP Besson, P (corresponding author), Swiss Fed Inst Technol EPFL, CH-1015 Lausanne, Switzerland.
EM patricia.besson@a3.epfl.ch; popovici@isb-sib.ch;
   jean-marc.vesin@epfl.ch; jp.thiran@epfl.ch; murat.kunt@epfl.ch
RI Popovici, Vlad C./C-2039-2008
OI Popovici, Vlad C./0000-0002-1311-9188; Thiran,
   Jean-Philippe/0000-0003-2938-9657
CR BESSON P, 2006, TRITS2006003 EC POL
   BESSON P, 2005, P EUR SIGN PROC C EU
   BESSON P, 2005, 082005 EPFLITS
   BESSON P, 2006, P 2 INT WORKSH BIOS, P106
   Bowman A., 1997, Applied Smoothing Techniques for Data Analysis: The Kernel Approach with S-Plus Illustrations
   Butz T, 2005, SIGNAL PROCESS, V85, P875, DOI 10.1016/j.sigpro.2004.11.027
   BUTZ T, 2002, P ICME LAUS SWITZ, V2, P361
   Cole-Rhodes AA, 2003, IEEE T IMAGE PROCESS, V12, P1495, DOI 10.1109/TIP.2003.819237
   Cover Thomas M, 1999, Elements of information theory
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Fisher JW, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P1712, DOI 10.1109/IJCNN.1998.687114
   GLOVER F, 1986, COMPUT OPER RES, V13, P533, DOI 10.1016/0305-0548(86)90048-1
   Gold Ben., 2000, SPEECH AUDIO SIGNAL
   Hershey J, 2000, ADV NEUR IN, V12, P813
   Holland J.H., 1992, Adaptation in Natural and Artificial Systems, DOI DOI 10.7551/MITPRESS/1090.001.0001
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Joshi R, 1999, IEEE T SYST MAN CY A, V29, P63, DOI 10.1109/3468.736361
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Leung YW, 2001, IEEE T EVOLUT COMPUT, V5, P41, DOI 10.1109/4235.910464
   MEYNET J, 2005, 200535 EPFL
   MONACI G, 2005, P IEEE INT C IM PROC, P814
   Nock HJ, 2003, LECT NOTES COMPUT SC, V2728, P488
   PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   PICONE JW, 1993, P IEEE, V81, P1215, DOI 10.1109/5.237532
   Press W. H, 1992, NUMERICAL RECIPES C
   Price K.V, 1999, New Ideas in Optimization, P79
   QI XF, 1994, IEEE T NEURAL NETWOR, V5, P102, DOI 10.1109/72.265965
   Rechenberg Ingo, 1973, EVOLUTIONSSTRATEGIE
   Schroeter P, 1998, IEEE T MED IMAGING, V17, P172, DOI 10.1109/42.700730
   Schwefel H.P., 1981, Numerical optimization of computer models
   SLANEY M, 2001, P NIPS, V13
   Smaragdis P., 2003, 4 INT S INDPENDENT C, P709
   Spalek T, 2005, J CHEM INF MODEL, V45, P18, DOI 10.1021/ci049863s
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   VAERMAN V, 1999, THESIS ECOLE POLYTEC
NR 36
TC 18
Z9 20
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 63
EP 73
DI 10.1109/TMM.2007.911302
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200007
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, Y
   Hu, Y
   Au, OC
   Li, HQ
   Chen, CW
AF Chen, Yan
   Hu, Yang
   Au, Oscar C.
   Li, Houqiang
   Chen, Chang Wen
TI Video error concealment using spatio-temporal boundary matching and
   partial differential equation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; H.264; motion compensation; partial differential
   equation
AB Error concealment techniques are very important for video communication since compressed video sequences may be corrupted or lost when transmitted over error-prone networks. In this paper, we propose a novel two-stage error concealment scheme for erroneouslv received video sequences. In the first stage, we propose a novel spatio-temporal boundary matching algorithm (STBMA) to reconstruct the lost motion vectors (MV). A well defined cost function is introduced which exploits both spatial and temporal smoothness properties of video signals. By minimizing the cost function, the MV of each lost macroblock (MB) is recovered and the corresponding reference NIB in the reference frame is obtained using this MV. In the second stage, instead of directly copying the reference MB as the final recovered pixel values, we use a novel partial differential equation (PDE) based algorithm to refine the reconstruction. We minimize, in a weighted manner, the difference between the gradient field of the reconstructed NIB in current frame and that of the reference MB in the reference frame under given boundary condition. A weighting factor is used to control the regulation level according to the local blockiness degree. With this algorithm, the annoying blocking artifacts are effectively reduced while the structures of the reference NIB are well preserved. Compared with the error concealment feature implemented in the H.264 reference software, our algorithm is able to achieve significantly higher PSNR as well as better visual quality.
C1 [Chen, Yan; Au, Oscar C.] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
   [Hu, Yang; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Anhua 230026, Peoples R China.
   [Chen, Chang Wen] Florida Inst Technol, Dept Elect & Comp Engn, Melbourne, FL 32901 USA.
C3 Hong Kong University of Science & Technology; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS; Florida
   Institute of Technology
RP Chen, Y (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
EM eecyan@ust.hk; yanghu@ustc.hk; eeau@ust.hk; lihq@ustc.edu; cchen@fit.edu
RI Chen, Yan/C-6466-2014; Chen, Yan/P-8901-2017; Chen, Yan/P-5344-2019; Li,
   Houqiang Li/B-6259-2013; Chen, Yan/H-5483-2012
OI Chen, Yan/0000-0002-3227-4562; Chen, Yan/0000-0002-3227-4562; 
CR Alter F, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P221
   Atzori L, 2001, IEEE T MULTIMEDIA, V3, P326, DOI 10.1109/6046.944476
   Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972
   Chen Y, 2005, P IEEE INT C IM PROC, V2, P1050
   Chen Y, 2006, IEEE INT SYMP CIRC S, P686
   Gao ZW, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P69
   Gothandaraman A, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P455, DOI 10.1109/ICIP.2001.958526
   HASKELL P, 1992, P ICASSP, V3, P545
   HU Y, 2005, P SPIE MULTIMEDIA SY, V8, P150
   LAM WM, 1993, P ICASSP, V5, P417
   Lie WN, 2006, IEEE T CIRC SYST VID, V16, P982, DOI 10.1109/TCSVT.2006.879119
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Shen MY, 1998, J VIS COMMUN IMAGE R, V9, P2, DOI 10.1006/jvci.1997.0378
   SHIRANI S, 1999, P CAN C EL COMP ENG, V2, P835
   WANG Y, 1993, IEEE T COMMUN, V41, P1544, DOI 10.1109/26.237889
   Wang YK, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P729
   Yang S, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P346, DOI 10.1109/ICIP.1997.638767
   Zheng JH, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 2, PROCEEDINGS, P133
NR 19
TC 74
Z9 92
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 2
EP 15
DI 10.1109/TMM.2007.911223
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200002
DA 2024-07-18
ER

PT J
AU Oh, HR
   Song, HJ
AF Oh, Hyung Rai
   Song, Hwangjun
TI Metatile-based scalable caching and dynamic replacing algorithms for
   multiple videos over quality-of-service networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE buffer size; channel bandwidth; dynamic replacing algorithm; metafile;
   proxy server; scalable caching algorithm
ID STREAMING MEDIA DISTRIBUTION; PROXY CACHE
AB This paper presents metafile-based scalable caching and dynamic replacing algorithms for multiple videos over quality-of-service networks. A metafile for each video includes the cached-frame sequence, which is made by scalable caching algorithm minimizing client's buffer size and channel bandwidth under the general video traffic condition. Based on the metafiles, we propose a caching algorithm for multiple videos to effectively reduce both the buffer size and the required bandwidth and replacing algorithm to dynamically update the cached frames when user access pattern changes. Finally, experimental results are provided to compare the proposed algorithms with various existing caching and replacing algorithms.
C1 Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Gyungbuk 790784, South Korea.
C3 Pohang University of Science & Technology (POSTECH)
RP Oh, HR (corresponding author), Pohang Univ Sci & Technol, Dept Comp Sci & Engn, Gyungbuk 790784, South Korea.
EM raibest@postech.ae.kr; hwangjun@postech.ac.kr
CR AGGARWAL CC, 1996, P IEEE ICMS JUN
   [Anonymous], 1949, Human behaviour and the principle of least-effort
   Dan A, 1996, P SOC PHOTO-OPT INS, V2667, P344, DOI 10.1117/12.235887
   HUA KA, 1977, P ACM SIGCOMM C, P89
   *ISO, 2001, JTC1SC29WG11 ISO
   *ISO, 13818 ISO IEC
   Ma WH, 2004, MULTIMED TOOLS APPL, V22, P53, DOI 10.1023/B:MTAP.0000008659.52373.fc
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   Oh HR, 2006, J VIS COMMUN IMAGE R, V17, P57, DOI 10.1016/j.jvcir.2005.01.003
   Rizzo L, 2000, IEEE ACM T NETWORK, V8, P158, DOI 10.1109/90.842139
   SEN S, 1999, P IEEE INF 99 NEW YO, V99
   Shen B, 2004, IEEE T MULTIMEDIA, V6, P375, DOI 10.1109/TMM.2003.822791
   Shim J, 1999, IEEE T KNOWL DATA EN, V11, P549, DOI 10.1109/69.790804
   VISWANATHAN S, 1998, MULTIMEDIA SYST, V4, P397
   Wang B, 2004, IEEE T MULTIMEDIA, V6, P366, DOI 10.1109/TMM.2003.822788
   Wu KL, 2004, IEEE T MULTIMEDIA, V6, P770, DOI 10.1109/TMM.2004.834870
   Yu F, 2003, IEEE T CIRC SYST VID, V13, P257, DOI 10.1109/TCSVT.2003.809829
   Zhang ZL, 1997, IEEE J SEL AREA COMM, V15, P1148, DOI 10.1109/49.611165
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
   [No title captured]
   [No title captured]
NR 21
TC 6
Z9 8
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1535
EP 1542
DI 10.1109/TMM.2007.906590
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400018
DA 2024-07-18
ER

PT J
AU Zhu, GY
   Huang, QM
   Xu, CS
   Xing, LY
   Gao, W
   Yao, HX
AF Zhu, Guangyu
   Huang, Qingming
   Xu, Changsheng
   Xing, Liyuan
   Gao, Wen
   Yao, Hongxun
TI Human behavior analysis for highlight ranking in broadcast racket sports
   video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE action recognition; affective analysis; highlight ranking; semantic
   analysis; sports video analysis
ID HUMAN MOVEMENT; SOCCER VIDEO; EXTRACTION; RECOGNITION; INFORMATION;
   ROBUST
AB The majority of existing work on sports video analysis concentrates on highlight extraction. Little work focuses on the important issue as how the extracted highlights should be organized. In this paper, we present a multimodal approach to organize the highlights extracted from racket sports video grounded on human behavior analysis using a nonlinear affective ranking model. Two research challenges of highlight ranking are addressed, namely affective feature extraction and ranking model construction. The basic principle of affective feature extraction in our work is to extract sensitive features which can stimulate user's emotion. Since the users pay most attention to player behavior and audience response in racket sport highlights, we extract affective features from player behavior including action and trajectory, and game-specific audio keywords. We propose a novel motion analysis method to recognize the player actions. We employ support vector regression to construct the nonlinear highlight ranking model from affective features. A new subjective evaluation criterion is proposed to guide the model construction. To evaluate the performance of the proposed approaches, we have tested them on more than ten-hour broadcast tennis and badminton videos. The experimental results demonstrate that our action recognition approach significantly outperforms the existing appearance-based method. Moreover, our user study shows that the affective highlight ranking approach is effective.
C1 Harbin Inst Technol, Sch Comp Sci, Harbin 150001, Peoples R China.
   Chinese Acad Sci, Grad Sch, Beijing 100039, Peoples R China.
   Inst Infocomm Res, Singapore 119613, Singapore.
C3 Harbin Institute of Technology; Chinese Academy of Sciences; University
   of Chinese Academy of Sciences, CAS; Agency for Science Technology &
   Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)
RP Zhu, GY (corresponding author), Harbin Inst Technol, Sch Comp Sci, Harbin 150001, Peoples R China.
EM gyzhu@jdl.ac.cn; qmhuang@jdl.ac.cn; xucs@i2r.a-star.edu.sg;
   lyxing@jdl.ac.cn; wgao@jdi.ac.cn; yhx@vilab.hit.edu.cn
RI Zhu, Guangyu/H-3805-2013; Huang, Qingming/GLR-3473-2022; xu,
   cj/HJZ-3488-2023; chen, yue/JXW-9556-2024
OI Huang, Qingming/0000-0002-3025-7099; 
CR [Anonymous], P ACM MM
   [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   [Anonymous], P 12 ANN ASS COMP MA
   [Anonymous], 2005, P INT C IMAGE PROCES
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878
   Cheng CC, 2006, IEEE T MULTIMEDIA, V8, P585, DOI 10.1109/TMM.2006.870726
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P564, DOI 10.1109/TPAMI.2003.1195991
   Cutler R, 2000, IEEE T PATTERN ANAL, V22, P781, DOI 10.1109/34.868681
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Dufaux F, 2000, IEEE T IMAGE PROCESS, V9, P497, DOI 10.1109/83.826785
   Efros A., 2003, Proc. IEEE Internationa Conference on Computer Vision, V2, P726
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716
   GONG UY, 1995, P INT C MULT COMP SY, P167
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hartley R., 2003, MULTIPLE VIEW GEOMET
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Jain AK, 2000, IEEE T PATTERN ANAL, V22, P4, DOI 10.1109/34.824819
   Kijak E, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL III, PROCEEDINGS, P309
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Lu WS, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P49
   Miyamori H, 2002, INT C PATT RECOG, P826, DOI 10.1109/ICPR.2002.1048430
   Miyamori H., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P320, DOI 10.1109/AFGR.2000.840653
   Peker KA, 2002, P SOC PHOTO-OPT INS, V4862, P126, DOI 10.1117/12.473029
   Picard R.W., 2000, Affective Computing
   PINGALI G, 2000, P IEEE INT C MULT EX, V3, P1433
   Pingali GS, 1998, PROC CVPR IEEE, P260, DOI 10.1109/CVPR.1998.698618
   ROH MC, 2006, P EUR C COMP VIS, P347
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   Schölkopf B, 2000, NEURAL COMPUT, V12, P1207, DOI 10.1162/089976600300015565
   SHAH M., 1997, MOTION BASED RECOGNI
   Song Y, 2003, IEEE T PATTERN ANAL, V25, P814, DOI 10.1109/TPAMI.2003.1206511
   Sudhir G, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P81, DOI 10.1109/CAIVD.1998.646036
   TJONDRONEGORO D, 2005, P ACM MULTIMEDIA, P1035
   TONG X, 2005, P 13 ACM INT C MULT, P519
   Vapnik V., 1999, NATURE STAT LEARNING
   Wan KW, 2004, LECT NOTES COMPUT SC, V3332, P19
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   Xing LY, 2006, PROC SPIE, V6073, DOI 10.1117/12.652142
   Xiong ZY, 2003, IEEE IMAGE PROC, P5
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   Xu M, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P189
   YE Q, 2003, P INT C AC SPEECH SI, V3, P345
   YE Q, 2005, P 13 ANN ACM INT C M, P455
   Ye QX, 2005, P SOC PHOTO-OPT INS, V5960, P1599, DOI 10.1117/12.632735
   Zhu G., 2006, Proc. ACM Multimedia, P431, DOI [DOI 10.24963/IJCAI.2018/227, DOI 10.1145/1180639.1180728, 10.1145/1180639.1180728]
   [No title captured]
NR 52
TC 62
Z9 65
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2007
VL 9
IS 6
BP 1167
EP 1182
DI 10.1109/TMM.2007.902847
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 215XJ
UT WOS:000249842000008
DA 2024-07-18
ER

PT J
AU Koskela, M
   Smeaton, AF
   Laaksonen, J
AF Koskela, Markus
   Smeaton, Alan F.
   Laaksonen, Jorma
TI Measuring concept similarities in multimedia ontologies: Analysis and
   evaluations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE clustering-based analysis; concept detection; inter-concept relations;
   multimedia ontology
ID IMAGE; SEMANTICS
AB The recent development of large-scale multimedia concept ontologies has provided a new momentum for research in the semantic analysis of multimedia repositories. Different methods for generic concept detection have been extensively studied, but the question of how to exploit the structure of a multimedia ontology and existing inter-concept relations has not received similar attention. In this paper, we present a clustering-based method for modeling semantic concepts on low-level feature spaces and study the evaluation of the quality of such models with entropy-based methods. We cover a variety of methods for assessing the similarity of different concepts in a multimedia ontology. We study three ontologies and apply the proposed techniques in experiments involving the visual and semantic similarities, manual annotation of video, and concept detection. The results show that modeling inter-concept relations can provide a promising resource for many different application areas in semantic multimedia processing.
C1 Aalto Univ, Adapt Informat Res Ctr, FI-02015 Helsinki, Finland.
   Dublin City Univ, Ctr Digital Video Proc & Adapt Informat Cluster, Dublin 9, Ireland.
C3 Aalto University; Dublin City University
RP Koskela, M (corresponding author), Aalto Univ, Adapt Informat Res Ctr, FI-02015 Helsinki, Finland.
EM markus.koskela@hut.fi
RI Koskela, Markus/O-4165-2016; Laaksonen, Jorma/Q-1307-2016
OI Laaksonen, Jorma/0000-0001-7218-3131; Smeaton, Alan
   F./0000-0003-1028-8389
CR [Anonymous], P WORKSH GEN MOD BAS
   [Anonymous], 2006, LSCOM LEXICON DEFINI
   Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214
   BUDANITSKY A, 2001, P NAACL WORKSH WORDN
   Christel MG, 2005, LECT NOTES COMPUT SC, V3568, P134
   Colombo C, 1999, IEEE MULTIMEDIA, V6, P38, DOI 10.1109/93.790610
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   GAUGHAN G, 2006, THESIS DUBLIN CITY U
   Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950
   Jeon J., 2003, P 26 ANN INT ACM SIG
   KENDER JR, 2005, P IEEE C COMP VIS PA
   Kohonen T., 2001, SPRINGER SERIES INFO, V30, P1
   KOSKELA M, 2006, P INT C MULT EXP ICM
   KOSKELA M, 2005, P 4 INT C IM VID RET, P518
   KOSKELA M, 2006, P 3 IFIP C ART INT A
   Laaksonen J, 2002, IEEE T NEURAL NETWOR, V13, P841, DOI 10.1109/TNN.2002.1021885
   Laaksonen JT, 2004, NEURAL NETWORKS, V17, P1121, DOI 10.1016/j.neunet.2004.07.007
   LENAT DB, 1995, COMMUN ACM, V38, P33, DOI 10.1145/219717.219745
   LIN WH, 2006, P IEEE INT C MULT EX
   Liu H, 2004, BT TECHNOL J, V22, P211, DOI 10.1023/B:BTTJ.0000047600.45421.6d
   Naphade M.R., 2005, A light scale concept ontology for multimedia understanding for trecvid 2005
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Naphade MR, 2002, IEEE IMAGE PROC, P145
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   Naphade MR, 2002, IEEE T NEURAL NETWOR, V13, P793, DOI 10.1109/TNN.2002.1021881
   NAPHADE MR, 2003, P INT C IM PROC ICIP, V2, P531
   Popat K, 1997, IEEE T IMAGE PROCESS, V6, P268, DOI 10.1109/83.551697
   PUZICHA J, 1997, P C COMP VIS PATT RE
   Quelhas P, 2005, IEEE I CONF COMP VIS, P883
   Rikert T. D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1046, DOI 10.1109/ICCV.1999.790386
   RUBENSTEIN H, 1965, COMMUN ACM, V8, P627, DOI 10.1145/365628.365657
   Sivic J, 2005, IEEE I CONF COMP VIS, P370
   Smeaton AF, 2005, LECT NOTES COMPUT SC, V3568, P11
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   Szummer M, 1998, 1998 IEEE INTERNATIONAL WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO DATABASE, PROCEEDINGS, P42, DOI 10.1109/CAIVD.1998.646032
   Vailaya A, 1998, PATTERN RECOGN, V31, P1921, DOI 10.1016/S0031-3203(98)00079-X
   Wu Y, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1003, DOI 10.1109/ICME.2004.1394372
   Xie L, 2006, P IEEE INT C MULT EX
   YAN R, 2006, P IEEE INT C MULT EX
   YANAI K, 2005, P 13 ACM INT C MULT
NR 41
TC 18
Z9 45
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 912
EP 922
DI 10.1109/TMM.2007.900137
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800002
OA Green Accepted, Green Published
DA 2024-07-18
ER

PT J
AU Joly, A
   Buisson, O
   Frélicot, C
AF Joly, Alexis
   Buisson, Olivier
   Frelicot, Carl
TI Content-based copy retrieval using distortion-based probabilistic
   similarity search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
ID IMAGE RETRIEVAL
AB Content-based copy retrieval (CBCR) aims at retrieving in a database all the modified versions or the previous versions of a given candidate object. In this paper, we present a copy-retrieval scheme based on local features that can deal with very large databases both in terms of quality and speed. We first propose a new approximate similarity search technique in which the probabilistic selection of the feature space regions is not based on the distribution in the database but on the distribution of the features distortion. Since our CBCR framework is based on local features, the approximation can be strong and reduce drastically the amount of data to explore. Furthermore, we show how the discrimination of the global retrieval can be enhanced during its post-processing step, by considering only the geometrically consistent matches. This framework is applied to robust video copy retrieval and extensive experiments are presented to study the interactions between the approximate search and the retrieval efficiency. Largest used database contains more than I billion local features corresponding to 30 000 h of video.
C1 INRIA Rocquencourt, F-78153 Le Chesnay, France.
   INA, F-94366 Bry Sur Marne, France.
   Univ La Rochelle, F-17042 La Rochelle, France.
C3 La Rochelle Universite
RP Joly, A (corresponding author), INRIA Rocquencourt, F-78153 Le Chesnay, France.
EM alexis.joly@inria.fr; obuisson@ina.fr; carl.frelicot@univ-lr.fr
CR Amsaleg L, 2004, MULTIMED TOOLS APPL, V23, P221, DOI 10.1023/B:MTAP.0000031758.46389.00
   [Anonymous], P ACM SIGMOD INT C M
   BECKMANN N, 1990, SIGMOD REC, V19, P322, DOI 10.1145/93605.98741
   BENNETT KP, 1999, P 5 ACM SIGKDD INT C, P233
   Berchtold S, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P28
   Berrani S. - A., 2003, P 2003 ACM CIKM INT, P24
   BERRANI SA, 2003, P ACM INT WORKSH MUL, P70
   Böhm C, 2001, ACM COMPUT SURV, V33, P322, DOI 10.1145/502807.502809
   BOUJEMAA N, 2004, TRENDS ADV CONTENT B
   Chang EY, 1998, P SOC PHOTO-OPT INS, V3527, P58, DOI 10.1117/12.325852
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   Ciaccia P., 2000, Proceedings of 16th International Conference on Data Engineering (Cat. No.00CB37073), P244, DOI 10.1109/ICDE.2000.839417
   Eickeler S, 1999, INT CONF ACOUST SPEE, P2997, DOI 10.1109/ICASSP.1999.757471
   Faloutsos C., 1995, P 1995 ACM SIGMOD IN, P163
   Ferhatosmanoglu H, 2001, PROC INT CONF DATA, P503, DOI 10.1109/ICDE.2001.914864
   Ferman AM, 2002, IEEE T IMAGE PROCESS, V11, P497, DOI 10.1109/TIP.2002.1006397
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Hampapur A, 2002, P SOC PHOTO-OPT INS, V4676, P194
   Houle ME, 2005, PROC INT CONF DATA, P619
   HSU CY, 2004, P WORKSH MULT SEC MA
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   JAIMES A, 2000, P PAC RIM C MULT, P16
   JAIMES A, 2002, P ACM MULT, P423
   Jain AK, 1999, MULTIMEDIA SYST, V7, P369, DOI 10.1007/s005300050139
   Joly A, 2003, LECT NOTES COMPUT SC, V2728, P414
   JOLY A, 2004, P INT C IM PROC
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Katayama N., 1997, P ACM SIGMOD, P369
   KE Y, 2004, P ACM INT C MULT NEW
   Li C, 2002, IEEE T KNOWL DATA EN, V14, P792, DOI 10.1109/TKDE.2002.1019214
   Lienhart R, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P509, DOI 10.1109/MMCS.1997.609763
   Lowe, 1999, P INT C COMP VIS, P1150, DOI DOI 10.1109/ICCV.1999.790410
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng Y, 2003, PROC CVPR IEEE, P416
   MIKOLAJCZYK K, 2004, INT J COMPUT VIS
   Miller ML, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P182
   OOSTVEEN J, 2002, P INT C REC ADV VIS, P117
   Pua KM, 2004, COMPUT VIS IMAGE UND, V93, P310, DOI 10.1016/j.cviu.2003.10.005
   ROTHGANGER F, 2004, INT J COMPUT VIS
   Schaffalitzky F, 2002, LECT NOTES COMPUT SC, V2350, P414
   Schmid C, 1997, IEEE T PATTERN ANAL, V19, P530, DOI 10.1109/34.589215
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tuncel E., 2002, ACM MULTIMEDIA, P543
   Vu K, 2003, IEEE T KNOWL DATA EN, V15, P1045, DOI 10.1109/TKDE.2003.1209021
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Weber R, 2000, LECT NOTES COMPUT SC, V1777, P21
   Yuan J., 2004, PROC ACM MULTIMEDIA, P61, DOI DOI 10.1145/1026711.1026722
   Zezula P, 1998, VLDB J, V7, P275, DOI 10.1007/s007780050069
   ZHANG DQ, 2004, ACM INT C MULT
   Zhang T., 1996, BIRCH EFFICIENT DATA, V25, P103, DOI [DOI 10.1145/233269.233324, 10.1145/235968.233324]
NR 50
TC 167
Z9 199
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 293
EP 306
DI 10.1109/TMM.2006.886278
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Andreopoulos, Y
   Keralapura, R
   van der Schaar, M
   Chuah, CN
AF Andreopoulos, Yiannis
   Keralapura, Ram
   van der Schaar, Mihaela
   Chuah, Chen-Nee
TI Failure-aware, open-loop, adaptive video streaming with packet-level
   optimized redundancy
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 12th IEEE International Workshop on Quality of Service
CY JUN 07-09, 2004
CL Montreal, CANADA
SP IEEE Commun Soc, TCCC
DE network-layer feedback; open-loop scalable video coding; predicting
   network availability; routing-layer adaptation; video transmission under
   path failures
ID SELECTION
AB A plethora of coding and streaming mechanisms have been proposed for real-time multimedia transmission over the Internet. However, most proposed mechanisms rely only on global (e.g. based on end-to-end measurements), delayed (at least by the round-trip-time), or statistical (often based on simplistic network models) information available about the network state. Based on recently-proposed state-of-the-art open-loop video coding schemes, we propose a new integrated streaming and routing framework for robust and efficient video transmission over networks exhibiting path failures. Our approach explicitly takes into account the network dynamics, path diversity, and the modeled video distortion at the receiver side to optimize the packet redundancy and scheduling. In the derived framework, multimedia streams can be adapted dynamically at the video server based on instantaneous routing-layer information or failure-modeling statistics. The performance of our integrated application and network-layer method is simulated against equivalent approaches that are not optimized based on routing-layer feedback and distortion modeling, and the obtained gains in video quality are quantified.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
   Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
C3 University of California System; University of California Los Angeles;
   University of California System; University of California Davis
RP Andreopoulos, Y (corresponding author), Queen Mary Univ London, Dept Elect Engn, London E1 4NS, England.
EM yiannis.a@elec.qmul.ac.uk; rkeralapura@ucdavis.edu; mihaela@ee.ucla.edu;
   chuah@ece.ucdavis.edu
RI Andreopoulos, Ioannis/C-8377-2009
OI Chuah, Chen-Nee/0000-0002-2772-387X
CR ANDERSON D, 2001, P ACM S OP SYST PRIN
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], P PACK VID WORKSH NA
   [Anonymous], 1998, RFC
   APOSTOLOPOULOS JG, 2001, VISUAL COMMUN IMAGE
   Begen AC, 2005, SIGNAL PROCESS-IMAGE, V20, P39, DOI 10.1016/j.image.2004.09.002
   BOUTREMANS C, 2002, P ACM INT WORKSH NET
   CASTRO M, 2003, P ACM S OP SYST PRIN
   Chakareski J, 2004, IEEE DATA COMPR CONF, P202
   Chakareski J, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P1001
   Chen PS, 2004, IEEE T CIRC SYST VID, V14, P1183, DOI 10.1109/TCSVT.2004.833165
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   CIVANLAR MR, 2000, ADV MULTIMEDIA SYSTE
   Dilley J, 2002, IEEE INTERNET COMPUT, V6, P50, DOI 10.1109/MIC.2002.1036038
   FLOYD S, 2000, P ACM INT MEAS WORKS
   Frossard P, 2001, IEEE T IMAGE PROCESS, V10, P1815, DOI 10.1109/83.974566
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Hengartner U., 2002, P ACM SIGCOMM INT ME
   IANNACCONE G, 2002, P ACM SIGCOMM INT ME
   IANNACCONE G, 2004, IEEE NETWORK     MAR
   *IEC, 2003, JTC1SC29WG11 IEC MPE
   JANNOTTI J, 2000, P ACM S OP SYST DES
   JIANG W, 1999, P VISUAL COMMUN IMAG
   KERALAPURA R, 2004, P IEEE INT WORKSH QU
   LABOVITZ C, 2000, P ACM SIGCOMM INT ME
   Luo L, 2004, SIGNAL PROCESS-IMAGE, V19, P601, DOI 10.1016/j.image.2004.05.004
   MARKOPOULOU A, 2004, P IEEE INF
   Miao ZR, 2000, CONF REC ASILOMAR C, P1357, DOI 10.1109/ACSSC.2000.911213
   MUNTEANU A, P IEEE INT C IM PROC, V2, P61
   NGUYEN T, 2003, P IEEE INF APR
   Oran D., 1990, IETF RFC 1142
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   REJAIE R, 1999, P ACM SIGCOMM INT ME
   Rubenstein D, 2002, IEEE ACM T NETWORK, V10, P381, DOI 10.1109/TNET.2002.1012369
   Rusert T, 2003, PROC SPIE, V5150, P682, DOI 10.1117/12.509881
   SAVAGE S, 1999, IEEE MICRO       JAN
   SCHWARZ H, 2004, JTC1SC29WG11 MPEG IS
   Secker A, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P1029, DOI 10.1109/ICIP.2001.958672
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   TILLIER C, 2004, P IEEE INT C AC SPEE, V3, P125
   van der Schaar M, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P489
   Wang Y, 2002, IEEE T CIRC SYST VID, V12, P438, DOI 10.1109/TCSVT.2002.800320
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 44
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1274
EP 1290
DI 10.1109/TMM.2006.884607
PG 17
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700016
DA 2024-07-18
ER

PT J
AU González, S
   Navarro, A
   López, J
   Zapata, EL
AF Gonzalez, Sonia
   Navarro, Angeles
   Lopez, Juan
   Zapata, Emilio L.
TI A case study of load sharing based on popularity in distributed VoD
   systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed system; load sharing; partial replication; performance
   modeling; popularity; video-on-demand
AB In our research, we consider a distributed video-on-demand (VoD) system in which only the most popular videos are replicated in all the servers, whereas the rest of them are distributed through the system following some allocation scheme. In this paper, we present an algorithm to efficiently share the load in such a system and an analytical model that captures the performance of this algorithm, which we validate through simulations. One novelty in our work is that our analytical model lets us relate popularity and partial replication of some of the videos and to predict the user waiting time. We exploit such relationships to assist the system designer to select the size of the servers and network, the optimal number of servers to maintain short waiting time and to predict when the network encounters bottleneck.
C1 Univ Malaga, Dept Comp Architecture, E-29071 Malaga, Spain.
C3 Universidad de Malaga
RP González, S (corresponding author), Univ Malaga, Dept Comp Architecture, E-29071 Malaga, Spain.
EM sonia@ac.uma.es; angeles@ae.uma.es; juan@ac.uma.es; ezapata@ac.uma.es
RI NAVARRO, SONIA GONZALEZ/T-1080-2019; G. Navarro, Angeles/L-5300-2014
OI NAVARRO, SONIA GONZALEZ/0000-0002-5636-7042; G. Navarro,
   Angeles/0000-0002-4140-2589
CR Aggarwal CC, 2001, IEEE T COMPUT, V50, P97, DOI 10.1109/12.908987
   Allen A. O., 1990, PROBABILITY STAT QUE
   CHERVENAK A, 1994, 94847 UDBCSD
   GONZALEZ S, 2005, UMADAC0506
   Gross D., 1998, FUNDAMENTALS QUEUEIN
   Lazar I., 2001, IT Professional, V3, P47, DOI 10.1109/6294.946620
   LEUNG Y, 2003, COMP03009 HONG KONG
   Little T D., 1995, Multimedia Systems, V2, P280
   Little T. D. C., 1994, IEEE Multimedia, V1, P14, DOI 10.1109/MMUL.1994.318978
   Mourad AN, 1996, MULTIMEDIA SYST, V4, P70, DOI 10.1007/s005300050013
   Ng JKY, 2000, J SYST SOFTWARE, V51, P217, DOI 10.1016/S0164-1212(99)00125-9
   Tay YC, 2000, IEEE T KNOWL DATA EN, V12, P410, DOI 10.1109/69.846293
NR 12
TC 9
Z9 9
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1299
EP 1304
DI 10.1109/TMM.2006.884604
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700018
DA 2024-07-18
ER

PT J
AU Park, SB
   Kim, CS
   Lee, SU
AF Park, Sung-Bum
   Kim, Chang-Su
   Lee, Sang-Uk
TI Error resilient 3-D mesh compression
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error concealment; POCS; robust transmission; shape adaptive mesh
   partitioning; 3-D mesh compression
ID TRANSMISSION
AB An error resilient three-dimensional (3-D) mesh coding system is proposed in this paper. The encoder uses a shape adaptive data partitioning scheme to alleviate the effect of error propagation. An input mesh surface is coarsely divided into smooth and detailed regions, and each region is further divided into partitions of similar sizes. Then, those partitions are progressively compressed and their joint boundaries are compressed using the boundary edge collapse rule. At the decoder, the boundary edge collapse rule facilitates the seamless assembly of the partitions. When no data is available for a partition due to transmission errors, we employ a concealment scheme based on the projection onto convex sets (POCS) theory. Simulation results demonstrate that the proposed algorithm reconstructs 3-D mesh surfaces faithfully even in severe error prone environments.
C1 Seoul Natl Univ, Sch Elect Engn, Signal Proc Lab, Seoul, South Korea.
   Korea Univ, Dept Elect & Comp Engn, Seoul 136701, South Korea.
C3 Seoul National University (SNU); Korea University
RP Park, SB (corresponding author), Seoul Natl Univ, Sch Elect Engn, Signal Proc Lab, Seoul, South Korea.
EM park@ipl.snu.ac.kr; cskim@ieee.org; sanguk@ipl.snu.ac.kr
OI Kim, Chang-Su/0000-0002-4276-1831
CR Alliez P, 2001, COMP GRAPH, P195, DOI 10.1145/383259.383281
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   ALREGIB G, 2002, P ICIP 2002 SEP, P161
   Carmo Do., 1976, DIFFERENTIAL GEOMETR
   Cignoni P, 1998, COMPUT GRAPH FORUM, V17, P167, DOI 10.1111/1467-8659.00236
   Combettes PL, 1993, IEEE T IMAGE PROCESS, V2, P269, DOI 10.1109/83.217232
   DEERING M, 1995, P 22 ANN C COMP GRAP, P13
   Gandoin PM, 2002, ACM T GRAPHIC, V21, P372, DOI 10.1145/566570.566591
   Girod B, 1999, P IEEE, V87, P1707, DOI 10.1109/5.790632
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Hoppe H., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P99, DOI 10.1145/237170.237216
   *ITUT, 2003, H323 ITUT
   Kanai T, 2000, IEEE COMPUT GRAPH, V20, P62, DOI 10.1109/38.824544
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   Khodakovsky A., 2002, Geometric Modeling for Scientific Visualization
   Liepa P., 2003, Symposium on Geometry Processing, P200
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   Paek H, 1998, IEEE T CIRC SYST VID, V8, P358, DOI 10.1109/76.678636
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   PARK SB, 2002, P ICIP 2002 SEP, V2, P233
   Peng JL, 2005, J VIS COMMUN IMAGE R, V16, P688, DOI 10.1016/j.jvcir.2005.03.001
   Press W. H., 2002, Numerical Recipes in C: the Art of Scientific Computing, V2nd ed., DOI DOI 10.1119/1.14981
   Schulzrinne H., 1996, Rtp: a transport protocol for real-time applications
   Sharf A, 2004, ACM T GRAPHIC, V23, P878, DOI 10.1145/1015706.1015814
   Sim JY, 2005, IEEE T CIRC SYST VID, V15, P854, DOI 10.1109/TCSVT.2005.848349
   Taubin G., 1995, P 22 ANN C COMP GRAP, P351, DOI DOI 10.1145/218380.218473
   Touma C, 1998, GRAPHICS INTERFACE '98 - PROCEEDINGS, P26
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WEN J, 1997, P ICIP 97 OCT
   WITTEN IH, 1987, COMMUN ACM, V30, P520, DOI 10.1145/214762.214771
   Yan ZD, 2005, IEEE T CIRC SYST VID, V15, P138, DOI 10.1109/TCSVT.2004.837023
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
   Yan ZD, 1999, IEEE DATA COMPR CONF, P560, DOI 10.1109/DCC.1999.785717
   Yang S, 2004, IEEE T CIRC SYST VID, V14, P1249, DOI 10.1109/TCSVT.2004.835153
   YANG YY, 1995, IEEE T IMAGE PROCESS, V4, P896, DOI 10.1109/83.392332
   Zakhor A, 1992, IEEE T CIRC SYST VID, V2, P91, DOI 10.1109/76.134377
   ZORIN D, 1997, THESIS CALTECH PASAD
   CYBERWARE SAMPLE MOD
NR 38
TC 16
Z9 17
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 885
EP 895
DI 10.1109/TMM.2006.879914
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400001
DA 2024-07-18
ER

PT J
AU Fei, ZM
   Ammar, MH
   Kamel, I
   Mukherjee, S
AF Fei, ZM
   Ammar, MH
   Kamel, I
   Mukherjee, S
TI An active buffer management technique for providing interactive
   functions in broadcast video-on-demand systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE interactive functions; multimedia streaming; video-on-demand
AB Multicast delivery is an efficient approach to the provision of a video-on-demand (VoD) service. Interacting with the video stream is a desirable feature for users. However, it is a challenging task to provide the functionality in the multicast environment because a lot of users share multicast delivery channels. In this paper, we propose an active buffer management technique to provide interactive functions in broadcast VoD systems. In our scheme, the client can selectively prefetch segments from broadcast channels based on the observation of the play point in its local buffer. The content of the buffer is adjusted in such a way that the relative position of the play point is kept in the middle part of the buffer. Our simulations show that the active buffer management scheme can implement interactive actions through buffering with a high probability in a wide range of user interaction levels.
C1 Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA.
   Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.
   Zayed Univ, Coll Informat Syst, Dubai, U Arab Emirates.
   Bell Labs, Lucent Technol, Holmdel, NJ 07733 USA.
C3 University of Kentucky; University System of Georgia; Georgia Institute
   of Technology; Zayed University; Alcatel-Lucent; Lucent Technologies;
   AT&T
RP Univ Kentucky, Dept Comp Sci, Lexington, KY 40506 USA.
EM fei@cs.uky.edu; ammar@cc.gatech.edu; ibrahim.kamel@zu.ac.ae;
   sarit@lucent.com
OI Kamel, Ibrahim/0000-0001-5546-939X
CR Abram-Profeta EL, 1998, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS, P66, DOI 10.1109/MMCS.1998.693626
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   ALMEROTH KC, 1995, P ICC 95 SEATTL WA, P292
   ALMEROTH KC, 1994, P INT C COMP COMM NE, P292
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   Fei ZM, 1999, LECT NOTES COMPUT SC, V1736, P152
   Fei ZM, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P949, DOI 10.1109/MMCS.1999.778617
   GAO L, 1998, P NOSSDAV 98
   HUA KA, 1997, P ACM SIGC 97
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Liao W, 1997, IEEE MULTIMEDIA, V4, P51, DOI 10.1109/93.641879
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
NR 12
TC 6
Z9 8
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 942
EP 950
DI 10.1109/TMM.2005.854403
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900015
DA 2024-07-18
ER

PT J
AU Hariharakrishnan, K
   Schonfeld, D
AF Hariharakrishnan, K
   Schonfeld, D
TI Fast object tracking using adaptive block matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive motion estimation; K-means clustering; segmentation; visual
   tracking
ID VIDEO; REPRESENTATION; MESH
AB We propose a fast object tracking algorithm that predicts the object contour using motion vector information. The segmentation step common in region-based tracking methods is avoided, except for the initialization of the object. Tracking is achieved by predicting the object boundary using block motion vectors followed by updating the contour using occlusions/disocclusion detection. An adaptive block-based approach has been used for estimating motion between frames. An efficient modulation scheme is used to control the gap between frames used for motion estimation. The algorithm for detecting disocclusion proceeds in two steps. First, uncovered regions are estimated from the displaced frame difference. These uncovered regions are classified into actual disocclusions and false alarms by observing the motion characteristics of uncovered regions. Occlusion and disocclusion are considered as dual events and this relationship is explained in detail. The algorithm for detecting occlusion is developed by modifying the disocclusion detection algorithm in accordance with the duality principle. The overall tracking algorithm is computationally superior to existing region-based methods for object tracking. The immediate applications of the proposed tracking algorithm are video compression using MPEG-4 and content retrieval based on standards like H.264. Preliminary simulation results demonstrate the performance of the proposed algorithm.
C1 Univ Illinois, Multimedia Commun Lab, Dept Elect & Comp Engn, Chicago, IL 60607 USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital
RP Motorola Inc, Multimedia Grp, Bangalore, Karnataka, India.
EM h_karthik@yahoo.com; dans@uic.edu
CR Altunbasak Y, 1997, IEEE T IMAGE PROCESS, V6, P1270, DOI 10.1109/83.623190
   AMINI AA, 1990, IEEE T PATTERN ANAL, V12, P855, DOI 10.1109/34.57681
   [Anonymous], 1993, Cluster Analysis
   Black MJ, 1998, INT J COMPUT VISION, V26, P63, DOI 10.1023/A:1007939232436
   Fu Y, 2000, IEEE T IMAGE PROCESS, V9, P2051, DOI 10.1109/83.887973
   FU Y, 2001, IEEE T CIRCUITS SYST, V11, P788
   Gatica-Perez D, 2001, IEEE T CIRC SYST VID, V11, P603, DOI 10.1109/76.920190
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   HUANG CL, 1994, IEEE T CIRCUITS SYST, V4, P4251
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Peterfreund N, 1997, IEEE NONRIGID AND ARTICULATED MOTION WORKSHOP, PROCEEDINGS, P70, DOI 10.1109/NAMW.1997.609855
   SALEMBIER P, 1995, P IEEE, V83, P843, DOI 10.1109/5.387088
   SALEMBIER P, 1994, IEEE T IMAGE PROCESS, V3, P639, DOI 10.1109/83.334980
   Schonfeld D, 2000, J VIS COMMUN IMAGE R, V11, P154, DOI 10.1006/jvci.1999.0432
   Tekalp AM, 1998, P IEEE, V86, P1029, DOI 10.1109/5.687828
   Terzopoulos D., 1992, Active Vision, P3
   WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981
   WANG Y, 1994, IEEE T IMAGE PROCESS, V3, P610, DOI 10.1109/83.334982
NR 18
TC 64
Z9 80
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 853
EP 859
DI 10.1109/TMM.2005.854437
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900006
DA 2024-07-18
ER

PT J
AU Tzavidas, S
   Katsaggelos, AK
AF Tzavidas, S
   Katsaggelos, AK
TI A multicamera setup for generating stereo panoramic video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Image Processing
CY SEP 22-25, 2002
CL ROCHESTER, NY
SP IEEE Signal Proc Soc
DE circular projections; panoramic video; stereo vision; video signal
   processing
AB Traditional visual communication systems convey only two-dimensional (2-D) fixed field-of-view (FOV) video information. The viewer is presented with a series of flat, non-stereoscopic images, which fail to provide a realistic sense of depth. Furthermore, traditional video is restricted to only a small part of the scene, based on the director's discretion and the user is not allowed to "look around" in an environment. The objective of this work is to address both of these issues and develop new techniques for creating stereo panoramic video sequences. A stereo panoramic video sequence should be able to provide the viewer with stereo vision at any direction (complete 360-degree FOV) at video rates. In this paper, we propose a new technique for creating stereo panoramic video using a multicamera approach, thus creating a high-resolution output. We present a setup that is an extension of a previously known approach, developed for the generation of still stereo panoramas, and demonstrate that it is capable of creating high-resolution stereo panoramic video sequences. We further explore the limitations involved in a practical implementation of the setup, namely the limited number of cameras and the nonzero physical size of real cameras. The relevant tradeoffs are identified and studied.
C1 Northwestern Univ, Evanston, IL 60208 USA.
C3 Northwestern University
RP Motorola Inc, Performance Anal Dept, Arlington Hts, IL 60004 USA.
EM stavros_tzavidas@alumni.northwestern.edu
RI Katsaggelos, Aggelos K/B-7233-2009
OI Katsaggelos, Aggelos K/0000-0003-4554-0070
CR [Anonymous], SIGGRAPH 97
   [Anonymous], SIGGRAPH 98
   CHEN SE, 1995, P F ACM SIGGRAPH
   FREEMAN M, 1980, 35 MM HDB COMPLETE C
   Gluckman Joshua, 1998, P DARPA IM UND WORKS, P2
   Gumustekin S, 1996, THIRD IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV '96, PROCEEDINGS, P50, DOI 10.1109/ACV.1996.571998
   Huang HC, 1998, GRAPH MODEL IM PROC, V60, P196, DOI 10.1006/gmip.1998.0467
   Nalwa V., 1996, A true omnidirectional viewer
   NAYAR SK, 1997, P DARPA IM UND WORKS
   Onoe Y, 1998, COMPUT VIS IMAGE UND, V71, P154, DOI 10.1006/cviu.1998.0705
   Peleg S, 2000, PROC CVPR IEEE, P208, DOI 10.1109/CVPR.2000.855821
   PELEG S, 1999, P IEEE C COMP VIS PA
   SALMON TO, VISION SCI, V4
   Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677
   Szeliski Richard, 1997, P ACM SIGGRAPH
   TZAVIDAS S, 2002, P 2002 SPIE C VCIP S
   TZAVIDAS S, 2002, P IEEE INT C IM PROC
   TZAVIDAS S, 2001, THESIS NW U EVANSTON
   Xiong YL, 1997, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.1997.609326
NR 19
TC 23
Z9 29
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 880
EP 890
DI 10.1109/TMM.2005.854430
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900009
DA 2024-07-18
ER

PT J
AU Zhu, XQ
   Elmagarmid, AK
   Xue, XY
   Wu, LD
   Catlin, AC
AF Zhu, XQ
   Elmagarmid, AK
   Xue, XY
   Wu, LD
   Catlin, AC
TI Insight video: Toward hierarchical video content organization for
   efficient browsing, summarization and retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE video browsing; video content organization; video retrieval; video
   similarity assessment
ID CAMERA MOTION; DATABASE; SYSTEM; IMAGE
AB Hierarchical video browsing and feature-based video retrieval are two standard methods for accessing video content. Very little research, however, has addressed the benefits of integrating these two methods for more effective and efficient video content access. In this paper, we introduce InsightVideo, a video analysis and retrieval system, which joins video content hierarchy, hierarchical browsing and retrieval for efficient video access. We propose several video processing techniques to organize the content hierarchy of the video. We first apply a camera motion classification and key-frame extraction strategy that operates in the compressed domain to extract video features. Then, shot grouping, scene detection and pairwise scene clustering strategies are applied to construct the video content hierarchy. We introduce a video similarity evaluation scheme at different levels (key-frame, shot, group, scene, and video.) By integrating the video content hierarchy and the video similarity evaluation scheme, hierarchical video browsing and retrieval are seamlessly integrated for efficient content access. We construct a progressive video retrieval scheme to refine user queries through the interactions of browsing and retrieval. Experimental results and comparisons of camera motion classification, key-frame extraction, scene detection, and video retrieval are presented to validate the effectiveness and efficiency of the proposed algorithms and the performance of the system.
C1 Univ Vermont, Dept Comp Sci, Burlington, VT 05405 USA.
   Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   Fudan Univ, Dept Comp Sci, Shanghai 200433, Peoples R China.
C3 University of Vermont; Purdue University System; Purdue University;
   Fudan University
RP Univ Vermont, Dept Comp Sci, Burlington, VT 05405 USA.
EM xqzhu@cs.uvm.edu
RI ZOU, Fengcai/ABE-4598-2021
OI ZOU, Fengcai/0000-0002-9613-3734; Zhu, Xingquan/0000-0003-4129-9611
CR ADJEROH DA, 1998, P INT WORKSH MULT DA
   [Anonymous], 2000, P 2000 ACM WORKSH MU, DOI DOI 10.1145/357744.357942
   ARDIZZONE E, 1999, P IEEE C MULT COMP S
   Aslandogan YA, 1999, IEEE T KNOWL DATA EN, V11, P56, DOI 10.1109/69.755615
   Avrithis YS, 1998, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES - PROCEEDINGS, P91, DOI 10.1109/IVL.1998.694508
   Chabot T., 2020, SPIE, V1451
   Chang S., 1996, P 33 ANN CLIN LIB AP
   Chang SF, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P313, DOI 10.1145/266180.266382
   Davenport Glorianna, 1994, IEEE MULTIMEDIA, V1, P73
   DIMITROVA N, 1998, P SPIE, V3022
   Divakaran A, 2001, J ELECTRON IMAGING, V10, P909, DOI 10.1117/1.1406507
   DIVAKARAN A, 2002, P IEEE ICIP C ROCH N
   Dorai C., 1999, 1999 IEEE Third Workshop on Multimedia Signal Processing (Cat. No.99TH8451), P147, DOI 10.1109/MMSP.1999.793812
   ELMAGARMID A, 1997, VIDEO DATA BASES ISS
   Faloutsos C., 1994, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V3, P231, DOI 10.1007/BF00962238
   Fan JP, 2001, LECT NOTES COMPUT SC, V2195, P837
   Fan JP, 2001, J ELECTRON IMAGING, V10, P895, DOI 10.1117/1.1406944
   Fan JP, 2001, SIGNAL PROCESS-IMAGE, V16, P553, DOI 10.1016/S0923-5965(00)00036-9
   HAMPAPUR A, 1997, P SPIE, V3022
   Hanjalic A, 1999, IEEE T CIRC SYST VID, V9, P1280, DOI 10.1109/76.809162
   HAUPTMANN A, 1998, ADL 98 ADV DIG LIB C
   HOWARD DW, 1996, IEEE COMPUT, V29, P46
   Jain A.K., 1998, ALGORITHMS CLUSTERIN
   JAIN R, 1983, IEEE T PATTERN ANAL, V5, P58, DOI 10.1109/TPAMI.1983.4767345
   Jeannin S, 2001, IEEE T CIRC SYST VID, V11, P720, DOI 10.1109/76.927428
   Jiang HT, 1998, IEEE T KNOWL DATA EN, V10, P947, DOI 10.1109/69.738359
   KOBLA V, 1996, CARTR839 CFAR
   LACASCIA M, 1996, P ICASSP ATL GA
   LAZARESCU M, 2002, P IEEE ICME C LAUSS
   Li SZ, 1999, IEEE T NEURAL NETWOR, V10, P439, DOI 10.1109/72.750575
   Lienhart R, 1997, P SOC PHOTO-OPT INS, V3312, P271, DOI 10.1117/12.298460
   LIN T, 2000, P ICPR BARC SPAIN
   Liu Xiaoming., 1999, P ACM INT C MULTIMED, P41, DOI [10.1145/319878.319889, DOI 10.1145/319878.319889]
   MANJUNATH BS, 2002, INTRO MPEG7 MULTIMED
   Ngo C.W., 2001, Proc. ACM Multimedia, P51, DOI DOI 10.1145/500141.500151
   Ngo CW, 2000, PROC CVPR IEEE, P768, DOI 10.1109/CVPR.2000.854952
   OGLE VE, 1995, COMPUTER, V28, P40, DOI 10.1109/2.410150
   Pentland A, 1996, INT J COMPUT VISION, V18, P233, DOI 10.1007/BF00123143
   Rui Y, 1999, MULTIMEDIA SYST, V7, P359, DOI 10.1007/s005300050138
   SHAHRARAY B, 1995, P SOC PHOTO-OPT INS, V2417, P512, DOI 10.1117/12.206078
   Smith J. R., 1996, Proceedings ACM Multimedia 96, P87, DOI 10.1145/244130.244151
   Smith JR, 1999, IEEE T MULTIMEDIA, V1, P157, DOI 10.1109/6046.766737
   SMITH JR, 1996, 4599625 CTR
   Srinivasan MV, 1997, PATTERN RECOGN, V30, P593, DOI 10.1016/S0031-3203(96)00106-9
   Szeliski R, 1996, IEEE COMPUT GRAPH, V16, P22, DOI 10.1109/38.486677
   Tan YP, 2000, IEEE T CIRC SYST VID, V10, P133, DOI 10.1109/76.825867
   WACTLAR H, 2000, P IM C MON
   Wolf W, 1996, INT CONF ACOUST SPEE, P1228, DOI 10.1109/ICASSP.1996.543588
   Xiong W, 1998, COMPUT VIS IMAGE UND, V71, P166, DOI 10.1006/cviu.1998.0711
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   YEUNG M, 1996, P ICPR 96 VIENN AUST
   Yeung MM, 1997, IEEE T CIRC SYST VID, V7, P771, DOI 10.1109/76.633496
   Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4
   ZHANG HJ, 1995, P ACM MULT C SAN FRA
   ZHANG HJ, 1993, ACM MULTIMEDIA SYST, V1
   Zhang Y. T., 1998, P ICIP CHIC IL, V1, P866
   ZHONG D, 1997, CLUSTERING METHODS V
   ZHOU W, 2001, P ACM MULT C WORKSH
NR 58
TC 51
Z9 57
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 648
EP 666
DI 10.1109/TMM.2005.850977
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000007
DA 2024-07-18
ER

PT J
AU Zimmermann, R
   Ghandeharizadeh, S
AF Zimmermann, R
   Ghandeharizadeh, S
TI Highly available and heterogeneous continuous media storage systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE continuous media servers; fault tolerance; data protection; high
   availability; heterogeneous storage; magnetic disk replacement;
   streaming media servers
AB A number of recent technological trends have made data intensive applications such as continuous media (audio and video) servers a reality. These servers store and retrieve large volumes of data using magnetic disks. Servers consisting of multiple nodes and large arrays of heterogeneous disk drives have become a fact of life for several reasons. First, magnetic disks might fail. Failed disks are almost always replaced with newer disk models because the current technological trend for these devices is one of annual increase in both performance and storage capacity. Second, storage requirements are ever increasing, forcing servers to be scaled up progressively. In this study, we present a framework to enable parity-based data protection for heterogeneous storage systems and to compute their mean lifetime. We describe the tradeoffs associated with three alternative techniques: independent subservers, dependent subservers, and disk merging. The disk merging approach provides a solution for systems that require highly available secondary storage in environments that also necessitate maximum flexibility.
C1 Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
C3 University of Southern California
RP Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM rzimmerm@imsc.usc.edu; shahram@pollux.usc.edu
RI Zimmermann, Roger/D-7944-2015
OI Zimmermann, Roger/0000-0002-7410-2590
CR BERSON S, 1995, P ACM SIGMOD INT C M, P364
   BIERSACK EW, 1997, P MULT COMP NETW 199, P106
   Bitton D., 1988, Proceedings of the Fourteenth International Conference on Very Large Databases, P331
   CHEN LT, 1995, P 21 INT C VER LARG, P110
   CHEN PM, 1994, ACM COMPUT SURV, V26, P145, DOI 10.1145/176979.176981
   Dan A., 1995, P ACM SIGMOD INT C M, P376
   GEMMELL DJ, 1995, IEEE COMPUTER    MAY
   GHANDEHARIZADEH S, 1993, P FDN DAT ORG ALG FO
   Gibson Garth Alan, 1991, THESIS U CALIFORNIA
   GROCHOWSKI E, 2001, INT RAT TREND STOR P
   MOURAD A, 1995, P 2 IASTED ISMM INT, P113
   OZDEN B, 1996, P ACM SIGMOD INT C M, P79
   Patterson D. A., 1988, SIGMOD Record, V17, P109, DOI 10.1145/971701.50214
   Siewiorek D., 1982, THEORY PRACTICE RELI
   TEWARI R, 1996, P IS T SPIE MULT COM
   Tobagi F. A., 1993, Proceedings ACM Multimedia 93, P393, DOI 10.1145/166266.168435
   WOLF JL, 1995, P ACM SIGMETRICS OTT
   ZIMMERMANN R, 1997, P 5 ACM MULT C SEATT, P277
   ZIMMERMANN R, 1999, CONTINUOUS MEDIA PLA
   [No title captured]
NR 20
TC 4
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2004
VL 6
IS 6
BP 886
EP 896
DI 10.1109/TMM.2004.837231
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 872QU
UT WOS:000225224200011
DA 2024-07-18
ER

PT J
AU Chang, SK
   Costagliola, G
   Jungert, E
   Orciuoli, F
AF Chang, SK
   Costagliola, G
   Jungert, E
   Orciuoli, F
TI Querying distributed multimedia databases and data sources for sensor
   data fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE data fusion; distributed database; multimedia database; query language;
   query processing; sensor data fusion
AB Sensor data fusion imposes a number of novel requirements on query languages and query processing techniques. A spatial/temporal query language called SigmaQL has been proposed to support the retrieval and fusion of multimedia information from multiple sources and databases. In this paper we investigate fusion techniques, multimedia data transformations and SigmaQL query processing techniques for sensor data fusion. Fusion techniques including fusion by the merge operation, the detection of moving objects, and the incorporation of belief values, have been developed. An experimental prototype has been implemented and tested to demonstrate the feasibility of these techniques.
C1 Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.
   Univ Salerno, Dipartimento Matemat & Informat, I-84100 Salerno, Italy.
   FOA, Swedish Def Res Inst, SE-17290 Linkoping, Sweden.
   Univ Salerno, Dipartimento Ingn Informaz & Matemat Applicata, I-84100 Salerno, Italy.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh; University of Salerno; FOI - Swedish Defence Research
   Agency; University of Salerno
RP Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.
EM chang@cs.pitt.edu; gencos@unisa.it; jungert@foi.se;
   orciuoli@crmpa.unisa.it
RI Costagliola, Gennaro/GLV-1530-2022; Orciuoli, Francesco/A-4860-2013
OI Costagliola, Gennaro/0000-0003-3816-7765; Orciuoli,
   Francesco/0000-0001-6899-4396
CR BAMER C, 1999, 1 INT WORKSH MOB AG, P1
   Baumann J., 1998, World Wide Web, V1, P123, DOI 10.1023/A:1019211714301
   CHAKRABARTI K, 2000, 16 IN C DAT ENG SAN
   Chandra S., 2003, CON MA SC T
   Chang S.-K., 1996, Symbolic Projection for Image Information Retrieval and Spatial Reasoning, DOI DOI 10.1016/B978-0-12-168030-5.X5000-1
   Chang SK, 1998, INT J COOP INF SYST, V7, P167, DOI 10.1142/S021884309800009X
   CHANG SK, 2002, P 5 INT C VIS INF SY
   Chee-Yee Chong, 1999, Proceedings of the Second International Conference on Information Fusion. FUSION '99, P239
   GRAFE G, 1993, ACM COMPUT SURV, V25
   Horney T, 2003, FUSION 2003: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE OF INFORMATION FUSION, VOLS 1 AND 2, P530
   JENSEN FV, 1996, INTRO BAESIAN NETWOR
   Jungert E, 1999, P SOC PHOTO-OPT INS, V3694, P12, DOI 10.1117/12.354474
   JUNGERT E, 1999, P 2 INT C INF FUS FU
   JUNGERT E, 2002, P 3 INT C INF FUS FU
   JUNGERT E, 1999, P C MULT DAT IM COMM
   KLEIN LA, 1993, IEEE T AERO ELEC SYS, V29, P317, DOI 10.1109/7.210070
   Kosch H., 2001, Multimedia Databases and Image Communication. Second International Workshop, MDIV 2001. Proceedings (Lecture Notes in Computer Science Vol.2184), P152
   LANGE D, 1999, PROGRAMMING DEPLOYIN
   LEE SY, 1992, PATTERN RECOGN, V25, P305, DOI 10.1016/0031-3203(92)90112-V
   Parker JR, 1999, P SOC PHOTO-OPT INS, V3720, P330, DOI 10.1117/12.357173
   STONEBRAKER M, 1975, SIGMOD
   VELEZ B, 1997, P 20 ACM C RES DEV I
   Waltz E., 1990, MULTISENSOR DATA FUS
   White F. E., 1998, Proceedings of EuroFusio 98. International Data Fusion Conference, P49
   YAGER FE, 1994, ADV DEMPSTERSHAFER T
NR 25
TC 12
Z9 13
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2004
VL 6
IS 5
BP 687
EP 702
DI 10.1109/TMM.2004.834862
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 854XI
UT WOS:000223936800003
DA 2024-07-18
ER

PT J
AU Lou, DC
   Sung, CH
AF Lou, DC
   Sung, CH
TI A steganographic scheme for secure communications based on the chaos and
   Euler theorem
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE asymmetric orbit; covert communication; dynamic system;
   man-in-the-middle attack; steganography
ID WATERMARKING
AB Steganography has been proposed as a methodology for transmitting messages through innocuous covers to conceal their existence. This work proposes an asymmetric image steganographic method based on a chaotic dynamic system and the Euler theorem. The hidden message can be recovered using orbits different from the embedding orbits, and the original image is not required to extract the hidden message. Experimental results and discussions reveal that the proposed scheme possesses security, imperceptibility and survivability.
C1 Natl Def Univ, Chung Chen Inst Technol, Dept Elect Engn, Taoyuan 33509, Taiwan.
C3 National Defense University - Taiwan
RP Natl Def Univ, Chung Chen Inst Technol, Dept Elect Engn, Taoyuan 33509, Taiwan.
EM dclou@ccit.edu.tw
OI Lou, Der-Chyuan/0000-0001-6697-7648
CR ANDERSON RJ, 1998, IEEE J SEL AREA COMM, V16, P525
   [Anonymous], The Prisoners' Problem and the Subliminal Channel, DOI 10.1007/978-1-4684-4730-95
   [Anonymous], 1986, An introduction to chaotic dynamical systems
   Bender W, 1996, IBM SYST J, V35, P313, DOI 10.1147/sj.353.0313
   Cox I. J., 2000, Proceedings International Conference on Information Technology: Coding and Computing (Cat. No.PR00540), P6, DOI 10.1109/ITCC.2000.844175
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   DYSON FJ, 1992, AM MATH MON, V99, P603, DOI 10.2307/2324989
   Johnson NF, 1998, LECT NOTES COMPUT SC, V1525, P273
   Knuth D. E., 1969, The Art of Computer Programming, Vol. 2, Seminumerical Algorithms, V2
   KONFELDER LM, 1978, COMMUN ACM, V21, P179
   Kurak C., 1992, Proceedings. Eighth Annual Computer Security Applications Conference (Cat. No.92TH0470-5), P153, DOI 10.1109/CSAC.1992.228224
   Lou DC, 1996, ELECTRON LETT, V32, P984, DOI 10.1049/el:19960662
   Lou DC, 2000, IEEE T CONSUM ELECTR, V46, P31, DOI 10.1109/30.826378
   Lou DC, 2001, IEICE T FUND ELECTR, VE84A, P2052
   Lu CS, 2000, IEEE T MULTIMEDIA, V2, P209, DOI 10.1109/6046.890056
   Marvel LM, 1999, IEEE T IMAGE PROCESS, V8, P1075, DOI 10.1109/83.777088
   MATSUI K, 1994, P IMA INTELLECTUAL P, V1, P187
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Rhee M.Y., 1994, CRYPTOGRAPHY SECURE
   RIVEST RL, 1978, COMMUN ACM, V21, P120, DOI [10.1145/359340.359342, 10.1145/357980.358017]
   Zöllner J, 1998, LECT NOTES COMPUT SC, V1525, P344
NR 21
TC 49
Z9 50
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2004
VL 6
IS 3
BP 501
EP 509
DI 10.1109/TMM.2004.827493
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 819XS
UT WOS:000221350200013
DA 2024-07-18
ER

PT J
AU Hannuksela, MM
   Wang, YK
   Gabbouj, M
AF Hannuksela, MM
   Wang, YK
   Gabbouj, M
TI Isolated regions in video coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error resilience; isolated regions; random access; video coding
AB Different types of prediction are applied in modern video coding. While predictive coding improves compression efficiency, the propagation of transmission errors becomes more likely. In addition, predictive coding brings difficulties to other aspects of video coding, including random access, parallel processing, and scalability. In order to combat the negative effects, video coding schemes introduce mechanisms, such as slices and intracoding, to limit and break the prediction. This paper proposes the use of the isolated regions coding tool that jointly limits in-picture prediction and interprediction on region-of-interest basis. The tool can be used to provide random access points from non-intrapictures and to respond to intrapicture update requests. Furthermore, it can be applied as an error-robust macroblock mode decision method and can be used in combination with unequal error protection. Finally, it enables mixing of scenes, which is useful in coding of masked scene transitions.
C1 Nokia Res Ctr, Tampere 33721, Finland.
   Nokia Mobile Software, Tampere 33721, Finland.
   Tampere Univ Technol, FIN-33101 Tampere, Finland.
C3 Nokia Corporation; Siemens AG; Nokia Siemens Networks; Nokia Finland;
   Nokia Corporation; Nokia Finland; Tampere University
RP Nokia Res Ctr, Tampere 33721, Finland.
EM miska.hannuksela@nokia.com; ye-kui.wang@nokia.com; moncef.gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014
OI Gabbouj, Moncef/0000-0002-9788-2323
CR Brady N, 1999, IEEE T CIRC SYST VID, V9, P1170, DOI 10.1109/76.809154
   Hannuksela MM, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P537, DOI 10.1109/ICIP.2002.1039026
   HANNUKSELA MM, 2001, P INT PACK VID WORKS
   *ISO IEC, 1449622001 ISOIEC
   LIAO JY, 1996, P IEEE INT C IM PROC
   ROSENBERG J, 2733 IETF RFC
   STOCKHAMMER T, 2002, P 2002 TYRRH INT WOR
   STOCKHAMMER T, 2002, P IEEE INT C IM PROC
   SUHRING K, H 264 AVC REF SOFTWA
   Wang Y, 2000, IEEE SIGNAL PROC MAG, V17, P61, DOI 10.1109/79.855913
   WANG YK, JOINT VID TEAM DOC J
   WENGER S, ITU T VID COD EXP GR
   WENGER S, 1999, P INT PACK VID WORKS
NR 13
TC 54
Z9 84
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 259
EP 267
DI 10.1109/TMM.2003.822784
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400005
DA 2024-07-18
ER

PT J
AU Hossain, E
   Bhargava, VK
AF Hossain, E
   Bhargava, VK
TI Link-level traffic scheduling for providing predictive QoS in wireless
   multimedia networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE correlated fading; predictive QoS; radio link level traffic scheduling;
   wireless ATM
ID PACKET NETWORKS; TRANSPORT; SERVICE
AB A set of centralized burst-level cell scheduling schemes, namely, First Come First Served with Frame Reservation (FCFS-FR), FCFR-FR+, Earliest Deadline First with Frame Reservation (EDF-FR), EDF-FR+, and Multitraffic Dynamic Reservation (MTDR), are investigated for transmission of multiservice traffic over time division multiple access (TDMA)/time division duplex (TDD) channels in wireless ATM (WATM) networks. In these schemes, the number of time slots allocated to a virtual circuit (VC) during a frame-time is changed dynamically depending on the traffic type, system traffic load, the time of arrival (TOA)/time of expiry (TOE) value of the data burst and data burst length. The performances of these schemes ate evaluated by computer simulation for realistic voice, Video and data traffic models and their quality-of-service (QoS) requirements in a wireless mobile multimedia network. Both the error-free and the correlated fading channel conditions are considered. Simulation results show that the EDF-FR+ and MTDR schemes outperform the other schemes and can provide high channel utilization with predictive QoS guarantee in a multiservice traffic environment even in the presence of bursty channel errors. The EDF-FR+ scheme is found to provide better cell multiplexing performance than the MTDR scheme. Such a scheme would be easy to implement and would also result in a power conservative TDMA/TDD medium access control (MAC) protocol for broadband wireless access. Burst-level cell scheduling schemes such as EDF-FR+ can be easily adapted as MAC protocols in the emerging differentiated services (DS) enhanced wireless Internet protocol (IP) networks.
C1 Univ Manitoba, Dept Elect & Comp Engn, Winnipeg, MB R3T 5V6, Canada.
   Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
C3 University of Manitoba; University of British Columbia
RP Univ Manitoba, Dept Elect & Comp Engn, Winnipeg, MB R3T 5V6, Canada.
EM ekram@ee.umanitoba.ca; vijayb@ece.ubc.ca
CR ACAMPORA A, 1996, IEEE PERSONAL CO AUG, P8
   AGRAWAL P, 1996, IEEE PERS COMMUN APR, P18
   [Anonymous], 1998, An Architecture for Differentiated Services, RFC 2475 Informational
   BAO Y, 1998, THESIS U DELAWARE NE
   BHAGWAT P, P IEEE INFOCOM 96, V3, P1133
   Bharghavan V, 1999, IEEE PERS COMMUN, V6, P44, DOI 10.1109/98.752787
   Chen JC, 1999, IEEE T MULTIMEDIA, V1, P187, DOI 10.1109/6046.766739
   FRAGOULI C, P IEEE INFOCOM 98
   Guérin R, 1999, COMPUT NETW, V31, P169, DOI 10.1016/S0169-7552(98)00261-X
   Heyman DP, 1997, IEEE ACM T NETWORK, V5, P554, DOI 10.1109/90.649513
   HONCHARENKO W, 1997, IEEE COMMUN MAG, P20
   Jamin S, 1997, IEEE ACM T NETWORK, V5, P56, DOI 10.1109/90.554722
   KRUNZ M, P ACM SIGMETRICS 97, P192
   Kurose Jim., 1993, ACM SIGCOMM COMP COM, V23, P6
   NANDA S, 1991, IEEE T VEH TECHNOL, V40, P584, DOI 10.1109/25.97513
   PARK K, INT C NETW PROT
   PASSAS N, 1998, ACM MOBILE NETWORKS, V3, P275
   PEHA JM, P IEEE INFOCOM 91, P741
   Ravindran K, 1996, IEEE J SEL AREA COMM, V14, P1360, DOI 10.1109/49.536485
   Raychaudhuri D, 1996, WIREL NETW, V2, P163, DOI 10.1007/BF01201051
   RAYCHAUDHURI D, 1994, IEEE J SEL AREA COMM, V12, P1401, DOI 10.1109/49.329336
   ZORZI M, 1995, P IEEE ICUPC 95 NOV, P211
NR 22
TC 23
Z9 25
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 199
EP 217
DI 10.1109/TMM.2003.819745
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200015
DA 2024-07-18
ER

PT J
AU Aghbari, Z
   Kaneko, K
   Makinouchi, A
AF Aghbari, Z
   Kaneko, K
   Makinouchi, A
TI Content-trajectory approach for searching video databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content trajectory; indexing; modeling; querying video databases
AB In the past few years, modeling and querying video databases have been a subject of extensive research to develop tools for effective search of videos. In this paper, we present a hierarchal approach to model videos at three levels, object level (OL), frame level (FL), and shot level (SL). The model captures the visual features of individual objects at OL, visual-spatio-temporal (VST) relationships between objects at FL, and time-varying visual features and time-varying VST relationships at SL. We call the combination of the time-varying visual features and the time-varying VST relationships a Content trajectory which is used to represent and index a shot. A novel query interface that allows users to describe the time-varying contents of complex video shots such as those of skiers, soccer players, etc., by sketch and feature specification is presented. Our experimental results prove the effectiveness of modeling and querying shots using the content trajectory approach.
C1 Univ Sharjah, Dept Comp Sci, Sharjah, U Arab Emirates.
   Kyushu Univ, Dept Intelligent Syst, Grad Sch Informat Sci & Elect Engn, Fukuoka 8128581, Japan.
C3 University of Sharjah; Kyushu University
RP Aghbari, Z (corresponding author), Univ Sharjah, Dept Comp Sci, Sharjah, U Arab Emirates.
CR AGHBARI Z, 1999, IEEE INT C MULT COMP
   AGHBARI Z, 1998, 9 INT WORKSH DAT EXP
   AGHBARI Z, 1997, P INT S DIG MED INF, P140
   ALLEN JF, 1983, COMMUN ACM, V26, P11
   [Anonymous], ACM MULTIMEDIA 96
   CASCIA ML, 1996, ICASSP ATL GA MAY
   CHANG SF, 1997, ACM MULT C SEATTL WA
   DAGTAS S, 1999, ICMCS 99 FLOR IT JUN
   DAS M, 1998, ACM MULT C
   EGENHOFER MJ, 1991, INT J GEOGR INF SYST, V5, P161, DOI 10.1080/02693799108927841
   EGENHOFER MJ, 1992, THEORIES METHODS SPA, V693, P196
   FERMAN AM, 1998, IEEE INT C IM PROC C
   FISCHER S, 1995, ACM MULT C
   Flickner M., 1995, Query by image and video content: the QBIC system
   FREKSA C, 1990, ARTIF INTELL, V54, P416
   Haering N, 2000, IEEE T CIRC SYST VID, V10, P857, DOI 10.1109/76.867923
   IYENGAR G, 1998, P SPIE MULTIMEDIA ST, V3312, P216
   KANEKO K, 1998, IEICE T COMPUT J, V81
   KAWASHIMA T, 1998, ICIP
   Kobla V, 2000, PROC SPIE, V3972, P332
   KOBLA V, 1997, ACM MULT C
   Li JZ, 1996, INTERNATIONAL WORKSHOP ON MULTI-MEDIA DATABASE MANAGEMENT SYSTEMS, PROCEEDINGS, P124, DOI 10.1109/MMDBMS.1996.541863
   Li JZ, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P336, DOI 10.1109/MMCS.1997.609625
   *MPEG REQ GROUP, 1999, MPEG7 CONT OBJ TECHN
   *MPEG4 SUBGR, 1999, REQ AUD DEL SYNC SYS
   Nabil M, 1996, IEEE T KNOWL DATA EN, V8, P533, DOI 10.1109/69.536246
   Nabil M., 1997, 5 INT C DAT SYST ADV
   Oh J. H., 2000, IEEE INT C MULT EXP
   SAUR D, 1997, P SPIE STOR RETR IM, P176
   Sundaram H., 2000, IEEE INT C MULT EXP
   Vasconcelos N., 1998, INT C COMP VIS PATT
NR 31
TC 17
Z9 20
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2003
VL 5
IS 4
BP 516
EP 531
DI 10.1109/TMM.2003.819092
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 742VA
UT WOS:000186537700003
DA 2024-07-18
ER

PT J
AU Lee, HY
   Lee, HK
   Ha, YH
AF Lee, HY
   Lee, HK
   Ha, YH
TI Spatial color descriptor for image retrieval and video segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE color histogram; color vector angle; graph representation; image
   retrieval; scene-cut detection
AB An important problem in color-based image retrieval and video segmentation is to lack information about how color is spatially distributed. To solve this problem and enhance the performance of image and video analyses, a spatial color descriptor is proposed involving a color adjacency histogram and color vector angle histogram. The color adjacency histogram represents the spatial distribution of color pairs at color edges in an image, thereby incorporating spatial information into the proposed color descriptor. Meanwhile, the color vector angle histogram represents the global color distribution of smooth pixels in an image. Since the proposed color descriptor includes spatial adjacency information between colors, it can robustly reduce the effect of a significant change in appearance and shape in image and video analyses. Moreover, since the color adjacency histogram is simply represented by binary streams, the storage space required for the image histogram values can be effectively reduced. Experimental results show that even with significant appearance changes, the proposed color descriptor could produce a high image retrieval rate and accurately detect abrupt scene-cuts in a video analysis.
C1 Kyungpook Natl Univ, Sch Elect Engn & Comp Sci, Taegu 702701, South Korea.
C3 Kyungpook National University
RP Kyungpook Natl Univ, Sch Elect Engn & Comp Sci, Taegu 702701, South Korea.
EM hylee@m80.knu.ac.kr; hoguni@m80.knu.ac.kr; yha@ee.knu.ac.kr
CR ALATTAR AM, 1993, IEEE INT S CIRC SYST, V1, P13
   ANDROUTSOS D, 1999, THESIS U TORONTO TOR
   Dony R. D., 1999, P IEEE CAN C EL COMP, P687
   Faloutsos C., 1994, Journal of Intelligent Information Systems: Integrating Artificial Intelligence and Database Technologies, V3, P231, DOI 10.1007/BF00962238
   GUPTA A, 1996, VISUAL INFORMATION R
   Huang J, 1997, PROC CVPR IEEE, P762, DOI 10.1109/CVPR.1997.609412
   Huang J, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P325, DOI 10.1145/266180.266383
   LAMMENS JMG, 1999, THESIS STATE U NEW Y
   Lupatini G, 1998, EIGHTH INTERNATIONAL WORKSHOP ON RESEARCH ISSUES IN DATA ENGINEERING - CONTINUOUS-MEDIA DATABASES AND APPLICATIONS, PROCEEDINGS, P34, DOI 10.1109/RIDE.1998.658276
   Ma WY, 1998, CONF REC ASILOMAR C, P253, DOI 10.1109/ACSSC.1998.750865
   OGLE VE, 1995, COMPUTER, V28, P40, DOI 10.1109/2.410150
   Park IK, 1999, IMAGE VISION COMPUT, V17, P465, DOI 10.1016/S0262-8856(98)00139-5
   Pass G., 1996, Proceeding. Third IEEE Workshop on Applications of Computer Vision. WACV'96 (Cat. No.96TB100084), P96, DOI 10.1109/ACV.1996.572008
   Pitas I., 2000, DIGITAL IMAGE PROCES
   Smith J. R., 1996, ACM MULT C
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Yu H, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P498, DOI 10.1109/ICIP.1997.638817
   1999, JTC1SC29WG11 ISOIEC
NR 18
TC 60
Z9 79
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2003
VL 5
IS 3
BP 358
EP 367
DI 10.1109/TMM.2003.814792
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 714BM
UT WOS:000184892500008
DA 2024-07-18
ER

PT J
AU Chu, WC
AF Chu, WC
TI DCT-based image watermarking using subsampling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE discrete cosine transform; image watermarking; subsampling
AB A DCT-based image watermarking algorithm is described, where the original image is not required for watermark recovery, and is achieved by inserting the watermark in subimages obtained through subsampling.
C1 DoCoMo USA Labs, San Jose, CA 95110 USA.
C3 NTT Docomo
RP Chu, WC (corresponding author), DoCoMo USA Labs, San Jose, CA 95110 USA.
CR Chu WC, 1999, ELECTRON LETT, V35, P2099, DOI 10.1049/el:19991449
   COX IJ, 1996, P INF HID 1 INT WORK, P185
   DUGAD R, 1998, IEEE ICIP
   FRIDRICH J, 1998, 2 INF HID WORKSH POR
   LANGELAAR GC, 2000, THESIS DELFT U TECHN
   Nelson M., 1996, The data compression book
   PIVA A, 1997, IEEE ICIP
NR 7
TC 201
Z9 225
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017-2394 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 34
EP 38
DI 10.1109/TMM.2003.808816
PG 5
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200004
DA 2024-07-18
ER

PT J
AU Bu, YQ
   Li, LW
   Xie, JY
   Liu, Q
   Cai, Y
   Huang, QB
   Li, Q
AF Bu, Yuqi
   Li, Liuwu
   Xie, Jiayuan
   Liu, Qiong
   Cai, Yi
   Huang, Qingbao
   Li, Qing
TI Scene-Text Oriented Referring Expression Comprehension
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Semantics; Text recognition; Pain; Natural
   languages; Image recognition; Referring expression comprehension; scene
   text representation; multimodal alignment
ID LOCALIZATION
AB Referring expression comprehension (REC) aims to identify and locate a specific object in visual scenes referred to by a natural language expression. Existing studies of REC only focus on basic visual attributes and neglect scene text. Since scene text has the functions of object identification and disambiguation, it is naturally and frequently used to refer to objects. However, existing methods do not explicitly recognize text in images and fail to align scene text mentioned in expressions with the text shown in images, resulting in object localization errors. This article takes the first step toward addressing these limitations. First, we introduce a new task called scene-text oriented referring expression comprehension, which aims to align visual cues and textual semantics of scene text with referring expressions and visual contents. Second, we propose a scene text awareness network that can bridge the gap between texts from two modalities by grounding visual representations of expression-correlated scene texts. Specifically, we propose a correlated text extraction module to solve the problem of lacking semantic understanding, and a correlated region activation module to address the fixed alignment problem and absent alignment problem. These modules ensure that the proposed method focuses on local regions that are most relevant to scene text, thus mitigating the misalignment of scene text with irrelevant regions. Third, to conduct quantitative evaluations, we establish a new benchmark dataset called RefText. Experimental results demonstrate that the proposed method can effectively comprehend scene-text oriented referring expressions and achieves excellent performance.
C1 [Bu, Yuqi; Li, Liuwu; Xie, Jiayuan; Liu, Qiong; Cai, Yi] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
   [Bu, Yuqi; Li, Liuwu; Xie, Jiayuan; Liu, Qiong; Cai, Yi] Key Lab Big Data & Intelligent Robot SCUT, MOE China, Guangzhou 510006, Peoples R China.
   [Huang, Qingbao] Guangxi Univ, Inst Artificial Intelligence, Sch Elect Engn, Nanning 530004, Guangxi, Peoples R China.
   [Huang, Qingbao] Guangxi Key Lab Intelligent Control & Maintenance, Nanning 530004, Peoples R China.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong 999077, Peoples R China.
C3 South China University of Technology; Guangxi University; Hong Kong
   Polytechnic University
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
EM seyqbu@mail.scut.edu.cn; liuwu.li@outlook.com;
   sexiejiayuan@mail.scut.edu.cn; liuqiong@scut.edu.cn; ycai@scut.edu.cn;
   qbhuang@gxu.edu.cn; csqli@comp.polyu.edu.hk
RI Li, Qing/JMH-1365-2023
OI Li, Qing/0000-0003-3370-471X; Huang, Qingbao/0000-0001-7691-347X; Bu,
   Yuqi/0000-0002-2158-8750
FU National Natural Science Foundation of China [62076100, 61976094,
   62276072]; Guangxi Natural Science Foundation [2022GXNS-FAA035627];
   Fundamental Research Funds for the Central Universities, SCUT [D2210010,
   D2200150, D2201300]; Science and Technology Planning Project of
   Guangdong Province [2020B0101100002]; CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076100, 61976094, and 62276072,in
   part by Guangxi Natural Science Foundation under Grant
   2022GXNS-FAA035627, in part by the Fundamental Research Funds for the
   Central Universities, SCUT under Grants D2210010, D2200150, and
   D2201300, in part by the Science and Technology Planning Project of
   Guangdong Province under Grant 2020B0101100002, and in part by
   CAAI-Huawei MindSpore Open Fund.
CR Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen L, 2021, AAAI CONF ARTIF INTE, V35, P1036
   Cirik Volkan, 2018, P 2018 C N AM CHAPT, V2, P781, DOI DOI 10.18653/V1/N18-2123
   CLAY MM, 1969, BRIT J EDUC PSYCHOL, V39, P47, DOI 10.1111/j.2044-8279.1969.tb02040.x
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Du YN, 2020, Arxiv, DOI arXiv:2009.09941
   Gao C, 2021, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR46437.2021.00308
   Gao Difei, 2020, CVPR, P12746
   Goldman E, 2019, PROC CVPR IEEE, P5222, DOI 10.1109/CVPR.2019.00537
   Hasan I, 2021, PROC CVPR IEEE, P11323, DOI 10.1109/CVPR46437.2021.01117
   Hatori J, 2018, IEEE INT CONF ROBOT, P3774
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Nguyen HV, 2011, LECT NOTES COMPUT SC, V6493, P709
   Hsiao E, 2012, PROC CVPR IEEE, P3146, DOI 10.1109/CVPR.2012.6248048
   Hsiao E, 2010, PROC CVPR IEEE, P2653, DOI 10.1109/CVPR.2010.5539981
   Hu R., 2020, COMPUTER VISION ECCV, P742
   Huang BB, 2021, PROC CVPR IEEE, P16883, DOI 10.1109/CVPR46437.2021.01661
   Ishigaki T., 2021, P 14 INT C NATURAL L, P103
   Jin ZX, 2023, IEEE T MULTIMEDIA, V25, P1, DOI 10.1109/TMM.2021.3120194
   Kamath A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1760, DOI 10.1109/ICCV48922.2021.00180
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karaoglu S, 2012, LECT NOTES COMPUT SC, V7585, P456, DOI 10.1007/978-3-642-33885-4_46
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuznetsov Andrey, 2020, Computer Vision and Graphics. International Conference, ICCVG 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12334), P87, DOI 10.1007/978-3-030-59006-2_8
   Li LW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5167, DOI 10.1145/3474085.3475629
   Li XY, 2018, IEEE T MULTIMEDIA, V20, P2749, DOI 10.1109/TMM.2018.2811621
   Li YJ, 2007, IEEE T PATTERN ANAL, V29, P1091, DOI 10.1109/TPAMI.2007.1070
   Mafla A, 2021, IEEE WINT CONF APPL, P2219, DOI 10.1109/WACV48630.2021.00227
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Qiao YY, 2021, IEEE T MULTIMEDIA, V23, P4426, DOI 10.1109/TMM.2020.3042066
   RAYNER K, 1995, STUD VIS INFORM PROC, V6, P3
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Rong XJ, 2022, IEEE T PATTERN ANAL, V44, P1638, DOI 10.1109/TPAMI.2020.3018491
   Rong XJ, 2020, IEEE T IMAGE PROCESS, V29, P591, DOI 10.1109/TIP.2019.2930176
   Rong XJ, 2017, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2017.349
   Shridhar M, 2020, INT J ROBOT RES, V39, P217, DOI 10.1177/0278364919897133
   Singh A, 2019, PROC CVPR IEEE, P8309, DOI 10.1109/CVPR.2019.00851
   Takeshita K, 2021, INT C PATT RECOG, P6227, DOI 10.1109/ICPR48806.2021.9412077
   Veit A, 2016, Arxiv, DOI arXiv:1601.07140
   Wang H, 2021, PROC CVPR IEEE, P4556, DOI 10.1109/CVPR46437.2021.00453
   Wang J, 2021, PROC CVPR IEEE, P1306, DOI 10.1109/CVPR46437.2021.00136
   Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43
   Xu GH, 2021, PROC CVPR IEEE, P12632, DOI 10.1109/CVPR46437.2021.01245
   Yang SB, 2021, IEEE T PATTERN ANAL, V43, P2765, DOI 10.1109/TPAMI.2020.2973983
   Yang ZY, 2021, PROC CVPR IEEE, P8747, DOI 10.1109/CVPR46437.2021.00864
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Yuankai Qi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9979, DOI 10.1109/CVPR42600.2020.01000
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Zamir AR, 2014, IEEE T PATTERN ANAL, V36, P1546, DOI 10.1109/TPAMI.2014.2299799
   Zhengyuan Yang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P387, DOI 10.1007/978-3-030-58568-6_23
NR 55
TC 0
Z9 0
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7208
EP 7221
DI 10.1109/TMM.2022.3219642
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000036
DA 2024-07-18
ER

PT J
AU Cheng, QR
   Wen, KY
   Gu, XD
AF Cheng, Qingrong
   Wen, Keyu
   Gu, Xiaodong
TI Vision-Language Matching for Text-to-Image Synthesis via Generative
   Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Image synthesis; Visualization; Task analysis; Measurement;
   Generative adversarial networks; Image quality; text-to-image synthesis;
   vision-language matching
ID ATTENTION; GAN
AB Text-to-image synthesis is an attractive but challenging task that aims to generate a photo-realistic and semantic consistent image from a specific text description. The images synthesized by off-the-shelf models usually contain limited components compared with the corresponding image and text description, which decreases the image quality and the textual-visual consistency. To address this issue, we propose a novel Vision-Language Matching strategy for text-to-image synthesis, named VLMGAN*, which introduces a dual vision-language matching mechanism to strengthen the image quality and semantic consistency. The dual vision-language matching mechanism considers textual-visual matching between the generated image and the corresponding text description, and visual-visual consistent constraints between the synthesized image and the real image. Given a specific text description, VLMGAN* firstly encodes it into textual features and then feeds them to a dual vision-language matching-based generative model to synthesize a photo-realistic and textual semantic consistent image. Besides, the popular evaluation metrics for text-to-image synthesis are borrowed from simple image generation, which mainly evaluate the reality and diversity of the synthesized images. Therefore, we introduce a metric named Vision-Language Matching Score (VLMS) to evaluate the performance of text-to-image synthesis which can consider both the image quality and the semantic consistency between the synthesized image and the description. The proposed dual multi-level vision-language matching strategy can be applied to other text-to-image synthesis methods. We implement this strategy on two popular baselines, which are marked with VLMGAN(+AttnGAN )and VLMGAN(+DFGAN). The experimental results on two widely-used datasets show that the model achieves significant improvements over other state-of-the-art methods.
C1 [Cheng, Qingrong; Wen, Keyu; Gu, Xiaodong] Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Gu, XD (corresponding author), Fudan Univ, Dept Elect Engn, Shanghai 200433, Peoples R China.
EM qrcheng17@fudan.edu.cn; kywen19@fudan.edu.cn; xdgu@fudan.edu.cn
OI Wen, Keyu/0000-0002-5048-9014; Cheng, Qingrong/0000-0001-6631-1504
FU National Natural Science Foundation of China [62176062]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62176062.
CR Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374
   Chen C, 2022, PROC CVPR IEEE, P18082, DOI 10.1109/CVPR52688.2022.01757
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Cheng Q., 2022, IEEE Trans. Circ. Syst. Video Technol., DOI [10.1109/TCSVT.2022.3182549, DOI 10.1109/TCSVT.2022.3182549]
   Cheng QR, 2021, NEURAL NETWORKS, V134, P143, DOI 10.1016/j.neunet.2020.11.011
   Cheng QR, 2020, DIGIT SIGNAL PROCESS, V107, DOI 10.1016/j.dsp.2020.102866
   Cheng QR, 2019, LECT NOTES COMPUT SC, V11731, P483, DOI 10.1007/978-3-030-30493-5_47
   Deng C, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3301274
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong YL, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107573
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fan XX, 2018, SIGNAL PROCESS, V146, P50, DOI 10.1016/j.sigpro.2017.12.017
   Gao LL, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107384
   Gao LL, 2019, AAAI CONF ARTIF INTE, P8312
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Gulrajani I, 2016, Arxiv, DOI arXiv:1611.05013
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Hensel M, 2017, ADV NEUR IN, V30
   Hinz T, 2022, IEEE T PATTERN ANAL, V44, P1552, DOI 10.1109/TPAMI.2020.3021209
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jiadong Liang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P491, DOI 10.1007/978-3-030-58548-8_29
   Joseph KJ, 2019, IEEE WINT CONF APPL, P358, DOI 10.1109/WACV.2019.00044
   Juang BH, 1997, IEEE T SPEECH AUDI P, V5, P257, DOI 10.1109/89.568732
   Jun Cheng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10908, DOI 10.1109/CVPR42600.2020.01092
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Kipf TN, 2016, ARXIV
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li B, 2021, J NEUROL, V268, P2042, DOI 10.1007/s00415-019-09596-3
   Li C, 2016, PROC CVPR IEEE, P2479, DOI 10.1109/CVPR.2016.272
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Li ZX, 2020, IEEE ACCESS, V8, P21847, DOI 10.1109/ACCESS.2020.2969808
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mansimov E., 2016, P 4 INT C LEARN REPR
   Peng DL, 2021, NEURAL NETWORKS, V138, P57, DOI 10.1016/j.neunet.2021.01.023
   Qi JW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P892
   Qiao TT, 2019, ADV NEUR IN, V32
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Reed S. E., 2016, ADV NEURAL INFORM PR, V29, P217
   Reed S, 2017, PR MACH LEARN RES, V70
   Reed S, 2016, PR MACH LEARN RES, V48
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Salimans T, 2016, ADV NEUR IN, V29
   Sauer A., 2021, Advances in Neural Information Processing Systems, V34, P17480
   Simonyan K, 2018, P 7 INT C LEARN REPR
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan HC, 2021, IEEE T IMAGE PROCESS, V30, P1275, DOI 10.1109/TIP.2020.3026728
   Tan Z., 2022, PROC IEEE INT C MULT, P1
   Tao M, 2022, PROC CVPR IEEE, P16494, DOI 10.1109/CVPR52688.2022.01602
   Vaswani A, 2017, ADV NEUR IN, V30
   Velikovi P., 2018, P INT C LEARN REPR
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12152
   Wang M, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3391709
   Welinder P., 2010, California Inst. Technol., Tech. Rep. CNS-TR-2010-001
   Wen KY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2188, DOI 10.1109/ICCV48922.2021.00221
   Wen KY, 2021, IEEE T CIRC SYST VID, V31, P2866, DOI 10.1109/TCSVT.2020.3030656
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang YH, 2021, IEEE T IMAGE PROCESS, V30, P2798, DOI 10.1109/TIP.2021.3055062
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yin GJ, 2019, PROC CVPR IEEE, P2322, DOI 10.1109/CVPR.2019.00243
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Yuan MK, 2020, IEEE T CIRC SYST VID, V30, P4258, DOI 10.1109/TCSVT.2019.2953753
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhu B, 2020, PROC CVPR IEEE, P5518, DOI 10.1109/CVPR42600.2020.00556
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 74
TC 1
Z9 1
U1 8
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7062
EP 7075
DI 10.1109/TMM.2022.3217384
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hao, FD
   Li, JJ
   Song, R
   Li, YS
   Cao, KL
AF Hao, Fengda
   Li, Jiaojiao
   Song, Rui
   Li, Yunsong
   Cao, Kailang
TI Structure-Aware Graph Convolution Network for Point Cloud Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Point cloud compression; Convolution; Task analysis; Kernel;
   Three-dimensional displays; Feature extraction; Decoding; Point cloud
   parsing; adaptive neighbor selection; shape message passing;
   feature-wise transformation
AB Point clouds are becoming a popular medium to describe 3D scenes, benefitting from their accuracy and completeness in expressing the spatial and geometrical information of objects. However, due to the disorder and uneven distribution nature, merely selecting neighbors for point clouds in Euclidean space is inefficient and position-ignoring. To fill this gap, we propose a structure-aware graph convolution network (SA-GCN), which consists of an adaptive dilated KNN module (ADKNN), a learnable graph filter (LGF), and a structure-aware feature transformation module (SFT). Specially, the ADKNN module can dynamically adjust the range of grouping neighbor points, while being universal to improve the performance of arbitrary KNN-based methods. Moreover, with the localized auxiliary information provided by LGF, our SFT module disentangles the spatial details as a sort of coding guidance for better deep feature representations. Extensive experimental results on point cloud classification and segmentation tasks demonstrate the superiority of our proposed network.
C1 [Hao, Fengda; Li, Jiaojiao; Song, Rui; Li, Yunsong; Cao, Kailang] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Xidian University
RP Li, JJ; Song, R (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM fdhao@stu.xidian.edu.cn; jjli@xidian.edu.cn; rsong@xidian.edu.cn;
   ysli@mail.xidian.edu.cn; caokl_xidian@stu.xidian.edu.cn
FU National Key Research and Development Program of China [2018AAA0102702];
   Fundamental Research Funds for the Central Universities [JBF22010];
   National Nature Science Foundation of China [61901343]; state Key
   Laboratory of Geo-Information Engineering [SKLGIE2020-M-3-1]; Science
   and Technology on Space Intelligent Control Laboratory [ZDSYS-2019-03];
   China Postdoctoral Science Foundation [2017M623124]; China Postdoctoral
   Science Special Foundation [2018T111019]; Youth Innovation Team of
   Shaanxi Universities; Open Research Fund of CAS Key Laboratory of
   Spectral Imaging Technology [LSIT201924W];
   Government-Business-University-Research Cooperation Foundation
   [XWYCXY-012021002-HT]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102702, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   JBF22010, in part by the National Nature Science Foundation of China
   under Grant 61901343, in part by the state Key Laboratory of
   Geo-Information Engineering under Grant SKLGIE2020-M-3-1, in part by the
   Science and Technology on Space Intelligent Control Laboratory under
   Grant ZDSYS-2019-03, in part by the China Postdoctoral Science
   Foundation under Grant 2017M623124, in part by China Postdoctoral
   Science Special Foundation under Grant 2018T111019, in part by the Youth
   Innovation Team of Shaanxi Universities, in part by the Open Research
   Fund of CAS Key Laboratory of Spectral Imaging Technology under Grant
   LSIT201924W, in part by the Government-Business-University-Research
   Cooperation Foundation between Wuhu and Xidian under Grant
   XWYCXY-012021002-HT
CR Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Arvanitis G, 2022, IEEE T MULTIMEDIA, V24, P2230, DOI 10.1109/TMM.2021.3089838
   Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Cheng SL, 2021, IEEE T IMAGE PROCESS, V30, P4436, DOI 10.1109/TIP.2021.3072214
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   Defferrard M, 2016, ADV NEUR IN, V29
   El Sayed AR, 2018, IMAGING SCI J, V66, P23, DOI 10.1080/13682199.2017.1376772
   Fujihashi T, 2022, IEEE T MULTIMEDIA, V24, P2179, DOI 10.1109/TMM.2021.3077772
   Guo MH, 2021, COMPUT VIS MEDIA, V7, P187, DOI 10.1007/s41095-021-0229-5
   Guo YL, 2021, IEEE T PATTERN ANAL, V43, P4338, DOI 10.1109/TPAMI.2020.3005434
   Han L, 2020, IEEE T VIS COMPUT GR, V26, P2012, DOI 10.1109/TVCG.2020.2973477
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hegde S, 2021, COMPUT GRAPH-UK, V95, P13, DOI 10.1016/j.cag.2021.01.004
   Kim S., 2021, PROC BRIT MACH VIS C
   Kipf TN, 2017, INT C LEARN REPR
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Lawin FJ, 2017, LECT NOTES COMPUT SC, V10424, P95, DOI 10.1007/978-3-319-64689-3_8
   Lei H, 2021, IEEE T PATTERN ANAL, V43, P3664, DOI 10.1109/TPAMI.2020.2983410
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979
   Li LX, 2019, PROC CVPR IEEE, P2647, DOI 10.1109/CVPR.2019.00276
   Li Y, 2021, IEEE T NEUR NET LEAR, V32, P3412, DOI 10.1109/TNNLS.2020.3015992
   Li YZ, 2018, ADV NEUR IN, V31
   Liu JY, 2018, IEEE T MULTIMEDIA, V20, P3252, DOI 10.1109/TMM.2018.2831636
   Liu JX, 2019, IEEE I CONF COMP VIS, P7545, DOI 10.1109/ICCV.2019.00764
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu KC, 2021, Arxiv, DOI arXiv:2012.09439
   Liu M, 2016, IEEE T CYBERNETICS, V46, P1217, DOI 10.1109/TCYB.2015.2430526
   Liu XH, 2019, AAAI CONF ARTIF INTE, P8778
   Liu YC, 2019, PROC CVPR IEEE, P8887, DOI 10.1109/CVPR.2019.00910
   Ma X., 2022, P INT C LEARN REPR
   Mohammadi SS, 2021, IEEE IMAGE PROC, P3103, DOI 10.1109/ICIP42928.2021.9506426
   Nt H, 2019, Arxiv, DOI [arXiv:1905.09550, DOI 10.48550/ARXIV.1905.09550]
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16
   Qingyong Hu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11105, DOI 10.1109/CVPR42600.2020.01112
   Qiu S, 2021, PROC CVPR IEEE, P1757, DOI 10.1109/CVPR46437.2021.00180
   Qiu S, 2022, IEEE T MULTIMEDIA, V24, P1943, DOI 10.1109/TMM.2021.3074240
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Saporta T, 2022, COMPUT GRAPH-UK, V102, P289, DOI 10.1016/j.cag.2021.10.020
   Savva M., 2016, Proceedings of the eurographics workshop on 3D object retrieval, P89
   Sharma Gopal, 2020, COMPUTER VISION ECCV, P261, DOI DOI 10.1007/978-3-030-58571-6_16
   Tchapmi LP, 2017, INT CONF 3D VISION, P537, DOI 10.1109/3DV.2017.00067
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651
   Valsesia D, 2021, IEEE T MULTIMEDIA, V23, P402, DOI 10.1109/TMM.2020.2976627
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xiang TG, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P895, DOI 10.1109/ICCV48922.2021.00095
   Xu MT, 2021, PROC CVPR IEEE, P3172, DOI 10.1109/CVPR46437.2021.00319
   Xu MT, 2021, AAAI CONF ARTIF INTE, V35, P3056
   Yang KZ, 2021, ACM T GRAPHIC, V40, DOI 10.1145/3450626.3459873
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Ze Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P326, DOI 10.1007/978-3-030-58592-1_20
   Zhang HD, 2020, IEEE T MULTIMEDIA, V22, P2012, DOI 10.1109/TMM.2019.2951461
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhao HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P16239, DOI 10.1109/ICCV48922.2021.01595
   Zhao HS, 2019, PROC CVPR IEEE, P5550, DOI 10.1109/CVPR.2019.00571
   Zheng TH, 2019, IEEE I CONF COMP VIS, P1598, DOI 10.1109/ICCV.2019.00168
   Zhou H., 2021, P IEEE COMP SOC C CO
NR 61
TC 2
Z9 2
U1 4
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7025
EP 7036
DI 10.1109/TMM.2022.3216951
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000023
DA 2024-07-18
ER

PT J
AU Jia, MX
   Cheng, XH
   Lu, SJ
   Zhang, J
AF Jia, Mengxi
   Cheng, Xinhua
   Lu, Shijian
   Zhang, Jian
TI Learning Disentangled Representation Implicitly Via Transformer for
   Occluded Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Feature extraction; Semantics; Decoding; Task analysis;
   Representation learning; Interference; Occlusion scene; person
   Re-identification; representation learning; visual Transformer
AB Person re-IDentification (re-ID) under various occlusions has been a long-standing challenge as person images with different types of occlusions often suffer from misalignment in image matching and ranking. Most existing methods tackle this challenge by aligning spatial features of body parts according to external semantic cues or feature similarities but this alignment approach is complicated and sensitive to noises. We design DRL-Net, a disentangled representation learning network that handles occluded re-ID without requiring strict person image alignment or any additional supervision. Leveraging transformer architectures, DRL-Net achieves alignment-free re-ID via global reasoning of local features of occluded person images. It measures image similarity by automatically disentangling the representation of undefined semantic components, e.g., human body parts or obstacles, under the guidance of semantic preference object queries in the transformer. In addition, we design a decorrelation constraint in the transformer decoder and impose it over object queries for better focus on different semantic components. To better eliminate interference from occlusions, we design a contrast feature learning technique (CFL) for better separation of occlusion features and discriminative ID features. Extensive experiments over occluded and holistic re-ID benchmarks show that the DRL-Net achieves superior re-ID performance consistently and outperforms the state-offi-the-art by large margins for occluded re-ID dataset.
C1 [Jia, Mengxi; Cheng, Xinhua; Zhang, Jian] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Zhang, Jian] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Lu, Shijian] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Peking University; Peng Cheng Laboratory; Nanyang Technological
   University
RP Zhang, J (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.; Zhang, J (corresponding author), Peng Cheng Lab, Shenzhen 518066, Peoples R China.
EM mxjia@pku.edu.cn; chengxinhua@stu.pku.edu.cn; shijian.lu@ntu.edu.sg;
   zhangjian.sz@pku.edu.cn
RI Lu, Shijian/AAU-4831-2021
OI Lu, Shijian/0000-0002-6766-2506; Zhang, Jian/0000-0001-5486-3125
FU Shenzhen Fundamental Research Program
   [GXWD20201231165807007-20200807164903001]
FX This work was supported in part by Shenzhen Fundamental Research Program
   under Grant GXWD20201231165807007-20200807164903001
CR Cao M, 2021, IEEE T MULTIMEDIA, V23, P1239, DOI 10.1109/TMM.2020.2994524
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Chen GY, 2019, IEEE I CONF COMP VIS, P9546, DOI 10.1109/ICCV.2019.00964
   Chen HT, 2021, PROC CVPR IEEE, P12294, DOI 10.1109/CVPR46437.2021.01212
   Chen T, 2020, PR MACH LEARN RES, V119
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dosovitskiy A, 2020, PROC INT C LEARN REP
   Ge Y., 2018, ADV NEUR IN, P1222
   Gehring J, 2017, PR MACH LEARN RES, V70
   Ghazvininejad M, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P6112
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Gong S, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4
   Gu Jiatao, 2018, ICLR
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   He LX, 2018, Arxiv, DOI arXiv:1810.07399
   He LX, 2019, IEEE I CONF COMP VIS, P8449, DOI 10.1109/ICCV.2019.00854
   He LX, 2018, PROC CVPR IEEE, P7073, DOI 10.1109/CVPR.2018.00739
   He ST, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14993, DOI 10.1109/ICCV48922.2021.01474
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Huang HJ, 2018, PROC CVPR IEEE, P5098, DOI 10.1109/CVPR.2018.00535
   Huang L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3136, DOI 10.1145/3394171.3413775
   Huang YR, 2020, AAAI CONF ARTIF INTE, V34, P11069
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuan Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P346, DOI 10.1007/978-3-030-58580-8_21
   Li W, 2018, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2018.00243
   Li YL, 2021, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR46437.2021.00292
   Lin Huang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P17, DOI 10.1007/978-3-030-58595-2_2
   Liu FY, 2019, IEEE I CONF COMP VIS, P6638, DOI 10.1109/ICCV.2019.00674
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Luo CC, 2019, IEEE I CONF COMP VIS, P4975, DOI 10.1109/ICCV.2019.00508
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2905, DOI 10.1109/TMM.2020.2965491
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Miao JX, 2019, IEEE I CONF COMP VIS, P542, DOI 10.1109/ICCV.2019.00063
   Sarfraz MS, 2018, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2018.00051
   Shang Gao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11741, DOI 10.1109/CVPR42600.2020.01176
   Song CF, 2018, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2018.00129
   Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25
   Sun H, 2019, IEEE I CONF COMP VIS, P6736, DOI 10.1109/ICCV.2019.00684
   Sun YF, 2020, PROC CVPR IEEE, P6397, DOI 10.1109/CVPR42600.2020.00643
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   van den Oord A, 2018, PR MACH LEARN RES, V80, DOI arXiv:1711.10433
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wang YM, 2014, IEEE T CIRC SYST VID, V24, P1350, DOI 10.1109/TCSVT.2014.2305519
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wei LH, 2019, IEEE T MULTIMEDIA, V21, P986, DOI 10.1109/TMM.2018.2870522
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xie EZ, 2021, Arxiv, DOI arXiv:2101.08461
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang X, 2018, Arxiv, DOI arXiv:1711.08184
   Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349
   Zheng L, 2016, Arxiv, DOI arXiv:1610.02984
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng SX, 2021, PROC CVPR IEEE, P6877, DOI 10.1109/CVPR46437.2021.00681
   Zheng WS, 2015, IEEE I CONF COMP VIS, P4678, DOI 10.1109/ICCV.2015.531
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou SP, 2018, IEEE T MULTIMEDIA, V20, P593, DOI 10.1109/TMM.2017.2755983
   Zhuo JX, 2018, IEEE INT CON MULTI
NR 68
TC 67
Z9 68
U1 19
U2 58
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1294
EP 1305
DI 10.1109/TMM.2022.3141267
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100021
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Li, SK
   Liu, TL
   Tan, JY
   Zeng, D
   Ge, SM
AF Li, Shikun
   Liu, Tongliang
   Tan, Jiyong
   Zeng, Dan
   Ge, Shiming
TI Trustable Co-Label Learning From Multiple Noisy Annotators
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise measurement; Data models; Training; Labeling; Supervised learning;
   Robustness; Deep learning; Label noise; multiple annotators;
   crowdsoucing; learning from crowds
ID DISCRIMINATION; CROWDS; SET
AB Supervised deep learning depends on massive accurately annotated examples, which is usually impractical in many real-world scenarios. A typical alternative is learning from multiple noisy annotators. Numerous earlier works assume that all labels are noisy, while it is usually the case that a few trusted samples with clean labels are available. This raises the following important question: how can we effectively use a small amount of trusted data to facilitate robust classifier learning from multiple annotators? This paper proposes a data-efficient approach, called Trustable Co-label Learning (TCL), to learn deep classifiers from multiple noisy annotators when a small set of trusted data is available. This approach follows the coupled-view learning manner, which jointly learns the data classifier and the label aggregator. It effectively uses trusted data as a guide to generate trustable soft labels (termed co-labels). A co-label learning can then be performed by alternately reannotating the pseudo labels and refining the classifiers. In addition, we further improve TCL for a special complete data case, where each instance is labeled by all annotators and the label aggregator is represented by multilayer neural networks to enhance model capacity. Extensive experiments on synthetic and real datasets clearly demonstrate the effectiveness and robustness of the proposed approach. Source code is available at https://github.com/ShikunLi/TCL.
C1 [Li, Shikun; Ge, Shiming] Chinese Acad Sci, Inst Informat Engn, Beijing 100095, Peoples R China.
   [Li, Shikun; Ge, Shiming] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Liu, Tongliang] Univ Sydney, Trustworthy Machine Learning Lab, Darlington, NSW 2008, Australia.
   [Tan, Jiyong] AISONO AIR Lab, Shenzhen 518049, Peoples R China.
   [Tan, Jiyong] Harbin Inst Technol, Harbin 150001, Peoples R China.
   [Zeng, Dan] Shanghai Univ, Dept Commun Engn, Shanghai 150001, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of Sydney; Harbin Institute of Technology; Shanghai
   University
RP Ge, SM (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100095, Peoples R China.; Ge, SM (corresponding author), Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
EM lishikun@iie.ac.cn; tongliang.liu@sydney.edu.au; scutjy2015@163.com;
   dzeng@shu.edu.cn; geshiming@iie.ac.cn
RI Tan, Jiyong/JAC-7779-2023; Liu, Tongliang/AAA-1506-2021; Li,
   Shikun/HCI-3712-2022
OI Tan, Jiyong/0000-0001-6356-1743; Liu, Tongliang/0000-0002-9640-6472; Li,
   Shikun/0000-0003-4297-9571; Ge, Shiming/0000-0001-5293-310X
FU National Natural Science Foundation of China [61772513]; Beijing Natural
   Science Foundation [L192040]; National Key Research and Development Plan
   [2020AAA0140001]; Youth Innovation Promotion Association, Chinese
   Academy of Sciences
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772513, in part by the Beijing Natural
   Science Foundation under Grant L192040, and in part by the National Key
   Research and Development Plan under Grant 2020AAA0140001. The work of
   Shiming Ge was supported by the Youth Innovation Promotion Association,
   Chinese Academy of Sciences.
CR Aït-Sahalia Y, 2010, J AM STAT ASSOC, V105, P1504, DOI 10.1198/jasa.2010.tm10163
   Albarqouni S, 2016, IEEE T MED IMAGING, V35, P1313, DOI 10.1109/TMI.2016.2528120
   [Anonymous], 2010, P NIPS WORKSH DEEP L
   [Anonymous], 2013, DECISION MAKING IMPE
   Bach SH, 2017, PR MACH LEARN RES, V70
   Bai YB, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9292, DOI 10.1109/ICCV48922.2021.00918
   Bi W, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P82
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Cao P., 2019, PROC INT C LEARN REP
   Cha YC, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P565, DOI 10.1145/2348283.2348360
   Chaudhary C, 2020, IEEE T MULTIMEDIA, V22, P897, DOI 10.1109/TMM.2019.2937181
   Chen HL, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P110, DOI 10.1145/3219819.3219928
   Chen JP, 2022, IEEE T MULTIMEDIA, V24, P2218, DOI 10.1109/TMM.2021.3055037
   Cheng J., 2020, INT C MACHINE LEARNI, P1789
   CHO SB, 1995, IEEE T SYST MAN CYB, V25, P380, DOI 10.1109/21.364825
   Chu ZD, 2021, AAAI CONF ARTIF INTE, V35, P5832
   Cortes C., 2010, Proceedings of the 27th International Conference on Machine Learning, P239
   Dalvi N., 2013, WWW 13, P285
   Dawid A. P., 1979, J ROY STAT SOC C, P28
   Deng C, 2014, IEEE T MULTIMEDIA, V16, P785, DOI 10.1109/TMM.2014.2298841
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Domeniconi C, 2007, DATA MIN KNOWL DISC, V14, P63, DOI 10.1007/s10618-006-0060-8
   DUBOIS D, 1985, INFORM SCIENCES, V36, P85, DOI 10.1016/0020-0255(85)90027-1
   Goldman S., 2000, ICML, P327
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Guan MY, 2018, AAAI CONF ARTIF INTE, P3109
   Guo CA, 2017, PR MACH LEARN RES, V70
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hashem S, 1997, NEURAL NETWORKS, V10, P599, DOI 10.1016/S0893-6080(96)00098-6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y. S., 1993, Proceedings. 1993 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.93CH3309-2), P347, DOI 10.1109/CVPR.1993.1626170
   Ipeirotis PG, 2014, DATA MIN KNOWL DISC, V28, P402, DOI 10.1007/s10618-013-0306-1
   Karger David R., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P284
   Khetan A., 2018, PROC INT C LEARN REP
   Kim H.-C., 2012, INT C ARTIFICIAL INT, P619
   Kim H, 2011, J KOREAN STAT SOC, V40, P437, DOI 10.1016/j.jkss.2011.03.002
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Kumar P., 2011, Adv. Neural Inf. Process. Syst., P1413, DOI DOI 10.5555/2986459.2986617
   Kuncheva LI, 2001, PATTERN RECOGN, V34, P299, DOI 10.1016/S0031-3203(99)00223-X
   Kuncheva LI, 2014, COMBINING PATTERN CLASSIFIERS: METHODS AND ALGORITHMS, 2ND EDITION, P111
   Kurve A, 2015, IEEE T KNOWL DATA EN, V27, P794, DOI 10.1109/TKDE.2014.2327026
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Li HW, 2014, Arxiv, DOI arXiv:1411.4086
   Li Q, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1187, DOI 10.1145/2588555.2610509
   Li SK, 2020, AAAI CONF ARTIF INTE, V34, P4667
   Li W, 2017, Arxiv, DOI arXiv:1708.02862
   Li X, 2018, IEEE T MULTIMEDIA, V20, P1169, DOI 10.1109/TMM.2017.2761985
   Li YC, 2017, IEEE I CONF COMP VIS, P1928, DOI 10.1109/ICCV.2017.211
   Lin YH, 2021, IEEE T MULTIMEDIA, V23, P1605, DOI 10.1109/TMM.2020.3001521
   Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899
   Lu J, 2018, PR MACH LEARN RES, V80
   Ma F, 2017, PR MACH LEARN RES, V70
   Merz CJ, 1999, MACH LEARN, V36, P33, DOI 10.1023/A:1007559205422
   Moral-Benito E, 2015, J ECON SURV, V29, P46, DOI 10.1111/joes.12044
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P854, DOI 10.1109/TMM.2015.2419452
   Parameswaran A, 2011, PROC VLDB ENDOW, V4, P267, DOI 10.14778/1952376.1952377
   Przybyla-Kasperek M, 2016, FUND INFORM, V147, P353, DOI 10.3233/FI-2016-1412
   Rahman S, 2020, IEEE T MULTIMEDIA, V22, P242, DOI 10.1109/TMM.2019.2924511
   Raykar VC, 2010, J MACH LEARN RES, V11, P1297
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodrigues F, 2018, AAAI CONF ARTIF INTE, P1611
   Rodrigues F, 2017, IEEE T PATTERN ANAL, V39, P2409, DOI 10.1109/TPAMI.2017.2648786
   Servajean M, 2017, IEEE T MULTIMEDIA, V19, P1376, DOI 10.1109/TMM.2017.2653763
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sindhwani V., 2008, Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008), Helsinki, Finland, June 5-9, 2008, P976, DOI DOI 10.1145/1390156.1390279
   Sun ZZ, 2018, IEEE T CIRC SYST VID, V28, P193, DOI 10.1109/TCSVT.2016.2605045
   Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582
   Tian T, 2015, ADV NEUR IN, V28
   TITTERINGTON DM, 1981, J ROY STAT SOC A STA, V144, P145, DOI 10.2307/2981918
   Varma P, 2019, PR MACH LEARN RES, V97
   Wang GT, 2020, IEEE T MED IMAGING, V39, P2653, DOI 10.1109/TMI.2020.3000314
   Wang HY, 2009, J SYST SCI COMPLEX, V22, P732, DOI 10.1007/s11424-009-9198-y
   Wang H, 2014, IEEE T MULTIMEDIA, V16, P1282, DOI 10.1109/TMM.2014.2312251
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   WERNECKE KD, 1992, BIOMETRICS, V48, P497, DOI 10.2307/2532305
   Whitehill J., 2009, ADV NEURAL INFORM PR, P2035
   WOLPERT DH, 1992, NEURAL NETWORKS, V5, P241, DOI 10.1016/S0893-6080(05)80023-1
   Xia X., 2021, PROC INT C LEARN REP
   Xia XB, 2019, ADV NEUR IN, V32
   Yadati K, 2018, IEEE T MULTIMEDIA, V20, P2526, DOI 10.1109/TMM.2018.2801719
   Yao YZ, 2017, IEEE T MULTIMEDIA, V19, P1771, DOI 10.1109/TMM.2017.2684626
   Ye H.-J., 2015, Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, P991
   Yin LA, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1542
   Zadrozny B., 2002, P 8 ACM SIGKDD INT C, P694, DOI [10.1145/775047.775151, DOI 10.1145/775047.775151]
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang J, 2018, IEEE T NEUR NET LEAR, V29, P1675, DOI 10.1109/TNNLS.2017.2677468
   Zheng YD, 2017, PROC VLDB ENDOW, V10, P541, DOI 10.14778/3055540.3055547
   Zhou DY, 2014, PR MACH LEARN RES, V32, P262
   Zhou Dengyong, 2012, Advances in Neural Information Processing Systems, P2195
   Zhou Y., 2016, P 25 INT JOINT C ART, P2435, DOI DOI 10.5555/3060832.3060962
   Zhou Z.H., 2012, Ensemble Methods: Foundations and Algorithms, P67
   Zhu Z., 2021, P IEEE C COMP VIS PA, p10 113
NR 93
TC 2
Z9 2
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1045
EP 1057
DI 10.1109/TMM.2021.3137752
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA C4YC5
UT WOS:000961977900028
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, ZF
   Ni, BB
   Yang, XK
   Zhang, WJ
   Gao, W
AF Li, Zefan
   Ni, Bingbing
   Yang, Xiaokang
   Zhang, Wenjun
   Gao, Wen
TI Residual Quantization for Low Bit-Width Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Training; Computational modeling; Neurons;
   Degradation; Task analysis; Optimization; Deep learning; network
   quantization; binarization; network acceleration
AB Neural network quantization has shown to be an effective way for network compression and acceleration. However, existing binary or ternary quantization methods suffer from two major issues. First, low bit-width input/activation quantization easily results in severe prediction accuracy degradation. Second, network training and quantization are always treated as two non-related tasks, leading to accumulated parameter training error and quantization error. In this work, we introduce a novel scheme, named Residual Quantization, to train a neural network with both weights and inputs constrained to low bit-width, e.g., binary or ternary values. On one hand, by recursively performing residual quantization, the resulting binary/ternary network is guaranteed to approximate the full-precision network with much smaller errors. On the other hand, we mathematically re-formulate the network training scheme in an EM-like manner, which iteratively performs network quantization and parameter optimization. During expectation, the low bit-width network is encouraged to approximate the full-precision network. During maximization, the low bit-width network is further tuned to gain better representation capability. Extensive experiments well demonstrate that the proposed quantization scheme outperforms previous low bit-width methods and achieves much closer performance to the full-precision counterpart.
C1 [Li, Zefan; Ni, Bingbing; Yang, Xiaokang; Zhang, Wenjun] Shanghi Jiao Tong Univ, Shanghai 200240, Peoples R China.
   [Gao, Wen] Peking Univ, Beijing 100871, Peoples R China.
C3 Peking University
RP Ni, BB (corresponding author), Shanghi Jiao Tong Univ, Shanghai 200240, Peoples R China.
EM leezf@sjtu.edu.cn; nibingbing@sjtu.edu.cn; xkyang@sjtu.edu.cn;
   zhangwenjun@sjtu.edu.cn; wgao@pku.edu.cn
RI Zhang, Wenjun/GNH-2095-2022; Yang, Xiaokang/C-6137-2009; Li,
   Zefan/HGD-1915-2022
OI Zhang, Wenjun/0000-0002-5282-3725; Yang, Xiaokang/0000-0003-4029-3322;
   Li, Zefan/0000-0003-3019-155X
FU National Science Foundation of China [U20B2072, 61976137]; Shanghai
   Jiaotong University Medical Engineering Cross Research [YG2021ZD18]
FX This work was supported in part by the National Science Foundation of
   China under Grants U20B2072, 61976137 and in part by Shanghai Jiaotong
   University Medical Engineering Cross Research under Grant YG2021ZD18.
CR Alvarez JM, 2017, ADV NEUR IN, V30
   Alvarez R, 2016, INTERSPEECH, P2746, DOI 10.21437/Interspeech.2016-128
   [Anonymous], 2014, P BRIT MACHINE VISIO
   Bai JL, 2020, IEEE T MULTIMEDIA, V22, P215, DOI 10.1109/TMM.2019.2922130
   Banner R, 2019, ADV NEUR IN, V32
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Bulat A., 2021, 9 INT C LEARNING REP
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Choi J, 2018, Arxiv, DOI arXiv:1807.06964
   Choi J, 2018, Arxiv, DOI [arXiv:1805.06085, DOI 10.48550/ARXIV.1805.06085, 10.48550/arXiv.1805.06085]
   Courbariaux M., 2015, P ADV NEUR INF PROC, P3123
   Denton E, 2014, ADV NEUR IN, V27
   Ding J, 2017, PROC INT C TOOLS ART, P61, DOI 10.1109/ICTAI.2017.00021
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Han  S., 2015, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Hinton G., 2015, COMPUT SCI, V2
   Hu HY, 2016, Arxiv, DOI arXiv:1607.03250
   Hubara I, 2016, ADV NEUR IN, V29
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li FF, 2016, Arxiv, DOI arXiv:1605.04711
   Li H., 2017, PROC INT C LEARN REP
   Li LQ, 2018, IEICE T INF SYST, VE101D, P1203, DOI 10.1587/transinf.2017EDL8248
   Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282
   Lin SH, 2019, IEEE T PATTERN ANAL, V41, P2889, DOI 10.1109/TPAMI.2018.2873305
   Lin XF, 2017, ADV NEUR IN, V30
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Martinez B, 2020, 8 INT C LEARNING REP
   Mishra Asit, 2018, ICLR
   Netzer Y., 2011, PROC NEURIPS WORKSHO
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Romero A., 2014, ARXIV14126550
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838
   Sironi A, 2015, IEEE T PATTERN ANAL, V37, P94, DOI 10.1109/TPAMI.2014.2343229
   Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779
   Szegedy C., 2014, P IEEE CVF C COMP VI, DOI 10.1109/CVPR.2015.7298594
   Tu GY, 2020, IEEE T MULTIMEDIA, V22, P148, DOI 10.1109/TMM.2019.2922129
   Wang H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2769
   Wang P., 2020, INT C MACHINE LEARNI, P9847
   Wang ZZ, 2020, IEEE T MULTIMEDIA, V22, P2126, DOI 10.1109/TMM.2019.2950523
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yiwen Guo, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P4040, DOI 10.1109/CVPR.2017.430
   Zhang W, 2019, LECT NOTES ARTIF INT, V11745, P332, DOI 10.1007/978-3-030-27529-7_29
   Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809
   Zhou A., 2017, ARXIV170203044
   Zhou AJ, 2018, PROC CVPR IEEE, P9426, DOI 10.1109/CVPR.2018.00982
   Zhou H, 2016, LECT NOTES COMPUT SC, V9908, P662, DOI 10.1007/978-3-319-46493-0_40
   Zhou Shuchang, 2016, arXiv
   Zhu WW, 2020, IEEE T MULTIMEDIA, V22, P1823, DOI 10.1109/TMM.2020.2969791
NR 57
TC 5
Z9 6
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 214
EP 227
DI 10.1109/TMM.2021.3124095
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400016
DA 2024-07-18
ER

PT J
AU Lu, Z
   Hu, Y
   Yu, C
   Chen, Y
   Zeng, B
AF Lu, Zhi
   Hu, Yang
   Yu, Cong
   Chen, Yan
   Zeng, Bing
TI Learning Fashion Compatibility With Context Conditioning Embedding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Compatibility learning; fashion analysis; metric learning; outfit
   recommendation
ID GRAPH
AB Fashion compatibility predictions have obtained a lot of attention recently. Mining the compatibility between fashion items in an outfit is different from learning the visual similarity, since this relationship is more delicate. Decomposing the outfit compatibility into pairwise item matching is a popular way to treat the problem. However, in most existing methods, the items are matched without considering the context, i.e, the remaining items in the outfit. Recent efforts have been made to learn the underlying high order relationships among items by treating the outfit as a whole. These models could be sensitive to the properties of different datasets, and the item representations in these models are not as compact as those in the pairwise models. In this paper, we propose a context conditioning embedding approach to learn compact representations that preserve the shared information among items under the existence of contextual items. We use two different spaces, the general and the contextual spaces, to embed items, where the representation in the contextual space contains information from the context. We employ mutual information maximization for model learning, which is shown to be more appropriate for the problem. With extensive experiments, we show that our model achieves superior performance than other state-of-the-art methods.
C1 [Lu, Zhi; Yu, Cong; Zeng, Bing] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Hu, Y (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM zhilu@std.uestc.edu.cn; eeyhu@ustc.edu.cn; congyu@std.uestc.edu.cn;
   eecyan@ustc.edu.cn; eezeng@uestc.edu.cn
RI Yu, Zhou/KBP-8384-2024; Lu, Zhi/KGM-0198-2024
OI Lu, Zhi/0000-0001-6941-981X
FU National Natural Science Foundation of China [62172381]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172381.
CR Bordes A., 2013, P 26 INT C NEURAL IN, P2787
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen ZX, 2018, IEEE T MULTIMEDIA, V20, P2126, DOI 10.1109/TMM.2017.2785253
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Cucurull G, 2019, PROC CVPR IEEE, P12609, DOI 10.1109/CVPR.2019.01290
   Cui ZY, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P307, DOI 10.1145/3308558.3313444
   Gao YM, 2023, IEEE T PATTERN ANAL, V45, P7019, DOI 10.1109/TPAMI.2020.3025062
   Han XT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1078, DOI 10.1145/3123266.3123394
   Han XT, 2017, IEEE I CONF COMP VIS, P1472, DOI 10.1109/ICCV.2017.163
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He RN, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P507, DOI 10.1145/2872427.2883037
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jing PG, 2022, IEEE T MULTIMEDIA, V24, P1277, DOI 10.1109/TMM.2021.3062736
   Khosla Prannay, 2020, ADV NEURAL INFORM PR, V33, P18661
   Li XC, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P159, DOI 10.1145/3397271.3401080
   Li Y., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1511.05493
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Li ZC, 2022, IEEE T PATTERN ANAL, V44, P9904, DOI 10.1109/TPAMI.2021.3132068
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Lin YL, 2020, PROC CVPR IEEE, P3308, DOI 10.1109/CVPR42600.2020.00337
   Liu X, 2021, IEEE T MULTIMEDIA, V23, P2894, DOI 10.1109/TMM.2020.3018021
   Lu Z, 2022, 6TH INTERNATIONAL CONFERENCE ON INNOVATION IN ARTIFICIAL INTELLIGENCE, ICIAI2022, P166, DOI 10.1145/3529466.3529472
   Lu Z, 2021, PROC CVPR IEEE, P12717, DOI 10.1109/CVPR46437.2021.01253
   Lu Z, 2019, PROC CVPR IEEE, P10554, DOI 10.1109/CVPR.2019.01081
   Mall U, 2019, IEEE I CONF COMP VIS, P411, DOI 10.1109/ICCV.2019.00050
   McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755
   Paszke A, 2019, ADV NEUR IN, V32
   Poole B., 2018, P INT C ADV NEUR INF, P1
   Poole B, 2019, PR MACH LEARN RES, V97
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Singhal A., 2020, P WINT C APPL COMP V, P3607
   Sohn K, 2016, ADV NEUR IN, V29
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613
   Tan RB, 2019, IEEE I CONF COMP VIS, P10372, DOI 10.1109/ICCV.2019.01047
   Tang JH, 2019, IEEE T PATTERN ANAL, V41, P2027, DOI 10.1109/TPAMI.2019.2906603
   Tangseng P, 2017, IEEE INT CONF COMP V, P2275, DOI 10.1109/ICCVW.2017.267
   vandenOord Aaron, 2018, ARXIV180703748
   Vasileva MI, 2018, LECT NOTES COMPUT SC, V11220, P405, DOI 10.1007/978-3-030-01270-0_24
   Vaswani A, 2017, ADV NEUR IN, V30
   Veit A, 2017, PROC CVPR IEEE, P1781, DOI 10.1109/CVPR.2017.193
   Veit A, 2015, IEEE I CONF COMP VIS, P4642, DOI 10.1109/ICCV.2015.527
   Velickovic Petar, 2018, INT C LEARN REPR
   Weinberger K.Q., 2006, Advances in neural information processing systems, P1473
   Wu C., 2021, arXiv
   Yamaguchi K., 2015, P BRIT MACH VIS C
   Yang XW, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2636, DOI 10.1145/3394171.3413936
   Yang X, 2020, AAAI CONF ARTIF INTE, V34, P287
   Yang X, 2019, AAAI CONF ARTIF INTE, P403
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Zhan HJ, 2022, IEEE T MULTIMEDIA, V24, P819, DOI 10.1109/TMM.2021.3059514
   Zhang H., 2020, arXiv
   Zhou J, 2020, AI OPEN, V1, P57, DOI 10.1016/j.aiopen.2021.01.001
NR 54
TC 1
Z9 1
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5516
EP 5526
DI 10.1109/TMM.2022.3193560
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300063
DA 2024-07-18
ER

PT J
AU Qi, C
   Feng, ZY
   Xing, M
   Su, Y
   Zheng, JQ
   Zhang, YM
AF Qi, Cheng
   Feng, Zhiyong
   Xing, Meng
   Su, Yong
   Zheng, Jinqing
   Zhang, Yiming
TI Energy-Based Temporal Summarized Attentive Network for Zero-Shot Action
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Zero-shot action recognition; energy-based model; representative frames
   selection; saliency detection
ID CLASSIFICATION; VIDEOS
AB Recently, Action Recognition (AR) is facing the scalability problem, since collecting and annotating data for the ever-growing action categories is exhausting and inappropriate. As an alternative to AR, Zero-Shot Action Recognition (ZSAR) is getting more and more attention in the community, as they could utilize a shared semantic/attribute space to recognize novel categories without annotated data. Different from the AR focuses on learning the correlation between actions, ZSAR needs to consider the correlation of action-action, label-label and action-label at the same time. However, as far as we know, there is no work to provide structural guidance for the framework design of ZSAR according to its task characteristics. In this paper, we demonstrate the rationality of using the Energy-Based Model (EBM) to guide the framework design of ZSAR based on their inference mechanism. Furthermore, under the guidance of EBM, we propose an Energy-based Temporal Summarized Attentive Network (ETSAN) to achieve ZSAR. Specifically, to ensure the effectiveness of cross-modal matching, EBM needs to capture the correlations of input-input, output-output and input-output, based on discriminative and focused input and output space. To this end, we first design the Temporal Summarized Attentive Mechanism (TSAM) to capture the correlation of action-action by constructing discriminative and focused input space. Then, a Label Semantic Adaptive Mechanism (LSAM) is proposed to learn the correlation of label-label by adjusting the semantic structure according to the target task. Finally, we devise an Energy Score Estimation Mechanism (ESEM) to measure the compatibility (i.e. energy score) between video representation and label semantic embedding. With end-to-end training, our framework can capture all three of the correlations mentioned above simultaneously by minimizing the energy score of the correct action-label pair. Experiments on the HMDB51 and UCF101 datasets show that the proposed architecture achieves comparable results among methods based on the spatial-temporal visual feature of sequence-level, which demonstrates the efficiency of the EBM in guiding the framework design of ZSAR.
C1 [Qi, Cheng; Feng, Zhiyong; Xing, Meng; Zheng, Jinqing; Zhang, Yiming] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Su, Yong] Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tra, Tianjin 300382, Peoples R China.
C3 Tianjin University; Tianjin Normal University
RP Xing, M (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.; Su, Y (corresponding author), Tianjin Normal Univ, Tianjin Key Lab Wireless Mobile Commun & Power Tra, Tianjin 300382, Peoples R China.
EM qicheng_525@tju.edu.cn; zyfeng@tju.edu.cn; xingmeng@tju.edu.cn;
   suyong@tju.edu.cn; cszjq@tju.edu.cn; kevin_zhangym@tju.edu.cn
RI su, yong/JEO-5411-2023; Feng, Zhi-Yong/I-7541-2016
OI su, yong/0000-0002-6851-4142; Meng, Xing/0000-0001-6082-4675; Zheng,
   Jinqing/0000-0002-1772-0085
FU Shenzhen Science and Technology Foundation [JCYJ20170816093943197]
FX The work was supported by Shenzhen Science and Technology Foundation
   under Grant JCYJ20170816093943197
CR Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911
   Bishay M., 2019, ARXIV190709021
   Brattoli B, 2020, PROC CVPR IEEE, P4612, DOI 10.1109/CVPR42600.2020.00467
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Carreira J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1808.01340
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen SZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13618, DOI 10.1109/ICCV48922.2021.01338
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.3115/V1/D14-1179, 10.48550/ARXIV.1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du YL, 2019, ADV NEUR IN, V32
   Estevam V, 2021, NEUROCOMPUTING, V439, P159, DOI 10.1016/j.neucom.2021.01.036
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Grathwohl W, 2020, arXiv
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jain M, 2015, IEEE I CONF COMP VIS, P4588, DOI 10.1109/ICCV.2015.521
   Jingen Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3337, DOI 10.1109/CVPR.2011.5995353
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140
   LeCun Y., 2006, PREDICTING STRUCTURE, V1
   Lin C., 2022, CVPR, P19978
   Liu JG, 2009, PROC CVPR IEEE, P1996
   Liu Weitang, 2020, ADV NEURAL INFORM PR, V33, P21464
   Mahasseni B, 2017, PROC CVPR IEEE, P2982, DOI 10.1109/CVPR.2017.318
   Mandal D, 2019, PROC CVPR IEEE, P9977, DOI 10.1109/CVPR.2019.01022
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   O'Hara S, 2011, Arxiv, DOI arXiv:1101.3354
   Paszke A, 2019, ADV NEUR IN, V32
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Pu S, 2022, PROC CVPR IEEE, P19936, DOI 10.1109/CVPR52688.2022.01934
   Qin J, 2017, PROC CVPR IEEE, P1042, DOI 10.1109/CVPR.2017.117
   Romera-Paredes Bernardino, 2015, ICML
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   Simonyan K, 2014, ADV NEUR IN, V27
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tian L, 2020, IEEE T IMAGE PROCESS, V29, P8429, DOI 10.1109/TIP.2020.3013168
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5
   Wang SH, 2016, Arxiv, DOI arXiv:1611.01747
   Xing M, 2021, PATTERN RECOGN, V116, DOI 10.1016/j.patcog.2021.107953
   Xu X, 2016, LECT NOTES COMPUT SC, V9906, P343, DOI 10.1007/978-3-319-46475-6_22
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Ye M, 2017, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR.2017.542
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P1177, DOI 10.1109/TIP.2016.2516952
   Zhao T, 2021, INT J COMPUT VISION, V129, P2474, DOI 10.1007/s11263-021-01473-9
   Zhou KY, 2018, AAAI CONF ARTIF INTE, P7582
   Zhou ZW, 2017, PROC CVPR IEEE, P4761, DOI 10.1109/CVPR.2017.506
   Zhu Z., 2021, P 32 BRIT MACH VIS C, P237
NR 60
TC 1
Z9 1
U1 4
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1940
EP 1953
DI 10.1109/TMM.2023.3264847
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100028
DA 2024-07-18
ER

PT J
AU Rao, J
   Ding, L
   Qi, SH
   Fang, M
   Liu, Y
   Shen, L
   Tao, DC
AF Rao, Jun
   Ding, Liang
   Qi, Shuhan
   Fang, Meng
   Liu, Yang
   Shen, Li
   Tao, Dacheng
TI Dynamic Contrastive Distillation for Image-Text Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-modal retrieval; neural networks; contrastive learning
ID ROBUST
AB The recent advancement in vision-and-language pretraining (VLP) has significantly improved the performance of cross-modal image-text retrieval (ITR) systems. However, the increasing size of VLP models presents a challenge for real-world deployment due to their high latency, making them unsuitable for practical search scenarios. To alleviate this problem, we present a novel plug-in dynamic contrastive distillation (DCD) framework to compress the large VLP models for the ITR task. Technically, we face the following two challenges: 1) the typical uni-modal metric learning approach is difficult to directly apply to cross-modal tasks due to the limited GPU memory to optimize too many negative samples during handling cross-modal fusion features. 2) it is inefficient to static optimize the student network from different hard samples, which affects distillation learning and student network optimization. We propose a method for multi-modal contrastive learning that balances training costs and effects. Our approach involves using a teacher network to identify hard samples for student networks to learn from, allowing the students to leverage the knowledge from pre-trained teachers and effectively learn from hard samples. To learn from hard sample pairs, we propose dynamic distillation to dynamically learn samples of different difficulties to balance better the difficulty of knowledge and students' self-learning ability. We successfully apply our proposed DCD strategy on two state-of-the-art vision-language pretrained models, i.e., ViLT and METER. Extensive experiments on MS-COCO and Flickr30K benchmarks show the effectiveness and efficiency of our DCD framework. We further provide in-depth analyses and discussions that explain how the performance improves.
C1 [Rao, Jun; Liu, Yang] Harbin Inst Technol, Shenzhen 518055, Peoples R China.
   [Qi, Shuhan] Harbin Inst Technol Shenzhen, Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Qi, Shuhan] Guangdong Prov Key Lab Novel Secur Intelligence Te, Shenzhen 518055, Peoples R China.
   [Ding, Liang; Shen, Li; Tao, Dacheng] JD Explore Acad JD Com, Beijing 101111, Peoples R China.
   [Fang, Meng] Univ Liverpool, Liverpool L69 3BX, England.
C3 Harbin Institute of Technology; Peng Cheng Laboratory; Harbin Institute
   of Technology; University of Liverpool
RP Qi, SH (corresponding author), Harbin Inst Technol Shenzhen, Peng Cheng Lab, Shenzhen 518055, Peoples R China.; Qi, SH (corresponding author), Guangdong Prov Key Lab Novel Secur Intelligence Te, Shenzhen 518055, Peoples R China.
EM rao7jun@gmail.com; liangding.liam@gmail.com; shuhanqi@cs.hitsz.edu.cn;
   m.fang@tue.nl; liu.yang@hit.edu.cn; mathshenli@gmail.com;
   dacheng.tao@sydney.edu.au
RI Liu, Yang/D-2306-2013; Shen, Li/AEZ-9528-2022; Tao, Dacheng/A-5449-2012
OI Liu, Yang/0000-0001-7300-9215; Shen, Li/0000-0001-5659-3464; Tao,
   Dacheng/0000-0001-7225-5449; Liu, Yang/0000-0003-2486-5765; shuhan,
   qi/0000-0002-6903-145X
FU Science and Technology Innovation 2030 #x2013;#x201C;Brain Science and
   Brain-like Research#x201D; Major Project
FX No Statement Available
CR [Anonymous], 2009, Technical report
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Chen PG, 2021, PROC CVPR IEEE, P5006, DOI 10.1109/CVPR46437.2021.00497
   Chen T, 2020, PR MACH LEARN RES, V119
   Cui Q, 2022, LECT NOTES COMPUT SC, V13696, P236, DOI 10.1007/978-3-031-20059-5_14
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding L., 2021, The USYD-JD speech translation system for IWSLT2021
   Ding L., 2021, P INT C LEARN REPR, P1
   Ding L, 2022, PROCEEDINGS OF THE 60TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), VOL 1: (LONG PAPERS), P2417
   Ding L, 2019, FOURTH CONFERENCE ON MACHINE TRANSLATION (WMT 2019), P175
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dou Z.-Y, 2022, P ADV NEUR INF PROC, P1
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Faghri F, 2018, P BRIT MACH VIS C, P1
   Fang M., 2017, P 2017 C EMP METH NA, P595, DOI 10.18653/v1/D17-1063
   Fang ZY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1408, DOI 10.1109/ICCV48922.2021.00146
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   Grill Jean-Bastien, 2020, ADV NEURAL INFORM PR
   Gu Xiuye, 2021, ZERO SHOT DETECTION, P2
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hinton G., 2015, COMPUT SCI, V2
   Jiao XQ, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P4163
   Jin X, 2019, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2019.00143
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Karpathy A, 2017, IEEE T PATTERN ANAL, V39, P664, DOI 10.1109/TPAMI.2016.2598339
   Kim T., 2021, P 30 INT JOINT C ART, P2628, DOI DOI 10.24963/IJCAI.2021/362
   Kim W., 2021, PROC INT C MACH LEAR, P5583
   Kyaw Z, 2017, IEEE T MULTIMEDIA, V19, P1272, DOI 10.1109/TMM.2017.2655422
   Lakshminarayanan B, 2017, ADV NEUR IN, V30
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Li BY, 2019, AAAI CONF ARTIF INTE, P8577
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Li KP, 2019, IEEE I CONF COMP VIS, P4653, DOI 10.1109/ICCV.2019.00475
   Li L, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P379
   Li W, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (ACL-IJCNLP 2021), VOL 1, P2592
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu JS, 2019, ADV NEUR IN, V32
   Lu X, 2021, IEEE T MULTIMEDIA, V23, P4541, DOI 10.1109/TMM.2020.3044473
   Qu LG, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1047, DOI 10.1145/3394171.3413961
   Rao J., 2022, Parameter-efficient and student-friendly knowledge distillation
   Rao J, 2021, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, CIKM 2021, P3383, DOI 10.1145/3459637.3482194
   Rao J, 2022, PROCEEDINGS OF THE 45TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '22), P2727, DOI 10.1145/3477495.3531715
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Robinson Joshua, 2020, INT C LEARN REPR
   Sanh, 2019, P 5 WORKSH EN EFF MA
   Shao Zhiyin, 2022, MM '22: Proceedings of the 30th ACM International Conference on Multimedia, P5566, DOI 10.1145/3503161.3548028
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Sohn K, 2016, ADV NEUR IN, V29
   Sun SQ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P4323
   Sun ZQ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2158
   Tang S, 2019, P BRIT MACH VIS C, P1
   vandenOord Aaron, 2018, ARXIV180703748
   Wang B, 2023, T I MEAS CONTROL, V45, P233, DOI 10.1177/01423312221104424
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang W., 2022, P ADV NEUR INF PROC
   Wang W., 2022, IMAGE FOREIGN LANGUA
   Wang ZH, 2019, IEEE I CONF COMP VIS, P5763, DOI 10.1109/ICCV.2019.00586
   Wu YL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2088, DOI 10.1145/3343031.3350940
   Ye M, 2019, PROC CVPR IEEE, P6203, DOI 10.1109/CVPR.2019.00637
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zan C, 2022, P WMTED EMNLP, P1
   Zeng Y, 2022, 39 INT C MACHINE LEA
   Zhang BW, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P823
   Zhang LF, 2019, IEEE I CONF COMP VIS, P3712, DOI 10.1109/ICCV.2019.00381
   Zhang Q, 2020, PROC CVPR IEEE, P3533, DOI 10.1109/CVPR42600.2020.00359
   Zhang YZ, 2020, APPL POWER ELECT CO, P658, DOI [10.1007/978-3-030-58529-7_39, 10.1109/apec39645.2020.9124291]
   Zhong Q, 2022, Panda: Prompt transfer meets knowledge distillation for efficient model adaptation
   Zhong Q, 2022, E2-2: Encoding-enhanced sequence-to-sequence pretraining for language understanding and generation
   Zhou H, 2021, P INT C LEARN REPR, P1
NR 74
TC 4
Z9 4
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8383
EP 8395
DI 10.1109/TMM.2023.3236837
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000033
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Weng, SW
   Zhou, Y
   Zhang, TC
   Xiao, MY
   Zhao, Y
AF Weng, Shaowei
   Zhou, Ye
   Zhang, Tiancong
   Xiao, Mengyao
   Zhao, Yao
TI Reversible Data Hiding for JPEG Images With Adaptive Multiple
   Two-Dimensional Histogram and Mapping Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Histograms; Transform coding; Distortion; Discrete cosine transforms;
   Payloads; Media; Optimization; Adaptive 2D mapping generation; band
   smoothness estimator; block smoothness estimator; IDPSO; JPEG images;
   reversible data hiding
ID PREDICTION-ERROR EXPANSION
AB Reversible data hiding based on joint photographic experts group (JPEG) images has been extensively studied to enhance embedding performance in terms of visual quality and file size preservation at the desired payload. In this paper, an efficient adaptive RDH method for JPEG images with multiple two-dimensional (2D) histogram modification is proposed. Firstly, the proposed method proposes the block smoothness estimator and the band smoothness estimator, and then combines the two estimators to reduce the embedding distortion as much as possible at the desired payload. Instead of adopting a fixed 2D mapping or choosing one from several empirically-designed mappings for each 2D histogram, the proposed method designs an adaptive 2D mapping generation strategy to adaptively generate a large number of mappings with considering the local characteristics of histogram distribution. Since exhaustively searching for the optimal mapping achieving the highest embedding performance for each 2D histogram is time-consuming, an improved discrete particle swarm optimization is utilized in the proposed method to speed up the optimization process. Extensive experimental results also demonstrate the effectiveness of the proposed method in terms of visual quality and file size increment of the stego image.
C1 [Weng, Shaowei; Zhang, Tiancong] Fujian Univ Technol, Fujian Prov Key Lab Big Data Min & Applicat, Fuzhou 350108, Peoples R China.
   [Weng, Shaowei; Zhang, Tiancong] Fujian Univ Technol, Sch Elect Elect Engn & Phys, Fuzhou 350108, Peoples R China.
   [Zhou, Ye] Fujian Univ Technol, Sch Comp Sci & Math, Fuzhou 350118, Peoples R China.
   [Xiao, Mengyao; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Xiao, Mengyao; Zhao, Yao] Beijing Key Lab Adv Informat Sci & Network Technol, Beijing 100044, Peoples R China.
C3 Fujian University of Technology; Fujian University of Technology; Fujian
   University of Technology; Beijing Jiaotong University
RP Zhang, TC (corresponding author), Fujian Univ Technol, Fujian Prov Key Lab Big Data Min & Applicat, Fuzhou 350108, Peoples R China.
EM wswweiwei@126.com; 1962270759@qq.com; kushentian@163.com;
   xiaomengyao@bjtu.edu.cn; yzhao@bjtu.edu.cn
OI Zhao, Yao/0000-0002-8581-9554
FU National NSF of China
FX No Statement Available
CR Alattar AM, 2004, IEEE T IMAGE PROCESS, V13, P1147, DOI 10.1109/TIP.2004.828418
   [Anonymous], 1999, Kodak lossless true color image database
   [Anonymous], 2010, Uncompressed colour image database
   [Anonymous], 1977, USC SIPI IMAGE DATAB
   Chang CC, 2007, INFORM SCIENCES, V177, P2768, DOI 10.1016/j.ins.2007.02.019
   Chang Q, 2021, IEEE T CIRC SYST VID, V31, P4850, DOI 10.1109/TCSVT.2021.3055612
   Efimushkina T., 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P94
   Fridrich J, 2002, P SOC PHOTO-OPT INS, V4675, P572, DOI 10.1117/12.465317
   Fridrich J, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P223, DOI 10.1109/ITCC.2001.918795
   He JH, 2020, SIGNAL PROCESS, V175, DOI 10.1016/j.sigpro.2020.107647
   He JH, 2020, IEEE T INF FOREN SEC, V15, P2121, DOI 10.1109/TIFS.2019.2958758
   Hou DD, 2018, SIGNAL PROCESS, V148, P41, DOI 10.1016/j.sigpro.2018.02.002
   Hu YJ, 2013, J SYST SOFTWARE, V86, P2166, DOI 10.1016/j.jss.2013.03.102
   Huang FJ, 2016, IEEE T CIRC SYST VID, V26, P1610, DOI 10.1109/TCSVT.2015.2473235
   Li N, 2020, SIGNAL PROCESS, V171, DOI 10.1016/j.sigpro.2020.107476
   Li XL, 2015, IEEE T INF FOREN SEC, V10, P2016, DOI 10.1109/TIFS.2015.2444354
   Mobasseri BG, 2010, IEEE T IMAGE PROCESS, V19, P958, DOI 10.1109/TIP.2009.2035227
   Ni ZC, 2006, IEEE T CIRC SYST VID, V16, P354, DOI 10.1109/TCSVT.2006.869964
   Ou B, 2020, IEEE T CIRC SYST VID, V30, P2329, DOI 10.1109/TCSVT.2019.2921812
   Ou B, 2019, IEEE T CIRC SYST VID, V29, P2176, DOI 10.1109/TCSVT.2018.2859792
   Ou B, 2013, IEEE T IMAGE PROCESS, V22, P5010, DOI 10.1109/TIP.2013.2281422
   Puteaux P, 2021, IEEE T MULTIMEDIA, V23, P636, DOI 10.1109/TMM.2020.2985537
   Qi WF, 2020, IEEE T CIRC SYST VID, V30, P2300, DOI 10.1109/TCSVT.2019.2942489
   Qian ZX, 2014, IEEE T MULTIMEDIA, V16, P1486, DOI 10.1109/TMM.2014.2316154
   Qian ZX, 2012, J SYST SOFTWARE, V85, P309, DOI 10.1016/j.jss.2011.08.015
   Sachnev V, 2009, IEEE T CIRC SYST VID, V19, P989, DOI 10.1109/TCSVT.2009.2020257
   Sakai H., 2008, P INT S INF THEOR IT, P1
   Shi YQ, 2016, IEEE ACCESS, V4, P3210, DOI 10.1109/ACCESS.2016.2573308
   Thodi DM, 2007, IEEE T IMAGE PROCESS, V16, P721, DOI 10.1109/TIP.2006.891046
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Wang JX, 2020, IEEE T CIRC SYST VID, V30, P2313, DOI 10.1109/TCSVT.2019.2915584
   Wang JX, 2019, SIGNAL PROCESS, V159, P193, DOI 10.1016/j.sigpro.2019.02.013
   Wang JX, 2017, IEEE T CYBERNETICS, V47, P315, DOI 10.1109/TCYB.2015.2514110
   Wang K, 2013, J SYST SOFTWARE, V86, P1965, DOI 10.1016/j.jss.2013.03.083
   Wedaj FT, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0206-1
   Weng SW, 2023, IEEE T MULTIMEDIA, V25, P5747, DOI 10.1109/TMM.2022.3198877
   Weng SW, 2022, J VIS COMMUN IMAGE R, V85, DOI 10.1016/j.jvcir.2022.103487
   Weng SW, 2021, INFORM SCIENCES, V549, P13, DOI 10.1016/j.ins.2020.10.063
   Weng SW, 2008, IEEE SIGNAL PROC LET, V15, P721, DOI 10.1109/LSP.2008.2001984
   Xiao MY, 2021, IEEE SIGNAL PROC LET, V28, P1620, DOI 10.1109/LSP.2021.3101424
   Xiao MY, 2021, IEEE T CIRC SYST VID, V31, P2535, DOI 10.1109/TCSVT.2020.3027391
   Xuan GR, 2007, LECT NOTES COMPUT SC, V4633, P715
   Yang X, 2022, SIGNAL PROCESS, V200, DOI 10.1016/j.sigpro.2022.108639
   Yin ZX, 2020, IEEE T CIRC SYST VID, V30, P2343, DOI 10.1109/TCSVT.2020.2969463
   Zhang XP, 2013, IEEE T MULTIMEDIA, V15, P316, DOI 10.1109/TMM.2012.2229262
NR 45
TC 7
Z9 7
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8738
EP 8752
DI 10.1109/TMM.2023.3241541
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000067
DA 2024-07-18
ER

PT J
AU Wu, F
   Wang, QZ
   Bian, J
   Ding, N
   Lu, FX
   Cheng, J
   Dou, DJ
   Xiong, HY
AF Wu, Fei
   Wang, Qingzhong
   Bian, Jiang
   Ding, Ning
   Lu, Feixiang
   Cheng, Jun
   Dou, Dejing
   Xiong, Haoyi
TI A Survey on Video Action Recognition in Sports: Datasets, Methods and
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Action recognition; video analysis; sports; computer vision; deep
   learning; survey
ID REPRESENTATION; HISTOGRAMS; FLOW
AB To understand human behaviors, action recognition based on videos is a common approach. Compared with image-based action recognition, videos provide much more information, reducing the ambiguity of actions. In the last decade, many works focus on datasets, novel models and learning approaches have improved video action recognition to a higher level. However, there are challenges and unsolved problems, in particular in sports analytics where data collection and labeling are more sophisticated, requiring people with domain knowledge and even sport professionals to annotate data. In addition, the actions could be extremely fast and it becomes difficult to recognize them. Moreover, in team sports like football and basketball, one action could involve multiple players, and to correctly recognize them, we need to analyze all players, which is relatively complicated. In this paper, we present a survey on video action recognition for sports analytics. We introduce more than ten types of sports, including team sports, such as football, basketball, volleyball, hockey and individual sports, such as figure skating, gymnastics, table tennis, tennis, diving and badminton. Then we compare numerous existing frameworks for sports analysis to present status quo of video action recognition in both team sports and individual sports. Finally, we discuss the challenges and unsolved problems in this area and to facilitate sports analytics, we develop a toolbox using PaddlePaddle, which supports football, basketball, table tennis and figure skating action recognition.
C1 [Wu, Fei; Ding, Ning] Peking Univ, Dept Phys Educ, Beijing 100080, Peoples R China.
   [Wang, Qingzhong; Bian, Jiang; Lu, Feixiang; Cheng, Jun; Xiong, Haoyi] Baidu Inc, Beijing 100080, Peoples R China.
   [Dou, Dejing] Boston Consulting Grp Greater China, Beijing 100080, Peoples R China.
C3 Peking University; Baidu
RP Wang, QZ; Xiong, HY (corresponding author), Baidu Inc, Beijing 100080, Peoples R China.
EM wufei@pku.edu.cn; qingzwang@outlook.com; bianjiang03@baidu.com;
   dn620@stu.pku.edu.cn; lufeixiang@baidu.com; chengjun@baidu.com;
   dejingdou@gmail.com; haoyi.xiong.fr@ieee.org
RI WU, Fei/ADX-5542-2022; XIONG, HAOYI/E-5079-2015
OI WU, Fei/0000-0002-2240-4829; XIONG, HAOYI/0000-0002-5451-3253; Dou,
   Dejing/0000-0003-2949-6874
FU National Key R&D Program of China [2021ZD0110303]; Humanities and Social
   Science Research of Ministry of Education of China [20YJA890024]
FX This work was support in part by the National Key R&D Program of China
   2021ZD0110303, and in part by the Humanities and Social Science Research
   of Ministry of Education of China under Grant 20YJA890024.
CR Abu-El-Haija Sami, 2016, arXiv, DOI [DOI 10.48550/ARXIV.1609.08675, DOI 10.48550/-ARXIV.1609.08675]
   Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Anuradha K., 2016, Indian J. Sci. Technol., V9, P1
   Arnab A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6816, DOI 10.1109/ICCV48922.2021.00676
   Bagautdinov T, 2017, PROC CVPR IEEE, P3425, DOI 10.1109/CVPR.2017.365
   Bao H., 2022, PROC INT C LEARN REP, P1
   Beal R, 2019, KNOWL ENG REV, V34, DOI 10.1017/S0269888919000225
   Bender Gabriel, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14311, DOI 10.1109/CVPR42600.2020.01433
   Nievas EB, 2011, LECT NOTES COMPUT SC, V6855, P332, DOI 10.1007/978-3-642-23678-5_39
   Bertasius G, 2021, PR MACH LEARN RES, V139
   Bertasius G, 2017, IEEE I CONF COMP VIS, P2196, DOI 10.1109/ICCV.2017.239
   Bian J, 2022, Arxiv, DOI arXiv:2207.12730
   Bian YL, 2017, Arxiv, DOI arXiv:1708.03805
   Bo Y, 2020, IEEE WINT CONF APPL, P584, DOI [10.1109/wacv45572.2020.9093481, 10.1109/WACV45572.2020.9093481]
   Bonidia RP, 2018, ADV HUM-COMPUT INTER, V2018, DOI 10.1155/2018/3426178
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Campos T. D., 2011, Applications of Computer Vision (WACV), 2011 IEEE Workshop on, P344
   Caron M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9630, DOI 10.1109/ICCV48922.2021.00951
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Carson F, 2008, INT J SPORTS SCI COA, V3, P381, DOI 10.1260/174795408786238515
   Chen F., 2008, NETWORKED ELECT MEDI, P1
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Chen PH, 2020, IEEE T MULTIMEDIA, V22, P2723, DOI 10.1109/TMM.2019.2959977
   Chen S., 2022, arXiv
   Chen SX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1523, DOI 10.1109/ICCV48922.2021.00157
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng Yan, 2021, 2021 16th International Conference on Computer Science & Education (ICCSE), P653, DOI 10.1109/ICCSE51940.2021.9569708
   Cioppa Anthony, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13123, DOI 10.1109/CVPR42600.2020.01314
   Cust EE, 2019, J SPORT SCI, V37, P568, DOI 10.1080/02640414.2018.1521769
   D'Orazio T, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P559, DOI 10.1109/AVSS.2009.69
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deliege A, 2021, IEEE COMPUT SOC CONF, P4503, DOI 10.1109/CVPRW53098.2021.00508
   Demir U, 2021, INT C PATT RECOG, P7387, DOI 10.1109/ICPR48806.2021.9412541
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Ding W, 2017, ASIAPAC SIGN INFO PR, P1368, DOI 10.1109/APSIPA.2017.8282246
   Direkoglu C, 2012, LECT NOTES COMPUT SC, V7578, P69, DOI 10.1007/978-3-642-33786-4_6
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duan H., 2022, CVPR, P2969, DOI DOI 10.1109/CVPR52688.2022.00298
   Evans MB, 2012, CAN PSYCHOL, V53, P301, DOI 10.1037/a0030202
   Fan HQ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P6804, DOI 10.1109/ICCV48922.2021.00675
   Fani M, 2017, IEEE COMPUT SOC CONF, P85, DOI 10.1109/CVPRW.2017.17
   Fathi A, 2008, PROC CVPR IEEE, P3064
   Faulkner H., 2017, P INT C DIG IM COMP, P1
   Fedus W, 2022, Arxiv, DOI [arXiv:2209.01667, DOI 10.48550/ARXIV.2209.01667]
   Feichtenhofer C, 2020, PROC CVPR IEEE, P200, DOI 10.1109/CVPR42600.2020.00028
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Feng N, 2020, MULTIMED TOOLS APPL, V79, P28971, DOI 10.1007/s11042-020-09414-3
   Francia S., 2018, Classificazione di azioni cestistiche mediante tecniche di deep learning
   Gan C, 2016, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2016.106
   Gavrilyuk K, 2020, PROC CVPR IEEE, P836, DOI 10.1109/CVPR42600.2020.00092
   Ghadiyaram D, 2019, PROC CVPR IEEE, P12038, DOI 10.1109/CVPR.2019.01232
   Ghosh A, 2018, IEEE WINT CONF APPL, P296, DOI 10.1109/WACV.2018.00039
   Giancola S, 2018, IEEE COMPUT SOC CONF, P1792, DOI 10.1109/CVPRW.2018.00223
   Giles B, 2020, J SPORT SCI, V38, P106, DOI 10.1080/02640414.2019.1684132
   Gong JP, 2017, 2017 16TH IEEE/ACIS INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCE (ICIS 2017), P499
   Gordon D., 2014, Group activity recognition using wearable sensing devices
   Gourgari S, 2013, IEEE COMPUT SOC CONF, P676, DOI 10.1109/CVPRW.2013.102
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Grehaigne JF, 1997, J TEACH PHYS EDUC, V16, P500, DOI 10.1123/jtpe.16.4.500
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Gu XF, 2020, INT CONF ACOUST SPEE, P2563, DOI [10.1109/icassp40776.2020.9053928, 10.1109/ICASSP40776.2020.9053928]
   Gudmundsson Joachim, 2017, ACM Computing Surveys, V50, DOI 10.1145/3054132
   Hao T, 2017, J VIS COMMUN IMAGE R, V48, P453, DOI 10.1016/j.jvcir.2017.01.019
   Hao Z., 2013, P 21 ACM INT C MULT, P377
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hendry D, 2020, SPORTS MED-OPEN, V6, DOI 10.1186/s40798-020-0237-5
   Herath S, 2017, IMAGE VISION COMPUT, V60, P4, DOI 10.1016/j.imavis.2017.01.010
   Herzig R, 2022, PROC CVPR IEEE, P3138, DOI 10.1109/CVPR52688.2022.00315
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu K., 2021, P IEEECVF INT C COMP, P7939
   Ibrahim MS, 2018, LECT NOTES COMPUT SC, V11207, P742, DOI 10.1007/978-3-030-01219-9_44
   Ibrahim MS, 2016, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2016.217
   Idrees H, 2017, COMPUT VIS IMAGE UND, V155, P1, DOI 10.1016/j.cviu.2016.10.018
   Ijjina Earnest Paul, 2020, Proceedings of the Third International Conference on Computational Intelligence and Informatics. ICCII 2018. Advances in Intelligent Systems and Computing (AISC 1090), P849, DOI 10.1007/978-981-15-1480-7_79
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Iosifidis A, 2012, EUR SIGNAL PR CONF, P1129
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Jiang Y., 2020, PROC 3 INT WORKSHOP, P1
   Jiang Y., 2022, arXiv
   Kajbafnezhad H., 2011, Journal of Physical Education and Sport, V11, P249
   Kanerva J., 2019, P NORD C COMP LING, P242
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Kim TK, 2007, PROC CVPR IEEE, P1275
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Klaser A., 2008, BMVC 2008 19 BRIT MA, P1
   Klaser A., 2010, Will person detection help bag-of-features action recognition?
   Kondratyuk D, 2021, PROC CVPR IEEE, P16015, DOI 10.1109/CVPR46437.2021.01576
   Kong Y, 2022, INT J COMPUT VISION, V130, P1366, DOI 10.1007/s11263-022-01594-9
   Kovashka A, 2010, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR.2010.5539881
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Kulkarni KM, 2021, IEEE COMPUT SOC CONF, P4571, DOI 10.1109/CVPRW53098.2021.00515
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Larsson Viktor, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P382, DOI 10.1007/978-3-030-58558-7_23
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Lee MG, 2018, LECT NOTES COMPUT SC, V11214, P392, DOI 10.1007/978-3-030-01249-6_24
   Li Ang, 2020, arXiv
   Li HX, 2022, FRACTALS, V30, DOI 10.1142/S0218348X22401569
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Li X., 2020, INT C MACHINE LEARNI, P6010
   Li X., 2019, PROC INT C LEARN REP, P1
   Li Y., 2021, PROC IEEECVF INT, P13536
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li YW, 2018, LECT NOTES COMPUT SC, V11210, P520, DOI 10.1007/978-3-030-01231-1_32
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Liu G., 2014, Int. J. Multimedia Ubiquitous Eng., V9, P297
   Liu SL, 2021, AAAI CONF ARTIF INTE, V35, P2163
   Liu SL, 2020, NEUROCOMPUTING, V413, P360, DOI 10.1016/j.neucom.2020.06.108
   Liu Z, 2022, PROC CVPR IEEE, P3192, DOI 10.1109/CVPR52688.2022.00320
   Long X, 2018, PROC CVPR IEEE, P7834, DOI 10.1109/CVPR.2018.00817
   Lorre G, 2020, IEEE WINT CONF APPL, P651, DOI [10.1109/WACV45572.2020.9093278, 10.1109/wacv45572.2020.9093278]
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lu K., 2017, PROC BRIT MACH VIS C, P1
   Ma CY, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11104426
   Ma ZX, 2022, PPOPP'22: PROCEEDINGS OF THE 27TH ACM SIGPLAN SYMPOSIUM ON PRINCIPLES AND PRACTICE OF PARALLEL PROGRAMMING, P192, DOI 10.1145/3503221.3508417
   Mahaseni B, 2021, IEEE ACCESS, V9, P61929, DOI 10.1109/ACCESS.2021.3074831
   Maksai A, 2016, PROC CVPR IEEE, P972, DOI 10.1109/CVPR.2016.111
   Manafifard M, 2017, COMPUT VIS IMAGE UND, V159, P19, DOI 10.1016/j.cviu.2017.02.002
   Martin P. -E., 2021, arXiv
   Martin PE, 2018, INT WORK CONTENT MUL
   Martin PE, 2020, MULTIMED TOOLS APPL, V79, P20429, DOI 10.1007/s11042-020-08917-3
   McNally W, 2019, IEEE COMPUT SOC CONF, P2553, DOI 10.1109/CVPRW.2019.00311
   Megrhi S, 2016, J VIS COMMUN IMAGE R, V41, P375, DOI 10.1016/j.jvcir.2016.10.016
   Miech A, 2019, IEEE I CONF COMP VIS, P2630, DOI 10.1109/ICCV.2019.00272
   Mishra A, 2018, IEEE WINT CONF APPL, P372, DOI 10.1109/WACV.2018.00047
   Modi R, 2022, IEEE COMPUT SOC CONF, P4907, DOI 10.1109/CVPRW56347.2022.00538
   Nag S, 2022, LECT NOTES COMPUT SC, V13663, P681, DOI 10.1007/978-3-031-20062-5_39
   Nakano T, 2020, Arxiv, DOI arXiv:2007.01089
   Neimark D, 2021, IEEE INT CONF COMP V, P3156, DOI [arXiv:2102.00719, 10.1109/ICCVW54120.2021.00355]
   Nekoui M, 2020, IEEE COMPUT SOC CONF, P3941, DOI 10.1109/CVPRW50498.2020.00458
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Niebles JC, 2010, LECT NOTES COMPUT SC, V6312, P392, DOI 10.1007/978-3-642-15552-9_29
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100
   Pan TY, 2022, IEEE T CYBERNETICS, V52, P3172, DOI 10.1109/TCYB.2020.3007173
   Pareek P, 2021, ARTIF INTELL REV, V54, P2259, DOI 10.1007/s10462-020-09904-8
   Parisot P, 2019, J REAL-TIME IMAGE PR, V16, P1335, DOI 10.1007/s11554-016-0638-3
   Parisot P, 2017, COMPUT VIS IMAGE UND, V159, P74, DOI 10.1016/j.cviu.2017.01.001
   Parmar P, 2022, IEEE WINT C APPL COM, P161, DOI 10.1109/WACVW54805.2022.00022
   Parmar P, 2019, PROC CVPR IEEE, P304, DOI 10.1109/CVPR.2019.00039
   Parmar P, 2019, IEEE WINT CONF APPL, P1468, DOI 10.1109/WACV.2019.00161
   Parmar P, 2017, IEEE COMPUT SOC CONF, P76, DOI 10.1109/CVPRW.2017.16
   Patrick M, 2021, P INT C NEUR INF PRO, P12493
   Perez M, 2022, PATTERN RECOGN, V122, DOI 10.1016/j.patcog.2021.108360
   Pers J., 2005, Cvbase 06 dataset: A dataset for development and testing of computer vision based methods in sport environments
   Pers J, 2010, PATTERN RECOGN LETT, V31, P1369, DOI 10.1016/j.patrec.2010.03.024
   Pickering Craig, 2019, J Funct Morphol Kinesiol, V4, DOI 10.3390/jfmk4020025
   Piergiovanni AJ, 2019, PROC CVPR IEEE, P9937, DOI 10.1109/CVPR.2019.01018
   Pirsiavash H, 2014, LECT NOTES COMPUT SC, V8694, P556, DOI 10.1007/978-3-319-10599-4_36
   Pouyanfar S, 2017, IEEE INT CON MULTI, P373, DOI 10.1109/ICME.2017.8019447
   Qi MS, 2020, IEEE T CIRC SYST VID, V30, P549, DOI 10.1109/TCSVT.2019.2894161
   Qing ZW, 2021, PROC CVPR IEEE, P485, DOI 10.1109/CVPR46437.2021.00055
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rahmad N. A., 2018, Indonesian J. Elect. Eng. Comput. Sci., V11, P987
   Ramanathan M, 2014, IEEE T HUM-MACH SYST, V44, P650, DOI 10.1109/THMS.2014.2325871
   Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332
   Ramesh A, 2021, PR MACH LEARN RES, V139
   Rangasamy K., 2020, Telecommun. Comput. Electron. Control, V18, P1926
   Rasmussen T. E., 2022, P NO LIGHTS DEEP LEA, P1
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727
   Russell B, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21020654
   Ibrahim MS, 2016, Arxiv, DOI arXiv:1607.02643
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Safdarnejad Seyed Morteza, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163105
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Schwarcz S, 2019, Arxiv, DOI arXiv:1912.06640
   Sermanet P, 2018, IEEE INT CONF ROBOT, P1134
   Sevilla-Lara Laura, 2019, Pattern Recognition. 40th German Conference, GCPR 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11269), P281, DOI 10.1007/978-3-030-12939-2_20
   Shao D, 2020, PROC CVPR IEEE, P2613, DOI 10.1109/CVPR42600.2020.00269
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shih HC, 2018, IEEE T CIRC SYST VID, V28, P1212, DOI 10.1109/TCSVT.2017.2655624
   Shroff N, 2010, IEEE T MULTIMEDIA, V12, P853, DOI 10.1109/TMM.2010.2058795
   Shu TM, 2017, PROC CVPR IEEE, P4255, DOI 10.1109/CVPR.2017.453
   Shukla P, 2018, IEEE COMPUT SOC CONF, P1881, DOI 10.1109/CVPRW.2018.00233
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Simonyan K, 2014, ADV NEUR IN, V27
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Soomro K., 2014, COMPUTER VISION SPOR, V71, P181, DOI [DOI 10.1007/978-3-319-09396-3_9, DOI 10.1007/978-3-319-09396-39]
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Sozykin K, 2018, 2018 19TH IEEE/ACIS INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, ARTIFICIAL INTELLIGENCE, NETWORKING AND PARALLEL/DISTRIBUTED COMPUTING (SNPD), P146, DOI 10.1109/SNPD.2018.8441034
   Sri-Iesaranusorn P., 2021, PROC 13 INT C, P1
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sun Y, 2021, Arxiv, DOI arXiv:2107.02137
   Sun ZH, 2023, IEEE T PATTERN ANAL, V45, P3200, DOI 10.1109/TPAMI.2022.3183112
   Tan H., 2021, arXiv
   Tejero-de-Pablos A, 2018, IEEE T MULTIMEDIA, V20, P2000, DOI 10.1109/TMM.2018.2794265
   Thilakarathne H, 2022, MACH VISION APPL, V33, DOI 10.1007/s00138-022-01346-2
   Tian L., 2020, Proceedings, V49, DOI DOI 10.3390/PROCEEDINGS2020049095
   Tirupattur P, 2021, PROC CVPR IEEE, P1460, DOI 10.1109/CVPR46437.2021.00151
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tsunoda T, 2017, IEEE COMPUT SOC CONF, P155, DOI 10.1109/CVPRW.2017.25
   Tung HYF, 2017, ADV NEUR IN, V30
   Vaswani A, 2017, ADV NEUR IN, V30
   Vats K, 2023, EXPERT SYST APPL, V213, DOI 10.1016/j.eswa.2022.119250
   Voeikov R, 2020, IEEE COMPUT SOC CONF, P3857, DOI 10.1109/CVPRW50498.2020.00450
   Wan RS, 2019, IEEE DATA MINING, P578, DOI 10.1109/ICDM.2019.00068
   Wang D., 2020, P IEEECVF C COMPUTER, P12695, DOI DOI 10.1109/CVPR42600.2020.01271
   Wang DG, 2018, LECT NOTES COMPUT SC, V11213, P457, DOI 10.1007/978-3-030-01240-3_28
   Wang H., 2009, BMVC
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang JB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P374, DOI 10.1145/3343031.3350910
   Wang JN, 2023, IEEE T PATTERN ANAL, V45, P2088, DOI 10.1109/TPAMI.2022.3159811
   Wang L, 2014, IEEE INT CONF VLSI
   Wang LC, 2019, IEEE I CONF COMP VIS, P6221, DOI 10.1109/ICCV.2019.00631
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang Q., 2020, PROC BRIT MACH VIS C, P1
   Wang Q., 2020, IEEE Trans. Pattern Anal. Mach. Intell., V44, P1035
   Wang R, 2022, PROC CVPR IEEE, P14713, DOI 10.1109/CVPR52688.2022.01432
   Wang Tao, 2021, ADV NEURAL INFORM PR, V34
   Wang W, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3293318
   Wang WY, 2022, AAAI CONF ARTIF INTE, P4219
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wei C, 2022, PROC CVPR IEEE, P14648, DOI 10.1109/CVPR52688.2022.01426
   Weinland D, 2010, LECT NOTES COMPUT SC, V6313, P635
   Wong S.F., 2007, P IEEE C COMP VIS PA
   Wongun Choi, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1282, DOI 10.1109/ICCVW.2009.5457461
   Wu D, 2017, IEEE IJCNN, P2865, DOI 10.1109/IJCNN.2017.7966210
   Wu DG, 2016, NEUROCOMPUTING, V190, P35, DOI 10.1016/j.neucom.2015.11.095
   Wu XX, 2011, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2011.5995624
   Xiao FY, 2020, Arxiv, DOI arXiv:2001.08740
   Xiao M, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/9066215
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xiong H., 2022, ACM Trans. Knowl. Discov. Data, V16, P1
   Xu X, 2015, IEEE IMAGE PROC, P63, DOI 10.1109/ICIP.2015.7350760
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang CY, 2020, PROC CVPR IEEE, P588, DOI 10.1109/CVPR42600.2020.00067
   Yang JW, 2022, PROC CVPR IEEE, P14043, DOI 10.1109/CVPR52688.2022.01367
   Yeung S, 2018, INT J COMPUT VISION, V126, P375, DOI 10.1007/s11263-017-1013-y
   Yu JQ, 2018, IEEE 1ST CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2018), P418, DOI 10.1109/MIPR.2018.00090
   Yuan HJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7456, DOI 10.1109/ICCV48922.2021.00738
   Zhan X., 2021, arXiv
   Zhan X., 2021, P 30 INT JOINT C ART, P4679, DOI [10.24963/ijcai.2021/634, DOI 10.24963/IJCAI.2021/634]
   Zhan Xueying, 2022, arXiv
   Zhang CH, 2021, PROC CVPR IEEE, P4484, DOI 10.1109/CVPR46437.2021.00446
   Zhang H., 2020, P EUR C COMP VIS, P525
   Zhang J, 2016, PATTERN RECOGN, V60, P86, DOI 10.1016/j.patcog.2016.05.019
   Zhang Q, 2022, LECT NOTES COMPUT SC, V13669, P227, DOI 10.1007/978-3-031-20077-9_14
   Zhang Q, 2022, INT J COMPUT VISION, V130, P3123, DOI 10.1007/s11263-022-01685-7
   Zhang S., 2022, arXiv
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang X, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7940, DOI 10.1109/ICCV48922.2021.00786
   Zhang YY, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13557, DOI [10.1109/iccv48922.2021.01332, 10.1109/ICCV48922.2021.01332]
   Zhang Y, 2023, Arxiv, DOI [arXiv:2110.04596, 10.48550/arXiv.2110.04596]
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, P5479, DOI 10.1109/TIP.2016.2605305
   Zhang ZQ, 2021, PROC CVPR IEEE, P9832, DOI 10.1109/CVPR46437.2021.00971
   Zhao K, 2019, Arxiv, DOI arXiv:1907.01221
   Zhao T., 2020, Peng Cheng Lab. Commumications, V1, P107
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zhou PP, 2017, J PHYS CONF SER, V844, DOI 10.1088/1742-6596/844/1/012044
   Zhu K, 2022, IEEE COMPUT SOC CONF, P3588, DOI 10.1109/CVPRW56347.2022.00403
   Zhu Y, 2020, Arxiv, DOI [arXiv:2012.06567, 10.48550/arXiv.2012.06567]
NR 263
TC 11
Z9 13
U1 21
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7943
EP 7966
DI 10.1109/TMM.2022.3232034
PG 24
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400026
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Xie, WC
   Lu, WY
   Peng, ZB
   Shen, LL
AF Xie, Weicheng
   Lu, Wenya
   Peng, Zhibin
   Shen, Linlin
TI Consistency Preservation and Feature Entropy Regularization for GAN
   Based Face Editing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Generative adversarial networks; Semantics; Facial features;
   Training; Faces; Task analysis; GAN; Consistency preservation; Entropy
   regularization; Self-adaptive dropout
ID NETWORKS; DROPOUT
AB Generative Adversarial Network (GAN) has been widely used for image-to-image translation-based facial attribute editing. Existing GAN networks are likely to generate samples with anomalies, which may be caused by the lack of consistency preservation and feature entanglement. For preserving image consistency, many studies resorted to the design of the network framework and loss functions, e.g. cycle-consistency loss. However, the generator with the cycle-consistency loss could not well preserve the attribute-irrelevant features, and its feature-level noises may possibly cause synthesis abnormalities. For feature disentanglement, previous works were devoted to mining the implicit semantics of feature spaces, while these semantics are not stable and intuitive enough. For consistency preservation, we propose a target consistency loss to complement the cycle-consistency loss, and enable the network to learn to preserve features of the image more directly. Meanwhile, we filter out outlier feature maps to reduce the synthesis abnormalities and propose a dynamic dropout to better preserve the attribute-irrelevant features. For feature disentanglement, we encode the image semantics more stably and intuitively and propose an entropy regularization to decouple these semantics to allow independent editing of different attributes. The proposed modules are general and can be easily integrated with available image-to-image-based GAN models like StarGAN, AttGAN, and STGAN. Extensive experiments on CelebA dataset show that the our strategy can largely reduce the artifacts and better preserve the subtle facial features, and thus significantly improve the facial editing performance of these mainstream GAN models, in terms of FID, PSNR and SSIM. Additional experiments on realistic expression editing show that our method outperforms StarGAN on RaFD, and achieves much better generalization performances than the three baselines on datasets of FFHQ, RaFD and LFW.
C1 [Xie, Weicheng; Lu, Wenya; Peng, Zhibin; Shen, Linlin] Shenzhen Univ, Comp Vis Inst, Shenzhen Inst Artificial Intelligence & Robot Soc, Sch Comp Sci & Software Engn,Guangdong Key Lab Int, Shenzhen 518060, Peoples R China.
   [Xie, Weicheng] Univ Nottingham, Sch Comp Sci, Nottingham NG81BB, England.
C3 Shenzhen Institute of Artificial Intelligence & Robotics for Society;
   Shenzhen University; University of Nottingham
RP Shen, LL (corresponding author), Shenzhen Univ, Comp Vis Inst, Shenzhen Inst Artificial Intelligence & Robot Soc, Sch Comp Sci & Software Engn,Guangdong Key Lab Int, Shenzhen 518060, Peoples R China.
EM wcxie@szu.edu.cn; 2060271052@email.szu.edu.cn;
   2100271055@email.szu.edu.cn; llshen@szu.edu.cn
OI Peng, Zhibin/0009-0001-7026-5421
FU National Natural Science Foundation of China
FX No Statement Available
CR Arantes Renato B., 2020, Advances in Visual Computing. 15th International Symposium, ISVC 2020. Proceedings. Lecture Notes in Computer Science (LNCS 12509), P170, DOI 10.1007/978-3-030-64556-4_14
   Chen TY, 2022, IEEE T MULTIMEDIA, V24, P2975, DOI 10.1109/TMM.2021.3091859
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dorta G, 2020, PROC CVPR IEEE, P5355, DOI 10.1109/CVPR42600.2020.00540
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fu H, 2019, PROC CVPR IEEE, P2422, DOI [10.1109/CVPR.2019.00253, 10.1109/cvpr.2019.00253]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He Z., 2021, P IEEE CVF INT C COM, P14408
   He ZL, 2020, Arxiv, DOI arXiv:2007.05892
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Hou XX, 2023, IEEE T MULTIMEDIA, V25, P3125, DOI 10.1109/TMM.2022.3155903
   Hou XX, 2022, NEURAL NETWORKS, V145, P209, DOI 10.1016/j.neunet.2021.10.017
   Huang G. B., 2008, P EUR C COMP VIS WOR
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jeong-gi Kwak, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P524, DOI 10.1007/978-3-030-58568-6_31
   Jinhyung Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12100, DOI 10.1109/CVPR42600.2020.01212
   Kang GL, 2018, IEEE T PATTERN ANAL, V40, P1245, DOI 10.1109/TPAMI.2017.2701831
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kim D, 2021, PROC CVPR IEEE, P6505, DOI 10.1109/CVPR46437.2021.00644
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li Z., 2022, ACM Comput. Surv, V55, P1
   Liu AH, 2018, ADV NEUR IN, V31
   Liu BC, 2020, AAAI CONF ARTIF INTE, V34, P4836
   Liu HZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P468, DOI 10.1109/ICCV48922.2021.00053
   Liu KL, 2019, IEEE I CONF COMP VIS, P6391, DOI 10.1109/ICCV.2019.00648
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Ma FR, 2022, IEEE T CIRC SYST VID, V32, P730, DOI 10.1109/TCSVT.2021.3064035
   Odena A, 2017, PR MACH LEARN RES, V70
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peebles William, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P581, DOI 10.1007/978-3-030-58539-6_35
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang YH, 2020, AAAI CONF ARTIF INTE, V34, P5964
   Van Hulle MM, 2012, Handbook of Natural Computing, V1, P585, DOI [10.1007/978-3-540-92910-919, DOI 10.1007/978-3-540-92910-919, 10.1007/978-3-540-92910-9_19]
   Wang XS, 2023, IEEE T MULTIMEDIA, V25, P4081, DOI 10.1109/TMM.2022.3171084
   Wang Y, 2020, PROC CVPR IEEE, P5093, DOI 10.1109/CVPR42600.2020.00514
   Wei Xiang, 2018, INT C LEARN REPR ICL
   Wei Yuxiang, 2021, P IEEE INT C COMP VI, P6721
   Xiao TH, 2018, LECT NOTES COMPUT SC, V11214, P172, DOI 10.1007/978-3-030-01249-6_11
   Yang GX, 2021, PROC CVPR IEEE, P2950, DOI 10.1109/CVPR46437.2021.00297
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang Huan, 2020, P INT C LEARN REPR
   Zhang ZY, 2022, IEEE T MULTIMEDIA, V24, P677, DOI 10.1109/TMM.2021.3057989
   Zhao ZL, 2021, AAAI CONF ARTIF INTE, V35, P11033
   Zheng ZQ, 2023, IEEE T MULTIMEDIA, V25, P2474, DOI 10.1109/TMM.2022.3147425
   Zhiwei Wen, 2020, 2020 25th International Conference on Pattern Recognition (ICPR), P3767, DOI 10.1109/ICPR48806.2021.9412434
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 54
TC 0
Z9 0
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8892
EP 8905
DI 10.1109/TMM.2023.3289757
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000063
DA 2024-07-18
ER

PT J
AU Yang, P
   Luo, X
   Sun, JK
AF Yang, Pan
   Luo, Xiong
   Sun, Jiankun
TI A Simple but Effective Method for Balancing Detection and
   Re-Identification in Multi-Object Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-object tracking (MOT); joint detection and embedding (JDE);
   feature conflict; MOT challenges
AB In recent years, joint detection and embedding (JDE) has become the research focus in multi-object tracking (MOT) due to its fast inference speed. JDE models are designed and widely utilized to train the detection task and the re-identification (Re-ID) task jointly. However, there exists a severe issue overlooked by previous JDE models, i.e., the detection task requires category-level features but the Re-ID task requires instance-level features. This could lead to feature conflict, which would hurt the performance of JDE models. Furthermore, inaccurate detection results can degrade the final tracking accuracy even when discriminative Re-ID features are provided. In this article, we propose a new balancing method for training JDE models, which monitors the training process of the detection task and adjusts the weights of the detection task and Re-ID task in the training phase. Our proposed balancing method ensures a well-trained detection model and a good trade-off between the detection task and Re-ID task. Comprehensive experiments on two public MOT benchmarks demonstrate the effectiveness and superiority of our proposed balancing method. In particular, our proposed balancing method could achieve new state-of-the-art results on MOT challenges without additional training data.
C1 [Yang, Pan; Luo, Xiong; Sun, Jiankun] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Yang, Pan; Luo, Xiong; Sun, Jiankun] Univ Sci & Technol Beijing, Shunde Innovat Sch, Foshan 528399, Peoples R China.
   [Yang, Pan; Luo, Xiong; Sun, Jiankun] Beijing Key Lab Knowledge Engn Mat Sci, Beijing 100083, Peoples R China.
C3 University of Science & Technology Beijing; University of Science &
   Technology Beijing
RP Luo, X (corresponding author), Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
EM g20208845@xs.ustb.edu.cn; xluo@ustb.edu.cn; b20170330@xs.ustb.edu.cn
RI Luo, Xiong/P-4343-2016
OI Luo, Xiong/0000-0002-1929-8447
FU Beijing Natural Science Foundation [19L2029, L211020, M21032]; National
   Natural Science Foundation of China [U1836106]; Scientific and
   Technological Innovation Foundation of Foshan [BK21BF001, BK20BF010]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grants 19L2029, L211020 and M21032, in part by the National
   Natural Science Foundation of China under Grant U1836106, and in part by
   the Scientific and Technological Innovation Foundation of Foshan under
   Grants BK21BF001 and BK20BF010. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Song Wang.
CR Bernardin K, 2008, EURASIP J IMAGE VIDE, DOI 10.1155/2008/246309
   Bewley A, 2016, IEEE IMAGE PROC, P3464, DOI 10.1109/ICIP.2016.7533003
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Fang K, 2018, IEEE WINT CONF APPL, P466, DOI 10.1109/WACV.2018.00057
   Ghazalian R, 2021, IEEE T MULTIMEDIA, V23, P823, DOI 10.1109/TMM.2020.2990077
   Jinlong Peng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P145, DOI 10.1007/978-3-030-58548-8_9
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kingma D. P., 2014, arXiv
   Lee B, 2016, LECT NOTES COMPUT SC, V9914, P68, DOI 10.1007/978-3-319-48881-3_6
   Li MH, 2021, IEEE T MULTIMEDIA, V23, P105, DOI 10.1109/TMM.2020.2978623
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Ma JQ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1930, DOI 10.1145/3219819.3220007
   Milan A, 2016, Arxiv, DOI arXiv:1603.00831
   Pang B, 2020, PROC CVPR IEEE, P6307, DOI 10.1109/CVPR42600.2020.00634
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sener O, 2018, ADV NEUR IN, V31
   Shen JB, 2019, IEEE T CYBERNETICS, V49, P1990, DOI 10.1109/TCYB.2018.2803217
   Shen JB, 2018, IEEE T INTELL TRANSP, V19, P162, DOI 10.1109/TITS.2017.2750082
   Sun SJ, 2021, IEEE T PATTERN ANAL, V43, P104, DOI 10.1109/TPAMI.2019.2929520
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Voigtlaender P, 2019, PROC CVPR IEEE, P7934, DOI 10.1109/CVPR.2019.00813
   Wan XY, 2018, IEEE IMAGE PROC, P788, DOI 10.1109/ICIP.2018.8451174
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wojke N, 2017, IEEE IMAGE PROC, P3645, DOI 10.1109/ICIP.2017.8296962
   Wu JL, 2021, PROC CVPR IEEE, P12347, DOI 10.1109/CVPR46437.2021.01217
   Xingyi Zhou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P474, DOI 10.1007/978-3-030-58548-8_28
   Yin JB, 2020, PROC CVPR IEEE, P6767, DOI 10.1109/CVPR42600.2020.00680
   Yu FW, 2016, LECT NOTES COMPUT SC, V9914, P36, DOI 10.1007/978-3-319-48881-3_3
   Zhang YF, 2021, INT J COMPUT VISION, V129, P3069, DOI 10.1007/s11263-021-01513-4
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P107, DOI 10.1007/978-3-030-58621-8_7
   Zhou ZW, 2018, INT C PATT RECOG, P1809, DOI 10.1109/ICPR.2018.8545450
NR 35
TC 4
Z9 4
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7456
EP 7468
DI 10.1109/TMM.2022.3222614
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000053
DA 2024-07-18
ER

PT J
AU Yao, MD
   He, DL
   Li, X
   Pan, ZH
   Xiong, ZW
AF Yao, Mingde
   He, Dongliang
   Li, Xin
   Pan, Zhihong
   Xiong, Zhiwei
TI Bidirectional Translation Between UHD-HDR and HD-SDR Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Task analysis; Neural networks; Spatial resolution;
   Visualization; Standards; Image color analysis; High dynamic range;
   inverse tone mapping; invertible neural network; super resolution; ultra
   high definition
ID IMAGE; DECOMPOSITION
AB With the popularization of ultra high definition (UHD) high dynamic range (HDR) displays, recent works focus on upgrading high definition (HD) standard dynamic range (SDR) videos to UHD-HDR versions, aiming to provides richer details and higher contrasts on advanced modern displays. However, joint considering the upgrading & downgrading translations between two types of videos, which is practical in real applications, is generally neglected. On the one hand, downgrading translation is the key to showing UHD-HDR videos on HD-SDR displays. On the other hand, considering both translations enables joint optimization and results in high quality translation. To this end, we propose the bidirectional translation network (BiT-Net), which jointly considers two translations in one network for the first time. In brief, BiT-Net is elaborately designed in an invertible fashion that can be efficiently inferred along forward and backward directions for downgrading and upgrading tasks, respectively. Based on this framework, we divide each direction into three sub-tasks, i.e., decomposition, structure-guided translation, and synthesis, to effectively translate the dynamic range and the high-frequency details. Benefiting from the dedicated architecture, our BiT-Net can work on 1) downgrading UHD-HDR videos, 2) upgrading existing HD-SDR videos, and 3) synthesizing UHD-HDR versions from the downgraded HD-SDR videos. Experiments show that the proposed method achieves state-of-the-art performances on all these three tasks.
C1 [Yao, Mingde; Xiong, Zhiwei] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
   [He, Dongliang] ByteDance Inc, Beijing 100010, Peoples R China.
   [Li, Xin] Baidu Inc, Dept Comp Vis Technol VIS, Beijing 100085, Peoples R China.
   [Pan, Zhihong] Zeku, Palo Alto, CA 94303 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Baidu
RP Xiong, ZW (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Peoples R China.
EM mdyao@mail.ustc.edu.cn; hedlcc@126.com; lixin41@baidu.com;
   zhihongp@gmail.com; zwxiong@ustc.edu.cn
OI Yao, Mingde/0000-0001-5994-6288
FU National Natural Science Foundation of China
FX No Statement Available
CR Akyüz AG, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1276377.1276425
   [Anonymous], 2018, P EUROPEAN C COMPUTE
   [Anonymous], 2017, Advanced high dynamic range imaging
   Ben Niu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P191, DOI 10.1007/978-3-030-58610-2_12
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Chan KCK, 2021, PROC CVPR IEEE, P14240, DOI 10.1109/CVPR46437.2021.01402
   Chen XY, 2022, Arxiv, DOI arXiv:2205.04437
   Chen XY, 2021, IEEE COMPUT SOC CONF, P354, DOI 10.1109/CVPRW53098.2021.00045
   Chen Xiangyu, 2021, P IEEECVF INT C COMP, P4500
   Chen XL, 2020, Arxiv, DOI arXiv:2003.04297
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dinh L, 2015, Arxiv, DOI [arXiv:1410.8516, 10.48550/arXiv.1410.8516]
   Dinh L, 2017, Arxiv, DOI arXiv:1605.08803
   Eilertsen G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130816
   Endo Y, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3130800.3130834
   Gu B, 2013, IEEE T IMAGE PROCESS, V22, P70, DOI 10.1109/TIP.2012.2214047
   Huang YC, 2021, PROC CVPR IEEE, P3526, DOI 10.1109/CVPR46437.2021.00353
   Huo YQ, 2014, VISUAL COMPUT, V30, P507, DOI 10.1007/s00371-013-0875-4
   ITU-R, 2015, Tech. Rep., ITU-R Rec, BT.2020-2
   ITU-R, 2018, Tech. Rep., ITU-R Rec, BT.2100-2
   Kim JH, 2021, AAAI CONF ARTIF INTE, V35, P1780
   Kim SY, 2020, AAAI CONF ARTIF INTE, V34, P11287
   Kim SY, 2019, IEEE I CONF COMP VIS, P3116, DOI 10.1109/ICCV.2019.00321
   Kim Soo Ye, 2018, P AS C COMP VIS, P379
   Kingma DP, 2018, ADV NEUR IN, V31
   Kong XT, 2021, PROC CVPR IEEE, P12011, DOI 10.1109/CVPR46437.2021.01184
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Lee S, 2018, LECT NOTES COMPUT SC, V11206, P613, DOI 10.1007/978-3-030-01216-8_37
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P1092, DOI 10.1109/TIP.2018.2872876
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Li ZY, 2022, IEEE COMPUT SOC CONF, P832, DOI 10.1109/CVPRW56347.2022.00099
   Liang J, 2021, PROC CVPR IEEE, P9387, DOI 10.1109/CVPR46437.2021.00927
   Liang ZT, 2018, PROC CVPR IEEE, P4758, DOI 10.1109/CVPR.2018.00500
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu Y, 2021, PROC CVPR IEEE, P13360, DOI 10.1109/CVPR46437.2021.01316
   Liu YL, 2020, PROC CVPR IEEE, P1648, DOI 10.1109/CVPR42600.2020.00172
   Lu SP, 2021, PROC CVPR IEEE, P10811, DOI 10.1109/CVPR46437.2021.01067
   Ma KD, 2015, IEEE T IMAGE PROCESS, V24, P3086, DOI [10.1109/TIP.2015.2436340, 10.1109/TIP.2015.2456638]
   Mai ZC, 2011, IEEE T IMAGE PROCESS, V20, P1558, DOI 10.1109/TIP.2010.2095866
   Mantiuk R, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964935
   Mantiuk R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360667
   Meylan L, 2006, IEEE T IMAGE PROCESS, V15, P2820, DOI 10.1109/TIP.2006.877312
   Mingqing Xiao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P126, DOI 10.1007/978-3-030-58452-8_8
   Motra A, 2010, IEEE IMAGE PROC, P2061, DOI 10.1109/ICIP.2010.5654069
   Preiss J, 2014, IEEE T IMAGE PROCESS, V23, DOI 10.1109/TIP.2014.2302684
   Rana A, 2020, IEEE T IMAGE PROCESS, V29, P1285, DOI 10.1109/TIP.2019.2936649
   Reinhard E, 2005, IEEE T VIS COMPUT GR, V11, P13, DOI 10.1109/TVCG.2005.9
   Santos MS, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3386569.3392403
   Schweiger Florian, 2017, Motion Imaging Journal, V126, P45, DOI 10.5594/JMI.2017.2660698
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Sun WJ, 2020, IEEE T IMAGE PROCESS, V29, P4027, DOI 10.1109/TIP.2020.2970248
   TUMBLIN J, 1993, IEEE COMPUT GRAPH, V13, P42, DOI 10.1109/38.252554
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Zamir SW, 2017, IEEE T IMAGE PROCESS, V26, P1595, DOI 10.1109/TIP.2017.2661404
   Xu J, 2020, IEEE T IMAGE PROCESS, V29, P5022, DOI 10.1109/TIP.2020.2974060
   Zamir SW, 2021, IEEE T PATTERN ANAL, V43, P1777, DOI 10.1109/TPAMI.2019.2938499
   Zamir SW, 2014, IEEE J-STSP, V8, P490, DOI 10.1109/JSTSP.2014.2313182
   Zhang SF, 2021, PROC CVPR IEEE, P620, DOI 10.1109/CVPR46437.2021.00068
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao L., 2021, P IEEE CVF INT C COM, P12075
   Zhou L, 2022, Arxiv, DOI arXiv:2210.05960
NR 64
TC 2
Z9 2
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8672
EP 8686
DI 10.1109/TMM.2023.3239656
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000035
DA 2024-07-18
ER

PT J
AU Yu, Z
   Jin, ZT
   Yu, J
   Xu, ML
   Wang, HB
   Fan, JP
AF Yu, Zhou
   Jin, Zitian
   Yu, Jun
   Xu, Mingliang
   Wang, Hongbo
   Fan, Jianping
TI Bilaterally Slimmable Transformer for Elastic and Efficient Visual
   Question Answering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Transformers; Adaptation models; Computational modeling; Visualization;
   Task analysis; Training; Neural networks; Visual question answering;
   slimmable network; transformer; multimodal learning; efficient deep
   learning
ID LANGUAGE
AB Recent Transformer architectures (Vaswani et al., 2017) have brought remarkable improvements to visual question answering (VQA). Nevertheless, Transformer-based VQA models are usually deep and wide to guarantee good performance, so they can only run on powerful GPU servers and cannot run on capacity-restricted platforms such as mobile phones. Therefore, it is desirable to learn an elastic VQA model that supports adaptive pruning at runtime to meet the efficiency constraints of different platforms. To this end, we present the bilaterally slimmable Transformer (BST), a general framework that can be seamlessly integrated into arbitrary Transformer-based VQA models to train a single model once and obtain various slimmed submodels of different widths and depths. To verify the effectiveness and generality of this method, we integrate the proposed BST framework with three typical Transformer-based VQA approaches, namely MCAN (Yu et al., 2019), UNITER (Chen et al., 2020), and CLIP-ViL (Shen et al., 2021), and conduct extensive experiments on two commonly-used benchmark datasets. In particular, one slimmed MCAN(BST) submodel achieves comparable accuracy on VQA-v2, while being 0.38x smaller in model size and having 0.27x fewer FLOPs than the reference MCAN model. The smallest MCAN(BST) submodel only has 9 M parameters and 0.16 G FLOPs during inference, making it possible to deploy it on a mobile device with less than 60 ms latency.
C1 [Yu, Zhou; Yu, Jun; Wang, Hongbo] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Jin, Zitian] Hangzhou Dianzi Univ, HDU ITMO Joint Inst, Hangzhou 310018, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450001, Peoples R China.
   [Fan, Jianping] Lenovo Res, AI Lab, Beijing 100094, Peoples R China.
C3 Hangzhou Dianzi University; Hangzhou Dianzi University; Zhengzhou
   University; Legend Holdings; Lenovo
RP Yu, J; Wang, HB (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
EM yuz@hdu.edu.cn; jinzt@hdu.edu.cn; yujun@hdu.edu.cn;
   iexumingliang@zzu.edu.cn; whongbo@hdu.edu.cn; jfan1@lenovo.com
OI Fan, Jianping/0000-0003-2290-1785; Yu, Zhou/0000-0001-8407-1137
FU Zhejiang Provincial Natural Science Foundation of China
FX No Statement Available
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Cai Han, 2019, INT C LEARN REPR
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   Cui BY, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P3548
   Cui YH, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P797, DOI 10.1145/3474085.3475251
   Dehghani M., 2018, ARXIV180703819
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P2371, DOI 10.1109/TMM.2018.2796248
   Dosovitskiy A, 2021, arXiv, DOI [10.48550/ARXIV.2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Dou ZY, 2022, PROC CVPR IEEE, P18145, DOI 10.1109/CVPR52688.2022.01763
   Nguyen DK, 2018, PROC CVPR IEEE, P6087, DOI 10.1109/CVPR.2018.00637
   Fan A., 2019, P INT C LEARN REPR
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao P, 2019, PROC CVPR IEEE, P6632, DOI 10.1109/CVPR.2019.00680
   Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670
   Gupta Rajiv, 2015, P 19 INT C EV ASS SO, P1
   Han  S., 2015, ARXIV151000149
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, COMPUT SCI, V2
   Hou L., 2020, P 34 INT C NEURAL IN, P9782
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686
   Khetan A, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2807
   Kim J. H., 2017, PROC INT C LEARN REP, P1
   Kim JH, 2018, ADV NEUR IN, V31
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Lan Z., 2019, ARXIV190911942, DOI DOI 10.48550/ARXIV.1909.11942
   Lei J, 2021, PROC CVPR IEEE, P7327, DOI 10.1109/CVPR46437.2021.00725
   Li JH, 2021, ADV NEUR IN, V34
   Li JN, 2022, PR MACH LEARN RES
   Li LJ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P2046
   Li X J., 2020, P EUR C COMP VIS, P121, DOI DOI 10.1007/978-3-030-58577-8_8
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu F, 2021, IEEE T MULTIMEDIA, V23, P3518, DOI 10.1109/TMM.2020.3026892
   Liu W., 2020, P 58 ANN M ASS COMPU, P6035, DOI 10.18653/v1/2020.acl-main.537
   Liu Y., 2019, ROBERTA ROBUSTLY OPT
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Ma XD, 2019, ADV NEUR IN, V32
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Qin BS, 2023, IEEE T MULTIMEDIA, V25, P4282, DOI 10.1109/TMM.2022.3173131
   Radford A., 2019, LANGUAGE MODELS ARE
   Radford A, 2021, PR MACH LEARN RES, V139
   Radford Alec, 2018, IMPROVING LANGUAGE U, DOI DOI 10.18653/V1/N18-1202
   Raffel C, 2020, J MACH LEARN RES, V21
   Sanh V, 2020, Arxiv, DOI arXiv:1910.01108
   Shen S., 2021, P INT C LEARN REPR, P1
   Su Weijie, 2020, INT C LEARN REPR
   Sun MY, 2023, IEEE T MULTIMEDIA, V25, P2446, DOI 10.1109/TMM.2022.3147385
   Sun ZQ, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2158
   Tan H, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5100
   Tang RP, 2019, Arxiv, DOI arXiv:1903.12136
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang Hanrui, 2020, ARXIV200514187, P7675
   Wang P, 2022, 39 INT C MACHINE LEA
   Wang W., 2020, P 28 INT C COMP LING, P6019
   Wang Wei, 2020, Advances in Neural Information Processing Systems
   Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Yen-Chun Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12375), P104, DOI 10.1007/978-3-030-58577-8_7
   Yu F, 2021, AAAI CONF ARTIF INTE, V35, P3208
   Yu J., 2018, PROC INT C LEARN REP, P1
   Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu J, 2020, IEEE T CIRC SYST VID, V30, P4467, DOI 10.1109/TCSVT.2019.2947482
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu Z, 2019, Arxiv, DOI arXiv:1908.04107
   Yu Z, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3743, DOI 10.1145/3394171.3413977
   Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340
   Yu Z, 2019, PROC CVPR IEEE, P6274, DOI 10.1109/CVPR.2019.00644
   Yu Z, 2017, IEEE I CONF COMP VIS, P1839, DOI 10.1109/ICCV.2017.202
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhu L., 2020, P IEEE CVF C COMP VI, DOI 10.1109/CVPR42600.2020.00877
NR 79
TC 1
Z9 1
U1 6
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9543
EP 9556
DI 10.1109/TMM.2023.3254205
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200034
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, J
   Jiao, LC
   Ma, WP
   Liu, F
   Liu, X
   Li, LL
   Chen, PH
   Yang, SY
AF Zhang, Jun
   Jiao, Licheng
   Ma, Wenping
   Liu, Fang
   Liu, Xu
   Li, Lingling
   Chen, Puhua
   Yang, Shuyuan
TI Transformer Based Conditional GAN for Multimodal Image Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image fusion; Generators; Training; Feature extraction; Transformers;
   Generative adversarial networks; Thermal sensors; Generative adversarial
   network; multimodal image fusion; transformer
ID GENERATIVE ADVERSARIAL NETWORK; MULTISCALE; FRAMEWORK; DEEP
AB Multimodal Image fusion is becoming urgent in multi-sensor information utilization. However, existing end-to-end image fusion frameworks ignore a priori knowledge integration and long-distance dependencies across domains, which brings challenges to the network convergence and global image perception in complex scenes. In this article, a conditional generative adversarial network with transformer (TCGAN) is proposed for multimodal image fusion. The generator is to generate a fused image with the source images content. The discriminators are adopted to distinguish the differences between the fused image and the source images. Adversarial training makes the final fused image to maintain the structural and textural details in the cross-modal images simultaneously. In particular, a wavelet fusion module makes the inputs contain image content from different domains as much as possible. The extracted convolutional features interact in the multiscale cross-modal transformer fusion module to fully complement the associated information. It makes the generator to focus on both local and global context. TCGAN fully considers the training efficiency of the adversarial process and the integrated retention of redundant information. Various experimental results of TCGAN have highlighted targets, rich details, and fast convergence properties on public datasets.
C1 [Zhang, Jun; Jiao, Licheng; Ma, Wenping; Liu, Fang; Liu, Xu; Li, Lingling; Chen, Puhua; Yang, Shuyuan] Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Int Res Ctr Intelligent Percept & Computat,Minist, Sch Artificial Intelligence,Key Lab Intelligent Pe, Xian 710071, Peoples R China.
C3 Xidian University
RP Jiao, LC (corresponding author), Xidian Univ, Joint Int Res Lab Intelligent Percept & Computat, Int Res Ctr Intelligent Percept & Computat,Minist, Sch Artificial Intelligence,Key Lab Intelligent Pe, Xian 710071, Peoples R China.
EM zhangjun_self@163.com; lchjiao@mail.xidian.edu.cn;
   wpma@mail.xidian.edu.cn; f63liu@163.com; xuliu361@163.com;
   llli@xidian.edu.cn; phchen@xidian.edu.cn; syyang@xidian.edu.cn
FU Ministry of Education
FX No Statement Available
CR Amolins K, 2007, ISPRS J PHOTOGRAMM, V62, P249, DOI 10.1016/j.isprsjprs.2007.05.009
   Azam MA, 2022, COMPUT BIOL MED, V144, DOI 10.1016/j.compbiomed.2022.105253
   Bhat S, 2021, APPL SOFT COMPUT, V106, DOI 10.1016/j.asoc.2021.107307
   Chen J, 2021, SIGNAL PROCESS, V182, DOI 10.1016/j.sigpro.2020.107936
   Diao WX, 2023, IEEE T NEUR NET LEAR, V34, P8195, DOI 10.1109/TNNLS.2021.3137373
   Dosovitskiy A, 2021, Arxiv, DOI arXiv:2010.11929
   Han K, 2023, IEEE T PATTERN ANAL, V45, P87, DOI 10.1109/TPAMI.2022.3152247
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Li GF, 2021, INFORM FUSION, V71, P109, DOI 10.1016/j.inffus.2021.02.008
   Li H, 2021, INFORM FUSION, V73, P72, DOI 10.1016/j.inffus.2021.02.023
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li JX, 2020, IEEE T IMAGE PROCESS, V29, P4816, DOI 10.1109/TIP.2020.2976190
   Li JW, 2022, COMPLEX INTELL SYST, V8, P4753, DOI 10.1007/s40747-022-00722-9
   Li QL, 2021, IEEE SENS J, V21, P7458, DOI 10.1109/JSEN.2019.2921803
   Li X, 2020, IEEE ACCESS, V8, P100578, DOI 10.1109/ACCESS.2020.2995541
   Liang JY, 2021, IEEE INT CONF COMP V, P1833, DOI 10.1109/ICCVW54120.2021.00210
   Liu J., 2022, P IEEECVF C COMPUTER, P5802
   Liu Y, 2020, INFORM FUSION, V64, P71, DOI 10.1016/j.inffus.2020.06.013
   Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9
   Ma JY, 2022, IEEE-CAA J AUTOMATIC, V9, P1200, DOI 10.1109/JAS.2022.105686
   Ma JY, 2020, IEEE T IMAGE PROCESS, V29, P4980, DOI 10.1109/TIP.2020.2977573
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma WP, 2019, IEEE T GEOSCI REMOTE, V57, P4834, DOI 10.1109/TGRS.2019.2893310
   Mao QY, 2022, MULTIMED TOOLS APPL, V81, P12305, DOI 10.1007/s11042-021-11278-0
   Masood S., 2017, J. Eng. Sci. Technol. Rev., V10, P186, DOI [10.25103/jestr.106.24, DOI 10.25103/JESTR.106.24]
   Meher B, 2019, INFORM FUSION, V48, P119, DOI 10.1016/j.inffus.2018.07.010
   Moorthy S, 2021, INFORM SCIENCES, V546, P996, DOI 10.1016/j.ins.2020.09.060
   Pajares G, 2004, PATTERN RECOGN, V37, P1855, DOI 10.1016/j.patcog.2004.03.010
   Paramanandham N, 2018, MULTIMED TOOLS APPL, V77, P12405, DOI 10.1007/s11042-017-4895-3
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qiang Zhang, 2021, Pattern Recognition, V113, DOI 10.1016/j.patcog.2020.107752
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Shreyamsha Kumar BK, 2013, SIGNAL IMAGE VIDEO P, V7, P1125, DOI 10.1007/s11760-012-0361-x
   Tan ZY, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3050551
   Tang LF, 2022, INFORM FUSION, V82, P28, DOI 10.1016/j.inffus.2021.12.004
   Tang XY, 2021, INFORM SCIENCES, V565, P326, DOI 10.1016/j.ins.2021.02.004
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang ZS, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2021.3139654
   Wei Q, 2015, IEEE T GEOSCI REMOTE, V53, P3658, DOI 10.1109/TGRS.2014.2381272
   Xu H, 2021, IEEE T COMPUT IMAG, V7, P824, DOI 10.1109/TCI.2021.3100986
   Xu H, 2022, IEEE T PATTERN ANAL, V44, P502, DOI 10.1109/TPAMI.2020.3012548
   Xu HH, 2021, IEEE J-STARS, V14, P8823, DOI 10.1109/JSTARS.2021.3108233
   Xu YD, 2021, INFORM SCIENCES, V548, P378, DOI 10.1016/j.ins.2020.09.066
   Yan H, 2022, IEEE T COMPUT IMAG, V8, P162, DOI 10.1109/TCI.2022.3151472
   Yan HB, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P597, DOI [10.1109/itnec.2019.8729143, 10.1109/ITNEC.2019.8729143]
   Yu X, 2021, INFORM SCIENCES, V568, P350, DOI 10.1016/j.ins.2021.03.059
   Zhang J., 2021, IEEE Trans. Neural Netw, Learn. Syst., DOI [10.1109/TNNLS2021.3130655, DOI 10.1109/TNNLS2021.3130655]
   Zhang J, 2019, IEEE GEOSCI REMOTE S, V16, P1210, DOI 10.1109/LGRS.2019.2896341
   Zhang K, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2022.3187025
   Zhang K, 2022, IEEE GEOSCI REMOTE S, V19, DOI 10.1109/LGRS.2022.3186985
   Zhang Y, 2020, INFORM FUSION, V54, P99, DOI 10.1016/j.inffus.2019.07.011
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
NR 54
TC 8
Z9 9
U1 23
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8988
EP 9001
DI 10.1109/TMM.2023.3243659
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000002
DA 2024-07-18
ER

PT J
AU Zhang, M
   Zhou, Y
   Liu, B
   Zhao, JQ
   Yao, R
   Shao, ZW
   Zhu, HC
AF Zhang, Man
   Zhou, Yong
   Liu, Bing
   Zhao, Jiaqi
   Yao, Rui
   Shao, Zhiwen
   Zhu, Hancheng
TI Weakly Supervised Few-Shot Semantic Segmentation via Pseudo Mask
   Enhancement and Meta Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; meta learning; weakly-supervised segmentation;
   transformer.
ID IMAGE
AB Few shot semantic segmentation has been proposed to enhance the generalization ability of traditional models with limited data. Previous works mainly focus on the supervised tasks, while limited amount of work is explored for the weakly supervised tasks. Weakly supervised semantic segmentation has become an active research area because weakly supervised labels effectively reduce the annotation cost of visual tasks. To this end, we propose a weakly supervised few-shot semantic segmentation model based on the meta learning framework, which utilizes prior knowledge and adjusts itself according to new tasks. Thereupon then, the proposed network is capable of both high efficiency and generalization ability to new tasks. In the pseudo mask generation stage, we develop a WRCAM method with the channel-spatial attention mechanism to refine the coverage size of targets in pseudo masks. In the few-shot semantic segmentation stage, the optimization based meta learning method is used to realize few-shot semantic segmentation by virtue of the refined pseudo masks. The experimental results show that the proposed method not only significantly outperforms weakly supervised SOTA methods, but also could be comparative to some supervised SOTA methods.
C1 [Zhang, Man; Zhou, Yong; Liu, Bing; Zhao, Jiaqi; Yao, Rui; Shao, Zhiwen; Zhu, Hancheng] China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.
   [Zhang, Man; Zhou, Yong; Liu, Bing; Zhao, Jiaqi; Yao, Rui; Shao, Zhiwen; Zhu, Hancheng] Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.
C3 China University of Mining & Technology
RP Zhou, Y (corresponding author), China Univ Min & Technol, Sch Comp Sci & Technol, Xuzhou 221116, Jiangsu, Peoples R China.; Zhou, Y (corresponding author), Minist Educ Peoples Republ China, Engn Res Ctr Mine Digitizat, Xuzhou 221116, Jiangsu, Peoples R China.
EM ts17170006a3@cumt.edu.cn; yzhou@cumt.edu.cn; liubing@cumt.edu.cn;
   jiaqizhao@cumt.edu.cn; ruiyao@cumt.edu.cn; zhiwen_shao@cumt.edu.cn;
   zhuhancheng@cumt.edu.cn
RI Shao, Zhiwen/N-8985-2018
OI Liu, Bing/0000-0002-2365-6606; Yao, Rui/0000-0003-2734-915X
FU National Natural Science Foundation of China [62106268]; Postgraduate
   Research & Practice Innovation Program of Jiangsu Province
   [KYCX22_2566]; Graduate Innovation Program of China University of Mining
   and Technology [2022WLKXJ116]; High-Level Talent Program for Innovation
   and Entrepreneurship (ShuangChuang Doctor) of Jiangsu Province
   [JSSCBS20211220]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62272461 and 62276266, in part by the
   Postgraduate Research & Practice Innovation Program of Jiangsu Province
   under Grant KYCX22_2566, in part by the Graduate Innovation Program of
   China University of Mining and Technology under Grant 2022WLKXJ116, in
   part by the National Natural Science Foundation of China under Grant
   62106268, and in part by the High-Level Talent Program for Innovation
   and Entrepreneurship (ShuangChuang Doctor) of Jiangsu Province under
   Grant JSSCBS20211220.
CR Abdollahzadeh M., 2021, ADV NEUR IN, V34, P14632
   [Anonymous], 2022, JOURNAL OF MACHINE LEARNING RESEARCH
   Challapalle N, 2020, J SIGNAL PROCESS SYS, V92, P1247, DOI 10.1007/s11265-020-01555-w
   Chen T, 2022, IEEE T MULTIMEDIA, V24, P968, DOI 10.1109/TMM.2021.3061816
   Chen ZZ, 2022, PROC CVPR IEEE, P959, DOI 10.1109/CVPR52688.2022.00104
   Cheng G, 2023, IEEE T PATTERN ANAL, V45, P4650, DOI 10.1109/TPAMI.2022.3193587
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Du Y, 2022, PROC CVPR IEEE, P4310, DOI 10.1109/CVPR52688.2022.00428
   Elsken Thomas, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12362, DOI 10.1109/CVPR42600.2020.01238
   Fan JS, 2020, AAAI CONF ARTIF INTE, V34, P10762
   Finn C, 2017, PR MACH LEARN RES, V70
   Gama PHT, 2023, IEEE T MULTIMEDIA, V25, P1784, DOI 10.1109/TMM.2022.3162951
   Gao N, 2022, PROC CVPR IEEE, P14756, DOI 10.1109/CVPR52688.2022.01436
   Goldblum M., 2020, P INT C MACH LEARN I, P3607
   Gong R, 2021, PROC CVPR IEEE, P8340, DOI 10.1109/CVPR46437.2021.00824
   Haochen Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P730, DOI 10.1007/978-3-030-58601-0_43
   Hou YT, 2021, AAAI CONF ARTIF INTE, V35, P13036
   Kim B, 2021, AAAI CONF ARTIF INTE, V35, P1754
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Lang CB, 2022, PROC CVPR IEEE, P8047, DOI 10.1109/CVPR52688.2022.00789
   Lee J., 2022, IEEE Trans. Pattern Anal. Mach. Intell, DOI [10.48550/arXiv.2204.04890, DOI 10.48550/ARXIV.2204.04890]
   Lee J, 2021, PROC CVPR IEEE, P2643
   Lee Jungbeom, 2021, Comput. Vis. Pattern Recog., Virtual/Online, P4071
   Lee Y.-H., 2022, P IEEECVF WINTER C A, P2170
   Li JL, 2023, IEEE T MULTIMEDIA, V25, P1686, DOI 10.1109/TMM.2022.3152388
   Li XY, 2021, AAAI CONF ARTIF INTE, V35, P1984
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y., 2020, P EUR C COMP VIS ECC, P142, DOI DOI 10.1007/978-3-030-58545-79
   Liu Y, 2020, IEEE INT C INT ROBOT, P8439, DOI 10.1109/IROS45743.2020.9341282
   Liu Yun, 2020, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2020.3023152
   Lu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8721, DOI 10.1109/ICCV48922.2021.00862
   Pan JW, 2022, INT J COMPUT VISION, V130, P1181, DOI 10.1007/s11263-022-01590-z
   Quande Liu, 2020, Medical Image Computing and Computer Assisted Intervention - MICCAI 2020. 23rd International Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12262), P475, DOI 10.1007/978-3-030-59713-9_46
   Ru LX, 2022, INT J COMPUT VISION, V130, P1127, DOI 10.1007/s11263-022-01586-9
   Shaban A., 2017, PROC BRIT MACH VIS C
   Siam M, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P860
   Strudel R, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P7242, DOI 10.1109/ICCV48922.2021.00717
   Sun FD, 2019, PATTERN RECOGN LETT, V120, P62, DOI 10.1016/j.patrec.2019.01.009
   Sun Q., 2022, INT J CONSTR MANAG, V44, P1443, DOI DOI 10.1080/15623599.2020.1866431
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Tian ZT, 2022, IEEE T PATTERN ANAL, V44, P1050, DOI 10.1109/TPAMI.2020.3013717
   Wang KX, 2019, IEEE I CONF COMP VIS, P9196, DOI 10.1109/ICCV.2019.00929
   Wang WG, 2024, IEEE T PATTERN ANAL, V46, P1635, DOI 10.1109/TPAMI.2022.3168530
   Wang X, 2020, INT J COMPUT VISION, V128, P1736, DOI 10.1007/s11263-020-01293-3
   Wu T, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P544
   Yang B., 2020, PROC EUR C COMPUT VI, P1
   Yude Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12272, DOI 10.1109/CVPR42600.2020.01229
   Zhang BF, 2020, AAAI CONF ARTIF INTE, V34, P12765
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
NR 52
TC 4
Z9 4
U1 13
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7980
EP 7991
DI 10.1109/TMM.2022.3232037
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AV4K5
UT WOS:001121212400028
DA 2024-07-18
ER

PT J
AU Zhang, XM
   Chen, YY
   Tang, M
   Wang, JQ
   Zhu, XY
   Lei, Z
AF Zhang, Xiaomei
   Chen, Yingying
   Tang, Ming
   Wang, Jinqiao
   Zhu, Xiangyu
   Lei, Zhen
TI Human Parsing With Part-Aware Relation Modeling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human parsing; modeling; part-aware relation
AB In this paper, a Part-aware Relation Modeling (PRM) is developed to handle the task of human parsing. For pixel-level recognition, it is essential to generate features with adaptive context for various sizes and shapes of human parts. To address the issue, we adaptively capture contexts based on the part-aware relation mechanism. PRM mainly consists of three modules, including a part class module, a part-relation aggregation module, and a part-relation dispersion module. The part class module selectively enhances spatial details of the high-level features to obtain enhanced original features, and then extracts the high-level representations of every human part from a categorical perspective. The part-relation aggregation module is developed to extract the representative global context by exploring associated semantics of human parts, adaptively augmenting the context for human parts. The part-relation dispersion module is designed to generate the discriminative and effective local context and neglect the distracting one by making the affinity of human parts disperse. It ensures that features of the same class will be close to each other and away from those of different classes. By fusing the outputs of the two part-relation modules and the first outputs of the part class module, our PRM produces adaptive contextual features for diverse sizes of human parts, boosting the parsing accuracy. Extensive experiments are conducted to validate the effectiveness of our network, and a new state-of-the-art segmentation performance is achieved on three challenging human parsing datasets, i.e., PASCAL-Person-Part, LIP, and CIHP. PRM is also extended to other tasks like animal parsing, and exhibits its generality.
C1 [Zhang, Xiaomei; Zhu, Xiangyu; Lei, Zhen] Chinese Acad Sci CASIA, Inst Automat, Ctr Biometr & Secur Res CBSR, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
   [Zhang, Xiaomei; Zhu, Xiangyu] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Chen, Yingying; Tang, Ming; Wang, Jinqiao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Lei, Zhen] Chinese Acad Sci, Hong Kong Inst Sci & Innovat, Ctr Artificial Intelligence & Robot, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences
RP Lei, Z (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Ctr Biometr & Secur Res CBSR, Natl Lab Pattern Recognit NLPR, Beijing 100190, Peoples R China.
EM xiaomei.zhang@nlpr.ia.ac.cn; yingying.chen@nlpr.ia.ac.cn;
   tangm@nlpr.ia.ac.cn; jqwang@nlpr.ia.ac.cn; xiangyu.zhu@nlpr.ia.ac.cn;
   zlei@nlpr.ia.ac.cn
RI Chen, Yingying/IQR-4560-2023
OI Zhang, Xiaomei/0000-0002-0221-6978; zhu, xiang yu/0000-0002-4636-9677;
   tang, ming/0000-0003-4976-3095; wang, jin qiao/0000-0002-9118-2780
FU National Key Research and Development Program of China [2020YFC2003901];
   National Natural Science Foundation of China [61976210, 61772527,
   61806200, 61702510, 61876086]; Youth Innovation Promotion Association
   CAS [Y2021131]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFC2003901, in part by the
   National Natural Science Foundation of China under Grants 61976210,
   61772527, 61806200, 61702510, and 61876086, and in part by the Youth
   Innovation Promotion Association CAS under Grant Y2021131. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Dr. Dan Zeng.& nbsp;
CR Bertinetto L., 2016, Advances in neural information processing systems, P523
   Chen LC, 2018, ADV NEUR IN, V31
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   De Brabandere B, 2016, ADV NEUR IN, V29
   Ding HH, 2019, PROC CVPR IEEE, P8877, DOI 10.1109/CVPR.2019.00909
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Fang HS, 2018, PROC CVPR IEEE, P70, DOI 10.1109/CVPR.2018.00015
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fu J, 2019, IEEE I CONF COMP VIS, P6747, DOI 10.1109/ICCV.2019.00685
   Fu J, 2020, PATTERN RECOGN, V101, DOI 10.1016/j.patcog.2019.107152
   Ghiasi G, 2016, LECT NOTES COMPUT SC, V9907, P519, DOI 10.1007/978-3-319-46487-9_32
   Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/cvpr.2019.00763, 10.1109/CVPR.2019.00763]
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Jaderberg M., 2015, ICLR, DOI DOI 10.48550/ARXIV.1506.02025
   Kipf T. N., 2017, 8 INT C LEARN REPR, P1
   Li GH, 2019, IEEE I CONF COMP VIS, P9266, DOI 10.1109/ICCV.2019.00936
   Li Q., 2017, PROC BRIT MACHINE VI, DOI DOI 10.5244/C.31.25
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2017, IEEE T PATTERN ANAL, V39, P115, DOI 10.1109/TPAMI.2016.2537339
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin K, 2021, IEEE T CIRC SYST VID, V31, P1066, DOI 10.1109/TCSVT.2020.2995122
   Liu XC, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P338, DOI 10.1145/3343031.3350857
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luc P., 2016, arXiv
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Nie XC, 2018, PROC CVPR IEEE, P2100, DOI 10.1109/CVPR.2018.00224
   Niepert M, 2016, PR MACH LEARN RES, V48
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Ruyi Ji, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P205, DOI 10.1007/978-3-030-58601-0_13
   Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192
   Su Z, 2021, NEUROCOMPUTING, V465, P437, DOI 10.1016/j.neucom.2021.08.124
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Wang HY, 2015, PROC CVPR IEEE, P1788, DOI 10.1109/CVPR.2015.7298788
   Wang JQ, 2019, IEEE I CONF COMP VIS, P3007, DOI 10.1109/ICCV.2019.00310
   Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3508, DOI 10.1109/TPAMI.2021.3055780
   Wang WG, 2019, IEEE I CONF COMP VIS, P5702, DOI 10.1109/ICCV.2019.00580
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang Y, 2012, J MACH LEARN RES, V13, P3075
   Wei Z, 2017, PROC CVPR IEEE, P3947, DOI 10.1109/CVPR.2017.420
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Xia FT, 2017, PROC CVPR IEEE, P6080, DOI 10.1109/CVPR.2017.644
   Xia FT, 2016, LECT NOTES COMPUT SC, V9909, P648, DOI 10.1007/978-3-319-46454-1_39
   Xiaomei Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P189, DOI 10.1007/978-3-030-58586-0_12
   Xiaomei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8968, DOI 10.1109/CVPR42600.2020.00899
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Yuhui Yuan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P173, DOI 10.1007/978-3-030-58539-6_11
   Zeng Dan, 2021, P IEEECVF INT C COMP, P11385
   Zhang XM, 2018, IEEE IMAGE PROC, P1588, DOI 10.1109/ICIP.2018.8451301
   Zhang Z., 2018, P EUR C COMP VIS ECC, P269
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou L, 2021, IEEE T MULTIMEDIA, V23, P1035, DOI 10.1109/TMM.2020.2991592
   Zhu BK, 2018, AAAI CONF ARTIF INTE, P7607
   Ziwei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8897, DOI 10.1109/CVPR42600.2020.00892
NR 72
TC 7
Z9 7
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2601
EP 2612
DI 10.1109/TMM.2022.3148595
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600011
DA 2024-07-18
ER

PT J
AU Zhong, CK
   Ng, WWY
AF Zhong, Cankun
   Ng, Wing W. Y.
TI A Robust Frequency-Domain-Based Graph Adaptive Network for Parkinson's
   Disease Detection From Gait Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Sensors; Frequency-domain analysis; Adaptation
   models; Sensor phenomena and characterization; Correlation; Foot;
   Frequency-domain; generalization error; graph network; parkinson's
   disease detection; vertical ground reaction force
ID MODEL; DIAGNOSIS; PATTERNS
AB Parkinson's disease (PD) is a neurodegenerative disease with a high incidence rate. Effective early diagnosis of PD is critical to prevent further deterioration of a patient's condition, where gait abnormalities are important factors for doctors to diagnose PD. Deep learning (DL)-based methods for PD detection using gait information recorded by non-invasive sensors have emerged to assist doctors in accurate and efficient disease diagnosis. However, most existing DL-based PD detection models neglect information in the frequency domain and do not adaptively model the correlation of signals among sensors. Moreover, different people have different gait patterns. Therefore, the generalization capabilities of PD detection models on diversities of individuals' gaits are essential. This work proposes a novel robust frequency-domain-based graph adaptive network (RFdGAD) for PD detection from gait information (i.e., vertical ground reaction force signals recorded by foot sensors). Specifically, the RFdGAD first learns the frequency-domain features of signals from each foot sensor by a frequency representation learning block. Then, the RFdGAD utilizes a graph adaptive network block taking frequency-domain features as input to adaptively learn and exploit the interconnection between different sensor signals for accurate PD detection. Moreover, the RFdGAD is trained by minimizing the proposed Jensen-Shannon divergence-based localized generalization error to improve the generalization performance of RFdGAD on unseen subjects. Experimental results show that the RFdGAD outperforms existing DL-based models for PD detection on three widely used datasets in terms of three metrics, including accuracy, F1-score, and geometric mean.
C1 [Zhong, Cankun; Ng, Wing W. Y.] South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cyb, Guangzhou 510006, Peoples R China.
C3 South China University of Technology
RP Ng, WWY (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cyb, Guangzhou 510006, Peoples R China.
EM curran.z@qq.com; wingng@ieee.org
OI Zhong, Cankun/0000-0002-4271-6483; Ng, Wing W. Y./0000-0003-0783-3585
FU National Natural Science Foundation of China [61876066]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61876066. The Associate Editor coordinating the review
   of this manuscript and approving it for publication was Dr. Ramanathan
   Subramanian
CR Abdulhay E, 2018, FUTURE GENER COMP SY, V83, P366, DOI 10.1016/j.future.2018.02.009
   Asuroglu T, 2018, BIOCYBERN BIOMED ENG, V38, P760, DOI 10.1016/j.bbe.2018.06.002
   Aydin F, 2021, ENG SCI TECHNOL, V24, P112, DOI 10.1016/j.jestch.2020.12.005
   Balaji E, 2021, MED ENG PHYS, V91, P54, DOI 10.1016/j.medengphy.2021.03.005
   Balaji E, 2020, APPL SOFT COMPUT, V94, DOI 10.1016/j.asoc.2020.106494
   Bo DY, 2021, AAAI CONF ARTIF INTE, V35, P3950
   El Maachi I, 2020, EXPERT SYST APPL, V143, DOI 10.1016/j.eswa.2019.113075
   Englesson Erik, 2021, ADV NEUR IN, V34
   Ertugrul ÖF, 2016, EXPERT SYST APPL, V56, P156, DOI 10.1016/j.eswa.2016.03.018
   Flagg C, 2021, WSDM '21: PROCEEDINGS OF THE 14TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P1081, DOI 10.1145/3437963.3441701
   Frenkel-Toledo S, 2005, MOVEMENT DISORD, V20, P1109, DOI 10.1002/mds.20507
   Ghaderyan P, 2021, MEASUREMENT, V177, DOI 10.1016/j.measurement.2021.109249
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Guo R, 2022, IEEE T MULTIMEDIA, V24, P1583, DOI 10.1109/TMM.2021.3068609
   Hausdorff JM, 2007, EUR J NEUROSCI, V26, P2369, DOI 10.1111/j.1460-9568.2007.05810.x
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Iwana BK, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0254841
   Jane YN, 2016, J BIOMED INFORM, V60, P169, DOI 10.1016/j.jbi.2016.01.014
   Jang E., 2017, P ICLR, P1
   Kleanthous N, 2020, PATTERN RECOGN LETT, V140, P119, DOI 10.1016/j.patrec.2020.09.011
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin T., 2022, AI Open, V3, P111, DOI [DOI 10.1016/J.AIOPEN.2022.10.001, 10.1016/j.aiopen.2022.10.001]
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu X, 2021, APPL INTELL, V51, P7221, DOI 10.1007/s10489-020-02182-5
   Minamisawa T, 2012, GAIT POSTURE, V35, P308, DOI 10.1016/j.gaitpost.2011.09.106
   Moon S, 2020, J NEUROENG REHABIL, V17, DOI 10.1186/s12984-020-00756-5
   Ng W, 2022, IEEE T MULTIMEDIA, V24, P1678, DOI 10.1109/TMM.2021.3070127
   Nguyen D. M. D., 2022, P 26 INT C PATT REC
   Paul A, 2020, J DRUG DELIV SCI TEC, V58, DOI 10.1016/j.jddst.2020.101790
   Pei XM, 2021, IET SIGNAL PROCESS, V15, P80, DOI 10.1049/sil2.12018
   Pereira CR, 2019, ARTIF INTELL MED, V95, P48, DOI 10.1016/j.artmed.2018.08.007
   Perumal SV, 2016, ICT EXPRESS, V2, P168, DOI 10.1016/j.icte.2016.10.005
   Pistacchi M, 2017, FUNCT NEUROL, V32, P28, DOI 10.11138/FNeur/2017.32.1.028
   Priya SJ, 2021, DIAGNOSTICS, V11, DOI 10.3390/diagnostics11081395
   Reeve A, 2014, AGEING RES REV, V14, P19, DOI 10.1016/j.arr.2014.01.004
   Rehman RZU, 2020, IEEE OPEN J ENG MED, V1, P65, DOI 10.1109/OJEMB.2020.2966295
   Rong Y., 2019, ARXIV, DOI [10.48550/arXiv.1907.10903, DOI 10.48550/ARXIV.1907.10903]
   Shang C., 2021, P INT C LEARN REPR
   Tang YH, 2022, PROC CVPR IEEE, P10925, DOI 10.1109/CVPR52688.2022.01066
   Wu ZH, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P753, DOI 10.1145/3394486.3403118
   Xia Y, 2020, IEEE T NEUR SYS REH, V28, P42, DOI 10.1109/TNSRE.2019.2946194
   Xia Y, 2018, BIOMED SIGNAL PROCES, V46, P221, DOI 10.1016/j.bspc.2018.07.015
   Ye Y, 2023, IEEE T KNOWL DATA EN, V35, P905, DOI 10.1109/TKDE.2021.3072345
   Yeung DS, 2007, IEEE T NEURAL NETWOR, V18, P1294, DOI 10.1109/TNN.2007.894058
   Yogev G, 2005, EUR J NEUROSCI, V22, P1248, DOI 10.1111/j.1460-9568.2005.04298.x
   Zhao AT, 2022, IEEE T MULTIMEDIA, V24, P846, DOI 10.1109/TMM.2021.3060280
   Zhao AT, 2022, IEEE T CYBERNETICS, V52, P9439, DOI 10.1109/TCYB.2021.3056104
   Zhao AT, 2018, NEUROCOMPUTING, V315, P1, DOI 10.1016/j.neucom.2018.03.032
   Zheng Cheng, 2020, PROC INT C MACH LEAR, V119, P11458
   Zhu Y., 2021, arXiv, DOI DOI 10.48550/ARXIV.2103.03036
NR 50
TC 2
Z9 2
U1 8
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7076
EP 7088
DI 10.1109/TMM.2022.3217392
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000027
DA 2024-07-18
ER

PT J
AU Zhou, XC
   Ding, R
   Wang, YX
   Wei, WJ
   Liu, HJ
AF Zhou, Xichuan
   Ding, Rui
   Wang, Yuxiao
   Wei, Wenjia
   Liu, Haijun
TI Cellular Binary Neural Network for Accurate Image Classification and
   Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary neural networks; image classification; lateral connections;
   regularization term; semantic segmentation
AB This paper presents the Cellular Binary Neural Network (CBNN), which is an efficient deep neural network with binary weights and activations. To address the challenge of performance drop caused by low-precision representation, the CBNN adopts multiple subnets which are connected via learnable global lateral paths. The introduced lateral connections are assumed to be sparse and grouped with respect to different source layers. The inter-network lateral connections and inner-network parameters are simultaneously optimized by the distributional loss, classification loss and the group sparse regularization term. Experiments on the CIFAR-10 and ImageNet datasets showed that, by incorporating optimized group-sparse lateral paths, the CBNN outperformed many state-of-the-art binary neural networks in terms of classification accuracy. Besides, to verify the generalization of the proposed binary model, we extended the CBNN on semantic segmentation task. CBNN takes advantage of the multiple subnets to derive the more informative feature maps which are computed by the parallel aggregation in the last convolution block. Experiments on PASCAL VOC segmentation dataset demonstrated that, under the same segmentation settings, the proposed method achieved the superior performance over other compared networks and even the full-precision counterpart.
C1 [Zhou, Xichuan; Ding, Rui; Wang, Yuxiao; Liu, Haijun] Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
   [Wei, Wenjia] Huawei Technol Co Ltd, Shenzhen 518129, Peoples R China.
C3 Chongqing University; Huawei Technologies
RP Liu, HJ (corresponding author), Chongqing Univ, Sch Microelect & Commun Engn, Chongqing 400044, Peoples R China.
EM zxc@cqu.edu.cn; dingrui96@qq.com; 202212021079t@stu.cqu.edu.cn;
   weiwenjia@huawei.com; haijun_liu@126.com
FU National Natural Science Foundation of China
FX No Statement Available
CR Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Bethge J, 2020, Arxiv, DOI arXiv:2001.05936
   Bulat Adrian, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P309, DOI 10.1007/978-3-030-58592-1_19
   Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574
   Chen LC, 2017, Arxiv, DOI [arXiv:1706.05587, DOI 10.48550/ARXIV.1706.05587]
   Dahyun Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P575, DOI 10.1007/978-3-030-58610-2_34
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Denton E, 2014, ADV NEUR IN, V27
   Ding RZ, 2019, PROC CVPR IEEE, P11400, DOI 10.1109/CVPR.2019.01167
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Faraone J, 2018, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR.2018.00452
   Gong RH, 2019, IEEE I CONF COMP VIS, P4851, DOI 10.1109/ICCV.2019.00495
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Han  S., 2015, ARXIV151000149
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Helwegen K., 2019, Advances in neural information processing systems
   Hinton Geoffrey, 2014, NIPS DEEP LEARN WORK
   Hou L, 2016, ARXIV
   Hubara I, 2016, ADV NEUR IN, V29
   Ji G., 2020, Advances in Neural Information Processing Systems, V33, P20823
   Jung S, 2019, PROC CVPR IEEE, P4345, DOI 10.1109/CVPR.2019.00448
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kim H, 2021, PROC CVPR IEEE, P7858, DOI 10.1109/CVPR46437.2021.00777
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee J, 2021, PROC CVPR IEEE, P6444, DOI 10.1109/CVPR46437.2021.00638
   Li RD, 2019, PROC CVPR IEEE, P2805, DOI 10.1109/CVPR.2019.00292
   Lin M., 2020, P ADV NEUR INF PROC, P1
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin XF, 2017, ADV NEUR IN, V30
   Liu CL, 2019, PROC CVPR IEEE, P2686, DOI 10.1109/CVPR.2019.00280
   Liu HB, 2022, IEEE T MULTIMEDIA, V24, P2902, DOI 10.1109/TMM.2021.3090274
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Martinez B., 2020, PROC INT C LEARN REP
   Park K, 2023, IEEE T MULTIMEDIA, V25, P907, DOI 10.1109/TMM.2021.3134172
   Paszke A, 2019, ADV NEUR IN, V32
   Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232
   Qin HT, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107281
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Sharma PK, 2023, IEEE T MULTIMEDIA, V25, P953, DOI 10.1109/TMM.2021.3134158
   Shomron G., 2021, Advances in Neural Information Processing Systems, V34, P17737
   Shoukai Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P1, DOI 10.1007/978-3-030-58610-2_1
   Sun SY, 2018, CAAI T INTELL TECHNO, V3, P191, DOI 10.1049/trit.2018.1026
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang ZW, 2020, PROC CVPR IEEE, P2046, DOI 10.1109/CVPR42600.2020.00212
   Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066
   Xu Yixing, 2021, Advances in Neural Information Processing Systems, V34, P25553
   Xu YH, 2020, IEEE T MULTIMEDIA, V22, P1874, DOI 10.1109/TMM.2019.2949857
   Xu Z, 2019, Arxiv, DOI arXiv:1909.11366
   Xu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5178, DOI 10.1109/ICCV48922.2021.00515
   Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9
   Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23
   Zhou Shuchang, 2016, arXiv
   Zhou X., 2020, International conference on machine learning, P11503
   Zhou XCA, 2021, AAAI CONF ARTIF INTE, V35, P3590
   Zhou ZG, 2021, IEEE T MULTIMEDIA, V23, P871, DOI 10.1109/TMM.2020.2990087
   Zhu C., 2016, ARXIV
   Zhu SL, 2019, PROC CVPR IEEE, P4918, DOI 10.1109/CVPR.2019.00506
   Zhuang BH, 2019, PROC CVPR IEEE, P413, DOI 10.1109/CVPR.2019.00050
NR 61
TC 2
Z9 2
U1 10
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8064
EP 8075
DI 10.1109/TMM.2022.3233255
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000010
DA 2024-07-18
ER

PT J
AU Chen, ZZ
   Li, J
   Wu, J
   Chang, J
   Xiao, YF
   Wang, XT
AF Chen, Zhongze
   Li, Jing
   Wu, Jia
   Chang, Jun
   Xiao, Yafu
   Wang, Xiaoting
TI Drift-Proof Tracking With Deep Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Reinforcement learning; Training; Robustness; Object
   tracking; Measurement; Real-time systems; Object tracking; deep
   reinforcement learning; drift problems
ID OBJECT TRACKING
AB Object tracking is an essential and challenging sub-domain in the field of computer vision owing to its wide range of applications and complexities of real-life situations. It has been studied extensively over the last decade, leading to the proposal of several tracking frameworks and approaches. Recently, the introduction of reinforcement learning and the 'Actor-Critic' framework has effectively improved the tracking speed of deep learning trackers. However, most existing deep reinforcement learning trackers experience a slight performance degradation mainly owing to the drift issues. Drifts pose a threat to the tracking performance, which may lead to losing the tracked target. Herein, we propose a drift-proof tracker with deep reinforcement learning that aims to improve the tracking performance by counteracting drifts while maintaining its real-time advantage. We utilize a reward function with the Distance-IoU (DIoU) metric to guide the reinforcement learning to alleviate the drifts caused by the trained model. Furthermore, double negative samples (hard negative and drift samples) are constructed in tracking for network initialization, which is followed by calculating the loss by a small error-friendly loss function. Therefore, our tracker can better discriminate between the positive and negative samples and correct the predicted bounding boxes when the drift occurs. Meanwhile, a generative adversarial network is adopted for positive sample augmentation. Extensive experimental results on multiple popular benchmarks show that our algorithm effectively reduces the occurrences of drift and boosts the tracking performance, compared to those of other state-of-the-art trackers.
C1 [Chen, Zhongze; Li, Jing; Chang, Jun; Xiao, Yafu] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Wu, Jia] Macquarie Univ, Fac Sci & Engn, Dept Comp, Sydney, NSW 2109, Australia.
   [Wang, Xiaoting] Sichuan Inst Aerosp Elect Equipment, Chengdu 610100, Peoples R China.
C3 Wuhan University; Macquarie University
RP Li, J (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM chen@whu.edu.cn; leejingcn@whu.edu.cn; jia.wu@mq.edu.au;
   chang.jun@whu.edu.cn; xiaoyafu@whu.edu.cn; 460693339@qq.com
RI WU, Jia/V-1766-2019
OI WU, Jia/0000-0001-9013-0818; Chen, Zhongze/0000-0001-9165-7224; Wu,
   Jia/0000-0002-1371-5801
FU Science and Technology Major Project of Hubei Province (Next-Generation
   AI Technologies) [2019AEA170]; Natural Science Foundation of Hubei
   Province [2018CFA050]
FX This work was supported in part by the Science and Technology Major
   Project of Hubei Province (Next-Generation AI Technologies) under Grant
   2019AEA170, and in part by the Natural Science Foundation of Hubei
   Province under Grant 2018CFA050.
CR Abdelpakey MH, 2020, IEEE T IMAGE PROCESS, V29, P1479, DOI 10.1109/TIP.2019.2942506
   Abdelpakey MH, 2018, LECT NOTES COMPUT SC, V11241, P463, DOI 10.1007/978-3-030-03801-4_41
   [Anonymous], ARXIV150104587
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen BY, 2018, LECT NOTES COMPUT SC, V11211, P328, DOI 10.1007/978-3-030-01234-2_20
   Chen ZD, 2020, PROC CVPR IEEE, P6667, DOI 10.1109/CVPR42600.2020.00670
   Cheng K., 2018, 2018 2nd IEEE Conference on Energy Internet and Energy System Integration (EI2), P1, DOI DOI 10.1109/ICC.2018.8422877
   Choi J, 2018, PROC CVPR IEEE, P479, DOI 10.1109/CVPR.2018.00057
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Dong XP, 2018, PROC CVPR IEEE, P518, DOI 10.1109/CVPR.2018.00061
   Dong XP, 2018, LECT NOTES COMPUT SC, V11217, P472, DOI 10.1007/978-3-030-01261-8_28
   Dunnhofer M, 2019, IEEE INT CONF COMP V, P2290, DOI 10.1109/ICCVW.2019.00282
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Gao J, 2020, IEEE T PATTERN ANAL, V42, P939, DOI 10.1109/TPAMI.2018.2889070
   Gao JY, 2017, IEEE T IMAGE PROCESS, V26, P1845, DOI 10.1109/TIP.2017.2656628
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo J, 2018, IEEE IMAGE PROC, P226, DOI 10.1109/ICIP.2018.8451440
   Guo Q, 2020, IEEE T IMAGE PROCESS, V29, P2999, DOI 10.1109/TIP.2019.2955292
   Han RZ, 2020, IEEE T IMAGE PROCESS, V29, P7128, DOI 10.1109/TIP.2020.2998978
   Hong ZB, 2015, PROC CVPR IEEE, P749, DOI 10.1109/CVPR.2015.7298675
   Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1
   Hu HW, 2019, IEEE T MULTIMEDIA, V21, P510, DOI 10.1109/TMM.2018.2859831
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang C, 2017, IEEE I CONF COMP VIS, P105, DOI 10.1109/ICCV.2017.21
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang HQ, 2019, IET IMAGE PROCESS, V13, P2687, DOI 10.1049/iet-ipr.2018.6672
   Huang ZY, 2019, IEEE I CONF COMP VIS, P2891, DOI 10.1109/ICCV.2019.00298
   Gulrajani I, 2017, ADV NEUR IN, V30
   Jung I., 2018, P ECCV, P83
   Kart U, 2019, PROC CVPR IEEE, P1339, DOI 10.1109/CVPR.2019.00143
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Li Y, 2015, LECT NOTES COMPUT SC, V8926, P254, DOI 10.1007/978-3-319-16181-5_18
   Li YM, 2021, IEEE T MULTIMEDIA, V23, P810, DOI 10.1109/TMM.2020.2990064
   Liang PP, 2015, IEEE T IMAGE PROCESS, V24, P5630, DOI 10.1109/TIP.2015.2482905
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H., 2016, CORR
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Park E, 2018, LECT NOTES COMPUT SC, V11207, P587, DOI 10.1007/978-3-030-01219-9_35
   Radford A., 2015, ARXIV151106434
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Song YB, 2018, PROC CVPR IEEE, P8990, DOI 10.1109/CVPR.2018.00937
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Supancic J, 2017, IEEE I CONF COMP VIS, P322, DOI 10.1109/ICCV.2017.43
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632
   Wang N, 2019, PROC CVPR IEEE, P1308, DOI 10.1109/CVPR.2019.00140
   Wang N, 2018, PROC CVPR IEEE, P4844, DOI 10.1109/CVPR.2018.00509
   Wang X, 2018, PROC CVPR IEEE, P4864, DOI 10.1109/CVPR.2018.00511
   Wu CL, 2020, IEEE ACCESS, V8, P7473, DOI 10.1109/ACCESS.2020.2964269
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Xu Y., 2020, AAAI, P556, DOI [10.1609/aaai.v34i07.6944, DOI 10.1609/AAAI.V34I07.6944]
   Yang TY, 2020, PROC CVPR IEEE, P6717, DOI 10.1109/CVPR42600.2020.00675
   Yang TY, 2021, IEEE T PATTERN ANAL, V43, P360, DOI 10.1109/TPAMI.2019.2929034
   Yang TY, 2018, LECT NOTES COMPUT SC, V11213, P153, DOI 10.1007/978-3-030-01240-3_10
   Yao R, 2017, IEEE T MULTIMEDIA, V19, P772, DOI 10.1109/TMM.2016.2631727
   Yuan D, 2020, KNOWL-BASED SYST, V194, DOI 10.1016/j.knosys.2020.105526
   Yun S, 2017, PROC CVPR IEEE, P1349, DOI 10.1109/CVPR.2017.148
   Zhang H, 2019, PR MACH LEARN RES, V97
   Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13
   Zhang MD, 2018, LECT NOTES COMPUT SC, V11207, P484, DOI 10.1007/978-3-030-01219-9_29
   Zhang Ning, 2020, ARXIV200506536
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zheng ZH, 2020, AAAI CONF ARTIF INTE, V34, P12993
   Zhong BN, 2019, IEEE T IMAGE PROCESS, V28, P2331, DOI 10.1109/TIP.2018.2885238
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 79
TC 4
Z9 4
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 609
EP 624
DI 10.1109/TMM.2021.3056896
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100008
DA 2024-07-18
ER

PT J
AU Peng, J
   Zhou, YY
   Sun, XS
   Cao, LJ
   Wu, YJ
   Huang, FY
   Ji, RR
AF Peng, Jun
   Zhou, Yiyi
   Sun, Xiaoshuai
   Cao, Liujuan
   Wu, Yongjian
   Huang, Feiyue
   Ji, Rongrong
TI Knowledge-Driven Generative Adversarial Network for Text-to-Image
   Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Generative adversarial networks; Task analysis;
   Semantics; Measurement; Image synthesis; Feature extraction;
   Text-to-image; knowledge-driven; generative adversarial network; pseudo
   turing test
AB Text-to-Image (T2I) synthesis is a challenging task that aims to convert natural language descriptions to real images. It remains an open problem mainly due to the diversity of text descriptions, which poses a huge obstacle in generating vivid and relevant images. Moreover, the existing evaluation metrics in T2I synthesis are mainly used to evaluate the visual quality of the generated images, while the semantic consistency between the two modalities is often ignored. To address these issues, we present a novel Knowledge-Driven Generative Adversarial Network, termed KD-GAN, and a new evaluation system, named Pseudo Turing Test (PTT for short). Concretely, KD-GAN takes a further step in imitating the behavior of human painting, i.e., drawing an image according to reference knowledge. The introduction of reference knowledge in KD-GAN not only improves the quality of the generated images but also enhances the semantic consistency between them and the input texts. In addition, KD-GAN can also greatly avoid some flaws against common sense during image generation, e.g., skiing in the blue sky. The proposed PTT is an important supplement to the existing evaluation system of T2I synthesis. It includes a set of pseudo-experts of different multimedia tasks to evaluate the semantic consistency between the given texts and the generated images. To validate the proposed KD-GAN, we conducted extensive experiments on two benchmark datasets, i.e., Caltech-UCSD Birds (CUB), and MS-COCO (COCO). The experimental results demonstrate that KD-GAN outperforms state-of-the-art methods on IS, FID, and the proposed PTT metrics.(1)
C1 [Peng, Jun; Zhou, Yiyi; Sun, Xiaoshuai; Cao, Liujuan; Ji, Rongrong] Xiamen Univ, Media Analyt & Comp Lab, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.
   [Peng, Jun] Xiamen Univ, Natl Inst Data Sci Hlth & Med, Xiamen 361005, Peoples R China.
   [Wu, Yongjian; Huang, Feiyue] Tencent Technol Shanghai Co Ltd, Shanghai 200233, Peoples R China.
   [Ji, Rongrong] Xiamen Univ, Inst Artificial Intelligence, Xiamen 361005, Peoples R China.
C3 Xiamen University; Xiamen University; Xiamen University
RP Zhou, YY (corresponding author), Xiamen Univ, Media Analyt & Comp Lab, Dept Artificial Intelligence, Xiamen 361005, Peoples R China.
EM pengjun@stu.xmu.edu.cn; zhouyiyi@xmu.edu.cn; xssun@xmu.edu.cn;
   caoliujuan@xmu.edu.cn; littlekenwu@tencent.com; garyhuang@tencent.com;
   rrji@xmu.edu.cn
OI Peng, Jun/0000-0003-0655-1594; Cao, Liujuan/0000-0002-7645-9606
FU National Science Fund for Distinguished Young Scholars [62025603];
   Fundamental Research Funds for the central universities [20720200077,
   20720200090, 20720200091]; National Natural Science Foundation of China
   [U1705262, 62072386, 62072387, 62072389, 62002305, 61772443, 61802324,
   61702136]; China Postdoctoral Science Foundation [2021T40397]; Guangdong
   Basic and Applied Basic Research Foundation [2019B1515120049]
FX This work was supported in part by the National Science Fund for
   Distinguished Young Scholars under Grant 62025603, in part by
   Fundamental Research Funds for the central universities under Grants
   20720200077, 20720200090, and 20720200091, in part by the National
   Natural Science Foundation of China under Grants U1705262, 62072386,
   62072387, 62072389, 62002305, 61772443, 61802324, and 61702136, in part
   by China Postdoctoral Science Foundation under Grant 2021T40397, and in
   part by Guangdong Basic and Applied Basic Research Foundation under
   Grant 2019B1515120049. (Corresponding author: Yiyi Zhou.)
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2004, P WORKSH TEXT SUMM B
   Barratt S, 2018, Arxiv, DOI [arXiv:1801.01973, DOI 10.48550/ARXIV.1801.01973]
   Brock A., 2019, INT C LEARN REPR
   Cha M, 2017, IEEE INT WORKS MACH
   Dash A, 2017, Arxiv, DOI arXiv:1703.06412
   Dong H, 2017, IEEE IMAGE PROC, P2015
   Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Gumbel E. J., 1948, SERIES LECT US GOV P, V33
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Jang E., 2017, P ICLR, P1
   Kingma D. P., 2014, arXiv
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Li XY, 2021, PROC CVPR IEEE, P8635, DOI 10.1109/CVPR46437.2021.00853
   Li YK, 2019, Arxiv, DOI arXiv:1905.01608
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mansimov E., 2016, P INT C LEARN REPR
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qiao TT, 2019, PROC CVPR IEEE, P1505, DOI 10.1109/CVPR.2019.00160
   Reed S, 2016, PR MACH LEARN RES, V48
   Salimans T, 2016, ADV NEUR IN, V29
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Xu WJ, 2019, IEEE T MULTIMEDIA, V21, P2387, DOI 10.1109/TMM.2019.2898777
   Yang L, 2015, IEEE T IMAGE PROCESS, V24, P4701, DOI 10.1109/TIP.2015.2465157
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang S, 2018, LECT NOTES COMPUT SC, V11166, P417, DOI 10.1007/978-3-030-00764-5_38
   Zhang ZZ, 2018, PROC CVPR IEEE, P6199, DOI 10.1109/CVPR.2018.00649
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhu MF, 2019, PROC CVPR IEEE, P5795, DOI 10.1109/CVPR.2019.00595
NR 42
TC 11
Z9 12
U1 2
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4356
EP 4366
DI 10.1109/TMM.2021.3116416
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J3WX2
UT WOS:001008961300003
DA 2024-07-18
ER

PT J
AU Sheng, XH
   Li, L
   Liu, D
   Xiong, ZW
   Li, Z
   Wu, F
AF Sheng, Xihua
   Li, Li
   Liu, Dong
   Xiong, Zhiwei
   Li, Zhu
   Wu, Feng
TI Deep-PCAC: An End-to-End Deep Lossy Compression Framework for Point
   Cloud Attributes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Image coding; Geometry; Transforms;
   Convolution; Transform coding; Decoding; Attribute compression; deep
   neural network; end-to-end compression; point-based learning; point
   cloud compression
ID EFFICIENCY
AB The large data volume of point clouds poses severe challenges for efficient storage and transmission in recent years. In this paper, we propose the first--to our best knowledge--end-to-end deep framework for compressing point cloud attributes. Specifically, we propose a point cloud lossy attribute autoencoder, which directly encodes and decodes point cloud attributes with the help of geometry, instead of voxelizing or projecting the points. In the autoencoder, we propose a second-order point convolution that utilizes the spatial correlations between more points and the nonlinear relationship between attribute features. We introduce a dense point-inception block, which derives from a combination of an inception-style block and a dense block, to improve feature propagation. In addition, we devise a multiscale loss to guide the autoencoder in focusing attention on the coarse-grained points with better coverage of the entire point cloud, which makes it easier for the autoencoder to obtain better optimization of the qualities of all points. Experimental results show that our proposed framework still has a performance gap compared with the state-of-the-art algorithms in the MPEG G-PCC reference software TMC13. However, it does outperform the RAHT-RLGR, which is one of the core transforms used in TMC13 without many well-designed techniques that make TMC13 what it is today. It outperforms RAHT-RLGR by 2.63 dB, 1.77 dB, and 3.40 dB on average in terms of the BD-PSNR for the Y, U, and V components. A subjective quality comparison demonstrates that our framework can preserve more textures and reduce blocking and color noise artifacts.
C1 [Sheng, Xihua; Li, Li; Liu, Dong; Xiong, Zhiwei; Wu, Feng] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Missouri System; University of Missouri Kansas
   City
RP Li, L (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Peoples R China.
EM xhsheng@mail.ustc.edu.cn; lil1@ustc.edu.cn; dongeliu@ustc.edu.cn;
   zwxiong@ustc.edu.cn; zhu.li@ieee.org; fengwu@ustc.edu.cn
RI wang, jiajun/JRW-6032-2023; liu, dong/GRJ-9115-2022; liu,
   dongsheng/IWM-1597-2023; Li, Zhu/AAD-8182-2021; Liu, DY/JPL-4171-2023;
   SUN, YANLING/JTT-9082-2023; Yang, Tian/JFB-1008-2023; Wu,
   Feng/KCY-3017-2024; Wu, Jiale/JQV-3750-2023
OI Li, Zhu/0000-0002-8246-177X; Liu, Dong/0000-0001-9100-2906
FU Natural Science Foundation of China [61931014, 61772483, 62021001]; USTC
   Research Funds of the Double First-Class Initiative [YD3490002001]
FX This work was supported by the Natural Science Foundation of China under
   Grants 61931014, 61772483, and 62021001 and in part by USTC Research
   Funds of the Double First-Class Initiative under Grant YD3490002001. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof Raouf Hamzaoui.
CR Agustsson E, 2017, ADV NEUR IN, V30
   Anis A, 2016, INT CONF ACOUST SPEE, P6360, DOI 10.1109/ICASSP.2016.7472901
   Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170
   Balle J, 2018, ICLR
   Bjotegaard G., 2001, VCEGM33
   Chen H., 2020, ARXIV200700649
   Cheng ZX, 2020, IEEE T MULTIMEDIA, V22, P860, DOI 10.1109/TMM.2019.2938345
   Cheng ZX, 2019, PROC CVPR IEEE, P10063, DOI 10.1109/CVPR.2019.01031
   Chou PA, 2020, IEEE T IMAGE PROCESS, V29, P2203, DOI 10.1109/TIP.2019.2908095
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   de Queiroz RL, 2016, IEEE T IMAGE PROCESS, V25, P3947, DOI 10.1109/TIP.2016.2575005
   Garcia DC, 2020, IEEE T IMAGE PROCESS, V29, P313, DOI 10.1109/TIP.2019.2931466
   Gu S, 2020, IEEE T IMAGE PROCESS, V29, P796, DOI 10.1109/TIP.2019.2936738
   Haar A., 1911, Mathematische Annalen, V71, P38, DOI DOI 10.1007/BF01456927
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang TX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P890, DOI 10.1145/3343031.3351061
   Huang Y, 2008, IEEE T VIS COMPUT GR, V14, P440, DOI 10.1109/TVCG.2007.70441
   Kathariya B, 2019, IEEE DATA COMPR CONF, P580, DOI 10.1109/DCC.2019.00092
   Khaled M., 2018, DOCUMENT ISOIEC JTC1
   Kingma D. P., 2015, ARXIV14126980, V9
   Krivokuca M, 2020, IEEE T IMAGE PROCESS, V29, P2217, DOI 10.1109/TIP.2019.2957853
   Li L, 2021, IEEE T MULTIMEDIA, V23, P2806, DOI 10.1109/TMM.2020.3016894
   Lim T., 2016, Proc. ICLR
   Lin HW, 2019, IEEE T MULTIMEDIA, V21, P3010, DOI 10.1109/TMM.2019.2919433
   Lu G, 2019, PROC CVPR IEEE, P10998, DOI 10.1109/CVPR.2019.01126
   Luo FL, 2019, IEEE T MULTIMEDIA, V21, P851, DOI 10.1109/TMM.2018.2867260
   Malvar HS, 2006, IEEE DATA COMPR CONF, P23
   Mammou K., 2017, Document ISO/IEC JTC1/SC29/WG11 m41649
   Mammou K., 2018, document ISO/IEC JTC1/SC29/WG11,MPEG2018/m44837
   Mekuria R, 2017, IEEE T CIRC SYST VID, V27, P828, DOI 10.1109/TCSVT.2016.2543039
   Mentzer F, 2018, PROC CVPR IEEE, P4394, DOI 10.1109/CVPR.2018.00462
   Minnen D, 2018, ADV NEUR IN, V31
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Quach M, 2020, IEEE IMAGE PROC, P3309, DOI 10.1109/ICIP40778.2020.9191180
   Quach M, 2019, IEEE IMAGE PROC, P4320, DOI [10.1109/ICIP.2019.8803413, 10.1109/icip.2019.8803413]
   Rente PD, 2019, IEEE T MULTIMEDIA, V21, P284, DOI 10.1109/TMM.2018.2859591
   Schwarz S., 2018, Document ISO/IEC JTC1/SC29/WG11 w17766
   Schwarz S, 2019, IEEE J EM SEL TOP C, V9, P133, DOI 10.1109/JETCAS.2018.2885981
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun HM, 2020, IEEE T MULTIMEDIA, V22, P2764, DOI 10.1109/TMM.2019.2963620
   Tian D., 2017, DOCUMENT ISOIEC JTC1
   Tulvan C., 2016, ISOIEC JTC1SC29WG11
   Veit A, 2016, ADV NEUR IN, V29
   WALLACE GK, 1992, IEEE T CONSUM ELECTR, V38, pR18, DOI 10.1109/30.125072
   Wang J., 2021, IEEE Transactions on Circuits and Systems for Video Technology
   Xu YQ, 2021, IEEE T CIRC SYST VID, V31, P1968, DOI 10.1109/TCSVT.2020.3015901
   Xu YQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1753, DOI 10.1109/ICASSP.2018.8462684
   Zakharchenko V., 2018, DOC UMENT ISOIEC JTC
   Zhang C, 2014, IEEE IMAGE PROC, P2066, DOI 10.1109/ICIP.2014.7025414
   Zhang M, 2020, IEEE T MULTIMEDIA, V22, P1744, DOI 10.1109/TMM.2019.2963592
   Zhu LW, 2020, IEEE T MULTIMEDIA, V22, P45, DOI 10.1109/TMM.2019.2924591
NR 53
TC 22
Z9 24
U1 5
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2617
EP 2632
DI 10.1109/TMM.2021.3086711
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600029
DA 2024-07-18
ER

PT J
AU Sun, C
   Song, H
   Wu, XX
   Jia, YD
   Luo, JB
AF Sun, Che
   Song, Hao
   Wu, Xinxiao
   Jia, Yunde
   Luo, Jiebo
TI Exploiting Informative Video Segments for Temporal Action Localization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Motion segmentation; Location awareness; Proposals; Generators;
   Aggregates; Image segmentation; Feature extraction; Temporal action
   localization; informative video segments; supervised temporal attention
   network; attention mechanism
ID ACTION RECOGNITION; ATTENTION; NETWORKS
AB We propose a novel method of exploiting informative video segments by learning segment weights for temporal action localization in untrimmed videos. Informative video segments represent the intrinsic motion and appearance of an action, and thus contribute crucially to action localization. The learned segment weights represent the informativeness of video segments to recognize actions and help infer the boundaries required to temporally localize actions. We build a supervised temporal attention network (STAN) that includes a supervised segment-level attention module to dynamically learn the weights of video segments, and a feature-level attention module to effectively fuse multiple features of segments. Through the cascade of the attention modules, STAN exploits informative video segments and generates descriptive and discriminative video representations. We use a proposal generator and a classifier to estimate the boundaries of actions and classify the classes of actions. Extensive experiments are conducted on two public benchmarks, i.e., THUMOS2014 and ActivityNet1.3. The results demonstrate that our proposed method achieves competitive performance compared with existing state-of-the-art methods. Moreover, compared with the baseline method that treats video segments equally, STAN achieves significant improvements with an increase of the mean average precision from 30.4% to 39.8% on the THUMOS2014 dataset, and from 31.4% to 35.9% on the ActivityNet1.3 dataset, demonstrating the effectiveness of learning informative video segments for temporal action localization.
C1 [Sun, Che; Song, Hao; Wu, Xinxiao; Jia, Yunde] Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 10081, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
C3 Beijing Institute of Technology; University of Rochester
RP Wu, XX (corresponding author), Beijing Inst Technol, Sch Comp Sci, Beijing Lab Intelligent Informat Technol, Beijing 10081, Peoples R China.
EM sunche@bit.edu.cn; songhao@bit.edu.cn; wuxinxiao@bit.edu.cn;
   jiayunde@bit.edu.cn; jiebo.luo@gmail.com
RI Luo, Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729; Wu, Xinxiao/0000-0002-2056-6947; Sun,
   Che/0000-0002-7555-9146
FU Natural Science Foundation of China (NSFC) [61673062, 62072041]
FX This work was supported in part by the Natural Science Foundation of
   China (NSFC) under Grants 61673062 and 62072041.
CR Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16
   Alwassel H, 2018, LECT NOTES COMPUT SC, V11207, P264, DOI 10.1007/978-3-030-01219-9_16
   [Anonymous], 2016, P NAACL HUM COMP QUE
   [Anonymous], 2018, ICMR 18 P 2018 ACM, DOI DOI 10.1145/3206025.3206029
   [Anonymous], 2016, ARXIV160706416
   [Anonymous], 2016, ARXIV161105215
   [Anonymous], 2014, P COMP VIS PATT REC
   [Anonymous], 2015, P C COMP VIS PATT RE
   Ba J., 2014, ARXIV PREPRINT ARXIV
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Buch S., 2017, P BRIT MACH VIS C BM
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Gao Jiyang, 2017, BMVC
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Karaman S, 2014, ECCV THUMOS Workshop, V1, P5
   Kong WJ, 2019, INT CONF ACOUST SPEE, P1647, DOI 10.1109/ICASSP.2019.8682466
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J, 2020, AAAI CONF ARTIF INTE, V34, P4626
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu QY, 2020, AAAI CONF ARTIF INTE, V34, P11612
   Liu YF, 2019, PROC CVPR IEEE, P7099, DOI [10.1109/CVPR.2019.00726, 10.1109/CVPR.2019.00372]
   Oliver NM, 2000, IEEE T PATTERN ANAL, V22, P831, DOI 10.1109/34.868684
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh G., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Soomro K., 2012, ARXIV12120402CS
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang L, 2014, IEEE INT CONF VLSI
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu Z., 2015, Proceedings of the International Conference on Computer Vision, P1
   Yang JL, 2017, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2017.554
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhu Y, 2017, IEEE WINT CONF APPL, P197, DOI 10.1109/WACV.2017.29
NR 62
TC 19
Z9 19
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 274
EP 287
DI 10.1109/TMM.2021.3050067
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300021
DA 2024-07-18
ER

PT J
AU Wang, H
   Sahoo, D
   Liu, CH
   Shu, K
   Achananuparp, P
   Lim, EP
   Hoi, SCH
AF Wang, Hao
   Sahoo, Doyen
   Liu, Chenghao
   Shu, Ke
   Achananuparp, Palakorn
   Lim, Ee-peng
   Hoi, Steven C. H.
TI Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images
   and Recipes With Semantic Consistency and Attention Mechanism
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Task analysis; Data models; Correlation; Visualization;
   Training; Sugar; Deep learning; cross-modal retrieval;
   vision-and-language
AB Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention mechanism to improve the embedding of recipes. We evaluate the performance of the proposed method on the large-scale Recipe1M dataset, and show that we can outperform several state-of-the-art cross-modal retrieval strategies for food images and cooking recipes by a significant margin.
C1 [Wang, Hao] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Sahoo, Doyen; Liu, Chenghao; Shu, Ke; Achananuparp, Palakorn; Lim, Ee-peng; Hoi, Steven C. H.] Singapore Management Univ, Singapore 188065, Singapore.
   [Hoi, Steven C. H.] Salesforce Res Asia, Singapore 038985, Singapore.
C3 Nanyang Technological University; Singapore Management University;
   Salesforce
RP Wang, H (corresponding author), Nanyang Technol Univ, Singapore 639798, Singapore.
EM hao005@e.ntu.edu.sg; doyens@smu.edu.sg; chliu@smu.edu.sg;
   keshu@smu.edu.sg; palakorna@smu.edu.sg; eplim@smu.edu.sg;
   chhoi@smu.edu.sg
RI Liu, Chenghao/M-1202-2014; HOI, Steven C. H./A-3736-2011; Achananuparp,
   Palakorn/GLS-6036-2022; Wang, Hao/HLP-5008-2023; hao,
   wang/JWA-3237-2024; Lim, Ee-Peng/E-8562-2012
OI Achananuparp, Palakorn/0000-0002-7684-1725; Wang,
   Hao/0000-0002-3086-3128; Lim, Ee-Peng/0000-0003-0065-8665
FU National Research Foundation, Singapore, under its Strategic
   Capabilities Research Centres Funding Initiative
FX This research is supported by the National Research Foundation,
   Singapore, under its Strategic Capabilities Research Centres Funding
   Initiative. Any opinions, findings and conclusions or recommendations
   expressed in this material are those of the author(s) and do not reflect
   the views of National Research Foundation, Singapore. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. J. Tang.
CR Ba J. L., 2016, LAYER NORMALIZATION, DOI DOI 10.48550/ARXIV.1607.06450
   Carvalho M, 2018, ACM/SIGIR PROCEEDINGS 2018, P35, DOI 10.1145/3209978.3210036
   Chen JJ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1020, DOI 10.1145/3240508.3240627
   Chen JJ, 2017, LECT NOTES COMPUT SC, V10132, P588, DOI 10.1007/978-3-319-51811-4_48
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Cheng D, 2016, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2016.149
   Dizaji KG, 2018, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR.2018.00386
   Ge M., 2015, P 5 INT C DIGITAL HL, P105, DOI DOI 10.1145/2750511.2750528
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Han Fu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14558, DOI 10.1109/CVPR42600.2020.01458
   Hao Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P359, DOI 10.1007/978-3-030-58583-9_22
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Jiang SQ, 2020, IEEE T IMAGE PROCESS, V29, P265, DOI 10.1109/TIP.2019.2929447
   Jin L, 2023, IEEE T NEUR NET LEAR, V34, P1838, DOI 10.1109/TNNLS.2020.2997020
   Jing M., 2020, MM 20 P 28 ACM INT C, P3283, DOI DOI 10.1145/3394171.3413676
   Kingma D. P., 2014, arXiv
   Kiros R, 2015, 29 ANN C NEURAL INFO, V28
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2019, IEEE T PATTERN ANAL, V41, P2070, DOI 10.1109/TPAMI.2018.2852750
   Liu Chenchen, 2020, P IEEE CVF C COMP VI
   Min W., 2020, P MMM 20, P393
   Min WQ, 2020, IEEE T MULTIMEDIA, V22, P2659, DOI 10.1109/TMM.2019.2958761
   Min WQ, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3329168
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Min WQ, 2017, IEEE T MULTIMEDIA, V19, P1100, DOI 10.1109/TMM.2016.2639382
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Salvador A, 2017, PROC CVPR IEEE, P3068, DOI 10.1109/CVPR.2017.327
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H., 2019, P IEEE C COMP VIS PA, P572
   Zhu B, 2019, PROC CVPR IEEE, P11469, DOI 10.1109/CVPR.2019.01174
   Zichen Zan, 2020, ICMR '20: Proceedings of the 2020 International Conference on Multimedia Retrieval, P117, DOI 10.1145/3372278.3390681
NR 36
TC 15
Z9 17
U1 5
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2515
EP 2525
DI 10.1109/TMM.2021.3083109
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600022
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, JP
   Lin, YQ
   Zhang, ML
   Gao, Y
   Ma, AJ
AF Wang, Jinpeng
   Lin, Yiqi
   Zhang, Manlin
   Gao, Yuan
   Ma, Andy J.
TI Multi-Level Temporal Dilated Dense Prediction for Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Three-dimensional displays; Convolution; Image
   recognition; Task analysis; Solid modeling; Predictive models; Action
   Recognition; Temporal Dilated Dense Prediction; Multi-level Fusion; 3D
   Convolutional Neural Network
ID SPATIOTEMPORAL ATTENTION NETWORKS
AB 3D convolutional neural networks have achieved great success for action recognition. However, large variations of temporal dynamics have not been properly processed and low-level features have not been fully exploited in most existing works. To solve these two problems, we present a general and flexible framework, namely multi-level temporal dilated dense prediction network, which can incorporate with most of existing methods as backbone to improve the temporal modeling capacity. In the proposed method, a novel temporal dilated dense prediction block is designed to fully utilize temporal features with various temporal dilated rates for dense prediction while maintaining relatively low computational cost. To fuse information from low to high levels, our method combines the predictions from multiple such blocks inserted at different stages of the backbone network. In-depth analysis is given to show that short- to long-term temporal dependencies can be captured and multi-level spatio-temporal features are effectively fused for video action recognition by the proposed method. Experimental results demonstrate that our method achieves impressive performance improvement on four publicly available action recognition benchmarks including Charades, Kinetics, Something-Something-V1 and HMDB51.
C1 [Wang, Jinpeng] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
   [Lin, Yiqi; Zhang, Manlin; Gao, Yuan; Ma, Andy J.] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University
RP Ma, AJ (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM wangjp23@mail2.sysu.edu.cn; linyq29@mail2.sysu.edu.cn;
   zhangmlin3@mail2.sysu.edu.cn; gaoy266@mail2.sysu.edu.cn;
   majh8@mail.sysu.edu.cn
OI Wang, Jinpeng/0000-0001-6127-9146; Lin, Yiqi/0000-0002-8208-3705; Ma,
   Jinhua/0000-0002-0165-8416; Manlin, Zhang/0000-0003-2319-2749
FU NSFC [61906218]; Guangdong Basic and Applied Basic Research Foundation
   [2020A1515011497]; Science and Technology Program of Guangzhou
   [202002030371]
FX This work was partially supported in part by the NSFC under Grant
   61906218, in part by the Guangdong Basic and Applied Basic Research
   Foundation under Grant 2020A1515011497, and in part by the Science and
   Technology Program of Guangzhou under Grant 202002030371. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Marco Carli. JinpengWang and Yiqi Lin contribute to
   this work equally.
CR Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Choutas V, 2018, PROC CVPR IEEE, P7024, DOI 10.1109/CVPR.2018.00734
   Crasto N, 2019, PROC CVPR IEEE, P7874, DOI 10.1109/CVPR.2019.00807
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Ghiasi G, 2019, PROC CVPR IEEE, P7029, DOI 10.1109/CVPR.2019.00720
   Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034
   Jain A, 2013, PROC CVPR IEEE, P2571, DOI 10.1109/CVPR.2013.332
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kay W., 2017, ARXIV170506950
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Martinez B, 2019, IEEE I CONF COMP VIS, P5481, DOI 10.1109/ICCV.2019.00558
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rahmani H, 2016, IEEE T PATTERN ANAL, V38, P2430, DOI 10.1109/TPAMI.2016.2533389
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Shou Z, 2019, PROC CVPR IEEE, P1268, DOI 10.1109/CVPR.2019.00136
   Sigurdsson GA, 2017, PROC CVPR IEEE, P5650, DOI 10.1109/CVPR.2017.599
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2018, IEEE T IMAGE PROCESS, V27, P3459, DOI 10.1109/TIP.2018.2818328
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Wang J, 2018, PROC CVPR IEEE, P1149, DOI 10.1109/CVPR.2018.00126
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yu F, 2015, P INT C LEARN REPR
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhou BL, 2018, LECT NOTES COMPUT SC, V11205, P831, DOI 10.1007/978-3-030-01246-5_49
   Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43
NR 47
TC 12
Z9 12
U1 4
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2553
EP 2566
DI 10.1109/TMM.2021.3087023
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600025
DA 2024-07-18
ER

PT J
AU Wang, Q
   Min, WD
   Han, Q
   Liu, Q
   Zha, C
   Zhao, HY
   Wei, ZT
AF Wang, Qi
   Min, Weidong
   Han, Qing
   Liu, Qian
   Zha, Cheng
   Zhao, Haoyu
   Wei, Zitai
TI Inter-Domain Adaptation Label for Data Augmentation in Vehicle
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Semisupervised learning; Cameras; Training data; Data models;
   Adaptation models; Smoothing methods; Vehicle re-identification; domain
   adaptation; semi-supervised learning; multi-domain joint network;
   inter-domain adaptation label smoothing regularization
AB Vehicle re-identification (Re-ID) methods often fail to achieve robust performance due to insufficient training data and domain diversities. Although state-of-the-art methods apply image-to-image translation or web data to achieve data augmentation, the construct of new datasets will not only introduce noise, but also undergo a mismatch issue with the source domain. Moreover, the label noise of cross-domain data in existing label distribution technologies cannot be alleviated. In this paper, a multi-domain joint learning with inter-domain adaptation label smoothing regularization (IALSR) is proposed using a semi-supervised learning framework. The overall framework consists of two parts. In one part, a multi-domain joint network (MJNet) is proposed to learn multiple vehicle attributes simultaneously. The output of the training model is employed to group several inter-domain subsets, which are regarded as different domains. To adapt to domain diversities, style transfer models are learned for each pair of subsets to generate free and rich data as a novel data augmentation approach. In the other part, IALSR, which preserves self-similarity and domain-transitivity, is designed to smooth the noise of style-transferred data. Upon our basis, we further introduce the web data to verify the superiority of the IALSR. The results of extensive experimental on two large-scale vehicle Re-ID datasets demonstrate that the proposed approach is superior to other state-of-the-art ones.
C1 [Wang, Qi; Min, Weidong] Nanchang Univ, Sch Software, Nanchang, Jiangxi, Peoples R China.
   [Wang, Qi; Min, Weidong] Jiangxi Key Lab Smart City, Nanchang 330047, Jiangxi, Peoples R China.
   [Han, Qing; Liu, Qian; Zha, Cheng; Zhao, Haoyu; Wei, Zitai] Nanchang Univ, Sch Informat Engn, Nanchang 330031, Jiangxi, Peoples R China.
C3 Nanchang University; Nanchang University
RP Min, WD (corresponding author), Nanchang Univ, Sch Software, Nanchang, Jiangxi, Peoples R China.; Min, WD (corresponding author), Jiangxi Key Lab Smart City, Nanchang 330047, Jiangxi, Peoples R China.
EM qiwang@email.ncu.edu.cn; minweidong@ncu.edu.cn; hanqing@ncu.edu.cn;
   liuqian.ncu@gmail.com; zhacheng960507@163.com;
   zhaohaoyu@email.ncu.edu.cn; weizitai@email.ncu.edu.cn
RI Han, Qing-Long/B-6635-2013; han, qing/KCZ-0174-2024; Min,
   Weidong/D-4585-2017
OI Han, Qing-Long/0000-0002-7207-0716; Zhao, Haoyu/0000-0003-3832-6439;
   Wang, Qi/0009-0001-3708-6560; Min, Weidong/0000-0003-2526-2181
FU National Natural Science Foundation of China [62076117, 61762061];
   Natural Science Foundation of Jiangxi Province, China [20161ACB20004];
   Jiangxi Key Laboratory of Smart City [20192BCD40002]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62076117 and 61762061, in part by the
   Natural Science Foundation of Jiangxi Province, China under Grant
   20161ACB20004, and in part by the Jiangxi Key Laboratory of Smart City
   under Grant 20192BCD40002. The associate editor coordinating the
   reviewof this manuscript and approving it for publication was Prof.
   Yazhou Yao.
CR Alfasly SAS, 2019, IEEE IMAGE PROC, P3118, DOI [10.1109/icip.2019.8803366, 10.1109/ICIP.2019.8803366]
   Bashir RMS, 2019, PATTERN RECOGN, V90, P52, DOI 10.1016/j.patcog.2019.01.008
   Chen T. S, 2020, P EUR C COMP VIS, P603
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Guo HY, 2019, IEEE T IMAGE PROCESS, V28, P4328, DOI 10.1109/TIP.2019.2910408
   Guo HY, 2018, AAAI CONF ARTIF INTE, P6853
   Hou JH, 2019, IEEE T VEH TECHNOL, V68, P8512, DOI 10.1109/TVT.2019.2927353
   Hou JH, 2019, NEUROCOMPUTING, V345, P15, DOI 10.1016/j.neucom.2018.11.088
   Hou RB, 2019, PROC CVPR IEEE, P9309, DOI 10.1109/CVPR.2019.00954
   Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117
   Lee D.-H., 2013, WORKSHOP CHALLENGES, V3, P896
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu JX, 2018, PROC CVPR IEEE, P4099, DOI 10.1109/CVPR.2018.00431
   Liu XB, 2018, IEEE INT CON MULTI
   Liu XC, 2016, IEEE INT CON MULTI
   Liu XC, 2018, IEEE T MULTIMEDIA, V20, P645, DOI 10.1109/TMM.2017.2751966
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Matsukawa T, 2020, IEEE T PATTERN ANAL, V42, P2179, DOI 10.1109/TPAMI.2019.2914686
   Meng DC, 2020, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR42600.2020.00713
   Min WD, 2018, IEEE T INTELL TRANSP, V19, P174, DOI 10.1109/TITS.2017.2756989
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Salimans T., 2016, P 30 C NEUR INF PROC, P2234
   Si JL, 2018, PROC CVPR IEEE, P5363, DOI 10.1109/CVPR.2018.00562
   Sun YF, 2019, PROC CVPR IEEE, P393, DOI 10.1109/CVPR.2019.00048
   Tay CP, 2019, PROC CVPR IEEE, P7127, DOI 10.1109/CVPR.2019.00730
   Wang Q, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-2811-8
   Wang ZD, 2017, IEEE I CONF COMP VIS, P379, DOI 10.1109/ICCV.2017.49
   Xiong X, 2020, APPL INTELL, V50, P3521, DOI 10.1007/s10489-020-01751-y
   Yang H, 2021, IEEE T MULTIMEDIA, V23, P572, DOI 10.1109/TMM.2020.2985536
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2021, IEEE T MULTIMEDIA, V23, P2683, DOI 10.1109/TMM.2020.3014488
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhou LH, 2020, IEEE T VEH TECHNOL, V69, P3604, DOI 10.1109/TVT.2020.2969427
   Zhou Y, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00679
   Zhu JQ, 2020, IEEE T INTELL TRANSP, V21, P410, DOI 10.1109/TITS.2019.2901312
   Zhu JQ, 2019, MULTIMED TOOLS APPL, V78, P29043, DOI 10.1007/s11042-018-6270-4
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 44
TC 24
Z9 25
U1 5
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1031
EP 1041
DI 10.1109/TMM.2021.3104141
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ZN1SD
UT WOS:000764821800002
DA 2024-07-18
ER

PT J
AU Yan, YT
   Liu, CC
   Chen, CY
   Sun, XF
   Jin, LC
   Peng, XY
   Zhou, X
AF Yan, Yitong
   Liu, Chuangchuang
   Chen, Changyou
   Sun, Xianfang
   Jin, Longcun
   Peng, Xinyi
   Zhou, Xiang
TI Fine-Grained Attention and Feature-Sharing Generative Adversarial
   Networks for Single Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Feature extraction; Superresolution; Gallium nitride;
   Generative adversarial networks; Image reconstruction; Standards;
   Feature-sharing; fine-grained attention; generative adversarial network;
   image super-resolution
ID QUALITY ASSESSMENT
AB Traditional super-resolution (SR) methods by minimize the mean square error usually produce images with over-smoothed and blurry edges, due to the lack of high-frequency details. In this paper, we propose two novel techniques within the generative adversarial network framework to encourage generation of photo-realistic images for image super-resolution. Firstly, instead of producing a single score to discriminate real and fake images, we propose a variant, called Fine-grained Attention Generative Adversarial Network (FASRGAN), to discriminate each pixel of real and fake images. FASRGAN adopts a UNet-like network as the discriminator with two outputs: an image score and an image score map. The score map has the same spatial size as the HR/SR images, serving as the fine-grained attention to represent the degree of reconstruction difficulty for each pixel. Secondly, instead of using different networks for the generator and the discriminator, we introduce a feature-sharing variant (denoted as Fs-SRGAN) for both the generator and the discriminator. The sharing mechanism can maintain model express power while making the model more compact, and thus can improve the ability of producing high-quality images. Quantitative and visual comparisons with state-of-the-art methods on benchmark datasets demonstrate the superiority of our methods. We further apply our super-resolution images for object recognition, which further demonstrates the effectiveness of our proposed method. The code is available at https://github.com/Rainyfish/FASRGAN-and-Fs-SRGAN.
C1 [Yan, Yitong; Liu, Chuangchuang; Jin, Longcun; Peng, Xinyi] South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
   [Chen, Changyou] State Univ, Dept Comp Sci & Engn, New York, NY 14260 USA.
   [Sun, Xianfang] Cardiff Univ, Sch Comp Sci Informat, Cardiff CF24 3AA, Wales.
   [Zhou, Xiang] City Univ Hong Kong, Sch Data Sci, Coll Sci, Hong Kong, Peoples R China.
   [Zhou, Xiang] City Univ Hong Kong, Dept Math, Coll Sci, Hong Kong, Peoples R China.
C3 South China University of Technology; Cardiff University; City
   University of Hong Kong; City University of Hong Kong
RP Jin, LC (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510006, Peoples R China.
EM seyannis_yan@mail.scut.edu.cn; selcc@mail.scut.edu.cn;
   changyouli@buffalo.edu; sunx2@cardiff.ac.uk; lcjin@scut.ddu.cn;
   adxypeng@scut.edu.cn; Xiang.Zhou@cityu.edu.hk
RI Peng, Xinyi/JQJ-4691-2023; Sun, Xianfang/ABG-8970-2021
OI Liu, Chuangchuang/0000-0002-7120-3601; Jin, Longcun/0000-0002-1300-345X
FU Pearl River Technology Nova Project [201710010020]; National Science
   Foundation of China [61300135]
FX This work was supported in part by the Pearl River Technology Nova
   Project under Grant 201710010020 and in part by the National Science
   Foundation of China under Grant 61300135. The associate editor
   coordinating the reviewof this manuscript and approving it for
   publication was Dr. Xavier Giro-i-Nieto.
CR Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Blau Y, 2019, LECT NOTES COMPUT SC, V11133, P334, DOI 10.1007/978-3-030-11021-5_21
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cai JR, 2019, IEEE I CONF COMP VIS, P3086, DOI 10.1109/ICCV.2019.00318
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Fritsche M, 2019, IEEE INT CONF COMP V, P3599, DOI 10.1109/ICCVW.2019.00445
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Jolicoeur-Martineau A., 2019, P INT C LEARN REPR
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kim JH, 2018, IEEE COMPUT SOC CONF, P913, DOI 10.1109/CVPRW.2018.00124
   Kingma D. P., 2014, arXiv
   Kumar N, 2016, IEEE T MULTIMEDIA, V18, P1504, DOI 10.1109/TMM.2016.2571625
   Lai WS, 2019, IEEE T PATTERN ANAL, V41, P2599, DOI 10.1109/TPAMI.2018.2865304
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu CC, 2019, IEEE ACCESS, V7, P60572, DOI 10.1109/ACCESS.2019.2915943
   Lugmayr A, 2019, IEEE INT CONF COMP V, P3575, DOI 10.1109/ICCVW.2019.00442
   Ma C, 2017, COMPUT VIS IMAGE UND, V158, P1, DOI 10.1016/j.cviu.2016.12.009
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mechrez R, 2019, LECT NOTES COMPUT SC, V11363, P427, DOI 10.1007/978-3-030-20893-6_27
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P328, DOI 10.1109/TMM.2018.2863602
   Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang L, 2020, INT J COMPUT VISION, V128, P479, DOI 10.1007/s11263-019-01253-6
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang WL, 2019, IEEE I CONF COMP VIS, P3096, DOI 10.1109/ICCV.2019.00319
   Zhang Yang, 2019, INT C LEARN REPR
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
NR 50
TC 31
Z9 32
U1 6
U2 47
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1473
EP 1487
DI 10.1109/TMM.2021.3065731
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, XY
   Song, YK
   Li, ZP
   Jiang, JM
AF Zhang, Xiaoyan
   Song, Yukai
   Li, Zhuopeng
   Jiang, Jianmin
TI PR-RL: Portrait Relighting Via Deep Reinforcement Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Geometry; Faces; Reinforcement learning; Task analysis;
   Rendering (computer graphics); Image resolution; Image editing; portrait
   relighting; deep reinforcement learning
ID NETWORK; MODELS
AB In this paper, we propose a portrait relighting method based on deep reinforcement learning (called PR-RL). Our PR-RL model could conduct portrait relighting by sequentially predicting local light editing strokes, and use strokes to conduct dodge and burn operations on the image lightness, simulating image editing by artists using brush strokes. Reinforcement learning with Deep Deterministic Policy Gradient is introduced to design our PR-RL model, defining the action (stroke parameters) in a continuous space, through which a reward can be designed to guide the agent to learn and relight a portrait image like an artist. To optimize the relighting effect, we further enable the reward to be location relevant and hence a coarse-to-fine strategy can be applied to select corresponding actions and maximize the performance of the proposed method. In comparison with the existing efforts, our proposed PR-RL method is locally effective, scale-invariant and interpretable. We apply the proposed method to tasks of portrait relighting based on both SH-lighting and reference images. The experiments show that our PR-RL method outperforms state-of-the-art methods in generating locally effective and interpretable high resolution relighting results for wild portrait images.
C1 [Zhang, Xiaoyan; Song, Yukai; Li, Zhuopeng; Jiang, Jianmin] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
   [Zhang, Xiaoyan; Song, Yukai; Li, Zhuopeng; Jiang, Jianmin] Shenzhen Univ, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518000, Peoples R China.
C3 Shenzhen University; Guangming Laboratory; Shenzhen University
RP Jiang, JM (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518000, Peoples R China.
EM xyzhang15@szu.edu.cn; songyukai2019@email.szu.edu.cn;
   lizhuopeng2017@email.szu.edu.cn; jianmin.jiang@szu.edu.cn
FU Chinese Natural Science Foundation [62032015, 61620106008]; Guangdong
   Basic and Applied Basic Research Fund [2020B1515120047]
FX This work was supported by Chinese Natural Science Foundation under
   Grants 62032015, 61620106008, and Guangdong Basic and Applied Basic
   Research Fund under Grant 2020B1515120047.
CR [Anonymous], 2013, P NEURIPS DEEP LEARN
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153
   Chen XW, 2013, IEEE T IMAGE PROCESS, V22, P4249, DOI 10.1109/TIP.2013.2271548
   Chen XW, 2012, COMPUT GRAPH FORUM, V31, P1425, DOI 10.1111/j.1467-8659.2012.03138.x
   Egger B, 2018, INT J COMPUT VISION, V126, P1269, DOI 10.1007/s11263-018-1064-8
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Furuta R, 2020, IEEE T MULTIMEDIA, V22, P1704, DOI 10.1109/TMM.2019.2960636
   Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464
   Goldwaser A, 2020, AAAI CONF ARTIF INTE, V34, P1701
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3181974
   Huang ZW, 2019, IEEE I CONF COMP VIS, P8708, DOI 10.1109/ICCV.2019.00880
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Joshi Nikhil, 2020, Journal of Threatened Taxa, V12, P15173, DOI 10.11609/jott.5041.12.1.15173-15180
   Kajiya J. T., 1986, SIGGRAPH, P143, DOI 10.1145/15886.15902
   Kanamori Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275104
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Katuk Norliza, 2019, International Journal of Interactive Mobile Technologies, V13, P102, DOI 10.3991/ijim.v13i02.10166
   King DE, 2009, J MACH LEARN RES, V10, P1755
   Konda VR, 2000, ADV NEUR IN, V12, P1008
   Kosugi S, 2020, AAAI CONF ARTIF INTE, V34, P11296
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Lillicrap Timothy P, 2015, ARXIV150902971
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nestmeyer T, 2020, PROC CVPR IEEE, P5123, DOI 10.1109/CVPR42600.2020.00517
   Peng YX, 2020, IEEE T MULTIMEDIA, V22, P2061, DOI 10.1109/TMM.2019.2951462
   Prautzsch H., 2002, BEZIER B SPLINE TECH, P9
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578
   Shu ZX, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2926713
   Sun TC, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3323008
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZB, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3414685.3417824
   WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698
   Wu SZ, 2020, PROC CVPR IEEE, P1, DOI 10.1109/CVPR42600.2020.00008
   Xu N, 2020, IEEE T MULTIMEDIA, V22, P1372, DOI 10.1109/TMM.2019.2941820
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Ye DH, 2020, AAAI CONF ARTIF INTE, V34, P6672
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang XY, 2021, IEEE T MULTIMEDIA, V23, P2545, DOI 10.1109/TMM.2020.3013350
   Zhao XY, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P95, DOI 10.1145/3240323.3240374
   Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994
   Zhou H, 2019, IEEE I CONF COMP VIS, P7193, DOI 10.1109/ICCV.2019.00729
   Zhu H., 2020, P ADV NEURAL INF PRO, V33, P21699
   Zhu XY, 2019, IEEE T PATTERN ANAL, V41, P78, DOI 10.1109/TPAMI.2017.2778152
NR 52
TC 3
Z9 3
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 14
PY 2021
VL 24
BP 3240
EP 3255
DI 10.1109/TMM.2021.3096009
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NU
UT WOS:000824707900001
DA 2024-07-18
ER

PT J
AU Jian, LH
   Rayhana, R
   Ma, L
   Wu, SW
   Liu, Z
   Jiang, HQ
AF Jian, Lihua
   Rayhana, Rakiba
   Ma, Ling
   Wu, Shaowu
   Liu, Zheng
   Jiang, Huiqin
TI Infrared and Visible Image Fusion Based on Deep Decomposition Network
   and Saliency Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Image fusion; Task analysis; Fuses; Visualization;
   Image edge detection; Data mining; Bi-direction edge-strength; deep
   decomposition network; image fusion; stacked sparse autoencoder; visual
   saliency mechanism
ID MULTI-FOCUS IMAGE; ENHANCEMENT
AB Traditional image fusion focuses on selecting an effective decomposition approach to extract representative features from the source image and attempts to find appropriate fusion rules to merge extracted features respectively. However, the existing image decomposition tools are mostly based on kernels or global energy-optimized functions limiting the performance of the wide range of image contents. This paper proposes a novel infrared and visible image fusion method based on deep decomposition network and saliency analysis (named DDNSA). First, the modified residual dense network (MRDN) is trained with a publicly available dataset to learn the decomposition process. Second, the structure and texture features of source images are separated by the trained decomposition network. Then, according to the characteristics of the above features, we construct the combination of local and global saliency maps by using stacked sparse autoencoder and visual saliency mechanism to fuse the structural features. Besides, we propose a bi-direction edge-strength fusion strategy for merging the texture features. Finally, the resultant image is reconstructed by combining the fused structure and texture features. The experimental results confirm that our proposed method outperforms the state-of-the-art methods in both visual perception and objective evaluation.
C1 [Jian, Lihua; Ma, Ling; Jiang, Huiqin] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Henan, Peoples R China.
   [Rayhana, Rakiba; Liu, Zheng] Univ British Columbia, Sch Engn, Kelowna, BC V1V 1V7, Canada.
   [Wu, Shaowu] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Hubei, Peoples R China.
C3 Zhengzhou University; University of British Columbia; Wuhan University
RP Ma, L (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Henan, Peoples R China.
EM ielhjian@zzu.edu.cn; rraihana29@gmail.com; ielma@zzu.edu.cn;
   wshaow@126.com; zheng.liu@ubc.ca; iehqjiang@zzu.edu.cn
RI LI, RUIJIAN/GYJ-0470-2022; li, jian/IAQ-2794-2023; Liu,
   Zheng/D-8678-2016
OI Liu, Zheng/0000-0002-7241-3483; Ma, Ling/0000-0002-3424-3540; Jian,
   Lihua/0000-0002-1922-566X; Rayhana, Rakiba/0000-0002-1512-4335
FU Key Project of the National Natural Science Foundation of China (NSFC)
   [U1604262]; Zhengzhou Collaborative Innovation Major Special Project
   [20XTZX11020]
FX This work was supported in part by the Key Project of the National
   Natural Science Foundation of China (NSFC) under Grant U1604262 and in
   part by Zhengzhou Collaborative Innovation Major Special Project under
   Grant 20XTZX11020.
CR [Anonymous], 2018, MULTISPECTRAL IMAGE
   Bai ZX, 2021, POLYM ENG SCI, V61, P278, DOI 10.1002/pen.25574
   Bai ZW, 2019, DISCRETE MATH THEOR, V21
   Bavirisetti DP, 2017, 2017 20TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P701
   Bavirisetti DP, 2016, INFRARED PHYS TECHN, V76, P52, DOI 10.1016/j.infrared.2016.01.009
   Deshmukh M., 2010, Int. J. Image Process. (IJIP), V4, P484
   Fan ZL, 2017, NEUROCOMPUTING, V243, P12, DOI 10.1016/j.neucom.2017.02.066
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Han JG, 2013, NEUROCOMPUTING, V111, P70, DOI 10.1016/j.neucom.2012.12.015
   Han Y, 2013, INFORM FUSION, V14, P127, DOI 10.1016/j.inffus.2011.08.002
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Hu JW, 2012, INFORM FUSION, V13, P196, DOI 10.1016/j.inffus.2011.01.002
   Jian LH, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3022438
   Jian LH, 2018, FUTURE GENER COMP SY, V83, P310, DOI 10.1016/j.future.2018.01.039
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li ST, 2019, IEEE T GEOSCI REMOTE, V57, P6690, DOI 10.1109/TGRS.2019.2907932
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liu Y, 2015, IET IMAGE PROCESS, V9, P347, DOI 10.1049/iet-ipr.2014.0311
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Lu KY, 2018, LECT NOTES COMPUT SC, V11208, P229, DOI 10.1007/978-3-030-01225-0_14
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JL, 2019, NEUROCOMPUTING, V335, P9, DOI 10.1016/j.neucom.2019.01.048
   Ma JL, 2017, INFRARED PHYS TECHN, V82, P8, DOI 10.1016/j.infrared.2017.02.005
   Ma Y, 2016, NEUROCOMPUTING, V202, P12, DOI 10.1016/j.neucom.2016.03.009
   Masi G, 2016, REMOTE SENS-BASEL, V8, DOI 10.3390/rs8070594
   Minaee S., 2020, ARXIV 200105566
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Mou J, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1046, DOI 10.1109/CISP.2013.6745210
   Naidu AR, 2020, INT J SPEECH TECHNOL, V23, P815, DOI 10.1007/s10772-020-09755-2
   Naidu VPS, 2014, J OPT-INDIA, V43, P48, DOI 10.1007/s12596-013-0148-7
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Rajah P., 2018, Remote Sens. Appl.: Soc. Environ., V10, P198, DOI DOI 10.1016/J.RSASE.2018.04.007
   Rao YJ, 1997, MEAS SCI TECHNOL, V8, P355, DOI 10.1088/0957-0233/8/4/002
   Wang ZH, 2021, IEEE T PATTERN ANAL, V43, P3365, DOI 10.1109/TPAMI.2020.2982166
   Xu L, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366158
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yang B, 2014, OPTIK, V125, P4881, DOI 10.1016/j.ijleo.2014.04.036
   Yang B, 2010, IEEE T INSTRUM MEAS, V59, P884, DOI 10.1109/TIM.2009.2026612
   Yang XM, 2018, FUTURE GENER COMP SY, V88, P385, DOI 10.1016/j.future.2018.04.096
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Yuan J, 2015, IEEE T INSTRUM MEAS, V64, P2427, DOI 10.1109/TIM.2015.2407512
   Zhai Y., 2006, P 14 ACM INT C MULT, P815, DOI [DOI 10.1145/1180639.1180824, 10.1145/1180639.1180824]
   Zhang Q, 2021, IEEE T CIRC SYST VID, V31, P1804, DOI 10.1109/TCSVT.2020.3014663
   Zhang Q, 2020, IEEE T IMAGE PROCESS, V29, P3321, DOI 10.1109/TIP.2019.2959253
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zheng YF, 2007, INFORM FUSION, V8, P177, DOI 10.1016/j.inffus.2005.04.003
   Zhou ZQ, 2016, INFORM FUSION, V30, P15, DOI 10.1016/j.inffus.2015.11.003
   Zhou Z, 2014, INFORM FUSION, V20, P60, DOI 10.1016/j.inffus.2013.11.005
   Zhu FD, 2019, IEEE T IMAGE PROCESS, V28, P3556, DOI 10.1109/TIP.2019.2908778
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 60
TC 21
Z9 22
U1 2
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL 9
PY 2021
VL 24
BP 3314
EP 3326
DI 10.1109/TMM.2021.3096088
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2W7NT
UT WOS:000824707800004
DA 2024-07-18
ER

PT J
AU Fan, X
   Cheng, SC
   Huyan, K
   Hou, MJ
   Liu, RS
   Luo, ZX
AF Fan, Xin
   Cheng, Shichao
   Huyan, Kang
   Hou, Minjun
   Liu, Risheng
   Luo, Zhongxuan
TI Dual Neural Networks Coupling Data Regression With Explicit Priors for
   Monocular 3D Face Reconstruction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Face; Image reconstruction; Neural networks;
   Two dimensional displays; Solid modeling; Optimization; 3D face
   reconstruction; Deep optimization; Residual neural networks; Markov
   random fields
ID RECOGNITION; MODEL; RETRIEVAL; SHAPE
AB We address the challenging issue of reconstructing a 3D face from one single image under various expressions and illuminations, which is widely applied in multimedia tasks. Methods built upon classical parametric morphable models (3DMMs) gain success on reconstructing the global geometry of a 3D face, but fail to precisely characterize local facial details. Recently, deep neural networks (DNN) have been applied to the reconstruction that directly predicts depth maps, showing compelling performance on detail recovery. Unfortunately, their reconstruction is prone to structural distortions owing to the lack of explicit prior constraints. In this paper, we propose dual neural networks that optimize one energy coupling data fitting with local explicit geometric prior. Specifically, we build one residual network upon traditional convolution layers in order to directly predict 3D structures by fitting an input image. Meanwhile, we devise a novel architecture stacking shallow networks to refine 3D clouds with geometric priors given by Markov random fields (MRFs). Quantitative evaluations demonstrate the superior performance of the dual networks over either end-to-end DNNs or parametric models. Comparisons with the state-of-the-art also show competitive reconstruction quality on various conditions.
C1 [Fan, Xin; Huyan, Kang; Liu, Risheng; Luo, Zhongxuan] Dalian Univ Technol, DUT RU Int Sch Informat Sci Engn, Dalian 116024, Peoples R China.
   [Cheng, Shichao] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou 310018, Peoples R China.
   [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin 541004, Peoples R China.
C3 Dalian University of Technology; Hangzhou Dianzi University; Guilin
   University of Electronic Technology
RP Fan, X (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat Sci Engn, Dalian 116024, Peoples R China.
EM xin.fan@ieee.org; shichao.cheng@outlook.com; huyankang@hotmail.com;
   houminjun1995@gmail.com; rsliu@dlut.edu.cn; zxluo@dlut.edu.cn
OI Liu, Risheng/0000-0002-9554-0565
FU National Natural Science Foundation of China [61733002, 61922019,
   61672125, 61632019]; Natural Science Foundation of Zhejiang Provincial
   [Q20F020062]; LiaoNing Revitalization Talents Program [XLYC1807088];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61733002, 61922019, 61672125, and
   61632019, in part by the Natural Science Foundation of Zhejiang
   Provincial under Grant Q20F020062, in part by LiaoNing Revitalization
   Talents Program under Grant XLYC1807088, in part by the Fundamental
   Research Funds for the Central Universities under Grant DUT19TD19, and
   in part by the Fundamental Research Funds for the Central Universities.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Hantao Liu.
CR Aldrian O, 2013, IEEE T PATTERN ANAL, V35, P1080, DOI 10.1109/TPAMI.2012.206
   Amberg B, 2007, IEEE I CONF COMP VIS, P1326
   [Anonymous], 2015, ARXIV150906161
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556
   Booth J, 2017, PROC CVPR IEEE, P5464, DOI 10.1109/CVPR.2017.580
   Booth J, 2016, PROC CVPR IEEE, P5543, DOI 10.1109/CVPR.2016.598
   Cao C, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601204
   Cao C, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2462012
   Castelan Mario, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563049
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dou P, 2017, PROC CVPR IEEE, P1503, DOI 10.1109/CVPR.2017.164
   Fan X, 2018, IEEE T MULTIMEDIA, V20, P567, DOI 10.1109/TMM.2017.2751143
   Feng MT, 2018, LECT NOTES COMPUT SC, V11214, P508, DOI 10.1007/978-3-030-01249-6_31
   Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33
   Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874
   Gerig T, 2018, IEEE INT CONF AUTOMA, P75, DOI 10.1109/FG.2018.00021
   Guo YD, 2019, IEEE T PATTERN ANAL, V41, P1294, DOI 10.1109/TPAMI.2018.2837742
   He Y, 2019, IEEE I CONF COMP VIS, P2111, DOI 10.1109/ICCV.2019.00220
   Huber Patrik, 2016, VISIGRAPP 2016. 11th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. Proceedings: VISAPP 2016, P79
   Jackson AS, 2017, IEEE I CONF COMP VIS, P1031, DOI 10.1109/ICCV.2017.117
   Jing CC, 2019, IEEE T MULTIMEDIA, V21, P782, DOI 10.1109/TMM.2018.2866222
   Kenny S, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3301411
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Li S. Z., 1994, Computer Vision - ECCV '94. Third European Conference on Computer Vision. Proceedings. Vol.II, P361, DOI 10.1007/BFb0028368
   Liu F, 2016, LECT NOTES COMPUT SC, V9909, P545, DOI 10.1007/978-3-319-46454-1_33
   Liu RS, 2020, IEEE T PATTERN ANAL, V42, P3027, DOI 10.1109/TPAMI.2019.2920591
   Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767
   Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58
   Phillips PJ, 2005, PROC CVPR IEEE, P947
   Ponce J, 2002, COMPUTER VISION MODE, DOI 10.5555/580035
   Ranzato M., 2006, ADV NEURAL INFORM PR, V19, P1137, DOI DOI 10.7551/MITPRESS/7503.003.0147
   Ren J, 2016, AAAI CONF ARTIF INTE, P3581
   Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589
   Richardson E, 2016, INT CONF 3D VISION, P460, DOI 10.1109/3DV.2016.56
   Romdhani S, 2005, PROC CVPR IEEE, P986
   Romdhani S., 2006, PROC EUR C COMPUT VI, P3
   Sela M, 2017, IEEE I CONF COMP VIS, P1585, DOI 10.1109/ICCV.2017.175
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Tatarchenko M, 2016, LECT NOTES COMPUT SC, V9911, P322, DOI 10.1007/978-3-319-46478-7_20
   Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270
   Tewari A, 2017, IEEE INT CONF COMP V, P1274, DOI 10.1109/ICCVW.2017.153
   Trân AT, 2017, PROC CVPR IEEE, P1493, DOI 10.1109/CVPR.2017.163
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Zhao J., 2017, INT C LEARN REPRESEN
   Zhao XC, 2018, IEEE T MULTIMEDIA, V20, P552, DOI 10.1109/TMM.2017.2750415
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
NR 50
TC 17
Z9 17
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1252
EP 1263
DI 10.1109/TMM.2020.2994506
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XG0UQ
UT WOS:000724477100003
DA 2024-07-18
ER

PT J
AU Hadizadeh, H
   Bajic, IV
AF Hadizadeh, Hadi
   Bajic, Ivan V.
TI Soft Video Multicasting Using Adaptive Compressed Sensing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Receivers; Multicast communication; Bit rate; Visualization; Video
   recording; Quality assessment; OFDM; SoftCast; MultiCast; saliency;
   OFDM; compressed sensing
ID QUALITY ASSESSMENT; TRANSMISSION; MODEL
AB Recently, soft video multicasting has gained a lot of attention, especially in broadcast and mobile scenarios where the bit rate supported by the channel may differ across receivers, and may vary quickly over time. Unlike the conventional designs that force the source to use a single bit rate according to the receiver with the worst channel quality, soft video delivery schemes transmit the video such that the video quality at each receiver is commensurate with its specific instantaneous channel quality. In this paper, we present a soft video multicasting system using an adaptive block-based compressed sensing (BCS) method. The proposed system consists of an encoder, a transmission system, and a decoder. At the encoder side, each block in each frame of the input video is adaptively sampled with a rate that depends on the texture complexity and visual saliency of the block. The obtained BCS samples are then placed into several packets, and the packets are transmitted via a channel-aware OFDM (orthogonal frequency division multiplexing) transmission system with a number of subchannels. At the decoder side, the received BCS samples are first used to build an initial approximation of the transmitted frame. To further improve the reconstruction quality, an iterative BCS reconstruction algorithm is then proposed that uses an adaptive transform and an adaptive soft-thresholding operator, which exploits the temporal similarity between adjacent frames to achieve better reconstruction quality. The extensive objective and subjective experimental results indicate the superiority of the proposed system over the state-of-the-art soft video multicasting systems.
C1 [Hadizadeh, Hadi] Quchan Univ Technol, Quchan 9477167335, Iran.
   [Bajic, Ivan V.] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
C3 Simon Fraser University
RP Hadizadeh, H (corresponding author), Quchan Univ Technol, Quchan 9477167335, Iran.
EM h.hadizadeh@qiet.ac.ir; ibajic@ensc.sfu.ca
RI Bajic, Ivan/I-1241-2013
FU INSF [96010820]
FX This work was supported by the INSF under Grant 96010820.
CR [Anonymous], 2013, PROC IEEE VCIP
   [Anonymous], 2012, GRADUATE STUDIES MAT
   [Anonymous], 2010, The H.264 Advanced Video Compression Standard
   [Anonymous], 2005, Neurobiology of attention
   [Anonymous], 2000, HDB PARAMETRIC NONPA
   [Anonymous], 2007, PROC IEEE C COMPUT V, DOI 10.1109/CVPR.2007.383267
   [Anonymous], DIGITAL IMAGE PROCES
   [Anonymous], 2001, VCEG M
   Bertero M., 1998, Introduction to Inverse Problems in Imaging
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chamaret C, 2010, IEEE IMAGE PROC, P1077, DOI 10.1109/ICIP.2010.5651381
   Chen ZZ, 2010, IEEE INT CON MULTI, P784, DOI 10.1109/ICME.2010.5582549
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Do TT, 2009, IEEE IMAGE PROC, P1393, DOI 10.1109/ICIP.2009.5414631
   DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425
   Fan XP, 2015, IEEE T CIRC SYST VID, V25, P1801, DOI 10.1109/TCSVT.2015.2402831
   Fan XP, 2013, IEEE T CIRC SYST VID, V23, P1040, DOI 10.1109/TCSVT.2013.2249019
   FORNEY GD, 1988, IEEE T INFORM THEORY, V34, P1123, DOI 10.1109/18.21245
   Gan L, 2007, PROCEEDINGS OF THE 2007 15TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING, P403
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   Hamming R. W., 2012, Numerical Methods for Scientists and Engineers
   He CF, 2018, IEEE T IMAGE PROCESS, V27, P3599, DOI 10.1109/TIP.2018.2818019
   He DL, 2015, IEEE T MULTIMEDIA, V17, P1658, DOI 10.1109/TMM.2015.2451956
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jakubczak S., 2011, MITCSAILTR2011008
   Jakubczak S., 2009, P 8 ACM SIGCOMM HOTN, P449
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Khalilian H, 2013, IEEE T IMAGE PROCESS, V22, P4825, DOI 10.1109/TIP.2013.2278463
   Kokalj-Filipovic S, 2012, BELL LABS TECH J, V16, P171, DOI 10.1002/bltj.20540
   KRATOCHVIL T, 2009, LECT NOTES ELECT ENG, V41, P333
   Li Z., 2016, NETFLIX TECH BLOG
   Liu QG, 2013, IEEE T IMAGE PROCESS, V22, P4652, DOI 10.1109/TIP.2013.2277798
   Liu SS, 2017, MULTIMED TOOLS APPL, V76, P15587, DOI 10.1007/s11042-016-3859-3
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   MacKay D., 2003, INFORM THEORY INFERE
   Mihçak MK, 1999, IEEE SIGNAL PROC LET, V6, P300, DOI 10.1109/97.803428
   Mun S, 2009, IEEE IMAGE PROC, P3021, DOI 10.1109/ICIP.2009.5414429
   Mun S, 2011, IEEE DATA COMPR CONF, P183, DOI 10.1109/DCC.2011.25
   Pinson MH, 2004, IEEE T BROADCAST, V50, P312, DOI 10.1109/TBC.2004.834028
   Rhee W, 2000, 2000 IEEE 51ST VEHICULAR TECHNOLOGY CONFERENCE, PROCEEDINGS, VOLS 1-3, P1085, DOI 10.1109/VETECS.2000.851292
   Sampath H, 2002, IEEE COMMUN MAG, V40, P143, DOI 10.1109/MCOM.2002.1031841
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P335, DOI 10.1109/TIP.2009.2034992
   Shen ZK, 2005, IEEE T WIREL COMMUN, V4, P2726, DOI 10.1109/TWC.2005.858010
   Song ZQ, 2014, INTERNATIONAL CONFERENCE ON CONTROL ENGINEERING AND AUTOMATION (ICCEA 2014), P1
   Soundararajan R, 2013, IEEE T CIRC SYST VID, V23, P684, DOI 10.1109/TCSVT.2012.2214933
   TAYLOR MM, 1967, J ACOUST SOC AM, V41, P782, DOI 10.1121/1.1910407
   Wang A, 2015, J NON-OXIDE GLASSES, V7, P1
   Wang AH, 2014, SIGNAL PROCESS-IMAGE, V29, P599, DOI 10.1016/j.image.2014.03.002
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Watson A. B., 1993, DIGITAL IMAGES HUMAN
   Wu F, 2014, IEEE T IMAGE PROCESS, V23, P1015, DOI 10.1109/TIP.2014.2298972
   Xiong RG, 2014, IEEE DATA COMPR CONF, P133, DOI 10.1109/DCC.2014.55
   Xiong RQ, 2016, IEEE T IMAGE PROCESS, V25, P1820, DOI 10.1109/TIP.2016.2535288
   Yu L, 2014, IEEE T CIRC SYST VID, V24, P331, DOI 10.1109/TCSVT.2013.2273675
   Zhang J, 2014, SIGNAL PROCESS, V103, P114, DOI 10.1016/j.sigpro.2013.09.025
   Zhang Z., 2019, P INT JOINT C ART IN, P4376
   Zhang Z, 2018, IEEE T NEUR NET LEAR, V29, P3798, DOI 10.1109/TNNLS.2017.2740224
   Zhang Z, 2016, IEEE T SIGNAL PROCES, V64, P3790, DOI 10.1109/TSP.2016.2550016
   Zhao Zhang, 2019, 2019 IEEE International Conference on Data Mining (ICDM), P836, DOI 10.1109/ICDM.2019.00094
NR 64
TC 8
Z9 8
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 12
EP 25
DI 10.1109/TMM.2020.2975420
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600002
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jiang, B
   Zhou, ZT
   Wang, X
   Tang, J
   Luo, B
AF Jiang, Bo
   Zhou, Zitai
   Wang, Xiao
   Tang, Jin
   Luo, Bin
TI cmSalGAN: RGB-D Salient Object Detection With Cross-View Generative
   Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; Gallium nitride; Feature extraction; Object
   detection; Generative adversarial networks; Fuses; Task analysis; RGB-D
   Saliency Detection; Generative Adversarial Learning; Multi-view Learning
ID FUSION; IMAGE; SEGMENTATION
AB Image salient object detection (SOD) is an active research topic in computer vision and multimedia area. Fusing complementary information of RGB and depth has been demonstrated to be effective for image salient object detection which is known as RGB-D salient object detection problem. The main challenge for RGB-D salient object detection is how to exploit the salient cues of both intra-modality (RGB, depth) and cross-modality simultaneously which is known as cross-modality detection problem. In this paper, we tackle this challenge by designing a novel cross-modality Saliency Generative Adversarial Network (cmSalGAN). cmSalGAN aims to learn an optimal view-invariant and consistent pixel-level representation for RGB and depth images via a novel adversarial learning framework, which thus incorporates both information of intra-view and correlation information of cross-view images simultaneously for RGB-D saliency detection problem. To further improve the detection results, the attention mechanism and edge detection module are also incorporated into cmSalGAN. The entire cmSalGAN can be trained in an end-to-end manner by using the standard deep neural network framework. Experimental results show that cmSalGAN achieves the new state-of-the-art RGB-D saliency detection performance on several benchmark datasets.
C1 [Jiang, Bo] Anhui Univ, Inst Phys Sci & Informat Technol, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc,Minist Edu, Hefei 230601, Peoples R China.
   [Zhou, Zitai; Wang, Xiao; Luo, Bin] Anhui Univ, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
   [Tang, Jin] Anhui Univ, Sch Comp Sci & Technol, Anhui Prov Key Lab Multimodal Cognit Computat, Hefei 230601, Peoples R China.
C3 Anhui University; Anhui University; Anhui University
RP Wang, X (corresponding author), Anhui Univ, Minist Educ, Key Lab Intelligent Comp & Signal Proc, Sch Comp Sci & Technol, Hefei 230601, Peoples R China.
EM jiangbo@ahu.edu.cn; zicair@sina.com; wangxiaocvpr@foxmail.com;
   tj@ahu.edu.cn; luobin@ahu.edu.cn
RI Wang, Xiao/CAG-7835-2022; lu, bin/HPE-4790-2023; LUO, BIN/Y-1233-2018
OI Wang, Xiao/0000-0001-6117-6745; LUO, BIN/0000-0001-5948-5055
FU Major Project for New Generation of AI [2018AAA0100400]; NSFC Key
   Projects of International (Regional) Cooperation and Exchanges
   [61860206004]; Open fund for Discipline Construction, Institute of
   Physical Science and Information Technology, Anhui University
FX This work was supported in part by Major Project for New Generation of
   AI under Grant 2018AAA0100400, NSFC Key Projects of International
   (Regional) Cooperation and Exchanges under Grant 61860206004, Open fund
   for Discipline Construction, Institute of Physical Science and
   Information Technology, Anhui University. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Lamberto Ballan.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2016, NIPS Workshop on Adversarial Training
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Chao FY, 2018, IEEE INT CONF MULTI
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007
   Chen L.-C., 2016, PROC CVPR IEEE, P4545, DOI DOI 10.1109/CVPR.2016.492
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng Y, 2014, IEEE INT CON MULTI
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Dai PY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P677
   De Boer PT, 2005, ANN OPER RES, V134, P19, DOI 10.1007/s10479-005-5724-z
   Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dou Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P691
   Fan D.-P., 2019, ARXIV190706781
   Feng D, 2016, PROC CVPR IEEE, P2343, DOI 10.1109/CVPR.2016.257
   Fernando T, 2018, IEEE WINT CONF APPL, P1539, DOI 10.1109/WACV.2018.00172
   Gammulle H, 2019, IEEE WINT CONF APPL, P200, DOI 10.1109/WACV.2019.00027
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Graves A, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P273, DOI 10.1109/ASRU.2013.6707742
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kingma D. P., 2014, arXiv
   Lekic V, 2019, COMPUT VIS IMAGE UND, V184, P1, DOI 10.1016/j.cviu.2019.04.002
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   Liang HR, 2016, IEEE T MULTIMEDIA, V18, P2271, DOI 10.1109/TMM.2016.2613681
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu ZY, 2020, NEUROCOMPUTING, V387, P210, DOI 10.1016/j.neucom.2020.01.045
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Pan J., 2017, PROC IEEE C COMPUT V
   Park JH, 2020, IEEE T MULTIMEDIA, V22, P2262, DOI 10.1109/TMM.2017.2757759
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Peng YX, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3284750
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Piao YR, 2019, IEEE I CONF COMP VIS, P7253, DOI 10.1109/ICCV.2019.00735
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rosani A, 2015, IEEE T MULTIMEDIA, V17, P1359, DOI 10.1109/TMM.2015.2441003
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang W., 2019, ARXIV190409146
   Wang X, 2019, SIGNAL PROCESS-IMAGE, V75, P158, DOI 10.1016/j.image.2019.03.012
   Wang YL, 2020, IEEE T MULTIMEDIA, V22, P1796, DOI 10.1109/TMM.2019.2949872
   Xie D, 2020, IEEE T IMAGE PROCESS, V29, P3626, DOI 10.1109/TIP.2020.2963957
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Yu BT, 2019, IEEE T MED IMAGING, V38, P1750, DOI 10.1109/TMI.2019.2895894
   Yu JG, 2016, IEEE T MULTIMEDIA, V18, P273, DOI 10.1109/TMM.2015.2505908
   Zhang J, 2020, IEEE T CYBERNETICS, V50, P489, DOI 10.1109/TCYB.2018.2868826
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao LJ, 2019, PATTERN RECOGN, V88, P356, DOI 10.1016/j.patcog.2018.11.028
NR 65
TC 59
Z9 63
U1 3
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1343
EP 1353
DI 10.1109/TMM.2020.2997184
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200013
DA 2024-07-18
ER

PT J
AU Liu, AA
   Wang, YH
   Xu, N
   Nie, WZ
   Nie, J
   Zhang, YD
AF Liu, An-An
   Wang, Yanhui
   Xu, Ning
   Nie, Weizhi
   Nie, Jie
   Zhang, Yongdong
TI Adaptively Clustering-Driven Learning for Visual Relationship Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Task analysis; Semantics; Proposals; Object detection;
   Feature extraction; Portable computers; Adaptively clustering-driven
   learning; translation embedding unit; visual relationship detection
AB Visual relationship detection aims to describe the interactions between pairs of objects, such as person-ride-bike and bike-next to-car triplets. In reality, it is often the case that there exist some groups of strongly correlated relationships, while others are weakly related. Intuitively, the common relationships can be roughly categorized into several types such as geometric (e.g., next to), action (e.g., ride), and so on. However, previous studies ignore the relatedness discovery among multiple relationships, which only lie on a unified space to leverage visual features or statistical dependencies into categories. To tackle this problem, we propose an adaptively clustering-driven network for visual relationship detection, which can implicitly divide the unified relationship space into several subspaces with specific characteristics. Particularly, we propose two novel modules to discover the common distribution space and latent relationship association, respectively, which map pairs of object features into translation subspaces to induce the discriminative relationship clustering. Then, a fused inference is designed to integrate the group-induced representations with the language prior to facilitate the predicate inference. Especially, we design the Frobenius-norm regularization to boost the clustering. To the best of our knowledge, the proposed method is the first supervised framework to realize subject-predicate-object relationship-aware clustering for visual relationship detection. Extensive experiments show that the proposed method can achieve competing performances against the state-of-the-art methods on the Visual Genome dataset. Additional ablation studies further validate its effectiveness.
C1 [Liu, An-An; Wang, Yanhui; Xu, Ning; Nie, Weizhi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Nie, Jie] Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
   [Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Tianjin University; Ocean University of China; Chinese Academy of
   Sciences; University of Science & Technology of China, CAS
RP Xu, N (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.; Nie, J (corresponding author), Ocean Univ China, Coll Informat Sci & Engn, Qingdao 266100, Peoples R China.
EM anan0422@gmail.com; wangyanhui@tju.edu.cn; ningxu@tju.edu.cn;
   weizhinie@tju.edu.cn; niejie@ouc.edu.cn; zhyd73@ustc.edu.cn
RI Nie, Jie/ABG-9228-2021; wang, yanhui/HPG-3348-2023
OI Nie, Jie/0000-0003-4952-7666; nie, weizhi/0000-0002-0578-8138
FU National Key Research and Development Program of China [2020YFB1406602];
   National Natural Science Foundation of China [61772359, 61525206,
   62002257, 61702471]; Tianjin New Generation Artificial Intelligence
   Major Program [19ZXZNGX00110, 18ZXZNGX00150]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1406602, in part by the
   National Natural Science Foundation of China under Grants 61772359,
   61525206, 62002257, and 61702471, and in part by the Grant of Tianjin
   New Generation Artificial Intelligence Major Program (19ZXZNGX00110,
   18ZXZNGX00150). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Jianguo Zhang.
CR [Anonymous], 2013, NIPS
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chen YR, 2018, INFORM SCIENCES, V432, P559, DOI 10.1016/j.ins.2017.08.035
   Cheng ZY, 2019, ACM T INFORM SYST, V37, DOI 10.1145/3291060
   Cui CR, 2019, IEEE T MULTIMEDIA, V21, P1209, DOI 10.1109/TMM.2018.2875357
   Dai B, 2017, PROC CVPR IEEE, P3298, DOI 10.1109/CVPR.2017.352
   Divvala SK, 2009, PROC CVPR IEEE, P1271, DOI 10.1109/CVPRW.2009.5206532
   Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456
   Gong Pinghua, 2012, KDD, V2012, P895
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Holub AD, 2005, IEEE I CONF COMP VIS, P136
   Hwang SJ, 2018, PROC CVPR IEEE, P1014, DOI 10.1109/CVPR.2018.00112
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li SQ, 2019, IEEE T MULTIMEDIA, V21, P2361, DOI 10.1109/TMM.2019.2900134
   Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142
   Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766
   Lin YK, 2015, AAAI CONF ARTIF INTE, P2181
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu C, 2020, P IEEECVF C COMPUTER, p10 918
   Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51
   Murtaza F, 2018, IEEE IMAGE PROC, P3993, DOI 10.1109/ICIP.2018.8451255
   Newell A., 2017, ADV NEUR IN, P2171
   Norcliffe-Brown W, 2018, ADV NEUR IN, V31
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Ramanathan V, 2015, PROC CVPR IEEE, P1100, DOI 10.1109/CVPR.2015.7298713
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sadeghi MA, 2011, PROC CVPR IEEE, P1745, DOI 10.1109/CVPR.2011.5995711
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Ubalde S, 2016, IEEE IMAGE PROC, P3051, DOI 10.1109/ICIP.2016.7532920
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Xu N, 2021, IEEE T CIRC SYST VID, V31, P1031, DOI 10.1109/TCSVT.2020.2990989
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yatskar M, 2016, PROC CVPR IEEE, P5534, DOI 10.1109/CVPR.2016.597
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang F, 2016, NEUROCOMPUTING, V187, P75, DOI 10.1016/j.neucom.2015.07.132
   Zhang HW, 2017, PROC CVPR IEEE, P3107, DOI 10.1109/CVPR.2017.331
   Zhao N, 2017, IEEE T MULTIMEDIA, V19, P2080, DOI 10.1109/TMM.2017.2722687
   Zhu L, 2017, IEEE T MULTIMEDIA, V19, P2066, DOI 10.1109/TMM.2017.2729025
   Zhuang BH, 2017, IEEE I CONF COMP VIS, P589, DOI 10.1109/ICCV.2017.71
NR 51
TC 17
Z9 17
U1 4
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4515
EP 4525
DI 10.1109/TMM.2020.3043084
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800010
DA 2024-07-18
ER

PT J
AU Mesfin, G
   Saleme, EB
   Ademoye, OA
   Kani-Zabihi, E
   Santos, CAS
   Ghinea, G
AF Mesfin, Gebremariam
   Saleme, Estevao Bissoli
   Ademoye, Oluwakemi A.
   Kani-Zabihi, Elahe
   Santos, Celso A. S.
   Ghinea, Gheorghita
TI Less is (Just as Good as) More-an Investigation of Odor Intensity and
   Hedonic Valence in Mulsemedia QoE using Heart Rate and Eye Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality of experience; Olfactory; Heart rate; Media; Monitoring;
   Biomedical monitoring; Electroencephalography; QoE; olfaction; hedonic
   valence; intensity; odor hedonic quality; mulsemedia; eye tracking;
   heart rate
ID EXPERIENCE; QUALITY; SENSE
AB Using olfactory media to enhance traditional multimedia content opens up novel opportunities for user interactions. Whilst the influence of olfaction on user experience in mulsemedia (multiple sensorial media) environments has been previously studied, the impact of the fundamental dimensions of scent intensity and valence (odor hedonic dimension or pleasantness) have been largely unexplored. This is precisely what we target in this paper, which reports the results of an empirical investigation examining how scent intensity and valence impact mulsemedia Quality of Experience (QoE). Accordingly, 54 participants were exposed to different odor valences and scent intensity levels when viewing three short multimedia clips. In particular, we examine both subjective (self-reported) as well as objective QoE metrics, as evidenced by user heart rates and eye gaze patterns. Results show that whilst eye gaze patterns are largely unaffected by the experimental conditions, valence does have a statistically significant impact upon user heart rates, as does intensity for two of the three clips employed in our study. In terms of subjective QoE, results indicate that hedonic valence impacts on the sense of reality and enjoyment; however varying odor intensity levels do not seem to differentially impact on user experience, bringing into question the need for strong scent intensities.
C1 [Mesfin, Gebremariam; Santos, Celso A. S.; Ghinea, Gheorghita] Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
   [Saleme, Estevao Bissoli] Fed Inst Espirito Santo, Dept Informat, Vitoria, ES, Brazil.
   [Ademoye, Oluwakemi A.] Univ Wales Trinity St David, Fac Architecture Comp & Engn, Carmarthen, Dyfed, Wales.
   [Kani-Zabihi, Elahe] Univ West London, Sch Comp & Engn, London, England.
C3 Brunel University; Instituto Federal do Espirito Santo (IFES);
   University of Wales Trinity St David; University of West London
RP Ghinea, G (corresponding author), Brunel Univ, Dept Comp Sci, Uxbridge UB8 3PH, Middx, England.
EM estevaobissoli@gmail.com; kemi.ademoye@uwtsd.ac.uk;
   elahe.kani@uwl.ac.uk; saibel@nf.ufes.br
RI SANTOS, CELSO Alberto Saibel/M-9733-2014; Assres, Gebremariam
   Mesfin/AFM-0811-2022; Saleme, Estevao B./AAZ-7161-2020; Ghinea,
   Gheorghita/AAG-6770-2020
OI SANTOS, CELSO Alberto Saibel/0000-0002-3287-5843; Assres, Gebremariam
   Mesfin/0000-0002-6760-690X; Saleme, Estevao B./0000-0003-1856-3824;
   Ghinea, Gheorghita/0000-0003-2578-5580; Ademoye,
   Kemi/0000-0001-9597-4497; Kani-Zabihi, Elahe/0000-0002-5679-8512
FU European Union [688503]; Coordenacao de Aperfeicoamento de Pessoal de
   Nivel Superior - Brasil (CAPES) [88881.187844/2018-01]
FX This work was funded by European Union's Horizon 2020 Research and
   Innovation program under Grant 688503 and in part by the Coordenacao de
   Aperfeicoamento de Pessoal de Nivel Superior - Brasil (CAPES) - Finance
   Code 88881.187844/2018-01.
CR Ademoye OA, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2957753
   Ademoye OA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2487268.2487270
   Ademoye OA, 2009, IEEE T MULTIMEDIA, V11, P561, DOI 10.1109/TMM.2009.2012927
   [Anonymous], 2007, INT J LEADERSH EDUC
   [Anonymous], 2016, IEEE, DOI DOI 10.1109/QOMEX.2016.7498964
   Baus O, 2017, VIRTUAL REAL-LONDON, V21, P59, DOI 10.1007/s10055-016-0299-3
   Bensafi M, 2002, CHEM SENSES, V27, P703, DOI 10.1093/chemse/27.8.703
   Brendl CM, 1996, ADV EXP SOC PSYCHOL, V28, P95, DOI 10.1016/S0065-2601(08)60237-3
   Brunnstrom K., 2013, QUAL M NOV SAD MARCH
   Buscher G., 2010, P 28 INT C HUM FACT, P3307, DOI DOI 10.1145/1753846.1753976
   Chen M. C., 2001, CHI 01 HUM FACT COMP, P281, DOI DOI 10.1145/634067.634234
   Cherninskii AA, 2009, NEUROPHYSIOLOGY+, V41, P63, DOI 10.1007/s11062-009-9078-z
   Ciubotaru B, 2014, IEEE T BROADCAST, V60, P50, DOI 10.1109/TBC.2013.2290238
   Covaci A, 2018, PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON MANAGEMENT OF DIGITAL ECOSYSTEMS (MEDES'18), P176, DOI 10.1145/3281375.3281387
   Covaci A, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3233774
   Covaci A, 2018, MULTIMED TOOLS APPL, V77, P21245, DOI 10.1007/s11042-017-5459-2
   Dalmaijer E. S., 2014, PEERJ PREPRINTS
   Dalmaijer ES, 2014, BEHAV RES METHODS, V46, P913, DOI 10.3758/s13428-013-0422-2
   de Zambotti M, 2016, PHYSIOL BEHAV, V158, P143, DOI 10.1016/j.physbeh.2016.03.006
   Dinh HQ, 1999, P IEEE VIRT REAL ANN, P222, DOI 10.1109/VR.1999.756955
   Durand K, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070677
   Frasnelli J., WAY UNDERSTAND WE PE
   Ghinea G, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2071396.2071398
   Ghinea G, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2617994
   Greene J., 2005, Learning to use statistical tests in psychology
   Gulliver SR, 2004, IEEE T SYST MAN CY A, V34, P472, DOI 10.1109/TSMCA.2004.826309
   Hussain N., 2018, IEEE INT C MULTIMEDI, P1
   Jones L, 2004, HUMAN PERFORMANCE, SITUATION AWARENESS AND AUTOMATION: CURRENT RESEARCH AND TRENDS, VOL 2, P282
   Kim EY, 2005, IEEE ICCE, P207
   Kocejko T, 2008, EUROGR TECH REP SER, P193, DOI 10.1109/HSI.2008.4581433
   Kroupi E, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2637287
   Lundström JN, 2006, CHEM SENSES, V31, P705, DOI 10.1093/chemse/bjl012
   Mesfin G., 2018, 2018 IEEE INT C MULT, P1
   Murray N, 2017, IEEE T SYST MAN CY-S, V47, P2503, DOI 10.1109/TSMC.2016.2531654
   Murray N, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3108243
   Murray N, 2014, ACM T MULTIM COMPUT, V10, DOI 10.1145/2540994
   Murray Niall., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys'13, P162, DOI [https://doi.org/10.1145/2483977.2483999, DOI 10.1145/2483977.2483999]
   Obrist M, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2843, DOI 10.1145/2556288.2557008
   Ooms K, 2015, J EYE MOVEMENT RES, V8, DOI 10.16910/jemr.8.1.5
   Paas F, 2003, EDUC PSYCHOL-US, V38, P63, DOI 10.1207/S15326985EP3801_8
   Saleme EB, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3319853
   Saleme EB, 2019, MULTIMEDIA SYST, V25, P421, DOI 10.1007/s00530-019-00618-8
   Seo HS, 2010, APPETITE, V54, P544, DOI 10.1016/j.appet.2010.02.011
   Stahl Sarah E, 2016, BMJ Open Sport Exerc Med, V2, pe000106
   Sulema Y, 2016, INT CONF SYST SIGNAL, P19
   Timmerer C, 2014, T-LAB SER TELECOMMUN, P351, DOI 10.1007/978-3-319-02681-7_24
   Tullis T, 2007, AMCIS 2007 P, P133
   Wallen MP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154420
   Winston JS, 2005, J NEUROSCI, V25, P8903, DOI 10.1523/JNEUROSCI.1569-05.2005
   Yazdani A, 2012, INT WORK QUAL MULTIM, P272, DOI 10.1109/QoMEX.2012.6263860
   Yuan ZH, 2014, INT WIREL COMMUN, P1142, DOI 10.1109/IWCMC.2014.6906515
   Yuan ZH, 2015, IEEE T MULTIMEDIA, V17, P104, DOI 10.1109/TMM.2014.2371240
   Yuan ZH, 2014, ACM T MULTIM COMPUT, V11, DOI 10.1145/2661329
   Zhang LK, 2016, LECT NOTES COMPUT SC, V9654, P111, DOI 10.1007/978-3-319-40259-8_10
   Zou LH, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P315, DOI 10.1145/3083187.3084014
NR 55
TC 7
Z9 8
U1 2
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1095
EP 1105
DI 10.1109/TMM.2020.2992948
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300021
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Wang, L
   Lv, X
   Zhang, QL
   Niu, ZX
   Zheng, NN
   Hua, G
AF Wang, Le
   Lv, Xin
   Zhang, Qilin
   Niu, Zhenxing
   Zheng, Nanning
   Hua, Gang
TI Object Cosegmentation in Noisy Videos With Multilevel Hypergraph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Proposals; Computational modeling; Object segmentation; Noise
   measurement; Semantics; Task analysis; Object cosegmentation; hypergraph
   cut; object model; fully convolutional network
ID CO-SEGMENTATION; DISCOVERY
AB With the target of simultaneously segmenting semantically related videos to identify the common objects, video object cosegmentation has attracted the attention of researchers in recent years. Existing methods are primarily based on pair-wise relations between adjacent pixels and regions, which are susceptible to performance degradation from object entries/exists or occlusions. Specifically, we refer these video frames without the common objects present as the "empty" frames. In this paper, we propose a multilevel hypergraph-based full Video object CoSegmentation (VCS) method, which incorporates high-level semantics and low-level appearance/motion/saliency to construct the hyperedge among multiple spatially and temporally adjacent regions. Specifically, the high-level semantic model fuses multiple object proposals from each frame instead of relying on a single object proposal per frame. A hypergraph cut is subsequently utilized to calculate the object cosegmentation. Experiments on four video object segmentation/cosegmentation datasets against state-of-the-art methods with both objective and subjective results manifest the effectiveness of the proposed VCS method, including the SegTrack and VCoSeg datasets without "empty" frames, the XJTU-Stevens dataset with 3.7% "empty" frames, and the Noisy-ViCoSeg dataset proposed together with our method with 30.3% "empty" frames.
C1 [Wang, Le; Lv, Xin; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Zhang, Qilin] HERE Technol, Chicago, IL 60606 USA.
   [Niu, Zhenxing] Alibaba Grp, Hangzhou 311121, Peoples R China.
   [Hua, Gang] Wormpex AI Res, Bellevue, WA 98004 USA.
C3 Xi'an Jiaotong University; Alibaba Group
RP Hua, G (corresponding author), Wormpex AI Res, Bellevue, WA 98004 USA.
EM lewang@mail.xjtu.edu.cn; lvxin1@mail.xjtu.edu.cn; samqzhang@gmail.com;
   niuzhenxing@gmail.com; nnzheng@mail.xjtu.edu.cn; ganghua@gmail.com
RI Zhang, Qilin/HOF-0365-2023; Zhang, Qilin/B-5171-2018
OI Wang, Le/0000-0001-6636-6396; Zhang, Qilin/0000-0002-7917-9749
FU National Key R&D Program of China [2018AAA0101400]; NSFC [61629301,
   61773312, 61976171]; China Postdoctoral Science Foundation
   [2019M653642]; Young Elite Scientists Sponsorship Program by CAST
   [2018QNRC001]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2018AAA0101400, in part by NSFC under Grants 61629301,
   61773312, and 61976171, in part by China Postdoctoral Science Foundation
   under Grant 2019M653642, and in part by Young Elite Scientists
   Sponsorship Program by CAST under Grant 2018QNRC001.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Chen D.-J., 2012, P 20 ACM INT C MULTI, P805, DOI [10.1145/2393347.2396317, DOI 10.1145/2393347.2396317]
   Chen YD, 2019, IEEE T MULTIMEDIA, V21, P1934, DOI 10.1109/TMM.2018.2890361
   Cheng JC, 2018, PROC CVPR IEEE, P7415, DOI 10.1109/CVPR.2018.00774
   Chiu WC, 2013, PROC CVPR IEEE, P321, DOI 10.1109/CVPR.2013.48
   Endres I, 2010, LECT NOTES COMPUT SC, V6315, P575, DOI 10.1007/978-3-642-15555-0_42
   Faktor Alon, 2014, BMVC
   Fragkiadaki K, 2015, PROC CVPR IEEE, P4083, DOI 10.1109/CVPR.2015.7299035
   Fu HZ, 2015, IEEE T IMAGE PROCESS, V24, P3415, DOI 10.1109/TIP.2015.2442915
   Griffin BA, 2019, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2019.00912
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Guo JM, 2013, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2013.278
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Hu YT, 2018, LECT NOTES COMPUT SC, V11205, P813, DOI 10.1007/978-3-030-01246-5_48
   Huang YC, 2009, PROC CVPR IEEE, P1738, DOI 10.1109/CVPRW.2009.5206795
   Khoreva A, 2015, PROC CVPR IEEE, P951, DOI 10.1109/CVPR.2015.7298697
   Kowdle A, 2012, LECT NOTES COMPUT SC, V7576, P789, DOI 10.1007/978-3-642-33715-4_57
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   LI J, 2018, IEEE MULTIMEDIA, V26, P9, DOI DOI 10.1080/1061186X.2017.1363209
   Li KQ, 2016, IEEE T IMAGE PROCESS, V25, P1898, DOI 10.1109/TIP.2016.2526900
   Liu ZY, 2018, IEEE T IMAGE PROCESS, V27, P5840, DOI 10.1109/TIP.2018.2859622
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lou ZY, 2014, IEEE T MULTIMEDIA, V16, P2110, DOI 10.1109/TMM.2014.2363936
   Lu XK, 2019, PROC CVPR IEEE, P3618, DOI 10.1109/CVPR.2019.00374
   Lv X, 2018, IEEE IMAGE PROC, P2207, DOI 10.1109/ICIP.2018.8451806
   Ma JZ, 2017, IEEE T IMAGE PROCESS, V26, P1216, DOI 10.1109/TIP.2016.2631883
   Ma TY, 2012, PROC CVPR IEEE, P670, DOI 10.1109/CVPR.2012.6247735
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Mueen A, 2015, KNOWL INF SYST, V45, P105, DOI 10.1007/s10115-014-0793-4
   Mustafa A, 2017, PROC CVPR IEEE, P5583, DOI 10.1109/CVPR.2017.592
   Panagiotakis C, 2018, PATTERN RECOGN, V79, P1, DOI 10.1016/j.patcog.2018.02.001
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Rubio J.C., 2012, ASIAN C COMPUTER VIS, P13, DOI DOI 10.1007/978-3-642-37444-9
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Tiburzi F, 2008, IEEE IMAGE PROC, P17, DOI 10.1109/ICIP.2008.4711680
   Tsai D., 2010, P BRIT MACH VIS C, P56
   Tsai YH, 2016, LECT NOTES COMPUT SC, V9908, P760, DOI 10.1007/978-3-319-46493-0_46
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Wang C, 2017, IEEE T IMAGE PROCESS, V26, P5825, DOI 10.1109/TIP.2017.2750410
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Wang L, 2017, IEEE T PATTERN ANAL, V39, P2074, DOI 10.1109/TPAMI.2016.2612187
   Wang L, 2014, LECT NOTES COMPUT SC, V8692, P640, DOI 10.1007/978-3-319-10593-2_42
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P3137, DOI 10.1109/TIP.2015.2438550
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Wenguan, 2018, IEEE TPAMI, V41, P985
   Wu OY, 2017, ASIA CONTROL CONF AS, P1417, DOI 10.1109/ASCC.2017.8287380
   Xu L, 2012, IEEE T PATTERN ANAL, V34, P1744, DOI 10.1109/TPAMI.2011.236
   Xu SJ, 2019, PROC CVPR IEEE, P314, DOI 10.1109/CVPR.2019.00040
   Zhang D, 2014, LECT NOTES COMPUT SC, V8695, P551, DOI 10.1007/978-3-319-10584-0_36
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhou D, 2007, 2007 IEEE NORTH-EAST WORKSHOP ON CIRCUITS AND SYSTEMS, P167, DOI 10.1109/CADCG.2007.4407875
NR 56
TC 4
Z9 4
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1287
EP 1300
DI 10.1109/TMM.2020.2995266
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200009
DA 2024-07-18
ER

PT J
AU Wang, Q
   Fan, HJ
   Sun, G
   Ren, WH
   Tang, YD
AF Wang, Qiang
   Fan, Huijie
   Sun, Gan
   Ren, Weihong
   Tang, Yandong
TI Recurrent Generative Adversarial Network for Face Completion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face; Feature extraction; Recurrent neural networks; Generative
   adversarial networks; Semantics; Image restoration; Gallium nitride;
   Recurrent neural network; generative adversarial network; face
   completion; short link
ID NEURAL-NETWORK; IMAGE
AB Most recently-proposed face completion algorithms use high-level features extracted from convolutional neural networks (CNNs) to recover semantic texture content. Although the completed face is natural-looking, the synthesized content still lacks lots of high-frequency details, since the high-level features cannot supply sufficient spatial information for details recovery. To tackle this limitation, in this paper, we propose a Recurrent Generative Adversarial Network (RGAN) for face completion. Unlike previous algorithms, RGAN can take full advantage of multi-level features, and further provide advanced representations from multiple perspectives, which can well restore spatial information and details in face completion. Specifically, our RGAN model is composed of a CompletionNet and a DisctiminationNet, where the CompletionNet consists of two deep CNNs and a recurrent neural network (RNN). The first deep CNN is presented to learn the internal regulations of a masked image and represent it with multi-level features. The RNN model then exploits the relationships among the multi-level features and transfers these features in another domain, which can be used to complete the face image. Benefiting from bidirectional short links, another CNN is used to fuse multi-level features transferred from RNN and reconstruct the face image in different scales. Meanwhile, two context discrimination networks in the DisctiminationNet are adopted to ensure the completed image consistency globally and locally. Experimental results on benchmark datasets demonstrate qualitatively and quantitatively that our model performs better than the state-of-the-art face completion models, and simultaneously generates realistic image content and high-frequency details. The code will be released available soon.
C1 [Wang, Qiang; Fan, Huijie; Sun, Gan; Ren, Weihong; Tang, Yandong] Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.
   [Wang, Qiang; Fan, Huijie; Sun, Gan; Ren, Weihong; Tang, Yandong] Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110016, Peoples R China.
   [Wang, Qiang; Sun, Gan; Ren, Weihong] Univ Chinese Acad Sci, Huairou 100049, Peoples R China.
C3 Chinese Academy of Sciences; Shenyang Institute of Automation, CAS;
   Chinese Academy of Sciences; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Fan, HJ (corresponding author), Chinese Acad Sci, Shenyang Inst Automat, State Key Lab Robot, Shenyang 110016, Peoples R China.; Fan, HJ (corresponding author), Chinese Acad Sci, Inst Robot & Intelligent Mfg, Shenyang 110016, Peoples R China.
EM wangqiang@sia.cn; fanhuijie@sia.cn; sungan@sia.cn; renweihong@sia.cn;
   ytang@sia.cn
RI Sun, Gan/ABD-6793-2021
OI Sun, Gan/0000-0003-1111-6909; Wang, Qiang/0000-0002-2018-1764; ren, wei
   hong/0000-0003-3839-0078; Tang, Yandong/0000-0003-3805-7654
FU National Natural Science Foundation of China [61873259, 61821005];
   Cooperation Projects of CAS ITRI [CAS-ITRI201905]; Key Research and
   Development Program of Liaoning [2019JH2/10100014]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61873259 and 61821005, in part by the
   Cooperation Projects of CAS & ITRI (CAS-ITRI201905), and in part by the
   Key Research and Development Program of Liaoning (2019JH2/10100014).
CR [Anonymous], 2015, P 28 INF C NEUR INF
   [Anonymous], 2006, COMPUTER VISION PATT
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.90
   Barnes C, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531330
   Bertalmio M, 2003, IEEE T IMAGE PROCESS, V12, P882, DOI 10.1109/TIP.2003.815261
   Charalampidis D, 2006, IEEE T IMAGE PROCESS, V15, P777, DOI 10.1109/TIP.2005.860604
   Chen BC, 2014, LECT NOTES COMPUT SC, V8694, P768, DOI 10.1007/978-3-319-10599-4_49
   Chung J., 2014, P C NEUR INF PROC SY
   DARABI S, 2012, ACM T GRAPHIC, V31, DOI DOI 10.1145/2185520.2185578
   Deng JK, 2019, PROC CVPR IEEE, P4685, DOI 10.1109/CVPR.2019.00482
   Deng JK, 2018, PROC CVPR IEEE, P7093, DOI 10.1109/CVPR.2018.00741
   Deng Y, 2011, IEEE T IMAGE PROCESS, V20, P2329, DOI 10.1109/TIP.2011.2109729
   Doersch C, 2014, LECT NOTES COMPUT SC, V8691, P362, DOI 10.1007/978-3-319-10578-9_24
   Dong J., 2020, P IEEE C COMP VIS PA
   Dong JH, 2019, IEEE I CONF COMP VIS, P10711, DOI 10.1109/ICCV.2019.01081
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761
   Elad M, 2017, IEEE T IMAGE PROCESS, V26, P2338, DOI 10.1109/TIP.2017.2678168
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Hays J, 2008, COMMUN ACM, V51, P87, DOI 10.1145/1400181.1400202
   Heusel M., 2017, ADV NEURAL INFORM PR, P6626
   Hu P, 2018, IEEE T MULTIMEDIA, V20, P2814, DOI 10.1109/TMM.2018.2815784
   Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129
   Iizuka S, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073659
   Kumar V, 2016, IEEE T IMAGE PROCESS, V25, P5212, DOI 10.1109/TIP.2016.2605919
   Li F, 2014, IEEE T IMAGE PROCESS, V23, P4242, DOI 10.1109/TIP.2014.2346030
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Li ZD, 2015, IEEE T IMAGE PROCESS, V24, P1138, DOI 10.1109/TIP.2014.2383322
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mohammed U, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531363
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Papadakis N, 2012, IEEE T IMAGE PROCESS, V21, P2513, DOI 10.1109/TIP.2012.2183144
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qu LQ, 2017, PROC CVPR IEEE, P2308, DOI 10.1109/CVPR.2017.248
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Shi X., 2015, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1506.04214
   Talebi H, 2014, IEEE T IMAGE PROCESS, V23, P4460, DOI 10.1109/TIP.2014.2348870
   Tao X., P IEEE C COMP VIS PA, P8174
   Nguyen TD, 2019, IEEE T MULTIMEDIA, V21, P1345, DOI 10.1109/TMM.2018.2880954
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang Q, 2019, PATTERN RECOGN, V88, P493, DOI 10.1016/j.patcog.2018.11.020
   Wang Q, 2019, IEEE SIGNAL PROC LET, V26, P400, DOI 10.1109/LSP.2018.2890205
   Wexler Y, 2004, PROC CVPR IEEE, P120
   Wilczkowiak M., 2008, P BRIT MACH VIS C, P492
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yu JH, 2018, PROC CVPR IEEE, P5505, DOI 10.1109/CVPR.2018.00577
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhang JW, 2018, PROC CVPR IEEE, P2521, DOI 10.1109/CVPR.2018.00267
   Zhang YB, 2018, IEEE T IMAGE PROCESS, V27, P3150, DOI 10.1109/TIP.2018.2812081
   Zhao B, 2017, IEEE T MULTIMEDIA, V19, P1245, DOI 10.1109/TMM.2017.2648498
NR 61
TC 19
Z9 24
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 429
EP 442
DI 10.1109/TMM.2020.2978633
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600034
DA 2024-07-18
ER

PT J
AU Xu, ZY
   Chang, W
   Zhu, YD
   Dong, L
   Zhou, HY
   Zhang, QN
AF Xu, Zongyi
   Chang, Wei
   Zhu, Yindi
   Dong, Le
   Zhou, Huiyu
   Zhang, Qianni
TI Building High-Fidelity Human Body Models From User-Generated Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biological system modeling; Three-dimensional displays; Solid modeling;
   Image reconstruction; Shape; Two dimensional displays; Data models;
   Human body modelling; user-generated data; point clouds; single image;
   virtual dressing
ID RECONSTRUCTION; SHAPE; MOTION
AB We propose a key point-based approach, refers to as KPhub-PC, to estimate high-fidelity human body models from low-quality point clouds acquired with an affordable 3D scanner and a variation KPhub-I that can achieve the same purpose based on low-resolution single images taken by smartphones. In KPhub-PC, a sparse set of key points is annotated to guide the deformation of a parametric 3D human body model SMPL and then a high-fidelity human body model that can explain the target point clouds is built. Besides building 3D human body models from point clouds, KPhub-I is designed to estimate accurate 3D human body models from single 2D images. The SMPL model is fitted to 2D joints and the boundary of the human body which are detected using CNN based methods automatically. Considering that people are in stable poses most of the time, a stable pose prior is defined from CMU motion capture dataset for further improving accuracy. Extensive experiments demonstrate that in both types of user-generated data, the proposed approaches can build believable and animatable human body models robustly. Our approach outperforms the state-of-the-arts in the accuracy of both human body shape and pose estimation.
C1 [Xu, Zongyi] Chongqing Univ Posts & Telecommun, Key Lab Data Engn & Visual Comp, Chongqing 400065, Peoples R China.
   [Chang, Wei; Zhu, Yindi] Beijing Inst Fash Technol, Beijing 100029, Peoples R China.
   [Dong, Le] Univ Elect Sci & Technol China, Chengdu 611731, Peoples R China.
   [Zhou, Huiyu] Univ Leicester, Leicester LE1 7RH, Leics, England.
   [Zhang, Qianni] Queen Mary Univ London, Dept Elect Engn & Comp Sci, London E1 4NS, England.
C3 Chongqing University of Posts & Telecommunications; Beijing Institute of
   Fashion Technology; University of Electronic Science & Technology of
   China; University of Leicester; University of London; Queen Mary
   University London
RP Zhang, QN (corresponding author), Queen Mary Univ London, Dept Elect Engn & Comp Sci, London E1 4NS, England.
EM xuzy@cqupt.edu.cn; changwei_d@126.com; nice_zyd@163.com;
   ledong@uestc.edu.cn; hz143@leicester.ac.uk; qianni.zhang@qmul.ac.uk
RI Zhou, Huiyu/O-2692-2014; Xu, Zongyi/ABH-3489-2020
OI Zhou, Huiyu/0000-0003-1634-9840; Zhang, Qianni/0000-0001-7685-2187
FU Science and Technology Research Program of Chongqing Municipal Education
   Commission [KJQN201900628]; Natural Science Foundation of Chongqing
   [E021D2019034]
FX This work was supported in part by the Science and Technology Research
   Program of Chongqing Municipal Education Commission under Grant
   KJQN201900628 and in part by the Natural Science Foundation of Chongqing
   under Grant E021D2019034. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Sebastian
   Knorr.
CR Alexiadis DS, 2017, IEEE T CIRC SYST VID, V27, P798, DOI 10.1109/TCSVT.2016.2576922
   Alldieck T, 2019, IEEE I CONF COMP VIS, P2293, DOI 10.1109/ICCV.2019.00238
   Allen B, 2003, ACM T GRAPHIC, V22, P587, DOI 10.1145/882262.882311
   Allen B., 2006, P 2006 ACM SIGGRAPHE, P147
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2017, P IEEE C VIS PATT RE
   [Anonymous], 2017, ACM Transactions on Graphics (TOG), DOI DOI 10.1145/3083722
   [Anonymous], 2004, P 2004 EUR ACM SIGGR
   Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34
   Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Chen X, 2020, IEEE T VIS COMPUT GR, V26, P1466, DOI 10.1109/TVCG.2018.2871190
   Chen YP, 2013, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2013.21
   Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38
   Cui Y., 2012, ACCV Workshops, P133
   Dou MS, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925969
   Dou MS, 2015, PROC CVPR IEEE, P493, DOI 10.1109/CVPR.2015.7298647
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Hasler N, 2009, COMPUT GRAPH-UK, V33, P211, DOI 10.1016/j.cag.2009.03.026
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hinton G., 1976, PROC 2 SUMMER C ARTI, P148
   Hirshberg DA, 2012, LECT NOTES COMPUT SC, V7577, P242, DOI 10.1007/978-3-642-33783-3_18
   Innmann M, 2016, LECT NOTES COMPUT SC, V9912, P362, DOI 10.1007/978-3-319-46484-8_22
   Jin HL, 2005, INT J COMPUT VISION, V63, P175, DOI 10.1007/s11263-005-6876-7
   Joo H, 2018, PROC CVPR IEEE, P8320, DOI 10.1109/CVPR.2018.00868
   Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241
   Kanazawa A, 2018, PROC CVPR IEEE, P7122, DOI 10.1109/CVPR.2018.00744
   Kolotouros N, 2019, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2019.00234
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Liu ZB, 2017, IEEE T CYBERNETICS, V47, P695, DOI 10.1109/TCYB.2016.2524406
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Loper M, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661273
   Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11
   Lu Y, 2018, COMPUT ANIMAT VIRT W, V29, DOI 10.1002/cav.1807
   Lun ZL, 2017, INT CONF 3D VISION, P67, DOI 10.1109/3DV.2017.00018
   Ma Qianli, 2019, ARXIV PREPRINT ARXIV
   Mueller F, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3306346.3322958
   Newcombe RA, 2015, PROC CVPR IEEE, P343, DOI 10.1109/CVPR.2015.7298631
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Olson E, 2013, INT J ROBOT RES, V32, P826, DOI 10.1177/0278364913479413
   Orts-Escolano S, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P741, DOI 10.1145/2984511.2984517
   Pavlakos G, 2019, PROC CVPR IEEE, P10967, DOI 10.1109/CVPR.2019.01123
   Pishchulin L, 2017, PATTERN RECOGN, V67, P276, DOI 10.1016/j.patcog.2017.02.018
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Sigal L, 2012, INT J COMPUT VISION, V98, P15, DOI 10.1007/s11263-011-0493-4
   Sminchisescu C, 2003, INT J ROBOT RES, V22, P371, DOI 10.1177/0278364903022006003
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tong J, 2012, IEEE T VIS COMPUT GR, V18, P643, DOI 10.1109/TVCG.2012.56
   Wu ZH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P293, DOI 10.1145/3343031.3351083
   Xu ZQ, 2018, PROCEEDINGS OF 2018 IEEE WORLD SYMPOSIUM ON COMMUNICATION ENGINEERING (WSCE), P1, DOI 10.1109/WSCE.2018.8690540
   Xu ZY, 2018, MULTIMEDIA SYST, V24, P257, DOI 10.1007/s00530-017-0541-1
   Ye Mao, 2014, IEEE Trans Vis Comput Graph, V20, P550, DOI 10.1109/TVCG.2014.35
   Yu T, 2017, IEEE I CONF COMP VIS, P910, DOI 10.1109/ICCV.2017.104
   Zhao TH, 2019, IEEE T MULTIMEDIA, V21, P114, DOI 10.1109/TMM.2018.2844087
   Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976
NR 55
TC 14
Z9 14
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1542
EP 1556
DI 10.1109/TMM.2020.3001540
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300007
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zheng, Y
   Yao, HX
   Sun, XS
AF Zheng, Ying
   Yao, Hongxun
   Sun, Xiaoshuai
TI Deep Semantic Parsing of Freehand Sketches With Homogeneous
   Transformation, Soft-Weighted Loss, and Staged Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Labeling; Task analysis; Training; Image segmentation; Image
   retrieval; Image edge detection; Sketch parsing; homogeneous
   transformation; sketch-based image retrieval; soft-weighted loss; staged
   learning
ID IMAGE SEGMENTATION; RETRIEVAL
AB In this paper, we propose a novel deep framework for part-level semantic parsing of freehand sketches, which makes three main contributions that are experimentally shown to have substantial practical merit. First, we propose a homogeneous transformation method to address the problem of domain adaptation. For the task of sketch parsing, there is no available data of labeled freehand sketches that can be directly used for model training. An alternative solution is to learn from datasets of real image parsing, while the domain adaptation is an inevitable problem. Unlike existing methods that utilize the edge maps of real images to approximate freehand sketches, the proposed homogeneous transformation method transforms the data from domains of real images and freehand sketches into a homogeneous space to minimize the semantic gap. Second, we design a soft-weighted loss function as guidance for the training process, which gives attention to both the ambiguous label boundary and class imbalance. Third, we present a staged learning strategy to improve the parsing performance of the trained model, which takes advantage of the shared information and specific characteristic from different sketch categories. Extensive experimental results demonstrate the effectiveness of the above three methods. Specifically, to evaluate the generalization ability of our homogeneous transformation method, additional experiments for the task of sketch-based image retrieval are conducted on the QMUL FG-SBIR dataset. Finally, by integrating the proposed three methods into a unified framework of deep semantic sketch parsing (DeepSSP), we achieve the state-of-the-art on the public SketchParse dataset.
C1 [Zheng, Ying] Artificial Intelligence Res Inst, Zhejiang Lab, Hangzhou 310023, Peoples R China.
   [Zheng, Ying; Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Sun, Xiaoshuai] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
C3 Zhejiang Laboratory; Harbin Institute of Technology; Xiamen University
RP Yao, HX (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM zhengyinghit@outlook.com; h.yao@hit.edu.cn; xiaoshuaisun@hit.edu.cn
FU China Postdoctoral Science Foundation [U1711265, 61772158]; Research
   Program of Zhejiang Lab [2020M681961];  [2019KD0AC02];  [2020KD0AA02]
FX Manuscript received December 29, 2018; revised December 27, 2019, May
   18, 2020, andAugust 4, 2020; accepted September 22, 2020. Date of
   publication October 2, 2020; date of current version October 19, 2021.
   This work was supported in part by theNationalNatural Science Foundation
   of China underGrants U1711265 and 61772158, in part by China
   Postdoctoral Science Foundation under Grant 2020M681961, and in part by
   the Research Program of Zhejiang Lab under Grants 2019KD0AC02 and
   2020KD0AA02. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Benoit HUET.
   (Corresponding author: Hongxun Yao.)
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bergamo Alessandro, 2010, ADV NEURAL INFORM PR, V23
   Bulò SR, 2017, PROC CVPR IEEE, P7082, DOI 10.1109/CVPR.2017.749
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Chen T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618470
   Chen T, 2013, IEEE T VIS COMPUT GR, V19, P824, DOI 10.1109/TVCG.2012.148
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Choi J, 2019, IEEE T MULTIMEDIA, V21, P2083, DOI 10.1109/TMM.2019.2892301
   Das Bhattacharjee S, 2018, IEEE T MULTIMEDIA, V20, P2761, DOI 10.1109/TMM.2018.2814338
   Das Bhattacharjee S, 2015, COMPUT VIS IMAGE UND, V139, P73, DOI 10.1016/j.cviu.2015.06.005
   Dong J, 2014, PROC CVPR IEEE, P843, DOI 10.1109/CVPR.2014.113
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185540
   Farhadi A, 2010, PROC CVPR IEEE, P2352, DOI 10.1109/CVPR.2010.5539924
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2016, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2016.349
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang Z, 2014, ACM T GRAPHIC, V33, DOI [10.1145/2661229.2661280, 10.1145/2661228.2661280]
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li K, 2018, LECT NOTES COMPUT SC, V11212, P593, DOI 10.1007/978-3-030-01237-3_36
   Li X., 2020, P IEEE CVF C COMP VI, P8950, DOI 10.1109/CVPR42600.2020.00897
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Li Y, 2015, COMPUT VIS IMAGE UND, V137, P1, DOI 10.1016/j.cviu.2015.02.003
   Liang XD, 2016, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2016.347
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Ma CX, 2012, IEEE T MULTIMEDIA, V14, P1153, DOI 10.1109/TMM.2012.2190389
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059
   Qi YG, 2015, PROC CVPR IEEE, P1856, DOI 10.1109/CVPR.2015.7298795
   Qi YG, 2015, NEUROCOMPUTING, V165, P338, DOI 10.1016/j.neucom.2015.03.023
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rozantsev A, 2019, IEEE T PATTERN ANAL, V41, P801, DOI 10.1109/TPAMI.2018.2814042
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sangkloy P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925954
   Sarvadevabhatla RK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P10, DOI 10.1145/3123266.3123270
   Schneider RG, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2898351
   Seddati O, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P189, DOI 10.1145/3078971.3078985
   Shao TJ, 2011, COMPUT GRAPH FORUM, V30, P2011, DOI 10.1111/j.1467-8659.2011.02050.x
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JF, 2017, IEEE I CONF COMP VIS, P5552, DOI 10.1109/ICCV.2017.592
   Sun ZB, 2012, LECT NOTES COMPUT SC, V7572, P626, DOI 10.1007/978-3-642-33718-5_45
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tolias G, 2017, PROC CVPR IEEE, P6185, DOI 10.1109/CVPR.2017.655
   van Opbroek A, 2015, IEEE T MED IMAGING, V34, P1018, DOI 10.1109/TMI.2014.2366792
   Wang F, 2015, PROC CVPR IEEE, P1875, DOI 10.1109/CVPR.2015.7298797
   Wang P, 2015, IEEE I CONF COMP VIS, P1573, DOI 10.1109/ICCV.2015.184
   Wang S, 2015, IEEE T MULTIMEDIA, V17, P1045, DOI 10.1109/TMM.2015.2431492
   Wang XL, 2017, PROC CVPR IEEE, P3039, DOI 10.1109/CVPR.2017.324
   Wu X., 2018, Policy Capacity and Governance: Assessing Governmental Competences and Capabilities in Theory and Practice, P1, DOI DOI 10.1007/978-3-319-54675-9
   Xu K, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461968
   Xu Z, 2018, IEEE T PATTERN ANAL, V40, P1100, DOI 10.1109/TPAMI.2016.2637331
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yu Q, 2016, PROC CVPR IEEE, P799, DOI 10.1109/CVPR.2016.93
   Yu Q, 2017, INT J COMPUT VISION, V122, P411, DOI 10.1007/s11263-016-0932-3
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhang YT, 2016, IEEE T MULTIMEDIA, V18, P1604, DOI 10.1109/TMM.2016.2568138
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zou CQ, 2018, LECT NOTES COMPUT SC, V11219, P438, DOI 10.1007/978-3-030-01267-0_26
NR 70
TC 0
Z9 0
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3590
EP 3602
DI 10.1109/TMM.2020.3028466
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhong, H
   Wu, F
   Xu, Y
   Cui, J
AF Zhong, Hong
   Wu, Fei
   Xu, Yan
   Cui, Jie
TI QoS-Aware Multicast for Scalable Video Streaming in Software-Defined
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Software-defined networking; quality of service; scalable multimedia
   streaming multicast
ID TRANSMISSION; SDN
AB Scalable Video Coding (SVC) is a promising coding technique that enables flexible video transmission to support heterogeneous devices. However, the current best-effort delivery model characterized by the Internet cannot fully exploit the scalability feature of SVC because the network nodes are transparent for multimedia streaming applications. Software-defined networking (SDN) has emerged as an innovative network paradigm that decouples the control and forwarding planes while routing. This architecture with the OpenFlow protocol enables network operators to obtain per-flow QoS control in a more scalable, flexible, and finely granular manner compared to the conventional network architecture. Inspired by these facts, this paper designs a comprehensive QoS-aware multicast scheme for scalable video streaming over SDN networks. First, we present the designed multimedia streaming multicast system architecture and formulate the QoS-aware multicast routing problem as a non-linear programming model. We then propose a fundamental tree construction algorithm that decomposes the video streaming requests into subrequests and serves them in a bottom-up manner. Furthermore, we design a layer switching strategy based on the video distortion model to mitigate network-induced video quality distortion. Finally, the experimental results are presented to validate the performance of our methods in terms of scalability, network utility, video playout quality, and playback interruption ratio.
C1 [Zhong, Hong; Wu, Fei; Xu, Yan; Cui, Jie] Anhui Univ, Minist Educ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Hefei 230039, Peoples R China.
   [Zhong, Hong; Wu, Fei; Xu, Yan; Cui, Jie] Anhui Prov Key Lab Network & Informat Secur, Wuhu 241002, Anhui, Peoples R China.
   [Zhong, Hong; Wu, Fei; Xu, Yan; Cui, Jie] Anhui Univ, Anhui Engn Lab IoT Secur Technol, Hefei 230039, Peoples R China.
   [Zhong, Hong; Wu, Fei; Xu, Yan; Cui, Jie] Anhui Univ, Inst Phys Sci & Informat Technol, Hefei 230039, Peoples R China.
C3 Anhui University; Anhui University; Anhui University
RP Cui, J (corresponding author), Anhui Univ, Minist Educ, Sch Comp Sci & Technol, Key Lab Intelligent Comp & Signal Proc, Hefei 230039, Peoples R China.; Cui, J (corresponding author), Anhui Prov Key Lab Network & Informat Secur, Wuhu 241002, Anhui, Peoples R China.; Cui, J (corresponding author), Anhui Univ, Anhui Engn Lab IoT Secur Technol, Hefei 230039, Peoples R China.
EM zhongh@ahu.edu.cn; mrwu0908@gmail.com; xuyan@ahu.edu.cn;
   cuijie@mail.ustc.edu.cn
FU National Natural Science Foundation of China [U1936220, 61872001,
   61702005]; Special Fund for Key Program of Science and Technology of
   Anhui Province, China [18030901027]; Open Fund for Discipline
   Construction, Institute of Physical Science and Information Technology,
   Anhui University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1936220, 61872001, and 61702005, in
   part by the Special Fund for Key Program of Science and Technology of
   Anhui Province, China under Grants 18030901027, and in part by the Open
   Fund for Discipline Construction, Institute of Physical Science and
   Information Technology, Anhui University.
CR AlSaeed Z, 2018, J NETW COMPUT APPL, V104, P61, DOI 10.1016/j.jnca.2017.12.011
   [Anonymous], 2019, CISC VIS NETW IND FO
   Autefage V., 2017, GLOBECOM 2017 2017 I, P1
   Blake S, 1998, RFC, V2475, P1
   Boyd S., 2006, IEEE Trans Autom Control, V51, P1859, DOI DOI 10.1109/TAC.2006.884922
   Braden R., 1994, RFC 1633 (Informational)
   Cormen T.H., 2009, INTRO ALGORITHMS
   Craig A, 2017, COMPUT COMMUN, V110, P83, DOI 10.1016/j.comcom.2017.05.018
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Fenner W., 1997, Rfc, V2236, P1
   Hasrouty C. A., 2017, **DROPPED REF**, P1
   Hu Zhen., 2016, 2016 CLEMSON U POWER, P1
   Huang LH, 2016, IEEE INFOCOM SER
   Hwang I, 2013, IEEE COMMUN MAG, V51, P20, DOI 10.1109/MCOM.2013.6525591
   Islam S, 2018, IEEE COMMUN SURV TUT, V20, P355, DOI 10.1109/COMST.2017.2776213
   Kaibel V, 2006, BOTTLENECK SHORTEST, P6
   Karakus M, 2017, J NETW COMPUT APPL, V80, P200, DOI 10.1016/j.jnca.2016.12.019
   Laga S, 2014, IEEE IFIP NETW OPER
   Liang WE, 2016, INT CONF COMPUT NETW, P556, DOI 10.1109/iccnc.2017.7876189
   Lin YD, 2017, J NETW COMPUT APPL, V78, P125, DOI 10.1016/j.jnca.2016.11.014
   Mahajan K, 2017, INT CONF COMMUN SYST, P174, DOI 10.1109/COMSNETS.2017.7945374
   Masoudi R, 2016, J NETW COMPUT APPL, V67, P1, DOI 10.1016/j.jnca.2016.03.016
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   Nithin V, 2018, INT CONF COMMUN SYST, P113, DOI 10.1109/COMSNETS.2018.8328187
   Pfaff B, 2012, OEPNFLOW SWITCH SPEC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shan-Hsiang Shen, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P181, DOI 10.1109/INFOCOM.2015.7218381
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tang SY, 2014, 2014 9TH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND NETWORKING IN CHINA (CHINACOM), P90, DOI 10.1109/CHINACOM.2014.7054265
   Wu JY, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3152116
   Xiang S., 2012, ACM MMSys '12, P167
   Xie JJ, 2017, 2017 IEEE/ACM 25TH INTERNATIONAL SYMPOSIUM ON QUALITY OF SERVICE (IWQOS)
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yang EZ, 2016, FRONT INFORM TECH EL, V17, P672, DOI 10.1631/FITEE.1601087
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
   Yang J, 2015, IEEE INTERNET COMPUT, V19, P36, DOI 10.1109/MIC.2015.87
   Yu CH, 2018, IEEE ACCESS, V6, P64533, DOI 10.1109/ACCESS.2018.2877686
   Yu ML, 2010, ACM SIGCOMM COMP COM, V40, P351, DOI 10.1145/1851275.1851224
   Yu TF, 2015, 2015 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P318, DOI 10.1109/ICOIN.2015.7057904
   Zhang W., 2010, Proc. of INFOCOM, P2178
   Zhang XC, 2018, IEEE SYST J, V12, P1945, DOI 10.1109/JSYST.2016.2634325
   Zhao M, 2014, IEEE ICC, P1729, DOI 10.1109/ICC.2014.6883572
   Zhu T., 2016, QUAL SERV IWQOS 2016, P1
   Zhu XQ, 2005, SIGNAL PROCESS-IMAGE, V20, P773, DOI 10.1016/j.image.2005.05.005
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
NR 46
TC 7
Z9 7
U1 3
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 982
EP 994
DI 10.1109/TMM.2020.2991539
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300012
DA 2024-07-18
ER

PT J
AU Zhou, L
   Gong, C
   Liu, Z
   Fu, KR
AF Zhou, Lei
   Gong, Chen
   Liu, Zhi
   Fu, Keren
TI SAL:Selection and Attention Losses for Weakly Supervised Semantic
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Annotations; Image segmentation; Semantics; Noise measurement; Training;
   Boundary conditions; Convolution; Deep learning; weakly supervised
   semantic segmentation; selection loss; attention loss
AB Training a fully supervised semantic segmentation network requires a large amount of expensive pixel-level annotations in manual labor. In this work, we focus on studying the semantic segmentation problem using only image-level supervision. An effective scheme for weakly supervised segmentation is employed to produce the proxy annotations via image tags firstly. Then the segmentation network is retrained on the generated noisy proxy annotations. However, learning from noisy annotations is risky, as proxy annotations of poor quality may deteriorate the performance of the baseline segmentation and classification networks. In order to train the segmentation network using noisy annotations more effectively, two novel loss functions are proposed in this paper, namely, the selection loss and attention loss. Firstly, a selection loss is designed by weighting the proxy annotations based on a coarse-to-fine strategy for evaluating the quality of segmentation masks. Secondly, an attention loss taking the clean image tags as supervision is utilized to correct the classification errors caused by ambiguous pixel-level labels. Finally, we propose an end-to-end semantic segmentation network SAL-Net guided by the above two losses. From the extensive experiments conducted on PASCAL VOC 2012 dataset, SAL-Net reaches state-of-the-art performance with mean IoU (mIoU) as 62.5% and 66.6% on the test set by taking VGG16 network and ResNet101 network as the baselines respectively, which demonstrates the superiority of the proposed algorithm over eight representative weakly supervised segmentation methods. The code and models are available at https://github.com/zmbhou/SALTMM.
C1 [Zhou, Lei] Univ Shanghai Sci & Technol, Sch Med Instrument & Food Engn, Shanghai 200093, Peoples R China.
   [Zhou, Lei] Univ Shanghai Sci & Technol, Shanghai Engn Res Ctr Assist Devices, Shanghai 200093, Peoples R China.
   [Gong, Chen] Nanjing Univ Sci & Technol, Minist Educ, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab, Nanjing 210094, Peoples R China.
   [Gong, Chen] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Kowloon, Hong Kong 999077, Peoples R China.
   [Liu, Zhi] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Fu, Keren] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
C3 University of Shanghai for Science & Technology; University of Shanghai
   for Science & Technology; Nanjing University of Science & Technology;
   Hong Kong Polytechnic University; Shanghai University; Shanghai
   University; Sichuan University
RP Zhou, L (corresponding author), Univ Shanghai Sci & Technol, Sch Med Instrument & Food Engn, Shanghai 200093, Peoples R China.
EM davidzhou@usst.edu.cn; chen.gong@njust.edu.cn; liuzhisjtu@163.com;
   fkrsuper@scu.edu.cn
RI GONG, CHEN/JDW-5727-2023; Fu, Keren/HPG-4742-2023; LIU, Zhi/D-4518-2012;
   LI, MINGZE/KEI-2317-2024
OI Fu, Keren/0000-0002-3195-2077; LIU, Zhi/0000-0002-8428-1131; 
FU National Natural Science Foundation of China [61906121, 61973162,
   61771301]; NSF of Jiangsu Province [BK20171430]; Fundamental Research
   Funds for the Central Universities [30918011319]; "Summit of the Six Top
   Talents" Program [DZXX-027]; "Young Elite Scientists Sponsorship
   Program" by Jiangsu Province; "Young Elite Scientists Sponsorship
   Program" by CAST [2018QNRC001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61906121, 61973162, and 61771301, in
   part by NSF of Jiangsu Province under Grant BK20171430, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   30918011319, in part by the "Summit of the Six Top Talents" Program
   (DZXX-027), in part by the "Young Elite Scientists Sponsorship Program"
   by Jiangsu Province, and in part by the "Young Elite Scientists
   Sponsorship Program" by CAST (2018QNRC001).
CR Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Balle J., 2016, P INT C LEARN REPR
   Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2017, PROC CVPR IEEE, P2224, DOI 10.1109/CVPR.2017.239
   Hou QB, 2018, ADV NEUR IN, V31
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Huang Zilong, 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00733
   Jiang BR, 2018, LECT NOTES COMPUT SC, V11218, P816, DOI 10.1007/978-3-030-01264-9_48
   Jiang PT, 2019, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2019.00216
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kim D, 2017, IEEE I CONF COMP VIS, P3554, DOI 10.1109/ICCV.2017.382
   Kolesnikov A, 2016, LECT NOTES COMPUT SC, V9908, P695, DOI 10.1007/978-3-319-46493-0_42
   Kwak S, 2017, AAAI CONF ARTIF INTE, P4111
   Lee J, 2019, PROC CVPR IEEE, P5262, DOI 10.1109/CVPR.2019.00541
   Li AX, 2018, IEEE T CYBERNETICS, V48, P253, DOI 10.1109/TCYB.2016.2631528
   Li KP, 2018, PROC CVPR IEEE, P9215, DOI 10.1109/CVPR.2018.00960
   Li Y, 2019, IEEE T MULTIMEDIA, V21, P875, DOI 10.1109/TMM.2018.2867720
   Lin D, 2016, PROC CVPR IEEE, P3159, DOI 10.1109/CVPR.2016.344
   Lu ZW, 2017, IEEE T PATTERN ANAL, V39, P486, DOI 10.1109/TPAMI.2016.2552172
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Min SB, 2019, AAAI CONF ARTIF INTE, P4578, DOI 10.1609/aaai.v33i01.33014578
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203
   Sasaki Y., 2007, Teach Tutor Mater, V1, P1
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shen T, 2018, PROC CVPR IEEE, P1363, DOI 10.1109/CVPR.2018.00148
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Shimoda W, 2019, IEEE I CONF COMP VIS, P5207, DOI 10.1109/ICCV.2019.00531
   Simonyan K., 2014, CORR
   Song CF, 2019, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2019.00325
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Wang X, 2018, PROC CVPR IEEE, P1354, DOI 10.1109/CVPR.2018.00147
   Wang XG, 2018, PATTERN RECOGN, V74, P15, DOI 10.1016/j.patcog.2017.08.026
   Wei YC, 2017, PROC CVPR IEEE, P6488, DOI 10.1109/CVPR.2017.687
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Wei YC, 2017, IEEE T PATTERN ANAL, V39, P2314, DOI 10.1109/TPAMI.2016.2636150
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7222, DOI 10.1109/ICCV.2019.00732
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XL, 2018, LECT NOTES COMPUT SC, V11216, P610, DOI 10.1007/978-3-030-01258-8_37
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 50
TC 34
Z9 35
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1035
EP 1048
DI 10.1109/TMM.2020.2991592
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300016
DA 2024-07-18
ER

PT J
AU Zhu, J
   Yang, H
   Lin, WY
   Liu, N
   Wang, J
   Zhang, WJ
AF Zhu, Ji
   Yang, Hua
   Lin, Weiyao
   Liu, Nian
   Wang, Jia
   Zhang, Wenjun
TI Group Re-Identification With Group Context Graph Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Layout; Feature extraction; Cameras; Task analysis; Kernel; Training;
   Measurement; Group re-identification; spatial K-NN graph; group context
   graph neural network
ID MODEL
AB Group re-identification aims to match groups of people across disjoint cameras. In this task, the contextual information from neighbor individuals can be exploited for re-identifying each individual within the group as well as the entire group. However, compared with single person re-identification, it brings new challenges including group layout and group membership changes. Motivated by the observation that individuals who are close together are more likely to keep in the same group under different cameras than those who are far apart, we propose to model each group as a spatial K-nearest neighbor graph (SKNNG) and design a group context graph neural network (GCGNN) for graph representation learning. Specifically, for each node in the graph, the proposed GCGNN learns an embedding which aggregates the contextual information from neighbor nodes. We design multiple weighting kernels for neighborhood aggregation based on the graph properties including node in-degrees and spatial relationship attributes. We compute the similarity scores between node embeddings of two graphs for group member association and obtain the matching score between the two graphs by summing up the similarity scores of all linked node pairs. Experimental results on three public datasets show that our approach performs favorably against state-of-the-art methods and achieves high efficiency.
C1 [Zhu, Ji; Yang, Hua; Lin, Weiyao; Wang, Jia; Zhang, Wenjun] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Zhu, Ji] Visbody Inc, Shanghai 200240, Peoples R China.
   [Liu, Nian] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi 54115, U Arab Emirates.
C3 Shanghai Jiao Tong University; Mohamed Bin Zayed University of
   Artificial Intelligence
RP Yang, H (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM jizhu1023@gmail.com; hyang@sjtu.edu.cn; wylin@sjtu.edu.cn;
   liunian228@gmail.com; jiawang@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn
RI Zhang, Wenjun/GNH-2095-2022; lin, yuxi/HKF-6212-2023
OI Zhang, Wenjun/0000-0002-5282-3725; Lin, Weiyao/0000-0001-8307-7107; Liu,
   Nian/0000-0002-0825-6081; yang, hua/0000-0002-0417-234X
FU National Natural Science Foundation of China NSFC [61771303, 61771305,
   61971277]; Science and Technology Commission of ShanghaiMunicipality
   STCSM [19DZ1209303, 18DZ1200102, 18DZ2270700]; Shaanxi Province
   Technological Innovation Guidance Special Fund [2020QFY01-04]
FX This work was supported in part by the National Natural Science
   Foundation of China NSFC, Grants 61771303, 61771305, and 61971277, in
   part by the Science and Technology Commission of ShanghaiMunicipality
   STCSM, Grants 19DZ1209303, 18DZ1200102, and 18DZ2270700), in part by the
   Shaanxi Province Technological Innovation Guidance Special Fund
   (2020QFY01-04), in part by the SJTUYitu/Thinkforce Joint Laboratory for
   Visual Computing and Application. The associate editor coordinating the
   review of this manuscript and approving it for publication was Andrew D.
   Bagdanov.
CR Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016
   Alameda-Pineda X, 2016, IEEE T PATTERN ANAL, V38, P1707, DOI 10.1109/TPAMI.2015.2496269
   [Anonymous], 2010, P EUR C COMP VIS
   [Anonymous], I LIDS MULT CAM TRAC
   Assari SM, 2016, LECT NOTES COMPUT SC, V9906, P119, DOI 10.1007/978-3-319-46475-6_8
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171
   Bialkowski A, 2013, 2013 INTERNATIONAL CONFERENCE ON DIGITAL IMAGE COMPUTING: TECHNIQUES & APPLICATIONS (DICTA), P104
   Cao M, 2017, IEEE INT CONF COMP V, P2573, DOI 10.1109/ICCVW.2017.302
   Carruthers P., 2016, PHILOS PSYCHOL, P1
   Chen Liang-Chieh, 2017, P IEEE C COMP VIS PA
   Chen XJ, 2014, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2014.162
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gori M, 2005, IEEE IJCNN, P729
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang ZL, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1888, DOI 10.1145/3343031.3351027
   Kingma D. P., 2014, arXiv
   Kipf T. N., 2017, ARXIV
   Landrieu L, 2018, PROC CVPR IEEE, P4558, DOI 10.1109/CVPR.2018.00479
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li Wei, 2012, AS C COMP VIS ACCV 2, P31
   Liang XD, 2016, LECT NOTES COMPUT SC, V9905, P125, DOI 10.1007/978-3-319-46448-0_8
   Lisanti G, 2019, IMAGE VISION COMPUT, V83-84, P29, DOI 10.1016/j.imavis.2019.02.009
   Lisanti G, 2017, IEEE I CONF COMP VIS, P2468, DOI 10.1109/ICCV.2017.268
   Liu LH, 2016, LECT NOTES COMPUT SC, V9914, P676, DOI 10.1007/978-3-319-48881-3_48
   Liu ZM, 2017, IEEE I CONF COMP VIS, P2448, DOI 10.1109/ICCV.2017.266
   MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003
   Narasimhan M, 2018, ADV NEUR IN, V31
   Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Teney D, 2017, PROC CVPR IEEE, P3233, DOI 10.1109/CVPR.2017.344
   Ukita N, 2016, COMPUT VIS IMAGE UND, V144, P228, DOI 10.1016/j.cviu.2015.06.011
   Velivckovic P., 2018, P INT C LEARN REPR
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xiao H, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P192, DOI 10.1145/3240508.3240539
   Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16
   Yinghao Cai, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2744, DOI 10.1109/ICPR.2010.672
   Yu HX, 2017, IEEE I CONF COMP VIS, P994, DOI 10.1109/ICCV.2017.113
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Wei-Shi, 2009, P BRIT MACH VIS C, P23, DOI DOI 10.5244/C.23.23
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zhu F, 2016, IEEE IMAGE PROC, P4279, DOI 10.1109/ICIP.2016.7533167
   Zou J., 2019, IEEE T CYBERN
NR 50
TC 14
Z9 16
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2614
EP 2626
DI 10.1109/TMM.2020.3013531
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600006
DA 2024-07-18
ER

PT J
AU Dai, TY
   Zhang, XG
   Zhang, YH
   Guo, ZM
AF Dai, Tongyu
   Zhang, Xinggong
   Zhang, Yihang
   Guo, Zongming
TI Statistical Learning Based Congestion Control for Real-Time Video
   Communication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Congestion control; real-time video streaming; low latency; statistical
   learning; adaptive adjustment
ID HIGH-SPEED; TCP VEGAS; PERFORMANCE; ALGORITHMS; ADAPTATION; RENO
AB The existing congestion control is hard to simultaneously achieve low latency, high throughput, good adaptability and fair bandwidth allocation, mainly because of the hardwired control strategy and egocentric convergence objective. To address these issues, we propose an end-to-end statistical learning based congestion control, named Iris. By exploring the underlying principles of self-inflicted delay, we find that RTT variation is linearly related to the difference between sending rate and receiving rate, which inspires us to control video bit rate using a statistical-learning congestion control model. The key idea of Iris is to force all flows to converge to the same queue load and adjust bit rate by the model. All flows keep a small and fixed number of packets queuing in the network, thus the fair bandwidth allocation and low latency are both achieved. Besides, the adjustment step size of sending rate is updated by online learning, to better adapt to dynamically changing networks. We carried out extensive experiments to evaluate the performance of Iris, with the implementations over transport layer and application layer respectively. The testing environment includes emulated network, real-world Internet and commercial cellular networks. Compared against Transmission Control Protocol (TCP) flavors and state-of-the-art protocols, Iris is able to achieve high bandwidth utilization, low latency and good fairness concurrently. Especially for HyperText Transfer Protocol (HTTP) video streaming service, Iris is able to increase the video bitrate up to 25% and Peak Signal to Noise Ratio (PSNR) up to 1 dB.
C1 [Dai, Tongyu; Zhang, Xinggong; Zhang, Yihang; Guo, Zongming] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
   [Zhang, Xinggong; Guo, Zongming] PKU UCLA JRI, Beijing 100871, Peoples R China.
C3 Peking University
RP Zhang, XG (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100871, Peoples R China.
EM dai_tongyu@pku.edu.cn; zhangxg@pku.edu.cn; 1600012825@pku.edu.cn
RI Zhang, Xinggong/U-3544-2019
OI ZHANG, XINGGONG/0000-0003-0484-5951; Dai, Tongyu/0000-0001-5811-5829
FU National Key R&D Program of China [2019YFB1802701, 2018YFB0803702];
   Alibaba Innovative Research (AIR) Program [XT622018001708]
FX This work was supported in part by the National Key R&D Program of China
   under Grants 2019YFB1802701 and 2018YFB0803702, and in part by Alibaba
   Innovative Research (AIR) Program under Grant XT622018001708.
CR Aguayo M, 2018, IEEE T MULTIMEDIA, V20, P1224, DOI 10.1109/TMM.2017.2764325
   Alizadeh M, 2013, ACM SIGCOMM COMP COM, V43, P435, DOI 10.1145/2534169.2486031
   Alizadeh M, 2010, ACM SIGCOMM COMP COM, V40, P63, DOI 10.1145/1851275.1851192
   Alizadeh M, 2012, IEEE CONF OPEN SYST, P196
   [Anonymous], 2016, PCC USER SPACE IMPLE
   [Anonymous], 2013, UDT BREAKING DATA TR
   [Anonymous], 2013, SPROUT USER SPACE IM
   [Anonymous], 2017, BANDWIDTH LOGS USED
   Bakar G, 2019, IEEE T MULTIMEDIA, V21, P429, DOI 10.1109/TMM.2018.2856629
   BRAKMO LS, 1995, IEEE J SEL AREA COMM, V13, P1465, DOI 10.1109/49.464716
   Cai L, 2005, IEEE T MULTIMEDIA, V7, P339, DOI 10.1109/TMM.2005.843360
   Cardwell N, 2017, COMMUN ACM, V60, P58, DOI 10.1145/3009824
   Carlucci G, 2016, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON MULTIMEDIA SYSTEMS (MMSYS'16), P133, DOI 10.1145/2910017.2910605
   Chen JW, 2012, IEEE T MULTIMEDIA, V14, P111, DOI 10.1109/TMM.2011.2169046
   Chen YP, 2009, WREN 2009, P73
   D. I. Forum, 2019, DASH JS
   Dai T, 2018, INT CONF CLOUD ENG, P1, DOI 10.1109/IC2E.2018.00022
   Dong M., 2015, P 12 USENIX C NETW S, P395
   Dong M, 2018, PROCEEDINGS OF THE 15TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION (NSDI'18), P343
   FAHRMEIR L, 1985, ANN STAT, V13, P342, DOI 10.1214/aos/1176346597
   Flach T, 2013, ACM SIGCOMM COMP COM, V43, P159, DOI 10.1145/2534169.2486014
   Gettys J, 2012, COMMUN ACM, V55, P57, DOI 10.1145/2063176.2063196
   Greco C, 2012, IEEE T MULTIMEDIA, V14, P1337, DOI 10.1109/TMM.2012.2195480
   Grieco LA, 2004, ACM SIGCOMM COMP COM, V34, P25, DOI 10.1145/997150.997155
   Gu YH, 2007, COMPUT NETW, V51, P1777, DOI 10.1016/j.comnet.2006.11.009
   Habachi O, 2013, IEEE T SIGNAL PROCES, V61, P1460, DOI 10.1109/TSP.2012.2237171
   Hajiesmaili MH, 2017, IEEE T MULTIMEDIA, V19, P2760, DOI 10.1109/TMM.2017.2710799
   Hu YC, 2016, IEEE T MULTIMEDIA, V18, P840, DOI 10.1109/TMM.2016.2538721
   Kim S, 2019, IEEE T MULTIMEDIA, V21, P442, DOI 10.1109/TMM.2018.2856626
   Kurdoglu E, 2018, IEEE T MULTIMEDIA, V20, P1876, DOI 10.1109/TMM.2017.2781362
   Langley A, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P183, DOI 10.1145/3098822.3098842
   Liu YW, 2019, IEEE T MULTIMEDIA, V21, P1302, DOI 10.1109/TMM.2018.2876044
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Obata H., 2011, 2011 Tenth International Symposium on Autonomous Decentralized Systems (ISADS), P605, DOI 10.1109/ISADS.2011.86
   Padhye J, 2000, IEEE ACM T NETWORK, V8, P133, DOI 10.1109/90.842137
   Rossi Dario, 2010, ICCCN, P1
   Sangtae Ha, 2008, Operating Systems Review, V42, P64, DOI 10.1145/1400097.1400105
   Shiang HP, 2012, IEEE T MULTIMEDIA, V14, P896, DOI 10.1109/TMM.2012.2187178
   Sivaraman A, 2014, ACM SIGCOMM COMP COM, V44, P479, DOI 10.1145/2740070.2626324
   Srijith KN, 2005, COMPUT COMMUN, V28, P429, DOI 10.1016/j.comcom.2004.08.016
   Tan K, 2006, IEEE INFOCOM SER, P1217
   The Chromium Projects, 2019, QUIC MULTIPLEXED STR
   Wei DX, 2006, IEEE ACM T NETWORK, V14, P1246, DOI 10.1109/TNET.2006.886335
   Winstein K, 2013, ACM SIGCOMM COMP COM, V43, P123, DOI 10.1145/2534169.2486020
   Winstein Keith, 2013, 10 USENIX S NETW SYS, P459
   Wu JY, 2019, IEEE T MULTIMEDIA, V21, P1593, DOI 10.1109/TMM.2018.2879748
   Xylomenos G, 2001, IEEE COMMUN MAG, V39, P52, DOI 10.1109/35.917504
   Yan C., IEEE T MULTIMEDIA
   Zaki Y, 2015, SIGCOMM'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P509, DOI 10.1145/2785956.2787498
   Zaki Y, 2015, ACM SIGCOMM COMP COM, V45, P509, DOI 10.1145/2829988.2787498
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zhu XF, 2014, J AEROSPACE ENG, V27, DOI 10.1061/(ASCE)AS.1943-5525.0000363
NR 52
TC 7
Z9 7
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2020
VL 22
IS 10
BP 2672
EP 2683
DI 10.1109/TMM.2019.2959448
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NT0FL
UT WOS:000572628000013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Asorey-Cacheda, R
   Garcia-Sanchez, AJ
   Garcia-Haro, J
AF Asorey-Cacheda, Rafael
   Garcia-Sanchez, Antonio-Javier
   Garcia-Haro, Joan
TI An Efficient NVoD Scheme Using Implicit Error Correction and Subchannels
   for Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE IEC; Bandwidth; Redundancy; Packet loss; Decoding; Error correction;
   Multimedia communications; video streaming; near video-on-demad; error
   resilience; erasure coding
ID VIDEO; MULTIMEDIA
AB Implicit Error Correction (IEC) is a near Video-on-Demand (nVoD) scheme that trades bandwidth utilization for initial playback delay to potentially support an infinite number of users. Additionally, it provides error protection without any further bandwidth increase by exploiting the implicit redundancy of nVoD protocols, using linear combinations of the segments transmitted in a given time slot. However, IEC packet loss protection is weaker at the beginning of the playback due to the lack of implicit redundancy and lower decoding efficiency, resulting in worse subjective playback quality. In tackling this issue, this paper contributes with an extension of the original nVoD architecture, enhancing its performance by adding a new element namely, subchannels. These subdivisions of the original channels do not provide further packet loss protection but significantly improve the decoding efficiency, which in turn increases playback quality, especially at the beginning. Even for very high packet loss probabilities, subchannels are designed to obtain higher decoding efficiency which results in greater packet loss protection than that provided by IEC. The proposed scheme is especially useful in wireless cooperative networks using techniques such as network coding, as content transmissions can be split into different subchannels in order to maximize network efficiency.
C1 [Asorey-Cacheda, Rafael; Garcia-Sanchez, Antonio-Javier; Garcia-Haro, Joan] Tech Univ Cartagena, Dept Informat & Commun Technol, Cartagena 30202, Spain.
C3 Universidad Politecnica de Cartagena
RP Asorey-Cacheda, R (corresponding author), Tech Univ Cartagena, Dept Informat & Commun Technol, Cartagena 30202, Spain.
EM rafael.asorey@upct.es; antoniojavier.garcia@upct.es; joang.haro@upct.es
RI Asorey-Cacheda, Rafael/K-1778-2015; Garcia-Sanchez,
   Antonio-Javier/F-5197-2019; Garcia-Haro, Joan/D-3503-2015
OI Asorey-Cacheda, Rafael/0000-0003-0722-4181; Garcia-Sanchez,
   Antonio-Javier/0000-0001-5095-3035; Garcia-Haro,
   Joan/0000-0003-0741-7530
FU AEI/FEDER, UE Project AIM [TEC2016-76465-C2-1-R]
FX This work was supported by the AEI/FEDER, UE Project AIM under Grant
   TEC2016-76465-C2-1-R. The associate editor coordinating the reviewof
   this manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR Abozeid A., 2017, P INT C COMP DAT AN, P215, DOI [10.1145/3093241.3093287, DOI 10.1145/3093241.3093287]
   Anang K., 2011, P IEEE 73 VEH TECHN, P1
   Andrews J. G., 2007, FUNDAMENTALS WIMAX U
   [Anonymous], 2013, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-012-1044-X
   [Anonymous], 2005, 2005 IEEE INT C
   [Anonymous], 2013, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-011-0966-Z
   [Anonymous], 2005, LECT NOTES COMPUT SC, DOI DOI 10.1007/11551898_17
   [Anonymous], 2017, 2017 2 INT C MULT, DOI DOI 10.1109/ICMIP.2017.13
   [Anonymous], 2017, IEEE T MOBILE COMPUT, DOI DOI 10.1109/TMC.2016.2639500
   [Anonymous], 2018, IEEE T VEH TECHNOL, DOI DOI 10.1109/TVT.2017.2740953
   [Anonymous], 2017, MULTIMED TOOLS APPL, DOI DOI 10.1007/S11042-016-3426-Y
   [Anonymous], 2018, ADV COMPUTER SCI UBI, DOI DOI 10.1007/978-981-10-7605-3_124
   Asorey-Cacheda R, 2008, IEEE ICC, P2017, DOI 10.1109/ICC.2008.387
   Asorey-Cacheda R., 2010, P INT C CONS EL JAN, P353
   Asorey-Cacheda R., 2009, P GLOB TEL C NOV, P1
   Bing B., 2015, ASSESSING ENHANCING
   Blomer J., 1995, TR95048 ICSI
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Dobkin D. M., 2004, RF ENG WIRELESS NETW
   González-Castaño FJ, 2010, MULTIMED TOOLS APPL, V48, P291, DOI 10.1007/s11042-009-0331-7
   Hu H, 2016, IEEE T CIRC SYST VID, V26, P1320, DOI 10.1109/TCSVT.2015.2455712
   Hua KA, 2013, ACM T MULTIM COMPUT, V9, DOI 10.1145/2502435
   Jayasundara C, 2014, IEEE T CIRC SYST VID, V24, P489, DOI 10.1109/TCSVT.2013.2290397
   Jenkac H., 2006, Journal of Zhejiang University (Science), V7, P873, DOI 10.1631/jzus.2006.A0873
   Kim B.G., 2018, J MULTIMED INF SYST, V5, P27, DOI DOI 10.9717/JMIS.2018.5.1.27
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Memos VA, 2018, FUTURE GENER COMP SY, V83, P619, DOI 10.1016/j.future.2017.04.039
   Nayfeh K., 2015, IEEE T CIRCUITS SYST, V26, P1907
   Nayfeh K., 2013, P IEEE INT C MULT EX, P1
   Nayfeh K. K., 2016, P IEEE INT C MULT EX, P1
   Plageras AP, 2018, FUTURE GENER COMP SY, V82, P349, DOI 10.1016/j.future.2017.09.082
   Psannis KE, 2006, EURASIP J WIREL COMM, DOI 10.1155/WCN/2006/24616
   Stergiou C, 2018, FUTURE GENER COMP SY, V78, P964, DOI 10.1016/j.future.2016.11.031
   Vukobratovic D, 2014, IEEE T MULTIMEDIA, V16, P277, DOI 10.1109/TMM.2013.2282129
   Wang XF, 2013, IEEE T MULTIMEDIA, V15, P811, DOI 10.1109/TMM.2013.2239630
   Xu XL, 2018, IEEE T MULTIMEDIA, V20, P271, DOI 10.1109/TMM.2017.2742699
   Zhou YP, 2015, IEEE T MULTIMEDIA, V17, P1273, DOI 10.1109/TMM.2015.2447277
   Zorzi M, 1997, IEEE T COMMUN, V45, P660, DOI 10.1109/26.592604
NR 38
TC 0
Z9 0
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2396
EP 2408
DI 10.1109/TMM.2019.2953812
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200014
DA 2024-07-18
ER

PT J
AU Hu, SF
   Shum, HPH
   Aslam, N
   Li, FWB
   Liang, XH
AF Hu, Shanfeng
   Shum, Hubert P. H.
   Aslam, Nauman
   Li, Frederick W. B.
   Liang, Xiaohui
TI A Unified Deep Metric Representation for Mesh Saliency Detection and
   Non-Rigid Shape Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Shape; Measurement; Saliency detection; Strain; Task analysis;
   Three-dimensional displays; Computer architecture; Mesh saliency;
   non-rigid shape matching; metric learning; deep learning; recurrent
   neural network
ID DESCRIPTORS; GEOMETRY
AB In this paper, we propose a deep metric for unifying the representation of mesh saliency detection and non-rigid shape matching. While saliency detection and shape matching are two closely related and fundamental tasks in shape analysis, previous methods approach them separately and independently, failing to exploit their mutually beneficial underlying relationship. In view of the existing gap between saliency and matching, we propose to solve them together using a unified metric representation of surface meshes. We show that saliency and matching can be rigorously derived from our representation as the principal eigenvector and the smoothed Laplacian eigenvectors respectively. Learning the representation jointly allows matching to improve the deformation-invariance of saliency while allowing saliency to improve the feature localization of matching. To parameterize the representation from a mesh, we also propose a deep recurrent neural network (RNN) for effectively integrating multi-scale shape features and a soft-thresholding operator for adaptively enhancing the sparsity of saliency. Results show that by jointly learning from a pair of saliency and matching datasets, matching improves the accuracy of detected salient regions on meshes, which is especially obvious for small-scale saliency datasets, such as those having one to two meshes. At the same time, saliency improves the accuracy of shape matchings among meshes with reduced matching errors on surfaces.
C1 [Hu, Shanfeng; Shum, Hubert P. H.; Aslam, Nauman] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Li, Frederick W. B.] Univ Durham, Dept Comp Sci, Durham DH1 3LE, England.
   [Liang, Xiaohui] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Northumbria University; Durham University; Beihang University
RP Shum, HPH (corresponding author), Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
EM shanfeng.hu@northumbria.ac.uk; hubert.shum@northumbria.ac.uk;
   nauman.aslam@northumbria.ac.uk; frederick.li@durham.ac.uk;
   liang_xiaohui@buaa.edu.cn
RI Li, Frederick W. B./AAM-6662-2021; Shum, Hubert P. H./E-8060-2015
OI Shum, Hubert P. H./0000-0001-5651-6039; Hu,
   Shanfeng/0000-0002-0709-8384; Li, Frederick W. B./0000-0002-4283-4228;
   liang, xiaohui/0000-0001-6351-2538
FU Erasmus Mundus Action 2 Programme [2014-0861/001-001]; Royal Society
   [IES\R2\181024]; National Key Research and Development Program of China
   [2017YFB1002702]; NationalNature Science Foundation of China [61572058];
   Defence and Security Accelerator [ACC6007422]
FX This work was supported in part by the Erasmus Mundus Action 2 Programme
   under Grant 2014-0861/001-001, in part by Royal Society under Grant
   IES\R2\181024, in part by the National Key Research and Development
   Program of China under Grant 2017YFB1002702, in part by the
   NationalNature Science Foundation of China underGrant 61572058, and in
   part by Defence and Security Accelerator under Grant ACC6007422.
CR Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.171
   BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Ben Hamza A, 2006, IEEE T IMAGE PROCESS, V15, P2249, DOI 10.1109/TIP.2006.875250
   Berman A., 1994, NONNEGATIVE MATRICES, V9
   Biasotti S, 2016, VISUAL COMPUT, V32, P217, DOI 10.1007/s00371-015-1146-3
   Biasotti S., 2014, P EUR WORKSH 3D OBJ, P111
   Bogo F, 2014, PROC CVPR IEEE, P3794, DOI 10.1109/CVPR.2014.491
   Boscaini D, 2016, COMPUT GRAPH FORUM, V35, P431, DOI 10.1111/cgf.12844
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Boscaini D., 2016, P INT C NEUR INF PRO, P3189
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Chang A. X., 2015, ARXIV151203012
   Chen XB, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185525
   Cho K., 2014, ARXIV14091259
   Corman E, 2017, ACM T GRAPHIC, V36, DOI 10.1145/2999535
   Corman É, 2015, LECT NOTES COMPUT SC, V8928, P283, DOI 10.1007/978-3-319-16220-1_20
   Cosmo L, 2016, INT CONF 3D VISION, P1, DOI 10.1109/3DV.2016.10
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dokmanic I, 2015, IEEE SIGNAL PROC MAG, V32, P12, DOI 10.1109/MSP.2015.2398954
   Dym N, 2017, SIAM J OPTIMIZ, V27, P1513, DOI 10.1137/16M1078628
   Fang Y, 2015, PROC CVPR IEEE, P2319, DOI 10.1109/CVPR.2015.7298845
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Harel J., 2006, P INT C NEUR INF PRO, P545
   Hurley N, 2009, IEEE T INFORM THEORY, V55, P4723, DOI 10.1109/TIT.2009.2027527
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jeong SW, 2017, IEEE T MULTIMEDIA, V19, P2692, DOI 10.1109/TMM.2017.2710802
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974
   Lee CH, 2005, ACM T GRAPHIC, V24, P659, DOI 10.1145/1073204.1073244
   Leifman G, 2012, PROC CVPR IEEE, P414, DOI 10.1109/CVPR.2012.6247703
   Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2
   Litany O, 2017, IEEE I CONF COMP VIS, P5660, DOI 10.1109/ICCV.2017.603
   Litman R, 2014, IEEE T PATTERN ANAL, V36, P171, DOI 10.1109/TPAMI.2013.148
   Magnus JR, 1985, EC THEORY, V1, P179, DOI 10.1017/S0266466600011129
   Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913
   Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526
   Qi CR, 2017, ADV NEUR IN, V30
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rodolà E, 2014, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2014.532
   Rustamov R. M., 2007, 5 EUR S GEOM PROC, P225
   Savva M., 2016, P EUR WORKSH 3D OBJ, P89
   Sherstinsky A, 2020, PHYSICA D, V404, DOI 10.1016/j.physd.2019.132306
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Sinha A, 2016, LECT NOTES COMPUT SC, V9910, P223, DOI 10.1007/978-3-319-46466-4_14
   Song R, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2530691
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tao PP, 2015, COMPUT GRAPH-UK, V46, P264, DOI 10.1016/j.cag.2014.09.023
   Toshev A, 2007, PROC CVPR IEEE, P33
   van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x
   Vestner M, 2017, PROC CVPR IEEE, P6681, DOI 10.1109/CVPR.2017.707
   Vlasic D, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360696
   von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Yi L, 2017, PROC CVPR IEEE, P6584, DOI 10.1109/CVPR.2017.697
   Yuan XT, 2013, J MACH LEARN RES, V14, P899
   Zhu ZT, 2016, NEUROCOMPUTING, V204, P41, DOI 10.1016/j.neucom.2015.08.127
NR 60
TC 6
Z9 6
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2278
EP 2292
DI 10.1109/TMM.2019.2952983
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200006
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Park, JH
   Gutenko, I
   Kaufman, AE
AF Park, Ji Hwan
   Gutenko, Ievgeniia
   Kaufman, Arie E.
TI Transfer Function-Guided Saliency-Aware Compression for Transmitting
   Volumetric Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Image coding; Data visualization; Rendering
   (computer graphics); Encoding; Discrete cosine transforms; Compression;
   saliency; volume visualization; wavelets; discrete cosine transform
ID OF-THE-ART; IMAGE COMPRESSION; DETECTION MODEL; VIDEO; DOMAIN;
   GENERATION; ATTENTION; TEXTURE; COLOR
AB We introduce a transfer-function-guided three-dimensional (3-D) block-based saliency-aware compression scheme for volumetric data that is both content and spatially scalable. Salient 3-D volumetric blocks are identified and weighted with the help of a transfer function which is used to render the data. We describe our method in the form of a framework for processing, progressive transmission, and visualization of volumetric data on a target device, such as a mobile device with limited computational resources. In particular, we address the transmission bottleneck incurred when transferring 3-D volumetric data. Identified salient regions are progressively transmitted to the target device. The received data are rendered progressively in the respective order with a predefined or user-defined transfer function. Our method is developed with medical applications in mind, where preservation of all information is essential for clinical diagnosis. Because our method is integrated into a resolution scalable coding scheme with an integer wavelet transform of the image, it allows the rendering of each significant region at a different resolution up to fully lossless reconstruction. We perform a thorough qualitative and quantitative evaluation of the saliency detection method and the resulting saliency-aware compression schemes. Our results show reduced error in representation of the volumetric data with our method.
C1 [Park, Ji Hwan; Gutenko, Ievgeniia; Kaufman, Arie E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook
RP Park, JH (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM jihwpark@cs.stonybrook.edu; igutenko@cs.stonybrook.edu;
   ari@cs.stonybrook.edu
OI Park, Ji Hwan/0000-0002-7971-2419
FU National Science Foundation [IIS0916235, CNS0959979, IIP1069147,
   CNS1302246, NRT1633299, CNS1650499]; Center of Excellence in Wireless
   and Information Technology at Stony Brook University
FX This work was supported in part by National Science Foundation under
   Grants IIS0916235, CNS0959979, IIP1069147, CNS1302246, NRT1633299, and
   CNS1650499 and in part by the Center of Excellence in Wireless and
   Information Technology at Stony Brook University. The Foot, Bonsai,
   Engine, and the Visible Korean dataset are courtesy of Phillips
   Research, University of Stuttgart, General Electric, and HuminTec Inc.,
   respectively.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Ahmad S, 2009, IEEE T MULTIMEDIA, V11, P1381, DOI 10.1109/TMM.2009.2030546
   [Anonymous], 2007, PR IEEE COMP DESIGN
   [Anonymous], 2011, INT C PAR DISTRIB SY, DOI DOI 10.1109/ICPADS.2011.158
   [Anonymous], 2014, IEEE T VIS COMPUT GR, DOI DOI 10.1109/TVCG.2013.257
   [Anonymous], 2003, ISPA 2003 P 3 INT S
   [Anonymous], 2001, SPRING EUROGRAP
   [Anonymous], 2001, COMPUT GRAPH FORUM
   Beyer J, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12605
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Campoalegre L, 2013, PERS UBIQUIT COMPUT, V17, P1503, DOI 10.1007/s00779-012-0596-0
   Chalechale A, 2005, IEEE T SYST MAN CY A, V35, P28, DOI 10.1109/TSMCA.2004.838464
   Chang HS, 2005, IEEE T IMAGE PROCESS, V14, P145, DOI 10.1109/TIP.2004.840706
   Chen ZZ, 2010, IEEE T CIRC SYST VID, V20, P806, DOI 10.1109/TCSVT.2010.2045912
   Chiueh TC, 1997, VISUALIZATION '97 - PROCEEDINGS, P329, DOI 10.1109/VISUAL.1997.663900
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Fleischmann D, 2007, DISEASES OF THE HEART, CHEST & BREAST, P119, DOI 10.1007/978-88-470-0633-1_20
   Fout N, 2007, IEEE T VIS COMPUT GR, V13, P1600, DOI 10.1109/TVCG.2007.70516
   Fu JJ, 2013, IEEE T MULTIMEDIA, V15, P1340, DOI 10.1109/TMM.2013.2247584
   Gobbetti E, 2012, COMPUT GRAPH FORUM, V31, P1315, DOI 10.1111/j.1467-8659.2012.03124.x
   Gu SX, 2011, PHYS MED BIOL, V56, P5845, DOI 10.1088/0031-9155/56/18/005
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Guthe S, 2004, COMPUT GRAPH-UK, V28, P51, DOI 10.1016/j.cag.2003.10.018
   Hadizadeh H, 2013, IEEE T MULTIMEDIA, V15, P2099, DOI 10.1109/TMM.2013.2281024
   Hadizadeh H, 2014, IEEE T IMAGE PROCESS, V23, P19, DOI 10.1109/TIP.2013.2282897
   He TS, 1996, IEEE VISUAL, P227, DOI 10.1109/VISUAL.1996.568113
   Huang YL, 1999, INT CONF ACOUST SPEE, P3013, DOI 10.1109/ICASSP.1999.757475
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kim Y, 2006, IEEE T VIS COMPUT GR, V12, P925, DOI 10.1109/TVCG.2006.174
   Kniss J, 2002, IEEE T VIS COMPUT GR, V8, P270, DOI 10.1109/TVCG.2002.1021579
   Krishnan K, 2006, IEEE T MED IMAGING, V25, P1189, DOI 10.1109/TMI.2006.879956
   Lalos AS, 2017, IEEE T MULTIMEDIA, V19, P41, DOI 10.1109/TMM.2016.2605927
   Li HL, 2006, IEEE T IMAGE PROCESS, V15, P1300, DOI 10.1109/TIP.2005.863970
   Ljung P, 2004, IEEE SYMPOSIUM ON VOLUME VISUALIZATION AND GRAPHICS 2004, PROCEEDINGS, P25
   Mekuria R, 2014, IEEE T MULTIMEDIA, V16, P1809, DOI 10.1109/TMM.2014.2331919
   Moorthy AK, 2009, IEEE J-STSP, V3, P193, DOI 10.1109/JSTSP.2009.2015374
   Moser M., 2008, P VIS MOD VIS, P217
   Murmuria R., 2012, P IEEE INT C SOFTW S, P147
   Noguera JM, 2016, IEEE T VIS COMPUT GR, V22, P1164, DOI 10.1109/TVCG.2015.2430343
   Pantanowitz L, 2014, J PATHOL INFORM, V5, P39
   Rodríguez MB, 2014, COMPUT GRAPH FORUM, V33, P77, DOI 10.1111/cgf.12280
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Sanchez V, 2010, IEEE T MED IMAGING, V29, P1808, DOI 10.1109/TMI.2010.2052628
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Suter SK, 2011, IEEE T VIS COMPUT GR, V17, P2135, DOI 10.1109/TVCG.2011.214
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Theoharatos C, 2006, PATTERN RECOGN, V39, P1892, DOI 10.1016/j.patcog.2006.04.015
   Tong HHY, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P428, DOI 10.1109/ICIP.1998.999032
   Walther D, 2006, NEURAL NETWORKS, V19, P1395, DOI 10.1016/j.neunet.2006.10.001
   Wang C, 2008, IEEE T VIS COMPUT GR, V14, P590, DOI 10.1109/TVCG.2007.70628
   Wang JL, 2013, IEEE T IMAGE PROCESS, V22, P2151, DOI 10.1109/TIP.2013.2246176
   Wang SQ, 2012, IEEE T CIRC SYST VID, V22, P516, DOI 10.1109/TCSVT.2011.2168269
   Wang YS, 2011, IEEE T VIS COMPUT GR, V17, P171, DOI 10.1109/TVCG.2010.34
   Wei H, 2002, IEEE T IMAGE PROCESS, V11, P912, DOI 10.1109/TIP.2002.801125
   Wolf L, 2007, IEEE I CONF COMP VIS, P1418
   Xiong ZX, 2003, IEEE T MED IMAGING, V22, P459, DOI 10.1109/TMI.2003.809585
   Xu JZ, 2001, APPL COMPUT HARMON A, V10, P290, DOI 10.1006/acha.2000.0345
   Xu X, 2014, COMPUT GRAPH FORUM, V33, P111, DOI 10.1111/cgf.12367
   YEO BL, 1995, IEEE T VIS COMPUT GR, V1, P29, DOI 10.1109/2945.468390
   Zhong Y, 2000, PATTERN RECOGN, V33, P671, DOI 10.1016/S0031-3203(99)00079-5
   Zhou MH, 2012, IEEE T CIRC SYST VID, V22, P1839, DOI 10.1109/TCSVT.2012.2221524
NR 62
TC 5
Z9 6
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2262
EP 2277
DI 10.1109/TMM.2017.2757759
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NG7MB
UT WOS:000564163800001
OA hybrid
DA 2024-07-18
ER

PT J
AU Long, FC
   Yao, T
   Qiu, ZF
   Tian, XM
   Mei, T
   Luo, JB
AF Long, Fuchen
   Yao, Ting
   Qiu, Zhaofan
   Tian, Xinmei
   Mei, Tao
   Luo, Jiebo
TI Coarse-to-Fine Localization of Temporal Action Proposals
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Videos; Painting; Brushes; Microsoft Windows; Task analysis;
   Feature extraction; Action Proposals; Action Recognition; Action
   Detection; Video Captioning
AB Localizing temporal action proposals from long videos is a fundamental challenge in video analysis (e.g., action detection and recognition or dense video captioning). Most existing approaches often overlook the hierarchical granularities of actions and thus fail to discriminate fine-grained action proposals (e.g., hand washing laundry or changing a tire in vehicle repair). In this paper, we propose a novel coarse-to-fine temporal proposal (CFTP) approach to localize temporal action proposals by exploring different action granularities. Our proposed CFTP consists of three stages: a coarse proposal network (CPN) to generate long action proposals, a temporal convolutional anchor network (CAN) to localize finer proposals, and a proposal reranking network (PRN) to further identify proposals from previous stages. Specifically, CPN explores three complementary actionness curves (namely pointwise, pairwise, and recurrent curves) that represent actions at different levels for generating coarse proposals, while CAN refines these proposals by a multiscale cascaded 1D-convolutional anchor network. In contrast to existing works, our coarse-to-fine approach can progressively localize fine-grained action proposals. We conduct extensive experiments on two action benchmarks (THUMOS14 and ActivityNet v1.3) and demonstrate the superior performance of our approach when compared to the state-of-the-art techniques on various video understanding tasks.
C1 [Long, Fuchen; Qiu, Zhaofan; Tian, Xinmei] Univ Sci & Technol China, Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Yao, Ting; Mei, Tao] JD AI Res, Vis & Multimedia Lab, Beijing 100105, Peoples R China.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14604 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; University of Rochester
RP Tian, XM (corresponding author), Univ Sci & Technol China, Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
EM longfc.ustc@gmail.com; tingyao.ustc@gmail.com; zhaofanqiu@gmail.com;
   xinmei@ustc.edu.cn; tmei@live.com; jluo@cs.rochester.edu
RI Mei, Tao/GQZ-0596-2022; Luo, Jiebo/AAI-7549-2020
OI Mei, Tao/0000-0002-5990-7307; Long, Fuchen/0000-0003-0818-0985; Luo,
   Jiebo/0000-0002-4516-9729; Yao, Ting/0000-0001-7587-101X
FU 973 Programme [2015CB351803]
FX This work was supported in part by the 973 Programme under contract No.
   2015CB351803.
CR [Anonymous], N AM CHAPTER ASS COM
   [Anonymous], 2017, CORR
   [Anonymous], P CVPR ACTIVITYNET C
   [Anonymous], 2014, P COMP VIS PATT REC
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2014, P ECCVTHUMOS CHALL W
   Buch S., 2017, PROC BRIT MACH VIS C, P1
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   De Geest R, 2016, LECT NOTES COMPUT SC, V9909, P269, DOI 10.1007/978-3-319-46454-1_17
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gaidon A, 2013, IEEE T PATTERN ANAL, V35, P2782, DOI 10.1109/TPAMI.2013.65
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hosang Jan, 2016, IEEE Trans Pattern Anal Mach Intell, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jain M, 2014, PROC CVPR IEEE, P740, DOI 10.1109/CVPR.2014.100
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang Y., 2014, ECCV WORKSH
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Kuo WC, 2015, IEEE I CONF COMP VIS, P2479, DOI 10.1109/ICCV.2015.285
   Lai KT, 2014, PROC CVPR IEEE, P2251, DOI 10.1109/CVPR.2014.288
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Long FC, 2019, PROC CVPR IEEE, P344, DOI 10.1109/CVPR.2019.00043
   Ma CX, 2016, IEEE T MULTIMEDIA, V18, P2171, DOI 10.1109/TMM.2016.2614229
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Singh G., 2016, ARXIV160701979
   Sun C, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P371, DOI 10.1145/2733373.2806226
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Tang KV, 2013, IEEE I CONF COMP VIS, P2696, DOI 10.1109/ICCV.2013.335
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Xiong C, 2014, IEEE T MULTIMEDIA, V16, P1473, DOI 10.1109/TMM.2014.2316475
   Xu HZ, 2017, PROC CVPR IEEE, P3530, DOI 10.1109/CVPR.2017.376
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Yu J, 2018, IEEE T IND ELECTRON, V65, P5060, DOI 10.1109/TIE.2017.2739691
   Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou K, 2016, DESTECH TRANS COMP
NR 54
TC 21
Z9 21
U1 1
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1577
EP 1590
DI 10.1109/TMM.2019.2943204
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100017
DA 2024-07-18
ER

PT J
AU Chen, KW
   Luo, YS
   Lai, YC
   Chen, YL
   Yao, CY
   Chu, HK
   Lee, TY
AF Chen, Kuo-Wei
   Luo, Ying-Sheng
   Lai, Yu-Chi
   Chen, Yan-Lin
   Yao, Chih-Yuan
   Chu, Hung-Kuo
   Lee, Tong-Yee
TI Image Vectorization With Real-Time Thin-Plate Spline
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image color analysis; Interpolation; Real-time systems; Feature
   extraction; Image coding; Scalability; Rendering (computer graphics);
   Real-time vector graphics; hybrid vector representation; scalability;
   real-time editability
ID COLOR
AB The vector graphics with gradient mesh can be attributed to their compactness and scalability; however, they tend to fall short when it comes to real-time editing due to a lack of real-time rasterization and an efficient editing tool for image details. In this paper, we encode global manipulation geometries and local image details within a hybrid vector structure, using parametric patches and detailed features for localized and parallelized thin-plate spline interpolation in order to achieve good compressibility, interactive expressibility, and editability. The proposed system then automatically extracts an optimal set of detailed color features while considering the compression ratio of the image as well as reconstruction error and its characteristics applicable to the preservation of structural and irregular saliency of the image. The proposed real-time vector representation makes it possible to construct an interactive editing system for detail-maintained image magnification and color editing as well as material replacement in cross mapping, without maintaining spatial and temporal consistency while editing in a raster space. Experiments demonstrate that our representation method is superior to several state-of-the-art methods and as good as JPEG, while providing real-time editability and preserving structural and irregular saliency information.
C1 [Chen, Kuo-Wei; Luo, Ying-Sheng; Lai, Yu-Chi; Chen, Yan-Lin; Yao, Chih-Yuan] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Chen, Kuo-Wei; Luo, Ying-Sheng; Lai, Yu-Chi; Chen, Yan-Lin; Yao, Chih-Yuan] Natl Taiwan Univ Sci & Technol, Taiwan Bldg Technol Ctr, Taipei 106, Taiwan.
   [Chu, Hung-Kuo] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 National Taiwan University of Science & Technology; National Taiwan
   University of Science & Technology; National Tsing Hua University;
   National Cheng Kung University
RP Yao, CY (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.; Yao, CY (corresponding author), Natl Taiwan Univ Sci & Technol, Taiwan Bldg Technol Ctr, Taipei 106, Taiwan.
EM chen51202@gmail.com; tray307969@gmail.com; cheeryuchi@gmail.com;
   foodgoldsoldier@gmail.com; cyuan.yao@gmail.com; hkchu@cs.nthu.edu.tw;
   tonylee@mail.ncku.edu.tw
OI Lai, Yu-Chi/0000-0001-8578-3101; Luo, Ying-Sheng/0000-0002-8851-5855
FU National Science Council of Taiwan [107-2221-E-011-115-MY2,
   107-2221-E-011-112-MY2, 107-2221-E-011-114-MY2, 106-2221-E-006-233-MY2,
   107-2221-E-006-196-MY3]; Taiwan BuildingTechnology Center from The
   Featured Areas Research Center Program within the framework of the
   Higher Education Sprout Project by the Ministry of Education in Taiwan
FX This work was supported in part by the National Science Council of
   Taiwan underGrant 107-2221-E-011-115-MY2, Grant 107-2221-E-011-112-MY2,
   Grant 107-2221-E-011-114-MY2, Grant 106-2221-E-006-233-MY2, and Grant
   107-2221-E-006-196-MY3, and in part by theTaiwan BuildingTechnology
   Center from The Featured Areas Research Center Program within the
   framework of the Higher Education Sprout Project by the Ministry of
   Education in Taiwan. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Sanjeev
   Mehrotra.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Agoston M.K., 2005, COMPUTER GRAPHICS GE
   [Anonymous], 2005, INTRO STOCHASTIC SEA
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], 2010, P INT S NONPH AN REN
   Barrett WA, 2002, ACM T GRAPHIC, V21, P777, DOI 10.1145/566570.566651
   Bowers JC, 2011, COMPUT GRAPH FORUM, V30, P1345, DOI 10.1111/j.1467-8659.2011.01994.x
   Boyé S, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366192
   Chazelle B.M., 1985, Machine Intelligence and Pattern Recognition, V2, P63
   Cohen-Or D, 2006, ACM T GRAPHIC, V25, P624, DOI 10.1145/1141911.1141933
   Delong A, 2012, INT J COMPUT VISION, V96, P1, DOI 10.1007/s11263-011-0437-z
   Demaret L, 2006, SIGNAL PROCESS, V86, P1604, DOI 10.1016/j.sigpro.2005.09.003
   Elder JH, 1999, INT J COMPUT VISION, V34, P97, DOI 10.1023/A:1008183703117
   Finch M, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024200
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Ilbery P, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508426
   Jacobson A, 2014, COMMUN ACM, V57, P99, DOI 10.1145/2578850
   Jeschke S, 2011, COMPUT GRAPH FORUM, V30, P523, DOI 10.1111/j.1467-8659.2011.01877.x
   Jeschke S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618462
   Jeschke S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618463
   Kraevoy V, 2004, ACM T GRAPHIC, V23, P861, DOI 10.1145/1015706.1015811
   Lai YK, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531391
   Lecot G., 2006, P EUR S REND EGSR, P349
   Liao ZC, 2012, IEEE T VIS COMPUT GR, V18, P1858, DOI 10.1109/TVCG.2012.76
   Orzan A, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360691
   Pang WM, 2012, IEEE COMPUT GRAPH, V32, P68, DOI 10.1109/MCG.2011.86
   Powell J., 1995, A Thin Plate Spline Method for Mapping Curves Into Curves in Two Dimensions
   Prévost R, 2015, COMPUT GRAPH FORUM, V34, P253, DOI 10.1111/cgf.12510
   Schreiner J, 2004, ACM T GRAPHIC, V23, P870, DOI 10.1145/1015706.1015812
   Sorkine O, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P191, DOI 10.1109/SMI.2004.1314506
   Sun J, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239462
   Sun T, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601187
   Sun X, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185570
   TERZOPOULOS D, 1983, COMPUT VISION GRAPH, V24, P52, DOI 10.1016/0734-189X(83)90020-8
   Xia T, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618461
   Xiao Y, 2013, IEEE T MULTIMEDIA, V15, P549, DOI 10.1109/TMM.2012.2233725
   Xie GF, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661275
   Zhou HL, 2014, IEEE T IMAGE PROCESS, V23, P3268, DOI 10.1109/TIP.2014.2327807
NR 39
TC 6
Z9 6
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 15
EP 29
DI 10.1109/TMM.2019.2922126
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000004
DA 2024-07-18
ER

PT J
AU Ghimire, S
   Choi, JY
   Lee, B
AF Ghimire, Sarala
   Choi, Jae Young
   Lee, Bumshik
TI Using Blockchain for Improved Video Integrity Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blockchain; video integrity; elliptic curve cryptography (ECC); HMAC
ID DOUBLE COMPRESSION
AB A video record plays a crucial role in providing evidence for crime scenes or road accidents. However, the main problem with the video record is that it is often vulnerable to various video tampering attacks. Although visual evidence is required to conduct an integrity verification before investigations, it is still difficult for human vision to detect a forgery. In this paper, we propose a novel video integrity verification method (IVM) that takes advantage of a blockchain framework. The proposed method employs an effective blockchain model in centralized video data, by combining a hash-based message authentication code and elliptic curve cryptography to verify the integrity of a video. In our method, video content with a predetermined size (segments) is key-hashed in a real-time manner and stored in a chronologically chained fashion, thus establishing an irrefutable database. The verification process applies the same procedure to the video segment and generates a hash value that can be compared with the hash in the blockchain. The proposed IVM is implemented on a PC environment, as well as on an accident data recorder-embedded system for verification. The experimental results show that the proposed method has better detection capabilities and robustness toward various kinds of tampering, such as copy-move, insert, and delete, as compared to other state-of-the-art methods. An analysis based on execution time along with an increase in the number of blocks within the blockchain shows a minimal overhead in the proposed method.
C1 [Ghimire, Sarala] Chosun Univ, Gwangju 61452, South Korea.
   [Lee, Bumshik] Chosun Univ, Dept Informat & Commun Engn, Gwangju 61452, South Korea.
   [Choi, Jae Young] Hankuk Univ Foreign Studies, Yongin 17035, South Korea.
C3 Chosun University; Chosun University; Hankuk University Foreign Studies
RP Lee, B (corresponding author), Chosun Univ, Dept Informat & Commun Engn, Gwangju 61452, South Korea.
EM srlaghm@chosun.kr; jychoi@hufs.ac.kr; bslee@chosun.ac.kr
OI Lee, Bumshik/0000-0003-2482-1869; Ghimire, Sarala/0000-0002-6645-6559
CR Aitzhan NZ, 2018, IEEE T DEPEND SECURE, V15, P840, DOI 10.1109/TDSC.2016.2616861
   Al-Sanjary Omar Ismael, 2015, Journal of Theoretical and Applied Information Technology, V74, P207
   [Anonymous], 2006, P 8 WORKSHOP MULTIME, DOI DOI 10.1145/1161366.1161375
   Arshad MU, 2018, IEEE T KNOWL DATA EN, V30, P866, DOI 10.1109/TKDE.2017.2776221
   Asikuzzaman M, 2016, IEEE T MULTIMEDIA, V18, P1733, DOI 10.1109/TMM.2016.2589208
   Azaria A, 2016, PROCEEDINGS 2016 2ND INTERNATIONAL CONFERENCE ON OPEN AND BIG DATA - OBD 2016, P25, DOI 10.1109/OBD.2016.11
   Bellare M., 1996, Advances in Cryptology - CRYPTO'96. 16th Annual International Cryptology Conference. Proceedings, P1
   Bruyn A, 2017, BLOCKCHAIN INTRO
   Calabresi M., 2016, INTRO ELLIPTIC CURVE, P10
   Chou CL, 2015, IEEE T MULTIMEDIA, V17, P382, DOI 10.1109/TMM.2015.2391674
   Dickson L.E., 2003, Linear Groups: With an Exposition of the Galois Field Theory
   Dobre RA, 2018, 2018 INTERNATIONAL CONFERENCE ON CONTROL, ARTIFICIAL INTELLIGENCE, ROBOTICS & OPTIMIZATION (ICCAIRO), P211, DOI 10.1109/ICCAIRO.2018.00042
   Doulamis ND, 2000, IEEE T CIRC SYST VID, V10, P93, DOI 10.1109/76.825864
   Gayoso V., 2010, J. Comput. Sci. Eng, V2, P7
   Gemalto, 2012, BEN ELL CURV CRYPT
   Ghimire S., 2018, P KIIT C JUN, P419
   Ghimire S., 2018, P KSAE ANN AUT C EXH, P788
   Gipp B., 2016, 10 MED C INF SYST MC, P3
   Halevi S, 2006, LECT NOTES COMPUT SC, V4117, P41
   Hankerson D., 2012, GUIDE ELLIPTIC CURVE
   He PS, 2016, J VIS COMMUN IMAGE R, V35, P55, DOI 10.1016/j.jvcir.2015.11.014
   Hsu CC, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P170, DOI 10.1109/MMSP.2008.4665069
   Jana B., 2016, 2016 INT C COMP EL C, P1
   Jiang XH, 2013, IEEE SIGNAL PROC LET, V20, P447, DOI 10.1109/LSP.2013.2251632
   Jones P., 2001, US secure hash algorithm 1 (SHA1) RFC 3174
   Kim M, 2014, I C INF COMM TECH CO, P636, DOI 10.1109/ICTC.2014.6983237
   Krawczyk H., 1997, RFC 2104, DOI DOI 10.17487/RFC2104
   Kwon H., 2016, SIGMATA STORAGE INTE
   Lee C, 2016, KSII T INTERNET INF, V10, P3943, DOI 10.3837/tiis.2016.08.028
   Lee S, 2015, IEICE T INF SYST, VE98D, P95, DOI 10.1587/transinf.2014MUL0001
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Nie XS, 2017, IEEE T MULTIMEDIA, V19, P785, DOI 10.1109/TMM.2016.2629758
   NIST, 2012, NIST SPEC PUBL
   Olleros FX., 2016, Research Handbook on digital transformations
   Preneel Bart., 1997, CryptoBytes, V3, P9
   Ravi H, 2014, IEEE IMAGE PROC, P5352, DOI 10.1109/ICIP.2014.7026083
   Singh LD, 2015, PROCEDIA COMPUT SCI, V54, P73, DOI 10.1016/j.procs.2015.06.009
   Singh LD, 2015, PROCEDIA COMPUT SCI, V54, P472, DOI 10.1016/j.procs.2015.06.054
   Singh M, 2017, INT SOC DESIGN CONF, P15, DOI 10.1109/ISOCC.2017.8368806
   Singh S, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P463, DOI 10.1109/IC3I.2016.7918009
   Soliman SM, 2016, 2016 29TH IEEE INTERNATIONAL SYSTEM-ON-CHIP CONFERENCE (SOCC), P206, DOI 10.1109/SOCC.2016.7905466
   Song J, 2016, DIGIT INVEST, V18, P1, DOI 10.1016/j.diin.2016.06.001
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Stallings W., 2005, CRYPTOGRAPHY NETWORK, V4th
   Subramanyam AV, 2013, INT CONF ACOUST SPEE, P3038, DOI 10.1109/ICASSP.2013.6638216
   Vázquez-Padín D, 2012, IEEE INT WORKS INFOR, P151, DOI 10.1109/WIFS.2012.6412641
   Yu LY, 2016, NEUROCOMPUTING, V205, P84, DOI 10.1016/j.neucom.2016.03.051
   Yuting Su, 2011, 2011 6th IEEE Joint International Information Technology and Artificial Intelligence Conference (ITAIC 2011), P461, DOI 10.1109/ITAIC.2011.6030373
   Zhao Yong-Xia, 2010, Proceedings of the 2010 Second International Conference on Multimedia and Information Technology (MMIT 2010), P271, DOI 10.1109/MMIT.2010.186
NR 49
TC 33
Z9 34
U1 1
U2 29
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 108
EP 121
DI 10.1109/TMM.2019.2925961
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000011
DA 2024-07-18
ER

PT J
AU Xiao, B
   Wang, HL
   Wu, J
   Kwong, S
   Kuo, CCJ
AF Xiao, Bo
   Wang, Hanli
   Wu, Jun
   Kwong, Sam
   Kuo, C-C Jay
TI A Multi-Grained Parallel Solution for HEVC Encoding on Heterogeneous
   Platforms
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video coding; HEVC; GPU; parallel computing; heterogeneous computing
ID DECISION; EFFICIENCY; FRAMEWORK; MOTION
AB To improve the parallel processing capability of video coding, the emerging high efficiency video coding (HEVC) standard introduces two parallel techniques, i.e., Wavefront Parallel Processing (WPP) and Tiles, to make it much more parallel-friendly than its predecessors. However, these two techniques are designed to explore coarse-grained parallelism in HEVC encoding on multicore Central Processing Unit (CPU) platforms. As the computing architecture undergoes a trend toward heterogeneity in the last decade, multi-grained parallel computing methods can be designed to accelerate HEVC encoding on heterogeneous systems. In this paper, a multi-grained parallel solution (MPS) is proposed to optimize HEVC encoding on a typical heterogeneous platform. A massively parallel motion estimation algorithm is employed by MPS to parallelize part of HEVC encoding on Graphic Processing Unit (GPU). Meanwhile, several other HEVC encoding modules are accelerated on CPU through the cooperation of WPP and an adaptive parallel mode decision algorithm. The parallelism between CPU and GPU is well designed and implemented to guarantee an efficient concurrent execution of HEVC encoding on multi-grained parallel levels. The effectiveness of the proposed MPS for HEVC encoding is verified on a number of experiments.
C1 [Xiao, Bo; Wang, Hanli; Wu, Jun] Tongji Univ, Minist Educ, Key Lab Embedded Syst & Serv Comp, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   [Kwong, Sam] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Kuo, C-C Jay] Univ Southern Calif, Signal & Image Proc Inst, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Tongji University; City University of Hong Kong; University of Southern
   California
RP Wang, HL (corresponding author), Tongji Univ, Minist Educ, Key Lab Embedded Syst & Serv Comp, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
EM 1314xiaobo@tongji.edu.cn; hanliwang@tongji.edu.cn; wujun@tongji.edu.cn;
   cssamk@cityu.edu.hk; cckuo@sipi.usc.edu
RI Wang, Hanli/G-5111-2014; Kwong, Sam/C-9319-2012; Kuo, C.-C.
   Jay/A-7110-2011
OI Wang, Hanli/0000-0002-9999-4871; Kwong, Sam/0000-0001-7484-7261; Kuo,
   C.-C. Jay/0000-0001-9474-5035
FU National Natural Science Foundation of China [61622115, 61472281];
   National KeyRAMP;D Program of China [2017YFB1401404]; Shanghai
   Engineering Research Center of Industrial Vision Perception and
   Intelligent Computing [17DZ2251600]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 61622115 and Grant 61472281, in part by National
   KeyR&D Program of China (2017YFB1401404), and in part by Shanghai
   Engineering Research Center of Industrial Vision Perception and
   Intelligent Computing (17DZ2251600). The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Marta Mrak.
CR Bjotegaard G., 2001, VCEGM33
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Chen WN, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P697
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Fuldseth A., 2011, JCTVCE408
   Henry F., 2011, JCTVCE196
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   Jiang CY, 2016, IEEE T CIRC SYST VID, V26, P346, DOI 10.1109/TCSVT.2015.2402853
   Kurdoglu E, 2016, IEEE T MULTIMEDIA, V18, P90, DOI 10.1109/TMM.2015.2496872
   Lee D, 2016, J REAL-TIME IMAGE PR, V12, P549, DOI 10.1007/s11554-015-0522-6
   Li Y, 2017, IEEE T MULTIMEDIA, V19, P1431, DOI 10.1109/TMM.2017.2669863
   McCann K., 2014, document JCTVC-R1002
   Momcilovic S, 2016, J REAL-TIME IMAGE PR, V11, P571, DOI 10.1007/s11554-013-0357-y
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Radicke S, 2016, IEEE T BROADCAST, V62, P103, DOI 10.1109/TBC.2015.2505401
   Rodríguez-Sánchez R, 2012, LECT NOTES COMPUT SC, V7131, P551
   Shahid MU, 2015, IEEE T CIRC SYST VID, V25, P701, DOI 10.1109/TCSVT.2014.2351111
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Vieron J., 2012, JCTVCI0198
   Wang B, 2017, INT J PARALLEL PROG, V45, P1515, DOI 10.1007/s10766-017-0488-z
   Wang B, 2015, IEEE T CIRC SYST VID, V25, P525, DOI 10.1109/TCSVT.2014.2344512
   Wang HL, 2018, IEEE T MULTIMEDIA, V20, P2935, DOI 10.1109/TMM.2018.2830120
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Xiao W, 2015, IEEE T CIRC SYST VID, V25, P1830, DOI 10.1109/TCSVT.2015.2406199
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
NR 31
TC 5
Z9 9
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 2997
EP 3009
DI 10.1109/TMM.2019.2916462
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200003
DA 2024-07-18
ER

PT J
AU Shen, LQ
   Feng, GR
AF Shen, Liquan
   Feng, Guorui
TI Content-Based Adaptive SHVC Mode Decision Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Encoding; Complexity theory; Correlation; Video coding; Copper;
   Standards; Prediction algorithms; SHVC; scalable video coding; mode
   decision; interlayer correlation
ID MOTION ESTIMATION; INTRA PREDICTION; UNIT DEPTH; VIDEO; COMPLEXITY;
   QUALITY; SCHEME; SIZE
AB The scalable video coding extensions of the High Efficient Video Coding (HEVC) standard (SHVC) have adopted a new quadtree-structured coding unit (CU). The SHVC test model (SHM) needs to test seven intermode sizes and one intramode size at depth levels of "0," "1," "2," and four intermode sizes and two intramode sizes at a depth level of "3" for interframe CUs. It checks all possible depth levels and prediction modes to find the one with the lowest rate distortion cost using the Lagrange multiplier method in the mode decision procedure to achieve high coding efficiency at the expense of computational complexity. Furthermore, it utilizes the conventional approach for the base layer (BL) and enhancement layer (EL) coding to support SNRspatial scalable coding. Both the intralayer and interlayer predictions should be performed for each EL CU. Although there is a large amount of interlayer redundancy that can be exploited to speed up the EL encoding, the mode decision procedure is independently performed for the BL and the ELs. In this paper, we propose a content-adaptive mode decision algorithm to reduce the SHVC complexity at the ELs. When the major characteristics of the CUs, such as mode complexity and motion activity, can be estimated early and used for adjusting the mode decision procedure, unnecessary mode and CU size searches can be avoided. First, an experimental analysis is performed to study the interlayer and spatiotemporal correlations in the coding information and the interlevel correlations among the quadtree structures. Based on these correlations, three parameters, including the conditional probability of a SKIPMerge mode, motion activity, and mode complexity, are defined to describe the video content and are further utilized to adaptively adjust the EL mode decision procedure. The experimental results show that the proposed algorithm can reduce the coding time for ELs by 62-67 with less than a 1.5 Bjontegaard rate increase compared to the original SHVC encoder.
C1 [Shen, Liquan] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200072, Peoples R China.
   [Shen, Liquan] Shanghai Univ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
   [Feng, Guorui] Shanghai Univ, Key Lab Specialty Fiber Opt & Opt Access Networks, Sch Commun & Informat Engn, Shanghai 200072, Peoples R China.
C3 Shanghai University; Shanghai University; Shanghai University
RP Shen, LQ (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200072, Peoples R China.; Shen, LQ (corresponding author), Shanghai Univ, Key Lab Adv Display & Syst Applicat, Shanghai 200072, Peoples R China.
EM jsslq@163.com; grfeng@shu.edu.cn
RI Shen, Liquan/D-4832-2012
OI Shen, Liquan/0000-0002-2148-6279
FU National Natural Science Foundation of China [61671282, 61422111];
   Shanghai Pjiang Program [15pjd015]; Shanghai Science and Technology
   Innovation Plan [18010500200]; Shanghai Shuguang Program [17SG37]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61671282 and 61422111, in part by
   Shanghai Pjiang Program under Grant 15pjd015, in part by Shanghai
   Science and Technology Innovation Plan under Grant 18010500200, and in
   part by Shanghai Shuguang Program under Grant 17SG37.
CR [Anonymous], 13 VID COD EXP GROUP
   Bailleul Robin, 2014, 2014 IEEE International Conference on Consumer Electronics (ICCE), P193, DOI 10.1109/ICCE.2014.6775968
   Balaji L., 2015, SCI WORLD J, V2015
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Chen J., 2013, JOINT COLLABORATIVE
   Choi K., 2011, JOINT COLLABORATIVE
   Diaz-Honrubia AJ, 2017, J SUPERCOMPUT, V73, P277, DOI 10.1007/s11227-016-1802-z
   Fu GL, 2018, IEEE SIGNAL PROC LET, V25, P1665, DOI 10.1109/LSP.2018.2867895
   Ge QY, 2014, PROCEEDINGS OF 2014 IEEE WORKSHOP ON ADVANCED RESEARCH AND TECHNOLOGY IN INDUSTRY APPLICATIONS (WARTIA), P1366, DOI 10.1109/WARTIA.2014.6976537
   Grois D, 2014, IEEE T CIRC SYST VID, V24, P1025, DOI 10.1109/TCSVT.2014.2302557
   Heindel A, 2017, IEEE T CIRC SYST VID, V27, P1749, DOI 10.1109/TCSVT.2016.2556338
   Herrou G, 2018, IEEE DATA COMPR CONF, P411, DOI 10.1109/DCC.2018.00064
   Jiang YB, 2015, 2015 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P290, DOI 10.1109/GlobalSIP.2015.7418203
   Jung SW, 2010, IEEE T CIRC SYST VID, V20, P201, DOI 10.1109/TCSVT.2009.2031387
   Jung SH, 2016, IEEE T CIRC SYST VID, V26, P1846, DOI 10.1109/TCSVT.2015.2473303
   Kamal JMM, 2014, 2014 IEEE/ACM INTERNATIONAL SYMPOSIUM ON BIG DATA COMPUTING (BDC), P8, DOI 10.1109/BDC.2014.8
   Katayama Takafumi, 2016, 2016 IEEE International Conference on Consumer Electronics (ICCE), P419, DOI 10.1109/ICCE.2016.7430673
   Katayama T, 2016, PROCEEDINGS OF THE 2016 IEEE REGION 10 CONFERENCE (TENCON), P3079, DOI 10.1109/TENCON.2016.7848614
   Kessentini A, 2016, J REAL-TIME IMAGE PR, V11, P385, DOI 10.1007/s11554-013-0362-1
   Li GL, 2013, IEEE T CIRC SYST VID, V23, P1837, DOI 10.1109/TCSVT.2013.2248496
   Li Q., 2018, MATH PROBL ENG, P1
   Li XN, 2017, MULTIMED TOOLS APPL, V76, P8011, DOI 10.1007/s11042-016-3460-9
   Li Y, 2017, IEEE T MULTIMEDIA, V19, P1431, DOI 10.1109/TMM.2017.2669863
   Lin CY, 2018, IEEE T MULTIMEDIA, V20, P1209, DOI 10.1109/TMM.2017.2766043
   Lu X, 2018, IEEE IMAGE PROC, P1792, DOI 10.1109/ICIP.2018.8451844
   Mallikarachchi T, 2018, IEEE T CIRC SYST VID, V28, P693, DOI 10.1109/TCSVT.2016.2619499
   Na S, 2010, IEEE T CIRC SYST VID, V20, P1475, DOI 10.1109/TCSVT.2010.2077493
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Pan ZQ, 2014, IEEE T BROADCAST, V60, P405, DOI 10.1109/TBC.2014.2321682
   Park CS, 2009, IEEE T CIRC SYST VID, V19, P1915, DOI 10.1109/TCSVT.2009.2031520
   Seregin V., 2014, JOINT COLLABORATIVE
   Shen LQ, 2018, IEEE T IMAGE PROCESS, V27, P4195, DOI 10.1109/TIP.2018.2837379
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen LQ, 2012, IEEE T IMAGE PROCESS, V21, P2582, DOI 10.1109/TIP.2011.2177849
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tohidypour H. R., 2014, 2014 IEEE International Conference on Consumer Electronics (ICCE), P191, DOI 10.1109/ICCE.2014.6775967
   Tohidypour HR, 2017, IEEE T CIRC SYST VID, V27, P2204, DOI 10.1109/TCSVT.2016.2576738
   Tohidypour HR, 2016, IEEE T BROADCAST, V62, P664, DOI 10.1109/TBC.2016.2576600
   Tohidypour HR, 2016, IEEE T MULTIMEDIA, V18, P182, DOI 10.1109/TMM.2015.2510332
   Wali I, 2017, I C SCI TECH AUTO CO, P152, DOI 10.1109/STA.2017.8314923
   Wang DY, 2019, IEEE T IMAGE PROCESS, V28, P2063, DOI 10.1109/TIP.2017.2740161
   Wang DY, 2016, SIGNAL IMAGE VIDEO P, V10, P625, DOI 10.1007/s11760-015-0786-0
   Wang DY, 2016, J VIS COMMUN IMAGE R, V34, P78, DOI 10.1016/j.jvcir.2015.10.002
   Wang TH, 2011, IEEE T CONSUM ELECTR, V57, P1194, DOI 10.1109/TCE.2011.6018874
   Wei-Ju Chiang, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P381, DOI 10.1109/ICMEW.2017.8026217
   Yang J., 2011, JOINT COLLABORATIVE
   Yeh CH, 2018, J VIS COMMUN IMAGE R, V55, P342, DOI 10.1016/j.jvcir.2018.06.008
   Yeh CH, 2010, IEEE T CIRC SYST VID, V20, P563, DOI 10.1109/TCSVT.2010.2041825
   Yin P., 2013, JOINT COLLABORATIVE
   Yu C., 2017, LECT NOTES ELECT ENG
   Zhang Y, 2015, IEEE T IMAGE PROCESS, V24, P2225, DOI 10.1109/TIP.2015.2417498
   Zhong GY, 2015, MULTIMED TOOLS APPL, V74, P11023, DOI 10.1007/s11042-014-2216-7
   Zhu GJ, 2016, 2016 IEEE 12TH INTERNATIONAL COLLOQUIUM ON SIGNAL PROCESSING & ITS APPLICATIONS (CSPA), P50, DOI 10.1109/CSPA.2016.7515802
   Zuo XG, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P394, DOI 10.1109/VCIP.2014.7051589
   Zupancic I, 2015, INT CONF ACOUST SPEE, P1419, DOI 10.1109/ICASSP.2015.7178204
NR 56
TC 6
Z9 6
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2714
EP 2725
DI 10.1109/TMM.2019.2909859
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000002
DA 2024-07-18
ER

PT J
AU Tang, YB
   Wu, XQ
AF Tang, Youbao
   Wu, Xiangqian
TI Salient Object Detection Using Cascaded Convolutional Neural Networks
   and Adversarial Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; cascaded convolutional neural networks;
   conditional generative adversarial networks; adversarial learning
ID REGION DETECTION
AB Salient object detection has received much attention and achieved great success in last several years. It is still challenging to get clear boundaries and consistent saliencies, which can be considered as the structural information of salient objects. A popular solution is to conduct some post-processes (e.g., conditional random field (CRF)) to refine these structural information. In this paper, a novel cascaded convolutional neural networks (CNNs) based method is proposed to implicitly learn these structural information via adversarial learning for salient object detection (we termed the proposed method as CCAL). A cascaded CNNs model is first designed as a generator G, which consists of an encoder-decoder network for global saliency estimation and a deep residual network for local saliency refinement. It is hard to explicitly learn such structural information due to the limitation of frequently-used pixel-wise loss functions. Instead, a discriminator D is then designed to distinguish the real salient maps (i.e., ground truths) from the fake ones produced by G, based on which an adversarial loss is introduced to optimize G. G and D are trained in a fully end-to-end fashion by following the strategy of conditional generative adversarial networks to make G well learn the structural information. At last, G is able to produce high quality salient maps without requiring any post-process to fool D. Experimental results on eight benchmark datasets demonstrate the effectiveness and efficiency (about 17 fps on graphics processing unit (GPU)) of the proposed method for salient object detection.
C1 [Tang, Youbao; Wu, Xiangqian] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Wu, XQ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM tybxiaobao@gmail.com; xqwu@hit.edu.cn
RI Wu, Xiangqian/HKE-1904-2023; Tang, Youbao/L-7328-2019
OI Tang, Youbao/0000-0001-8719-3375
FU Natural Science Foundation of China [61672194]; National Key R&D Program
   of China [2018YFC0832304]; Distinguished Youth Science Foundation of
   Heilongjiang Province of China [JC2018021]; Shandong Provincial Natural
   Science Foundation, China [ZR2016FM04]; Humanity and Social Science
   Youth Foundation of the Ministry of Education of China [14YJC760001];
   Open Foundation of the State Key Laboratory of Robotics and System
   [SKLRS-2019-KF-14]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61672194, in part by the National Key R&D Program of
   China under Grant 2018YFC0832304, in part by the Distinguished Youth
   Science Foundation of Heilongjiang Province of China under Grant
   JC2018021, in part by the Shandong Provincial Natural Science
   Foundation, China under Grant ZR2016FM04, in part by the Humanity and
   Social Science Youth Foundation of the Ministry of Education of China
   under Grant 14YJC760001, and in part by Open Foundation of the State Key
   Laboratory of Robotics and System under Grant SKLRS-2019-KF-14. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang.
CR Abadi M, ARXIV, DOI DOI 10.48550/ARXIV.1603.04467
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   [Anonymous], CVPR
   [Anonymous], 2017, IEEE C COMP VIS PATT
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], 2015, IEEE T IMAGE PROCESS, DOI DOI 10.1109/TIP.2015.2487833
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2017, ARXIV170101081
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   Chen XW, 2017, IEEE I CONF COMP VIS, P1050, DOI 10.1109/ICCV.2017.119
   Cheng MM, 2015, IEEE T PATTERN ANAL, V37, P569, DOI 10.1109/TPAMI.2014.2345401
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Gidaris S, 2015, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2015.135
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gross Sam, 2016, Facebook AI Res.
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Jiang P, 2015, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2015.33
   Jung C, 2012, IEEE T IMAGE PROCESS, V21, P1272, DOI 10.1109/TIP.2011.2164420
   Kim J, 2014, PROC CVPR IEEE, P883, DOI 10.1109/CVPR.2014.118
   King DB, 2015, ACS SYM SER, V1214, P1
   Kuen J, 2016, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2016.399
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184
   Li HY, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2440174
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mi JX, 2017, 2017 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P660, DOI 10.1109/SPAC.2017.8304358
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pan P, 2016, PROCEEDINGS OF 2016 IEEE 9TH UK-EUROPE-CHINA WORKSHOP ON MILLIMETRE WAVES AND TERAHERTZ TECHNOLOGIES (UCMMT), P39, DOI 10.1109/UCMMT.2016.7873954
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rutishauser U, 2004, PROC CVPR IEEE, P37
   Scharfenberger C, 2015, IEEE T IMAGE PROCESS, V24, P457, DOI 10.1109/TIP.2014.2380351
   Simonyan K., 2014, 14091556 ARXIV
   Sun JG, 2015, IEEE T IMAGE PROCESS, V24, P1639, DOI 10.1109/TIP.2015.2403241
   Tang Y, 2017, IEEE T IMAGE PROCESS, V26, P994, DOI [10.1109/TIP.2016.2639440, 10.1109/TIP.2017.2656474]
   Tang YB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P618, DOI 10.1145/3123266.3123318
   Tang YB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1083, DOI 10.1145/2733373.2806287
   Tang YB, 2016, LECT NOTES COMPUT SC, V9912, P809, DOI 10.1007/978-3-319-46484-8_49
   Tian YH, 2015, INT J COMPUT VISION, V111, P153, DOI 10.1007/s11263-014-0737-1
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Wang HS, 2017, PROC CVPR IEEE, P3633, DOI 10.1109/CVPR.2017.387
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Xiao HX, 2018, IEEE T MULTIMEDIA, V20, P3239, DOI 10.1109/TMM.2018.2830098
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zou WB, 2015, IEEE I CONF COMP VIS, P406, DOI 10.1109/ICCV.2015.54
NR 70
TC 34
Z9 35
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2237
EP 2247
DI 10.1109/TMM.2019.2900908
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200007
DA 2024-07-18
ER

PT J
AU Xu, J
   Wang, CH
   Qi, CZ
   Shi, CZ
   Xiao, BH
AF Xu, Jian
   Wang, Chunheng
   Qi, Chengzuo
   Shi, Cunzhao
   Xiao, Baihua
TI Iterative Manifold Embedding Layer Learned by Incomplete Data for
   Large-Scale Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Iterative manifold embedding layer; image retrieval; incomplete data
ID QUERY EXPANSION; DESCRIPTORS; FEATURES
AB Existing manifold learning methods are not appropriate for image retrieval tasks, because most of them are unable to process query images and they have much greater computational cost especially for large-scale database. Therefore, we propose the iterative manifold embedding (IME) layer, of which the weights are learned offline by an unsupervised strategy, to explore the intrinsic manifolds by incomplete data. On the large-scale database that contains 27 000 images, the IME layer is more than 120 times faster than other manifold learning methods to embed the original representations at query time. We embed the original descriptors of database images that lie on manifold in a high-dimensional space into manifold-based representations iteratively to generate the IME representations in an offline learning stage. According to the original descriptors and the IME representations of database images, we estimate the weights of the IME layer by ridge regression. In the online retrieval stage, we employ the IME layer to map the original representation of a query image with an ignorable time cost (2 ms per image). We experiment on five public standard datasets for image retrieval. The proposed IME layer significantly outperforms the related dimension reduction methods and manifold learning methods. Without postprocessing, our IME layer achieves a boost in the performance of state-of-the-art image retrieval methods with postprocessing on most datasets, and needs less computational cost.
C1 [Xu, Jian; Qi, Chengzuo] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Xu, Jian; Wang, Chunheng; Qi, Chengzuo; Shi, Cunzhao; Xiao, Baihua] Chinese Acad Sci, State Key Lab Management & Control Complex Syst, Inst Automat, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Wang, CH (corresponding author), Chinese Acad Sci, State Key Lab Management & Control Complex Syst, Inst Automat, Beijing 100190, Peoples R China.
EM xujian2015@ia.ac.cn; chunheng.wang@ia.ac.cn; qichengzuo2013@ia.ac.cn;
   cunzhao.shi@ia.ac.cn; baihua.xiao@ia.ac.cn
OI xiao, bai hua/0000-0003-3941-1141
FU National Natural Science Foundation of China [61531019, 61601462,
   71621002]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61531019, Grant 61601462, and Grant 71621002. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Mohammed Daoudi.
CR Abdi H, 2010, WIRES COMPUT STAT, V2, P433, DOI 10.1002/wics.101
   [Anonymous], 2010, Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on, IEEE, DOI [DOI 10.1109/CVPR.2010.5540018, 10.1109/CVPR.2010.5540018]
   ARANDJELOVIC R, 2012, PROC CVPR IEEE, P2911, DOI DOI 10.1109/CVPR.2012.6248018
   Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/TPAMI.2017.2711011, 10.1109/CVPR.2016.572]
   Babenko A, 2014, LECT NOTES COMPUT SC, V8689, P584, DOI 10.1007/978-3-319-10590-1_38
   Belkin M, 2002, ADV NEUR IN, V14, P585
   Chadha A, 2017, IEEE T MULTIMEDIA, V19, P1596, DOI 10.1109/TMM.2017.2673415
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Donoser M, 2013, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2013.174
   FLOYD RW, 1962, COMMUN ACM, V5, P345, DOI 10.1145/367766.368168
   Gao Y, 2015, IEEE T MULTIMEDIA, V17, P359, DOI 10.1109/TMM.2015.2389616
   Gao ZN, 2016, IEEE T MULTIMEDIA, V18, P1661, DOI 10.1109/TMM.2016.2568748
   Gong YC, 2014, LECT NOTES COMPUT SC, V8695, P392, DOI 10.1007/978-3-319-10584-0_26
   Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8
   Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15
   Hinton G. E., 2002, Advances in Neural InformationProcessing Systems, P857
   Husain SS, 2017, IEEE T PATTERN ANAL, V39, P1783, DOI 10.1109/TPAMI.2016.2613873
   Hyvarinen A., 2004, INDEPENDENT COMPONEN
   Iscen A, 2017, PROC CVPR IEEE, P926, DOI 10.1109/CVPR.2017.105
   Jégou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235
   Jégou H, 2014, PROC CVPR IEEE, P3310, DOI 10.1109/CVPR.2014.417
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jegou H, 2010, IEEE T PATTERN ANAL, V32, P2, DOI 10.1109/TPAMI.2008.285
   Jian X., 2018, P AAAI C ART INT
   Kalantidis Yannis, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P685, DOI 10.1007/978-3-319-46604-0_48
   Kruskal J.B., 1978, Multidimensional scaling
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Philbin J, 2008, PROC CVPR IEEE, P2285
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Razavian A. S., 2016, ITE Trans. Media Technol. Appl., V4, P251, DOI [DOI 10.3169/MTA.4.251, 10.3169/mta.4.251]
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Seung HS, 2000, SCIENCE, V290, P2268, DOI 10.1126/science.290.5500.2268
   Shen XH, 2014, IEEE T PATTERN ANAL, V36, P1229, DOI 10.1109/TPAMI.2013.237
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Spyromitros-Xioufis E, 2014, IEEE T MULTIMEDIA, V16, P1713, DOI 10.1109/TMM.2014.2329648
   Tang J, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P287, DOI 10.1145/2872427.2883041
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Do TT, 2015, PROC CVPR IEEE, P3556, DOI 10.1109/CVPR.2015.7298978
   Tolias G., 2015, ARXIV151105879
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Gemert JC, 2010, IEEE T PATTERN ANAL, V32, P1271, DOI 10.1109/TPAMI.2009.132
   Wang S, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2700292
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Yandex Artem Babenko, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1269, DOI 10.1109/ICCV.2015.150
NR 51
TC 9
Z9 9
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2019
VL 21
IS 6
BP 1551
EP 1562
DI 10.1109/TMM.2018.2883860
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IA1QQ
UT WOS:000469337400017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Cui, CR
   Liu, HH
   Lian, T
   Nie, LQ
   Zhu, L
   Yin, YL
AF Cui, Chaoran
   Liu, Huihui
   Lian, Tao
   Nie, Liqiang
   Zhu, Lei
   Yin, Yilong
TI Distribution-Oriented Aesthetics Assessment With Semantic-Aware Hybrid
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image aesthetics assessment; label distribution learning; fully
   convolutional networks; semantic fusion
ID PHOTO
AB Image aesthetics assessment has emerged as a hot topic in recent years due to its potential in numerous high-level vision applications. In this paper, distinguished from existing studies relying on a single label, we propose quantifying image aesthetics by a distribution over multiple quality levels. The distribution-based representation characterizes the disagreement among users' aesthetic preferences regarding the same image, and is also compatible with the traditional task of aesthetic label prediction. Our framework is developed based on fully convolutional networks and enables inputs of varying sizes. In this way, we circumvent the fixed-size constraint of prevalent convolutional neural networks, and avoid the risk of impairing the intrinsic aesthetic appeal of images. Moreover, given the fact that aesthetic perceiving is coupled with semantic understanding, we present a novel semantic-aware hybrid NEtwork (SANE), which harvests the information from object categorization and scene recognition to enhance image aesthetics assessment. Experiments on two benchmark datasets have well verified the effectiveness of our approach in both scenarios of aesthetic distribution prediction and aesthetic label prediction, and highlighted the benefits of input preserving as well as semantic understanding for images.
C1 [Cui, Chaoran] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250014, Shandong, Peoples R China.
   [Liu, Huihui; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Lian, Tao] Taiyuan Univ Technol, Coll Data Sci, Taiyuan 030024, Shanxi, Peoples R China.
   [Zhu, Lei] Shandong Normal Univ, Sch Informat Sci & Engn, Jinan 250358, Shandong, Peoples R China.
   [Yin, Yilong] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University; Taiyuan
   University of Technology; Shandong Normal University; Shandong
   University
RP Yin, YL (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
EM cr-cui@sdufe.edu.cn; huihui-liu@foxmail.com; liantao1988@gmail.com;
   nieliqiang@gmail.com; leizhu0608@gmail.com; ylyin@sdu.edu.cn
RI Zhu, Lei/GQQ-1130-2022; Liu, huihui/HME-1734-2023
OI Zhu, Lei/0000-0002-5348-7532; Liu, huihui/0000-0001-5331-8264; Lian,
   Tao/0000-0002-8941-5143; Zhu, Lei/0000-0002-2993-7142
FU National Natural Science Foundation of China [61573219, 61701281,
   61876098]; Shandong Provincial Natural Science Foundation [ZR2017QF009];
   Fostering Project of Dominant Discipline and Talent Team of Shandong
   Province Higher Education Institutions
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61573219, Grant 61701281, and Grant
   61876098, in part by Shandong Provincial Natural Science Foundation
   under Grant ZR2017QF009, and in part by the Fostering Project of
   Dominant Discipline and Talent Team of Shandong Province Higher
   Education Institutions. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Pablo Cesar.
CR [Anonymous], 2002, proceedings of the 6th conference on Natural language learning-Volume 20
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], PROC 17TH ACM INT CO
   [Anonymous], PROC 21ST INT CONF M
   Aydin TO, 2015, IEEE T VIS COMPUT GR, V21, P31, DOI 10.1109/TVCG.2014.2325047
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Choi K, 2017, INT CONF ACOUST SPEE, P2392, DOI 10.1109/ICASSP.2017.7952585
   Cui CR, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P1013, DOI 10.1145/3077136.3080704
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng YB, 2017, IEEE SIGNAL PROC MAG, V34, P80, DOI 10.1109/MSP.2017.2696576
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Geng B., 2011, Proceedings of the 19th ACM international conference on multimedia, P63
   Geng X, 2017, AAAI CONF ARTIF INTE, P1331
   Geng X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3511
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Glorot X., 2010, P INT C ART INT STAT, P249
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   He KM, 2015, PROC CVPR IEEE, P5353, DOI 10.1109/CVPR.2015.7299173
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   He ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3846, DOI 10.1109/TIP.2017.2655445
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Li XR, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906152
   Lin M., 2013, P 2 INT C LEARNING R
   Lo KY, 2012, INT C PATT RECOG, P2186
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Lu X, 2015, IEEE T MULTIMEDIA, V17, P2021, DOI 10.1109/TMM.2015.2477040
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Mai L, 2016, PROC CVPR IEEE, P497, DOI 10.1109/CVPR.2016.60
   Marchesotti L, 2015, INT J COMPUT VISION, V113, P246, DOI 10.1007/s11263-014-0789-2
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Ren J, 2017, IEEE I CONF COMP VIS, P638, DOI 10.1109/ICCV.2017.76
   Simonyan K., 2014, 14091556 ARXIV
   Springenberg Jost Tobias, 2015, Striving for simplicity: The all convolutional net, DOI DOI 10.48550/ARXIV.1412.6806
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tian XM, 2015, IEEE T MULTIMEDIA, V17, P2035, DOI 10.1109/TMM.2015.2479916
   Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296
   Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50
   Wu O, 2011, IEEE I CONF COMP VIS, P225, DOI 10.1109/ICCV.2011.6126246
   Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 48
TC 62
Z9 65
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1209
EP 1220
DI 10.1109/TMM.2018.2875357
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600011
DA 2024-07-18
ER

PT J
AU Liu, YW
   Liu, JX
   Argyriou, A
   Ci, S
AF Liu, Yanwei
   Liu, Jinxia
   Argyriou, Antonios
   Ci, Song
TI MEC-Assisted Panoramic VR Video Streaming Over Millimeter Wave Mobile
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Link adaptation; millimeter wave mobile networks; mobile edge
   transcoding; panoramic VR video; viewport rendering offloading
AB Panoramic virtual reality video (PVRV) is becoming increasingly popular since it offers a true immersive experience. However, the ultra-high resolution of PVRV requires significant bandwidth and ultra-low latency for PVRV streaming, something that makes challenging the extension of this application to mobile networks. Besides bandwidth, the frequent perspective viewport rendering induces a heavy computational load on battery-constrained mobile devices. To attack these problems jointly, this paper proposes a PVRV streaming system that is designed for modern multiconnectivity-based millimeter wave (mmWave) cellular networks in conjunction with mobile edge computing (MEC). First, mmWave is deployed to support the high bandwidth needs of PVRV streaming. Next, the multiple mmWave links that tend to suffer from outages are coupled with a sub-6 GHz link to ensure disruption-free wireless communication. With the help of an MEC server, the tradeoff among link adaptation, transcoding-based chunk quality adaptation, and viewport rendering offloading is sought to improve the wireless bandwidth utilization and mobile device's energy efficiency. Simulation results show that the proposed scheme can improve the streaming performance in both energy efficiency and the quality of received viewport over the state-of-the-art schemes.
C1 [Liu, Yanwei] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100049, Peoples R China.
   [Liu, Yanwei] Univ Chinese Acad Sci, Sch Cyber Secur, Beijing 100049, Peoples R China.
   [Liu, Jinxia] Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
   [Argyriou, Antonios] Univ Thessaly, Dept Elect & Comp Engn, Volos 38221, Greece.
   [Ci, Song] Univ Nebraska Lincoln, Dept Elect & Comp Engn, Omaha, NE 68046 USA.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Zhejiang Wanli University; University of Thessaly; University of
   Nebraska System; University of Nebraska Lincoln
RP Liu, JX (corresponding author), Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
EM liuyanwei@iie.ac.cn; liujinxia@zjwu.edu.cn; anargyr@uth.gr;
   sci@engr.unl.edu
RI Liu, Jinxia/H-1794-2011; Argyriou, Antonios/AAF-9586-2021
OI Argyriou, Antonios/0000-0002-2510-3124
FU National Natural Science Foundation of China [61771469]; Zhejiang
   Provincial Natural Science Foundation of China [LY17F010001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61771469 and Zhejiang Provincial Natural
   Science Foundation of China under Grant LY17F010001. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao.
CR 3GPP, 2013, TR 36.842
   Abari O, 2017, PROCEEDINGS OF NSDI '17: 14TH USENIX SYMPOSIUM ON NETWORKED SYSTEMS DESIGN AND IMPLEMENTATION, P531
   [Anonymous], P ACM MMSYS 17
   [Anonymous], 2017, DISPL TECHN VIRT REA
   [Anonymous], 2017, JVETG1030
   [Anonymous], 2017, 012 ETSI GS MEC
   [Anonymous], P 18 ACM INT C MOD A
   [Anonymous], 2016, WHIT VR OR BEAR NETW
   Bao Y., 2017, P 14 ANN IEEE INT C, P1, DOI DOI 10.1109/SAHCN.2017.7964928
   Budagavi M, 2015, IEEE IMAGE PROC, P750, DOI 10.1109/ICIP.2015.7350899
   Choi MW, 2016, IEEE T MULTIMEDIA, V18, P627, DOI 10.1109/TMM.2016.2525012
   CORBILLON X, 2017, P IEEE INT C COMM SY
   Corbillon X, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P199, DOI 10.1145/3083187.3083215
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Drago M, 2018, INT CONF COMPUT NETW, P508, DOI 10.1109/ICCNC.2018.8390387
   Ehrgott  M., 2003, ADV SOFT COMPUTING, V21
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Hassan M., 2017, P IEEE GLOB COMM C D, P1
   He YJ, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON RFID TECHNOLOGY AND APPLICATIONS (RFID-TA), P1, DOI 10.1109/RFID-TA.2016.7750739
   Herglotz C, 2019, IEEE T CIRC SYST VID, V29, P171, DOI 10.1109/TCSVT.2017.2771819
   Kim J, 2017, MOB INF SYST, V2017, DOI 10.1155/2017/5040347
   Lei L, 2014, 2014 11TH INTERNATIONAL SYMPOSIUM ON WIRELESS COMMUNICATIONS SYSTEMS (ISWCS), P187, DOI 10.1109/ISWCS.2014.6933344
   Li HC, 2016, IEEE GEOSCI REMOTE S, V13, P3, DOI 10.1109/LGRS.2015.2487141
   Liu K., 2017, PROC IREP S, P1
   Lu ZJ, 2003, PR IEEE COMP DESIGN, P489, DOI 10.1109/ICCD.2003.1240945
   Milan P., 2014, White paper, P854
   MPEG Experts, 2016, 1SC29WG11 ISOIEC JTC
   Ng KT, 2005, IEEE T CIRC SYST VID, V15, P82, DOI 10.1109/TCSVT.2004.839989
   Petrangeli  S., 2017, ACM MULT C MM17 MOUN, P306
   Polese M, 2017, IEEE J SEL AREA COMM, V35, P2069, DOI 10.1109/JSAC.2017.2720338
   Qiao J, 2015, IEEE T WIREL COMMUN, V14, P5692, DOI 10.1109/TWC.2015.2441708
   Qualcomm Technologies Inc, 2016, WHIT MAK IMM VIRT RE
   Roh W, 2014, IEEE COMMUN MAG, V52, P106, DOI 10.1109/MCOM.2014.6736750
   Shen ZK, 2005, IEEE T WIREL COMMUN, V4, P2726, DOI 10.1109/TWC.2005.858010
   Singh H, 2008, IEEE COMMUN MAG, V46, P71, DOI 10.1109/MCOM.2008.4689210
   Song  Y., 2017, ARXIV171010755
   Woltering M., 2013, Performance of HARQ with reduced size retransmissions using network coding principles, P1
   Wu CL, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P193, DOI 10.1145/3083187.3083210
   Xiao MB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P708, DOI 10.1145/3123266.3123339
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Yu M, 2015, 2015 IEEE International Symposium on Mixed and Augmented Reality, P31, DOI 10.1109/ISMAR.2015.12
   Yu Matt., 2015, P 3 INT WORKSHOP IMM, P1
   Zhao PH, 2014, IEEE GLOB COMM CONF, P1394, DOI 10.1109/GLOCOM.2014.7037003
   2018, IEEE COMMUN SURV TUT, V20, P2237, DOI DOI 10.1109/COMST.2018.2828880
   2014, IEEE COMMUN MAG, V52, P82
   2015, P IEEE INT C IM PROC, P2244
NR 46
TC 70
Z9 70
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1302
EP 1316
DI 10.1109/TMM.2018.2876044
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600018
DA 2024-07-18
ER

PT J
AU Qiao, T
   Shi, R
   Luo, XY
   Xu, M
   Zheng, N
   Wu, YM
AF Qiao, Tong
   Shi, Ran
   Luo, Xiangyang
   Xu, Ming
   Zheng, Ning
   Wu, Yiming
TI Statistical Model-Based Detector via Texture Weight Map: Application in
   Re-Sampling Authentication
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Digital forensics; resampling forgery; Gaussian noise model; nuisance
   parameters; hypothesis testing
ID IDENTIFICATION; FORGERY
AB The problem of authenticating a re-sampled image has been investigated over many years. Currently, however, little research proposes a statistical model-based test, resulting in that statistical performance of the resampling detector could not be completely analyzed. To fill the gap, we utilize a parametric model to expose the traces of resampling forgery, which is described with the distribution of residual noise. Afterward, we propose a statistical model describing the residual noise from a resampled image. Then, the detection problem is cast into the framework of hypothesis testing theory. By considering the image content with designing a texture weight map, two types of statistical detectors are established. In an ideal context in which all distribution parameters are perfectly known, the likelihood ratio test (LRT) is presented and its performance is theoretically established. An upper bound of the detection power can be successfully obtained from the statistical performance of an LRT. For practical use, when the distribution parameters are not known, a generalized LRT with three different maps based on estimation of parameters is established. Numerical results on simulated data and real natural images highlight the relevance of our proposed approach.
C1 [Qiao, Tong; Xu, Ming; Zheng, Ning; Wu, Yiming] Hangzhou Dianzi Univ, Sch Cyberspace, Hangzhou 310000, Zhejiang, Peoples R China.
   [Qiao, Tong; Luo, Xiangyang] Zhengzhou Sci & Technol Inst, State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Henan, Peoples R China.
   [Shi, Ran] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Jiangsu, Peoples R China.
C3 Hangzhou Dianzi University; PLA Information Engineering University;
   Nanjing University of Science & Technology
RP Luo, XY (corresponding author), Zhengzhou Sci & Technol Inst, State Key Lab Math Engn & Adv Comp, Zhengzhou 450001, Henan, Peoples R China.
EM tong.qiao@hdu.edu.cn; rshi@njust.edu.cn; xiangyangluo@126.com;
   mxu@hdu.edu.cn; nzheng@hdu.edu.cn; ymwu@hdu.edu.cn
OI Qiao, Tong/0000-0003-4912-2132; Wu, Yiming/0000-0001-9766-2307
FU Natural Science Foundation of China [61702150, 61379151, U1636219];
   National Key R&D Program of China [2016YFB0801303, 2016QY01W0105]; Plan
   for Scientific Innovation Talent of Henan Province [2018JR]; State Key
   Program of Zhejiang Province Natural Science Foundation of China
   [LZ15F020003]; Key research and development plan project of Zhejiang
   Province [2017C01062, 2017C01065]; Public Research Project of Zhejiang
   Province [LGG18F020015]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61702150, 61379151, and U1636219, in part by the
   National Key R&D Program of China under Grants 2016YFB0801303 and
   2016QY01W0105, in part by the Plan for Scientific Innovation Talent of
   Henan Province under Grant 2018JR, in part by the State Key Program of
   Zhejiang Province Natural Science Foundation of China under Grant
   LZ15F020003, in part by the Key research and development plan project of
   Zhejiang Province under Grants 2017C01062 and 2017C01065, and in part by
   the Public Research Project of Zhejiang Province under Grant
   LGG18F020015.
CR [Anonymous], 2011, P 13 INF HID C PRAG
   [Anonymous], 1979, IEEE T SYST MAN CYBE, DOI DOI 10.1109/TSMC.1979.4310076
   Correia PL, 2003, IEEE T IMAGE PROCESS, V12, P186, DOI 10.1109/TIP.2002.807355
   F. Technologies, 2016, PHOT TAMP HIST
   Feng XY, 2012, IEEE T MULTIMEDIA, V14, P536, DOI 10.1109/TMM.2012.2191946
   Gallagher AC, 2005, 2nd Canadian Conference on Computer and Robot Vision, Proceedings, P65, DOI 10.1109/CRV.2005.33
   Jin ZW, 2017, IEEE T MULTIMEDIA, V19, P598, DOI 10.1109/TMM.2016.2617078
   Kirchner M, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P13
   Kirchner M, 2008, MM&SEC'08: PROCEEDINGS OF THE MULTIMEDIA & SECURITY WORKSHOP 2008, P11, DOI 10.1145/1411328.1411333
   LEHMANN E. L., 2006, Springer Texts in Statistics
   Li C, 2017, NEUROCOMPUTING, V228, P29, DOI 10.1016/j.neucom.2016.04.068
   Li HD, 2018, IEEE T CIRC SYST VID, V28, P31, DOI 10.1109/TCSVT.2016.2599849
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Lin XD, 2016, IEEE T MULTIMEDIA, V18, P1480, DOI 10.1109/TMM.2016.2571999
   Mahdian B, 2008, IEEE T INF FOREN SEC, V3, P529, DOI 10.1109/TIFS.2004.924603
   Pasquini C, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P3, DOI 10.1145/3082031.3083233
   Popescu AC, 2005, IEEE T SIGNAL PROCES, V53, P758, DOI 10.1109/TSP.2004.839932
   Qiao T., 2014, P 2 ACM WORKSHOP INF, P3
   Qiao T, 2018, MULTIMED TOOLS APPL, V77, P1501, DOI 10.1007/s11042-016-4314-1
   Qiao T, 2017, SIGNAL PROCESS-IMAGE, V52, P74, DOI 10.1016/j.image.2016.12.011
   Qiao T, 2015, IEEE IMAGE PROC, P3812, DOI 10.1109/ICIP.2015.7351518
   Qiao T, 2014, IEEE IMAGE PROC, P5517, DOI 10.1109/ICIP.2014.7026116
   Qiao T, 2015, EURASIP J INF SECUR, DOI 10.1186/s13635-015-0019-7
   Ryu SJ, 2014, PATTERN RECOGN LETT, V36, P89, DOI 10.1016/j.patrec.2013.09.028
   Schaefer G, 2004, PROC SPIE, V5307, P472, DOI 10.1117/12.525375
   Sencar HusrevTaha., 2012, Digital Image Forensics - There is More to a Picture than Meets the Eye, DOI DOI 10.1007/978-1-4614-0757-7
   Su LC, 2018, IEEE T MULTIMEDIA, V20, P825, DOI 10.1109/TMM.2017.2760098
   Su YT, 2017, J VIS COMMUN IMAGE R, V48, P480, DOI 10.1016/j.jvcir.2017.01.009
   Thai TH, 2014, IEEE T IMAGE PROCESS, V23, P250, DOI 10.1109/TIP.2013.2290596
   Valsesia D, 2015, IEEE T MULTIMEDIA, V17, P1439, DOI 10.1109/TMM.2015.2455417
   Vázquez-Padín D, 2017, IEEE T INF FOREN SEC, V12, P2115, DOI 10.1109/TIFS.2017.2699638
   Vázquez-Padín D, 2013, IEEE INT WORKS INFOR, P150, DOI 10.1109/WIFS.2013.6707810
   Zhao XD, 2015, IEEE T CIRC SYST VID, V25, P185, DOI 10.1109/TCSVT.2014.2347513
   Zhu N, 2016, NEUROCOMPUTING, V204, P33, DOI 10.1016/j.neucom.2015.06.113
   2010, IEEE IMAGE PROC, P1745
NR 35
TC 40
Z9 40
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2019
VL 21
IS 5
BP 1077
EP 1092
DI 10.1109/TMM.2018.2872863
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HV8HY
UT WOS:000466223600001
DA 2024-07-18
ER

PT J
AU Garg, S
   Kaur, K
   Kumar, N
   Rodrigues, JJPC
AF Garg, Sahil
   Kaur, Kuljeet
   Kumar, Neeraj
   Rodrigues, Joel J. P. C.
TI Hybrid Deep-Learning-Based Anomaly Detection Scheme for Suspicious Flow
   Detection in SDN: A Social Multimedia Perspective
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Anomaly detection; deep learning; flow routing; software defined
   networks; social multimedia
ID SOFTWARE-DEFINED-NETWORKING; INTRUSION DETECTION; NEURAL-NETWORKS;
   THREATS; SYSTEM
AB The continuous development and usage of multimedia-based applications and services have contributed to the exponential growth of social multimedia traffic. In this context, secure transmission of data plays a critical role in realizing all of the key requirements of social multimedia networks such as reliability, scalability, quality of information, and quality of service (QoS). Thus, a trust-based paradigm for multimedia analytics is highly desired to meet the increasing user requirements and deliver more timely and actionable insights. In this regard, software-defined networks (SDNs) play a vital role; however, several factors such as as-runtime security, and energy-aware networking limit its capabilities to facilitate efficient network control and management. Thus, with the view to enhance the reliability of the SDN, a hybrid deep-learning-based anomaly detection scheme for suspicious flow detection in the context of social multimedia is proposed. It consists of the following two modules: 1) an anomaly detection module that leverages improved restricted Boltzmann machine and gradient descent-based support vector machine to detect the abnormal activities, and 2) an end-to-end data delivery module to satisfy strict QoS requirements of the SDN, that is, high bandwidth and low latency. Finally, the proposed scheme has been experimentally evaluated on both real-time and benchmark datasets to prove its effectiveness and efficiency in terms of anomaly detection and data delivery essential for social multimedia. Further, a large-scale analysis over a Carnegie Mellon University (CMU)-based insider threat dataset has been conducted to identify its performance in terms of detecting malicious events such as-Identity theft, profile cloning, confidential data collection, etc.
C1 [Garg, Sahil; Kaur, Kuljeet] Univ Quebec, Dept Elect Engn, Ecole Technol Super, Montreal, PQ H3C 1K3, Canada.
   [Kumar, Neeraj] Thapar Inst Engn & Technol Deemed Be Univ, Dept Comp Sci & Engn, Patiala 147003, Punjab, India.
   [Rodrigues, Joel J. P. C.] Natl Inst Telecommun Inatel, BR-37540000 Santa Rita Do Sapuca, Brazil.
   [Rodrigues, Joel J. P. C.] Inst Telecomunicacoes, P-3810164 Aveiro, Portugal.
   [Rodrigues, Joel J. P. C.] Univ Fortaleza UNIFOR, BR-60811905 Fortaleza, Ceara, Brazil.
C3 University of Quebec; Ecole de Technologie Superieure - Canada;
   University of Quebec Montreal; Thapar Institute of Engineering &
   Technology; Instituto Nacional de Telecomunicacoes (INATEL);
   Universidade de Aveiro; Universidade Fortaleza
RP Rodrigues, JJPC (corresponding author), Natl Inst Telecommun Inatel, BR-37540000 Santa Rita Do Sapuca, Brazil.
EM garg.sahil1990@gmail.com; kuljeet0389@gmail.com;
   neeraj.kumar@thapar.edu; joeljr@ieee.org
RI Kumar, Neeraj/L-3500-2016; Rodrigues, Joel J. P. C./A-8103-2013; Kaur,
   Kuljeet/AAD-8655-2020; Garg, Sahil/AAN-2480-2020
OI Kumar, Neeraj/0000-0002-3020-3947; Rodrigues, Joel J. P.
   C./0000-0001-8657-3800; Kaur, Kuljeet/0000-0003-4597-1700; Garg,
   Sahil/0000-0003-0229-608X
FU FCT-Fundacao para a Ciencia e a Tecnologia [UID/EEA/50008/2013]; Finep;
   Funttel, under the Centro de Referencia em Radiocomunicacoes-CRR project
   of the Instituto Nacional de Telecomunicacoes (Inatel), Brazil
   [01.14.0231.00]; Brazilian National Council for Research and Development
   (CNPq) [309335/2017-5]
FX This work was supported in part by the National Funding from the
   FCT-Fundacao para a Ciencia e a Tecnologia through the
   UID/EEA/50008/2013 Project; in part by Finep, with resources from
   Funttel, under Grant 01.14.0231.00, under the Centro de Referencia em
   Radiocomunicacoes-CRR project of the Instituto Nacional de
   Telecomunicacoes (Inatel), Brazil; and in part by the Brazilian National
   Council for Research and Development (CNPq) under Grant 309335/2017-5.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Honggang Wang. (Corresponding
   author: Joel J. P. C. Rodrigues.)
CR Al-Yaseen WL, 2017, EXPERT SYST APPL, V67, P296, DOI 10.1016/j.eswa.2016.09.041
   Cid-Fuentes JA, 2020, IEEE T DEPEND SECURE, V17, P928, DOI 10.1109/TDSC.2018.2821693
   Alvi SA, 2015, AD HOC NETW, V33, P87, DOI 10.1016/j.adhoc.2015.04.006
   [Anonymous], 2017, GLOBECOM 2017 2017 I, P1
   Bigdeli E, 2018, INFORM SCIENCES, V429, P315, DOI 10.1016/j.ins.2017.11.023
   Carvalho LF, 2018, EXPERT SYST APPL, V104, P121, DOI 10.1016/j.eswa.2018.03.027
   Chaffey  D., 2018, TECH REP
   Chattopadhyay P, 2018, IEEE T COMPUT SOC SY, V5, P660, DOI 10.1109/TCSS.2018.2857473
   Chaudhary R, 2018, IEEE T IND INFORM, V14, P2629, DOI 10.1109/TII.2018.2789442
   Chiba Z, 2018, COMPUT SECUR, V75, P36, DOI 10.1016/j.cose.2018.01.023
   Chu WQ, 2019, IEEE T MULTIMEDIA, V21, P246, DOI 10.1109/TMM.2018.2846411
   de la Rosa E, 2020, IEEE T SYST MAN CY-S, V50, P2316, DOI 10.1109/TSMC.2018.2812156
   Feng YC, 2017, NEUROCOMPUTING, V219, P548, DOI 10.1016/j.neucom.2016.09.063
   Fire M, 2014, IEEE COMMUN SURV TUT, V16, P2019, DOI 10.1109/COMST.2014.2321628
   Garg S., 2017, P 36 IEEE GLOB COMM, P1, DOI [10.1109/GLOCOM.2017.8255025, DOI 10.1109/GLOCOM.2017.8255025]
   Garg S., 2018, 2018 IEEE International Conference on Communications (ICC), P1, DOI [10.1109/ATSIP.2018.8364481, DOI 10.1109/ATSIP.2018.8364481]
   Garg S, 2018, COMPUT ELECTR ENG, V71, P798, DOI 10.1016/j.compeleceng.2017.07.008
   Garg S, 2017, INT J COMMUN SYST, V30, DOI 10.1002/dac.3248
   Guo C, 2016, NEUROCOMPUTING, V214, P391, DOI 10.1016/j.neucom.2016.06.021
   Ha T, 2016, COMPUT NETW, V109, P172, DOI 10.1016/j.comnet.2016.05.019
   He DJ, 2017, IEEE INTERNET THINGS, V4, P1890, DOI 10.1109/JIOT.2017.2694702
   Hinton G. E., 2012, Neural networks: tricks of the trade, P599
   Jiang Y, 2017, OPTIK, V140, P794, DOI 10.1016/j.ijleo.2017.02.088
   Kaur K, 2018, IEEE COMMUN MAG, V56, P44, DOI 10.1109/MCOM.2018.1700622
   Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991
   Li H, 2018, NATL SCI REV, V5, P24, DOI 10.1093/nsr/nwx110
   Lopez-Martin M, 2018, IEEE COMMUN MAG, V56, P110, DOI 10.1109/MCOM.2018.1701156
   Micholia P, 2018, IEEE COMMUN SURV TUT, V20, P3581, DOI 10.1109/COMST.2018.2817686
   Molina E, 2018, COMPUT ELECTR ENG, V66, P407, DOI 10.1016/j.compeleceng.2017.05.013
   Moore A. P., 2011, CMUSEI2011TN013
   Peng HJ, 2018, IEEE ACCESS, V6, P27809, DOI 10.1109/ACCESS.2018.2839684
   Perdisci R, 2009, COMPUT NETW, V53, P864, DOI 10.1016/j.comnet.2008.11.011
   Rathore S, 2017, INFORM SCIENCES, V421, P43, DOI 10.1016/j.ins.2017.08.063
   Ribeiro M, 2018, PATTERN RECOGN LETT, V105, P13, DOI 10.1016/j.patrec.2017.07.016
   Sabokrou M, 2018, COMPUT VIS IMAGE UND, V172, P88, DOI 10.1016/j.cviu.2018.02.006
   Shone N, 2018, IEEE T EM TOP COMP I, V2, P41, DOI 10.1109/TETCI.2017.2772792
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun X, 2020, MULTIMED TOOLS APPL, V79, P9687, DOI 10.1007/s11042-018-5665-6
   Tan ZY, 2014, IEEE T PARALL DISTR, V25, P447, DOI 10.1109/TPDS.2013.146
   Tsai PW, 2018, IEEE SYST J, V12, P3958, DOI 10.1109/JSYST.2018.2798060
   Wan M, 2017, IEEE T INF FOREN SEC, V12, P3011, DOI 10.1109/TIFS.2017.2730581
   Xu D, 2017, COMPUT VIS IMAGE UND, V156, P117, DOI 10.1016/j.cviu.2016.10.010
   Xu K, 2018, IEEE T MULTIMEDIA, V20, P1062, DOI 10.1109/TMM.2018.2818942
   Zhang Z, 2016, FUTURE GENER COMP SY, V86, P914
   Zisserman A., 2013, C19 MACHINE LEARNING
NR 45
TC 164
Z9 168
U1 2
U2 99
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 566
EP 578
DI 10.1109/TMM.2019.2893549
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800004
DA 2024-07-18
ER

PT J
AU Liu, SG
   Chai, QP
AF Liu, Shiguang
   Chai, Qingpeng
TI Shape-Optimizing and Illumination-Smoothing Image Stitching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image stitching; hybrid transformation; projective distortion; color
   difference; triangulation
ID COLOR TRANSFER; ARTIFACTS; WARPS
AB Image stitching is usually subject to projective distortion and color difference problems. This paper presents an illumination-smoothing image stitching method based on the shape-optimizing hybrid transformation. An automatic mesh generation strategy is especially designed to reduce the calculation of the hybrid transformation within a reasonable range and guarantee the accuracy of image alignment. We consider stitching multiple images while constraining the distortion with the hybrid transformation and alleviate the color difference. In this paper, the method of color correction is adapted according to the characteristics of image stitching so as to achieve illumination-smoothing results. Moreover, the triangulation of the matching feature points is used to partition the color regions of the image and then more abundant color information is obtained for the calculation of the color transformation model. For general horizontal images, our method can achieve better results with less distortion in comparison with state-of-the-art methods. Various experimental results validate our new method.
C1 [Liu, Shiguang; Chai, Qingpeng] Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China.
   [Liu, Shiguang] Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; qpchai@tju.edu.cn
FU Natural Science Foundation of China [61672375, 61170118]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 61672375 and 61170118. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Cha Zhang. (Corresponding author: Shiguang Liu.)
CR Agarwala A, 2004, ACM T GRAPHIC, V23, P294, DOI 10.1145/1015706.1015718
   Badra F, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P50, DOI 10.1109/ACV.1998.732857
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Chai Q., 2016, P IEEE INT C MULTIME, P1
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chang CH, 2014, INT C PATT RECOG, P64, DOI 10.1109/ICPR.2014.21
   Faridul HS, 2014, IEEE IMAGE PROC, P56, DOI 10.1109/ICIP.2014.7025010
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Gupta MR, 2005, PROC SPIE, V5674, P248, DOI 10.1117/12.598888
   HaCohen Y, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964965
   HE K, 2013, ACM T GRAPHIC, V32, P79
   He L, 2015, SIGNAL IMAGE VIDEO P, V9, P1965, DOI 10.1007/s11760-014-0691-y
   Igarashi T, 2005, ACM T GRAPHIC, V24, P1134, DOI 10.1145/1073204.1073323
   Ji Hu, 2015, 2015 17th European Conference on Power Electronics and Applications (EPE'15 ECCE-Europe), P1, DOI 10.1109/EPE.2015.7309180
   Li Feng, 2004, Proceedings. Third International Conference on Image and Graphics, P108
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, COMPUT GRAPH FORUM, V35, P479, DOI 10.1111/cgf.12848
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu Y, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1559755.1559758
   Lu SP, 2015, IEEE T MULTIMEDIA, V17, P577, DOI 10.1109/TMM.2015.2412879
   Lucas B D, 1981, P INT JOINT C ART IN, P285
   Ly DS, 2014, IEEE IMAGE PROC, P640, DOI 10.1109/ICIP.2014.7025128
   Qu Z, 2015, MATH PROBL ENG, V2015, DOI 10.1155/2015/428076
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shewchuk J. R., 1996, Applied Computational Geometry. Towards Geometric Engineering. FCRC'96 Workshop, WACG'96. Selected Papers, P203, DOI 10.1007/BFb0014497
   Shimada M, 2009, IEEE T GEOSCI REMOTE, V47, P3915, DOI 10.1109/TGRS.2009.2023909
   Shum HY, 2000, INT J COMPUT VISION, V36, P101, DOI 10.1023/A:1008195814169
   Song ZC, 2017, IEEE T MULTIMEDIA, V19, P702, DOI 10.1109/TMM.2016.2631123
   Su Z, 2014, IEEE T MULTIMEDIA, V16, P988, DOI 10.1109/TMM.2014.2305914
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tian GY, 2002, SIXTH INTERNATIONAL CONFERENCE ON INFORMATION VISUALISATION, PROCEEDINGS, P483, DOI 10.1109/IV.2002.1028817
   Uyttendaele M, 2001, PROC CVPR IEEE, P509
   Vedaldi A., 2010, P 18 ACM INT C MULT, P1469, DOI DOI 10.1145/1873951.1874249
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xiao Xuezhong, 2006, PACM INT C VIRT REAL, P305, DOI DOI 10.1145/1128923.1128974
   Xu N, 2014, I SYMP CONSUM ELECTR, P224
   Xu W, 2010, PROC CVPR IEEE, P263, DOI 10.1109/CVPR.2010.5540202
   Yan W, 2016, INT J MACH LEARN CYB, P1
   Zaragoza J, 2013, PROC CVPR IEEE, P2339, DOI 10.1109/CVPR.2013.303
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
NR 41
TC 18
Z9 21
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2019
VL 21
IS 3
BP 690
EP 703
DI 10.1109/TMM.2018.2864576
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HN6YJ
UT WOS:000460333800014
DA 2024-07-18
ER

PT J
AU Gao, L
   Zhang, R
   Qi, L
   Chen, EQ
   Guan, L
AF Gao, Lei
   Zhang, Rui
   Qi, Lin
   Chen, Enqing
   Guan, Ling
TI The Labeled Multiple Canonical Correlation Analysis for Information
   Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Labeled multiple canonical correlation analysis (LMCCA); information
   fusion; handwritten digit recognition; face recognition; object
   recognition; human emotion recognition
ID EMOTION RECOGNITION; CLASSIFICATION; KERNEL; FEATURES; REPRESENTATION;
   MACHINE; MODELS
AB The objective of multimodal information fusion is to mathematically analyze information carried in different sources and create a new representation that will be more effectively utilized in pattern recognition and other multimedia information processing tasks. In this paper, we introduce a new method for multimodal information fusion and representation based on the Labeled Multiple Canonical Correlation Analysis (LMCCA). By incorporating class label information of the training samples, the proposed LMCCA ensures that the fused features carry discriminative characteristics of the multimodal information representations and are capable of providing superior recognition performance. We implement a prototype of LMCCA to demonstrate its effectiveness on handwritten digit recognition, face recognition, and object recognition utilizing multiple features, bimodal human emotion recognition involving information from both audio and visual domains. The generic nature of LMCCA allows it to take as input features extracted by any means, including those by deep learning (DL) methods. Experimental results show that the proposed method enhanced the performance of both statistical machine learning methods, and methods based on DL.
C1 [Gao, Lei; Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
   [Zhang, Rui] Epson Canada, Toronto, ON L3R 6G3, Canada.
   [Qi, Lin; Chen, Enqing] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450066, Henan, Peoples R China.
C3 Toronto Metropolitan University; Zhengzhou University
RP Gao, L (corresponding author), Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
EM iegaolei@gmail.com; ray.rui.zhang@gmail.com; ielqi@zzu.edu.cn;
   ieeqchen@zzu.edu.cn; lguan@ee.ryerson.ca
RI ARSLAN, Okan/AAA-3232-2020
OI Gao, Lei/0000-0001-5583-713X
FU National Natural Science Foundation of China (NSFC) [61071211]; State
   Key Program of NSFC [61331021]; Key International Collaboration Program
   of NSFC [61210005]; Discovery Grant of Natural Science and Engineering
   Council of Canada [238813/2010]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 61071211, in part by the State
   Key Program of NSFC under Grant 61331021, in part by the Key
   International Collaboration Program of NSFC under Grant 61210005, and in
   part by the Discovery Grant of Natural Science and Engineering Council
   of Canada under Grant 238813/2010. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Abdulmotaleb El Saddik.
CR [Anonymous], 2017, MULTIMED TOOLS APPL
   [Anonymous], L1JS SIMPLE MATLAB S
   [Anonymous], 2006, 22 INT C DATA ENG WO
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE T CIRCUITS SYST
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Black MJ, 1997, INT J COMPUT VISION, V25, P23, DOI 10.1023/A:1007977618277
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen X, 2012, IEEE T MULTIMEDIA, V14, P3, DOI 10.1109/TMM.2011.2167223
   Cowie R, 2001, IEEE SIGNAL PROC MAG, V18, P32, DOI 10.1109/79.911197
   Dai DX, 2013, IEEE I CONF COMP VIS, P2072, DOI 10.1109/ICCV.2013.259
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Du B, 2017, IEEE T CYBERNETICS, V47, P1017, DOI 10.1109/TCYB.2016.2536638
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Evangelopoulos G, 2013, IEEE T MULTIMEDIA, V15, P1553, DOI 10.1109/TMM.2013.2267205
   Gao L, 2018, IEEE T IMAGE PROCESS, V27, P1951, DOI 10.1109/TIP.2017.2765820
   Gao SH, 2016, IEEE T CIRC SYST VID, V26, P494, DOI 10.1109/TCSVT.2015.2389413
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hamamoto Y, 1998, PATTERN RECOGN, V31, P395, DOI 10.1016/S0031-3203(97)00057-5
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Huang D, 2014, IEEE T IMAGE PROCESS, V23, P4680, DOI 10.1109/TIP.2014.2353814
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Khaleghi B, 2013, INFORM FUSION, V14, P28, DOI 10.1016/j.inffus.2011.08.001
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Lajevardi SM, 2012, IEEE T IMAGE PROCESS, V21, P3721, DOI 10.1109/TIP.2012.2197628
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li S, 2018, IEEE T NEUR NET LEAR, V29, P645, DOI 10.1109/TNNLS.2016.2633275
   Li YO, 2012, J SIGNAL PROCESS SYS, V68, P31, DOI 10.1007/s11265-010-0572-8
   Li YO, 2009, IEEE T SIGNAL PROCES, V57, P3918, DOI 10.1109/TSP.2009.2021636
   Lin GF, 2017, INFORM FUSION, V36, P275, DOI 10.1016/j.inffus.2016.12.010
   Ma C, 2019, IEEE T CYBERNETICS, V49, P781, DOI 10.1109/TCYB.2017.2785621
   Martìnez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974
   Melzer T, 2003, PATTERN RECOGN, V36, P1961, DOI 10.1016/S0031-3203(03)00058-X
   Nanni L, 2015, NEUROCOMPUTING, V149, P526, DOI 10.1016/j.neucom.2014.08.021
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Ooi CS, 2014, EXPERT SYST APPL, V41, P5858, DOI 10.1016/j.eswa.2014.03.026
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123
   Serre T, 2005, PROC CVPR IEEE, P994
   Shekar BH, 2011, NEUROCOMPUTING, V74, P1053, DOI 10.1016/j.neucom.2010.10.012
   Shen CC, 2014, J MULTIVARIATE ANAL, V130, P310, DOI 10.1016/j.jmva.2014.05.011
   Singh N, 2016, DIGIT SIGNAL PROCESS, V55, P22, DOI 10.1016/j.dsp.2016.05.003
   Sun LA, 2011, IEEE T PATTERN ANAL, V33, P194, DOI 10.1109/TPAMI.2010.160
   Sun QS, 2005, PATTERN RECOGN, V38, P449, DOI 10.1016/j.patcog.2004.08.009
   Tang P, 2017, IEEE T IMAGE PROCESS, V26, P3385, DOI 10.1109/TIP.2016.2642781
   Tenenhaus A, 2014, EUR J OPER RES, V238, P391, DOI 10.1016/j.ejor.2014.01.008
   Vía J, 2007, IEEE T SIGNAL PROCES, V55, P3867, DOI 10.1109/TSP.2007.894273
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1880, DOI 10.1109/TMM.2013.2269314
   Wu CH, 2011, IEEE T AFFECT COMPUT, V2, P10, DOI 10.1109/T-AFFC.2010.16
   Xiao R, 2011, PATTERN RECOGN, V44, P107, DOI 10.1016/j.patcog.2010.07.017
   Xiong W, 2017, PATTERN RECOGN, V62, P225, DOI 10.1016/j.patcog.2016.08.006
   Xu Y, 2017, IEEE T NEUR NET LEAR, V28, P2233, DOI 10.1109/TNNLS.2016.2580572
   Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292
   Yang J, 2003, PATTERN RECOGN, V36, P1369, DOI 10.1016/S0031-3203(02)00262-5
   Yang JF, 2012, PATTERN RECOGN LETT, V33, P623, DOI 10.1016/j.patrec.2011.11.002
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yeh YR, 2012, IEEE T MULTIMEDIA, V14, P563, DOI 10.1109/TMM.2012.2188783
   Yu HG, 2012, APPL MECH MATER, V195-196, P104, DOI 10.4028/www.scientific.net/AMM.195-196.104
   Yu W, 2018, COMPUT VIS IMAGE UND, V169, P40, DOI 10.1016/j.cviu.2018.01.001
   Yun T, 2013, PATTERN RECOGN, V46, P529, DOI 10.1016/j.patcog.2012.08.002
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhou Y, 2015, NEUROCOMPUTING, V151, P1042, DOI 10.1016/j.neucom.2014.04.083
NR 69
TC 29
Z9 29
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 375
EP 387
DI 10.1109/TMM.2018.2859590
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tan, ZY
   Zhang, Y
AF Tan, Zhiyi
   Zhang, Ya
TI Predicting the Top-N Popular Videos via a Cross-Domain Hybrid Model
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Popularity prediction; top-n popular videos; cross-domain
AB Predicting the top-N popular videos and their future views for a large batch of newly uploaded videos is of great commercial value to online video services (OVSs). Although many attempts have been made on video popularity prediction, the existing models has a much lower performance in predicting the top-N popular videos than that of the entire video set. The reason for this phenomenon is that most videos in an OVS system are unpopular, so models preferentially learn the popularity trends of unpopular videos to improve their performance on the entire video set. However, in most cases, it is critical to predict the performance on the top-N popular videos, which is the focus of this study. The challenge for the task are as follows. First, popular and unpopular videos may have similar early view patterns. Second, prediction models that are overly dependent on early view patterns limit the effects of other features. To address these challenges, we propose a novel multifactor differential influence prediction model based on multivariate linear regression. The model is designed to improve the discovery of popular videos and their popularity trends are learnt by enhancing the discriminative power of early patterns for different popularity trends and by optimizing the utilization of multisource data. We evaluate the proposed model using real-world YouTube data, and extensive experiments have demonstrated the effectiveness of our model.
C1 [Tan, Zhiyi; Zhang, Ya] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Zhang, Y (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM skt1zytan@qq.com; ya_zhang@sjtu.edu.cn
RI Zhang, Ya/Y-8255-2019
OI Zhang, Ya/0000-0002-5390-9053
FU High Technology Research and Development Program of China
   [2015AA015801]; National Natural Science Foundation of China [61521062];
   STCSM [18DZ2270700]
FX This work was supported in part by the High Technology Research and
   Development Program of China under Grant 2015AA015801, in part by the
   National Natural Science Foundation of China under Grant 61521062, and
   in part by the STCSM under Grant 18DZ2270700.
CR Ahmed Mohamed, 2013, P 6 ACM INT C WEB SE, P607, DOI [DOI 10.1145/2433396.2433473, 10.1145/2433396.2433473]
   [Anonymous], 2015, INT C INF KNOWL MAN, DOI DOI 10.1145/2806416.2806505
   [Anonymous], 2015, ACM T MULTIM COMPUT
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], ARXIV14042570
   Bao P, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1897, DOI 10.1145/2983323.2983868
   Bollegala D, 2016, IEEE T KNOWL DATA EN, V28, P398, DOI 10.1109/TKDE.2015.2475761
   Cha M, 2009, IEEE ACM T NETWORK, V17, P1357, DOI 10.1109/TNET.2008.2011358
   Chen JY, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P898, DOI 10.1145/2964284.2964314
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Hoiles W, 2017, IEEE T KNOWL DATA EN, V29, P1426, DOI 10.1109/TKDE.2017.2682858
   Hongliang Yu, 2006, Operating Systems Review, V40, P333, DOI 10.1145/1218063.1217968
   Krishnappa DK, 2015, ACM T MULTIM COMPUT, V11, DOI 10.1145/2716310
   Li CY, 2016, IEEE ACCESS, V4, P1630, DOI 10.1109/ACCESS.2016.2552218
   Li H, 2013, TRANSGENIC RES, V22, P169, DOI 10.1007/s11248-012-9623-1
   Long Y, 2015, IEEE ICC, P1244, DOI 10.1109/ICC.2015.7248493
   Ouyang SX, 2016, IEEE ACCESS, V4, P3026, DOI 10.1109/ACCESS.2016.2580911
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Susarla A, 2012, INFORM SYST RES, V23, P23, DOI 10.1287/isre.1100.0339
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tan ZY, 2016, IEEE T BROADCAST, V62, P436, DOI 10.1109/TBC.2016.2540522
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Wang A, 2016, HOTPOST 2016 P 8 MOB, P7, DOI [10.1145/2944789.2944872, DOI 10.1145/2944789.2944872]
   Wu JQ, 2016, IEEE T MULTIMEDIA, V18, P1882, DOI 10.1109/TMM.2016.2579600
   Xu J, 2015, IEEE J-STSP, V9, P330, DOI 10.1109/JSTSP.2014.2370942
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
NR 26
TC 8
Z9 8
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 147
EP 156
DI 10.1109/TMM.2018.2845688
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700013
DA 2024-07-18
ER

PT J
AU Getreuer, P
   Gnegy, C
   Lyon, RF
   Saurous, RA
AF Getreuer, Pascal
   Gnegy, Chet
   Lyon, Richard F.
   Saurous, Rif A.
TI Ultrasonic Communication Using Consumer Hardware
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustics; audio systems; ultrasound; spread spectrum communication;
   device-to-device communication; indoor communication; mobile
   applications
AB We have implemented a near-ultrasonic communication protocol in the 18.5-20 kHz band, which is inaudible to most humans, using commodity smartphone speakers and microphones to transmit and receive signals. The protocol described in this paper is a component of Google's Nearby platform, where near-ultrasound signals are used to establish copresence between nearby devices by transmitting a short token. High-frequency sound does not pass through walls (most energy is reflected), so identified devices are constrained to approximately the same room, "within earshot" of one another. Our protocol has a raw data rate of 94.5 b/s, and we find in real indoor environments that transmission between mobile devices is reliable at 2 m distance and often works at 10 m. We use direct-sequence spread spectrum modulation, which makes it highly robust to multipath, motion, and narrowband noise. We use a 127-chip pseudorandom code, repeating once per data symbol, and modulate its amplitude with orthogonal sine waveforms encoding 4-bit symbol values. We add the orthogonal sines to a constant "pedestal," which is inefficient in an information-theoretic sense, but makes synchronization easier. We describe a robust and computationally efficient transmitter and receiver implementations and show experiments on real and simulated data.
C1 [Getreuer, Pascal; Gnegy, Chet; Lyon, Richard F.; Saurous, Rif A.] Google Res, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Getreuer, P (corresponding author), Google Res, Mountain View, CA 94043 USA.
EM getreuer@google.com; chetgnegy@google.com; dicklyon@google.com;
   rif@google.com
OI Lyon, Richard/0000-0003-2348-811X
CR Alloulah M., 2010, P IPIN 2010, P1
   [Anonymous], 2014, CORR
   [Anonymous], P 8 USENIX WORKSH OF
   [Anonymous], 2016, THESIS
   [Anonymous], EIGEN A C LINEAR ALG
   [Anonymous], THESIS
   Ashihara K, 2006, ACOUST SCI TECHNOL, V27, P12, DOI 10.1250/ast.27.12
   Ballou G., 2015, HDB SOUND ENG, V5th
   Cowan J.P., 1993, Handbook of Environmental Acoustics
   Dixon R., 1976, Spread Spectrum Techniques
   Do Q, 2015, COMPUT SECUR, V48, P74, DOI 10.1016/j.cose.2014.10.016
   Galluccio L., 2012, 2012 9th Annual Conference on Wireless On-demand Network Systems and Services (WONS), P182, DOI 10.1109/WONS.2012.6152227
   GOLD R, 1967, IEEE T INFORM THEORY, V13, P619, DOI 10.1109/TIT.1967.1054048
   Hanspach M., 2014, LNI, V228, P243
   Hazas M., 2002, UbiComp 2002: Ubiquitous Computing. 4th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2498), P264
   Hyewon Lee, 2015, 2015 IEEE Conference on Computer Communications (INFOCOM). Proceedings, P2407, DOI 10.1109/INFOCOM.2015.7218629
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Kirovski Darko., 2001, LECT NOTES COMPUTER, V2137, P354
   Lazic N, 2006, IEEE T MULTIMEDIA, V8, P918, DOI 10.1109/TMM.2006.879880
   Lopes CV, 2001, PROCEEDINGS OF THE 2001 IEEE WORKSHOP ON THE APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS, P219, DOI 10.1109/ASPAA.2001.969582
   Matsuoka H., 2006, NTT DoCoMo Tech. J, V8, P2
   Nandakumar R, 2013, ACM SIGCOMM COMP COM, V43, P63, DOI 10.1145/2534169.2486037
   RALSTON H J, 1958, Int Z Angew Physiol, V17, P277
   Riva O, 2008, COMPUTER, V41, P23, DOI 10.1109/MC.2008.414
   Santagati G.E., 2015, P 13 ANN INT C MOBIL, P241
   Santagati GE, 2013, IEEE WIREL COMMUN, V20, DOI 10.1109/MWC.2013.6590053
   Santagati GE, 2015, IEEE ACM T NETWORK, V23, P1121, DOI 10.1109/TNET.2014.2316675
   Simon M., 1994, Spread Spectrum Communications Handbook, V1
   Sozer EM, 1999, OCEANS '99 MTS/IEEE : RIDING THE CREST INTO THE 21ST CENTURY, VOLS 1-3, P228, DOI 10.1109/OCEANS.1999.799743
   Winch R.G., 1998, TELECOMMUNICATION TR, V2nd
   Yun HS, 2010, IEEE SIGNAL PROC LET, V17, P67, DOI 10.1109/LSP.2009.2032751
NR 31
TC 15
Z9 20
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2018
VL 20
IS 6
BP 1277
EP 1290
DI 10.1109/TMM.2017.2766049
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GG0WG
UT WOS:000432400400001
OA hybrid
DA 2024-07-18
ER

PT J
AU Ning, GH
   Zhang, Z
   He, ZQ
AF Ning, Guanghan
   Zhang, Zhi
   He, Zhiquan
TI Knowledge-Guided Deep Fractal Neural Networks for Human Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human pose estimation; fractal networks; knowledge-guided learning
ID PICTORIAL STRUCTURES; RETRIEVAL; MODELS
AB Human pose estimation using deep neural networks aims to map input images with large variations into multiple body keypoints, which must satisfy a set of geometric constraints and interdependence imposed by the human body model. This is a very challenging nonlinear manifold learning process in a very high dimensional feature space. We believe that the deep neural network, which is inherently an algebraic computation system, is not the most efficient way to capture highly sophisticated human knowledge, for example those highly coupled geometric characteristics and interdependence between keypoints in human poses. In this work, we propose to explore how external knowledge can be effectively represented and injected into the deep neural networks to guide its training process using learned projections that impose proper prior. Specifically, we use the stacked hourglass design and inception-resnet module to construct a fractal network to regress human pose images into heatmaps with no explicit graphical modeling. We encode external knowledge with visual features, which are able to characterize the constraints of human body models and evaluate the fitness of intermediate network output. We then inject these external features into the neural network using a projection matrix learned using an auxiliary cost function. The effectiveness of the proposed inception-resnet module and the benefit in guided learning with knowledge projection is evaluated on two widely used human pose estimation benchmarks. Our approach achieves state-of-the-art performance on both datasets.
C1 [Ning, Guanghan; Zhang, Zhi] Univ Missouri, Dept Elect Engn & Comp Sci, Columbia, MO 65211 USA.
   [He, Zhiquan] Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
C3 University of Missouri System; University of Missouri Columbia; Shenzhen
   University
RP He, ZQ (corresponding author), Shenzhen Univ, Coll Informat Engn, Shenzhen 518060, Peoples R China.
EM gnxr9@mail.missouri.edu; zhiquan@szu.edu.cn; zzbhf@mail.missouri.edu
FU National Science Foundation [US Ignite 1647213, CyberSEES 1539389]
FX This work was supported in part by the National Science Foundation under
   Grants US Ignite 1647213 and CyberSEES 1539389.
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   [Anonymous], 2017, 31 AAAI C ART INT AA
   [Anonymous], 2010, BMVC
   [Anonymous], 2015, ARXIV150502496
   [Anonymous], 2016, ARXIV160502914
   [Anonymous], 2015, INAISTATS
   [Anonymous], 2009, P INT C ART INT STAT, DOI DOI 10.1145/3301282
   [Anonymous], 2011, Torch7: A matlab-like environment for machine learning
   Ba LJ, 2014, ADV NEUR IN, V27
   Belagiannis V, 2016, IEEE T PATTERN ANAL, V38, P1929, DOI 10.1109/TPAMI.2015.2509986
   Belagiannis V, 2015, IEEE I CONF COMP VIS, P2830, DOI 10.1109/ICCV.2015.324
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535
   Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dantone M, 2014, IEEE T PATTERN ANAL, V36, P2131, DOI 10.1109/TPAMI.2014.2318702
   Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391
   Eichner M, 2012, INT J COMPUT VISION, V99, P190, DOI 10.1007/s11263-012-0524-9
   Eichner M, 2012, IEEE T PATTERN ANAL, V34, P2282, DOI 10.1109/TPAMI.2012.85
   Fan XC, 2015, PROC CVPR IEEE, P1347, DOI 10.1109/CVPR.2015.7298740
   Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49
   Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468
   FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602
   Fu LR, 2017, IEEE T IMAGE PROCESS, V26, P927, DOI 10.1109/TIP.2016.2639441
   Gkioxari G, 2016, LECT NOTES COMPUT SC, V9908, P728, DOI 10.1007/978-3-319-46493-0_44
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G., 2015, arXiv preprint arXiv:1503.02531
   Hu PY, 2016, PROC CVPR IEEE, P5600, DOI 10.1109/CVPR.2016.604
   Ikizler-Cinbis N, 2012, IEEE T MULTIMEDIA, V14, P1031, DOI 10.1109/TMM.2012.2187180
   Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang H, 2011, IEEE T PATTERN ANAL, V33, P1911, DOI 10.1109/TPAMI.2011.92
   Kadu H, 2014, IEEE T MULTIMEDIA, V16, P2191, DOI 10.1109/TMM.2014.2360793
   Karlinsky L, 2012, LECT NOTES COMPUT SC, V7574, P326, DOI 10.1007/978-3-642-33712-3_24
   Li QW, 2017, IEEE ACCESS, V5, P443, DOI 10.1109/ACCESS.2016.2643439
   Lifshitz I, 2016, LECT NOTES COMPUT SC, V9906, P246, DOI 10.1007/978-3-319-46475-6_16
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Marcos-Ramiro A, 2015, IEEE T MULTIMEDIA, V17, P1721, DOI 10.1109/TMM.2015.2464152
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533
   Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433
   Pishchulin L, 2013, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2013.82
   Rafi U., 2016, P BRIT MACH VIS C
   Ramakrishna V, 2014, LECT NOTES COMPUT SC, V8690, P33, DOI 10.1007/978-3-319-10605-2_3
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren RD, 2012, IEEE T MULTIMEDIA, V14, P1652, DOI 10.1109/TMM.2012.2199971
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren SQ, 2014, PROC CVPR IEEE, P1685, DOI 10.1109/CVPR.2014.218
   Romero A., 2014, 3 INT C LEARN REPRES
   Sapp B, 2013, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2013.471
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun M, 2011, IEEE I CONF COMP VIS, P723, DOI 10.1109/ICCV.2011.6126309
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tian YD, 2012, LECT NOTES COMPUT SC, V7576, P256, DOI 10.1007/978-3-642-33715-4_19
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tompson J, 2014, ADV NEUR IN, V27
   Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Wang F, 2013, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2013.83
   Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511
   Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335
   Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261
   Yu X, 2016, LECT NOTES COMPUT SC, V9909, P52, DOI 10.1007/978-3-319-46454-1_4
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zhang XQ, 2014, IEEE T SYST MAN CY-S, V44, P580, DOI 10.1109/TSMC.2013.2280438
   Zhao L, 2015, IEEE T NEUR NET LEAR, V26, P3176, DOI 10.1109/TNNLS.2015.2411287
   Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1492, DOI 10.1109/TPAMI.2016.2526002
   Zhu XX, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.80
NR 74
TC 97
Z9 109
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1246
EP 1259
DI 10.1109/TMM.2017.2762010
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400018
OA Green Submitted, Bronze
DA 2024-07-18
ER

PT J
AU Hu, YT
   Zheng, L
   Yang, Y
   Huang, YF
AF Hu, Yuting
   Zheng, Liang
   Yang, Yi
   Huang, Yongfeng
TI Twitter100k: A Real-World Dataset for Weakly Supervised Cross-Media
   Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-media retrieval; Twitter100k dataset; weakly supervised method;
   text-image embeddings
ID FUSION
AB This paper contributes a new large-scale dataset for weakly supervised cross-media retrieval, named Twitter100k. Current datasets, such as Wikipedia, NUS Wide, and Flickr30k, have two major limitations. First, these datasets are lacking in content diversity, i.e., only some predefined classes are covered. Second, texts in these datasets are written in well-organized language, leading to inconsistency with realistic applications. To overcome these drawbacks, the proposed Twitter100k dataset is characterized by two aspects: it has 100 000 image text pairs randomly crawled from Twitter, and thus, has no constraint in the image categories; and text in Twitter100k is written in informal language by the users. Since strongly supervised methods leverage the class labels that may be missing in practice, this paper focuses on weakly supervised learning for cross-media retrieval, in which only text-image pairs are exploited during training. We extensively benchmark the performance of four subspace learning methods and three variants of the correspondence AutoEncoder, along with various text features on Wikipedia, Flickr30k, and Twitter100k. As a minor contribution, we also design a deep neural network to learn cross-modal embeddings tbr Twitter100k. Inspired by the characteristic of Twitter100k, we propose a method to integrate optical character recognition into cross-media retrieval. The experiment results show that the proposed method improves the baseline performance.
C1 [Hu, Yuting; Huang, Yongfeng] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Dept Elect Engn, Beijing 100084, Peoples R China.
   [Zheng, Liang; Yang, Yi] Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo, NSW 2007, Australia.
C3 Tsinghua University; University of Technology Sydney
RP Hu, YT (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Dept Elect Engn, Beijing 100084, Peoples R China.
EM huyt16@mails.tsinghua.edu.cn; liangzheng06@gmail.com;
   yi.yang@uts.edu.au; yfhuang@tsinghua.edu.cn
RI yang, yang/GWB-9426-2022; yang, yang/GVT-5210-2022; yang,
   yang/HGT-7999-2022; Lang, Ming/HIK-0758-2022; Yang, Yi/B-9273-2017
OI Yang, Yi/0000-0002-0512-880X; Zheng, Liang/0000-0002-1464-9500; Huang,
   Yongfeng/0000-0003-3825-2230
FU Key Program of the National Natural Science Foundation of China
   [U1536201, U1536207, U1636113]; Tsinghua Eudaoyuan Research Fund
FX This work was supported in part by the Key Program of the National
   Natural Science Foundation of China under Grant U1536201, Grant
   U1536207, and Grant U1636113, and in part by the Tsinghua Eudaoyuan
   Research Fund. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Xavier Giro-i-Nieto.
CR Andrew G., 2013, P ICML, P1247
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2013, Proceedings of the 21st ACM international conference on Multimedia
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blei DM, 2003, P 26 ANN INT ACM SIG, P127, DOI DOI 10.1145/860435.860460
   Bronstein MM, 2010, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2010.5539928
   Cao Y, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1445, DOI 10.1145/2939672.2939812
   Castanon G., 2015, P 23 ACM INT C MULTI, P391, DOI DOI 10.1145/2733373.2806229
   Chang XJ, 2016, IEEE T NEUR NET LEAR, V27, P1502, DOI 10.1109/TNNLS.2015.2441735
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dalton J, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1857, DOI 10.1145/2505515.2507880
   Deng C, 2016, IEEE T MULTIMEDIA, V18, P208, DOI 10.1109/TMM.2015.2508146
   Deng C, 2013, IEEE I CONF COMP VIS, P2600, DOI 10.1109/ICCV.2013.323
   Deng C, 2014, IEEE T MULTIMEDIA, V16, P785, DOI 10.1109/TMM.2014.2298841
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Ding K, 2017, IEEE T MULTIMEDIA, V19, P571, DOI 10.1109/TMM.2016.2625747
   Elhoseiny M, 2016, AAAI CONF ARTIF INTE, P3478
   Everitt B.S., 2001, Applied Multivariate Data Analysis, VSecond, P48, DOI DOI 10.1002/9781118887486.CH3
   Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902
   Grubinger M., 2006, Language Resources and Evaluation, P13
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hua Y, 2016, IEEE T MULTIMEDIA, V18, P1201, DOI 10.1109/TMM.2016.2535864
   Hwang SJ, 2012, IEEE T PATTERN ANAL, V34, P1145, DOI 10.1109/TPAMI.2011.190
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jia YQ, 2011, IEEE I CONF COMP VIS, P2407, DOI 10.1109/ICCV.2011.6126524
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Kumar Shaishav, 2011, P 22 INT JOINT C ART, P1360, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-230
   Lin ZJ, 2015, PROC CVPR IEEE, P3864, DOI 10.1109/CVPR.2015.7299011
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Ma L, 2015, IEEE I CONF COMP VIS, P2623, DOI 10.1109/ICCV.2015.301
   Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Putthividhya D, 2010, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2010.5540000
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Rosipal R, 2006, LECT NOTES COMPUT SC, V3940, P34, DOI 10.1007/11752790_2
   Sharma A, 2012, PROC CVPR IEEE, P2160, DOI 10.1109/CVPR.2012.6247923
   Simonyan K, 2015, IEEE INT C ICLR
   Singh B, 2015, IEEE I CONF COMP VIS, P4561, DOI 10.1109/ICCV.2015.518
   Socher R., 2014, Trans Assoc Comput Linguist, V2, P207, DOI [DOI 10.1162/TACLA00177, 10.1162/tacl_a_00177, DOI 10.1162/TACL_A_00177]
   Song J., 2013, P ACM SIGMOD INT C M, P785, DOI DOI 10.1145/2463676.2465274
   Tenenbaum JB, 1997, ADV NEUR IN, V9, P662
   Wang J, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P347, DOI 10.1145/2671188.2749341
   Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Wu S, 2014, PROC CVPR IEEE, P2665, DOI 10.1109/CVPR.2014.341
   Xu X, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P305, DOI 10.1145/2911996.2912056
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Yang YS, 2009, PROCEEDINGS OF 2009 CONFERENCE ON SYSTEMS SCIENCE, MANAGEMENT SCIENCE & SYSTEM DYNAMICS, VOL 8, P175, DOI 10.1145/1631272.1631298
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang L, 2017, IEEE T MULTIMEDIA, V19, P1220, DOI 10.1109/TMM.2016.2646219
   Zheng L, 2016, INT J COMPUT VISION, V120, P1, DOI 10.1007/s11263-016-0889-2
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2016, LECT NOTES COMPUT SC, V9910, P868, DOI 10.1007/978-3-319-46466-4_52
   Zheng L, 2014, IEEE T IMAGE PROCESS, V23, P3368, DOI 10.1109/TIP.2014.2330763
   Zhou JL, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P415
   Zhuang YT, 2008, IEEE T MULTIMEDIA, V10, P221, DOI 10.1109/TMM.2007.911822
NR 67
TC 38
Z9 43
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2018
VL 20
IS 4
BP 927
EP 938
DI 10.1109/TMM.2017.2760101
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FZ5HM
UT WOS:000427623000013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Liu, XC
   Liu, W
   Mei, T
   Ma, HD
AF Liu, Xinchen
   Liu, Wu
   Mei, Tao
   Ma, Huadong
TI PROVID: Progressive and Multimodal Vehicle Reidentification for
   Large-Scale Urban Surveillance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Progressive search; vehicle re-identification; deep learning; license
   plate verification; contextual information
AB Compared with person reidentification, which has attracted concentrated attention, vehicle reidentification is an important yet frontier problem in video surveillance and has been neglected by the multimedia and vision communities. Since most existing approaches mainly consider the general vehicle appearance for reidentification while overlooking the distinct vehicle identifier, such as the license plate number, they attain suboptimal performance. In this paper, we propose PROVID, a PROgressive Vehicle re-IDentification framework based on deep neural networks. In particular, our framework not only utilizes the multimodality data in large-scale video surveillance, such as visual features, license plates, camera locations, and contextual information, but also considers vehicle reidentification in two progressive procedures: coarse-to-fine search in the feature domain, and near-to-distant search in the physical space. Furthermore, to evaluate our progressive search framework and facilitate related research, we construct the VeRi dataset, which is the most comprehensive dataset from real-world surveillance videos. It not only provides large numbers of vehicles with varied labels and sufficient cross-camera recurrences but also contains license plate numbers and contextual information. Extensive experiments on the VeRi dataset demonstrate both the accuracy and efficiency of our progressive vehicle reidentification framework.
C1 [Liu, Xinchen; Liu, Wu; Ma, Huadong] Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
   [Mei, Tao] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Microsoft; Microsoft
   Research Asia
RP Ma, HD (corresponding author), Beijing Univ Posts & Telecommun, Beijing Key Lab Intelligent Telecommun Software &, Beijing 100876, Peoples R China.
EM xinchenliu@bupt.edu.cn; liuwu@bupt.edu.cn; tmei@microsoft.com;
   mhd@bupt.edu.cn
RI Liu, Xinchen/AAA-3951-2021; Liu, Wu/AAG-3615-2019; Mei,
   Tao/GQZ-0596-2022
OI Liu, Xinchen/0000-0003-4931-8821; Liu, Wu/0000-0003-1633-7575; Mei,
   Tao/0000-0002-5990-7307
FU National Key Research and Development Plan [2016YFC0801005]; Beijing
   Training Project for the Leading Talents in ST [1jrc 201502]; Funds for
   Creative Research Groups of China [61421061]; National Natural Science
   Foundation of China [61602049]
FX This work was supported in part by the National Key Research and
   Development Plan under Grant 2016YFC0801005, in part by the Funds for
   Creative Research Groups of China under Grant 61421061, in part by the
   National Natural Science Foundation of China under Grant 61602049, and
   in part by the Beijing Training Project for the Leading Talents in S&T
   under Grant 1jrc 201502.
CR [Anonymous], 2016, P IEEE C COMP VIS PA
   [Anonymous], 2014, CORR
   [Anonymous], 2005, IEE P VISION IMAGE S
   [Anonymous], 2016, Scientific Programming
   [Anonymous], INT C NEUR INT PROC
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Du S, 2013, IEEE T CIRC SYST VID, V23, P322, DOI 10.1109/TCSVT.2012.2203741
   Feris RS, 2012, IEEE T MULTIMEDIA, V14, P28, DOI 10.1109/TMM.2011.2170666
   FOLEY DH, 1975, IEEE T COMPUT, VC 24, P281, DOI 10.1109/T-C.1975.224208
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo YF, 2006, PATTERN RECOGN, V39, P2248, DOI 10.1016/j.patcog.2006.05.009
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Javed O, 2008, COMPUT VIS IMAGE UND, V109, P146, DOI 10.1016/j.cviu.2007.01.003
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kettnaker V., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P253, DOI 10.1109/CVPR.1999.784638
   Li NX, 2013, IEEE T MULTIMEDIA, V15, P1213, DOI 10.1109/TMM.2013.2241416
   Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Liu W, 2015, PROC CVPR IEEE, P3707, DOI 10.1109/CVPR.2015.7298994
   Liu XC, 2016, LECT NOTES COMPUT SC, V9906, P869, DOI 10.1007/978-3-319-46475-6_53
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma HD, 2018, IEEE MULTIMEDIA, V25, P76, DOI 10.1109/MMUL.2017.265091429
   Matei B. C., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3465, DOI 10.1109/CVPR.2011.5995575
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Meng JJ, 2016, IEEE T MULTIMEDIA, V18, P116, DOI 10.1109/TMM.2015.2500734
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Song JK, 2013, IEEE T MULTIMEDIA, V15, P1997, DOI 10.1109/TMM.2013.2271746
   Sunderrajan S, 2016, IEEE T MULTIMEDIA, V18, P51, DOI 10.1109/TMM.2015.2496139
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wen Y, 2011, IEEE T INTELL TRANSP, V12, P830, DOI 10.1109/TITS.2011.2114346
   Xie LX, 2015, IEEE T MULTIMEDIA, V17, P636, DOI 10.1109/TMM.2015.2408566
   Xu JJ, 2013, IEEE T MULTIMEDIA, V15, P2046, DOI 10.1109/TMM.2013.2281019
   Yang LJ, 2015, PROC CVPR IEEE, P3973, DOI 10.1109/CVPR.2015.7299023
   Zhang C, 2016, INT CONF ACOUST SPEE, P2832, DOI 10.1109/ICASSP.2016.7472194
   Zhang JP, 2011, IEEE T INTELL TRANSP, V12, P1624, DOI 10.1109/TITS.2011.2158001
   Zhang L, 2016, PROC CVPR IEEE, P1239, DOI 10.1109/CVPR.2016.139
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng Y, 2014, ACM T INTEL SYST TEC, V5, DOI 10.1145/2629592
NR 42
TC 268
Z9 299
U1 8
U2 84
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 645
EP 658
DI 10.1109/TMM.2017.2751966
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500011
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Kumar, K
   Shrimankar, DD
AF Kumar, Krishan
   Shrimankar, Deepti D.
TI F-DES: Fast and Deep Event Summarization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; event summarization; FASTA; local alignment; multi-view
   video; nucleotide sequence
ID VIDEO SYNOPSIS
AB In the multimedia era, a large volume of video data can be recorded during a certain period of time by multiple cameras. Such a rapid growth of video data requires both effective and efficient multiview video summarization techniques. The users can quickly browse and comprehend a large amount of audiovisual data. It is very difficult in real-time to manage and access the huge amount of video-content-handling issues of interview dependencies, significant variations in illumination, and presence of many unimportant frames with low activity. In this paper, we propose a local-alignment-based FASTA approach to summarize the events in multiview videos as a solution of the aforementioned problems. A deep learning framework is used to extract the features to resolve the problem of variations in illumination and to remove fine texture details and detect the objects in a frame. Interview dependencies among multiple views of video are then captured via the FASTA algorithm through local alignment. Finally, object tracking is applied to extract the frames with low activity. Subjective as well as objective evaluations clearly indicate the effectiveness of the proposed approach. Experiments show that the proposed summarization method successfully reduces the video content while keeping momentous information in the form of events. A computing analysis of the system also shows that it meets the requirement of real-time applications.
C1 [Kumar, Krishan; Shrimankar, Deepti D.] Visvesvaraya Natl Inst Technol, Dept Comp Sci & Engn, Nagpur 440010, Maharashtra, India.
   [Kumar, Krishan] Natl Inst Technol Uttarakhand, Srinagar 246174, Uttarakhand, India.
C3 National Institute of Technology (NIT System); Visvesvaraya National
   Institute of Technology, Nagpur; National Institute of Technology (NIT
   System); National Institute of Technology Uttarakhand
RP Kumar, K (corresponding author), Visvesvaraya Natl Inst Technol, Dept Comp Sci & Engn, Nagpur 440010, Maharashtra, India.
EM kkberwal@nituk.ac.in; dshrimankar@cse.vnit.ac.in
RI Kumar, Krishan/AAE-1656-2021; Kumar, Dr. Krishan/N-9846-2018; Berwal,
   Krishan/AAC-3473-2020
OI Kumar, Dr. Krishan/0000-0002-7068-6541; Berwal,
   Krishan/0000-0002-7068-6541; Shrimankar, Deepti/0000-0002-6212-0986
CR Adams MD, 2000, SCIENCE, V287, P2185, DOI 10.1126/science.287.5461.2185
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], P 2006 IEEE COMP SOC
   Chen F, 2014, IEEE T MULTIMEDIA, V16, P455, DOI 10.1109/TMM.2013.2291967
   Chen KW, 2011, IEEE T MULTIMEDIA, V13, P625, DOI 10.1109/TMM.2011.2131639
   Chu WS, 2015, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR.2015.7298981
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   de Leo Carter., 2014, ACM T SENSOR NETWORK, V10, P27, DOI DOI 10.1145/2530285
   Dumont E, 2009, INT WORK CONTENT MUL, P44, DOI 10.1109/CBMI.2009.49
   Fu YW, 2010, IEEE T MULTIMEDIA, V12, P717, DOI 10.1109/TMM.2010.2052025
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Gribskov M., 1991, SEQUENCE ANAL PRIMER, P169
   Gygli M, 2015, PROC CVPR IEEE, P3090, DOI 10.1109/CVPR.2015.7298928
   Gygli M, 2014, LECT NOTES COMPUT SC, V8695, P505, DOI 10.1007/978-3-319-10584-0_33
   Ju Z., 2016, 2016 IEEE INT C MULT, P1
   Kanungo, 2005, GENES AGING, P115
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Kumar KM, 2018, MATER MANUF PROCESS, V33, P414, DOI 10.1080/10426914.2017.1291951
   Kumar K, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY & INTERNET-BASED SYSTEMS (SITIS), P119, DOI 10.1109/SITIS.2016.27
   Kumar KP, 2017, ADV BUS STRATEGY COM, P1, DOI 10.4018/978-1-5225-1008-6.ch001
   Kurzhals K, 2016, IEEE T MULTIMEDIA, V18, P2149, DOI 10.1109/TMM.2016.2614184
   Lin WY, 2015, NEUROCOMPUTING, V155, P84, DOI 10.1016/j.neucom.2014.12.044
   Lo Presti L, 2012, IEEE T MULTIMEDIA, V14, P346, DOI 10.1109/TMM.2011.2173323
   Lu SY, 2014, IEEE T MULTIMEDIA, V16, P1497, DOI 10.1109/TMM.2014.2319778
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Mazloom M, 2015, ICMR'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P123, DOI 10.1145/2671188.2749402
   Mount D. W., 2017, FASTA SEQUENCE DATAB
   Mueller C, 2006, IEEE T PARALL DISTR, V17, P764, DOI 10.1109/TPDS.2006.104
   Navjot S., 2015, SIGNAL IMAGE VIDEO P, V9, P427
   Nie YW, 2013, IEEE T VIS COMPUT GR, V19, P1664, DOI 10.1109/TVCG.2012.176
   Ou SH, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2331916
   Potapov D, 2014, LECT NOTES COMPUT SC, V8694, P540, DOI 10.1007/978-3-319-10599-4_35
   Pritch Y, 2008, IEEE T PATTERN ANAL, V30, P1971, DOI 10.1109/TPAMI.2008.29
   Quera V, 2007, BEHAV RES METHODS, V39, P39, DOI 10.3758/BF03192842
   Reid I, 2010, IMAGE VISION COMPUT, V28, P1022, DOI 10.1016/j.imavis.2009.09.007
   Singh N, 2014, PATTERN RECOGN, V47, P1731, DOI 10.1016/j.patcog.2013.11.012
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Subrahmanyam GRKS, 2007, IEEE SIGNAL PROC LET, V14, P453, DOI 10.1109/LSP.2006.891345
   Wang F, 2012, IEEE T MULTIMEDIA, V14, P76, DOI 10.1109/TMM.2011.2165531
   Wang M, 2012, IEEE T MULTIMEDIA, V14, P975, DOI 10.1109/TMM.2012.2185041
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Xu BH, 2016, IEEE MULTIMEDIA, V23, P23, DOI 10.1109/MMUL.2016.18
   Yao T, 2016, PROC CVPR IEEE, P982, DOI 10.1109/CVPR.2016.112
   Yeung S, 2016, PROC CVPR IEEE, P2678, DOI 10.1109/CVPR.2016.293
   Zhang TZ, 2012, IEEE T MULTIMEDIA, V14, P1206, DOI 10.1109/TMM.2012.2191944
   Zhang YG, 2014, PHYSIOL REP, V2, DOI 10.14814/phy2.12147
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 47
TC 104
Z9 105
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 323
EP 334
DI 10.1109/TMM.2017.2741423
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200006
DA 2024-07-18
ER

PT J
AU Ye, LW
   Liu, Z
   Li, L
   Shen, LQ
   Bai, C
   Wang, Y
AF Ye, Linwei
   Liu, Zhi
   Li, Lina
   Shen, Liquan
   Bai, Cong
   Wang, Yang
TI Salient Object Segmentation via Effective Integration of Saliency and
   Objectness
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Graph-based integration; objectness map; object probability; salient
   object segmentation; saliency map
ID DETECTION MODEL; SPATIOTEMPORAL SALIENCY; VISUAL SALIENCY; IMAGE; VIDEO;
   ATTENTION
AB This paper proposes an effective salient object segmentation method via the graph-based integration of saliency and objectness. Based on the superpixel segmentation result of the input image, a graph is built to represent superpixels using regular vertex, background seed vertex with the addition of a terminal vertex. The edge weights on the graph are defined by integrating the difference of appearance, saliency, and objectness between superpixels. Then, the object probability of each superpixel is measured by finding the shortest path from the corresponding vertex to the terminal vertex on the graph, and the resultant object probability map can generally better highlight salient objects and suppress background regions compared to both saliency map and objectness map. Finally, the object probability map is used to initialize salient object and background, and effectively incorporated into the framework of graph cut to obtain the final salient object segmentation result. Extensive experimental results on three public benchmark datasets show that the proposed method consistently improves the salient object segmentation performance and outperforms the state-of-the-art salient object segmentation methods. Furthermore, experimental results also demonstrate that the proposed graph-based integration method is more effective than other fusion schemes and robust to saliency maps generated using various saliency models.
C1 [Ye, Linwei; Liu, Zhi; Li, Lina; Shen, Liquan] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
   [Ye, Linwei; Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
   [Bai, Cong] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 311122, Zhejiang, Peoples R China.
C3 Shanghai University; University of Manitoba; Zhejiang University of
   Technology
RP Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM yelinweimail@163.com; liuzhisjtu@163.com; linda9293@163.com;
   jsslq@163.com; congbai@zjut.edu.cn; ywang@cs.umanitoba.ca
RI Shen, Liquan/D-4832-2012; Bai, Cong/T-9188-2019; Zhang,
   Han/JMR-0670-2023; LIU, Zhi/D-4518-2012
OI Bai, Cong/0000-0002-6177-3862; LIU, Zhi/0000-0002-8428-1131
FU National Natural Science Foundation of China [61471230, 61171144,
   61502424]; Program for Professor of Special Appointment (Eastern
   Scholar) at the Shanghai Institutions of Higher Learning; Zhejiang
   Provincial Natural Science Foundation of China [LY15F020028]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61471230, Grant 61171144, and Grant
   61502424, in part by the Program for Professor of Special Appointment
   (Eastern Scholar) at the Shanghai Institutions of Higher Learning, and
   in part by the Zhejiang Provincial Natural Science Foundation of China
   under Grant LY15F020028. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Xiao-Ping
   Zhang. (Corresponding author: Zhi Liu.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Aytekin Ç, 2015, IEEE IMAGE PROC, P1692, DOI 10.1109/ICIP.2015.7351089
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chang KY, 2011, IEEE I CONF COMP VIS, P914, DOI 10.1109/ICCV.2011.6126333
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fang YM, 2014, IEEE T CIRC SYST VID, V24, P27, DOI 10.1109/TCSVT.2013.2273613
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI DOI 10.1109/CVPR.2010.5539929
   Guo CL, 2010, IEEE T IMAGE PROCESS, V19, P185, DOI 10.1109/TIP.2009.2030969
   Harel J, 2006, Advances in Neural Information Processing Systems, V19
   Hou X., 2007, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.1109/CVPR.2007.383267
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209
   Jiang HZ, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.110
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Liu Z, 2014, IEEE T IMAGE PROCESS, V23, P1937, DOI 10.1109/TIP.2014.2307434
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Liu Z, 2010, IEEE IMAGE PROC, P253, DOI 10.1109/ICIP.2010.5652613
   Marat S, 2009, INT J COMPUT VISION, V82, P231, DOI 10.1007/s11263-009-0215-3
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Rutishauser U., 2004, P IEEE COMP SOC C CO, DOI DOI 10.1109/CVPR.2004.1315142
   Setlur V, 2007, IEEE COMPUT GRAPH, V27, P80, DOI 10.1109/MCG.2007.133
   Shah R, 2013, IEEE T CIRC SYST VID, V23, P1565, DOI 10.1109/TCSVT.2013.2248972
   Shen LQ, 2013, MULTIMED TOOLS APPL, V63, P709, DOI 10.1007/s11042-011-0893-z
   Shi R, 2012, IEEE SIGNAL PROC LET, V19, P215, DOI 10.1109/LSP.2012.2188388
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xue JR, 2011, IEEE T IMAGE PROCESS, V20, P1177, DOI 10.1109/TIP.2010.2077643
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang Y, 2014, IEEE T MULTIMEDIA, V16, P1700, DOI 10.1109/TMM.2014.2326836
   Ye LL, 2016, PURE APPL GEOPHYS, V173, P321, DOI 10.1007/s00024-015-1202-y
   Zhang W, 2010, IEEE T MULTIMEDIA, V12, P300, DOI 10.1109/TMM.2010.2047607
   Zhengqin L., 2015, PROC CVPR IEEE, P1356, DOI DOI 10.1109/CVPR.2015.7298741
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
   Zou WB, 2015, IEEE T IMAGE PROCESS, V24, P3858, DOI 10.1109/TIP.2015.2456497
   Zou WB, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.78
NR 54
TC 73
Z9 73
U1 2
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1742
EP 1756
DI 10.1109/TMM.2017.2693022
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400005
DA 2024-07-18
ER

PT J
AU Zhan, YB
   Zhang, R
   Wu, Q
AF Zhan, Yibing
   Zhang, Rong
   Wu, Qian
TI A Structural Variation Classification Model for Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fuzzy logic; image quality assessment (IQA); structural variation
   classification
ID RECOGNITION
AB Structural information is critical for image quality assessment (IQA). In this paper, first, we propose a novel model of structural variations in images. The proposed model classifies the types of structural variation within images into four categories: slight deformations, additive impairments, detail losses, and confusing contents. This system of classification applies to most types of structural variations observed in practice. In this model, each pixel from the distorted images is classified according to its structural variation using fuzzy logic based on a set of structural features extracted from the images. Then, a novel IQA method based on these pixel classifications is proposed. This proposed method evaluates the image quality by combining two aspects: the distribution of different structural variations and the degree of structural differences. We test the proposed method using seven public databases. The experimental results indicate that our method is more consistent with the results of the subjective evaluation than were the nine other state-of-the-art IQA methods. The MATLAB source code of our method is available at http://image.ustc.edu.cn/IQA.html.
C1 [Zhan, Yibing; Zhang, Rong; Wu, Qian] Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhang, R (corresponding author), Univ Sci & Technol China, Hefei 230026, Anhui, Peoples R China.
EM zybjy@mail.ustc.edu.cn; zrong@ustc.edu.cn; qwu@mail.ustc.edu.cn
FU National Nature Science Foundation of China [61331020]
FX This work was supported by the National Nature Science Foundation of
   China under Grant 61331020. This paper was presented in part at the IEEE
   International Conference on Acoustics, Speech and Signal Processing,
   Brisbane Convention & Exhibition Centre, South Brisbane, QLD, Australia,
   April 2015. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Christian
   Timmerer. (Corresponding author: Rong Zhang.)
CR [Anonymous], 2000, FIN REP VID QUAL EXP
   [Anonymous], 2005, SUBJECTIVE QUALITY A
   Bovik AC, 2006, MODERN IMAGE QUALITY
   Chandler DM, 2007, IEEE T IMAGE PROCESS, V16, P2284, DOI 10.1109/TIP.2007.901820
   Davies E.R., 1997, MACHINE VISION THEOR, V2nd
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Horita Y., 2000, MICT Image Quality Evaluation Database
   International Telecommunications Union, 2009, INT TEL UN BT SER
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lam L, 1997, IEEE T SYST MAN CY A, V27, P553, DOI 10.1109/3468.618255
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Mathis J, 1998, ELECTROMYOGR MOTOR C, V109, P426, DOI 10.1016/S0924-980X(98)00042-3
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ruta D., 2005, Information Fusion, V6, P63, DOI 10.1016/j.inffus.2004.04.008
   Saxena A., 2005, ADV NEURAL INFORM PR, V18, P1
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Schilling D, 2002, IEEE T MULTIMEDIA, V4, P320, DOI 10.1109/TMM.2002.802844
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Yeh HH, 2013, IEEE T MULTIMEDIA, V15, P1944, DOI 10.1109/TMM.2013.2280250
   Zhan YB, 2016, IEEE IMAGE PROC, P2072, DOI 10.1109/ICIP.2016.7532723
   Zhan YB, 2015, INT CONF ACOUST SPEE, P1662, DOI 10.1109/ICASSP.2015.7178253
   Zhang F, 2011, IEEE T MULTIMEDIA, V13, P615, DOI 10.1109/TMM.2011.2134079
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
NR 32
TC 11
Z9 14
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1837
EP 1847
DI 10.1109/TMM.2017.2689923
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400012
DA 2024-07-18
ER

PT J
AU Wang, WC
   Yuan, XH
   Wu, XJ
   Liu, YL
AF Wang, Wencheng
   Yuan, Xiaohui
   Wu, Xiaojin
   Liu, Yunlong
TI Fast Image Dehazing Method Based on Linear Transformation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dehazing; image restoration; linear transformation; transmission map
ID CONTRAST ENHANCEMENT; QUALITY; VISIBILITY; EQUALIZATION; RESTORATION;
   ALGORITHM; MODEL
AB Images captured in hazy or foggy weather conditions are seriously degraded by the scattering of atmospheric particles, which directly influences the performance of outdoor computer vision systems. In this paper, a fast algorithm for single image dehazing is proposed based on linear transformation by assuming that a linear relationship exists in the minimum channel between the hazy image and the haze-free image. First, the principle of linear transformation is analyzed. Accordingly, the method of estimating a medium transmission map is detailed and the weakening strategies are introduced to solve the problem of the brightest areas of distortion. To accurately estimate the atmospheric light, an additional channel method is proposed based on quad-tree subdivision. In this method, average grays and gradients in the region are employed as assessment criteria. Finally, the haze-free image is obtained using the atmospheric scattering model. Numerous experimental results show that this algorithm can clearly and naturally recover the image, especially at the edges of sudden changes in the depth of field. It can, thus, achieve a good effect for single image dehazing. Furthermore, the algorithmic time complexity is a linear function of the image size. This has obvious advantages in running time by guaranteeing a balance between the running speed and the processing effect.
C1 [Wang, Wencheng; Wu, Xiaojin; Liu, Yunlong] Weifang Univ, Coll Informat & Control Engn, Weifang 261061, Peoples R China.
   [Wang, Wencheng] Univ North Texas, Denton, TX 76203 USA.
   [Yuan, Xiaohui] Univ North Texas, Dept Sci & Engn, Denton, TX 76207 USA.
C3 Weifang University; University of North Texas System; University of
   North Texas Denton; University of North Texas System; University of
   North Texas Denton
RP Wang, WC (corresponding author), Weifang Univ, Coll Informat & Control Engn, Weifang 261061, Peoples R China.; Wang, WC (corresponding author), Univ North Texas, Denton, TX 76203 USA.
EM wwcwfu@126.com; xiaohui.yuan@unt.edu; wfuwxj@163.com; fhylren@163.com
RI Yuan, Xiaohui/AAQ-1172-2020; Liu, Yunlong/P-8832-2014; Wang,
   Wencheng/A-6146-2018; Liu, Yunlong/GZN-1795-2022
OI Liu, Yunlong/0000-0002-3662-3512; Wang, Wencheng/0000-0002-0888-9225;
   Liu, Yunlong/0000-0002-3662-3512; Yuan, Xiaohui/0000-0001-6897-4563
FU National Natural Science Foundation of China [61403283]; Shandong
   Provincial Natural Science Foundation [ZR2013FQ036, ZR2015PE025]; Spark
   Program of China [2013GA740053]; Spark Program of Shandong Province
   [2013XH06034]; Technology Development Plan of Weifang City [201301015]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61403283, in part by the Shandong
   Provincial Natural Science Foundation under Grant ZR2013FQ036 and Grant
   ZR2015PE025, in part by the Spark Program of China under Grant
   2013GA740053, in part by the Spark Program of Shandong Province under
   Grant 2013XH06034, and in part by the Technology Development Plan of
   Weifang City under Grant 201301015. The associate editor coordinating
   the review of this manuscript and approving it for publication was Prof.
   Abdulmotaleb El Saddik.
CR Ancuti CO, 2013, IEEE T IMAGE PROCESS, V22, P3271, DOI 10.1109/TIP.2013.2262284
   [Anonymous], 2014, IEEE International Conference on Computational Photography (ICCP)
   Chiang JY, 2012, IEEE T IMAGE PROCESS, V21, P1756, DOI 10.1109/TIP.2011.2179666
   Cooper TJ, 2004, J ELECTRON IMAGING, V13, P85, DOI 10.1117/1.1636182
   Ding M, 2013, SCI CHINA INFORM SCI, V56, DOI 10.1007/s11432-012-4566-y
   Dippel S, 2002, IEEE T MED IMAGING, V21, P343, DOI 10.1109/TMI.2002.1000258
   Fattal R, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2651362
   Fattal R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360671
   Gibson KB, 2012, IEEE T IMAGE PROCESS, V21, P662, DOI 10.1109/TIP.2011.2166968
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hautiere N., 2007, 2007 IEEE C COMP VIS, P1
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Huang SC, 2014, IEEE T INTELL TRANSP, V15, P2321, DOI 10.1109/TITS.2014.2314696
   Kim JH, 2013, J VIS COMMUN IMAGE R, V24, P410, DOI 10.1016/j.jvcir.2013.02.004
   Kim TK, 1998, IEEE T CONSUM ELECTR, V44, P82, DOI 10.1109/30.663733
   Kopf J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409069
   Kratz L, 2009, IEEE I CONF COMP VIS, P1701, DOI 10.1109/ICCV.2009.5459382
   Ma ZL, 2016, NEUROCOMPUTING, V173, P1257, DOI 10.1016/j.neucom.2015.08.084
   McCartney E.J., 1976, Optics of the atmosphere: scattering by molecules and particles, P1
   Meng GF, 2013, IEEE I CONF COMP VIS, P617, DOI 10.1109/ICCV.2013.82
   Narasimhan S. G., 2003, Interactive (de) weathering of an image using physical models, V6, P1
   Narasimhan SG, 2003, IEEE T PATTERN ANAL, V25, P713, DOI 10.1109/TPAMI.2003.1201821
   Narasimhan SG, 2000, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2000.855874
   Nayar S. K., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P820, DOI 10.1109/ICCV.1999.790306
   Negru M, 2015, IEEE T INTELL TRANSP, V16, P2257, DOI 10.1109/TITS.2015.2405013
   Nishino K, 2012, INT J COMPUT VISION, V98, P263, DOI 10.1007/s11263-011-0508-1
   Oakley JP, 1998, IEEE T IMAGE PROCESS, V7, P167, DOI 10.1109/83.660994
   Pan XX, 2015, IEEE SIGNAL PROC LET, V22, P1806, DOI 10.1109/LSP.2015.2432466
   Saini M, 2012, IEEE T MULTIMEDIA, V14, P555, DOI 10.1109/TMM.2012.2186957
   Schechner YY, 2001, PROC CVPR IEEE, P325
   Seow MJ, 2006, NEUROCOMPUTING, V69, P954, DOI 10.1016/j.neucom.2005.07.003
   Shiau YH, 2013, IEEE T CIRC SYST VID, V23, P1369, DOI 10.1109/TCSVT.2013.2243650
   Shwartz S., 2006, 2006 IEEE COMP SOC C, V2, P1984, DOI DOI 10.1109/CVPR.2006.71
   Tan R. T., 2008, P IEEE C COMP VIS PA, P1
   Tang XO, 2013, IEEE T MULTIMEDIA, V15, P1930, DOI 10.1109/TMM.2013.2269899
   Tarel JP, 2009, IEEE I CONF COMP VIS, P2201, DOI 10.1109/ICCV.2009.5459251
   Wang JZ, 2016, IEEE T MULTIMEDIA, V18, P1000, DOI 10.1109/TMM.2016.2544099
   Wang WC, 2016, IEEE IMAGE PROC, P2241, DOI 10.1109/ICIP.2016.7532757
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yu Jing, 2011, Acta Automatica Sinica, V37, P143, DOI 10.3724/SP.J.1004.2011.00143
   Zhu QS, 2015, IEEE T IMAGE PROCESS, V24, P3522, DOI 10.1109/TIP.2015.2446191
NR 43
TC 188
Z9 189
U1 0
U2 54
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2017
VL 19
IS 6
BP 1142
EP 1155
DI 10.1109/TMM.2017.2652069
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5YS
UT WOS:000404059400003
DA 2024-07-18
ER

PT J
AU Sekhavat, YA
AF Sekhavat, Yoones A.
TI Privacy Preserving Cloth Try-On Using Mobile Augmented Reality
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Augmented reality (AR); body customization; privacy preserving cloth
   try-on; virtual cloth try-on
ID RECONSTRUCTION
AB Virtual try-on applications make it possible for buyers to watch themselves wearing different garments without physically trying on them. The prevailing approach for virtual try-on has been based on virtual fitting rooms, in which several cameras are used to identify the skeleton and posture of a user in order to render a garment on the user's image. Although this approach has been implemented successfully using different techniques, the privacy of users can be compromised as some users might be reluctant to stand in front of cameras in a fitting room. This paper proposes an alternative approach that allows a customer to watch a three-dimensional (3D) model of her/him wearing garments on a personal mobile device using augmented reality (AR). Among 3D human models that are automatically generated, a model selection technique is proposed that makes it possible to find the right size model representing the anthropometric features of the user. This approach is accompanied by body customization and face generation modules to generate a realistic representation. Several quantitative experiments as well as user studies were performed to evaluate the accuracy, efficiency, usefulness, and privacy of the proposed technique.
C1 [Sekhavat, Yoones A.] Tabriz Islamic Art Univ, Fac Multimedia, Tabriz 5164736931, Iran.
RP Sekhavat, YA (corresponding author), Tabriz Islamic Art Univ, Fac Multimedia, Tabriz 5164736931, Iran.
EM sekhavat@tabriziau.ac.ir
RI Sekhavat, Yoones A./KGK-5867-2024; Sekhavat, Yoones A./ABC-4693-2020
OI Sekhavat, Yoones A./0000-0003-3654-9583; Sekhavat, Yoones
   A./0000-0003-3654-9583
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2009, FULLY HOMOMORPHIC EN, DOI 10.1145/1536414.1536440
   [Anonymous], P SIGGRAPH AS
   Buchanan T, 2007, J AM SOC INF SCI TEC, V58, P157, DOI 10.1002/asi.20459
   Cai XY, 2016, IEEE T MULTIMEDIA, V18, P141, DOI 10.1109/TMM.2015.2505089
   Chen YH, 2008, IEEE T MULTIMEDIA, V10, P585, DOI 10.1109/TMM.2008.921741
   HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837
   Hauswiesner S, 2011, P 10 INT C VIRT REAL, P23, DOI DOI 10.1145/2087756.2087759
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   Jin T, 2012, 2012 IEEE COMPUTER SOCIETY ANNUAL SYMPOSIUM ON VLSI (ISVLSI), P195, DOI 10.1109/ISVLSI.2012.30
   Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63
   Li JT, 2011, COMPUT IND, V62, P693, DOI 10.1016/j.compind.2011.04.002
   Meng YW, 2010, COMPUT AIDED DESIGN, V42, P310, DOI 10.1016/j.cad.2009.12.004
   Sekhavat Y. A., 2013, INT J INTELL SCI, V3, P34
   Sekhavat YA, 2016, INT J COMPUT GAMES T, V2016, DOI 10.1155/2016/7690754
   Tang D., 2014, PROC INT C MULTIMEDI, P1
   Thien CC, 2002, COMPUT GRAPH-UK, V26, P766
   Xianhui Zeng, 2009, Proceedings of the Second International Symposium on Information Science and Engineering (ISISE 2009), P383, DOI 10.1109/ISISE.2009.9
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang H., 2006, 2006 IEEE COMP SOC C, V2, P2126, DOI DOI 10.1109/CVPR.2006.301
NR 20
TC 32
Z9 45
U1 1
U2 44
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 1041
EP 1049
DI 10.1109/TMM.2016.2639380
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000012
DA 2024-07-18
ER

PT J
AU Cao, Y
   Pang, XF
   Chan, AB
   Lau, RWH
AF Cao, Ying
   Pang, Xufang
   Chan, Antoni B.
   Lau, Rynson W. H.
TI Dynamic Manga: Animating Still Manga via Camera Movement
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Camera movement; comics; 2-D animation; semantic estimation
AB We propose a method for animating still manga imagery through camera movements. Given a series of existing manga pages, we start by automatically extracting panels, comic characters, and balloons from the manga pages. Then, we use a data-driven graphical model to infer per-panel motion and emotion states from low-level visual patterns. Finally, by combining domain knowledge of film production and characteristics of manga, we simulate camera movements over the manga pages, yielding an animation. The results augment the still manga contents with animated motion that reveals the mood and tension of the story, while maintaining the original narrative. We have tested our method on manga series of different genres, and demonstrated that our method can generate animations that are more effective in storytelling and pacing, with less human efforts, as compared with prior works. We also show two applications of our method, mobile comic reading, and comic trailer generation.
C1 [Cao, Ying; Pang, Xufang; Chan, Antoni B.; Lau, Rynson W. H.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Cao, Y (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM caoying59@gmail.com; pangxufang@gmail.com; abchan@cityu.edu.hk;
   rynson.lau@cityu.edu.hk
RI ; CHAN, Antoni B./D-7858-2013
OI PANG, Xufang/0000-0002-4648-4317; CHAN, Antoni B./0000-0002-2886-2513
FU GRF grants from the Research Grants Council of Hong Kong [CityU
   11200314, CityU 115112]
FX This work was supported in part by two GRF grants from the Research
   Grants Council of Hong Kong (CityU 11200314 and CityU 115112). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Klara Nahrstedt.
CR [Anonymous], 2013, KEN BURNS EFFECT
   [Anonymous], 2015, Manga25
   Arai Kohei, 2010, Proceedings of the Seventh International Conference on Information Technology: New Generations (ITNG 2010), P370, DOI 10.1109/ITNG.2010.22
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Cao Y., 2012, P ACM SIGGRAPH AS
   Cao Y, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0106249
   Chan CH, 2007, LECT NOTES COMPUT SC, V4810, P775
   Cheredar T., 2013, DC PLANS EVOLVE DIGI
   Chuang YY, 2005, ACM T GRAPHIC, V24, P853, DOI 10.1145/1073204.1073273
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   FORNEY GD, 1973, P IEEE, V61, P268, DOI 10.1109/PROC.1973.9030
   FREEMAN WT, 1991, COMP GRAPH, V25, P27, DOI 10.1145/127719.122721
   G. Office, 2004, MOR DRAW MANG, V1
   Hornung A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186645
   Huang M., 2010, P ACM VIRTL REAL CON, P1214
   Jain E., 2012, THESIS
   Kensinger EA, 2004, REV NEUROSCIENCE, V15, P241, DOI 10.1515/REVNEURO.2004.15.4.241
   Lafferty John, 2001, INT C MACH LEARN ICM
   Li LY, 2013, PROC INT CONF DOC, P1190, DOI 10.1109/ICDAR.2013.241
   Litwinowicz P., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P409, DOI 10.1145/192161.192270
   Malik J., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P918, DOI 10.1109/ICCV.1999.790346
   McCloud Scott, 2006, Making Comics: Storytelling secrets of comics, manga and graphic novels
   Pang XF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1125, DOI 10.1145/2647868.2654990
   Ren X., 2005, P 10 IEEE INT C COMP
   Shinya M., 1999, P ACM SIGGRAPH
   Shlizerman I., 2011, P ACM SIGGRAPH
   Sun M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P96
   Tibbetts N., 2012, CAMERA YOUR EMOTIONS
   Xu L., 2012, P ACM SIGGRAPH AS
   Xu X., 2008, P ACM SIGGRAPH AS
   Zhang SH, 2009, IEEE T VIS COMPUT GR, V15, P618, DOI 10.1109/TVCG.2009.9
NR 31
TC 11
Z9 12
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2017
VL 19
IS 1
BP 160
EP 172
DI 10.1109/TMM.2016.2609415
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EH0SX
UT WOS:000391475200013
DA 2024-07-18
ER

PT J
AU Bahrami, K
   Kot, AC
AF Bahrami, Khosro
   Kot, Alex C.
TI Efficient Image Sharpness Assessment Based on Content Aware Total
   Variation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content aware total variation (CATV); human vision system (HVS); image
   sharpness/blurriness assessment
ID QUALITY ASSESSMENT; EDGE; BLUR
AB State-of-the-art sharpness assessment methods are mostly based on edge width, gradient, high-frequency energy, or pixel intensity variation. Such methods consider very little the image content variation in conjunction with the sharpness assessment which causes the sharpness metric to be less effective for different content images. In this paper, we propose an efficient no-reference image sharpness assessment called content aware total variation (CATV) by considering the importance of image content variation in sharpness measurement. By parameterizing the image TV statistics using generalized Gaussian distribution, the sharpness measure is identified by the standard deviation, and the image content variation evaluator is indicated by the shape parameter. However, the standard deviation is content-dependent which is different for the regions with strong edges, high frequency textures, low frequency textures, and blank areas. By incorporating the shape-parameter in moderating of the standard deviation, we propose a content aware sharpness metric. The experimental results show that the proposed method is highly correlated with the human vision system and has better sharpness assessment results than the state-of-the-art techniques on the blurred subset images of LIVE, TID2008, CSIQ, and IVC databases. Also, our method has very low computational complexity which is suitable for online applications. The correlations with the subjective of the four databases and statistical significance analysis reveal that our method has superior results when compared with previous techniques.
C1 [Bahrami, Khosro; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Bahrami, K (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM khosro1@ntu.edu.sg; eackot@ntu.edu.sg
FU Singapore National Research Foundation
FX This work was carried out at the Rapid-Rich Object Search (ROSE)
   Laboratory at the Nanyang Technological University, Singapore. The ROSE
   Laboratory is supported by a grant from the Singapore National Research
   Foundation and administered by the Interactive and Digital Media
   Programme Office, Media Development Authority.
CR [Anonymous], 2005, SUBJECTIVE QUALITY A
   [Anonymous], 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Blomgren P, 1998, IEEE T IMAGE PROCESS, V7, P304, DOI 10.1109/83.661180
   Caviedes J, 2004, SIGNAL PROCESS-IMAGE, V19, P147, DOI 10.1016/j.image.2003.08.002
   Caviedes J, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P53, DOI 10.1109/ICIP.2002.1038901
   Cho TS, 2010, PROC CVPR IEEE, P169, DOI 10.1109/CVPR.2010.5540214
   Chung YC, 2004, CONF CYBERN INTELL S, P356
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Gao XB, 2009, IEEE T IMAGE PROCESS, V18, P1409, DOI 10.1109/TIP.2009.2018014
   Goldluecke B, 2010, PROC CVPR IEEE, P327, DOI 10.1109/CVPR.2010.5540194
   Gu K., 2015, SIGNAL IMAGE VIDEO P, P1
   Gu K., 2015, IEEE T IMAGE PROCESS, V21, P1
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Gu K, 2014, IEEE T BROADCAST, V60, P555, DOI 10.1109/TBC.2014.2344471
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Kristan M, 2006, PATTERN RECOGN LETT, V27, P1431, DOI 10.1016/j.patrec.2006.01.016
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li CF, 2010, SIGNAL PROCESS-IMAGE, V25, P517, DOI 10.1016/j.image.2010.03.004
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Miyata T, 2012, IEEE IMAGE PROC, P3057, DOI 10.1109/ICIP.2012.6467545
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Saad M. A., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3093, DOI 10.1109/ICIP.2011.6116319
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shaked D, 2005, IEEE IMAGE PROC, P841
   SHARIFI K, 1995, IEEE T CIRC SYST VID, V5, P52, DOI 10.1109/76.350779
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Shen J, 2011, IEEE T IMAGE PROCESS, V20, P2089, DOI 10.1109/TIP.2011.2108661
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Tao DC, 2009, IEEE T SYST MAN CY B, V39, P1623, DOI 10.1109/TSMCB.2009.2021951
   Varadarajan S, 2008, IEEE IMAGE PROC, P401, DOI 10.1109/ICIP.2008.4711776
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zhong SH, 2010, IEEE IMAGE PROC, P1553, DOI 10.1109/ICIP.2010.5653807
   Zhu X, 2009, INT WORK QUAL MULTIM, P64, DOI 10.1109/QOMEX.2009.5246976
NR 48
TC 14
Z9 15
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1568
EP 1578
DI 10.1109/TMM.2016.2573139
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000010
DA 2024-07-18
ER

PT J
AU Nguyen, LS
   Gatica-Perez, D
AF Nguyen, Laurent Son
   Gatica-Perez, Daniel
TI Hirability in the Wild: Analysis of Online Conversational Video Resumes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hirability; nonverbal behavior; personality; recruitment social videos;
   social computing; video resumes; YouTube
ID THIN SLICES; BEHAVIOR; IMPRESSIONS; SELECTION
AB Online social media is changing the personnel recruitment process. Until now, resumes were among the most widely used tools for the screening of job applicants. The advent of inexpensive sensors combined with the success of online video platforms has enabled the introduction of a new type of resume. Video resumes are short video messages where job applicants present themselves to potential employers. Online video resumes represent an opportunity to study the formation of first impressions in an employment context at a scale never attempted before, and to our knowledge they have not been studied from a behavioral standpoint. We collected a dataset of 939 conversational English-speaking video resumes from YouTube. Annotations of demographics, skills, and first impressions were collected using the Amazon Mechanical Turk crowdsourcing platform. Basic demographics were then analyzed to understand the population who uses video resumes to find a job, and results showed that applicants mainly consisted of young people looking for internship and junior positions. We developed a computational framework for the prediction of organizational first impressions, where the inference and nonverbal cue extraction steps were fully automated. Results demonstrate automatic prediction of first impressions of up to 27% of the variance explained for extraversion, and up to 20% for social and communication skills.
C1 [Nguyen, Laurent Son] Idiap Res Inst, Social Comp Grp, CH-1920 Martigny, Switzerland.
   [Gatica-Perez, Daniel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Gatica-Perez, Daniel] Idiap Res Inst, CH-1920 Martigny, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Nguyen, LS (corresponding author), Idiap Res Inst, Social Comp Grp, CH-1920 Martigny, Switzerland.
EM lnguyen@idiap.ch; gatica@idiap.ch
FU UBIMPRESSED Project of the Sinergia Interdisciplinary Program of the
   Swiss National Science Foundation
FX This work was supported by the UBIMPRESSED Project of the Sinergia
   Interdisciplinary Program of the Swiss National Science Foundation. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jiebo Luo.
CR AMBADY N, 1992, PSYCHOL BULL, V111, P256, DOI 10.1037/0033-2909.111.2.256
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   [Anonymous], 2005, MIT SPEECH FEAT EXTR
   [Anonymous], THESIS
   [Anonymous], 2011, P INT AAAI C WEB SOC
   Batrinca L, 2011, P 13 INT C MULT INT, P255, DOI [DOI 10.1145/2070481.2070528, 10.1145/2070481.2070528]
   Batrinca LM, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P39
   Biel J.-I., 2010, ACM T MULTIM COMPUT, V2, P1
   Biel JI, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P119, DOI 10.1145/2522848.2522877
   Biel JI, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P53
   Biel JI, 2013, IEEE T MULTIMEDIA, V15, P41, DOI 10.1109/TMM.2012.2225032
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Curhan JR, 2007, J APPL PSYCHOL, V92, P802, DOI 10.1037/0021-9010.92.3.802
   DeGroot T, 2009, J BUS PSYCHOL, V24, P179, DOI 10.1007/s10869-009-9098-0
   Doyle A., 2016, SKILLS LIST RESUMES
   Ekman P., 1982, EMOTION IN THE HUMAN
   Fleiss J.L., 2003, The Measurement of Interrater Agreement. Statistical Methods for Rates and Proportions
   FORBES RJ, 1980, J OCCUP PSYCHOL, V53, P65, DOI 10.1111/j.2044-8325.1980.tb00007.x
   Gatica-Perez D, 2009, IMAGE VISION COMPUT, V27, P1775, DOI 10.1016/j.imavis.2009.01.004
   Gosling S. D., 2007, ICWSM, V7, P1
   Gosling SD, 2003, J RES PERS, V37, P504, DOI 10.1016/S0092-6566(03)00046-1
   Hiemstra A. M. F., 2013, THESIS
   Huffcutt AI, 2001, J APPL PSYCHOL, V86, P897, DOI 10.1037//0021-9010.86.5.897
   IMADA AS, 1977, J APPL PSYCHOL, V62, P295, DOI 10.1037/0021-9010.62.3.295
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   KELLY JF, 1992, AM ANN DEAF, V137, P404, DOI 10.1353/aad.2012.0361
   Kemp KatieJ., 2013, J MARKETING DEV COMP, V7, P84
   Knapp M.L., 2009, Nonverbal communication in human interaction, V7th
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Lepri B, 2012, IEEE T AFFECT COMPUT, V3, P443, DOI 10.1109/T-AFFC.2012.17
   Naim Iftekhar, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163127
   NISBETT RE, 1977, J PERS SOC PSYCHOL, V35, P250, DOI 10.1037//0022-3514.35.4.250
   Piotrowski C., 2006, North American Journal of Psychology, V8, P489
   Pollak Levine S., 2002, APPL HRM RES, V7, P1
   Rolls J. A., 1993, P WORLD C COOP ED, P1
   Sanchez-Cortes D., 2013, Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia, MUM'13, P22
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   SHROUT PE, 1979, PSYCHOL BULL, V86, P420, DOI 10.1037/0033-2909.86.2.420
   Smith C., 2016, NUMBERS 120 AMAZING
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   U. S. Bureau of Labor Statistics (BLS), 2012, AM TIM US SURV
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
NR 42
TC 32
Z9 34
U1 0
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1422
EP 1437
DI 10.1109/TMM.2016.2557058
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhen, QK
   Huang, D
   Wang, YH
   Chen, LM
AF Zhen, Qingkai
   Huang, Di
   Wang, Yunhong
   Chen, Liming
TI Muscular Movement Model-Based Automatic 3D/4D Facial Expression
   Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D/4D facial expression recognition; muscle movement model (MMM); shape
   representation
ID FACE
AB Facial expression is an important channel for human nonverbal communication. This paper presents a novel and effective approach to automatic 3D/4D facial expression recognition based on the muscular movement model (MMM). In contrast to most of existing methods, the MMM deals with such an issue in the viewpoint of anatomy. It first automatically segments the input 3D face (frame) by localizing the corresponding points within each muscular region of the reference using iterative closest normal point. A set of features with multiple differential quantities, including coordinate, normal, and shape index values, are then extracted to describe the geometry deformation of each segmented region. Meanwhile, we analyze the importance of these muscular areas, and a score level fusion strategy is exploited to optimize their weights by the genetic algorithm in the learning step. The support vector machine and the hidden Markov model are finally used to predict the expression label in 3D and 4D, respectively. The experiments are conducted on the BU-3DFE and BU-4DFE databases, and the results achieved clearly demonstrate the effectiveness of the proposed method.
C1 [Zhen, Qingkai; Huang, Di; Wang, Yunhong] Beihang Univ, Sch Comp Sci & Engn, Lab Intelligent Recognit & Image Proc, Beijing 100191, Peoples R China.
   [Chen, Liming] Ecole Cent Lyon, MI Dept, LIRIS Lab, F-69134 Lyon, France.
C3 Beihang University; Institut National des Sciences Appliquees de Lyon -
   INSA Lyon; Ecole Centrale de Lyon
RP Huang, D (corresponding author), Beihang Univ, Sch Comp Sci & Engn, Lab Intelligent Recognit & Image Proc, Beijing 100191, Peoples R China.
EM qingkai.zhen@buaa.edu.cn; dhuang@buaa.edu.cn; yhwang@buaa.edu.cn;
   liming.chen@ec-lyon.fr
RI Huang, Di/JBJ-3541-2023
OI Huang, Di/0000-0001-7877-7301
FU Hong Kong, Macao, and Taiwan Science and Technology Cooperation Program
   of China [L2015TGA9004]; National Natural Science Foundation of China
   [61540048, 61273263, 61421003]; Specialized Research Fund for the
   Doctoral Program of Higher Education [20121102120016]; French research
   agency, l'Agence Nationale de Recherche (ANR) [ANR-13-CORD-0004-02];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the Hong Kong, Macao, and Taiwan
   Science and Technology Cooperation Program of China under Grant
   L2015TGA9004, in part by the National Natural Science Foundation of
   China under Grant 61540048, Grant 61273263, and Grant 61421003, in part
   by the Specialized Research Fund for the Doctoral Program of Higher
   Education under Grant 20121102120016, in part by the French research
   agency, l'Agence Nationale de Recherche (ANR), through the project
   Jemime under Grant ANR-13-CORD-0004-02, and in part by the Fundamental
   Research Funds for the Central Universities. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sen-Ching Samson Cheung. (Corresponding author: Di
   Huang.)
CR [Anonymous], 2012, Computer Vision and Pattern Recognition Workshops CVPRW
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2011, P ACM WORKSH HUM GES
   [Anonymous], 2013, 2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)
   Ben Amor B, 2014, IEEE T CYBERNETICS, V44, P2443, DOI 10.1109/TCYB.2014.2308091
   Berretti S., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4125, DOI 10.1109/ICPR.2010.1002
   Berretti S, 2013, VISUAL COMPUT, V29, P1333, DOI 10.1007/s00371-013-0869-2
   Dahmane M, 2014, IEEE T MULTIMEDIA, V16, P1574, DOI 10.1109/TMM.2014.2321113
   Ekman P, 1978, FACIAL ACTION CODING
   Fang T., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P603, DOI 10.1109/FG.2011.5771466
   Fang TH, 2012, IMAGE VISION COMPUT, V30, P738, DOI 10.1016/j.imavis.2012.02.004
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Goldfeather J, 2004, ACM T GRAPHIC, V23, P45, DOI 10.1145/966131.966134
   Gong B., 2009, Proceedings of the 17th ACM International Conference on Multimedia, P569
   Hao Tang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563052
   Klassen E, 2006, LECT NOTES COMPUT SC, V3951, P95
   Li H, 2011, CONTROL ENG APPL INF, V13, P3
   Li HB, 2014, NEUROCOMPUTING, V133, P179, DOI 10.1016/j.neucom.2013.11.018
   Maalej A, 2011, PATTERN RECOGN, V44, P1581, DOI 10.1016/j.patcog.2011.02.012
   Meng H., 2013, P 3 ACM INT WORKSH A, P21, DOI [10.1145/2512530.2512532., DOI 10.1145/2512530.2512532]
   Mohammadzade H, 2013, IEEE T PATTERN ANAL, V35, P381, DOI 10.1109/TPAMI.2012.107
   Mower E, 2009, IEEE T MULTIMEDIA, V11, P843, DOI 10.1109/TMM.2009.2021722
   Mpiperis I, 2008, IEEE T INF FOREN SEC, V3, P498, DOI 10.1109/TIFS.2008.924598
   Nair P, 2009, IEEE T MULTIMEDIA, V11, P611, DOI 10.1109/TMM.2009.2017629
   Ocegueda O., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P1270, DOI 10.1109/ICCVW.2011.6130397
   Ocegueda O, 2011, PROC CVPR IEEE, P641, DOI 10.1109/CVPR.2011.5995613
   Pantic M, 2000, IEEE T PATTERN ANAL, V22, P1424, DOI 10.1109/34.895976
   Qingkai Zhen, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P522, DOI 10.1007/978-3-319-14445-0_45
   Reale M, 2013, IEEE INT CONF AUTOMA
   Sandbach G., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P406, DOI 10.1109/FG.2011.5771434
   Sandbach G, 2012, IMAGE VISION COMPUT, V30, P762, DOI 10.1016/j.imavis.2012.01.006
   Soyel H, 2007, LECT NOTES COMPUT SC, V4633, P831
   Sun Y, 2008, LECT NOTES COMPUT SC, V5303, P58, DOI 10.1007/978-3-540-88688-4_5
   Sun Y, 2010, IEEE T SYST MAN CY A, V40, P461, DOI 10.1109/TSMCA.2010.2041659
   Szeptycki P., 2009, PROC IEEE 3 INT C BI, P1
   Tawari A, 2013, IEEE T MULTIMEDIA, V15, P1543, DOI 10.1109/TMM.2013.2266635
   Vuong Le, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P414, DOI 10.1109/FG.2011.5771435
   Wang J., 2006, P IEEE C COMPUTER VI, P1399
   Wang SF, 2010, IEEE T MULTIMEDIA, V12, P682, DOI 10.1109/TMM.2010.2060716
   Wei J, 2013, IEEE ICCE, P1, DOI 10.1109/ICCE.2013.6486769
   Wu CH, 2013, IEEE T MULTIMEDIA, V15, P1732, DOI 10.1109/TMM.2013.2272917
   Xue ML, 2015, IEEE WINT CONF APPL, P199, DOI 10.1109/WACV.2015.34
   Yeasin M, 2006, IEEE T MULTIMEDIA, V8, P500, DOI 10.1109/TMM.2006.870737
   Yin L., 2008, PROC IEEE INT C AUTO, P1
   Yin LJ, 2006, INT C PATT RECOG, P1248
   Zafeiriou S, 2008, IEEE T MULTIMEDIA, V10, P1528, DOI 10.1109/TMM.2008.2007292
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2008, IEEE T MULTIMEDIA, V10, P570, DOI 10.1109/TMM.2008.921737
NR 48
TC 67
Z9 70
U1 0
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1438
EP 1450
DI 10.1109/TMM.2016.2557063
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600018
DA 2024-07-18
ER

PT J
AU Meng, SB
   Sun, J
   Duan, YZ
   Guo, ZM
AF Meng, Shengbin
   Sun, Jun
   Duan, Yizhou
   Guo, Zongming
TI Adaptive Video Streaming With Optimized Bitstream Extraction and
   PID-Based Quality Control
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bitstream extraction; distortion model; proportional-integral-derivative
   (PID); quality control; scalable video coding (SVC); video streaming
ID SCALABLE VIDEO; RESOURCE-ALLOCATION; BIT EXTRACTION; EXTENSION;
   OPENFLOW; MULTICAST
AB To cope with the challenges brought about by bandwidth fluctuation and improve the experience of watching online videos, an adaptive video streaming system that can adjust video quality according to actual network conditions is proposed based on the scalable video coding (SVC) extension of H.264/AVC. First, a simple and effective linear error model is proposed and verified for quality scalability of SVC. The model exploits the linear feature of pixel value errors and can be used to accurately estimate the distortion caused by discarding any combination of enhancement data packets in an SVC bitstream. On that basis, a greedy-like algorithm is designed to assign each data packet a priority value according to its rate-distortion (R-D) impact, thus enabling R-D optimized bitstream extraction under certain bitrate constraints. Finally, the proportional-integral-derivative (PID) method is utilized to control the video quality adjustment and determine a suitable bitrate for transmission. By monitoring and predicting the past, current, and future bandwidth information, the PID-based quality control algorithm is able to reduce quality fluctuation, while still preserving a high quality level. Experimental results show that compared with the baseline software, the proposed system that integrates the above algorithms can achieve much lower video quality fluctuation, with PSNR variance reduced from 1.24 to 0.69, and at the same time deliver higher video quality, with the PSNR average increased by 0.83 dB.
C1 [Meng, Shengbin; Duan, Yizhou] Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
   [Sun, Jun; Guo, Zongming] Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
   [Sun, Jun; Guo, Zongming] Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
C3 Peking University; Peking University
RP Sun, J (corresponding author), Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
EM mengshengbin@pku.edu.cn; sunjun@pku.edu.cn; duanyizhou@pku.edu.cn;
   guozongming@pku.edu.cn
FU National Key Technology R&D Program of China [2015AA011605]; National
   Natural Science Foundation of China [61271020]
FX This work was supported by the National Key Technology R&D Program of
   China under Grant 2015AA011605 and by the National Natural Science
   Foundation of China under Contract 61271020. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shiwen Mao. (Corresponding author: Jun Sun.)
CR Amonou I, 2007, IEEE T CIRC SYST VID, V17, P1186, DOI 10.1109/TCSVT.2007.906870
   [Anonymous], 2006, OPT THEOR APPR OPT E
   [Anonymous], IEEE SARN S PRINC NJ
   [Anonymous], 2007, 14496 4 2001 PDAM 19
   [Anonymous], IEEE T CIRCUITS SYST
   Astrom K.J., 2002, CONTROL SYSTEM DESIG, P216
   Black P.E., 2005, Dictionary of algorithms and data structures
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Chen YS, 2013, IEEE T MULTIMEDIA, V15, P2087, DOI 10.1109/TMM.2013.2280123
   Chuah SP, 2012, IEEE T MULTIMEDIA, V14, P1324, DOI 10.1109/TMM.2012.2193560
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Grois D, 2013, IEEE ICCE, P588, DOI 10.1109/ICCE.2013.6487029
   Gualdi G, 2008, IEEE T MULTIMEDIA, V10, P1142, DOI 10.1109/TMM.2008.2001378
   Li BC, 1999, IEEE J SEL AREA COMM, V17, P1632, DOI 10.1109/49.790486
   Li BC, 1998, INT WORKSH QUAL SERV, P145, DOI 10.1109/IWQOS.1998.675232
   Li SH, 2014, IEEE COMMUN LETT, V18, P1699, DOI 10.1109/LCOMM.2014.2349991
   Maani E, 2009, IEEE T IMAGE PROCESS, V18, P2022, DOI 10.1109/TIP.2009.2023152
   Palaniappan R, 2012, IEEE IMAGE PROC, P2241, DOI 10.1109/ICIP.2012.6467341
   Schierl T, 2011, MULTIMED TOOLS APPL, V55, P227, DOI 10.1007/s11042-010-0572-5
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Sun J, 2009, IEEE T CIRC SYST VID, V19, P323, DOI 10.1109/TCSVT.2009.2013494
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wien M, 2007, IEEE T CIRC SYST VID, V17, P1194, DOI 10.1109/TCSVT.2007.905530
   Winken M, 2008, IEEE IMAGE PROC, P1220, DOI 10.1109/ICIP.2008.4711981
   Wong CW, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P221, DOI 10.1109/ICME.2004.1394165
   Xue NN, 2015, IEEE T MULTIMEDIA, V17, P1617, DOI 10.1109/TMM.2015.2450014
   Yang EZ, 2014, IEEE GLOB COMM CONF, P1323, DOI 10.1109/GLOCOM.2014.7036991
   Yang KF, 2013, J VIS COMMUN IMAGE R, V24, P752, DOI 10.1016/j.jvcir.2013.04.013
   Zhang WY, 2012, IEEE INT SYMP CIRC S, P1887
   Zhu T, 2011, PRZ ELEKTROTECHNICZN, V87, P284
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
   Ziegler J. G., 1993, Transactions of the ASME. Journal of Dynamic Systems, Measurement and Control, V115, P220, DOI 10.1115/1.2899060
NR 34
TC 11
Z9 11
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1124
EP 1137
DI 10.1109/TMM.2016.2535270
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100015
DA 2024-07-18
ER

PT J
AU Zhang, L
   Zhang, D
AF Zhang, Lei
   Zhang, David
TI Visual Understanding via Multi-Feature Shared Learning With Global
   Consistency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-feature learning; multimedia understanding; semi-supervised
   learning; visual recognition
ID DIMENSIONALITY REDUCTION; FEATURE FUSION; RECOGNITION; FACE; DICTIONARY;
   EXTENSIONS; FRAMEWORK
AB Image/video data is usually represented with multiple visual features. Fusion of multi-source information for establishing attributes has been widely recognized. Multi-feature visual recognition has recently received much attention in multimedia applications. This paper studies visual understanding via a newly proposed l(2)-norm-based multi-feature shared learning framework, which can simultaneously learn a global label matrix and multiple sub-classifiers with the labeled multi-feature data. Additionally, a group graph manifold regularizer composed of the Laplacian and Hessian graph is proposed. It can better preserve the manifold structure of each feature, such that the label prediction power is much improved through semi-supervised learning with global label consistency. For convenience, we call the proposed approach global-label-consistent classifier (GLCC). The merits of the proposed method include the following: 1) the manifold structure information of each feature is exploited in learning, resulting in a more faithful classification owing to the global label consistency; 2) a group graph manifold regularizer based on the Laplacian and Hessian regularization is constructed; and 3) an efficient alternative optimization method is introduced as a fast solver owing its speed to convex sub-problems. Experiments on several benchmark visual datasets-the 17-category Oxford Flower dataset, the challenging 101-category Caltech dataset, the YouTube and Consumer Videos dataset, and the large-scale NUS-WIDE dataset-have been used for multimedia understanding. The results demonstrate that the proposed approach compares favorably with state-of-the-art algorithms. An extensive experiment using the deep convolutional activation features also shows the effectiveness of the proposed approach. The code will be available on http://www.escience.cn/people/lei/index.html
C1 [Zhang, Lei] Chongqing Univ, Coll Commun Engn, Chongqing 400044, Peoples R China.
   [Zhang, Lei; Zhang, David] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
C3 Chongqing University; Hong Kong Polytechnic University
RP Zhang, L (corresponding author), Chongqing Univ, Coll Commun Engn, Chongqing 400044, Peoples R China.; Zhang, L (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Hong Kong, Peoples R China.
EM leizhang@cqu.edu.cn; csdzhang@comp.polyu.edu.hk
RI Zhang, Hao/HHM-1940-2022; Zhang, David D/O-9396-2016
OI Zhang, David D/0000-0002-5027-5286
FU National Natural Science Foundation of China [61401048]; Research Fund
   Project for Central Universities; Hong Kong Scholar Program [XJ2013044]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61401048, by the Research Fund Project for Central
   Universities, and by the Hong Kong Scholar Program under Grant
   XJ2013044. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Martha Larson.
   (Corresponding author: Lei Zhang.)
CR [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2007, Computer Vision
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   [Anonymous], 2007, P 24 INT C MACH LEAR, DOI DOI 10.1145/1273496.1273594
   [Anonymous], 2010, SDM
   [Anonymous], P ACM MM
   [Anonymous], P ANN M ASS COMP LIN
   Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bi J., 2004, ACM SIGKDD INT C KNO, P521
   Boureau YL, 2010, PROC CVPR IEEE, P2559, DOI 10.1109/CVPR.2010.5539963
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Farquhar JDR, 2005, ADV NEURAL INFORM PR, V18, P355, DOI DOI 10.5555/2976248.2976293
   Gehler P, 2009, IEEE I CONF COMP VIS, P221, DOI 10.1109/ICCV.2009.5459169
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Hanghang Tong, 2005, 13th Annual ACM International Conference on Multimedia, P862, DOI 10.1145/1101149.1101337
   Jiang ZL, 2011, PROC CVPR IEEE, P1697, DOI 10.1109/CVPR.2011.5995354
   Kim K., 2009, P ASME TURBO EXPO 20, P1, DOI DOI 10.1145/1657120.1657121
   Kim TK, 2007, IEEE T PATTERN ANAL, V29, P1005, DOI 10.1109/TPAMI.2007.1037
   Klausner A, 2007, 2007 FIRST ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P63
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lu JW, 2013, IEEE T INF FOREN SEC, V8, P510, DOI 10.1109/TIFS.2013.2243146
   Ma ZG, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P77, DOI 10.1145/2647868.2654907
   Ma ZG, 2012, IEEE T MULTIMEDIA, V14, P1662, DOI 10.1109/TMM.2012.2199293
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Nilsback M.E., 2006, IEEE C COMP VIS PATT, P1447, DOI DOI 10.1109/CVPR.2006.42
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964
   Ross A, 2005, PROC SPIE, V5779, P196, DOI 10.1117/12.606093
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Shekhar S, 2014, IEEE T PATTERN ANAL, V36, P113, DOI 10.1109/TPAMI.2013.109
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Vedaldi A, 2009, IEEE I CONF COMP VIS, P606, DOI 10.1109/ICCV.2009.5459183
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Wu F, 2014, IEEE T MULTIMEDIA, V16, P427, DOI 10.1109/TMM.2013.2291214
   Xia TA, 2010, IEEE T SYST MAN CY B, V40, P1438, DOI 10.1109/TSMCB.2009.2039566
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang M, 2011, IEEE I CONF COMP VIS, P543, DOI 10.1109/ICCV.2011.6126286
   Yang Y, 2009, PR ELECTROMAGN RES S, P311, DOI 10.1145/1631272.1631316
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P572, DOI 10.1109/TMM.2012.2234731
   Yang Y, 2013, IEEE T MULTIMEDIA, V15, P661, DOI 10.1109/TMM.2012.2237023
   Yuan XT, 2012, IEEE T IMAGE PROCESS, V21, P4349, DOI 10.1109/TIP.2012.2205006
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HC, 2011, IEEE I CONF COMP VIS, P595, DOI [10.1109/ICCV.2011.6126293, 10.1109/APAP.2011.6180470]
   Zhang QA, 2010, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2010.5539989
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhou N, 2014, IEEE T PATTERN ANAL, V36, P715, DOI 10.1109/TPAMI.2013.189
   Zhou XL, 2006, INT C PATT RECOG, P529
NR 57
TC 44
Z9 45
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2016
VL 18
IS 2
BP 247
EP 259
DI 10.1109/TMM.2015.2510509
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DB3HX
UT WOS:000368402400009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wu, ZF
   Huang, YZ
   Wang, L
AF Wu, Zifeng
   Huang, Yongzhen
   Wang, Liang
TI Learning Representative Deep Features for Image Set Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Album classification; deep learning; gait recognition; image set
ID CLASSIFICATION; APPEARANCE; CONTEXT
AB This paper proposes to learn features from sets of labeled raw images. With this method, the problem of over-fitting can be effectively suppressed, so that deep CNNs can be trained from scratch with a small number of training data, i e., 420 labeled albums with about 30 000 photos. This method can effectively deal with sets of images, no matter if the sets bear temporal structures. A typical approach to sequential image analysis usually leverages motions between adjacent frames, while the proposed method focuses on capturing the co-occurrences and frequencies of features. Nevertheless, our method outperforms previous best performers in terms of album classification, and achieves comparable or even better performances in terms of gait based human identification. These results demonstrate its effectiveness and good adaptivity to different kinds of set data.
C1 [Wu, Zifeng] Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
   [Huang, Yongzhen; Wang, Liang] Chinese Acad Sci, Inst Automat, CAS Ctr Excellence Brain Sci & Intelligence Techn, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
C3 University of Adelaide; Chinese Academy of Sciences; Institute of
   Automation, CAS
RP Wu, ZF (corresponding author), Univ Adelaide, Australian Ctr Visual Technol, Adelaide, SA 5005, Australia.
EM zifeng.wu@adelaide.edu.au; yzhuang@nlpr.ia.ac.cn;
   wangliang@nlpr.ia.ac.cn
FU National Basic Research Program of China [2012CB316300]; National
   Natural Science Foundation of China [61135002, 61420106015]; CCF-Tencent
   Open Fund; 360 OpenLab Program
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316300, by the National Natural Science
   Foundation of China under Grant 61135002 and Grant 61420106015, by the
   CCF-Tencent Open Fund, and by the 360 OpenLab Program. The guest editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Guo-Jun Qi.
CR [Anonymous], CORR
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2008, 2008 IEEE C COMPUTER, DOI DOI 10.1109/CVPR.2008.4587383
   [Anonymous], 2012, IMPROVING NEURAL NET
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], JOINT ACM WORKSH MOD
   [Anonymous], P ECCV 14 INT WORKSH
   [Anonymous], 2014, P IEEE C COMPUTER VI
   Ariyanto G., 2011, 2011 INT JOINT C BIO, P1, DOI DOI 10.1109/IJCB.2011.6117582
   Bashir K., 2010, the British Machine Vision Conference, P1, DOI DOI 10.1049/IC.2009.0230
   Bodor R, 2009, IMAGE VISION COMPUT, V27, P1194, DOI 10.1016/j.imavis.2008.11.008
   Bossard L, 2013, IEEE I CONF COMP VIS, P1193, DOI 10.1109/ICCV.2013.151
   Cao L., 2008, Proc. ACM Multimedia, P121
   Cao LL, 2009, IEEE T MULTIMEDIA, V11, P208, DOI 10.1109/TMM.2008.2009693
   Chang SY, 2013, IEEE DATA MINING, P979, DOI 10.1109/ICDM.2013.49
   Chen SK, 2013, PROC CVPR IEEE, P452, DOI 10.1109/CVPR.2013.65
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Garcia-Perez A., 2019, Designing and tracking knowledge management metrics, P163
   Han J, 2006, IEEE T PATTERN ANAL, V28, P316, DOI 10.1109/TPAMI.2006.38
   Hu HF, 2013, IEEE T CIRC SYST VID, V23, P1274, DOI 10.1109/TCSVT.2013.2242640
   Hu MD, 2013, IEEE T INF FOREN SEC, V8, P2034, DOI 10.1109/TIFS.2013.2287605
   Iwama H, 2012, IEEE T INF FOREN SEC, V7, P1511, DOI 10.1109/TIFS.2012.2204253
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Kusakunniran Worapan, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1058, DOI 10.1109/ICCVW.2009.5457587
   Kusakunniran W, 2014, IEEE T IMAGE PROCESS, V23, P696, DOI 10.1109/TIP.2013.2294552
   Kusakunniran W, 2012, IEEE T CIRC SYST VID, V22, P966, DOI 10.1109/TCSVT.2012.2186744
   Kusakunniran W, 2010, PROC CVPR IEEE, P974, DOI 10.1109/CVPR.2010.5540113
   Lam THW, 2011, PATTERN RECOGN, V44, P973, DOI 10.1016/j.patcog.2010.10.011
   LeCun Y., 1990, NeurIPS, P396
   Lee KC, 2003, PROC CVPR IEEE, P313
   Leibe B, 2003, PROC CVPR IEEE, P409
   Lu JW, 2013, IEEE I CONF COMP VIS, P329, DOI 10.1109/ICCV.2013.48
   Makihara Y, 2006, LECT NOTES COMPUT SC, V3953, P151, DOI 10.1007/11744078_12
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Qi G.J., 2007, P 15 ACM INT C MULTI, P17, DOI DOI 10.1145/1291233.1291245
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Qi GJ, 2009, IEEE T PATTERN ANAL, V31, P1880, DOI 10.1109/TPAMI.2008.218
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Simonyan K., 2014, CORR
   Tang K, 2012, PROC CVPR IEEE, P1250, DOI 10.1109/CVPR.2012.6247808
   Wang C, 2010, LECT NOTES COMPUT SC, V6311, P257, DOI 10.1007/978-3-642-15549-9_19
   Yu SQ, 2006, INT C PATT RECOG, P441
   Zhao GY, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P529
NR 43
TC 72
Z9 76
U1 1
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1960
EP 1968
DI 10.1109/TMM.2015.2477681
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400009
DA 2024-07-18
ER

PT J
AU Hachicha, W
   Kaaniche, M
   Beghdadi, A
   Cheikh, FA
AF Hachicha, Walid
   Kaaniche, Mounir
   Beghdadi, Azeddine
   Cheikh, Faouzi Alaya
TI Efficient Inter-View Bit Allocation Methods for Stereo Image Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit allocation; compression; quantization; rate-distortion theory;
   stereo image; wavelets
ID DEPENDENT QUANTIZATION; COMPRESSION; VIDEO
AB In this paper, we present efficient bit allocation methods for stereo image coding purpose. Since the common idea behind most of the existing stereo compression schemes consists of encoding a reference and residual images as well as a disparity map, we mainly focus on the bit allocation issue between the reference and residual images. Generally, this problem is solved in an empirical manner by looking for the optimal rates leading to the minimum distortion value. Thanks to recent approximations of the entropy and distortion functions, we propose accurate and fast bit allocation schemes appropriate for the open-loop- and closed-loop-based stereo coding structures. Experimental results show the benefits which can be drawn from the proposed bit allocation methods.
C1 [Hachicha, Walid; Kaaniche, Mounir; Beghdadi, Azeddine] Univ Paris 13, L2TI, Inst Galilee, F-93430 Villetaneuse, France.
   [Cheikh, Faouzi Alaya] Gjovik Univ Coll, Norwegian Colour & Visual Comp Lab, N-2815 Gjovik, Norway.
C3 Universite Paris 13; Norwegian University of Science & Technology (NTNU)
RP Hachicha, W (corresponding author), Univ Paris 13, L2TI, Inst Galilee, F-93430 Villetaneuse, France.
EM walid.hachicha@univ-paris13.fr; mounir.kaaniche@univ-paris13.fr;
   azeddine.beghdadi@univ-paris13.fr; faouzi.cheikh@hig.no
RI Beghdadi, Azeddine/ABF-9801-2022
OI Beghdadi, Azeddine/0000-0002-5595-0615
CR Alvers U, 2003, IEEE IMAGE PROC, P761
   André T, 2007, EURASIP J IMAGE VIDE, DOI 10.1155/2007/30852
   AYDINOGLU H, 1995, IEEE INT SYMP CIRC S, P247, DOI 10.1109/ISCAS.1995.521497
   Bjontegaard G., 2001, ITU SG16 AUST TX US
   Boulgouris NV, 2002, IEEE T CIRC SYST VID, V12, P898, DOI 10.1109/TCSVT.2002.804895
   Cheung G, 2011, IEEE T IMAGE PROCESS, V20, P3179, DOI 10.1109/TIP.2011.2158230
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   Ellinas JN, 2004, IMAGE VISION COMPUT, V22, P281, DOI 10.1016/j.imavis.2003.09.017
   Frajka T, 2003, OPT ENG, V42, P182, DOI 10.1117/1.1526492
   Fraysse A, 2009, IEEE T INFORM THEORY, V55, P3243, DOI 10.1109/TIT.2009.2021329
   Gelman A, 2012, IEEE T IMAGE PROCESS, V21, P4092, DOI 10.1109/TIP.2012.2201490
   GISH H, 1968, IEEE T INFORM THEORY, V14, P676, DOI 10.1109/TIT.1968.1054193
   György A, 1999, IEEE T INFORM THEORY, V45, P2110, DOI 10.1109/18.782151
   Hachicha W., 2013, P EUR SIGN PROC C MA, P1
   Kaaniche M, 2014, IEEE T IMAGE PROCESS, V23, P137, DOI 10.1109/TIP.2013.2286325
   Kaaniche M, 2009, IEEE T IMAGE PROCESS, V18, P2463, DOI 10.1109/TIP.2009.2026672
   Lucas L., 2011, P IEEE INT C COMP TO, P1
   Lukacs M. E., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P521
   Maalouf A, 2010, INT CONF ACOUST SPEE, P698, DOI 10.1109/ICASSP.2010.5495084
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Moellenhoff MS, 1998, IEEE T IMAGE PROCESS, V7, P804, DOI 10.1109/83.679421
   Nadenau MJ, 2003, IEEE T IMAGE PROCESS, V12, P58, DOI 10.1109/TIP.2002.807358
   Parrilli S, 2008, 2008 IEEE 10TH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, VOLS 1 AND 2, P200, DOI 10.1109/MMSP.2008.4665075
   PERKINS MG, 1992, IEEE T COMMUN, V40, P684, DOI 10.1109/26.141424
   Rajaei B, 2013, ANN TELECOMMUN, V68, P627, DOI 10.1007/s12243-013-0375-6
   RAMCHANDRAN K, 1994, IEEE T IMAGE PROCESS, V3, P533, DOI 10.1109/83.334987
   Shao F, 2013, IEEE T MULTIMEDIA, V15, P1843, DOI 10.1109/TMM.2013.2269897
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   Sullivan GJ, 1996, IEEE T INFORM THEORY, V42, P1365, DOI 10.1109/18.532878
   SZEPANSKI W, 1980, ELECTRON LETT, V16, P109, DOI 10.1049/el:19800083
   Taubman D., 2012, JPEG2000: image compression fundamentals, standards and practice, V642
   Van Leuven S, 2011, IEEE IMAGE PROC, P1661, DOI 10.1109/ICIP.2011.6115773
   WALLACE GK, 1991, COMMUN ACM, V34, P30, DOI 10.1145/103085.103089
   Wang YH, 2004, IEEE GEOSCI REMOTE S, V1, P136, DOI 10.1109/LGRS.2004.824762
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Woo O, 2000, IEEE T CIRC SYST VID, V10, P194, DOI 10.1109/76.825718
   Woo W, 1999, IEEE T CIRC SYST VID, V9, P861, DOI 10.1109/76.785724
   Woo W, 1997, P SOC PHOTO-OPT INS, V3024, P391, DOI 10.1117/12.263251
   Wu SW, 1991, IEEE T CIRC SYST VID, V1, P100, DOI 10.1109/TCSVT.1991.4519809
NR 39
TC 11
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 765
EP 777
DI 10.1109/TMM.2015.2417099
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Harte, N
   Gillen, E
AF Harte, Naomi
   Gillen, Eoin
TI TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio-visual speech recognition
ID RECOGNITION; FEATURES; DATABASE
AB Automatic audio-visual speech recognition currently lags behind its audio-only counterpart in terms of major progress. One of the reasons commonly cited by researchers is the scarcity of suitable research corpora. This paper details the creation of a new corpus designed for continuous audio-visual speech recognition research. TCD-TIMIT consists of high-quality audio and video footage of 62 speakers reading a total of 6913 phonetically rich sentences. Three of the speakers are professionally-trained lipspeakers, recorded to test the hypothesis that lipspeakers may have an advantage over regular speakers in automatic visual speech recognition systems. Video footage was recorded from two angles: straight on, and at. The paper outlines the recording of footage, and the required post-processing to yield video and audio clips for each sentence. Audio, visual, and joint audio-visual baseline experiments are reported. Separate experiments were run on the lipspeaker and non-lipspeaker data, and the results compared. Visual and audio-visual baseline results on the non-lipspeakers were low overall. Results on the lipspeakers were found to be significantly higher. It is hoped that as a publicly available database, TCD-TIMIT will now help further state of the art in audio-visual speech recognition research.
C1 [Harte, Naomi; Gillen, Eoin] Trinity Coll Dublin, Dept Elect & Elect Engn, Sigmedia Grp, Dublin 2, Ireland.
C3 Trinity College Dublin
RP Harte, N (corresponding author), Trinity Coll Dublin, Dept Elect & Elect Engn, Sigmedia Grp, Dublin 2, Ireland.
EM nharte@tcd.ie; ogiollae@tcd.ie
OI Harte, Naomi/0000-0002-9274-209X
FU Science Foundation Ireland [07/EN/E007]; Science Foundation Ireland
   (SFI) [07/EN/E007] Funding Source: Science Foundation Ireland (SFI)
FX Manuscript received September 26, 2014; revised February 06, 2015;
   accepted February 09, 2015. Date of publication February 26, 2015; date
   of current version April 15, 2015. This work was supported by Science
   Foundation Ireland under Grant 07/EN/E007. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Gokhan Tur.
CR [Anonymous], 2009, PROC HCSNET WORKSHOP
   [Anonymous], 2004, ICMI'04-Sixth International Conference on Multimodal Interfaces, DOI [DOI 10.1145/1027933.1027972, 10.1145/1027933.1027972]
   [Anonymous], 2010, 17703 BSI
   [Anonymous], P 14 AUSTR INT C SPE
   [Anonymous], P INT C SIGN PROC CO
   [Anonymous], 2003, P FON
   Bailly-Bailliére E, 2003, LECT NOTES COMPUT SC, V2688, P625
   Bliss A., 1972, PROC R IRACAD SCI, V72, P63
   BRUGNARA F, 1993, SPEECH COMMUN, V12, P357, DOI 10.1016/0167-6393(93)90083-W
   Cappelletta L., 2012, THESIS TRINITY COLL
   Chen TH, 2001, IEEE SIGNAL PROC MAG, V18, P9
   Chibelushi C. C., 1996, P IEE C INT AUD VIS
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Chitu AG, 2007, EUROMEDIA '2007, P88
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Driel K., 2009, THESIS DELFT U TECHN
   Fox N. A., 2005, AUDIO VIDEO BASED BI
   Galatas G., 2011, P 4 INT C PERV TECHN
   Galatas G, 2012, EUR SIGNAL PR CONF, P2714
   Gan T., 2012, THESIS HAMBURG U HAM, DOI Dept. Informat., Hamburg Univ.
   Garofolo J. S., 1993, Timit acoustic phonetic continuous speech corpus
   Goecke R., 2004, P 10 AUST INT C SPEE, P486
   Gowdy JN, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P993
   Heckmann M., 2002, Proceedings of International Conference on Spoken Lan- guage Processing, P1925
   Hosom J.-P., 2000, THESIS OREGON GRADUA
   Jeffers J., 1971, SPEECHREADING LIPREA
   Kumar K, 2007, INT CONF ACOUST SPEE, P429
   Lamel L.F., 1986, Proceedings of the DARPA Speech Recognition Workshop, P100
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Lucey P, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P24, DOI 10.1109/MMSP.2006.285261
   Matthews I, 2002, IEEE T PATTERN ANAL, V24, P198, DOI 10.1109/34.982900
   Matthews I., 2001, P INT C MULT EXP, P22
   Messer K., 1999, 2 INT C AUD VID BAS, V964, P965
   Movellan J. R., 1995, Advances in Neural Information Processing Systems 7, P851
   Neti C., 2000, CLSP SUMM WORKSH JOH
   Papandreou G, 2009, IEEE T AUDIO SPEECH, V17, P423, DOI 10.1109/TASL.2008.2011515
   Pass A, 2010, IEEE IMAGE PROC, P2417, DOI 10.1109/ICIP.2010.5650963
   Patterson EK, 2002, INT CONF ACOUST SPEE, P2017
   Pellom BL, 1998, SPEECH COMMUN, V25, P97, DOI 10.1016/S0167-6393(98)00031-4
   Petajan E. D., 1984, THESIS U ILLINOIS UR
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Potamianos G, 2001, INT CONF ACOUST SPEE, P165, DOI 10.1109/ICASSP.2001.940793
   Saitoh T., 2010, P AVSP 2010, P131
   Sanderson C., 2008, Biometric person recognition : face, speech and fusion
   Scanlon P., 2003, P INT C AUD VIS SPEE, P127
   Seymour R., 2008, J IMAGE VIDEO PROCES, V2008
   Taylor Sarah, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3037, DOI 10.1109/ICASSP.2014.6854158
   Vertanen K., 2006, Baseline WSJ Acoustic Models for HTK and Sphinx
   Wijitha Senadeera Wijitha Senadeera, 2006, International Journal of Food Engineering, V2, P7
   Wong YW, 2011, PATTERN RECOGN LETT, V32, P1503, DOI 10.1016/j.patrec.2011.06.011
   Xiao L, 2008, I C WIREL COMM NETW, P1
   Yuan J, 2008, J ACOUST SOC AM, V124, P2078, DOI 10.1121/1.2968700
   Yuxuan Lan, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P432, DOI 10.1109/ICME.2012.192
   Zhang XZ, 2002, EURASIP J APPL SIG P, V2002, P1228, DOI 10.1155/S1110865702206137
NR 54
TC 130
Z9 135
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 603
EP 615
DI 10.1109/TMM.2015.2407694
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300003
DA 2024-07-18
ER

PT J
AU Yang, XS
   Zhang, TZ
   Xu, CS
   Hossain, MS
AF Yang, Xiaoshan
   Zhang, Tianzhu
   Xu, Changsheng
   Hossain, M. Shamim
TI Automatic Visual Concept Learning for Social Event Understanding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event analysis; video recognition
ID RECOGNITION
AB Vision-based event analysis is extremely difficult due to the various concepts (object, action, and scene) contained in videos. Though visual concept-based event analysis has achieved significant progress, it has two disadvantages: visual concept is defined manually, and has only one corresponding classifier in traditional methods. To deal with these issues, we propose a novel automatic visual concept learning algorithm for social event understanding in videos. First, instead of defining visual concept manually, we propose an effective automatic concept mining algorithm with the help of Wikipedia, N-gram Web services, and Flickr. Then, based on the learned visual concept, we propose a novel boosting concept learning algorithm to iteratively learn multiple classifiers for each concept to enhance its representative discriminability. The extensive experimental evaluations on the collected dataset well demonstrate the effectiveness of the proposed algorithm for social event understanding.
C1 [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yang, Xiaoshan; Zhang, Tianzhu; Xu, Changsheng] China Singapore Inst Digital Media, Singapore 119613, Singapore.
   [Hossain, M. Shamim] King Saud Univ, Coll Comp & Informat Sci, SWE Dept, Riyadh 12372, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Saud
   University
RP Yang, XS (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xiaoshang.yang@nlpr.ia.ac.cn; tzzhang@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn;
   mshossain@ksu.edu.sa
RI Zhang, Tianzhu/AGY-9389-2022; Guizani, Mohsen/AAX-4534-2021; xu,
   cj/HJZ-3488-2023; Hossain, M. Shamim/K-1362-2014
OI Zhang, Tianzhu/0000-0003-0764-6106; Guizani, Mohsen/0000-0002-8972-8094;
   Hossain, M. Shamim/0000-0001-5906-9422
FU National Program on Key Basic Research Project under the 973 Program
   [2012CB316304]; National Natural Science Foundation of China [61225009,
   61303173, 61373122, 61432019]; Beijing Natural Science Foundation
   [4131004]; Deanship of Scientific Research, King Saud University [RGP
   VPP-228]
FX This work was supported in part by the National Program on Key Basic
   Research Project under the 973 Program, Project 2012CB316304, by the
   National Natural Science Foundation of China under Grant 61225009, Grant
   61303173, Grant 61373122, and Grant 61432019, by the Beijing Natural
   Science Foundation under Grant 4131004, and by the Deanship of
   Scientific Research, King Saud University under the research group
   Project RGP VPP-228. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. K. Selcuk
   Candan.
CR [Anonymous], P 21 ACM INT C MULT
   [Anonymous], 2010, ACM INT C MULTIMEDIA
   [Anonymous], 2012, ACM MM'12'
   [Anonymous], 2009, P 17 ACM INT C MULT, DOI DOI 10.1145/1631272.1631371
   [Anonymous], 2012, P 2 ACM INT C MULTIM, DOI DOI 10.1145/2324796.2324825
   [Anonymous], P 2 ACM INT C MULT R
   [Anonymous], P 20 ACM INT C MULT
   [Anonymous], 2013, P 3 ACM C INT C MULT
   [Anonymous], 2012, P 29 INT C MACH LEAR
   Bao B.-K., 2013, Proceedings of the International Conference on Multimedia Retrieval, P135
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Brenner M., 2012, P 2 ACM INT C MULT R
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819
   Farquhar J.D. R., 2005, NIPS
   Glorot X., 2011, P 28 INT C INT C MAC, P513
   Gong B., 2013, P INT C MACH LEARN, P222
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Hauptmann AG, 2005, LECT NOTES COMPUT SC, V3568, P1
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li C., 2012, Cikm, P155, DOI DOI 10.1145/2396761.2396785
   Liu JE, 2013, IEEE WORK APP COMP, P339, DOI 10.1109/WACV.2013.6475038
   Liu LQ, 2011, IEEE I CONF COMP VIS, P2486, DOI 10.1109/ICCV.2011.6126534
   Liu XC, 2013, PROCEEDINGS OF THE 1ST INTERNATIONAL JOINT SYMPOSIUM ON JOINING AND WELDING, P151
   Lu WT, 2013, IEEE T MULTIMEDIA, V15, P1920, DOI 10.1109/TMM.2013.2280895
   Luo J., 2008, ACM International Conference on Multimedia, P1071, DOI DOI 10.1145/1459359.1459574MULTIMEDIA-MM'PLACE
   Ma Z., 2012, MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, P469, DOI DOI 10.1145/2393347.2393414
   Ma ZG, 2013, PROC CVPR IEEE, P2627, DOI 10.1109/CVPR.2013.339
   Orlando S, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P1285
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Ramanathan V, 2013, IEEE I CONF COMP VIS, P905, DOI 10.1109/ICCV.2013.117
   Reuter T., 2012, P 2 ACM INT C MULT R
   Revaud J, 2013, PROC CVPR IEEE, P2459, DOI 10.1109/CVPR.2013.318
   Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Wang W, 2013, P AS C MACH LEARN, P467
   Wang Y., 2012, Proceedings of the 20th ACM International Conference on Multimedia, P865, DOI [DOI 10.1145/2393347.239633216, 10.1145/2393347.239633216]
   Yang X, 2007, INT J PATTERN RECOGN, V21, P961, DOI 10.1142/S0218001407005703
   Yang Y, 2013, IEEE I CONF COMP VIS, P2104, DOI 10.1109/ICCV.2013.456
   Yu Q., 2012, P 20 ACM INT C MULT, P1073
   Zhu J, 2009, STAT INTERFACE, V2, P349
NR 45
TC 50
Z9 52
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 346
EP 358
DI 10.1109/TMM.2015.2393635
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700007
DA 2024-07-18
ER

PT J
AU Tang, SY
   Alface, PR
AF Tang, Siyu
   Alface, Patrice Rondao
TI Impact of Random and Burst Packet Losses on H.264 Scalable Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error propagation; Markov model; packet-loss-induced quality
   degradation; scalable video coding (SVC)
ID QUALITY; TRANSMISSION
AB This paper presents an encoder-based prediction model to study the impact of packet loss on H. 264 scalable video coding (SVC). A Markov Chain (MC) with states is developed to describe the error propagation process inside a group of pictures (GOP). Using the packet loss rate reported by RTCP (RTP Control Protocol) and analyzing the inter-frame prediction rules, we evaluate packet-loss-induced quality degradation in SVC as the probability that a given scalable layer can be correctly received. Based on the proposed model, the performance of the SVC hierarchical B-frame structure, the zero delay encoding/decoding structure, and the advance video coding (AVC) IPPP structure (compatible base layer in SVC) are evaluated and compared under both random and burst packet loss events.
C1 [Tang, Siyu; Alface, Patrice Rondao] Alcatel Lucent, Bell Labs, B-2018 Antwerp, Belgium.
C3 Alcatel-Lucent
RP Tang, SY (corresponding author), Alcatel Lucent, Bell Labs, B-2018 Antwerp, Belgium.
EM Siyu.Tang@Alcatel-Lucent.com; Patrice.Rondao_Alface@Alcatel-Lucent.com
CR [Anonymous], 2007, 1449610 ISOIEC ITUT
   Ardestani Majid R., 2010, 2010 17th International Conference on Telecommunications (ICT 2010), P923, DOI 10.1109/ICTEL.2010.5478827
   Argyropoulos S, 2011, INT WORK QUAL MULTIM, P31, DOI 10.1109/QoMEX.2011.6065708
   Borella M. S., 1999, P IEEE INFOCOM 99, P3
   Chen ZF, 2012, IEEE T IMAGE PROCESS, V21, P1123, DOI 10.1109/TIP.2011.2168411
   Clark A., 2013, 6958 RFC
   Ekmekci S, 2004, IEEE IMAGE PROC, P187
   ELLIOTT EO, 1963, AT&T TECH J, V42, P1977, DOI 10.1002/j.1538-7305.1963.tb00955.x
   Feller W., 1971, INTRO PROBABILITY TH
   Ghareeb M., 2011, 2011 International Conference on Information Networking (ICOIN), P206, DOI 10.1109/ICOIN.2011.5723179
   GILBERT EN, 1960, BELL SYST TECH J, V39, P1253, DOI 10.1002/j.1538-7305.1960.tb03959.x
   Halinger Gerhard, 2008, Proc. 14th GI/ITG Conf. Meas. Modelling Evaluation Comput. Commun. Syst, P1
   J. V. Team, 2009, H 264 SVC REF SOFTW
   Liang YJ, 2008, IEEE T CIRC SYST VID, V18, P861, DOI 10.1109/TCSVT.2008.923139
   Loguinov D, 2002, IEEE INFOCOM SER, P723, DOI 10.1109/INFCOM.2002.1019318
   Mansour H, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND INFORMATION TECHNOLOGY, VOLS 1 AND 2, P536, DOI 10.1109/ISSPIT.2006.270859
   Monteiro JM, 2008, IEEE T BROADCAST, V54, P652, DOI 10.1109/TBC.2008.2001717
   Naccari M, 2009, IEEE T MULTIMEDIA, V11, P932, DOI 10.1109/TMM.2009.2021785
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Schulzrinne H., 2003, RTP TRANSPORT PROTOC
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Staelens N, 2012, IEEE T BROADCAST, V58, P187, DOI 10.1109/TBC.2012.2189334
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tang S., 2013, 20130606 BELL LAB
   Tang S., 2013, 5 INT C ADV MULT MME
   Tao S, 2008, IEEE ACM T NETWORK, V16, P1052, DOI 10.1109/TNET.2007.910617
   Van Mieghem P., 2006, Performance analysis of communications networks and systems
   Ying-Hong Wang, 2009, 2009 Symposia and Workshops on Ubiquitous, Autonomic and Trusted Computing in conjunction with the UIC 2009 and ATC 2009 Conferences, P1, DOI [10.1109/ICBBE.2009.5163482, 10.1109/UIC-ATC.2009.19]
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   Yang H, 2007, IEEE T CIRC SYST VID, V17, P845, DOI 10.1109/TCSVT.2007.897116
   Zhang CY, 2008, SIGNAL PROCESS-IMAGE, V23, P116, DOI 10.1016/j.image.2007.12.002
   Zhang R, 2000, IEEE J SEL AREA COMM, V18, P966, DOI 10.1109/49.848250
   Zhang R, 2010, IEEE T IMAGE PROCESS, V19, P2947, DOI 10.1109/TIP.2010.2051624
   Zhou Y, 2011, IEEE T CIRC SYST VID, V21, P1679, DOI 10.1109/TCSVT.2011.2133390
NR 34
TC 8
Z9 11
U1 1
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2256
EP 2269
DI 10.1109/TMM.2014.2348947
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300015
DA 2024-07-18
ER

PT J
AU Tsai, JT
   Lin, YY
   Liao, HYM
AF Tsai, Jeng-Tsung
   Lin, Yen-Yu
   Liao, Hong-Yuan Mark
TI Per-Cluster Ensemble Kernel Learning for Multi-Modal Image Clustering
   With Group-Dependent Feature Selection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cluster-dependent feature selection; clustering; image grouping;
   multiple kernel learning (MKL); object categorization
ID SCALE; ILLUMINATION
AB In this paper, we present a clustering approach, MK-SOM, that carries out cluster-dependent feature selection, and partitions images with multiple feature representations into clusters. This work is motivated by the observations that human visual systems (HVS) can receive various kinds of visual cues for interpreting the world. Images identified by HVS as the same category are typically coherent to each other in certain crucial visual cues, but the crucial cues vary from category to category. To account for this observation and bridge the semantic gap, the proposed MK-SOM integrates multiple kernel learning (MKL) into the training process of self-organizing map (SOM), and associates each cluster with a learnable, ensemble kernel. Hence, it can leverage information captured by various image descriptors, and discoveries the cluster-specific characteristics via learning the per-cluster ensemble kernels. Through the optimization iterations, cluster structures are gradually revealed via the features specified by the learned ensemble kernels, while the quality of these ensemble kernels is progressively improved owing to the coherent clusters by enforcing SOM. Besides, MK-SOM allows the introduction of side information to improve performance, and it hence provides a new perspective of applying MKL to address both unsupervised and semi-supervised clustering tasks. Our approach is comprehensively evaluated in the two applications. The superior and promising results manifest its effectiveness.
C1 [Tsai, Jeng-Tsung] Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Lin, Yen-Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Liao, Hong-Yuan Mark] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
C3 University of Southern California; Academia Sinica - Taiwan; Academia
   Sinica - Taiwan
RP Tsai, JT (corresponding author), Univ So Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM jengtsut@usc.edu; yylin@citi.sinica.edu.tw; liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
OI Lin, Yen-Yu/0000-0002-7183-6070
FU Ministry of Science and Technology (MOST) [103-2221-E-001-026-MY2];
   Institute for Information Industry (III) [103-EC-17-A-24-1170]
FX This work was supported in part by Ministry of Science and Technology
   (MOST) under Grant 103-2221-E-001-026-MY2 and by Institute for
   Information Industry (III) under Grant 103-EC-17-A-24-1170. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. K. Selcuk Candan.
CR [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], P INT C MACH LEARN
   [Anonymous], P EUR C COMP VIS
   [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], INT C VER LARG DAT
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], ACM T KNOWLEDGE DISC
   [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], 2004, P INT C MACH LEARN
   [Anonymous], P INT C PATT REC
   [Anonymous], 2004, Workshop on Generative Model Based Vision
   [Anonymous], 2012, IEEE C EVOL COMPUTAT
   [Anonymous], EUR S ART NEUR NETW
   [Anonymous], ADV NEURAL INF PROCE
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], P C COMP VIS PATT RE
   [Anonymous], ADV NEURAL INF PROCE
   Bickel S, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P19, DOI 10.1109/ICDM.2004.10095
   Bosch A, 2007, IEEE I CONF COMP VIS, P1863
   Boulet R, 2008, NEUROCOMPUTING, V71, P1257, DOI 10.1016/j.neucom.2007.12.026
   Chen XJ, 2013, IEEE T KNOWL DATA EN, V25, P932, DOI 10.1109/TKDE.2011.262
   Chen XL, 2012, PROC CVPR IEEE, P2533, DOI 10.1109/CVPR.2012.6247970
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Deng ZH, 2010, PATTERN RECOGN, V43, P767, DOI 10.1016/j.patcog.2009.09.010
   Nguyen DT, 2012, IEEE T KNOWL DATA EN, V24, P988, DOI 10.1109/TKDE.2011.86
   Dueck D, 2007, IEEE I CONF COMP VIS, P198
   Filippone M, 2008, PATTERN RECOGN, V41, P176, DOI 10.1016/j.patcog.2007.05.018
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Goldberger J, 2006, IEEE T IMAGE PROCESS, V15, P449, DOI 10.1109/TIP.2005.860593
   Gross R, 2003, LECT NOTES COMPUT SC, V2688, P10
   Grozavu N, 2010, STUD COMPUT INTELL, V292, P133
   He P, 2012, LECT NOTES COMPUT SC, V7585, P223, DOI 10.1007/978-3-642-33885-4_23
   Huang HC, 2012, IEEE T FUZZY SYST, V20, P120, DOI 10.1109/TFUZZ.2011.2170175
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Lanckriet GRG, 2004, J MACH LEARN RES, V5, P27
   Lau KW, 2006, NEUROCOMPUTING, V69, P2033, DOI 10.1016/j.neucom.2005.10.003
   Law MHC, 2004, IEEE T PATTERN ANAL, V26, P1154, DOI 10.1109/TPAMI.2004.71
   Li JX, 2012, IEEE T MULTIMEDIA, V14, P471, DOI 10.1109/TMM.2011.2181151
   Li T, 2009, IEEE T MULTIMEDIA, V11, P477, DOI 10.1109/TMM.2009.2012942
   Lin YY, 2011, IEEE T PATTERN ANAL, V33, P1147, DOI 10.1109/TPAMI.2010.183
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Mirzaei Hamidreza, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P1007, DOI 10.1109/ICPR.2010.252
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Omran M, 2005, INT J PATTERN RECOGN, V19, P297, DOI 10.1142/S0218001405004083
   Rakotomamonjy A, 2008, J MACH LEARN RES, V9, P2491
   Scholkopf B., 2002, Encyclopedia of Biostatistics
   Shechtman E., 2007, PROC C COMPUT VISION, P1
   Sim T, 2003, IEEE T PATTERN ANAL, V25, P1615, DOI 10.1109/TPAMI.2003.1251154
   Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531
   Strehl A., 2002, J. Machine Learning Res, V3, P583
   Tzortzis GF, 2010, IEEE T NEURAL NETWOR, V21, P1925, DOI 10.1109/TNN.2010.2081999
   Vesanto J, 2000, IEEE T NEURAL NETWOR, V11, P586, DOI 10.1109/72.846731
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Yan SC, 2007, IEEE T PATTERN ANAL, V29, P40, DOI 10.1109/TPAMI.2007.250598
   Yang Y, 2010, IEEE T IMAGE PROCESS, V19, P2761, DOI 10.1109/TIP.2010.2049235
   Zhao B, 2009, IEEE DATA MINING, P637, DOI 10.1109/ICDM.2009.37
NR 61
TC 10
Z9 10
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2014
VL 16
IS 8
BP 2229
EP 2241
DI 10.1109/TMM.2014.2359769
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AU4HL
UT WOS:000345571300013
DA 2024-07-18
ER

PT J
AU Bourtsoulatze, E
   Thomos, N
   Frossard, P
AF Bourtsoulatze, Eirina
   Thomos, Nikolaos
   Frossard, Pascal
TI Distributed Rate Allocation in Inter-Session Network Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Delay minimization; distributed rate allocation; inter-session network
   coding; multimedia communications; overlay networks
ID VIDEO
AB In this work, we propose a distributed rate allocation algorithm that minimizes the average decoding delay for multimedia clients in inter-session network coding systems. We consider a scenario where the users are organized in a mesh network and each user requests the content of one of the available sources. We propose a novel distributed algorithm where network users determine the coding operations and the packet rates to be requested from the parent nodes, such that the decoding delay is minimized for all clients. A rate allocation problem is solved by every user, which seeks the rates that minimize the average decoding delay for its children and for itself. Since this optimization problem is a priori non-convex, we introduce the concept of equivalent packet flows, which permits to estimate the expected number of packets that every user needs to collect for decoding. We then decompose our original rate allocation problem into a set of convex subproblems, which are eventually combined to obtain an effective approximate solution to the delay minimization problem. The results demonstrate that the proposed scheme eliminates the bottlenecks and reduces the decoding delay experienced by users with limited bandwidth resources. We validate the performance of our distributed rate allocation algorithm in different video streaming scenarios using the NS-3 network simulator. We show that our system is able to take benefit of inter-session network coding for simultaneous delivery of video sessions in networks with path diversity.
C1 [Bourtsoulatze, Eirina] Univ Bern, Inst Comp Sci & Appl Math, Commun & Distributed Syst CDS Lab, CH-3012 Bern, Switzerland.
   [Thomos, Nikolaos] Univ Essex, Dept Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4 4, CH-1015 Lausanne, Switzerland.
C3 University of Bern; University of Essex; Swiss Federal Institutes of
   Technology Domain; Ecole Polytechnique Federale de Lausanne
RP Bourtsoulatze, E (corresponding author), Univ Bern, Inst Comp Sci & Appl Math, Commun & Distributed Syst CDS Lab, CH-3012 Bern, Switzerland.
EM bourtsoulatze@iam.unibe.ch; nthomos@essex.ac.uk; pascal.frossard@epfl.ch
RI Thomos, Nikolaos/AAU-2328-2020; Frossard, Pascal/AAF-2268-2019;
   Bourtsoulatze, Eirina/ABG-5003-2021
OI Thomos, Nikolaos/0000-0001-7266-2642
FU Swiss National Science Foundation [200021-118230, 200021-138083,
   PZ00P2-137275]; Swiss National Science Foundation (SNF) [200021-118230,
   200021_138083, PZ00P2_137275] Funding Source: Swiss National Science
   Foundation (SNF)
FX This work was supported by the Swiss National Science Foundation under
   Grant 200021-118230, Grant 200021-138083, and Grant PZ00P2-137275. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Feng Wu.
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   [Anonymous], P 41 ALL C COMM CONT
   [Anonymous], DISTRIBUTED RATE ALL
   [Anonymous], 2008, Network Coding: An Introduction
   [Anonymous], P 41 ALL C COMM CONT
   [Anonymous], IEEE INT S NETW COD
   Bourtsoulatze E., 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P89, DOI 10.1109/PV.2012.6229757
   Bourtsoulatze E., IEEE T COMM IN PRESS
   Boyd S., 2004, CONVEX OPTIMIZATION
   Costa Rui A., 2008, 2008 5th IEEE International Conference on Mobile Ad Hoc and Sensor Systems (MASS), P80, DOI 10.1109/MAHSS.2008.4660042
   Das A, 2010, IEEE INT SYMP INFO, P1878, DOI 10.1109/ISIT.2010.5513311
   Dougherty R, 2005, IEEE T INFORM THEORY, V51, P2745, DOI 10.1109/TIT.2005.851744
   Eryilmaz A, 2011, IEEE T INFORM THEORY, V57, P1092, DOI 10.1109/TIT.2010.2095110
   Grant M., 2020, CVX MATLAB SOFTWARE
   Heindlmaier M, 2011, IEEE ICC
   Katti S, 2008, IEEE ACM T NETWORK, V16, P497, DOI 10.1109/TNET.2008.923722
   Khreishah A., 2011, P GLOB TEL C GLOBECO, P1
   Khreishah A, 2010, IEEE ACM T NETWORK, V18, P816, DOI 10.1109/TNET.2009.2032353
   Khreishah A, 2009, IEEE J SEL AREA COMM, V27, P606, DOI 10.1109/JSAC.2009.090604
   Kim M, 2009, IEEE INFOCOM SER, P450, DOI 10.1109/INFCOM.2009.5061950
   Liu Zhong-Hua, 2011, International Journal of Neuropsychopharmacology, V14, P618, DOI 10.1017/S1461145710000520
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   ParandehGheibi A, 2011, IEEE J SEL AREA COMM, V29, P1064, DOI 10.1109/JSAC.2011.110516
   Rajawat K, 2011, IEEE T SIGNAL PROCES, V59, P6186, DOI 10.1109/TSP.2011.2165061
   Saltarin J., 2011, P IEEE INT MULT EXP, P1
   Seferoglu H., 2011, P IEEE INFOCOM
   Seferoglu H, 2010, IEEE ICC
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Thomos N, 2010, IEEE T CIRC SYST VID, V20, P1834, DOI 10.1109/TCSVT.2010.2087830
   Traskov D, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P1758, DOI 10.1109/ISIT.2006.261656
   Wang CC, 2010, IEEE T INFORM THEORY, V57, P3879, DOI 10.1109/TIT.2010.2050932
   Wang M, 2007, IEEE J SEL AREA COMM, V25, P1655, DOI 10.1109/JSAC.2007.071205
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 33
TC 8
Z9 8
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1752
EP 1765
DI 10.1109/TMM.2014.2328320
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200022
OA Green Submitted, Green Accepted
DA 2024-07-18
ER

PT J
AU Koloda, J
   Peinado, AM
   Sánchez, V
AF Koloda, Jan
   Peinado, Antonio M.
   Sanchez, Victoria
TI Kernel-Based MMSE Multimedia Signal Reconstruction and Its Application
   to Spatial Error Concealment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth estimation; kernel density estimation; multimedia signal
   reconstruction; spatial error concealment
ID DENSITY-ESTIMATION; ALGORITHM
AB This paper proposes a novel approach for multimedia signal reconstruction based on kernel density estimation (KDE). We make use of a vector formalism in which vectors consist of a first subvector containing a set of missing samples and a second one containing a set of available context samples. The missing subvector is reconstructed by a minimum mean square error estimator which employs a probability density function (pdf) obtained by KDE. As in any kernel-based method, the main issue to deal with is the estimation of an appropriate kernel bandwidth. We propose an adaptive procedure for bandwidth estimation (BE) especially conceived for signal reconstruction. Thus, unlike general KDE or kernel-based regression, which try to obtain a general fit, the focus of this BE procedure is on the specific missing subvector. Also, in order to exploit local signal correlations, our BE proposal adopts a scaling approach in which the bandwidth is computed as the local covariance matrix scaled by two factors. These two scale factors are obtained by minimization of two different approximations to the reconstruction error. The resulting reconstruction methodology is tested on a spatial error concealment (EC) application in which intracoded images have been transmitted through an error prone channel. The experimental results show the superiority of the proposed approach over a wide range of existing EC techniques.
C1 [Koloda, Jan] Univ Granada, Dept Signal Theory Networking & Commun, Res Grp Signal Proc Multimedia Transmiss & Speech, Granada, Spain.
   [Peinado, Antonio M.; Sanchez, Victoria] Univ Granada, Dept Signal Theory Networking & Commun, Granada, Spain.
C3 University of Granada; University of Granada
RP Koloda, J (corresponding author), Univ Granada, Dept Signal Theory Networking & Commun, Res Grp Signal Proc Multimedia Transmiss & Speech, Granada, Spain.
EM janko@ugr.es; amp@ugr.es; victoria@ugr.es
RI Peinado Herreros, Antonio Miguel/C-2401-2012
OI Peinado Herreros, Antonio Miguel/0000-0001-8214-6676; Sanchez Calle,
   Victoria Eugenia/0000-0003-1546-9728
FU Spanish MEC/FEDER [TEC 2010-18009]
FX This work was supported by the Spanish MEC/FEDER Project TEC 2010-18009.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Feng Wu.
CR [Anonymous], 1998, FUNDEMENTALS STAT SI
   [Anonymous], 2000, Smoothing and regression
   Asheri H, 2012, IEEE T CONSUM ELECTR, V58, P880, DOI 10.1109/TCE.2012.6311331
   Bors AG, 2009, IEEE T SYST MAN CY B, V39, P1543, DOI 10.1109/TSMCB.2009.2020688
   Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P281, DOI 10.1109/TPAMI.2003.1177159
   Duda R., 1973, Pattern Classification and Scene Analysis
   Elgammal A, 2002, P IEEE, V90, P1151, DOI 10.1109/JPROC.2002.801448
   Flåm JT, 2012, IEEE T SIGNAL PROCES, V60, P3840, DOI 10.1109/TSP.2012.2192112
   Gharavi H, 2008, INT CONF ACOUST SPEE, P1153, DOI 10.1109/ICASSP.2008.4517819
   Harrison P., 2005, THESIS MONASH U CLAY
   Katkovnik V, 1999, IEEE T SIGNAL PROCES, V47, P2567, DOI 10.1109/78.782208
   Koloda J, 2013, INT CONF ACOUST SPEE, P1330, DOI 10.1109/ICASSP.2013.6637867
   Koloda J, 2013, IEEE T MULTIMEDIA, V15, P957, DOI 10.1109/TMM.2013.2238524
   Koloda J, 2013, CIRC SYST SIGNAL PR, V32, P815, DOI 10.1007/s00034-012-9504-0
   Kristan M, 2011, PATTERN RECOGN, V44, P2630, DOI 10.1016/j.patcog.2011.03.019
   Li X, 2002, IEEE T CIRC SYST VID, V12, P857, DOI 10.1109/TCSVT.2002.804882
   MARRON JS, 1995, J AM STAT ASSOC, V90, P499, DOI 10.2307/2291060
   Persson D, 2008, IEEE T IMAGE PROCESS, V17, P145, DOI 10.1109/TIP.2007.914151
   Scott DW, 2015, WILEY SER PROBAB ST, P1
   Seiler J, 2008, INT CONF ACOUST SPEE, P781, DOI 10.1109/ICASSP.2008.4517726
   Sheather SJ, 2004, STAT SCI, V19, P588, DOI 10.1214/088342304000000297
   Shirani S., 1999, P ICIP, V6, P3117
   Silverman, 2018, DENSITY ESTIMATION S, DOI 10.1201/9781315140919
   SUN HF, 1995, IEEE T IMAGE PROCESS, V4, P470, DOI 10.1109/83.370675
   Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330
   WAND MP, 1993, J AM STAT ASSOC, V88, P520, DOI 10.2307/2290332
   Yang LJ, 1999, J ROY STAT SOC B, V61, P793, DOI 10.1111/1467-9868.00203
   Zhai GT, 2010, IEEE T CIRC SYST VID, V20, P1224, DOI 10.1109/TCSVT.2010.2057019
   Zhai GT, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P621, DOI 10.1109/ICME.2008.4607511
   Zhang KB, 2012, IEEE T IMAGE PROCESS, V21, P4544, DOI 10.1109/TIP.2012.2208977
   Zhang RF, 2004, IEEE T CONSUM ELECTR, V50, P335, DOI 10.1109/TCE.2004.1277882
   Zhao Y., 2005, P DICTA, P278
NR 32
TC 22
Z9 25
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2014
VL 16
IS 6
BP 1729
EP 1738
DI 10.1109/TMM.2014.2330314
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UW
UT WOS:000344720200020
DA 2024-07-18
ER

PT J
AU Chen, ZF
   Babacan, SD
   Molina, R
   Katsaggelos, AK
AF Chen, Zhaofu
   Babacan, S. Derin
   Molina, Rafael
   Katsaggelos, Aggelos K.
TI Variational Bayesian Methods For Multimedia Problems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayes methods; graphical models; multimedia signal processing;
   variational Bayes; inverse problems
ID LOW-RANK; APPROXIMATION; INFERENCE; MATRIX
AB In this paper we present an introduction to Variational Bayesian (VB) methods in the context of probabilistic graphical models, and discuss their application in multimedia related problems. VB is a family of deterministic probability distribution approximation procedures that offer distinct advantages over alternative approaches based on stochastic sampling and those providing only point estimates. VB inference is flexible to be applied in different practical problems, yet is broad enough to subsume as its special cases several alternative inference approaches including Maximum A Posteriori (MAP) and the Expectation-Maximization (EM) algorithm. In this paper we also show the connections between VB and other posterior approximation methods such as the marginalization-based Loopy Belief Propagation (LBP) and the Expectation Propagation (EP) algorithms. Specifically, both VB and EP are variational methods that minimize functionals based on the Kullback-Leibler (KL) divergence. LBP, traditionally developed using graphical models, can also be viewed as a VB inference procedure. We present several multimedia related applications illustrating the use and effectiveness of the VB algorithms discussed herein. We hope that by reading this tutorial the readers will obtain a general understanding of Bayesian methods and establish connections among popular algorithms used in practice.
C1 [Chen, Zhaofu; Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
   [Babacan, S. Derin] Google Inc, Mountain View, CA 94043 USA.
   [Molina, Rafael] Univ Granada, Dept Ciencias Comp, E-18071 Granada, Spain.
   [Molina, Rafael] Univ Granada, IAETS Ing Informat & Telecomunicac, E-18071 Granada, Spain.
C3 Northwestern University; Google Incorporated; University of Granada;
   University of Granada
RP Chen, ZF (corresponding author), Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60208 USA.
EM zhaofuchen2014@u.northwestern.edu; dbabacan@gmail.com;
   rms@decsai.ugr.es; aggk@eecs.northwestern.edu
RI Molina Soriano, Rafael/B-1849-2012; Katsaggelos, Aggelos K/B-7233-2009
OI Molina Soriano, Rafael/0000-0003-4694-8588; Katsaggelos, Aggelos
   K/0000-0003-4554-0070
FU Department of Energy [DE-NA0000457]; Spanish Ministry of Economy and
   Competitiveness [TIN2010-15137]; European Regional Development Fund
   (FEDER); CEI BioTic at the Universidad de Granada
FX This work was supported in part by a grant from the Department of Energy
   (DE-NA0000457), the Spanish Ministry of Economy and Competitiveness
   under project TIN2010-15137, the European Regional Development Fund
   (FEDER), and the CEI BioTic at the Universidad de Granada. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR [Anonymous], P IM VIS COMP NZ 200
   [Anonymous], 2012, LECT NOTES COMPUT SC
   [Anonymous], P IEEE INT GEOSC REM
   [Anonymous], 2008, Advances in Neural Information Processing Systems
   [Anonymous], 2013, THESIS
   [Anonymous], 2004, THESIS CITESEER
   [Anonymous], THESIS
   [Anonymous], 2012, Computer vision: models, learning, and inference
   [Anonymous], 2001, A family of algorithms for approximate Bayesian inference
   [Anonymous], 2012, MACHINE LEARNING PRO
   Babacan SD, 2012, IEEE T SIGNAL PROCES, V60, P3964, DOI 10.1109/TSP.2012.2197748
   Babacan SD, 2009, IEEE T IMAGE PROCESS, V18, P12, DOI 10.1109/TIP.2008.2007354
   Barber D., 2012, Bayesian reasoning and machine learning
   Barzigar N, 2012, CONF REC ASILOMAR C, P1684, DOI 10.1109/ACSSC.2012.6489319
   Bazi Y, 2010, IEEE T GEOSCI REMOTE, V48, P186, DOI 10.1109/TGRS.2009.2023983
   Beal M. J., 2003, THESIS
   Bishop C, 2007, RECOGNITION PATTERN
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen P, 2004, IEEE T PATTERN ANAL, V26, P1051, DOI 10.1109/TPAMI.2004.52
   Wang C, 2009, PROC CVPR IEEE, P1903, DOI [10.1109/CVPR.2009.5206800, 10.1109/CVPRW.2009.5206800]
   Dao MD, 2010, CONF REC ASILOMAR C, P758, DOI 10.1109/ACSSC.2010.5757666
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479
   Fox CW, 2012, ARTIF INTELL REV, V38, P85, DOI 10.1007/s10462-011-9236-8
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Goodall T, 2012, S APP ACC HIGH-PERF, P121, DOI 10.1109/SAAHPC.2012.16
   Grimmer J, 2010, POLIT ANAL, V18, P1, DOI [10.1093/pan/mpp034, 10.1093/pan/mpq027]
   Haldar JP, 2010, I S BIOMED IMAGING, P716, DOI 10.1109/ISBI.2010.5490076
   Huang PS, 2012, INT CONF ACOUST SPEE, P57, DOI 10.1109/ICASSP.2012.6287816
   Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310
   Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849
   Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105
   Kabán A, 2008, LECT NOTES ARTIF INT, V5211, P580, DOI 10.1007/978-3-540-87479-9_56
   Koller D., 2009, Probabilistic graphical models: principles and techniques
   Li X, 2009, IEEE T CIRC SYST VID, V19, P27, DOI 10.1109/TCSVT.2008.2005805
   Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436
   Liu ZQ, 2007, STAT APPL GENET MOL, V6, DOI 10.2202/1544-6115.1248
   MacKay D.J. C., 2007, Information Theory, Inference, and Learning Algorithms
   Minka T., 2001, P 17 C UNC ART INT, P362
   Nefian AV, 2002, EURASIP J APPL SIG P, V2002, P1274, DOI 10.1155/S1110865702206083
   Oh S., 2010, 2010 IEEE Information Theory Workshop on Information Theory (ITW 2010, Cairo), P1, DOI [10.1109/WD.2010.5657708, DOI 10.1109/WD.2010.5657708]
   Palmer J., 2006, Advances in Neural Information Processing Systems 18 [Neural Information Processing Systems, NIPS 2005, December 5-8, 2005, Vancouver, British Columbia, Canada], P1059
   Papadimitriou CH, 2000, J COMPUT SYST SCI, V61, P217, DOI 10.1006/jcss.2000.1711
   Parisi G., 1988, STAT FIELD THEORY, DOI DOI 10.1063/1.2811677
   Patcha A, 2007, COMPUT NETW, V51, P3448, DOI 10.1016/j.comnet.2007.02.001
   Pearl J., 1988, PROBABILISTIC REASON
   Peng YG, 2010, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2010.5540138
   Penny W, 2003, NEUROIMAGE, V19, P727, DOI 10.1016/S1053-8119(03)00071-5
   Piccardi M, 2004, IEEE SYS MAN CYBERN, P3099, DOI 10.1109/ICSMC.2004.1400815
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Pruteanu-Malinici I, 2008, IEEE T IMAGE PROCESS, V17, P811, DOI 10.1109/TIP.2008.919359
   Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1
   Rockafellar R. T., 2015, CONVEX ANAL, DOI DOI 10.1515/9781400873173
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Tam Y.-C., 2005, Proceedings of INTERSPEECH, P5
   Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236
   Tzikas DG, 2008, IEEE SIGNAL PROC MAG, V25, P131, DOI 10.1109/MSP.2008.929620
   Wagner A, 2012, IEEE T PATTERN ANAL, V34, P372, DOI 10.1109/TPAMI.2011.112
   Wai-tian Tan, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P3225, DOI 10.1109/ICIP.2011.6116356
   Wang J, 2011, PALGR MAC SER GLOB, P1
   Wang XY, 2012, IEEE IMAGE PROC, P897, DOI 10.1109/ICIP.2012.6467005
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87
   Watanabe S, 2004, IEEE T SPEECH AUDI P, V12, P365, DOI 10.1109/TSA.2004.828640
   Zeng J, 2013, IEEE T PATTERN ANAL, V35, P1121, DOI 10.1109/TPAMI.2012.185
   Zhaofu Chen, 2013, Journal of Communications, V8, P600
NR 66
TC 21
Z9 22
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1000
EP 1017
DI 10.1109/TMM.2014.2307692
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800010
DA 2024-07-18
ER

PT J
AU Wen, YG
   Zhu, XQ
   Rodrigues, JJPC
   Chen, CW
AF Wen, Yonggang
   Zhu, Xiaoqing
   Rodrigues, Joel J. P. C.
   Chen, Chang Wen
TI Cloud Mobile Media: Reflections and Outlook
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud Computing; Cloud Media; Mobile Media; Content Distribution
   Network; Quality of Experience; Cloud-Centric Media Network and Media
   Analytics
ID CONTENT DELIVERY
AB This paper surveys the emerging paradigm of cloud mobile media. We start with two alternative perspectives for cloud mobile media networks: an end-to-end view and a layered view. Summaries of existing research in this area are organized according to the layered service framework: i) cloud resource management and control in infrastructure-as-a-service (IaaS), ii) cloud-based media services in platform-as-a-service (PaaS), and iii) novel cloud-based systems and applications in software-as-a-service (SaaS). We further substantiate our proposed design principles for cloud-based mobile media using a concrete case study: a cloud-centric media platform (CCMP) developed at Nanyang Technological University. Finally, this paper concludes with an outlook of open research problems for realizing the vision of cloud-based mobile media.
C1 [Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Zhu, Xiaoqing] Cisco Syst Inc, Adv Architecture & Res, San Jose, CA 95134 USA.
   [Rodrigues, Joel J. P. C.] Univ Beira Interior, Inst Telecomunicacoes, Covilha, Portugal.
   [Chen, Chang Wen] SUNY Buffalo, Buffalo, NY 14260 USA.
C3 Nanyang Technological University; Cisco Systems Inc; Universidade da
   Beira Interior; State University of New York (SUNY) System; State
   University of New York (SUNY) Buffalo
RP Wen, YG (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM ygwen@ntu.edu.sg; zhuxq@alumni.stanford.edu; joeljr@ieee.org;
   chencw@buffalo.edu
RI Wen, Yonggang/P-9406-2017; Rodrigues, Joel J. P. C./A-8103-2013; Wen,
   Yonggang/B-8848-2011
OI Wen, Yonggang/0000-0002-2751-5114; Rodrigues, Joel J. P.
   C./0000-0001-8657-3800; Chen, Chang Wen/0000-0002-6720-234X
FU NTU Start-up Grant; MOE Tier-1 Grant [RG 31/11]; Singapore EMA;
   Singapore National Research Foundation under is IDM Futures Funding
   Initiative; InstitutodeTelecomunicacoes, Next Generation Networks and
   Applications Group (NetGNA), Covilha Delegation; FCT -
   FundacaoparaaCienciaeTecnologia [Pest-OE/EEI/LA0008/2013]; AAL4ALL
   (Ambient Assisted Living for All); COMPETE under FEDER via QREN
   Programme; US NSF [0915842, 0964797]; Microsoft Research; Huawei
   Technologies, Inc.; Div Of Electrical, Commun & Cyber Sys; Directorate
   For Engineering [0915842] Funding Source: National Science Foundation;
   Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [0964797] Funding Source: National Science Foundation
FX The work of Y. G. Wen was supported by NTU Start-up Grant, MOE Tier-1
   Grant (RG 31/11), EIRP02 Grant from Singapore EMA, and the Singapore
   National Research Foundation under is IDM Futures Funding Initiative and
   administered by the Interactive & Digital Media Programme Office, Media
   Development Authority. The work of J. Rodrigues was supported in part by
   InstitutodeTelecomunicacoes, Next Generation Networks and Applications
   Group (NetGNA), Covilha Delegation, by National Funding from the FCT -
   FundacaoparaaCienciaeTecnologia through the Pest-OE/EEI/LA0008/2013
   Project, and by the AAL4ALL (Ambient Assisted Living for All), project
   co-funded by COMPETE under FEDER via QREN Programme. The work of C. W.
   Chen was supported in part by US NSF Grants 0915842 and 0964797, the
   Gift Grants from Microsoft Research and Huawei Technologies, Inc. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Charles D. (Chuck) Creusere.
CR Ahlehagh H., 2012, IEEE International Conference on Communications (ICC 2012), P7082, DOI 10.1109/ICC.2012.6364966
   Ali T, 2010, P 5 INT C FUT INF TE, P1
   [Anonymous], P IEEE 4 INT C CLOUD
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], 2011, P IEEE GLOB TEL C HO
   [Anonymous], P ACM MULT MM 11 SCO
   [Anonymous], P IEEE INT C CONS EL
   [Anonymous], P IEEE INT PACK VID
   [Anonymous], 2011, EXTRACTING VALUE CHA
   [Anonymous], 2011, P 20 INT C COMP COMM
   [Anonymous], P ACM CFI 11 SEOUL K
   [Anonymous], P NOSSDAV 11 VANC BC
   [Anonymous], P ACM MCS 10 SAN FRA
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], 2013, GLOB MOB DAT TRAFF F
   [Anonymous], P TEL POL RES C
   [Anonymous], P 6 IEEE INT S SERV
   [Anonymous], P IEEE C COMM ENT CO
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], 2011, NIST DEFINITION CLOU
   [Anonymous], P IEEE INT PACK VID
   [Anonymous], P IFIP WMNC 11
   [Anonymous], FUJITSU SCI TECH J
   [Anonymous], 2001, Dynamic Programming and Optimal Control
   [Anonymous], NEW MULTISCREEN WORL
   [Anonymous], IEEE T MULT IN PRESS
   [Anonymous], P 2 IEEE WORKSH MULT
   [Anonymous], P ACM ACE 11 KARLSR
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], P 2012 IEEE INT C CO
   [Anonymous], P IEEE INF 2012 MAR
   [Anonymous], P IEEE INT WORKSH MU
   [Anonymous], P IEEE WCSP 11
   [Anonymous], P 3 INT C INT ICONI
   [Anonymous], IEEE PERVASIVE COMPU
   [Anonymous], 2012, SOFTWARE DEFINED NET
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], P IEEE INT C COMP CO
   [Anonymous], P 5 INT C NEXT GEN M
   [Anonymous], COMPUT ENTERTAIN
   [Anonymous], 2010, P 2010 ACM MULT WORK
   [Anonymous], NEW FRONTIERS INFORM
   [Anonymous], P EUROSYS 11 SALZB A
   [Anonymous], IEEE T CONSUM ELECT
   [Anonymous], P ACM MCMC 10 FIR IT
   [Anonymous], MOBILE NETW APPL
   [Anonymous], P IEEE 1 PERCOM WORK
   [Anonymous], P ACM MUM 10 LIM CYP
   [Anonymous], 2009, 2009 AS COMM PHOT C
   [Anonymous], P IEEE 3 INT C CLOUD
   [Anonymous], P 9 WORKSH LARG SCAL
   [Anonymous], P ACM MCMC 10 FIR IT
   [Anonymous], 2010, NDN0001 XER PAL ALT
   Bailloeul T., 2008, Proc. Multimedia Information Retrieval, P75, DOI DOI 10.1145/1460096.1460110
   Beloglazov A, 2011, ADV COMPUT, V82, P47, DOI 10.1016/B978-0-12-385512-1.00003-7
   Bertsekas DP., 1996, NEURO DYNAMIC PROGRA
   Boyd S., 2004, CONVEX OPTIMIZATION
   Daniels J., 2009, XRDS, Crossroads, ACM Mag. Students, V16, P8, DOI [10.1145/1618588, DOI 10.1145/1618588, DOI 10.1145/1618588.1618592]
   Dey S., 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P929, DOI 10.1109/ICCNC.2012.6167561
   Endo P.T., 2010, 8th Workshop on Cloud and Grid Applications, P3
   Espeland H., 2011, Proceedings of the 2011 International Conference on Parallel Processing Workshops (ICPPW 2011), P416, DOI 10.1109/ICPPW.2011.22
   Fernando N, 2013, FUTURE GENER COMP SY, V29, P84, DOI 10.1016/j.future.2012.05.023
   Ferretti S., 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P548, DOI 10.1109/CLOUD.2010.16
   Garcia A, 2011, IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE 2011), P379, DOI 10.1109/ICCE.2011.5722637
   Giro X, 2010, ACM INT C IM VID RET, P358, DOI DOI 10.1145/1816041.1816093
   Girod B, 2002, WIREL COMMUN MOB COM, V2, P573, DOI 10.1002/wcm.87
   Hai Anh Tran, 2011, Proceedings of the 2011 IEEE International Conference on Network/Cloud Computing and Applications (NCCA 2011), P14, DOI 10.1109/NCCA.2011.10
   Hoelzle Urs., 2009, The Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale Machines, V1st
   Hongbin Liang, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P191, DOI 10.1109/INFCOMW.2011.5928806
   Huang TC, 2011, INT C PAR DISTRIB SY, P1, DOI 10.1109/ICPADS.2011.61
   Isard M., 2007, Operating Systems Review, V41, P59, DOI 10.1145/1272998.1273005
   Jiann-Liang Chen, 2011, 2011 7th International Wireless Communications and Mobile Computing Conference (IWCMC 2011), P1463, DOI 10.1109/IWCMC.2011.5982754
   Klein Andreas, 2010, Proceedings 11th International Conference on Mobile Data Management (MDM 2010), P387, DOI 10.1109/MDM.2010.79
   Lämmel R, 2008, SCI COMPUT PROGRAM, V70, P1, DOI 10.1016/j.scico.2007.07.001
   Lihua Zheng, 2011, Proceedings of the 2011 International Conference on Cloud and Service Computing (CSC 2011), P131, DOI 10.1109/CSC.2011.6138510
   Liu SJ, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2348816.2348822
   Loui Alexander., 2007, MIR 07, P245
   Miao Dan., 2011, Proceedings of the 19th ACM international conference on Multimedia, P1237
   Minqi Zhou, 2010, Proceedings 2010 Sixth International Conference on Semantics Knowledge and Grid (SKG 2010), P105, DOI 10.1109/SKG.2010.19
   Pallis G, 2006, COMMUN ACM, V49, P101, DOI 10.1145/1107458.1107462
   Pereira R., 2010, 2010 IEEE 3rd International Conference on Cloud Computing (CLOUD 2010), P482, DOI 10.1109/CLOUD.2010.73
   Pereira R, 2011, IEEE DATA COMPR CONF, P471, DOI 10.1109/DCC.2011.75
   Plass M.F., 2009, P 5 INT C EMERGING N, P1, DOI [10.1145/1658939.1658941, DOI 10.1145/1658939.1658941]
   Rimal Bhaskar Prasad, 2009, Proceedings of the 2009 Fifth International Joint Conference on INC, IMS and IDC, P44, DOI 10.1109/NCM.2009.218
   Sanaei Z, 2014, IEEE COMMUN SURV TUT, V16, P369, DOI 10.1109/SURV.2013.050113.00090
   Saranya S. M., 2011, 2011 International Conference on Recent Trends in Information Technology (ICRTIT 2011), P673, DOI 10.1109/ICRTIT.2011.5972458
   Satyanarayanan M, 2009, IEEE PERVAS COMPUT, V8, P14, DOI 10.1109/MPRV.2009.82
   Sun J, 2011, IEEE INFOCOM SER, P381, DOI 10.1109/INFCOM.2011.5935187
   Verbelen T., 2012, P 3 ACM WORKSH MOB C, P29, DOI [10.1145/2307849.2307858, DOI 10.1145/2307849.2307858]
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang ZC, 2011, 2011 INTERNATIONAL CONFERENCE ON FUTURE COMPUTERS IN EDUCATION (ICFCE 2011), VOL III, P179
   Wei Pu, 2012, 2012 Proceedings of the 19th International Packet Video Workshop (PV 2012), P65, DOI 10.1109/PV.2012.6229745
   Wen YG, 2013, J INTERNET TECHNOL, V14, P353, DOI 10.6138/JIT.2013.14.3.01
   Wendell P., 2011, Proceedings of the 2011 ACM SIGCOMM conference on Internet measurement conference, P549
   Xiaoqing Zhu, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P949, DOI 10.1109/ICCNC.2012.6167565
   Xinwen Zhang, 2011, 2011 19th IEEE International Conference on Network Protocols, P1, DOI 10.1109/ICNP.2011.6089053
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   Zhang Y., 2012, 2012 ACM SIGSAC C CO, P305, DOI DOI 10.1145/2382196.2382230
   Zhou Minqi., 2010, Universal Communication Symposium (IUCS), 2010 4th International, P40
   Zhu WW, 2011, IEEE SIGNAL PROC MAG, V28, DOI 10.1109/MSP.2011.940269
NR 101
TC 123
Z9 131
U1 1
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 885
EP 902
DI 10.1109/TMM.2014.2315596
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800001
OA Bronze
DA 2024-07-18
ER

PT J
AU Chen, KT
   Chang, YC
   Hsu, HJ
   Chen, DY
   Huang, CY
   Hsu, CH
AF Chen, Kuan-Ta
   Chang, Yu-Chun
   Hsu, Hwai-Jung
   Chen, De-Yu
   Huang, Chun-Ying
   Hsu, Cheng-Hsin
TI On the Quality of Service of Cloud Gaming Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cloud gaming; live video streaming; measurement; performance evaluation;
   remote rendering
ID PERFORMANCE
AB Cloud gaming, i.e., real-time game playing via thin clients, relieves users from being forced to upgrade their computers and resolve the incompatibility issues between games and computers. As a result, cloud gaming is generating a great deal of interests among entrepreneurs, venture capitalists, general publics, and researchers. However, given the large design space, it is not yet known which cloud gaming system delivers the best user-perceived Quality of Service (QoS) and what design elements constitute a good cloud gaming system. This study is motivated by the question: How good is the QoS of current cloud gaming systems? Answering the question is challenging because most cloud gaming systems are proprietary and closed, and thus their internal mechanisms are not accessible for the research community. In this paper, we propose a suite of measurement techniques to evaluate the QoS of cloud gaming systems and prove the effectiveness of our schemes using a case study comprising two well-known cloud gaming systems: OnLive and StreamMyGame. Our results show that OnLive performs better, because it provides adaptable frame rates, better graphic quality, and shorter server processing delays, while consuming less network bandwidth. Our measurement techniques are general and can be applied to any cloud gaming systems, so that researchers, users, and service providers may systematically quantify the QoS of these systems. To the best of our knowledge, the proposed suite of measurement techniques have never been presented in the literature.
C1 [Chen, Kuan-Ta; Chang, Yu-Chun; Hsu, Hwai-Jung; Chen, De-Yu] Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
   [Huang, Chun-Ying] Natl Taiwan Ocean Univ, Dept Comp Sci, Keelung, Taiwan.
   [Hsu, Cheng-Hsin] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.
C3 Academia Sinica - Taiwan; National Taiwan Ocean University; National
   Tsing Hua University
RP Chen, KT (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.
EM swc@iis.sinica.edu.tw; congo.chang@gmail.com; hjhsu@iis.sinica.edu.tw;
   dychen0208@iis.sinica.edu.tw; chuang@ntou.edu.tw; chsu@cs.nthu.edu.tw
OI Huang, Chun-Ying/0000-0001-5503-9541
FU National Science Council of Taiwan [NSC100-2628-E-001-002-MY3,
   NSC102-2219-E-019-001, NSC102-2221-E-007-062-MY3]
FX This work was supported in part by the National Science Council of
   Taiwan under the grants NSC100-2628-E-001-002-MY3,
   NSC102-2219-E-019-001, and NSC102-2221-E-007-062-MY3. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Wenwu Zhu.
CR [Anonymous], P IEEE ACM NETGAMES
   [Anonymous], 2012, P 11 ANN WORKSH NETW
   Chang Y.-C., 2011, P IEEE CQR 2011 MAY
   Chen Kuan-Ta, ACM MM 11
   Claypool KT, 2007, MULTIMEDIA SYST, V13, P3, DOI 10.1007/s00530-007-0081-1
   Claypool M., 2012, P ACM WORKSH NETW SY, P1, DOI DOI 10.1109/NETGAMES.2012.6404013
   Claypool M, 2006, COMMUN ACM, V49, P40, DOI 10.1145/1167838.1167860
   Claypool Mark., 2009, Proceedings of the 4th International Conference on Foundations of Digital Games, P34, DOI DOI 10.1145/1536513.1536529
   De Winter D., 2006, P ACM NOSSDAV 2006
   Eisert P, 2008, IEEE IMAGE PROC, P2704, DOI 10.1109/ICIP.2008.4712352
   Holthe OI, 2009, 2009 6TH IEEE CONSUMER COMMUNICATIONS AND NETWORKING CONFERENCE, VOLS 1 AND 2, P758
   Huang C.-Y., 2013, P ACM MMSYS 2013 FEB
   Hunt G., 1999, Proceedings of the 3rd Conference on USENIX Windows NT Symposium, V3, P14
   Jurgelionis A, 2009, INT J COMPUT GAMES T, V2009, DOI 10.1155/2009/231863
   Lagar-Cavilla HA, 2007, LECT NOTES COMPUT SC, V4834, P143
   Lai AM, 2006, ACM T COMPUT SYST, V24, P175, DOI 10.1145/1132026.1132029
   Nave I., 2008, P IEEE INT S CONS EL
   Nieh J, 2003, ACM T COMPUT SYST, V21, P87, DOI 10.1145/592637.592640
   Packard K., 2003, P USENIX ANN TECHN C
   Perlman S. G., 2009, US Patent, Patent No. [2009/0119736A1, 20090119736]
   Richardson T, 1998, IEEE INTERNET COMPUT, V2, P33, DOI 10.1109/4236.656066
   Ross PE, 2009, IEEE SPECTRUM, V46, P14, DOI 10.1109/MSPEC.2009.4795441
   Tolia N, 2006, COMPUTER, V39, P46, DOI 10.1109/MC.2006.101
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong AYI, 1999, PROCEEDINGS OF THE 3RD USENIX WINDOWS NT SYMPOSIUM, P145
NR 25
TC 102
Z9 108
U1 0
U2 34
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2014
VL 16
IS 2
BP 480
EP 495
DI 10.1109/TMM.2013.2291532
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 297HF
UT WOS:000330245800016
DA 2024-07-18
ER

PT J
AU Afonso, MV
   Nascimento, JC
   Marques, JS
AF Afonso, Manya V.
   Nascimento, Jacinto C.
   Marques, Jorge S.
TI Automatic Estimation of Multiple Motion Fields From Video Sequences
   Using a Region Matching Based Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Region matching; trajectories; vector fields; video segmentation
ID SURVEILLANCE; TRACKING; OBJECTS
AB Estimation of velocity fields from a video sequence is an important step towards activity classification in a surveillance system. It has been recently shown that multiple motion fields estimated from trajectories are an efficient tool to describe the movement of objects, allowing an automatic classification of activities in the scene. However, the trajectory detection in noisy environments is difficult, usually requiring some sort manual editing to complete or correct them. This paper proposes two novel contributions. First, an automatic method for building pedestrian trajectories in far-field surveillance scenarios is presented not requiring user intervention. This basically comprises the detection of multiple moving objects in a video sequence through the detection of the active regions, followed by the estimation of the velocity fields that is accomplished by performing region matching of the above regions at consecutive time instants. This leads to a sequence of centroids and corresponding velocity vectors, describing the local motions presented in the image. A motion correspondence algorithm is then applied to group the centroids in a contiguous sequence of frames into trajectories corresponding to each moving object. The second contribution is a method for automatically finding the trajectories from a library of previously computed ones. Experiments on extensive video sequences from university campuses show that motion fields can be reliably estimated from these automatically detected trajectories, leading to a fully automatic procedure for the estimation of multiple motion fields.
C1 [Afonso, Manya V.; Nascimento, Jacinto C.; Marques, Jorge S.] Inst Super Tecn, Inst Sistemas & Robot, P-1049001 Lisbon, Portugal.
C3 Universidade de Lisboa
RP Afonso, MV (corresponding author), Inst Super Tecn, Inst Sistemas & Robot, P-1049001 Lisbon, Portugal.
EM mafonso@isr.ist.utl.pt; jan@isr.ist.utl.pt; jsm@isr.ist.utl.pt
RI Afonso, Manya/D-1149-2011; Nascimento, Jacinto/B-6128-2009; Marques,
   Jorge/C-1427-2010
OI Afonso, Manya/0000-0003-0389-0515; Nascimento,
   Jacinto/0000-0001-7468-5127; Marques, Jorge/0000-0002-3800-7756
FU FCT [PEst-OE/EEI/LA0009/2013, 'ARGUS'-PTDC/EEA-CRO/098550/2008]
FX This work was supported by FCT projects PEst-OE/EEI/LA0009/2013, and
   project 'ARGUS'-PTDC/EEA-CRO/098550/2008. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Feng Wu.
CR ADELSON EH, 1982, NATURE, V300, P523, DOI 10.1038/300523a0
   [Anonymous], CYTOMETRY A
   [Anonymous], P IEEE 8 WORKSH IM M
   [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   [Anonymous], P INT C PATT REC APP
   [Anonymous], CMURITR0012
   [Anonymous], P 3 IEEE INT WORKSH
   [Anonymous], P IEEE INT C MULT EX
   [Anonymous], P IEEE WORKSH PERF E
   [Anonymous], P AM NUCL SOC ANS 8
   [Anonymous], P IEEE WORKSH PERF E
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Boult TE, 2001, P IEEE, V89, P1382, DOI 10.1109/5.959337
   Brendel W, 2009, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2009.5459242
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Fauqueur J., 2007, 2007 IEEE 11th International Conference on Computer Vision, P1
   Gonzalez R. C., 2006, Digital Image Processing, V3rd
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   Haritaoglu I, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P222, DOI 10.1109/AFGR.1998.670952
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   KOLLER D, 1994, INT C PATT RECOG, P126, DOI 10.1109/ICPR.1994.576243
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Lin Z, 2010, IEEE T PATTERN ANAL, V32, P604, DOI 10.1109/TPAMI.2009.204
   Lucas Bruce D., ITERATIVE IMAGE REGI, P674, DOI DOI 10.1109/HPDC.2004.1323531
   McKenna SJ, 1999, IMAGE VISION COMPUT, V17, P225, DOI 10.1016/S0262-8856(98)00104-8
   Monnet A, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1305
   Nascimento JC, 2013, IEEE T IMAGE PROCESS, V22, P2066, DOI 10.1109/TIP.2013.2244607
   Nascimento JC, 2013, IEEE T IMAGE PROCESS, V22, P1712, DOI 10.1109/TIP.2012.2226899
   Nascimento JC, 2012, IEEE IMAGE PROC, P761, DOI 10.1109/ICIP.2012.6466971
   Nascimento JC, 2010, IEEE T IMAGE PROCESS, V19, P1338, DOI 10.1109/TIP.2009.2039664
   Ohta N, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P481, DOI 10.1109/ICCV.2001.937664
   Sand Peter, 2008, International Journal of Computer Vision, V80, P72, DOI 10.1007/s11263-008-0136-6
   Seki M, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P207, DOI 10.1109/WACV.2000.895424
   Stauffer C, 2000, IEEE T PATTERN ANAL, V22, P747, DOI 10.1109/34.868677
   Sun HZ, 2000, INT C PATT RECOG, P843, DOI 10.1109/ICPR.2000.905544
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Veenman CJ, 2001, IEEE T PATTERN ANAL, V23, P54, DOI 10.1109/34.899946
   Wren CR, 1997, IEEE T PATTERN ANAL, V19, P780, DOI 10.1109/34.598236
   Zhong J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P44, DOI 10.1109/ICCV.2003.1238312
NR 41
TC 12
Z9 12
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 1
EP 14
DI 10.1109/TMM.2013.2281023
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100001
DA 2024-07-18
ER

PT J
AU Yu, J
   Rui, Y
   Chen, B
AF Yu, Jun
   Rui, Yong
   Chen, Bo
TI Exploiting Click Constraints and Multi-view Features for Image
   Re-ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hypergraph; image re-ranking; multi-view
ID DIMENSIONALITY REDUCTION; PAIRWISE CONSTRAINTS
AB Image re-ranking is effective in improving performance of text-based image searches. However, improvements from existing re-ranking algorithms are limited by two factors: one is that the associated textual information of images often mismatches their actual visual contents; the other is that a visual's features cannot accurately describe the semantic similarities between images. In this paper, we adopt click data to bridge the semantic gap. We propose a novel multi-view hypergraph-based learning (MHL) method that adaptively integrates click data with varied visual features. In particular, MHL considers pairwise discriminative constraints from click data to maximally distinguish images with high click counts from images with no click counts, and a semantic manifold is constructed. It then adopts hypergraph learning to build multiple manifolds from varied visual features. Finally, MHL integrates the semantic manifold with visual manifolds through an iterative optimization procedure. The weights of different manifolds and the re-ranking score are simultaneously obtained after using this optimization strategy. We conduct experiments on real world datasets and the results demonstrate that MHL outperforms state-of-the-art image re-ranking methods.
C1 [Yu, Jun] Xiamen Univ, Dept Comp Sci, Xiamen 361005, Peoples R China.
   [Rui, Yong; Chen, Bo] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Xiamen University; Microsoft Research Asia; Microsoft
RP Yu, J (corresponding author), Xiamen Univ, Dept Comp Sci, Xiamen 361005, Peoples R China.
EM zju.yujun@gmail.com; yongrui@microsoft.com; v-boch@microsoft.com
FU National Natural Science Foundation of China [61100104]; Program for New
   Century Excellent Talents in University [NCET-12-0323]; Natural Science
   Foundation of Fujian Province of China [2012J01287]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61100104, the Program for New Century
   Excellent Talents in University under Grant NCET-12-0323, and the
   Natural Science Foundation of Fujian Province of China under Grant
   2012J01287. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Cees Snoek..
CR [Anonymous], P NIPS
   [Anonymous], P WEBSCI 09 SOC ON L
   Baghshah MS, 2009, INTELL DATA ANAL, V13, P887, DOI 10.3233/IDA-2009-0399
   Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Carterette Ben, 2007, ADV NEURAL INFORM PR, P217
   Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939
   Craswell Nick, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P239, DOI 10.1145/1277741.1277784
   Dou Z., 2008, P 17 ACM C INFORM KN, P73
   Dupret G., 2010, Proceedings of the third ACM international conference on Web search and data mining, WSDM '10, P181
   Gao SH, 2013, IEEE T PATTERN ANAL, V35, P92, DOI 10.1109/TPAMI.2012.63
   Geng B, 2010, PROC CVPR IEEE, P3400, DOI 10.1109/CVPR.2010.5540003
   He Xiaofei., 2004, ACM MULTIMEDIA, P17
   Jain V., 2011, International Conference on World Wide Web, P277, DOI DOI 10.1145/1963405.1963447
   Järvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418
   Jing Y, 2008, IEEE T PATTERN ANAL, V30, P1877, DOI 10.1109/TPAMI.2008.121
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Krapac J, 2010, PROC CVPR IEEE, P1094, DOI 10.1109/CVPR.2010.5540092
   Li W, 2012, IEEE DATA MINING, P419, DOI 10.1109/ICDM.2012.78
   Liu Y, 2009, IEEE T CIRC SYST VID, V19, P1841, DOI 10.1109/TCSVT.2009.2026951
   Luo Y, 2013, IEEE T NEUR NET LEAR, V24, P709, DOI 10.1109/TNNLS.2013.2238682
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   Shokouhi M, 2008, LECT NOTES COMPUT SC, V4956, P591
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Tao DC, 2009, IEEE T PATTERN ANAL, V31, P260, DOI 10.1109/TPAMI.2008.70
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016
   Wang M., 2007, ACM Multi- media, P862
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Ye J., 2009, P 18 ACM C INF KNOWL, P2061, DOI [10.1145/1645953.1646301, DOI 10.1145/1645953.1646301]
   Yu J, 2013, PATTERN RECOGN, V46, P483, DOI 10.1016/j.patcog.2012.08.006
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P4636, DOI 10.1109/TIP.2012.2207395
   Yu J, 2012, IEEE T SYST MAN CY B, V42, P1413, DOI 10.1109/TSMCB.2012.2192108
   Yu J, 2012, IEEE T IMAGE PROCESS, V21, P3262, DOI 10.1109/TIP.2012.2190083
   Yu J, 2011, IEEE T IMAGE PROCESS, V20, P3257, DOI 10.1109/TIP.2011.2158225
   Zhang TH, 2009, IEEE T KNOWL DATA EN, V21, P1299, DOI 10.1109/TKDE.2008.212
   Zhou Dengyong, 2006, 19 INT C NEURAL INFO, V19, P1601
   Zitouni H., 2008, 19th International Conference on Pattern Recognition, P1
NR 37
TC 147
Z9 155
U1 0
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 159
EP 168
DI 10.1109/TMM.2013.2284755
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100014
DA 2024-07-18
ER

PT J
AU Lee, Z
   Juang, J
   Nguyen, TQ
AF Lee, Zucheul
   Juang, Jason
   Nguyen, Truong Q.
TI Local Disparity Estimation With Three-Moded Cross Census and Advanced
   Support Weight
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Census transform; disparity estimation; motion flow; spatial temporal
   consistency.
ID STEREO
AB The classical local disparity methods use simple and efficient structure to reduce the computation complexity. To increase the accuracy of the disparity map, new local methods utilize additional processing steps such as iteration, segmentation, calibration and propagation, similar to global methods. In this paper, we present an efficient one-pass local method with no iteration. The proposed method is also extended to video disparity estimation by using motion information as well as imposing spatial temporal consistency. In local method, the accuracy of stereo matching depends on precise similarity measure and proper support window. For the accuracy of similarity measure, we propose a novel three-moded cross census transform with a noise buffer, which increases the robustness to image noise in flat areas. The proposed similarity measure can be used in the same form in both stereo images and videos. We further improve the reliability of the aggregation by adopting the advanced support weight and incorporating motion flow to achieve better depth map near moving edges in video scene. The experimental results show that the proposed method is the best performing local method on the Middlebury stereo benchmark test and outperforms the other state-of-the-art methods on video disparity evaluation.
C1 [Lee, Zucheul; Juang, Jason; Nguyen, Truong Q.] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Lee, Z (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM z1lee@ucsd.edu; jajuang@ucsd.edu; tqn001@ucsd.edu
RI Nguyen, Truong/JXN-9786-2024
FU NSF [CCF1065305]; Intel/CISCO under the VAWN program; Technology
   Development Program for Commercializing System Semiconductor; Ministry
   of Knowledge Economy (MKE, Korea) [10041126]
FX This work was supported in part by NSF grant CCF1065305, by Intel/CISCO
   under the VAWN program and by the Technology Development Program for
   Commercializing System Semiconductor funded by the Ministry of Knowledge
   Economy (MKE, Korea). (No. 10041126, Title: International Collaborative
   R&BD Project for System Semiconductor).
CR Angens D., 2008, From Gestalt theory to image analysis : a Probabilistic approach
   [Anonymous], 2006, 2006 IEEE COMP SOC C, DOI DOI 10.1109/CVPR.2006.207
   Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1
   Bleyer Michael, 2009, 2009 Proceedings of 6th International Symposium on Image and Signal Processing and Analysis, P383
   Connolly C, 1997, IEEE T IMAGE PROCESS, V6, P1046, DOI 10.1109/83.597279
   Fusiello A, 1997, PROC CVPR IEEE, P858, DOI 10.1109/CVPR.1997.609428
   Gong ML, 2007, INT J COMPUT VISION, V75, P283, DOI 10.1007/s11263-006-0032-x
   Gu Z, 2008, PATTERN RECOGN LETT, V29, P1230, DOI 10.1016/j.patrec.2008.01.032
   Hirschmüller H, 2009, IEEE T PATTERN ANAL, V31, P1582, DOI 10.1109/TPAMI.2008.221
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hosni A, 2009, IEEE IMAGE PROC, P2093, DOI 10.1109/ICIP.2009.5414478
   KANADE T, 1994, IEEE T PATTERN ANAL, V16, P920, DOI 10.1109/34.310690
   Khoshabeh R, 2011, INT CONF ACOUST SPEE, P885
   Lee Z, 2012, EUR SIGNAL PR CONF, P1114
   Mei X, 2011, PROC CVPR IEEE, P1257
   Papari G, 2008, IEEE T IMAGE PROCESS, V17, P1950, DOI 10.1109/TIP.2008.2002306
   Rhemann C., 2011, P BMVC
   Rhemann C, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995372
   Richardt C., 2010, P ECCV
   Scharstein D., 2010, Middlebury stereo evaluation version 2
   Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939
   Tao H, 2000, FIFTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION, PROCEEDINGS, P246, DOI 10.1109/WACV.2000.895429
   Tombari F, 2007, LECT NOTES COMPUT SC, V4872, P427
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhang K, 2009, IEEE T CIRC SYST VID, V19, P1073, DOI 10.1109/TCSVT.2009.2020478
   Zhang L, 2009, PROC CVPR IEEE, P1542, DOI 10.1109/CVPRW.2009.5206836
NR 26
TC 44
Z9 52
U1 1
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1855
EP 1864
DI 10.1109/TMM.2013.2270456
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900011
DA 2024-07-18
ER

PT J
AU Lin, YH
   Chang, YP
   Wu, JL
AF Lin, Yu-Hsun
   Chang, Yu-Pei
   Wu, Ja-Ling
TI Appearance-Based QR Code Beautifier
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Aesthetic; mobile; QR code; Reed-Solomon codes; saliency; simulated
   annealing optimization
AB Quick Response (QR) code is a widely used matrix bar code with the increasing population of smart phones. However, QR code usually consists of random textures which are not suitable for incorporating with other visual designs (e.g. name card and business advertisement poster). In order to overcome the shortcomings of noise-like looks of QR codes, we propose a systematic QR code beautification framework where the visual appearance of QR code is composed of visually meaningful patterns selected by users, and more importantly, the correctness of message decoding is kept intact. Our work makes QR code from machine decodable only (i.e. standardized random texture) to a personalized form with human visual pleasing appearance. We expect the proposed QR code beautifier can inspire more visual-pleasant mobile multimedia applications.
C1 [Lin, Yu-Hsun] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
   [Chang, Yu-Pei] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Wu, Ja-Ling] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Commun & Multimedia Lab, Taipei 106, Taiwan.
C3 National Taiwan University; National Taiwan University; National Taiwan
   University
RP Lin, YH (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
RI Lin, Yu-Hsun/HTN-0354-2023
OI Lin, Yu-Hsun/0000-0002-6177-8639; WU, JA-LING/0000-0002-3631-1551
CR Anezaki T., 2011, 2011 17th Korea-Japan Joint Workshop on Frontiers of Computer Vision (FCV), P1
   [Anonymous], 2006, 180042006 ISOIEC
   [Anonymous], 2009, P 8 INT C VIRT REAL
   [Anonymous], 2010ACM INT C MULTIM
   Erol Berna., 2007, P 15 ACM INT C MULTI, P819, DOI DOI 10.1145/1291233.1291419
   Fujita K., 2011, P FOR INF TECHN SEP, P517
   Fujita K., 2011, IEICE TECH REPORT, V110, P39
   Nikolaos T., 2010, P ACM SIGGRAPH 2010, P144
   Ono S, 2008, ARTIF LIFE ROBOT, V13, P238, DOI 10.1007/s10015-008-0587-4
   Ono S, 2008, IEEE C EVOL COMPUTAT, P1068, DOI 10.1109/CEC.2008.4630929
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Samretwit D., 2011, Proceedings of the 2011 Third International Conference on Intelligent Networking and Collaborative Systems (INCoS 2011), P552, DOI 10.1109/INCoS.2011.117
   Satoru O., 2007, PROGRAM INFORM STORA
   Wakahara T., 2011, Proceedings of the 2011 14th International Conference on Network-Based Information Systems (NBiS 2011), P484, DOI 10.1109/NBiS.2011.80
   Wakahara Toshihiko, 2010, 2010 International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC 2010), P454, DOI 10.1109/3PGCIC.2010.77
NR 15
TC 42
Z9 49
U1 1
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2198
EP 2207
DI 10.1109/TMM.2013.2271745
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900038
DA 2024-07-18
ER

PT J
AU Emre, Y
   Chakrabarti, C
AF Emre, Yunus
   Chakrabarti, Chaitali
TI Energy and Quality-Aware Multimedia Signal Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error compensation; low-power; multimedia algorithms; variable
   precision; voltage scaling
ID PROCESS-VARIATION RESILIENT; LOW-POWER; MOTION ESTIMATION; INCREMENTAL
   REFINEMENT; DESIGN; ARCHITECTURE
AB This paper presents techniques to reduce energy with minimal degradation in system performance for multimedia signal processing algorithms. It first provides a survey of energy-saving techniques such as those based on voltage scaling, reducing number of computations and reducing dynamic range. While these techniques reduce energy, they also introduce errors that affect the performance quality. To compensate for these errors, techniques that exploit algorithm characteristics are presented. Next, several hybrid energy-saving techniques that further reduce the energy consumption with low performance degradation are presented. For instance, a combination of voltage scaling and dynamic range reduction is shown to achieve 85% energy saving in a low pass FIR filter for a fairly low noise level. A combination of computation reduction and dynamic reduction for Discrete Cosine Transform shows, on average, 33% to 46% reduction in energy consumption while incurring 0.5 dB to 1.5 dB loss in PSNR. Both of these techniques have very little overhead and achieve significant energy reduction with little quality degradation.
C1 [Emre, Yunus; Chakrabarti, Chaitali] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85281 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Emre, Y (corresponding author), Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85281 USA.
EM yemre@asu.edu; chaitali@asu.edu
FU NSF CSR [0910699]; Division Of Computer and Network Systems; Direct For
   Computer & Info Scie & Enginr [0910699] Funding Source: National Science
   Foundation
FX This work was supported in part by NSF CSR 0910699. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR Acharya T., 2004, JPEG2000 Standard for Image Compression: Concepts, Algorithms and VLSI Architectures
   Agarwal K, 2006, DES AUT CON, P57, DOI 10.1109/DAC.2006.229176
   Andreopoulos Y, 2008, IEEE T IMAGE PROCESS, V17, P1685, DOI 10.1109/TIP.2008.2001051
   Andreopoulos Y, 2008, IEEE T SIGNAL PROCES, V56, P140, DOI 10.1109/TSP.2007.906727
   Banerjee N, 2009, IEEE T COMPUT AID D, V28, P1127, DOI 10.1109/TCAD.2009.2022197
   Bowman KA, 2009, IEEE J SOLID-ST CIRC, V44, P49, DOI 10.1109/JSSC.2008.2007148
   CHANDRAKASAN AP, 1995, P IEEE, V83, P498, DOI 10.1109/5.371964
   Chang IJ, 2009, DES AUT CON, P670
   Chen CY, 2006, IEEE T CIRCUITS-I, V53, P578, DOI 10.1109/TCSI.2005.858488
   Chen GK, 2007, IEEE IC CAD, P660, DOI 10.1109/ICCAD.2007.4397341
   Chippa VK, 2010, DES AUT CON, P555
   Chishti Zeshan, 2009, Proceedings of the 2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO 2009), P89, DOI 10.1145/1669112.1669126
   Cho MK, 2009, ASIA S PACIF DES AUT, P823, DOI 10.1109/ASPDAC.2009.4796582
   Chowdhury M. R., 2009, P DES AUT TEST C EUR, P903
   Emre Yunus, 2010, Proceedings of the 2010 IEEE Workshop on Signal Processing Systems (SiPS 2010), P36, DOI 10.1109/SIPS.2010.5624759
   Emre Y, 2011, IEEE INT CONF ASAP, P176
   Emre Y, 2013, IEEE T VLSI SYST, V21, P159, DOI 10.1109/TVLSI.2011.2180407
   Emre Y, 2011, INT CONF ACOUST SPEE, P1589
   Ercegovac M. D., 2004, DIGITAL ARITHMETIC
   Ernst D, 2003, 36TH INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, PROCEEDINGS, P7
   George J., 2006, PROC INT C COMPILERS, P158, DOI DOI 10.1145/1176760.1176781
   Ghosh S, 2010, P IEEE, V98, P1718, DOI 10.1109/JPROC.2010.2057230
   Goel M, 1999, IEEE T SIGNAL PROCES, V47, P2821, DOI 10.1109/78.790662
   He ZL, 2000, IEEE T CIRC SYST VID, V10, P669, DOI 10.1109/76.856445
   Hegde R, 2001, IEEE T VLSI SYST, V9, P813, DOI 10.1109/92.974895
   Karakonstantis G, 2010, IEEE T VLSI SYST, V18, P1461, DOI 10.1109/TVLSI.2009.2025279
   Kim J, 2007, INT SYMP MICROARCH, P197, DOI 10.1109/MICRO.2007.19
   Kim SH, 2010, IEEE EMBED SYST LETT, V2, P77, DOI 10.1109/LES.2010.2060467
   Kulkarni P, 2011, J LOW POWER ELECTRON, V7, P490, DOI 10.1166/jolpe.2011.1157
   Kurdahi FJ, 2010, IEEE T VLSI SYST, V18, P852, DOI 10.1109/TVLSI.2009.2016665
   Lian CJ, 2007, IEEE CIRC SYST MAG, V7, P26, DOI 10.1109/MCAS.2007.4299440
   Lin K.-J., 1987, Proceedings of the Real-Time Systems Symposium (Cat. No.87CH2475-2), P210
   Liu Y, 2007, ISLPED'07: PROCEEDINGS OF THE 2007 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P250, DOI 10.1145/1283780.1283834
   Makhzan MA, 2009, IEEE T VLSI SYST, V17, P827, DOI 10.1109/TVLSI.2009.2016714
   Mohapatra D, 2009, I SYMPOS LOW POWER E, P195
   Mukhopadhyay S, 2005, IEEE T COMPUT AID D, V24, P1859, DOI 10.1109/TCAD.2005.852295
   Nawab SH, 1997, J VLSI SIG PROC SYST, V15, P177, DOI 10.1023/A:1007986707921
   Park J, 2010, IEEE T VLSI SYST, V18, P787, DOI 10.1109/TVLSI.2009.2016839
   Rao T.R. N., 1989, ERROR CONTROL CODING
   Sartori J, 2011, PROCEEDINGS OF THE PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON COMPILERS, ARCHITECTURES AND SYNTHESIS FOR EMBEDDED SYSTEMS (CASES '11), P115
   Shanbhag NR, 2010, DES AUT CON, P859
   Shim BY, 2004, IEEE T VLSI SYST, V12, P497, DOI 10.1109/TVLSI.2004.826201
   Shin D, 2010, DES AUT TEST EUROPE, P957
   Sinha A, 2002, IEEE T VLSI SYST, V10, P135, DOI 10.1109/92.994990
   Tong JYF, 2000, IEEE T VLSI SYST, V8, P273, DOI 10.1109/92.845894
   Varatkar GV, 2008, IEEE T VLSI SYST, V16, P1399, DOI 10.1109/TVLSI.2008.2000675
   Verma AK, 2008, DES AUT TEST EUROPE, P1092
   Zhang K, 2009, INTEGR CIRCUIT SYST, P1, DOI 10.1007/978-0-387-88497-4_1
   Zhu N, 2010, IEEE T VLSI SYST, V18, P1225, DOI 10.1109/TVLSI.2009.2020591
NR 49
TC 9
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1579
EP 1593
DI 10.1109/TMM.2013.2266094
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, MD
   Chen, ZZ
   Tan, YP
AF Li, Maodong
   Chen, Zhenzhong
   Tan, Yap-Peng
TI Scalable Resource Allocation for SVC Video Streaming Over Multiuser
   MIMO-OFDM Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiuser multiple-input multiple-output orthogonal; frequency-division
   multiplexing (MIMO-OFDM); resource allocation; scalable framework;
   scalable extension of H.264/AVC (SVC); scalable video; spatial division
   multiple access (SDMA)
ID ADAPTATION; EXTENSION
AB In this paper, we propose a scalable resource allocation framework for streaming scalable videos over multiuser multiple-input multiple-output orthogonal frequency-division multiplexing (MIMO-OFDM) networks. We exploit the utilities of scalable videos produced by the scalable extension of H.264/AVC (SVC) and investigate the multidimensional diversities of the multiuser MIMO-OFDM wireless networks. First, we study the rate-utility relationship of SVC via a packet prioritization scheme. Based on the rate-utility analysis, a scalable resource-allocation framework is proposed to achieve differentiated service objectives for different scalable video layers. To provide users with fair opportunities to acquire basic viewing experience, a fair scheme is designed to guarantee that each user is entitled to a MAXMIN fairness to have their base layer video packets received. After all users have their base layer packets successfully scheduled, resources are distributed to exploit the network efficiency. The two schemes are integrated into a unified bit loading and power allocation solution to enhance the practicability of the scalable framework. Experiment results confirms that the proposed scheme handles fairness and efficiency better at different scenarios than the conventional schemes.
C1 [Li, Maodong; Chen, Zhenzhong; Tan, Yap-Peng] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Li, MD (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
EM zzchen@ntu.edu.sg
RI Tan, Yap-Peng/A-5158-2011; Chen, Zhenzhong/C-2529-2015; 陈,
   震中/C-6857-2014
FU Agency for Science, Technology and Research (A*STAR), Singapore under
   the Mobile Media Thematic Strategic Research Programme of the Science
   and Engineering Research Council
FX This work was supported in part by a grant awarded The Agency for
   Science, Technology and Research (A*STAR), Singapore, under the Mobile
   Media Thematic Strategic Research Programme of the Science and
   Engineering Research Council. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Pal
   Halvorsen.
CR [Anonymous], 2007, N9195 JVT ISOIEC MPE
   [Anonymous], 1984, ACM Transaction on Computer Systems
   [Anonymous], 802162004 IEEE
   [Anonymous], 2008, Resource allocation for wireless networks: basics, techniques, and applications
   [Anonymous], 1998, INTEGER PROGRAMMING
   Bartolomé D, 2007, IEEE T COMMUN, V55, P1577, DOI 10.1109/TCOMM.2007.902567
   Cao ZR, 1999, IEEE INFOCOM SER, P793, DOI 10.1109/INFCOM.1999.751467
   Chung ST, 2001, IEEE T COMMUN, V49, P1561, DOI 10.1109/26.950343
   Fasano A, 2002, ISIT: 2002 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P243, DOI 10.1109/ISIT.2002.1023515
   Harks T., 2006, P IEEE INFOCOM, P1
   Hu DL, 2010, IEEE J SEL AREA COMM, V28, P434, DOI 10.1109/JSAC.2010.100414
   Huang JW, 2008, IEEE T CIRC SYST VID, V18, P582, DOI 10.1109/TCSVT.2008.919109
   Jang JH, 2003, IEEE J SEL AREA COMM, V21, P171, DOI 10.1109/JSAC.2002.807348
   Jiang M, 2007, P IEEE, V95, P1430, DOI 10.1109/JPROC.2007.898869
   Lee J, 2006, IEEE T COMMUN, V54, P1170, DOI 10.1109/TCOMM.2006.877955
   Li M., 2011, P IEEE RAD FREQ INT, P1, DOI DOI 10.1109/RFIC.2011.5940608
   Li M., P IEEE INT IN PRESS
   Li MD, 2011, IEEE INT SYMP CIRC S, P2645
   Luo ZQ, 2008, IEEE J-STSP, V2, P57, DOI 10.1109/JSTSP.2007.914876
   Maani E, 2009, IEEE T IMAGE PROCESS, V18, P2022, DOI 10.1109/TIP.2009.2023152
   Martello Silvano, 1990, Knapsack Problems: Algorithms and Computer Implementations
   Pahalawatta P, 2007, IEEE J SEL AREA COMM, V25, P749, DOI 10.1109/JSAC.2007.070511
   REICHEL J, 2007, N9212 ISOIEC MPEG IT
   Schuster GM, 1999, IEEE T MULTIMEDIA, V1, P3, DOI 10.1109/6046.748167
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Spencer QH, 2004, IEEE T SIGNAL PROCES, V52, P461, DOI 10.1109/TSP.2003.821107
   Stüber GL, 2004, P IEEE, V92, P271, DOI 10.1109/JPROC.2003.821912
   Su GM, 2006, IEEE T CIRC SYST VID, V16, P1217, DOI 10.1109/TCSVT.2006.883513
   Tsai CF, 2008, IEEE T WIREL COMMUN, V7, P1734, DOI 10.1109/TWC.2008.060994
   Yaghoobi H., 2004, INTEL TECHNOLOGY J, V8, P201
   Zhang YJ, 2005, IEEE T COMMUN, V53, P107, DOI 10.1109/TCOMM.2004.840666
NR 31
TC 26
Z9 28
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1519
EP 1531
DI 10.1109/TMM.2013.2267207
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800005
DA 2024-07-18
ER

PT J
AU Monteagudo-Pereira, JL
   Aulí-Llinàs, F
   Serra-Sagristà, J
AF Lino Monteagudo-Pereira, Jose
   Auli-Llinas, Francesc
   Serra-Sagrista, Joan
TI JPIP Proxy Server With Prefetching Strategies Based on User-Navigation
   Model and Semantic Map
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Interactive image transmission; JPEG2000; JPIP; prefetching strategies;
   semantic map; user-navigation model
ID JPEG2000 IMAGES; TRANSMISSION; REMOTE
AB The efficient transmission of large resolution images and, in particular, the interactive transmission of images in a client-server scenario, is an important aspect for many applications. Among the current image compression standards, JPEG2000 excels for its interactive transmission capabilities. In general, three mechanisms are employed to optimize the transmission of images when using the JPEG2000 Interactive Protocol (JPIP): 1) packet re-sequencing at the server; 2) prefetching at the client; and 3) proxy servers along the network infrastructure. To avoid the congestion of the network, prefetching mechanisms are not commonly employed when many clients within a local area network (LAN) browse images from a remote server. Aimed to maximize the responsiveness of all the clients within a LAN, this work proposes the use of prefetching strategies at the proxy server-rather than at the clients. The main insight behind the proposed prefetching strategies is a user-navigation model and a semantic map that predict the future requests of the clients. Experimental results indicate that the introduction of these strategies into a JPIP proxy server enhances the browsing experience of the end-users notably.
C1 [Lino Monteagudo-Pereira, Jose; Auli-Llinas, Francesc; Serra-Sagrista, Joan] Univ Autonoma Barcelona, Dept Informat & Commun Engn, E-08193 Barcelona, Spain.
C3 Autonomous University of Barcelona
RP Monteagudo-Pereira, JL (corresponding author), Univ Autonoma Barcelona, Dept Informat & Commun Engn, E-08193 Barcelona, Spain.
EM jlino@deic.uab.cat; fauli@deic.uab.cat; joan.serra@uab.cat
RI Serra-Sagristà, Joan/M-3284-2019; Serra-Sagrista, Joan/B-2000-2009;
   Auli-Llinas, Francesc/K-4395-2013
OI Serra-Sagristà, Joan/0000-0003-4729-9292; Serra-Sagrista,
   Joan/0000-0003-4729-9292; Auli-Llinas, Francesc/0000-0002-3208-9957
FU Spanish Government (MINECO); Catalan Government [RYC-2010-05671,
   TIN2009-14426-C02-01, TIN2012-38102-C03-03, 2009-SGR-1224]
FX This work was supported in part by the Spanish Government (MINECO) and
   in part by the Catalan Government, under Grants RYC-2010-05671,
   TIN2009-14426-C02-01, TIN2012-38102-C03-03, and 2009-SGR-1224. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Wenwu Zhu.
CR [Anonymous], 2000, ISO/IEC 15444-1
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Aulí-Llinàs F, 2011, IEEE T IMAGE PROCESS, V20, P1166, DOI 10.1109/TIP.2010.2077304
   Auli-Llinas F, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/803542
   Aulí-Llinàs F, 2008, IEEE IMAGE PROC, P2856, DOI 10.1109/ICIP.2008.4712390
   Camps-Valls G, 2005, IEEE T GEOSCI REMOTE, V43, P1351, DOI 10.1109/TGRS.2005.846154
   Descampe A, 2007, IEEE T IMAGE PROCESS, V16, P1339, DOI 10.1109/TIP.2007.894258
   Deshpande S., 2001, P ACM INT C MULT OCT, V9, P372
   Ortiz JPG, 2008, IEEE T MULTIMEDIA, V10, P629, DOI 10.1109/TMM.2008.921738
   Krishnan K, 2006, IEEE T MED IMAGING, V25, P1189, DOI 10.1109/TMI.2006.879956
   LEE DH, 2002, P INT C ADV INF SYST, P213
   Li J, 2003, IEEE T MULTIMEDIA, V5, P581, DOI [10.1109/TMM.2003.813284, 10.1109/TTM.2003.813284]
   Li ZC, 2011, IMAGE VISION COMPUT, V29, P1, DOI 10.1016/j.imavis.2010.07.001
   Lima L., 2008, P IEEE WORKSH MULT S, P844
   Monteagudo-Pereira JL, 2012, IEEE DATA COMPR CONF, P22, DOI 10.1109/DCC.2012.10
   MONTEAGUDOPEREI.J, 2008, P SPIE INT C SAT DAT, V7084, P1
   MONTEAGUDOPEREI.JL, 2010, P IEEE DAT COMPR C M, P99
   Müller D, 2009, COMPUT SCI ENG, V11, P38, DOI 10.1109/MCSE.2009.142
   Naman AT, 2011, IEEE T IMAGE PROCESS, V20, P2650, DOI 10.1109/TIP.2011.2126588
   Naman AT, 2011, IEEE T IMAGE PROCESS, V20, P1435, DOI 10.1109/TIP.2010.2093905
   Politou EA, 2004, IEEE T IMAGE PROCESS, V13, P293, DOI 10.1109/TIP.2003.821348
   PONS X, 2004, MIRAMON GEOGRAPHIC I
   Quinn S, 2010, T GIS, V14, P193, DOI 10.1111/j.1467-9671.2010.01191.x
   Richter T, 2012, IEEE DATA COMPR CONF, P13, DOI 10.1109/DCC.2012.9
   Rosenbaum R, 2005, PROC SPIE, V5685, P1019, DOI 10.1117/12.588191
   Taubman D, 2002, IEEE IMAGE PROC, P229
   Taubman D., 2003, P IEEE INT C IM PROC, V3, P765
   Taubman DS, 2006, IEEE IMAGE PROC, P3089, DOI 10.1109/ICIP.2006.313093
   Tuominen VJ, 2010, J DIGIT IMAGING, V23, P454, DOI 10.1007/s10278-009-9200-1
   Yesilmurat S, 2012, GEOINFORMATICA, V16, P435, DOI 10.1007/s10707-011-0141-8
   2005, 154449 ISOIEC
NR 31
TC 1
Z9 1
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1491
EP 1502
DI 10.1109/TMM.2013.2264655
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800003
OA Green Published
DA 2024-07-18
ER

PT J
AU Ma, Z
   Yang, Y
   Sebe, N
   Zheng, K
   Hauptmann, AG
AF Ma, Zhigang
   Yang, Yi
   Sebe, Nicu
   Zheng, Kai
   Hauptmann, Alexander G.
TI Multimedia Event Detection Using A Classifier-Specific Intermediate
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Intermediate representation; multimedia event detection; p-norm
ID VIDEO RETRIEVAL; PROJECTIONS; FEATURES
AB Multimedia event detection (MED) plays an important role in many applications such as video indexing and retrieval. Current event detection works mainly focus on sports and news event detection or abnormality detection in surveillance videos. Differently, our research aims to detect more complicated and generic events within a longer video sequence. In the past, researchers have proposed using intermediate concept classifiers with concept lexica to help understand the videos. Yet it is difficult to judge how many and what concepts would be sufficient for the particular video analysis task. Additionally, obtaining robust semantic concept classifiers requires a large number of positive training examples, which in turn has high human annotation cost. In this paper, we propose an approach that exploits the external concepts-based videos and event-based videos simultaneously to learn an intermediate representation from video features. Our algorithm integrates the classifier inference and latent intermediate representation into a joint framework. The joint optimization of the intermediate representation and the classifier makes them mutually beneficial and reciprocal. Effectively, the intermediate representation and the classifier are tightly correlated. The classifier dependent intermediate representation not only accurately reflects the task semantics but is also more suitable for the specific classifier. Thus we have created a discriminative semantic analysis framework based on a tightly coupled intermediate representation. Extensive experiments on multimedia event detection using real-world videos demonstrate the effectiveness of the proposed approach.
C1 [Ma, Zhigang; Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
   [Yang, Yi; Hauptmann, Alexander G.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.
   [Zheng, Kai] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
C3 University of Trento; Carnegie Mellon University; University of
   Queensland
RP Ma, Z (corresponding author), Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
EM ma@disi.unitn.it; yiyang@cs.cmu.edu; sebe@disi.unitn.it;
   kevinz@itee.uq.edu.au; alex@cs.cmu.edu
RI yang, yang/GWB-9426-2022; Lang, Ming/HIK-0758-2022; Yang,
   Yi/B-9273-2017; Sebe, Niculae/KEC-2000-2024; yang, yang/HGT-7999-2022;
   Ma, Zhigang/H-3543-2015; yang, yang/GVT-5210-2022
OI Yang, Yi/0000-0002-0512-880X; Sebe, Niculae/0000-0002-6597-7248; 
FU European Commission [FP7-248984 GLOCAL]; Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior National Business
   Center [D11PC20068]
FX This paper was supported in part by the European Commission under
   contract FP7-248984 GLOCAL and the Intelligence Advanced Research
   Projects Activity (IARPA) via Department of Interior National Business
   Center contract number D11PC20068. The U. S. Government is authorized to
   reproduce and distribute reprints for Governmental purposes
   notwithstanding any copyright annotation thereon. Disclaimer: The views
   and conclusions contained herein are those of the authors and should not
   be interpreted as necessarily representing the official policies or
   endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.
   S. Government. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Chong-Wah Ngo.
CR Adam A, 2008, IEEE T PATTERN ANAL, V30, P555, DOI 10.1109/TPAMI.2007.70825
   [Anonymous], 2006, ACM INT C MULTIMEDIA, DOI [10.1145/1180639.1180727, DOI 10.1145/1180639.1180727]
   [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], P ICMR
   [Anonymous], TREC VID RETR EV
   Chen M.-Y., 2009, Tech. Rep. CMU-CS- 09-161
   Fukunaga K., 1990, Introduction to StatisticalPattern Recognition
   Hauptmann A, 2007, IEEE T MULTIMEDIA, V9, P958, DOI 10.1109/TMM.2007.900150
   Hauptmann AG, 2008, P IEEE, V96, P602, DOI 10.1109/JPROC.2008.916355
   Hu WM, 2011, IEEE T SYST MAN CY C, V41, P797, DOI 10.1109/TSMCC.2011.2109710
   Ji SW, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1077
   Kokiopoulou E, 2007, IEEE T PATTERN ANAL, V29, P2143, DOI 10.1109/TPAMI.2007.1131
   Lan ZZ, 2012, LECT NOTES COMPUT SC, V7131, P173
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Lew MS, 2006, ACM T MULTIM COMPUT, V2, P1, DOI 10.1145/1126004.1126005
   Liu KH, 2008, IEEE T MULTIMEDIA, V10, P240, DOI 10.1109/TMM.2007.911826
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Ma Z., 2012, P ICMR
   Ma Z., 2012, P ACM MM
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Naphade MilindR., 2004, P 12 ANN ACM INT C M, P660, DOI DOI 10.1145/1027527.1027680
   Saberian M. J., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2929, DOI 10.1109/CVPR.2011.5995605
   Sadlier DA, 2005, IEEE T CIRC SYST VID, V15, P1225, DOI 10.1109/TCSVT.2005.854237
   Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467
   Shyu ML, 2008, IEEE T MULTIMEDIA, V10, P252, DOI 10.1109/TMM.2007.911830
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tseng VS, 2008, IEEE T MULTIMEDIA, V10, P260, DOI 10.1109/TMM.2007.911832
   van de Sande KEA, 2010, IEEE T PATTERN ANAL, V32, P1582, DOI 10.1109/TPAMI.2009.154
   Vitaladevuni SN, 2011, IEEE I CONF COMP VIS, P2312, DOI 10.1109/ICCV.2011.6126512
   Wang F, 2008, ADVANCES IN MATRIX THEORY AND ITS APPLICATIONS, VOL 1, P238
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Yang Y, 2008, IEEE T MULTIMEDIA, V10, P437, DOI 10.1109/TMM.2008.917359
   Yang Y, 2012, IEEE T PATTERN ANAL, V34, P723, DOI 10.1109/TPAMI.2011.170
   Zha ZJ, 2012, IEEE T MULTIMEDIA, V14, P17, DOI 10.1109/TMM.2011.2174782
   Zhang CS, 2010, NEUROCOMPUTING, V73, P959, DOI 10.1016/j.neucom.2009.08.014
NR 35
TC 54
Z9 60
U1 0
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1628
EP 1637
DI 10.1109/TMM.2013.2264928
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800014
DA 2024-07-18
ER

PT J
AU Park, M
   Luo, JB
   Gallagher, AC
   Rabbani, M
AF Park, Minwoo
   Luo, Jiebo
   Gallagher, Andrew C.
   Rabbani, Majid
TI Learning to Produce 3D Media From a Captured 2D Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D; stereo; learning; composition
AB Due to the advances in display technologies and commercial success of 3D motion pictures in recent years, there is renewed interest in enabling consumers to create 3D content. While new 3D content can be created using more advanced capture devices (i.e., stereo cameras), most people still own 2D capture devices. Further, enormously large collections of captured media exist only in 2D. We present a system for producing pseudo-stereo images from captured 2D videos. Our system employs a two-phase procedure where the first phase detects "good" pseudo-stereo images frames from a 2D video, which was captured a priori without any constraints on camera motion or content. We use a trained classifier to detect pairs of video frames that are suitable for constructing pseudo-stereo images. In particular, for a given frame I-t at time t, we determine if (t) over cap exists such that It+(t) over cap and I-t can form an acceptable pseudo-stereo image. Moreover, even if (t) over cap is determined, generating a good pseudo-stereo image from 2D captured video frames can be nontrivial since in many videos, professional or amateur, both foreground and background objects may undergo complex motion. Independent foreground motions from different scene objects define different epipolar geometries that cause the conventional method of generating pseudo-stereo images to fail. To address this problem, the second phase of the proposed system further recomposes the frame pairs to ensure consistent 3D perception for objects for such cases. In this phase, final left and right pseudo-stereo images are created by recompositing different regions of the initial frame pairs to ensure a consistent camera geometry. We verify the performance of our method for producing pseudo-stereo media from captured 2D videos in a psychovisual evaluation using both professional movie clips and amateur home videos.
C1 [Park, Minwoo] ObjectVideo, Reston, VA 20191 USA.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Gallagher, Andrew C.] Cornell Univ, Sch Elect & Comp Engn, Ithaca, NY 14853 USA.
   [Rabbani, Majid] Eastman Kodak Co, Corp Res & Engn, Rochester, NY 14650 USA.
C3 University of Rochester; Cornell University; Eastman Kodak
RP Park, M (corresponding author), ObjectVideo, Reston, VA 20191 USA.
EM mpark@objectvideo.com; jluo@cs.rochester.edu;
   andrew.c.gallagher@gmail.com; majid.rabbani@kodak.com
RI Luo, Jiebo/AAI-7549-2020
OI Park, Minwoo/0000-0002-9117-2128; Luo, Jiebo/0000-0002-4516-9729
CR [Anonymous], 2009, P IEEE C COMP VIS PA
   [Anonymous], DR DOBBS J SOFTW TOO
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   COMANDUCCI D, 2010, P INT S 3D DAT PROC
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   FENG Y, 2005, P INT C IM PROC ICIP, V3, P808
   Gernsheim H., 1986, A concise history of photography
   Guttmann M, 2009, IEEE I CONF COMP VIS, P136, DOI 10.1109/ICCV.2009.5459158
   Hall M., 2009, ACM SIGKDD Explor. Newsl, V11, P18, DOI DOI 10.1145/1656274.1656278
   Harman P, 2002, PROC SPIE, V4660, P78, DOI 10.1117/12.468020
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232
   Ideses I, 2007, J REAL-TIME IMAGE PR, V2, P3, DOI 10.1007/s11554-007-0038-9
   Komodakis N, 2007, IEEE T PATTERN ANAL, V29, P1436, DOI 10.1109/TPAMI.2007.1061
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Saxena A, 2007, IEEE I CONF COMP VIS, P1
   Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132
   Scharstein D, 2003, PROC CVPR IEEE, P195
   Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   TAM WJ, 2006, P IEEE INT C MULT EX
   VAREKAMP C, 2007, IET C PUBLICATIONS, V534, P29
   Vidal R, 2006, INT J COMPUT VISION, V68, P7, DOI 10.1007/s11263-005-4839-7
   Ward B, 2011, IEEE COMPUT GRAPH, V31, P36, DOI 10.1109/MCG.2010.103
   Wu CL, 2008, 3DTV CONF, P45, DOI 10.1109/3DTV.2008.4547809
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 27
TC 12
Z9 13
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1569
EP 1578
DI 10.1109/TMM.2013.2264926
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800009
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Vasic, B
   Vasic, B
AF Vasic, Bata
   Vasic, Bane
TI Simplification Resilient LDPC-Coded Sparse-QIM Watermarking for
   3D-Meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D mesh; data hiding; deletion channels; error correction; iterative
   decoding; low-density parity check codes; quantization index modulation
   watermarking
ID INSERTIONS; CHANNELS
AB We propose a blind watermarking scheme for 3D meshes that combines sparse quantization index modulation (QIM) with deletion correction codes. The QIM operates on the vertices in rough concave regions of the surface thus ensuring impeccability, while the deletion correction code recovers the data hidden in the vertices, which is removed by mesh optimization and/or simplification. The proposed scheme offers two orders of magnitude better performance in terms of recovered watermark bit error rate compared to the existing schemes of similar payloads and fidelity constraints.
C1 [Vasic, Bata] Univ Nis, Fac Elect Engn Nis, Dept Elect, Nish 18000, Serbia.
   [Vasic, Bane] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
C3 University of Nis; University of Arizona
RP Vasic, B (corresponding author), Univ Nis, Fac Elect Engn Nis, Dept Elect, Nish 18000, Serbia.
EM bata.vasic@elfak.ni.ac.rs; vasic@ece.arizona.edu
OI Vasic, Bata/0000-0001-8667-1507
FU NSF [CCF-0963726]
FX This work was supported in part by NSF, Grant CCF-0963726. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Ton Kalker.
CR [Anonymous], 1963, LOW DENSITY PARITY C
   Benedens O, 1999, IEEE COMPUT GRAPH, V19, P46, DOI 10.1109/38.736468
   Cayre F, 2003, IEEE T SIGNAL PROCES, V51, P939, DOI 10.1109/TSP.2003.809380
   Chen B, 2001, IEEE T INFORM THEORY, V47, P1423, DOI 10.1109/18.923725
   Chen J, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P218
   COSTA MHM, 1983, IEEE T INFORM THEORY, V29, P439, DOI 10.1109/TIT.1983.1056659
   Coumou DJ, 2008, IEEE T INF FOREN SEC, V3, P153, DOI 10.1109/TIFS.2008.920728
   Darazi R, 2010, INT CONF ACOUST SPEE, P1742, DOI 10.1109/ICASSP.2010.5495455
   Davey MC, 2001, IEEE T INFORM THEORY, V47, P687, DOI 10.1109/18.910582
   JIMBO M, 1979, INFORM CONTROL, V43, P216, DOI 10.1016/S0019-9958(79)90719-8
   Kalivas A, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P676
   Kauffman M., OPTIMIZING YOUR AUTO
   Kim K, 2010, IEEE T INF FOREN SEC, V5, P721, DOI 10.1109/TIFS.2010.2068546
   Krishnan A.T., 2006, PROC INT ELECT DEVIC, P1
   Moulin P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P73
   Nguyen D. V., IEEE T INF IN PRESS
   Rahmati M., IEEE T INF THE UNPUB
   Ratzer EA, 2005, ANN TELECOMMUN, V60, P29
   Serbian Film Center Naissa Trophy, NAISS TROPH
   Sorkine O., P EUR ACM SIGGRAPH S, P42
   Vasic B, 2004, IEEE T INFORM THEORY, V50, P1156, DOI 10.1109/TIT.2004.828066
   Vasic B, 1996, ELECTRON LETT, V32, P1551, DOI 10.1049/el:19961021
   Vasic B., ORDERED STAT VERTEX
   Vasic B, 2012, ADV ELECTR COMPUT EN, V12, P25, DOI 10.4316/AECE.2012.04004
   VERDU S, 1990, IEEE T INFORM THEORY, V36, P1019, DOI 10.1109/18.57201
   Wang K., 2009, ROBUST BLIND WATERMA
   Wang K, 2008, IEEE T MULTIMEDIA, V10, P1513, DOI 10.1109/TMM.2008.2007350
NR 27
TC 17
Z9 20
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1532
EP 1542
DI 10.1109/TMM.2013.2265673
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800006
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Chen, YY
   Hsu, WH
   Liao, HYM
AF Chen, Yan-Ying
   Hsu, Winston H.
   Liao, Hong-Yuan Mark
TI Automatic Training Image Acquisition and Effective Feature Selection
   From Community-Contributed Photos for Facial Attribute Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Facial attribute detection; feature selection; training image
   acquisition
AB Facial attributes are shown effective for mining specific persons and profiling human activities in large-scale media such as surveillance videos or photo-sharing services. For comprehensive analysis, a rich number of facial attributes is required. Generally, each attribute detector is obtained by supervised learning via the use of large training data. It is promising to leverage the exponentially growing community contributed photos and the associated informative contexts to ease the burden of manual annotation; however, such huge noisy data from the Internet still pose great challenges. We propose to measure the quality of training images by discriminable visual features, which are verified with the relative discrimination between the unlabeled images and the pseudo-positives (pseudo-negatives) retrieved by textual relevance. The proposed feature selection requires no heuristic threshold, therefore, can be generalized to multiple feature modalities. We further exploit the rich context cues (e. g., tags, geo-locations, etc.) associated with the publicly available photos for mining more semantically consistent but visually diverse training images around the world. Experimenting in the benchmarks, we demonstrate that our work can successfully acquire effective training images for learning generic facial attributes, where the classification error is relatively reduced up to 23.35% compared with that of the text-based approach and shown comparable with that of costly manual annotations. (All of the face images presented in this paper except the training images in Fig. 8 attribute to various Flickr users under Creative Commons Licenses).
C1 [Chen, Yan-Ying; Hsu, Winston H.] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
   [Hsu, Winston H.] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 10617, Taiwan.
   [Liao, Hong-Yuan Mark] Acad Sinica, Res Ctr, Inst Informat Sci, Taipei, Taiwan.
C3 National Taiwan University; National Taiwan University; Academia Sinica
   - Taiwan
RP Chen, YY (corresponding author), Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 10617, Taiwan.
EM yanying@cmlab.csie.ntu.edu.tw; winston@csie.ntu.edu.tw;
   liao@iis.sinica.edu.tw
RI Liao, Hong-Yuan Mark/AAQ-5514-2021
FU National Science Council of Taiwan [NSC 100-2221-E-002-211]; Excellent
   Research Projects of National Taiwan University [101R7762]
FX Manuscript received February 13, 2012; revised August 25, 2012 and
   November 27, 2012; accepted December 09, 2012. Date of publication March
   07, 2013; date of current version September 13, 2013. This work was
   supported in part by grants from the National Science Council of Taiwan,
   under Contracts NSC 100-2221-E-002-211, and Excellent Research Projects
   of National Taiwan University, 101R7762. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Christophe De Vleeschouwer.
CR [Anonymous], P IEEE INT C COMP VI
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], 2008, Multivariate Density Estimation. Theory
   [Anonymous], 2007, P ACM INT C IM VID R
   [Anonymous], 1983, INTRO MODERN INFORM
   [Anonymous], P EUR C COMP VIS
   Baluja S., 2007, INT J COMPUT VISION
   Berg T., 2004, P CVPR
   Black D., 1958, The theory of committees and elections
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cheng A.-J., 2011, P ACM INT C MULT
   Fergus R., 2005, P IEEE INT C COMP VI
   Field D. J., 1987, J OPT SOC AM A
   Geng X., 2007, P INT ACM SIGIR C RE
   Guo G., 2009, P IEEE C COMP VIS PA
   Internet World Stats, LAT INT IND US PEN
   Kennedy L. S., 2006, P E 8 ACM INT WORKSH
   Kumar N., 2008, P IEEE INT C COMP VI
   Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250
   Lei Y.-H., 2011, P ACM INT C MULT
   Mei T., 2010, IEEE MULTIMEDIA MAG
   Mensink T., 2008, P EUR C COMP VIS
   Moghaddam B, 2002, IEEE T PATTERN ANAL, V24, P707, DOI 10.1109/34.1000244
   Ni B., 2009, P E 8 ACM INT WORKSH
   Ojala T., 1996, PATTERN RECOGNIT
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Schroff F., 2007, P E 8 ACM INT WORKSH
   Taneva B., 2010, P ACM INT C WEB SEAR
   Tong S., 2002, The Journal of Machine Learning Research
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Viola P., 2006, P NEUR INF PROC SYST
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wang S., 2012, NEUROCOMPUTING
   Wu C., 2008, P IEEE C COMP VIS PA
   Yan R., 2008, P IEEE C COMP VIS PA
   Zhou M., 2006, P INT C PATT REC
NR 36
TC 10
Z9 11
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1388
EP 1399
DI 10.1109/TMM.2013.2250492
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400015
DA 2024-07-18
ER

PT J
AU Abdallah, RA
   Shanbhag, NR
AF Abdallah, Rami A.
   Shanbhag, Naresh R.
TI Robust and Energy Efficient Multimedia Systems via Likelihood Processing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Error resiliency; low-power design; media processing; robust design;
   stochastic computation; voltage overscaling
AB This paper presents likelihood processing (LP) for designing robust and energy-efficient multimedia systems in the presence of nanoscale non-idealities. LP exploits error statistics of the underlying hardware to compute the probability of a particular bit being a one or a zero. Multiple output observations are generated via either: 1) modular redundancy (MR), 2) estimation, or 3) exploiting spatio-temporal correlation. Energy efficiency and robustness of a 2D discrete-cosine transform (DCT) image codec employing LP is studied. Simulations in a commercial 45-nm CMOS process show that LP can tolerate up to 100x, and 5x greater component error probability as compared to conventional and triple-MR (TMR)-based systems, respectively, while achieving a peak-signal-to-noise ratio (PSNR) of 30 dB at a pre-correction error rate of 20%. Furthermore, LP is able to achieve energy savings of 71% over TMR at a PSNR of 28 dB, while tolerating a pre-correction error rate of 4%.
C1 [Abdallah, Rami A.; Shanbhag, Naresh R.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61820 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign
RP Abdallah, RA (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61820 USA.
EM rabdall3@illinois.edu; shanbhag@illinois.edu
FU Gigascale Systems Research Center (GSRC) under the Focus Center Research
   Program (FCRP), a Semiconductor Research Corporation entity
FX The authors acknowledge the support of the Gigascale Systems Research
   Center (GSRC), one of six research centers funded under the Focus Center
   Research Program (FCRP), a Semiconductor Research Corporation entity.
CR Abdallah R., 2011, P 11 ANN INT C NEW T, P1
   Abdallah R. A., 2011, SELSE
   Abdallah RA, 2010, PR IEEE COMP DESIGN, P38, DOI 10.1109/ICCD.2010.5647569
   Austin T, 2004, COMPUTER, V37, P57, DOI 10.1109/MC.2004.1274005
   Bahar RI, 2003, ICCAD-2003: IEEE/ACM DIGEST OF TECHNICAL PAPERS, P480
   CHEN WH, 1977, IEEE T COMMUN, V25, P1004, DOI 10.1109/TCOM.1977.1093941
   Choudhury MR, 2008, DES AUT TEST EUROPE, P782
   ERFANIAN J, 1994, IEEE T COMMUN, V42, P1661, DOI 10.1109/TCOMM.1994.582868
   Hegde R, 2001, IEEE T VLSI SYST, V9, P813, DOI 10.1109/92.974895
   Huber P.J., 1981, Robust Statistics, V1
   Karakonstantis Georgios, 2010, Proceedings of the 16th ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED 2010), P117, DOI 10.1145/1840845.1840871
   Karnik T, 2002, IEEE/ACM INTERNATIONAL CONFERENCE ON CAD-02, DIGEST OF TECHNICAL PAPERS, P203, DOI 10.1109/ICCAD.2002.1167535
   Kim EP, 2012, IEEE T COMPUT, V61, P323, DOI 10.1109/TC.2010.253
   Kurdahi FJ, 2006, ISQED 2006: PROCEEDINGS OF THE 7TH INTERNATIONAL SYMPOSIUM ON QUALITY ELECTRONIC DESIGN, P179
   Nisar MM, 2011, IEEE T COMPUT, V60, P1313, DOI 10.1109/TC.2010.277
   Rabaey JM, 2008, IEEE DES TEST COMPUT, V25, P358, DOI 10.1109/MDT.2008.118
   Shanbhag NR, 2010, DES AUT CON, P859
   Tschanz James, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P71, DOI 10.1145/1687399.1687414
   Varatkar GV, 2010, IEEE T VLSI SYST, V18, P1421, DOI 10.1109/TVLSI.2009.2024673
   Von Neumann J., 1956, Probabilistic Logics and the Synthesis of Reliable Organisms from Unreliable sources
   Weikang Qian, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P367
NR 21
TC 6
Z9 6
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 257
EP 267
DI 10.1109/TMM.2012.2231667
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500003
DA 2024-07-18
ER

PT J
AU Shah, GA
   Liang, WF
   Akan, OB
AF Shah, Ghalib A.
   Liang, Weifa
   Akan, Ozgur B.
TI Cross-Layer Framework for QoS Support in Wireless Multimedia Sensor
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-layer QoS; IEEE 80211e; wireless multimedia sensor networks
   (WMSNs)
ID RATE-DISTORTION ANALYSIS; VIDEO COMMUNICATION; PROTOCOL
AB The emergence of wireless multimedia sensor networks (WMSNs) has made it possible to realize multimedia delivery on tiny sensing devices. The volume and characteristics of multimedia data is quite different from the data generated in WSNs that has raised the need to explore communication protocols for multimedia delivery in WMSNs. The existing studies focus on providing quality-of-service (QoS) to each individual source but they are not adaptive to create room for maximizing the number of sources. In this paper, we propose a novel cross-layer framework for QoS support in WMSNs. The objective of the proposed framework is to maximize the capacity of the deployed network to enhance the number of video sources given that the QoS constraint of each individual source is also preserved. This is achieved by implementing Wyner-Ziv lossy distributed source coding at the sensor node with variable group of pictures (GOP) size, exploiting multipath routing for real-time delivery and link adaptation to enhance the bandwidth under the given bit error rate. Hence, application requirements are mapped on joint operations of application, network, link and MAC layers to achieve the desired QoS. Simulation results reveal that the framework admits larger number of video sources under the satisfied distortion constraint.
C1 [Shah, Ghalib A.; Akan, Ozgur B.] Koc Univ Sariyer, Dept Elect & Elect Engn, Next Generat & Wireless Commun Lab NWCL, TR-34450 Istanbul, Turkey.
   [Liang, Weifa] Australian Natl Univ, Sch Comp Sci, Canberra, ACT, Australia.
   [Liang, Weifa] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.
C3 Koc University; Australian National University; Australian National
   University
RP Shah, GA (corresponding author), Koc Univ Sariyer, Dept Elect & Elect Engn, Next Generat & Wireless Commun Lab NWCL, TR-34450 Istanbul, Turkey.
EM gshah@ku.edu.tr; wliang@cs.anu.edu.au; akan@ku.edu.tr
RI Shah, Ghalib Asadullah/ABB-4121-2020; Akan, Ozgur B./C-7150-2013; Liang,
   Weifa/A-7108-2019
OI Liang, Weifa/0000-0002-8207-6740
FU Turkish Scientific and Technical Research Council (TUBITAK) [110E249]
FX This work was supported by the Turkish Scientific and Technical Research
   Council (TUBITAK) Career Award under grant #110E249.
CR Igartua MA, 2011, COMPUT NETW, V55, P2985, DOI 10.1016/j.comnet.2011.06.007
   Akyildiz IF, 2007, COMPUT NETW, V51, P921, DOI 10.1016/j.comnet.2006.10.002
   [Anonymous], 2005, 80211E IEEE, V802, P11
   Ben-Othman J, 2010, J PARALLEL DISTR COM, V70, P849, DOI 10.1016/j.jpdc.2010.02.010
   Chen M, 2007, COMPUT COMMUN, V30, P3368, DOI 10.1016/j.comcom.2007.01.016
   Chou CT, 2006, IEEE ACM T NETWORK, V14, P1179, DOI 10.1109/TNET.2006.886336
   Fallah Y. P., 2008, EURASIP J WIREL COMM, V2008, P14
   Farrag O, 2009, GLOB TELECOMM CONF, P3781
   Felemban E, 2006, IEEE T MOBILE COMPUT, V5, P738, DOI 10.1109/TMC.2006.79
   Frossard P, 2008, P IEEE, V96, P39, DOI 10.1109/JPROC.2007.909876
   Guinard Dominique., 2009, 2009 Sixth International Conference on Networked Sensing Systems, P1, DOI DOI 10.1109/INSS.2009.5409925
   Haratcherev I, 2006, IEEE COMMUN MAG, V44, P115, DOI 10.1109/MCOM.2006.1580941
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   He ZH, 2008, IEEE T CIRC SYST VID, V18, P596, DOI 10.1109/TCSVT.2008.918802
   Huang XX, 2008, WIREL NETW, V14, P465, DOI 10.1007/s11276-006-0731-9
   James P. F., 2008, EURASIP J ADV SIG PR, V2008, P17
   Kei CH, 2008, J INF SCI ENG, V24, P425
   Kim H.H., 2009, Interdisciplinary Bio Central, V1, P1, DOI [DOI 10.4051/IBC.2009.2.0007, 10.4051/ibc.2009.2.0007]
   Lei Shu, 2009, 2009 International Conference on Computational Science and Engineering (CSE), P961, DOI 10.1109/CSE.2009.128
   Li S., 2010, INT J MULTIMEDIA ITS, V2, P10
   Mao GQ, 2007, COMPUT NETW, V51, P2467, DOI 10.1016/j.comnet.2006.11.007
   Melodia T, 2010, IEEE J SEL AREA COMM, V28, P653, DOI 10.1109/JSAC.2010.100604
   Mulligan Geoff., 2007, P 4 WORKSHOP EMBEDDE, P78
   Oldewurtel Frank, 2008, 2008 Second International Conference on Sensor Technologies and Applications (SENSORCOMM), P435, DOI 10.1109/SENSORCOMM.2008.10
   San X, 2007, IEEE T CIRC SYST VID, V17, P1536, DOI 10.1109/TCSVT.2007.905382
   Sarr C, 2008, IEEE T MOBILE COMPUT, V7, P1228, DOI 10.1109/TMC.2008.41
   Saxena N, 2008, COMPUT NETW, V52, P2532, DOI 10.1016/j.comnet.2008.05.009
   Shiang HP, 2009, IEEE T VEH TECHNOL, V58, P941, DOI 10.1109/TVT.2008.925308
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tagliasacchi M, 2007, IEEE SIGNAL PROC LET, V14, P625, DOI 10.1109/LSP.2007.896187
   van der Schaar M, 2007, IEEE T MULTIMEDIA, V9, P185, DOI 10.1109/TMM.2006.886384
   Varodayan D, 2006, SIGNAL PROCESS, V86, P3123, DOI 10.1016/j.sigpro.2006.03.012
   Vuran MC, 2004, COMPUT NETW, V45, P245, DOI 10.1016/j.comnet.2004.03.007
   Xiong ZX, 2004, IEEE SIGNAL PROC MAG, V21, P80, DOI 10.1109/MSP.2004.1328091
   Zhou L, 2010, IEEE J SEL AREA COMM, V28, P409, DOI 10.1109/JSAC.2010.100412
NR 35
TC 34
Z9 35
U1 0
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2012
VL 14
IS 5
BP 1442
EP 1455
DI 10.1109/TMM.2012.2196510
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 008XT
UT WOS:000308990600006
DA 2024-07-18
ER

PT J
AU Fang, L
   Au, OC
   Chen, Y
   Katsaggelos, AK
   Wang, HL
   Wen, X
AF Fang, Lu
   Au, Oscar C.
   Chen, Yan
   Katsaggelos, Aggelos K.
   Wang, Hanli
   Wen, Xing
TI Joint Demosaicing and Subpixel-Based Down-Sampling for Bayer Images: A
   Fast Frequency-Domain Analysis Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Demosaicking; down-sampling; subpixel rendering
ID COLOR; PERFORMANCE; ALGORITHM; ERROR
AB A portable device such as a digital camera with a single sensor and Bayer color filter array (CFA) requires demosaicing to reconstruct a full color image. To display a high resolution image on a low resolution LCD screen of the portable device, it must be down-sampled. The two steps, demosaicing and down-sampling, influence each other. On one hand, the color artifacts introduced in demosaicing may be magnified when followed by down-sampling; on the other hand, the detail removed in the down-sampling cannot be recovered in the demosaicing. Therefore, it is very important to consider simultaneous demosaicing and down-sampling.
   In this paper, we propose a fast frequency-domain analysis approach for joint demosaicing and subpixel-based down-sampling (FFA-JDSD) of single sensor Bayer images. In FFA-JDSD, we integrate demosaicing into down-sampling by directly performing subpixel-based down-sampling in the Bayer domain, due to which the computational complexity is significantly reduced. We use a frequency domain analysis tool to show that the cut-off frequency of the low-pass filter for JDSD can be effectively extended beyond the Nyquist frequency, resulting in much sharper down-sampled images. Experimental results show that, compared with the methods that separately perform demosaicing and down-sampling, FFA-JDSD has much less computational complexity, and can produce much sharper results at the expense of slight color fringing artifact.
C1 [Fang, Lu; Au, Oscar C.; Wen, Xing] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
   [Chen, Yan] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   [Katsaggelos, Aggelos K.] Northwestern Univ, Dept Elect Engn & Comp Sci, Evanston, IL 60625 USA.
   [Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Shanghai 200092, Peoples R China.
   [Wang, Hanli] Tongji Univ, Minist Educ, Key Lab Embedded Syst & Serv Comp, Shanghai 200092, Peoples R China.
C3 Hong Kong University of Science & Technology; University System of
   Maryland; University of Maryland College Park; Northwestern University;
   Tongji University; Tongji University
RP Fang, L (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Kowloon, Hong Kong, Peoples R China.
EM eefanglu@ust.hk; eeau@ust.hk; yan@umd.edu; aggk@eecs.northwestern.edu;
   hanliwang@tongji.edu.cn; wxxab@ust.hk
RI Fang, Lu/M-2457-2016; Wang, Hanli/G-5111-2014; Wang, Hanli/K-5717-2019;
   Chen, Yan/P-5344-2019; Katsaggelos, Aggelos K/B-7233-2009; Katsaggelos,
   Aggelos K/I-8002-2012; Chen, Yan/B-7131-2009; Chen, Yan/P-8901-2017;
   Chen, Yan/C-6466-2014; Chen, Yan/H-5483-2012
OI Wang, Hanli/0000-0002-9999-4871; Wang, Hanli/0000-0002-9999-4871; Chen,
   Yan/0000-0002-3227-4562; Chen, Yan/0000-0002-3227-4562; Katsaggelos,
   Aggelos K/0000-0003-4554-0070
FU Research Grants Council (RGC) of the Hong Kong Special Administrative
   Region, China [GRF 610109]; National Natural Science Foundation of China
   [61102059]; Fundamental Research Funds for the Central Universities
   [0800219158]; National Basic Research Program (973 Program) of China
   [2010CB328101]
FX This work was supported in part by the Research Grants Council (RGC) of
   the Hong Kong Special Administrative Region, China (GRF 610109), the
   National Natural Science Foundation of China under Grant 61102059, the
   Fundamental Research Funds for the Central Universities under Grant
   0800219158, and the National Basic Research Program (973 Program) of
   China under Grant 2010CB328101. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Sheng-Wei (Kuan-Ta) Chen.
CR Bayer B. E., 1976, U.S. patent, Patent No. 3,971,065
   Betrisey Claude, 2000, SID INT S, V31, P296
   Chung KH, 2006, IEEE T IMAGE PROCESS, V15, P2944, DOI 10.1109/TIP.2006.877521
   Daly S., 2001, SID International Symposium Digest of Technical Papers, V32, P1200
   Diniz P., 2010, DIGIT SIGNAL PROCESS
   Fang L, 2012, IEEE T CIRC SYST VID, V22, P740, DOI 10.1109/TCSVT.2011.2179458
   Fang L, 2012, IEEE T IMAGE PROCESS, V21, P1391, DOI 10.1109/TIP.2011.2165550
   Fang L, 2011, IEEE J-STSP, V5, P240, DOI 10.1109/JSTSP.2010.2053346
   GIBSON S, SUB PIXEL FONT RENDE
   Gonzalez R., 2005, WOODS DIGITAL IMAGE, P420
   Hirakawa K, 2005, IEEE T IMAGE PROCESS, V14, P360, DOI 10.1109/TIP.2004.838691
   ITU, 1995, REC ITU R BT 601 5
   Klompenhouwer M.A., 2003, Journal of the Society for Information Display, V11, P176
   Lam EY, 2000, IEEE T IMAGE PROCESS, V9, P1661, DOI 10.1109/83.869177
   Lian NX, 2007, IEEE T IMAGE PROCESS, V16, P2515, DOI 10.1109/TIP.2007.904459
   Longère P, 2002, P IEEE, V90, P123, DOI 10.1109/5.982410
   Lukac R, 2005, IEEE T CONSUM ELECTR, V51, P1260, DOI 10.1109/TCE.2005.1561853
   Lukac R, 2005, IEEE T CIRC SYST VID, V15, P1475, DOI 10.1109/TCSVT.2005.856923
   Sakamoto T, 1998, IEEE T CONSUM ELECTR, V44, P1342, DOI 10.1109/30.735836
   Wandell B. A, 1995, Foundations of vision
   Zhang L, 2005, IEEE T IMAGE PROCESS, V14, P2167, DOI 10.1109/TIP.2005.857260
NR 21
TC 11
Z9 14
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1359
EP 1369
DI 10.1109/TMM.2012.2191269
PN 2
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400020
DA 2024-07-18
ER

PT J
AU Sung, YH
   Wang, JC
AF Sung, Yu-Huan
   Wang, Jia-Ching
TI Fast Mode Decision for H. 264/AVC Based on Rate-Distortion Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264/AVC; mode classification; mode decision; nearest neighbor; video
   coding
ID INTERMODE DECISION; H.264/AVC; SELECTION; STANDARD
AB Although H.264/AVC is a promising video coding standard that achieves excellent coding performance in terms of visual quality and bitrate, its extremely high encoding complexity raises concerns about its computational burden on real-time applications. This work presents a multi-phase classification (MPC) scheme that builds a mode decision tree according to the clustering of rate-distortion costs. A nearest cluster mean criterion is used to examine candidate modes phase by phase, and a performance control mechanism is incorporated to maintain coding performance. Experimental results confirm that the proposed MPC algorithm reduces encoding time by an average of 68% with only negligible performance degradation.
C1 [Sung, Yu-Huan; Wang, Jia-Ching] Natl Cent Univ, Dept Comp Sci & Informat Engn, Thongli City 32001, Taoyuan County, Taiwan.
C3 National Central University
RP Sung, YH (corresponding author), Natl Cent Univ, Dept Comp Sci & Informat Engn, Thongli City 32001, Taoyuan County, Taiwan.
EM jcw@csie.ncu.edu.tw
CR Kannangara CS, 2006, IEEE T CIRC SYST VID, V16, P202, DOI 10.1109/TCSVT.2005.859026
   Kim C, 2007, IEEE T CIRC SYST VID, V17, P441, DOI 10.1109/TCSVT.2006.888829
   Kim J., 2009, P ICAPR 2009 KOLK IN
   Lee P. J., 2009, P NAFIPS 2009 CINC O
   Liu Z, 2009, IEEE T CIRC SYST VID, V19, P128, DOI 10.1109/TCSVT.2008.2005804
   Paul M, 2011, IEEE T IMAGE PROCESS, V20, P461, DOI 10.1109/TIP.2010.2063436
   Paul M, 2009, IEEE T MULTIMEDIA, V11, P581, DOI 10.1109/TMM.2009.2017610
   Ri SH, 2009, IEEE T CIRC SYST VID, V19, P302, DOI 10.1109/TCSVT.2008.2009257
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu D, 2005, IEEE T CIRC SYST VID, V15, P953, DOI 10.1109/TCSVT.2005.848304
   Yu ACW, 2008, IEEE T CIRC SYST VID, V18, P186, DOI 10.1109/TCSVT.2007.913970
   Zeng HQ, 2009, IEEE T CIRC SYST VID, V19, P491, DOI 10.1109/TCSVT.2009.2014014
   Zhao TS, 2010, IEEE T CIRC SYST VID, V20, P697, DOI 10.1109/TCSVT.2010.2045812
   Zhao Y., 2006, P ICIP 2006 ATL GA O
NR 14
TC 16
Z9 19
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 693
EP 702
DI 10.1109/TMM.2012.2186793
PN 2
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 943ZK
UT WOS:000304166700004
DA 2024-07-18
ER

PT J
AU Yamashita, Y
   Harada, T
   Kuniyoshi, Y
AF Yamashita, Yuya
   Harada, Tatsuya
   Kuniyoshi, Yasuo
TI Causal Flow
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Multimedia and Expo (ICME)
CY JUL 11-15, 2011
CL Univ Ramon Llull, La Salle, Barcelona, SPAIN
SP IEEE, IEEE Signal Proc Soc, IEEE Circuits & Syst Soc, IEEE Comp Soc, IEEE Commun Soc
HO Univ Ramon Llull, La Salle
DE Granger causality; optical flow; regularization; video feature
ID GRANGER CAUSALITY
AB Optical flow is a widely used technique for extracting flow information from video images. While it is useful for estimating temporary movement in video images, it only captures one aspect of extracting dominant flow information from a sequence of video images. In this paper, we propose a novel flow extraction approach called causal flow, which can estimate the dominant causal relationships among nearby pixels. We assume flows in video images as pixel-to-pixel information transfer, whereas the optical flow measures the relative motion of pixels. Causal flow is based on the Granger causality test, which measures causal influence based on prediction via vector autoregression, and is widely used in economics and brain science. The experimental results demonstrate that causal flow can extract dominant flow information which cannot be obtained by current methods.
C1 [Yamashita, Yuya; Harada, Tatsuya; Kuniyoshi, Yasuo] Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Mechanoinformat, Bunkyo Ku, Tokyo 1138656, Japan.
C3 University of Tokyo
RP Yamashita, Y (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Dept Mechanoinformat, Bunkyo Ku, Tokyo 1138656, Japan.
EM yamasita@isi.imi.i.u-tokyo.ac.jp; harada@isi.imi.i.u-tokyo.ac.jp;
   kuniyosh@isi.imi.i.u-tokyo.ac.jp
FU Grants-in-Aid for Scientific Research [24680017] Funding Source: KAKEN
CR Ali S., 2007, IEEE COMP VIS PATT R
   Anandan P., 1989, A Computational Framework and an Algorithm for the Measurement of Visual Motion
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Brox T., 2009, IEEE COMP VIS PATT R
   Fujita Andre, 2010, Journal of Bioinformatics and Computational Biology, V8, P679, DOI 10.1142/S0219720010004860
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   HIEMSTRA C, 1994, J FINANC, V49, P1639, DOI 10.2307/2329266
   HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634
   HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Ladroue C, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006899
   Lee J., 2009, IEEE INT C IM PROC C
   Loy C. C., 2009, IEEE COMP VIS PATT R
   Lucas B., 1981, P INT JOINT C ART IN, P674, DOI DOI 10.1364/J0SAA.19.002142
   Rao B.R., 1969, Trabajoso de Estadistica y de Investigacion operative, V20, P211
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Shibuya T., 2009, ACM SIGKDD PAR FRANC
   Vinod HD, 1976, J ECONOMETRICS, V4, P147, DOI DOI 10.1016/0304-4076(76)90010-5
   Yin XR, 2004, J MULTIVARIATE ANAL, V91, P161, DOI 10.1016/S0047-259X(03)00129-5
NR 19
TC 6
Z9 7
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2012
VL 14
IS 3
BP 619
EP 629
DI 10.1109/TMM.2012.2191396
PN 1
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 943ZG
UT WOS:000304166300013
DA 2024-07-18
ER

PT J
AU Liu, N
   Zhao, Y
   Zhu, ZF
   Lu, HQ
AF Liu, Nan
   Zhao, Yao
   Zhu, Zhenfeng
   Lu, Hanqing
TI Exploiting Visual-Audio-Textual Characteristics for Automatic TV
   Commercial Block Detection and Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Commercial detection; commercial segmentation; multi-modal fusion; text
   detection; video analysis
AB Automatic TV commercial block detection (CBD) and commercial block segmentation (CBS) are two key components of a smart commercial digesting system. In this paper, we focus our research on CBD and CBS by the means of collaborative exploitation of visual-audio-textual characteristics embedded in commercials. Rather than utilizing exclusively visual-audio characteristics like most previous works, an abundance of textual characteristics associated with commercials are fully exploited. Additionally, Tri-AdaBoost, an interactive ensemble learning manner, is proposed to form a consolidated semantic fusion across visual, audio, and textual characteristics. In order to segment a detected commercial block into multiple individual commercials, additional informative descriptors including textual characteristics are introduced to boost the robustness in the detection of frame marked with product information (FMPI). Together with the characteristics of audio spectral variation pointer and silent position, FMPI can provide a kind of complementary representation architecture to model the similarity of intra-commercial and the dissimilarity of inter-commercial. Experiments are conducted on a large video dataset from both China central television (CCTV) channels and TRECVID'05, and promising experimental results show the effectiveness of the proposed scheme.
C1 [Liu, Nan; Zhao, Yao; Zhu, Zhenfeng] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Liu, Nan; Zhao, Yao; Zhu, Zhenfeng] Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Lu, Hanqing] Chinese Acad Sci, Inst Automat, Beijing 100080, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Chinese
   Academy of Sciences; Institute of Automation, CAS
RP Liu, N (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM 05112073@bjtu.edu.cn; yzhao@bjtu.edu.cn; zh-fzhu@bjtu.edu.cn;
   luhq@nlpr.ia.ac.cn
RI Akalugwu, Kenneth/F-4815-2014
FU 973 Program [2012CB316401]; National Science Foundation of China
   [61025013, 61172129]; Sino-Singapore JRP [2010DFA11010]; Beijing Natural
   Science Foundation [4112043]; Fundamental Research Funds for the Central
   Universities [2009JBZ006-3]
FX Manuscript received October 31, 2010; revised March 22, 2011 and June
   06, 2011; accepted June 14, 2011. Date of publication June 23, 2011;
   date of current version September 16, 2011. This work was supported in
   part by 973 Program (No. 2012CB316401), National Science Foundation of
   China (No. 61025013, No. 61172129), Sino-Singapore JRP (No.
   2010DFA11010), Beijing Natural Science Foundation (No. 4112043), and
   Fundamental Research Funds for the Central Universities (No.
   2009JBZ006-3). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Zhu Liu.
CR Agnihotri L, 2003, PROC CVPR IEEE, P79
   Albiol A, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P541
   Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962
   Covell M, 2006, 2006 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P461, DOI 10.1109/MMSP.2006.285351
   Duan L., 2006, International Multimedia Conference, P201
   Duan LY, 2008, IEEE MULTIMEDIA, V15, P28, DOI 10.1109/MMUL.2008.4
   Duygulu P, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P1267, DOI 10.1109/ICME.2004.1394454
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Gauch JM, 2006, COMPUT VIS IMAGE UND, V103, P80, DOI 10.1016/j.cviu.2006.03.002
   Hua XS, 2005, 2005 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO (ICME), VOLS 1 AND 2, P149
   Huang YP, 2007, IEEE T SYST MAN CY B, V37, P485, DOI 10.1109/TSMCB.2006.883428
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Li HP, 2000, INT C PATT RECOG, P223, DOI 10.1109/ICPR.2000.906053
   Lienhart R, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P509, DOI 10.1109/MMCS.1997.609763
   LIU N, 2010, P ICME 10 JUL, P831
   Liu N, 2010, LECT NOTES COMPUT SC, V6297, P296, DOI 10.1007/978-3-642-15702-8_27
   LIU TY, 2004, P ICIP 04 OCT, V3, P1617
   Lyu MR, 2005, IEEE T CIRC SYST VID, V15, P243, DOI 10.1109/TCSVT.2004.841653
   Mingshan Li, 2009, 2009 Conference on Lasers & Electro-Optics Europe & 11th European Quantum Electronics Conference (CLEO/EQEC), DOI 10.1109/CLEOE-EQEC.2009.5196285
   MIZUTANI M, 2005, P IEEE ICME2007 BEIJ, V2, P157
   ONG BS, 2005, THESIS U POMPEU FABR
   Sadlier DA, 2002, PATTERN RECOGN, V35, P2719, DOI 10.1016/S0031-3203(01)00251-5
   Shen H.T., 2007, VLDB, P1374
   Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wang JQ, 2008, IEEE T MULTIMEDIA, V10, P393, DOI 10.1109/TMM.2008.917362
   Zhang L, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P587
   ZHENG YT, 2006, P ICME 06 JUL, P497
   Zhou ZH, 2005, IEEE T KNOWL DATA EN, V17, P1529, DOI 10.1109/TKDE.2005.186
NR 30
TC 16
Z9 17
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 961
EP 973
DI 10.1109/TMM.2011.2160334
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300011
DA 2024-07-18
ER

PT J
AU Luengo, I
   Navas, E
   Hernáez, I
AF Luengo, Iker
   Navas, Eva
   Hernaez, Inmaculada
TI Feature Analysis and Evaluation for Automatic Emotion Identification in
   Speech
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion identification; information fusion; parametrization
ID RECOGNITION
AB The definition of parameters is a crucial step in the development of a system for identifying emotions in speech. Although there is no agreement on which are the best features for this task, it is generally accepted that prosody carries most of the emotional information. Most works in the field use some kind of prosodic features, often in combination with spectral and voice quality parametrizations. Nevertheless, no systematic study has been done comparing these features. This paper presents the analysis of the characteristics of features derived from prosody, spectral envelope, and voice quality as well as their capability to discriminate emotions. In addition, early fusion and late fusion techniques for combining different information sources are evaluated. The results of this analysis are validated with experimental automatic emotion identification tests. Results suggest that spectral envelope features outperform the prosodic ones. Even when different parametrizations are combined, the late fusion of long-term spectral statistics with short-term spectral envelope parameters provides an accuracy comparable to that obtained when all parametrizations are combined.
C1 [Luengo, Iker; Navas, Eva; Hernaez, Inmaculada] Univ Basque Country, Dept Elect & Telecommun, Bilbao 48013, Spain.
C3 University of Basque Country
RP Luengo, I (corresponding author), Univ Basque Country, Dept Elect & Telecommun, Bilbao 48013, Spain.
EM iker.lu-engo@ehu.es; eva.navas@ehu.es; inma.hernaez@ehu.es
RI Navas, Eva/H-4317-2013; Hernaez-Rioja, Inma/AAQ-8183-2020
OI Navas, Eva/0000-0003-3804-4984; Hernaez-Rioja, Inma/0000-0003-4447-7575
FU Spanish Government [TEC2009-14094-C04-02, TEC2006-13694-C03-02]
FX Manuscript received December 11, 2009; revised March 22, 2010; accepted
   May 06, 2010. Date of current version September 15, 2010. This work was
   supported in part by the Spanish Government under the BUCEADOR project
   (TEC2009-14094-C04-02) and in part by the AVIVAVOZ project
   (TEC2006-13694-C03-02). The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Hamid K.
   Aghajan.
CR Alku P, 1999, CLIN NEUROPHYSIOL, V110, P1329, DOI 10.1016/S1388-2457(99)00088-7
   [Anonymous], P INT BRIGHT
   [Anonymous], 2005, INTERSPEECH
   Bäckström T, 2002, IEEE T SPEECH AUDI P, V10, P186, DOI 10.1109/TSA.2002.1001983
   Banse R, 1996, J PERS SOC PSYCHOL, V70, P614, DOI 10.1037/0022-3514.70.3.614
   Batliner Anton., 2006, Proc. IS-LTC 2006, P240
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Burkhardt F., 2000, Speech Emotion-2000, P151
   Burkhardt F, 2005, INTERSPEECH, V5, P1517, DOI DOI 10.21437/INTERSPEECH.2005-446
   Darwin C., 1872, P374
   DEVILLERS L, 2004, P SPEECH PROSODY, P205
   Duda R. O., 2001, PATTERN CLASSIFICATI
   EKMAN P, 1992, COGNITION EMOTION, V6, P169, DOI 10.1080/02699939208411068
   Erickson D, 2005, ACOUST SCI TECHNOL, V26, P317, DOI 10.1250/ast.26.317
   Fierrez-Aguilar J, 2005, PATTERN RECOGN LETT, V26, P2628, DOI 10.1016/j.patrec.2005.06.008
   Fukunaga K., 2013, INTRO STAT PATTERN R
   Gobl C, 2003, SPEECH COMMUN, V40, P189, DOI 10.1016/S0167-6393(02)00082-1
   GUTSCHOVEN B, 2000, P 3 INT C INF FUS, V2, P3
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Huang CF, 2008, SPEECH COMMUN, V50, P810, DOI 10.1016/j.specom.2008.05.017
   Kim S, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P48, DOI 10.1109/MMSP.2007.4412815
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Kwon OW., 2003, proceedings of Eurospeech, P125
   López-Cózar R, 2008, LECT NOTES ARTIF INT, V5246, P617, DOI 10.1007/978-3-540-87391-4_78
   LUENGO I, 2009, PROCESADO LENGUAJE N, V43, P121
   LUENGO I, 2007, P ICASSP 07, P1057
   Luengo I., 2005, INTERSPEECH 2005, P493
   Lugger M, 2007, INT CONF ACOUST SPEE, P17
   MULLER R, 2004, P SIGN SIGNS EM VIC
   Navas E, 2004, LECT NOTES COMPUT SC, V3287, P386
   Navas E, 2006, IEEE T AUDIO SPEECH, V14, P1117, DOI 10.1109/TASL.2006.876121
   Nicholson J, 2000, NEURAL COMPUT APPL, V9, P290, DOI 10.1007/s005210070006
   Nogueiras A., 2001, Proceedings of INTERSPEECH, P2679, DOI DOI 10.21437/EUROSPEECH.2001-627
   Nwe TL, 2003, SPEECH COMMUN, V41, P603, DOI 10.1016/S0167-6393(03)00099-2
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Ramírez J, 2004, SPEECH COMMUN, V42, P271, DOI 10.1016/j.specom.2003.10.002
   Ringeval F, 2008, LECT NOTES ARTIF INT, V5042, P243
   Ruta D., 2000, Computing and Information Systems, V7, P1
   Scherer K., 2000, Neuropsychology of Emotion, V137, P137
   SCHERER KR, 1991, MOTIV EMOTION, V15, P123, DOI 10.1007/BF00995674
   Scherer KR, 2003, SPEECH COMMUN, V40, P227, DOI 10.1016/S0167-6393(02)00084-5
   SCHRODER M, 2003, THESIS U SAARLANDES
   Tato R., 2002, INT C SPOKEN LANGUAG, P2029
   van Son RJJH, 1999, SPEECH COMMUN, V28, P125, DOI 10.1016/S0167-6393(99)00009-6
   Vlasenko B, 2007, LECT NOTES COMPUT SC, V4738, P139
   VOGT T, 2006, P LREC GEN IT MAY
NR 47
TC 100
Z9 102
U1 1
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2010
VL 12
IS 6
BP 490
EP 501
DI 10.1109/TMM.2010.2051872
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 668TB
UT WOS:000283291900003
DA 2024-07-18
ER

PT J
AU Mansour, H
   Fallah, YP
   Nasiopoulos, P
   Krishnamurthy, V
AF Mansour, Hassan
   Fallah, Yaser Pourmohammadi
   Nasiopoulos, Panos
   Krishnamurthy, Vikram
TI Dynamic Resource Allocation for MGS H.264/AVC Video Transmission Over
   Link-Adaptive Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Link-adaptation; multirate wireless networks; rate-distortion modeling;
   scalable video
ID EXTENSION; SVC
AB In this paper, we address the problem of efficiently allocating network resources to support multiple scalable video streams over a constrained wireless channel. We present a resource allocation framework that jointly optimizes the operation of the link adaptation scheme in the physical layer (PHY), and that of a traffic control module in the network or medium access control (MAC) layer in multirate wireless networks, while satisfying bandwidth/capacity constraints. Multirate networks, such as IEEE 802.16 or IEEE 802.11, adjust the PHY coding and modulation schemes to maintain the reliability of transmission under varying channel conditions. Higher reliability is achieved at the cost of reduced PHY bit-rate which in turn necessitates a reduction in video stream bit-rates. The rate reduction for scalable video is implemented using a traffic control module. Conventional solutions operate unaware of the importance and loss tolerance of data and drop the higher layers of scalable video altogether. In this paper, we consider medium grain scalable (MGS) extension of H. 264/AVC video and develop new rate and distortion models that characterize the coded bitstream. Performance evaluations show that our proposed framework results in significant gains over existing schemes in terms of average video PSNR that can reach 3 dB in some cases for different channel SNRs and different bandwidth budgets.
C1 [Mansour, Hassan; Nasiopoulos, Panos; Krishnamurthy, Vikram] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
   [Fallah, Yaser Pourmohammadi] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Inst Transportat Studies, Berkeley, CA 94720 USA.
C3 University of British Columbia; University of California System;
   University of California Berkeley
RP Mansour, H (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6T 1Z4, Canada.
EM hassanm@ece.ubc.ca; yaserpf@berkeley.edu; panosn@ece.ubc.ca;
   vikramk@ece.ubc.ca
OI Pourmohammadi Fallah, Yaser/0000-0002-4920-7104
FU NSERC Strategic Project [STPGP 322075-5]
FX This work was supported under NSERC Strategic Project Grant: STPGP
   322075-5. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. S.-H. Gary Chan.
CR [Anonymous], P MILCOM
   *ANSI IEEE, 1999, 802111999E ANSIIE 11
   *ANSI IEEE, 2004, 80216 ANSIIEEE
   *ANSI IEEE, 2005, 80211E ANSIIEEE
   Erceg V., 2004, 8021103940R2 IEEE
   Foh CH, 2007, IEEE T CIRC SYST VID, V17, P1665, DOI 10.1109/TCSVT.2007.903808
   *ISO IEC JTC 1SC, 2007, N8964 ISO IEC JTC 1S
   *ITU, 2005, H264ISO ITU JVT ISOI
   Ji X, 2008, IEEE ICC, P2512, DOI 10.1109/ICC.2008.476
   Jubran MK, 2009, IEEE T IMAGE PROCESS, V18, P106, DOI 10.1109/TIP.2008.2006600
   Jubran MK, 2008, IEEE T MULTIMEDIA, V10, P1698, DOI 10.1109/TMM.2008.2007317
   Kwon DK, 2007, IEEE T CIRC SYST VID, V17, P517, DOI 10.1109/TCSVT.2007.894053
   POURMOHAMMADIFA.Y, 2008, IEEE T CIRCUITS SYST, V18, P875
   POURMOHAMMADIFA.Y, 2008, J COMPUT NETW ELSEVI, V52, P3169
   REICHEL J, 2007, N8751 ISOIEC JTC 1SC
   SCHUMACHER L, WLAN MIMO CHANNEL MA
   Schwarz H, 2006, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2006.312374
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shi YB, 2007, IEEE T CONSUM ELECTR, V53, P363, DOI 10.1109/TCE.2007.381702
   Song D, 2007, IEEE T CIRC SYST VID, V17, P1218, DOI 10.1109/TCSVT.2007.905531
   SRINIVASAN D, IEEE T WIRE IN PRESS
   Stoufs M, 2008, IEEE T CIRC SYST VID, V18, P1657, DOI 10.1109/TCSVT.2008.2004922
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Sullivan G., 2003, 9 JVT M JVT I049D0
   Zhang Q, 2004, IEEE T CIRC SYST VID, V14, P1049, DOI 10.1109/TCSVT.2004.831966
NR 25
TC 19
Z9 21
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1478
EP 1491
DI 10.1109/TMM.2009.2032682
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000008
DA 2024-07-18
ER

PT J
AU De Silva, GC
   Yamasaki, T
   Aizawa, K
AF De Silva, Gamhewage Chaminda
   Yamasaki, Toshihiko
   Aizawa, Kiyoharu
TI Sketch-Based Spatial Queries for Retrieving Human Locomotion Patterns
   From Continuously Archived GPS Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE GPS data; locomotion patterns; multimedia retrieval; sketch-based
   querying; spatial queries
AB We propose a system for retrieving human locomotion patterns from tracking data captured within a large geographical area, over a long period of time. A GPS receiver continuously captures data regarding the location of the person carrying it. A constrained agglomerative hierarchical clustering algorithm segments these data according to the person's navigational behavior. Sketches made on a map displayed on a computer screen are used for specifying queries regarding locomotion patterns. Two basic sketch primitives, selected based on a user study, are combined to form five different types of queries. We implement algorithms to analyze a sketch made by a user, identify the query, and retrieve results from the collection of data. A graphical user interface combines the user interaction strategy and algorithms, and allows hierarchical querying and visualization of intermediate results.
   We evaluate the system using a collection of data captured during nine months. The constrained hierarchical clustering algorithm is able to segment GPS data at an overall accuracy of 94% despite the presence of location-dependent noise. A user study was conducted to evaluate the proposed user interaction strategy and the usability of the overall system. The results of this study demonstrate that the proposed user interaction strategy facilitates fast querying, and efficient and accurate retrieval, in an intuitive manner.
C1 [De Silva, Gamhewage Chaminda; Yamasaki, Toshihiko; Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Aizawa Lab, Tokyo 1138656, Japan.
C3 University of Tokyo
RP De Silva, GC (corresponding author), Univ Tokyo, Dept Informat & Commun Engn, Aizawa Lab, Tokyo 1138656, Japan.
EM chamds@hal.t.u-tokyo.ac.jp; yamasaki@hal.t.u-tokyo.ac.jp;
   aizawa@hal.t.u-tokyo.ac.jp
CR Adams B., 2006, MULTIMEDIA '06, P987, DOI DOI 10.1145/1180639.1180857
   [Anonymous], 1988, Proceedings of the SIGCHI conference on Human factors in computing systems-CHI, DOI DOI 10.1145/57167.57203
   [Anonymous], 1987, USGS PROF PAP
   *CABSP, 2006, EXPL MUS SCI ART HUM
   DESILVA GC, 2007, P ACM MULT 2007 ACM, P803
   DESILVA GC, 2005, ADV MULTIMEDIA UNPUB
   Egenhofer MJ, 1997, J VISUAL LANG COMPUT, V8, P403, DOI 10.1006/jvlc.1997.0054
   GEMMELL J, 2005, P IEEE ICME 2005 JUL
   *GOOGL, 2008, GOOGL MAPS API GOOGL
   GOTTFRIED B, 2006, PERV HLTH C WORKSH A, P81
   IVANOV YA, 2006, P PERV PTA 2006 ACM, P803
   KIM IJ, 2006, P ACM CARPE 2006 OCT
   Kimber D, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P1015
   Liao L, 2007, INT J ROBOT RES, V26, P119, DOI 10.1177/0278364907073775
   MORRIS S, 2005, P INT C SIM MOD DEC, P61
   PARKINSON BW, 1983, P IEEE, V71, P1177, DOI 10.1109/PROC.1983.12745
   TANCHAROEN D, 2005, P 2 ACM WORKSH CONT, P61
   WOOD K, P UBICOMP 2004
NR 18
TC 7
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2009
VL 11
IS 7
BP 1240
EP 1253
DI 10.1109/TMM.2009.2030603
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 506GB
UT WOS:000270761300003
DA 2024-07-18
ER

PT J
AU Avramova, Z
   De Vleeschauwer, D
   Wittevrongel, S
   Bruneel, H
AF Avramova, Zlatka
   De Vleeschauwer, Danny
   Wittevrongel, Sabine
   Bruneel, Herwig
TI Capacity Gain of Mixed Multicast/Unicast Transport Schemes in a TV
   Distribution Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Capacity planning; digital TV/video; IPTV; mobile TV; multicast;
   streaming; switched broadcast; unicast
ID BROADCAST
AB This paper presents three approaches to estimate the required resources in an infrastructure where digital TV channels can be delivered in unicast or multicast (broadcast) mode. Such situations arise for example in Cable TV, IPTV distribution networks or in (future) hybrid mobile TV networks.
   The three approaches presented are an exact calculation, a Gaussian approximation and a simulation tool. We investigate two scenarios that allow saving bandwidth resources. In a static scenario, the most popular channels are multicast and the less popular channels rely on unicast. In a dynamic scenario, the list of multicast channels is dynamic and governed by the users' behavior. We prove that the dynamic scenario always outperforms the static scenario.
   We demonstrate the robustness, versatility and the limits of our three approaches. The exact calculation application is limited because it is computationally expensive for cases with large numbers of users and channels, while the Gaussian approximation is good exactly for such systems. The simulation tool takes long to yield results for small blocking probabilities.
   We explore the capacity gain regions under varying model parameters. Finally, we illustrate our methods by discussing some realistic network scenarios using channel popularities based on measurement data as much as possible.
C1 [Avramova, Zlatka; Wittevrongel, Sabine; Bruneel, Herwig] Univ Ghent, Stochast Modeling & Anal Commun Syst Res Grp, Dept Telecommun & Informat Proc, Fac Engn, B-9000 Ghent, Belgium.
   [De Vleeschauwer, Danny] Alcatel Lucent Bell, Bell Labs, Antwerp, Belgium.
C3 Ghent University; Alcatel-Lucent
RP Avramova, Z (corresponding author), Univ Ghent, Stochast Modeling & Anal Commun Syst Res Grp, Dept Telecommun & Informat Proc, Fac Engn, B-9000 Ghent, Belgium.
EM kayzlat@telin.ugent.be; Danny.De_Vleeschauwer@alcatel-lucent.be;
   sw@telin.ugent.be; hb@telin.ugent.be
RI De Vleeschauwer, Danny/J-6432-2019
OI De Vleeschauwer, Danny/0000-0002-0718-8048
CR AALTONEN J, 2003, THESIS TAMPERE U TEC
   ADAMS M, SWITCHED VIDEO SEVER
   AVRAMOVA Z, 2006, P QUAL SERV INT WORK, P137
   AVRAMOVA Z, 2007, P INT TEL C MAN TRAF, P6
   Deering S, 1989, 1112 IETF RFC
   *DVB, 2005, 102401 ETSI TR
   *DVB, 2006, 102468V1111 ETSI TS
   *DVB H, DVB H TRIAL BERL PAR
   *EBU, 2008, 3327 EBU
   Hartung F, 2007, IEEE T BROADCAST, V53, P188, DOI 10.1109/TBC.2007.891711
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   HEUCK C, 2005, P 14 IST MOB WIR COM
   Kornfeld M, 2007, IEEE T BROADCAST, V53, P161, DOI 10.1109/TBC.2006.889210
   Lohmar T, 2006, C LOCAL COMPUT NETW, P850
   *MBMS, 3GPP TS 2X 46 STAND
   *MBMS, 3GPP TS23246
   NGUYEN J, 2007, EVOLVING SWITCHED BR
   Rosenwasser LJ, 1999, AM J RESP CELL MOL, V21, P4, DOI 10.1165/ajrcmb.21.1.f156
   SINHA N, 2005, P SCTE 2005 C EM TEC
   Unger P., 2005, P 14 IST MOB WIR COM
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   Wauters T, 2007, IEEE T BROADCAST, V53, P588, DOI 10.1109/TBC.2007.894950
   YATES R, 1999, PROBABILITY STOCHAST, P247
   Zipf J. K., 1932, SELECTIVE STUDIES PR
NR 24
TC 10
Z9 13
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 918
EP 931
DI 10.1109/TMM.2009.2021806
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300011
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhou, XM
   Zhou, XF
   Chen, L
   Bouguettaya, A
   Xiao, N
   Taylor, JA
AF Zhou, Xiangmin
   Zhou, Xiaofang
   Chen, Lei
   Bouguettaya, Athman
   Xiao, Nong
   Taylor, John A.
TI An Efficient Near-Duplicate Video Shot Detection Method Using Shot-Based
   Interest Points
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frame set; near-duplicate shots; shot consistency; shot interest point
ID IMAGE; DISTANCE
AB We propose a shot-based interest point selection approach for effective and efficient near-duplicate search over a large collection of video shots. The basic idea is to eliminate the local descriptors with lower frequencies among the selected video frames from a shot to ensure that the shot representation is compact and discriminative. Specifically, we propose an adaptive frame selection strategy called furthest point voronoi (FPV) to produce the shot frame set according to the shot content and frame distribution. We describe a novel strategy named reference extraction (RE) to extract the shot interest descriptors from a keyframe with the support of the selected frame set. We demonstrate the effectiveness and efficiency of the proposed approaches with extensive experiments.
C1 [Zhou, Xiangmin; Bouguettaya, Athman; Taylor, John A.] CSIRO, ICT Ctr, Canberra, ACT, Australia.
   [Zhou, Xiaofang] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld, Australia.
   [Chen, Lei] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Xiao, Nong] Natl Univ Def Technol, Changsha, Hunan, Peoples R China.
C3 Commonwealth Scientific & Industrial Research Organisation (CSIRO);
   University of Queensland; Hong Kong University of Science & Technology;
   National University of Defense Technology - China
RP Zhou, XM (corresponding author), CSIRO, ICT Ctr, Canberra, ACT, Australia.
EM xiangmin.zhou@csiro.au; zxf@itee.uq.edu.au; le-ichen@cse.ust.hk;
   Athman.Bouguettaya@csiro.au; nongxiao@nudt.edu.cn;
   John.A.Taylor@csiro.au
RI Zhou, Xiangmin/B-4341-2011; Zhou, Xiangfeng/KDO-8724-2024; Zhou,
   Xiaofang/C-6169-2013; Chen, Lei/HMD-2646-2023; Bouguettaya,
   Athman/B-7515-2011; Taylor, John/E-5894-2010
OI Zhou, Xiaofang/0000-0001-6343-1455; Chen, Lei/0000-0003-3718-9268;
   Bouguettaya, Athman/0000-0003-1254-8092; Taylor,
   John/0000-0001-9003-4076; Chen, Lei/0000-0002-8257-5806
FU ARC [DP0663272, LE0668542]; Hong Kong RGC [611608]; NSFC [60736013];
   Australian Research Council [LE0668542] Funding Source: Australian
   Research Council
FX This work was supported in part by ARC Grants (DP0663272 and LE0668542)
   and in part by Hong Kong RGC Grants under Project 611608, NSFC Key
   Project Grant 60736013.
CR [Anonymous], 2006, P 8 ACM INT WORKSHOP
   [Anonymous], 2007, P 6 ACM INT C IM VID
   Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014
   Cheung SCS, 2005, IEEE T MULTIMEDIA, V7, P524, DOI 10.1109/TMM.2005.846906
   Chum O., 2008, P BMVC
   CHUM O, 2007, P 6 ACM INT C IM VID, P549
   Foo J., 2007, P 18 C AUSTRALASIAN, P63
   Grauman K, 2005, PROC CVPR IEEE, P627
   Hoi S.C.H., 2006, TRECVID
   Hua XS, 2004, IEEE IMAGE PROC, P685
   Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876
   Joly A, 2007, IEEE T MULTIMEDIA, V9, P293, DOI 10.1109/TMM.2006.886278
   Ke Y, 2004, PROC CVPR IEEE, P506
   Ke Y., 2004, ACM MULTIMEDIA 04, P869
   Koprinska I., 2001, TEMPORAL VIDEO SEGME
   Laptev I, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P432, DOI 10.1109/iccv.2003.1238378
   Law-To J., 2006, P ACM INT C MULTIMED, P835
   LEJSEK H, 2006, P 14 ANN ACM INT C M, P589
   LIU C, 2006, TRECVID
   LIU Z, 2006, P TRECVID
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   MOREELS P, 2005, P ICCV, V1, P800
   Qamra A, 2005, IEEE T PATTERN ANAL, V27, P379, DOI 10.1109/TPAMI.2005.54
   Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054
   SCHRIJVER A, 2003, COMBINATORIAL OPTIMI, VA, P267
   Shen H. T., 2007, P ACM MULT C, P164
   Shen H.T., 2005, P ACM SIGMOD INT C M, P730
   Shen H.T., 2007, VLDB, P1374
   SIVIC J, 2003, P ICCV 02
   Wu X., 2007, ACM MULTIMEDIA 07, P168
   Yan Y, 2008, PROC INT CONF DATA, P853, DOI 10.1109/ICDE.2008.4497494
   ZHANG D.-Q., 2004, PROC ACM INT C MULTI, P877, DOI DOI 10.1145/1027527.1027730
   Zhao WL, 2007, IEEE T MULTIMEDIA, V9, P1037, DOI 10.1109/TMM.2007.898928
   Zhou XM, 2007, LECT NOTES COMPUT SC, V4505, P176
   [No title captured]
NR 35
TC 34
Z9 36
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2009
VL 11
IS 5
BP 879
EP 891
DI 10.1109/TMM.2009.2021794
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 474KN
UT WOS:000268282300008
DA 2024-07-18
ER

PT J
AU Tu, WQ
   Jin, X
   Excell, PS
AF Tu, Wanqing
   Jin, Xing
   Excell, Peter S.
TI Performance Analysis for Overlay Multimedia Multicast on <i>r</i>-ary
   Tree and <i>m</i>-D Mesh Topologies
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE m-D mesh-based overlay multicast; overlay multimedia multicast;
   performance analysis; r-ary tree-based overlay multicast
AB Without requiring multicast support from the underlying networks, overlay multicast has the advantage of implementing inter-domain multimedia multicast communications. Usually, overlay multicast protocols employ two different topologies: r-ary tree and m-D mesh. In this paper, we study the influence of topology selection on multimedia multicast performance. We present a set of theoretical results on the worst performance, the average performance, and the performance difference along the link stress, the number of overlay hops, and the number of shortest paths for r-ary tree-based and m-D mesh-based multicast, respectively. Furthermore, through simulations in NS2, we observe and compare tree and mesh topologies along the metrics analyzed theoretically. Simulation results match our theoretical analyses. Finally we give our evaluations of and insights into these two kinds of multicast when used to transmit multimedia streams. The selection of overlay topology is application dependent. To the best of our knowledge, this is the first evaluation of multimedia multicast performances in different overlay topologies. We believe that this study is useful for protocol design of target multimedia applications and for investigating multicast functions.
C1 [Tu, Wanqing; Excell, Peter S.] Glyndwr Univ, Sch Comp & Commun Technol, Wrexham, N East Wales, Wales.
   [Jin, Xing] Oracle USA Inc, Syst Technol Grp, Redwood Shores, CA 94065 USA.
C3 Glyndwr University; Oracle
RP Tu, WQ (corresponding author), Glyndwr Univ, Sch Comp & Commun Technol, Wrexham, N East Wales, Wales.
EM w.tu@glyndwr.ac.uk; xing.jin@oracle.com; p.excell@glyndwr.ac.uk
OI Tu, Wanqing/0000-0002-0849-6392
CR ADJIH C, 2002, P SODA
   Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Banerjee S, 2003, IEEE INFOCOM SER, P1521
   CASTRO M, 2002, IEEE JSAC, V20, P8
   Chawathe Y., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P795, DOI 10.1109/INFCOM.2000.832254
   Chu Y., 2001, P ACM SIGCOMM, P55
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   CHUANG J, 1998, P INT SOC INET JUL
   FAHMY S, 2003, P ICNP
   Francis P., 2000, Yoid: Extending the Internet Multicast Architecture
   JANNOTTI J, 2000, P 4 US S OP SYST DES, P194
   Pendarakis D, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P49
   Phillips G, 1999, COMP COMM R, V29, P41, DOI 10.1145/316194.316205
   Ratnasamy S., 2001, Proceedings of the 2001 conference on applications, technologies, architectures, and protocols for computer communications, P161
   RATNASAMY S, 2001, P 3 INT WORKSH NETW, P14
   Riabov A, 2004, INT CON DISTR COMP S, P654, DOI 10.1109/ICDCS.2004.1281633
   Rowstron Antony., 2001, PASTRY SCALABLE DIST
   Shi SY, 2002, IEEE INFOCOM SER, P1200, DOI 10.1109/INFCOM.2002.1019370
   TU W, 2007, P ICC
   TU W, 2004, P IEEE GLOB TEL C 20
   TU W, 2005, END HOST MULTICAST A, pCH3
   TU W, 2005, P 30 ANN IEEE C LOC
   Zhang BC, 2002, IEEE INFOCOM SER, P1366, DOI 10.1109/INFCOM.2002.1019387
NR 23
TC 3
Z9 4
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2009
VL 11
IS 4
BP 696
EP 706
DI 10.1109/TMM.2009.2017623
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 449LA
UT WOS:000266330900011
DA 2024-07-18
ER

PT J
AU Kannangara, CS
   Richardson, IE
   Bystrom, M
   Zhao, YF
AF Kannangara, Chaminda Sampath
   Richardson, Iain E.
   Bystrom, Maja
   Zhao, Yafan
TI Complexity Control of H.264/AVC Based on Mode-Conditional Cost
   Probability Distributions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayes decision theory; computational complexity control; H.264
ID RATE-DISTORTION; SELECTION
AB A computational complexity control algorithm is proposed for an H.264 encoder running on a processor/power constrained platform. This new computational complexity control algorithm is based on a macroblock mode prediction algorithm that employs a Bayesian framework for accurate early skip decision. Complexity control is achieved by relaxing the Bayesian maximum-likelihood (ML) criterion in order to match the mode decision threshold to a target complexity level. A feedback algorithm is used to maintain the performance of the algorithm with respect to achieving an average target complexity level, reducing frame by frame complexity variance and optimizing rate-distortion performance. Experimental results show that this algorithm can effectively, control the encoding computational complexity while maintaining a good rate-distortion performance at a range of target complexity levels.
C1 [Kannangara, Chaminda Sampath; Richardson, Iain E.; Zhao, Yafan] Robert Gordon Univ, Sch Engn, Aberdeen AB10 1FR, Scotland.
   [Bystrom, Maja] Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA.
C3 Robert Gordon University; Boston University
RP Kannangara, CS (corresponding author), Robert Gordon Univ, Sch Engn, Aberdeen AB10 1FR, Scotland.
EM s.kannangara@rgu.ac.uk; i.richardson@rgu.ac.uk; bystrom@bu.edu;
   y.zhao@rgu.ac.uk
OI Zhao, Yafan/0000-0002-3238-7567
FU EPSRC [EP/E027024/1] Funding Source: UKRI
CR AKYOL E, 2007, P IEEE INT C IM PROC
   Al Qaralleh EA, 2006, IEEE T CIRC SYST VID, V16, P1021, DOI 10.1109/TCSVT.2006.879103
   [Anonymous], 1995, MULTIPLE ATTRIBUTE D
   Ates HF, 2008, IEEE T CIRC SYST VID, V18, P159, DOI 10.1109/TCSVT.2008.918114
   Bjontegaard G., 2001, Document VCEG-M33
   Bystrom M, 2008, SIGNAL PROCESS-IMAGE, V23, P71, DOI 10.1016/j.image.2007.11.001
   CHEN J, 2004, P PICT COD S SAN FRA
   Chen LC, 2002, J SOLID STATE ELECTR, V7, P6, DOI 10.1007/s10008-002-0272-9
   Choi I, 2006, IEEE T CIRC SYST VID, V16, P1557, DOI 10.1109/TCSVT.2006.883506
   Dai QH, 2004, IEEE IMAGE PROC, P119
   He ZH, 2005, IEEE T CIRC SYST VID, V15, P645, DOI 10.1109/TCSVT.2005.846433
   *INT TEL UN TEL ST, 2003, H264 INT TEL UN TEL
   KANNANGARA CS, 1964, IEEE T CIRC IN PRESS
   KANNANGRA CS, 2006, P PCS 2006 BEIJ CHIN, V24
   KAPOTAS SK, 2007, PICT COD S 200 UNPUB
   Kim C, 2007, IEEE T CIRC SYST VID, V17, P441, DOI 10.1109/TCSVT.2006.888829
   KIM H, 2004, INT C IM PROC UNPUB
   LIM KP, 2003, P SAN DIEG ISO IEC M
   Ostermann J., 2004, IEEE Circuits and Systems Magazine, V4, P7, DOI 10.1109/MCAS.2004.1286980
   PAN F, 2003, P ISO IEC MPEG ITU T
   RICHARDSON IEG, 2006, P PCS 2006 BEIJ CHIN
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Winitzki S., 2006, HANDY APPROXIMATION
   YI X, 2005, P JOINT VIDEOTEAM JV
   YU AC, 2004, INT C IM PROC UNPUB
   Zhang H, 2007, J DATABASE MANAGE, V18, P1, DOI 10.4018/jdm.2007010101
   Zhao Y., 2006, P ICIP2006 ATL GA OC
NR 27
TC 19
Z9 20
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 433
EP 442
DI 10.1109/TMM.2009.2012937
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300009
DA 2024-07-18
ER

PT J
AU Zhu, WT
AF Zhu, Wen Tao
TI Collision Attacks With Budget Constraints on Key Management Schemes for
   Secure Multimedia Multicast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collision attack; digital rights management; key management; multimedia
   multicast
ID BROADCAST ENCRYPTION
AB We address the problem of distributing a confidentially shared session key to a multimedia multicast group for content protection. In two such schemes proposed by Trappe et al., the session key is distributed by employing a homogenized rekey message format. We show that their rekey algorithm in itself is vulnerable to specialized collision attacks, in which even a completely passive outer adversary, who never joins the system and thus never knows any secret user keys, can still reveal the session key of the multimedia multicast with an observable probability but only involving a time complexity far lower than an exhaustive search.
C1 Chinese Acad Sci, Grad Univ, State Key Lab Informat Secur, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Zhu, WT (corresponding author), Chinese Acad Sci, Grad Univ, State Key Lab Informat Secur, Beijing 100049, Peoples R China.
EM wtzhu@ieee.org
FU National Natural Science Foundation of China (NSFC) [60503046]; National
   High Technology Research and Development (863) Program of China
   [2006AA01Z437]
FX Manuscript received March 24, 2008: revised October 20, 2008. First
   published February 20, 2009 current version published March 18, 2009.
   This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 60503046 and in part by the
   National High Technology Research and Development (863) Program of China
   under Grant 2006AA01Z437. The associate editor coordinating the review
   of this manuscript and approving it for publication was Prof. Mohan S.
   Kankanhalli.
CR [Anonymous], 2001, LECT NOTES COMPUTER
   CHIOU GH, 1989, IEEE T SOFTWARE ENG, V15, P929, DOI 10.1109/32.31350
   Lotspiech J, 2004, P IEEE, V92, P898, DOI 10.1109/JPROC.2004.827353
   Mihaljevic MJ, 2007, IEEE COMMUN LETT, V11, P988, DOI 10.1109/LCOMM.2007.071029
   Trappe W, 2003, IEEE T MULTIMEDIA, V5, P544, DOI 10.1109/TMM.2003.813279
   Wong CK, 2000, IEEE ACM T NETWORK, V8, P16, DOI 10.1109/90.836475
   ZHU WT, 2008, P IEEE INT C COMM IC, P1620
NR 7
TC 3
Z9 5
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2009
VL 11
IS 3
BP 556
EP 561
DI 10.1109/TMM.2009.2012920
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 425JH
UT WOS:000264632300020
DA 2024-07-18
ER

PT J
AU Cui, P
   Sun, LF
   Wang, F
   Yang, SQ
AF Cui, Pena
   Sun, Li-Feng
   Wang, Fei
   Yang, Shi-Qiano
TI Contextual Mixture Tracking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Contextual observation model; contextual proposal distribution; motion
   tracking; particle filter
ID VARIABLE NUMBER; OBJECT TRACKING; FILTER
AB Multiple Object Tracking (MOT) poses three challenges to conventional well-studied Single Object Tracking (SOT) algorithms: 1) Multiple targets lead the configuration space to be exponential to the number of targets; 2) Multiple motion conditions due to multiple targets' entering, exiting and intersection make the prediction process degrade in precision; 3) Visual ambiguities among nearby targets make the trackers error prone. In this paper, we address the MOT problem by embedding contextual proposal distributions and contextual observation models into a mixture tracker which is implemented in a Particle Filter framework. The proposal distributions are adaptively selected by motion conditions of targets which are determined by context information, and the multiple features are combined according to their discriminative power between ambiguity prone objects. The induction of contextual proposal distribution and observation model can help to surmount the incapability of conventional mixture tracker in handling object occlusions, meanwhile retain its merits of flexibility and high efficiency. The final experiments show significant improvement in variable number objects tracking scenarios compared with other methods.
C1 [Cui, Pena; Sun, Li-Feng; Yang, Shi-Qiano] Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
   [Wang, Fei] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Cui, P (corresponding author), Tsinghua Univ, Dept Comp Sci, Beijing 100084, Peoples R China.
EM cuip05@mails.ts-inghua.edu.cn; sunlf@mail.tsinghua.edu.cn;
   yangshq@mail.tsinghua.edu.cn
RI Wang, Fei/HRA-7319-2023
OI Wang, Fei/0000-0001-9459-9461
FU National Natural Science Foundation of China [60573167]; National
   High-Tech Research and Development Plan of China [2006AA01Z 118]; the
   National Basic Research Program of China [2006CB303103]
FX This work was supported by National Natural Science Foundation of China
   (No. 60573167); the National High-Tech Research and Development Plan of
   China (No. 2006AA01Z 118); and the National Basic Research Program of
   China (No. 2006CB303103).
CR Angelova D, 2008, IEEE T SIGNAL PROCES, V56, P825, DOI 10.1109/TSP.2007.907851
   Cai Yizheng., 2006, ECCV
   Collins RT, 2005, IEEE T PATTERN ANAL, V27, P1631, DOI 10.1109/TPAMI.2005.205
   DECLERCQ A, 2007, IEEE C COMP VIS PATT, P1
   Guyon Isabelle, 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616
   Jepson AD, 2001, PROC CVPR IEEE, P415
   Khan Z, 2005, IEEE T PATTERN ANAL, V27, P1805, DOI 10.1109/TPAMI.2005.223
   Lanz O, 2006, IEEE T PATTERN ANAL, V28, P1436, DOI 10.1109/TPAMI.2006.177
   LI Y, 2007, P CVPR
   Moeslund TB, 2006, COMPUT VIS IMAGE UND, V104, P90, DOI 10.1016/j.cviu.2006.08.002
   Morelande MR, 2007, IEEE T SIGNAL PROCES, V55, P1589, DOI 10.1109/TSP.2006.889470
   Nguyen HT, 2007, IEEE T PATTERN ANAL, V29, P52, DOI 10.1109/TPAMI.2007.250599
   Nguyen HT, 2004, IEEE T PATTERN ANAL, V26, P1099, DOI 10.1109/TPAMI.2004.45
   Okuma K, 2004, LECT NOTES COMPUT SC, V3021, P28, DOI 10.1007/978-3-540-24670-1_3
   POLAT E, 2003, COMPUTER VISION IMAG, V89
   Qu W, 2007, IEEE T MULTIMEDIA, V9, P511, DOI 10.1109/TMM.2006.886266
   Smets P, 2007, INFORM FUSION, V8, P16, DOI 10.1016/j.inffus.2005.06.004
   SMITH K, 2005, USING PARTICLES TRAC, P962
   Stenger B, 2006, IEEE T PATTERN ANAL, V28, P1372, DOI 10.1109/TPAMI.2006.189
   Subakan Ö, 2007, IEEE I CONF COMP VIS, P708
   Vermaak J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1110
   WANG J, 2005, ONLINE SELECTING DIS, P1037
   Wang YD, 2006, INT C PATT RECOG, P1127
   YU T, 2005, P SPIE, V5682
   Zhao T., 2004, Computer Vision and Pattern Recognition
   Zhou SHK, 2004, IEEE T IMAGE PROCESS, V13, P1491, DOI 10.1109/TIP.2004.836152
NR 26
TC 13
Z9 13
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2009
VL 11
IS 2
SI SI
BP 333
EP 341
DI 10.1109/TMM.2008.2009722
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EG
UT WOS:000262714800013
DA 2024-07-18
ER

PT J
AU Zhu, GY
   Xu, CS
   Huang, QM
   Rui, Y
   Jiang, SQ
   Gao, W
   Yao, HX
AF Zhu, Guangyu
   Xu, Changsheng
   Huang, Qingming
   Rui, Yong
   Jiang, Shuqiang
   Gao, Wen
   Yao, Hongxun
TI Event Tactic Analysis Based on Broadcast Sports Video
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Event detection; object tracking; trajectory analysis; tactic analysis;
   sports video analysis
ID SOCCER VIDEO; TRACKING
AB Most existing approaches on sports video analysis have concentrated on semantic event detection. Sports professionals, however, are more interested in tactic analysis to help improve their performance. In this paper, we propose a novel approach to extract tactic information from the attack events in broadcast soccer video and present the events in a tactic mode to the coaches and sports professionals. We extract the attack events with far-view shots using the analysis and alignment of web-casting text and broadcast video. For a detected event, two tactic representations, aggregate trajectory and play region sequence, are constructed based on multi-object trajectories and field locations in the event shots. Based on the multi-object trajectories tracked in the shot, a weighted graph is constructed via the analysis of temporal-spatial interaction among the players and the ball. Using the Viterbi algorithm, the aggregate trajectory is computed based on the weighted graph. The play region sequence is obtained using the identification of the active field locations in the event based on line detection and competition network. The interactive relationship of aggregate trajectory with the information of play region and the hypothesis testing for trajectory temporal-spatial distribution are employed to discover the tactic patterns in a hierarchical coarse-to-fine framework. Extensive experiments on FIFA World Cup 2006 show that the proposed approach is highly effective.
C1 [Zhu, Guangyu; Gao, Wen; Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
   [Xu, Changsheng] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100080, Peoples R China.
   [Xu, Changsheng] China Singapore Inst Digital Media, Singapore, Singapore.
   [Huang, Qingming] Chinese Acad Sci, Grad Sch, Beijing 100039, Peoples R China.
   [Rui, Yong] Microsoft China R&D CRD Grp, Beijing 100080, Peoples R China.
   [Jiang, Shuqiang] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
C3 Harbin Institute of Technology; Chinese Academy of Sciences; Institute
   of Automation, CAS; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Microsoft; Chinese Academy of Sciences; Peking
   University
RP Zhu, GY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Peoples R China.
EM gyzhu@jdl.ac.cn; csxu@nlpr.ia.ac.cn; qmhuang@jdl.ac.cn;
   yongrui@microsoft.com; sqjiang@jdl.ac.cn; wgao@jdl.ac.cn;
   yhx@vilab.hit.edu.cn
RI Huang, Qingming/GLR-3473-2022; xu, cj/HJZ-3488-2023; Zhu,
   Guangyu/H-3805-2013
OI Huang, Qingming/0000-0002-3025-7099; 
FU National Natural Science Foundation of China [60773136, 60702035];
   National Hi-Tech Development Program (863 Program) of China
   [2006AA01Z117]; Chinese Academy of Sciences [99T3002T03]
FX Manuscript received September 05, 2007; revised February 04, 2008.
   Current version published January 08, 2009. This work was supported in
   part by National Natural Science Foundation of China under Grants
   60773136 and 60702035, in part by National Hi-Tech Development Program
   (863 Program) of China under Grant 2006AA01Z117, and in part by
   "Science100 Program" of Chinese Academy of Sciences under Grant
   99T3002T03. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Alan Hanjalic.
CR [Anonymous], 1968, MATH HDB SCI ENG
   [Anonymous], 1995, P IEEE INT C MULT CO
   [Anonymous], 2007, P 15 ACM INT C MULTI
   Assfalg E, 2003, COMPUT VIS IMAGE UND, V92, P285, DOI 10.1016/j.cviu.2003.06.004
   Babaguchi N, 2004, IEEE T MULTIMEDIA, V6, P575, DOI [10.1109/TMM.2004.830811, 10.1109/tmm.2004.830811]
   Babaguchi N, 2002, IEEE T MULTIMEDIA, V4, P68, DOI 10.1109/6046.985555
   Cai YZ, 2006, LECT NOTES COMPUT SC, V3954, P107
   CHIN J, 1998, P SIGCHI HUM FACT CS, P213
   Duan L.Y., 2003, Proc. ACM Int. Conf. Multimedia, P33
   Dufaux F, 2000, IEEE T IMAGE PROCESS, V9, P497, DOI 10.1109/83.826785
   Ekin A, 2003, IEEE T IMAGE PROCESS, V12, P796, DOI 10.1109/TIP.2003.812758
   Han M., 2002, Proc. ACM Multimedia, P347, DOI [DOI 10.1145/641007.641081, 10.1145/641007.641081]
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P1114, DOI 10.1109/TMM.2005.858397
   Hirano S, 2004, LECT NOTES ARTIF INT, V3202, P209
   Kang CH, 2006, ICDM 2006: Sixth IEEE International Conference on Data Mining, Workshops, P377
   LI Y, 2006, P INT C PATT REC HON, V4, P128
   Liang DW, 2005, LECT NOTES COMPUT SC, V3767, P864
   Liu Y, 2005, INT CONF ACOUST SPEE, P421
   Okuma K., 2004, PROC EUROPEAN C COMP, V1, P28
   Pingali GS, 1998, PROC CVPR IEEE, P260, DOI 10.1109/CVPR.1998.698618
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rui Y, 2001, PROC CVPR IEEE, P786
   Rui Y., 2000, Proceedings ACM Multimedia 2000, P105, DOI 10.1145/354384.354443
   SUDHIR G, 1999, P IEEE INT WORKSH CO, P81
   Taki T, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P815, DOI 10.1109/ICIP.1996.560865
   Vapnik V., 1999, NATURE STAT LEARNING
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang J., 2004, P 12 ANN ACM INT C M, P32
   Wang JJ, 2007, IEEE T MULTIMEDIA, V9, P576, DOI 10.1109/TMM.2006.888013
   Wang JR, 2005, 11TH INTERNATIONAL MULTIMEDIA MODELLING CONFERENCE, PROCEEDINGS, P102, DOI 10.1109/MMMC.2005.20
   WANG P, 2004, P PAC RIM C MULT, P49
   Xie LX, 2004, PATTERN RECOGN LETT, V25, P767, DOI 10.1016/j.patrec.2004.01.005
   Xu C., 2006, PROC 14 ANN ACM INT, P221, DOI DOI 10.1145/1180639.1180699
   Xu H, 2006, ACM T MULTIM COMPUT, V2, P44, DOI 10.1145/1126004.1126007
   Yu XG, 2006, IEEE T MULTIMEDIA, V8, P1164, DOI 10.1109/TMM.2006.884621
   Zhu G., 2006, Proc. ACM Multimedia, P431, DOI [DOI 10.24963/IJCAI.2018/227, DOI 10.1145/1180639.1180728, 10.1145/1180639.1180728]
   Zhu GY, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1629, DOI 10.1109/ICME.2006.262859
NR 38
TC 75
Z9 81
U1 2
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2009
VL 11
IS 1
BP 49
EP 67
DI 10.1109/TMM.2008.2008918
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 398EF
UT WOS:000262714700005
DA 2024-07-18
ER

PT J
AU Rezaei, M
   Bouazizi, I
   Gabbouj, M
AF Rezaei, Mehdi
   Bouazizi, Imed
   Gabbouj, Moncef
TI Joint Video Coding and Statistical Multiplexing for Broadcasting Over
   DVB-H Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Channel switching; delay; DVB-H (digital video broadcasting-handhelds);
   fuzzy control; joint rate control; mobile TV; statistical multiplexing;
   video coding
ID ALLOCATION
AB A novel joint video encoding and statistical multiplexing (StatMux) method for broadcasting over digital video broadcasting for handhelds (DVB-H) channels is proposed to improve the quality of encoded video and to decrease the end-to-end delay in a broadcast system. The main parts of end-to-end delay in a DVB-H system result from a time-sliced transmission scheme that is used in DVB-H and from the bit rate variations of service bit streams. The time-sliced transmission scheme is utilized in DVB-H to reduce the power consumption of DVB-H receivers. Variable bit rate (VBR) video bit streams are used in DVB-H to improve the video quality and compression performance. The time-sliced transmission scheme has increased the channel switching delay, i.e., switching to a new audio-visual service, in DVB-H. The used VBR bit streams increase the required buffering delays in the whole system. The different parts of end-to-end delay in a DVB-H system can be affected by the used video encoding and multiplexing methods. Different scenarios for encoding and StatMux of video sources for DVB-H application are studied in this paper. Moreover, a new method for jointly encoding and StatMux of video sources is proposed that not only decreases the end-to-end delay but also improves the average quality of compressed video by dynamically distributing available bandwidth between the video sources according to their relative complexity. Performance of the proposed method is validated by simulation results.
C1 [Rezaei, Mehdi; Gabbouj, Moncef] Tampere Univ Technol, Inst Signal Proc, Dept Signal Proc, FI-33720 Tampere, Finland.
   [Bouazizi, Imed] Nokia Res Ctr, FI-33720 Tampere, Finland.
C3 Tampere University; Nokia Corporation; Siemens AG; Nokia Siemens
   Networks; Nokia Finland
RP Rezaei, M (corresponding author), Tampere Univ Technol, Inst Signal Proc, Dept Signal Proc, FI-33720 Tampere, Finland.
EM mehdi.rezaei@ieee.org; Imed.Bouazizi@nokia.com; Moncef.Gabbouj@tut.fi
RI Gabbouj, Moncef/G-4293-2014; Rezaei, Mehdi/HPC-0221-2023
OI Gabbouj, Moncef/0000-0002-9788-2323; Rezaei, Mehdi/0000-0002-6918-9767
FU Nokia; Academy of Finland [213462]; Academy of Finland (AKA) [213462]
   Funding Source: Academy of Finland (AKA)
FX Manuscript received January 22, 2007: revised March 24, 2008. Current
   version published December 10, 2008. This work was supported in part by
   Nokia and the Academy of Finland under Project 213462 (Finnish Centre of
   Excellence program 2006-2011). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Yo-Sung Ho.
CR [Anonymous], 2004, 302304 EN ETSI
   Böröczky L, 2000, IEEE T CIRC SYST VID, V10, P1159, DOI 10.1109/76.875519
   BOUAZIZI I, 2006, P 56 ANN IEEE BROADC
   *ETSI, 2006, 102472 TS ETSI
   Faria G, 2006, P IEEE, V94, P194, DOI 10.1109/JPROC.2005.861011
   KOO I, 1999, P IEEE INT C AC SPEE, V4, P2227
   Lakshman TV, 1998, P IEEE, V86, P952, DOI 10.1109/5.664282
   REZAEI M, 2006, P IEEE INT C INT INF
   REZAEI M, 2005, P IEEE INT S PERS IN
   REZAEI M, 2006, P IEEE INT C AC SPEE
   Rezaei M, 2007, IEEE T BROADCAST, V53, P320, DOI 10.1109/TBC.2006.889682
   SULLIVAN G, 2003, JOINT VID TEAM JVT I
   Wang L.-X., 1993, IEEE Transactions on Fuzzy Systems, V1, P146, DOI 10.1109/91.227383
   Wang L. X., 1994, Adaptive Fuzzy Systems and Control: Design and Stability Analysis
   Wang LM, 1999, IEEE T CIRC SYST VID, V9, P949, DOI 10.1109/76.785733
   Xiong HK, 2004, IEEE T CONSUM ELECTR, V50, P849, DOI 10.1109/TCE.2004.1341690
   ZADEH LA, 1994, IEEE SOFTWARE, V11, P48, DOI 10.1109/52.329401
   ZADEH LA, 1988, IEEE COMPUT MAG, V21
   Zhang XM, 2003, IEEE T CIRC SYST VID, V13, P121, DOI 10.1109/TCSVT.2002.808437
   [No title captured]
NR 20
TC 17
Z9 24
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2008
VL 10
IS 8
BP 1455
EP 1464
DI 10.1109/TMM.2008.2007315
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 390KS
UT WOS:000262163600003
DA 2024-07-18
ER

PT J
AU Gualdi, G
   Prati, A
   Cucchiara, R
AF Gualdi, Giovanni
   Prati, Andrea
   Cucchiara, Rita
TI Video Streaming for Mobile Video Surveillance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE H.264 coding; mobile video surveillance; video streaming
AB Mobile video surveillance represents a new paradigm that encompasses, on the one side, ubiquitous video acquisition and, on the other side, ubiquitous video processing and viewing, addressing both computer-based and human-based surveillance. To this aim, systems must provide efficient video streaming with low latency and low frame skipping, even over limited bandwidth networks. This work presents MoSES (MObile Streaming for vidEo Surveillance), an effective system for mobile video surveillance for both PC and PDA clients; it relies over H.264/AVC video coding and GPRS/EDGE-GPRS network. Adaptive control algorithms are employed to achieve the best tradeoff between low latency and good video fluidity. MoSES provides a good-quality video streaming that is used as input to computer-based video surveillance applications for people segmentation and tracking. In this paper new and general-purpose methodologies for streaming performance evaluation are also proposed and used to compare MoSES with existing solutions in terms of different parameters (latency, image quality, video fluidity, and frame losses), as well as in terms of performance in people segmentation and tracking.
C1 [Gualdi, Giovanni; Cucchiara, Rita] Univ Modena & Reggio Emilia, Dipartimento Ingn Informaz, I-41100 Modena, Italy.
   [Prati, Andrea] Univ Modena & Reggio Emilia, Dipartimento Sci & Metodi Ingn, I-41100 Modena, Italy.
C3 Universita di Modena e Reggio Emilia; Universita di Modena e Reggio
   Emilia
RP Gualdi, G (corresponding author), Univ Modena & Reggio Emilia, Dipartimento Ingn Informaz, I-41100 Modena, Italy.
EM giovanni.gualdi@unimore.it; an-drea.prati@unimore.it;
   rita.cucchiara@unimore.it
RI Prati, Andrea/B-7440-2014; Cucchiara, Rita/L-3006-2015
OI Prati, Andrea/0000-0002-1211-529X; Cucchiara, Rita/0000-0002-2239-283X
CR Agrafiotis D, 2005, P SOC PHOTO-OPT INS, V5685, P39, DOI 10.1117/12.587028
   Ajib W, 2001, WIREL NETW, V7, P237, DOI 10.1023/A:1016622022016
   [Anonymous], IEEE T CIRCUITS SYST
   BRUNHEROTO J, 2000, P IEEE INT C MULT EX, V3, P1233
   CAI X, 2003, IEE C, V3, P81
   Chuang HC, 2007, IEEE T MULTIMEDIA, V9, P1273, DOI 10.1109/TMM.2007.902884
   Conklin GJ, 2001, IEEE T CIRC SYST VID, V11, P269, DOI 10.1109/76.911155
   Cucchiara R, 2003, IEEE T PATTERN ANAL, V25, P1337, DOI 10.1109/TPAMI.2003.1233909
   Guo M, 2005, Third IEEE International Conference on Pervasive Computing and Communications, Proceedings, P171
   Haritaoglu I, 2000, IEEE T PATTERN ANAL, V22, P809, DOI 10.1109/34.868683
   He ZH, 2006, IEEE T CIRC SYST VID, V16, P590, DOI 10.1109/TCSVT.2006.873154
   *ITU, 2003, H624ISO ITU
   Lam KY, 2007, MULTIMED TOOLS APPL, V33, P175, DOI 10.1007/s11042-006-0056-9
   LIM K, 2003, P IEEE INT C MULT EX, V2, P169
   LIU Z, 2005, P INT C EMB SOFTW SY
   Lu J, 2000, PROC SPIE, V3974, P246, DOI 10.1117/12.382918
   LU MT, 2005, P IEEE INT C IM PROC, V1, P193
   MACAULAY A, 2005, WHITEPAPER IP STREAM
   Mahonen P., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P1090, DOI 10.1109/ICIAP.1999.797745
   Puri A, 2004, SIGNAL PROCESS-IMAGE, V19, P793, DOI 10.1016/j.image.2004.06.003
   SCHMIDT B, 1995, LECT NOTES COMPUTER, V1018, P190
   WONG CF, 2005, 2 INT C QUAL SERV HE
NR 22
TC 55
Z9 58
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1142
EP 1154
DI 10.1109/TMM.2008.2001378
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600017
DA 2024-07-18
ER

PT J
AU Hoi, SCH
   Lyu, MR
AF Hoi, Steven C. H.
   Lyu, Michael R.
TI A multimodal and multilevel ranking scheme for large-scale video
   retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE content-based video retrieval; graph representation; multilevel ranking;
   multimodal fusion; multimedia retrieval; semi-supervised ranking;
   support vector machines
ID IMAGE RETRIEVAL; MODELS
AB A critical issue of large-scale multimedia retrieval is how to develop an effective framework for ranking the search results. This problem is particularly challenging for content-based video retrieval due to some issues such as short text queries, insufficient sample learning, fusion of multimodal contents, and large-scale learning with huge media data. In this paper, we propose a novel multimodal and multilevel (MMML) ranking framework to attack the challenging ranking problem of content-based video retrieval. We represent the video retrieval task by graphs and suggest a graph based semi-supervised ranking (SSR) scheme, which can learn with small samples effectively and integrate multimodal resources for ranking smoothly. To make the semi-supervised ranking solution practical for large-scale retrieval tasks, we propose a multilevel ranking framework that unifies several different ranking approaches in a cascade fashion. We have conducted empirical evaluations of our proposed solution for automatic search tasks on the benchmark testbed of TRECVID2005. The promising empirical results show that our ranking solutions are effective and very competitive with the state-of-the-art solutions in the TRECVID evaluations.
C1 [Hoi, Steven C. H.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Lyu, Michael R.] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Nanyang Technological University; Chinese University of Hong Kong
RP Hoi, SCH (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM chhoi@ntu.edu.sg; lyu@cse.cuhk.edu.hk
RI HOI, Steven C. H./A-3736-2011
OI Hoi, Steven/0000-0002-4584-3453
FU University Startup [RG67/07]; Computer Engineering, Nanyang
   Technological University; Council of the Hong Kong Special
   Administrative Region, China [CUHK4150/07E]
FX This work was supported in part by University Startup Grant RG67/07 from
   the School of Computer Engineering, Nanyang Technological University,
   and in part by a grant from the Research Grants Council of the Hong Kong
   Special Administrative Region, China (Project No. CUHK4150/07E).
CR AMIR A, 2005, P TRECVID WORKSH WAS
   [Anonymous], TRECVID: TREC Video Retrieval Evaluation
   [Anonymous], 2003, INT C MACH LEARN
   [Anonymous], P 13 ANN ACM INT C M
   [Anonymous], 1998, STAT LEARNING THEORY
   [Anonymous], ADV LARGE MARGIN CLA
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Chandra C, 1998, INT J SOFTW ENG KNOW, V8, P3, DOI 10.1142/S0218194098000030
   CHRISTEL M, 1995, COMMUN ACM, V38, P57, DOI 10.1145/205323.205337
   CHRISTEL M, 2007, P INT C IM VID RETR
   Dagtas S, 2000, IEEE T IMAGE PROCESS, V9, P88, DOI 10.1109/83.817601
   Datar M., 2004, PROC 20 ANN S COMPUT, P253
   Goh K., 2004, PROC ACM INT C MULTI, P564
   HAUPTMANN AG, 2005, P TRECVID WORKSH WAS
   HOI SC, 2007, P 24 INT C MACH LEAR
   HOI SC, 2006, P 15 INT WORLD WID W
   Hoi SCH, 2006, IEEE T KNOWL DATA EN, V18, P509, DOI 10.1109/TKDE.2006.1599389
   HOI SCH, 2006, P IEEE C COMP VIS PA
   HOI SCH, 2005, P IEEE C COMP VIS PA
   HSU WH, 2006, P ACM MULT C 2006 SA
   Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3
   JELINEK F, 1980, PATTERN RECOGNIT PRA
   LYU MR, 2002, P 11 INT WORLD WID W
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   NEY H, 1994, COMPUT SPEECH LANG, V8, P1, DOI 10.1006/csla.1994.1001
   OVER P, 2005, P TRECVID WORKSH
   Pan J.-Y., 2004, Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P653, DOI [DOI 10.1145/1014052.1014135, 10.1145/1014052, DOI 10.1145/1014052]
   Panda N, 2006, IEEE T KNOWL DATA EN, V18, P748, DOI 10.1109/TKDE.2006.101
   Ponte JM, 1998, P 21 ANN INT ACM SIG, P275, DOI DOI 10.1145/290941.291008
   Robertson SE, 1997, PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P16, DOI 10.1145/278459.258529
   Si L, 2006, MULTIMEDIA SYST, V12, P34, DOI 10.1007/s00530-006-0033-1
   Smeulders AWM, 2000, IEEE T PATTERN ANAL, V22, P1349, DOI 10.1109/34.895972
   Tong S, 2001, P 9 ACM INT C MULT, P107, DOI DOI 10.1145/500141.500159
   Wactlar HD, 1996, COMPUTER, V29, P46, DOI 10.1109/2.493456
   Wang CB, 2006, LECT NOTES COMPUT SC, V4035, P647, DOI 10.1145/1180639.1180774
   Wu Y., 2004, ACM INT C MULTIMEDIA, P572, DOI DOI 10.1145/1027527.1027665
   Yan R, 2003, LECT NOTES COMPUT SC, V2728, P238
   YAN R, 2006, P INT C IM VID RETR
   YAN R, 2003, P ACM MULT C MM 2003
   YAN R, 2004, P ACM MULT C MM 2004
   Yuan Jinhui., 2006, P 14 ANN ACM INT C M, P441
   Zhai CX, 2004, ACM T INFORM SYST, V22, P179, DOI 10.1145/984321.984322
   Zhu J. Lafferty R. Rosenfeld., 2005, SEMISUPERVISED LEARN
NR 43
TC 31
Z9 38
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 607
EP 619
DI 10.1109/TMM.2008.921735
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200006
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Cernea, DC
   Munteanu, A
   Alecu, A
   Cornelis, J
   Schelkens, P
AF Cernea, Dan C.
   Munteanu, Adrian
   Alecu, Alin
   Cornelis, Jan
   Schelkens, Peter
TI Scalable joint source and channel coding of meshes
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE error resilient coding; joint source and channel coding of meshes;
   L-infinite coding; MeshGrid; three-dimensional (3-D) graphics; unequal
   error protection
ID TRANSMISSION
AB This paper proposes a new approach for joint source and channel coding (JSCC) of meshes, simultaneously providing scalability and optimized resilience against transmission errors. An unequal error protection approach is followed, to cope with the different error-sensitivity levels characterizing the various resolution and quality layers produced by the input scalable source codec. The number of layers and the protection levels to be employed for each layer are determined by solving a joint source and channel coding problem. In this context, a novel fast algorithm for solving the optimization problem is conceived, enabling a real-time implementation of the JSCC rate-a I location. An instantiation of the proposed JSCC approach is demonstrated for MeshGrid, which is a scalable 3-D object representation method, part of MPEG-4 AFX. In this context, the L-infinite distortion metric is employed, which is to our knowledge a unique feature in mesh coding. Numerical results show the superiority of the L-infinite norm over the classical L-2 norm in a JSCC setting. One concludes that the proposed joint source and channel coding approach offers resilience against transmission errors, provides graceful degradation, enables a fast real-time implementation, and preserves all the scalability features and animation capabilities of the employed scalable mesh codec.
C1 [Cernea, Dan C.; Munteanu, Adrian; Alecu, Alin; Cornelis, Jan; Schelkens, Peter] Vrije Univ Brussel, Dept Elect & Informat ETRO, Interdisciplinary Inst Broadband Technol IBBT, B-1050 Brussels, Belgium.
C3 Vrije Universiteit Brussel
RP Cernea, DC (corresponding author), Vrije Univ Brussel, Dept Elect & Informat ETRO, Interdisciplinary Inst Broadband Technol IBBT, B-1050 Brussels, Belgium.
EM cdcostin@etro.vub.ac.be; acmuntea@etro.vub.ac.be; aalecu@etro.vub.ac.be;
   jpcornel@etro.vub.ac.be; pschelke@etro.vub.ac.be
RI Munteanu, Adrian/HKO-9955-2023; Schelkens, Peter/B-7831-2008; Cornelis,
   Jan/ABI-6396-2020
OI Munteanu, Adrian/0000-0001-7290-0428; Schelkens,
   Peter/0000-0003-0908-1655; Cornelis, Jan/0000-0002-1180-1968
FU IBBT; Scientific Research-Flanders (FWO)
FX This work was supported in part by IBBT under the ISBO-QoE project. P.
   Schelkens and A. Munteanu were supported by the Fund for Scientific
   Research-Flanders (FWO), B-1000 Brussels, Belgium. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Simon Lucey.
CR Al-Regib G, 2002, IEEE INFOCOM SER, P743, DOI 10.1109/INFCOM.2002.1019320
   Albanese A, 1996, IEEE T INFORM THEORY, V42, P1737, DOI 10.1109/18.556670
   Alecu A, 2004, IEEE SIGNAL PROC LET, V11, P367, DOI 10.1109/LSP.2003.822599
   Alecu A, 2003, J ELECTRON IMAGING, V12, P522, DOI 10.1117/1.1581731
   Alecu A, 2006, IEEE T IMAGE PROCESS, V15, P2499, DOI 10.1109/TIP.2006.877416
   AlRegib G, 2005, IEEE T MULTIMEDIA, V7, P766, DOI 10.1109/TMM.2005.850981
   Alregib G, 2005, ACM T GRAPHIC, V24, P182, DOI 10.1145/1061347.1061349
   ALREGIB G, 2005, IEEE T CIRCUITS SYST, V15
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   Banister BA, 2002, IEEE SIGNAL PROC LET, V9, P117, DOI 10.1109/97.1001646
   CERNEA CD, 2005, P SPIE S OPT E BOST, V6001
   Chen ZH, 2005, MULTIMEDIA SYST, V10, P230, DOI 10.1007/s00530-004-0154-3
   *ISO IEC, 14496162003 ISOIE 16
   Khodakovsky A, 2000, COMP GRAPH, P271, DOI 10.1145/344779.344922
   LI H, 2006, ACM T MULTIM COMPUT, V2, P4
   Lin S., 2004, Error Control Coding, Vsecond
   Lounsbery M, 1997, ACM T GRAPHIC, V16, P34, DOI 10.1145/237748.237750
   Pajarola R, 2000, IEEE T VIS COMPUT GR, V6, P79, DOI 10.1109/2945.841122
   Park SB, 2003, IEEE IMAGE PROC, P773
   Park SB, 2006, IEEE T MULTIMEDIA, V8, P885, DOI 10.1109/TMM.2006.879914
   PREDA M, 2003, 3 D MODELING ANIMATI
   Salomie IA, 2004, IEEE T CIRC SYST VID, V14, P950, DOI 10.1109/TCSVT.2004.830665
   SALOMIE IA, 2005, THESIS VRIJE U BRUSS
   Taubin G, 1998, ACM T GRAPHIC, V17, P84, DOI 10.1145/274363.274365
   TAUBIN G, 1998, P SIGGRAPH 98, P123
   Tian DH, 2007, IEEE T MULTIMEDIA, V9, P736, DOI 10.1109/TMM.2007.893341
   Verdicchio F, 2006, IEEE T IMAGE PROCESS, V15, P3114, DOI 10.1109/TIP.2006.877495
   Yan ZD, 2005, IEEE T CIRC SYST VID, V15, P138, DOI 10.1109/TCSVT.2004.837023
   Yan ZD, 2001, IEEE T CIRC SYST VID, V11, P860, DOI 10.1109/76.931112
NR 29
TC 10
Z9 12
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 503
EP 513
DI 10.1109/TMM.2008.917407
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100018
DA 2024-07-18
ER

PT J
AU Cha, Z
   Florencio, D
   Ba, DE
   Zhang, ZY
AF Cha Zhang
   Florencio, Dinei
   Ba, Demba E.
   Zhang, Zhengyou
TI Maximum likelihood sound source localization and beamforming for
   directional microphone arrays in distributed meetings
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE beamforming; directional mics; microphone array; sound source
   localization
AB In distributed meeting applications, microphone arrays have been widely used to capture superior speech sound and perform speaker localization through sound source localization (SSL) and beamforming. This paper presents a unified maximum likelihood framework of these two techniques, and demonstrates how such a framework can be adapted to create efficient SSL and beamforming algorithms for reverberant rooms and unknown directional patterns of microphones. The proposed method is closely related to steered response power-based algorithms, which are known to work extremely well in real-world environments. We demonstrate the effectiveness of the proposed method on challenging synthetic and real-world datasets, including over six hours of recorded meetings.
C1 [Cha Zhang; Florencio, Dinei; Zhang, Zhengyou] Microsoft Res, Redmond, WA 98052 USA.
   [Ba, Demba E.] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.
C3 Microsoft; Massachusetts Institute of Technology (MIT)
RP Cha, Z (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM chazhang@microsoft.com; dinei@microsoft.com; zhang@microsoft.com
RI Zhang, Zhang/JAX-2097-2023; zhang, zheng/HCH-9684-2022
CR ALLEN JB, 1979, J ACOUST SOC AM, V65, P943, DOI 10.1121/1.382599
   BASU S, 2005, P IEEE ICASSP SALT L, V5, P3361
   BRANDSTEIN M, 1997, P ICASSP MUN GERM AP
   Brandstein MS, 1997, COMPUT SPEECH LANG, V11, P91, DOI 10.1006/csla.1996.0024
   COEN M, 1998, P NAT C ART INT
   COX H, 1987, IEEE T ACOUST SPEECH, V35, P1365, DOI 10.1109/TASSP.1987.1165054
   CUTLER R, 2002, P ACM C MULT
   El-Keyi A, 2005, IEEE T SIGNAL PROCES, V53, P3032, DOI 10.1109/TSP.2005.851108
   GEORGIOUS P, 1997, P WASPAA NEW PALTZ N
   GRIFFITHS LJ, 1982, IEEE T ANTENN PROPAG, V30, P27, DOI 10.1109/TAP.1982.1142739
   GUSTAFSSON T, 2001, P ICASSP SALT LAK CI
   Harmanci K, 2000, IEEE T SIGNAL PROCES, V48, P1, DOI 10.1109/78.815474
   Hoshuyama O, 1999, IEEE T SIGNAL PROCES, V47, P2677, DOI 10.1109/78.790650
   KLEBAN J, 2000, COMBINED ACOUSTIC VI
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   Li D., 2002, P INT C ROB AUT WASH
   Mungamuru B, 2004, IEEE T SYST MAN CY B, V34, P1526, DOI 10.1109/TSMCB.2004.826398
   RUI Y, 2004, P ICASSP MONTR QC CA
   RUI Y, 2005, P ICASSP HON HI APR
   Sheng XH, 2005, IEEE T SIGNAL PROCES, V53, P44, DOI 10.1109/TSP.2004.838930
   SHRIBERG E, 2001, P EUR AAOLB DENM SEP
   Van Veen B. D., 1988, IEEE ASSP Magazine, V5, P4, DOI 10.1109/53.665
   WAHLSTER W, 2001, P EUR AALB DENM SEP
   WANG H, 1997, P IEEE ICASSP NEW PA
   Wang JY, 2001, J ACOUST SOC AM, V110, P310, DOI 10.1121/1.1377290
   WARSITZ E, 2005, P ICASSP PHIL PA MAR
   ZISKIND I, 1988, IEEE T ACOUST SPEECH, V36, P1553, DOI 10.1109/29.7543
NR 27
TC 129
Z9 161
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 538
EP 548
DI 10.1109/TMM.2008.917406
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100021
DA 2024-07-18
ER

PT J
AU Thomas-Kerr, J
   Burnett, I
   Ritz, C
AF Thomas-Kerr, Joseph
   Burnett, Ian
   Ritz, Christian
TI Format-independent rich media delivery using the Bitstream Binding
   Language
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bitstream binding language (BBL); MPEG-21 multimedia framework;
   multimedia communication; streaming
AB Several recent standards address virtual containers for rich multimedia content: collections of media with metadata describing the relationships between them and providing an immersive user experience. While these standards-which include MPEG-21 and TVAnytime-provide numerous tools for interacting with rich media objects, they do not provide a framework for streaming or delivery of such content. This paper presents the Bitstream Binding Language (BBL), a format-independent tool that describes how multimedia content and metadata may be bound into delivery formats. Using a BBL description, a generic processor can map rich content (an MPEG-21 digital item, for example) into a streaming or static delivery format. BBL provides a universal syntax for fragmentation and packetization of both XML and binary data, and allows new content and metadata formats to be delivered without requiring the addition of new software to the delivery infrastructure. Following its development by the authors, BBL was adopted by MPEG as Part 18 of the MPEG-21 Multimedia Framework.
C1 [Thomas-Kerr, Joseph; Burnett, Ian; Ritz, Christian] Whisper Labs, N Wollongong, NSW 2522, Australia.
RP Thomas-Kerr, J (corresponding author), Whisper Labs, N Wollongong, NSW 2522, Australia.
EM joetk@elec.uow.edu.au; i.burnett@elec.uow.edu.au; chritz@elec.uow.edu.au
RI Ritz, Christian H/AGE-1439-2022
OI Ritz, Christian H/0000-0002-3768-7569; Burnett, Ian/0000-0003-3795-7722
CR [Anonymous], 2002, Introduction to MPEG- 7: Multimedia content description interface
   [Anonymous], 2004, W3C recommendation
   Berglund A., 2007, XML PATH LANGUAGE X
   Bray T., 1999, WORLD WIDE WEB CONSO
   Burnett I.S., 2006, MPEG 21 BOOK, V1st
   De Neve W, 2005, LECT NOTES COMPUT SC, V3767, P641
   Diepold K, 2005, IEEE MULTIMEDIA, V12, P34, DOI 10.1109/MMUL.2005.79
   GIRARDOT M, 2000, P IEEE C MULT EXP IC
   Grube M, 2001, IEEE T CONSUM ELECTR, V47, P474, DOI 10.1109/30.964136
   HOFFMAN D, 1998, RFC2250 RTP PAYLOAD
   HONG D, 2002, P IEEE MULT EXP 2002
   *ISO IEC, 1381812000 ISO IEC
   *ISO IEC, 2007, WORK DRAFT 3 23001 4
   *ISO IEC, 2007, 2100018 ISO IEC 18
   JEITA, 2002, EXCH IM FIL FORM DIG
   JOHNSTON P, 2006, EXPRESSING DUBIN COR
   NIEDERMEIER U, 2002, P IEEE INT C MULT EX
   Nilsson M., 2000, ID3
   PFEIFFER S, 2003, P 5 ACM SIGMM INT WO
   RANSBURG M, 2005, P 1 INT C MULT SERV
   Reimers UH, 2006, P IEEE, V94, P173, DOI 10.1109/JPROC.2005.861004
   SCHIERL T, 2005, 3GPP COMPLIANT ADAPT
   Schulzrinne H., 2003, RFC3550 RTP TRANSPOR
   Thomas-Kerr J, 2006, LECT NOTES COMPUT SC, V4261, P349
   THOMASKERR J, 2006, P 2 INT C AUT PROD C
   Thompson H.S., 2001, Recommendation REC-xmlschema-1-20041028.
   TOL RM, 2000, P INT C CONS EL
   WENGER S, 2005, RFC3984 RTP PAYLOAD
   WONG EYC, 2003, P 27 ANN I CONF COMP
NR 29
TC 3
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2008
VL 10
IS 3
BP 514
EP 522
DI 10.1109/TMM.2008.917337
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EJ
UT WOS:000258767100019
OA Green Published
DA 2024-07-18
ER

PT J
AU Choudary, C
   Liu, TC
AF Choudary, Chekuri
   Liu, Tiecheng
TI Summarization of visual content in instructional videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE E-1 earning; instructional video analysis; key frame selection
ID TEXT DETECTION; IMAGES
AB In instructional videos of chalk board presentations, the visual content refers to the text and figures written on the boards. Existing methods on video summarization are not effective for this video domain because they are mainly based on low-level image features such as color and edges. In this work, we present a novel approach to summarizing the visual content in instructional videos using middle-level features. We first develop a robust algorithm to extract content text and figures from instructional videos by statistical modelling and clustering. This algorithm addresses the image noise, nonuniformity of the board regions, camera movements, occlusions, and other challenges in the instructional videos that are recorded in real classrooms. Using the extracted text and figures as the middle level features, we retrieve a set of key frames that contain most of the visual content. We further reduce content redundancy and build a mosaicked summary image by matching extracted content based on K-th Hausdorff distance and connected component decomposition. Performance evaluation on four full-length instructional videos shows that our algorithm is highly effective in summarizing instructional video content.
C1 Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
   Univ So Calif, Inst Informat Sci, Arlington, VA USA.
C3 University of South Carolina System; University of South Carolina
   Columbia; University of Southern California
RP Choudary, C (corresponding author), Univ S Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
EM choudary@cse.sc.edu; tiecheng@cse.se.edu
CR ALTMAN E, 2002, ACM MULTIMEDIA, P416
   Campisi P, 1999, PROC SPIE, V3813, P861, DOI 10.1117/12.366844
   Chang HS, 1999, IEEE T CIRC SYST VID, V9, P1269, DOI 10.1109/76.809161
   Chen Y, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P568
   Choudary C, 2007, PATTERN ANAL APPL, V10, P69, DOI 10.1007/s10044-006-0051-9
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   DeMenthon D., 1998, Proceedings ACM Multimedia 98, P211, DOI 10.1145/290747.290773
   Divakaran A, 2002, IEEE IMAGE PROC, P932
   DORAI C, 2003, P ICIP 2003, V3, P1029
   Erol B., 2005, IEEE INT C MULT EXP
   HAUBOLD A, 2004, P IEEE 6 INT S MULT, P961
   HE L, 2005, P ICASSP, P1113
   He LW, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL V, PROCEEDINGS, P776
   He LW, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P489, DOI 10.1145/319463.319691
   Heng WJ, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P436
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   Ju SX, 1998, IEEE T CIRC SYST VID, V8, P686, DOI 10.1109/76.718513
   Li HP, 2000, IEEE T IMAGE PROCESS, V9, P147, DOI 10.1109/83.817607
   Li Y, 2006, IEEE T AUDIO SPEECH, V14, P2264, DOI 10.1109/TASL.2006.872602
   Liang J., 2005, International Journal on Document Analysis and Recognition, V7, P84, DOI 10.1007/s10032-004-0138-z
   Lienhart R, 2002, IEEE T CIRC SYST VID, V12, P256, DOI 10.1109/76.999203
   Lin M, 2005, INT J TECHNOL HUM IN, V1, P27, DOI 10.4018/jthi.2005040102
   LIU T, 2003, P ICIP, V1, P921
   LIU T, 2002, INT C MULT EXP, V1, P77
   LIU T, 2002, P ICIP, V1, P601
   LIU T, 2001, P IEEE C COMP VIS PA, V2, P531
   Liu TC, 2006, MULTIMED TOOLS APPL, V28, P157, DOI 10.1007/s11042-006-6140-3
   Lyu MR, 2005, IEEE T CIRC SYST VID, V15, P243, DOI 10.1109/TCSVT.2004.841653
   MALIOUTOV I, 2006, INT C COMP LING 44 A, P25
   Mittal A, 2006, MULTIMEDIA SYST, V11, P249, DOI 10.1007/s00530-006-0022-4
   Mittal A, 2006, EDUC TECHNOL SOC, V9, P349
   Mukhopadhyay S, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P477, DOI 10.1145/319463.319690
   Ngo CW, 2005, MULTIMEDIA SYST, V10, P261, DOI 10.1007/s00530-004-0157-0
   Ngo CW, 2003, IEEE FIFTH INTERNATIOANL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P215
   Onishi M, 2000, INT C PATT RECOG, P102, DOI 10.1109/ICPR.2000.902874
   PHUNG D, 2002, P INT C PATT REC, V2, P835
   REPP S, 2006, IEEE INT C PERV COMP
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Stafford-Fraser Q., 1996, Proceedings of ACM CHI Conferecne on Human Factors in Computing Systems, P134
   SYEDAMAHMOOD T, 2000, ACM C MULTIMEDIA, P85
   WALLICK MN, 2005, MIRAGE 2005, P223
   WANG F, 2003, ACM MULTIMEDIA, P315
   Wang F, 2007, IEEE T MULTIMEDIA, V9, P397, DOI 10.1109/TMM.2006.886292
   Wienecke M., 2005, International Journal on Document Analysis and Recognition, V7, P188, DOI 10.1007/s10032-004-0132-5
   WOLF W, 1996, P IEEE INT C AC SPEE, P1228
   Yamamoto N., 2003, 8th European Conference on Speech Communication and Technology, P961
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   Yokoi T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P641, DOI 10.1109/ICME.2006.262527
   ZHANG Z, 2004, P IEEE INT C AC SPEE, V3, P533
   Zhuang YT, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P866, DOI 10.1109/ICIP.1998.723655
NR 50
TC 37
Z9 41
U1 2
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1443
EP 1455
DI 10.1109/TMM.2007.906602
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400010
DA 2024-07-18
ER

PT J
AU Sargin, ME
   Yemez, Y
   Erzin, E
   Tekalp, AM
AF Sargin, Mehmet Entre
   Yemez, Yuecel
   Erzin, Engin
   Tekalp, A. Murat
TI Audiovisual synchronization and fusion using canonical correlation
   analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audiovisual synchronization; correlation; multimodal; fusion; speaker
   recognition
ID SPEECH
AB It is well-known that early integration (also called data fusion) is effective when the modalities are correlated, and late integration (also called decision or opinion fusion) is optimal when modalities are uncorrelated. In this paper, we propose a new multimodal fusion strategy for open-set speaker identification using a combination of early and late integration following canonical correlation analysis (CCA) of speech and lip texture features. We also propose a method for high precision synchronization of the speech and lip features using CCA prior to the proposed fusion. Experimental results show that i) the proposed fusion strategy yields the best equal error rates (EER), which are used to quantify the performance of the fusion strategy for open-set speaker identification, and ii) precise synchronization prior to fusion improves the EER; hence, the best EER is obtained when the proposed synchronization scheme is employed together with the proposed fusion strategy. We note that the proposed fusion strategy outperforms others because the features used in the late integration are truly uncorrelated, since they are output of the CCA analysis.
C1 Koc Univ, Dept Comp Engn, Istanbul, Turkey.
   Koc Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
C3 Koc University; Koc University
RP Sargin, ME (corresponding author), Koc Univ, Dept Comp Engn, Istanbul, Turkey.
EM msargin@ku.edu.tr; yyemez@ku.edu.tr; eerzjn@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020; Erzin, Engin/H-1716-2011
OI Erzin, Engin/0000-0002-2715-2368; Tekalp, Ahmet
   Murat/0000-0003-1465-8121
CR Borga M, 1998, THESIS LINKOPING U S
   Broun CC, 2002, INT CONF ACOUST SPEE, P685
   Cetingül HE, 2004, IEEE IMAGE PROC, P2023
   Chen TH, 2001, IEEE SIGNAL PROC MAG, V18, P9
   Chibelushi C. C., 1997, European Conference on Security and Detection - ECOS97 Incorporating the One Day Symposium on Technology Used for Combatting Fraud (Conf. Publ. No.437), P26, DOI 10.1049/cp:19970414
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Choukri K., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P2659
   Erzin E, 2005, IEEE T MULTIMEDIA, V7, P840, DOI 10.1109/TMM.2005.854464
   Fisher JW, 2004, IEEE T MULTIMEDIA, V6, P406, DOI 10.1109/TMM.2004.827503
   Frischholz RW, 2000, COMPUTER, V33, P64, DOI 10.1109/2.820041
   HARDOON DR, 2003, CSDTR0302 U LOND DEP
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   Nakamura S, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P305, DOI 10.1109/ICMI.2002.1167011
   ODOBEZ JM, 1995, J VIS COMMUN IMAGE R, V6, P348, DOI 10.1006/jvci.1995.1029
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   SARGIN ME, 2005, P EUR SIGN PROC C EU
   Slaney M., 2000, Advances in Neural Information Processing Systems (NIPS), V13, P814
   SUN QS, 2004, IEEE INT C CONTR AUT, V2, P1547
   Wark T, 2001, DIGIT SIGNAL PROCESS, V11, P169, DOI 10.1006/dspr.2001.0397
NR 20
TC 122
Z9 135
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2007
VL 9
IS 7
BP 1396
EP 1403
DI 10.1109/TMM.2007.906583
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 224LB
UT WOS:000250447400006
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, HL
   Kwong, S
AF Wang, Hanli
   Kwong, Sam
TI Hybrid model to detect zero quantized DCT coefficients in H.264
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian distribution; H.264; integer DCT; quantization
ID VIDEO; ALGORITHM; DECISION
AB In H.264 coding, there are a large number of discrete cosine transform (DCT) coefficients of the prediction residue which are quantized to zeros. Therefore, it is desired to design a method which can early detect zero quantized DCT coefficients (ZQDCT) before implementing DCT and quantization (Q) and thus reduce redundant computations for H.264 coding. To achieve this, a hybrid model is proposed in this paper in order to predict ZQDCT coefficients. First, the Gaussian distribution is applied to study the integer DCT coefficients in H.264 and hence an adaptive scheme with multiple thresholds is derived to realize different types of DCT and Q implementations. Then the adaptive scheme is further optimized by considering a more efficient condition to sufficiently detect all-zero DCT blocks. As a result, a hybrid model is developed. Compared with other methods in the literature, the proposed hybrid model is able to detect more ZQDCT coefficients and hence reduce more computations for H.264 encoding. It is shown by experimental results that the proposed hybrid model can achieve the best performance in reducing computations and obtain almost the same rate-distortion (R-D) performance as the original encoder in the H.264 reference software JM9.5.
C1 City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Wang, HL (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM wanghl@cs.cityu.edu.hk; cssamk@cityu.edu.hk
RI Wang, Hanli/G-5111-2014; Wang, Hanli/K-5717-2019; Kwong, Sam/C-9319-2012
OI Wang, Hanli/0000-0002-9999-4871; Wang, Hanli/0000-0002-9999-4871; Kwong,
   Sam/0000-0001-7484-7261
CR Advanced Video Coding for Generic Audiovisual Services, 2005, 14496102005E ISOIEC
   Chen HT, 1996, IEEE T CONSUM ELECTR, V42, P781, DOI 10.1109/30.536185
   Cheung CH, 2005, IEEE T MULTIMEDIA, V7, P16, DOI 10.1109/TMM.2004.840609
   Jain A. K., 1989, Fundamentals of Digital Image Processing
   Jing X, 2004, ELECTRON LETT, V40, P1050, DOI 10.1049/el:20045243
   Malvar HS, 2003, IEEE T CIRC SYST VID, V13, P598, DOI 10.1109/TCSVT.2003.814964
   Moon YH, 2005, IEEE T CIRC SYST VID, V15, P1053, DOI 10.1109/TCSVT.2005.852411
   Pan F, 2005, IEEE T CIRC SYST VID, V15, P813, DOI 10.1109/TCSVT.2005.848356
   Pao IM, 1999, IEEE T CIRC SYST VID, V9, P608, DOI 10.1109/76.767126
   Sousa LA, 2000, ELECTRON LETT, V36, P306, DOI 10.1049/el:20000272
   Su YP, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P695, DOI 10.1109/ICME.2004.1394287
   Tu YK, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P789
   WANG H, 2006, P IEEE ISCAS 06 MAY, P1703
   Wang HL, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P801, DOI 10.1109/ICME.2006.262602
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P547, DOI 10.1109/TCSVT.2006.871390
   WIEGAND T, 2006, SCALABLE VIDEO CODIN
   Wu D, 2005, IEEE T CIRC SYST VID, V15, P953, DOI 10.1109/TCSVT.2005.848304
   XU J, 2003, P INT C INF COMM SIG, V1, P218
   Yang CL, 2004, IEEE IMAGE PROC, P461
   Yu A, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P21, DOI 10.1145/266180.266326
   YU A, 1997, P PICT COD S SEPT, P159
   Zhou X, 1998, ELECTRON LETT, V34, P1839, DOI 10.1049/el:19981308
   Zhu LQ, 2005, MATH BIOSCI ENG, V2, P1
NR 23
TC 43
Z9 50
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 728
EP 735
DI 10.1109/TMM.2007.893336
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200005
DA 2024-07-18
ER

PT J
AU Tang, CW
AF Tang, Chih-Wei
TI Spatiotemporal visual considerations for video coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptive quantization; bit allocation; eye movement; H.264;
   spatiovelocity contrast sensitivity; visual attention; visual masking
ID ATTENTION; MODEL; SENSITIVITY; FOVEATION; SALIENCY
AB Human visual sensitivity varies with not only spatial frequencies, but moving velocities of image patterns. Moreover, the loss of visual sensitivity due to object motions might be compensated by eye movement. Removing the psychovisual redundancies in both the spatial and temporal frequency domains facilitates an efficient coder without perceptual degradation. Motivate by this, a visual measure is proposed for the purpose of video compressions. The novelty of this analysis relies on combining three visual factors altogether: the motion attention model, unconstrained eye-movement incorporated spatiovelocity visual sensitivity model, and visual masking model. For each motion-unattended macroblock, the retinal velocity is evaluated so that discrete cosing transform coefficients to which the human visual system has low sensitivity are picked up with the aid of eye movement incorporated spatiovelocity visual model. Based on masking thresholds of those low-sensitivity coefficients, a spatiotemporal distortion masking measure is determined. Accordingly, quantization parameters at macroblock level for video coding are adjusted on the basis of this measure. Experiments conducted by H.264 exhibit the effectiveness of the proposed scheme in improving coding performance without picture quality degradation.
C1 Natl Cent Univ, Dept Commun Engn, Jhongli 320, Taiwan.
C3 National Central University
RP Tang, CW (corresponding author), Natl Cent Univ, Dept Commun Engn, Jhongli 320, Taiwan.
EM cwtang@ce.ncu.edu.tw
CR Agrafiotis D, 2003, ELECTRON LETT, V39, P1703, DOI 10.1049/el:20031140
   BASU A, 1994, P IEEE INT C PATT RE
   CHITPRASERT B, 1990, IEEE T COMMUN, V38, P1040, DOI 10.1109/26.57501
   DALY S, 1999, IS T SPIE C HUM VIS, V3644, P162
   Dennett DC, 1991, CONSCIOUSNESS EXPLAI
   Ferwerda J. A., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P143, DOI 10.1145/258734.258818
   Itti L, 2005, VIS COGN, V12, P1093, DOI 10.1080/13506280444000661
   Itti L, 2004, IEEE T IMAGE PROCESS, V13, P1304, DOI 10.1109/TIP.2004.834657
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   ITTI L, 2005, P IEEE INT C COMP VI
   KELLY DH, 1979, J OPT SOC AM, V69, P1340, DOI 10.1364/JOSA.69.001340
   KOCH C, 2004, BIOL MODELS MOTION P
   Lee JS, 2002, MAR BIOTECHNOL, V4, P1, DOI 10.1007/s10126-001-0077-3
   LUKAS FXJ, 1982, IEEE T COMMUN, V30, P1679, DOI 10.1109/TCOM.1982.1095616
   Ma YF, 2002, IEEE IMAGE PROC, P129
   Mack Arien, 1998, Inattentional Blindness
   Pei SC, 1998, IEEE J SEL AREA COMM, V16, P98, DOI 10.1109/49.650923
   PEREIRA F, 2002, MPEG 4 BOOK, P669
   Privitera CM, 2000, IEEE T PATTERN ANAL, V22, P970, DOI 10.1109/34.877520
   Rensink R. A., 2002, ACM P 2 INT S SMART, P63
   Rensink RA, 1997, PSYCHOL SCI, V8, P368, DOI 10.1111/j.1467-9280.1997.tb00427.x
   ROBSON JG, 1966, J OPT SOC AM, V56, P1141, DOI 10.1364/JOSA.56.001141
   Solomon J. A., 1994, Proceedings DCC '94. Data Compression Conference (Cat. No.94TH0626-2), P361, DOI 10.1109/DCC.1994.305944
   Tan SH, 1996, IEEE T CIRC SYST VID, V6, P375, DOI 10.1109/76.510930
   Wang Z, 2003, IEEE T IMAGE PROCESS, V12, P243, DOI 10.1109/TIP.2003.809015
   Watson A.B., 1993, Proc. AIAA Computing in Aerospace, V9, P286
   Watson AB, 1997, J OPT SOC AM A, V14, P2379, DOI 10.1364/JOSAA.14.002379
   Watson AB, 2001, J ELECTRON IMAGING, V10, P20, DOI 10.1117/1.1329896
   WESTEN SJP, 1997, P SPIE C HUM VIS EL
   Winkler S, 1999, PROC SPIE, V3644, P175, DOI 10.1117/12.348438
   Yang XK, 2005, IEEE T CIRC SYST VID, V15, P496, DOI 10.1109/TCSVT.2005.844458
   Yee H, 2001, ACM T GRAPHIC, V20, P39, DOI 10.1145/383745.383748
NR 32
TC 54
Z9 79
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2007
VL 9
IS 2
BP 231
EP 238
DI 10.1109/TMM.2006.886328
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 135CO
UT WOS:000244131900003
DA 2024-07-18
ER

PT J
AU Oottamakorn, C
   Mao, SW
   Panwar, SS
AF Oottamakorn, Chaiwat
   Mao, Shiwen
   Panwar, Shivendra S.
TI On generalized processor sharing with regulated multimedia traffic flows
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Communications (ICC 2005)
CY MAY 16-20, 2005
CL Seoul, SOUTH KOREA
SP IEEE Commun Soc, IEEE Comp Soc, TCPP, KT, SAMSUNG, LG Elect, SK Telecom, Cisco
DE generalized processor sharing (GPS); multi-media; quality-of-service
   (QoS); scheduling; traffic regulation
AB Multimedia traffic is becoming an increasing portion A today's Internet traffic due to the flourishing of multimedia applications such as music/video streaming, video teleconferencing, IP telephony, and distance learning. In this paper, we study the problem of supporting multimedia traffic using a generalize processor sharing (GPS) server. By examining the sample path behavior and exploring the inherent feasible ordering of the classes, we derive tight performance bounds on backlog and delay W regulated multimedia traffic classes in a GPS system. Our approach is quite general since we do not assume any arriving traffic model or any specific traffic regulator, other than I I each traffic flow is deterministically regulated. Such deterministic regulators, as well as approximations of the GPS server, are widely implemented in commercial routers. In addition, our analysis is very accurate and achieves a high utilization of the server capacity, since we exploit the independence among the traffic flows for higher statistical multiplexing gains. Numerical examples an simulation results are presented to demonstrate the accuracy and merits of our approach, which is practical and well suited for supporting multimedia applications in the Internet.
C1 Walailak Univ, Inst Engn & Resources, Nakhon Si Thammarat 80000, Thailand.
   Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
   Polytech Univ, Dept Elect & Comp Engn, Brooklyn, NY 11201 USA.
C3 Walailak University; Auburn University System; Auburn University; New
   York University
RP Oottamakorn, C (corresponding author), Walailak Univ, Inst Engn & Resources, Nakhon Si Thammarat 80000, Thailand.
EM ochaiwat@wu.ac.th; smao@ieee.org; panwar@catt.poly.edu
RI Panwar, Shivendra/K-6473-2019; Panwar, Shivendra S/A-6884-2016; Mao,
   Shiwen/AAY-4471-2020
OI Panwar, Shivendra S/0000-0002-9822-6838; 
CR [Anonymous], 1994, 1633 IETF RFC
   Bazaraa M.S., 2013, NONLINEAR PROGRAMMIN
   Bennett JCR, 1996, IEEE INFOCOM SER, P120, DOI 10.1109/INFCOM.1996.497885
   BLAKE S, 1998, 24MK IETF RFC
   Boorstyn RR, 2000, IEEE J SEL AREA COMM, V18, P2651, DOI 10.1109/49.898747
   Borst S, 2003, IEEE ACM T NETWORK, V11, P821, DOI 10.1109/TNET.2003.818195
   Borst S, 1999, TELETRAF SCI ENG, V3, P345
   *CISC SYST INC, QC CISC IOS REL 12 0
   Elwalid A, 1999, IEEE INFOCOM SER, P1220, DOI 10.1109/INFCOM.1999.751679
   FITZEK FHP, 2000, TKN0006 TU BERL DEP
   Mannersalo P, 2002, IEEE INFOCOM SER, P1660
   Parekh AK, 1993, IEEE ACM T NETWORK, V1, P344, DOI 10.1109/90.234856
   PEREIRA FM, 2001, IC0111 STAT U I COMP
   Presti FL, 1996, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1996.493078
   Sariowan H., 1995, Proceedings Fourth International Conference on Computer Communications and Networks (ICCCN'95) (Cat. No.95TB8110), P512, DOI 10.1109/ICCCN.1995.540168
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Xu L, 2005, IEEE T MOBILE COMPUT, V4, P166, DOI 10.1109/TMC.2005.26
   Yu X, 2005, IEEE ACM T NETWORK, V13, P676, DOI 10.1109/TNET.2005.850213
   ZHANG ZL, 1995, IEEE J SEL AREA COMM, V13, P1071, DOI 10.1109/49.400662
NR 19
TC 9
Z9 10
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2006
VL 8
IS 6
BP 1209
EP 1218
DI 10.1109/TMM.2006.884613
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 109MG
UT WOS:000242311700010
DA 2024-07-18
ER

PT J
AU Azimi, M
   Nasiopoulos, P
   Ward, RK
AF Azimi, Mehran
   Nasiopoulos, Panos
   Ward, Rabab Kreidieh
TI Data transmission schemes for DVD-like interactive TV
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE digital TV; interactive TV; traffic control; TV transmitters
ID VIDEO; MODELS
AB Current interactive services for digital TV are limited. They basically display a web page alongside the TV program, which enhances the viewer's experience by providing extra information about the TV program. We define new interactive services for digital TV, which provide DVD-like interactivity to TV viewers. These services enable viewers to control the content and final presentation of a TV program. Some of the attractive applications of our services include parental management, multilingual audio, multiangle video, video in video, etc. The challenge in implementing these services is in transmitting an extra audio or video stream (called incidental) along with the main streams of the TV program. In the first part of this paper, we present a framework for adding the incidental streams to the original transmission stream without increasing the required bandwidth, degrading the picture quality of the main streams, or violating the compatibility of the transmitted stream with standard TV receivers. In the second part of this paper, we explore the two basic mechanisms of the presented framework: traffic characterization and admission control. We present methods for implementing these mechanisms. Using our methods, one can determine whether a TV transmission network has the capability of sending an incidental stream or not. Simulations were conducted to test the validity of our method. The results verify that our method successfully transmits the incidental streams without any discrepancy and without affecting the quality of the main streams.
C1 Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6K 1Z4, Canada.
C3 University of British Columbia
RP Azimi, M (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6K 1Z4, Canada.
EM mehran@ece.ubc.ca; panosn@ece.ubc.ca; rababw@ece.ubc.ca
CR ADA A, 1995, P IEEE INF APR, P779
   *ATVEF FOR, ATVEF SPEC ENH TV ON
   Chan CC, 1997, IEEE T IND ELECTRON, V44, P3, DOI 10.1109/41.557493
   Chang CS, 2002, IEEE ACM T NETWORK, V10, P805, DOI 10.1109/TNET.2002.804824
   Firoiu V, 2002, P IEEE, V90, P1565, DOI 10.1109/JPROC.2002.802002
   Garett M.W., 1994, P ACM SIGCOMM, P269
   Heyman DP, 1992, IEEE T CIRC SYST VID, V2, P49, DOI 10.1109/76.134371
   Ismail MR, 1996, INT J COMMUN SYST, V9, P283, DOI 10.1002/(SICI)1099-1131(199611)9:6<283::AID-DAC314>3.0.CO;2-R
   KRUNZ M, 1995, P ACM SIGMETRICS OTT, P47
   Le Boudec J.-Y., 2001, Network calculus: a theory of deterministic queuing systems for the internet, V2050
   LEBOUDEC JY, 2000, P 2000 IEEE INT S CI, V4, P93
   Rose O, 1997, PERFORM EVALUATION, V30, P69, DOI 10.1016/S0166-5316(96)00054-5
   WANG Y, 2004, GLOBAL TELECOMMUNICA, V3, P1401
   Wrege DE, 1996, IEEE ACM T NETWORK, V4, P352, DOI 10.1109/90.502234
   ZHANG H, 1994, P ACM SIGMETRICS 94, P211
NR 15
TC 3
Z9 3
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 856
EP 865
DI 10.1109/TMM.2006.876234
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300018
DA 2024-07-18
ER

PT J
AU Wei, Y
   Bhandarkar, SM
   Chandra, S
AF Wei, Yong
   Bhandarkar, Suchendra M.
   Chandra, Surendar
TI A client-side statistical prediction scheme for energy aware multimedia
   data streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE energy-aware computing; linear prediction; mobile computing; multimedia
   streaming
AB The recent proliferation of streaming multimedia on a variety of mobile devices has severely tested their battery lifetime. The long running nature of typical streaming applications results in significant energy consumption by the wireless network interface card (WNIC) in these mobile devices. In this paper we explore linear prediction-based client-side strategies that reduce the WNIC energy consumption to receive multimedia streams by judiciously transitioning the WNIC to a lower power consuming sleep state during the no-data intervals in the multimedia stream, without explicit support from the multimedia servers themselves. Experimental results on popular streaming formats such as Microsoft Media, Real and Apple QuickTime show that a linear prediction-based strategy performs better than history-based strategies that use simple temporal averaging.
C1 Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
   Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University System of Georgia; University of Georgia; University of Notre
   Dame
RP Wei, Y (corresponding author), Univ Georgia, Dept Comp Sci, Athens, GA 30602 USA.
EM yong@cs.uga.edu; suchi@cs.uga.edu; surendar@cse.nd.edu
CR AGRAWAL P, 1998, P IEEE PIMRC 98 BOST, P116
   CHANDRA S, 2003, WIRELESS NETWORK INT, P185
   Chiasserini C.F., 1999, PROC MOBICOM 99, P88
   Datta A, 1997, PROC INT CONF DATA, P124, DOI 10.1109/ICDE.1997.581745
   DOUGLIS F, 1995, PROCEEDINGS OF THE SECOND USENIX SYMPOSIUM ON MOBILE AND LOCATION-INDEPENDENT COMPUTING, P121
   Feeney LM, 2001, IEEE INFOCOM SER, P1548, DOI 10.1109/INFCOM.2001.916651
   Govil Kinshuk., 1995, Proceedings of the 1st annual international conference on Mobile computing and networking, P13, DOI DOI 10.1145/215530.215546
   Hamilton J.D., 1994, Time series analysis
   HAVINGA PJM, 2000, THESIS U TWENTE ENSC
   Helmbold DavidP., 1996, MOBILE COMPUTING NET, P130
   IMIELINSKI I, 1995, P USENIX S LOC DEP C, P109
   KRAVETS R, 1998, P 4 ANN ACM IEEE INT, P157
   LI K, 1994, PROCEEDINGS OF THE WINTER 1994 USENIX CONFERENCE, P279
   Lorch JR, 1998, IEEE PERS COMMUN, V5, P60, DOI 10.1109/98.683740
   MAKHOU J, 1978, MODERN SPECTRUM ANAL, P34
   SHIH E, 2003, REDUCING ENERGY CONS, P37
   Singh S, 1998, NINTH IEEE INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR AND MOBILE RADIO COMMUNICATIONS, VOLS 1-3, P153, DOI 10.1109/PIMRC.1998.733533
   STEMM M, 1996, P 3 INT WORKSH MOB M, V3, P103
   Weiser M., 1994, Proceedings of the First USENIX Symposium on Operating Systems Design and Implementation (OSDI), P13
   [No title captured]
NR 20
TC 10
Z9 10
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 866
EP 874
DI 10.1109/TMM.2006.876232
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300019
DA 2024-07-18
ER

PT J
AU Kuzmanov, G
   Gaydadjiev, G
   Vassiliadis, S
AF Kuzmanov, G
   Gaydadjiev, G
   Vassiliadis, S
TI Multimedia rectangularly addressable memory
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE linear addressing; memory modules; module assignment functions;
   rectangular block addressing; separability
ID ACCESS; PROCESSOR
AB We propose a scalable data alignment scheme incorporating module assignment functions and a generic addressing function for parallel access of randomly aligned rectangular blocks of data. The addressing function implicitly embeds the module assignment functions and it is separable, which potentially enables short critical paths and saves hardware resources. We also discuss the interface between the proposed memory organization and a linearly addressable memory. An implementation, suitable for MPEG-4 is presented and mapped onto an FPGA technology as a case study. Synthesis results indicate reasonably small hardware costs in the order of up to a few thousand FPGA slices for an exemplary 512 x 1024 two-dimensional (2-D) addressable space and a range of access pattern dimensions. Experiments suggest that speedups close to 8x can be expected when compared to linear addressing schemes.
C1 Delft Univ Technol, Elect Engn Math & Comp Sci Dept, Comp Engn Lab, NL-2628 CD Delft, Netherlands.
C3 Delft University of Technology
RP Kuzmanov, G (corresponding author), Delft Univ Technol, Elect Engn Math & Comp Sci Dept, Comp Engn Lab, NL-2628 CD Delft, Netherlands.
EM G.Kuzmanov@ewi.tudelft.nl; G.N.Gaydadjiev@ewi.tudelft.nl;
   S.Vassiliadis@sewi.tudelft.nl
RI Gaydadjiev, Georgi N/F-1488-2010; Gaydadjiev, Georgi/AAY-3859-2020
OI Gaydadjiev, Georgi N/0000-0002-3678-7007; Gaydadjiev,
   Georgi/0000-0002-3678-7007
CR BUDNIK P, 1971, IEEE T COMPUT, VC 20, P1566, DOI 10.1109/T-C.1971.223171
   Haverkamp M., 2003, PRORISC 2003, P90
   *ISOIEC, JTC11SC29WG11N3312 I
   KIM K, 1993, IEEE T PARALL DISTR, V4, P361, DOI 10.1109/71.219753
   KLOOS H, 2002, P IEEE INT C AC SPEE, V3, P3112
   Kneip J., 1994, Proceedings. The International Conference on Application Specific Array Processors (Cat. No.94TH0687-4), P271, DOI 10.1109/ASAP.1994.331797
   KOGGE PM, 1981, ARCHITECTURE PIPELIN
   KUZMANOV G, 2001, WORKSH SYST ARCH MOD, V2268, P291
   LAWRIE DH, 1975, IEEE T COMPUT, V24, P1145, DOI 10.1109/T-C.1975.224157
   LEE D, 1988, P INT S COMP ARCH, P232
   Park JW, 2001, IEEE T PARALL DISTR, V12, P316, DOI 10.1109/71.914779
   SPROULL RF, 1983, ACM T GRAPHIC, V2, P32, DOI 10.1145/357314.357316
   VANVOORHIS DC, 1978, IEEE T COMPUT, V27, P113, DOI 10.1109/TC.1978.1675045
   Vassiliadis S, 2004, IEEE T COMPUT, V53, P1363, DOI 10.1109/TC.2004.104
   WITTENBURG J, 1997, P ICA3PP 97, P155
NR 15
TC 20
Z9 23
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2006
VL 8
IS 2
BP 315
EP 322
DI 10.1109/TMM.2005.864345
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 028GY
UT WOS:000236476300012
OA Green Published
DA 2024-07-18
ER

PT J
AU Asif, A
   Kouras, M
AF Asif, A
   Kouras, M
TI Scalable video codec by noncausal prediction, cascaded vector
   quantization, and conditional replenishment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE conditional replenishment; Gauss Markov random process; noncausal
   prediction; quality of service; scalable video compression; vector
   quantization
AB In this paper, we describe a bandwidth adaptable, low bit-rate video coding scheme, referred to as scalable, noncausal prediction with vector quantization and conditional replenishment (SNP/VQR). Practical implementations of SNP/VQR are derived by exploiting the convergence and block banded properties of the state matrices. The resulting subblock SNP/VQR reduces the computational complexity and the storage requirements of the direct SNP/VQR by two orders of the linear magnitude of the frame dimensions. The subblock SNP/VQR is also capable of offering different quality of services (QoS) in the spatial and temporal domains. In the experiments, the block SNP/VQR compares favorably with the standard video codecs including the International Standards Organization (ISO) proposed MPEG4 and the International Telecommunication Union (ITU) proposed H.263, especially at low bit rates.
C1 York Univ, Dept Comp Sci & Engn, Toronto, ON M3J 1P3, Canada.
   Minist Hlth & Long Term Care, Toronto, ON M3J 1P3, Canada.
C3 York University - Canada
RP Asif, A (corresponding author), York Univ, Dept Comp Sci & Engn, Toronto, ON M3J 1P3, Canada.
EM asif@cs.yorku.ca; cs203529@cs.yorku.ca
OI Asif, Amir/0000-0002-9393-7112
CR ANDREW H, 1977, DIGITAL IMAGE RESTOR, P211
   Asif A, 1996, IEEE T CIRC SYST VID, V6, P42, DOI 10.1109/76.486419
   Asif A, 2005, IEEE T SIGNAL PROCES, V53, P630, DOI 10.1109/TSP.2004.840709
   Asif A, 2004, IEEE SIGNAL PROC LET, V11, P371, DOI 10.1109/LSP.2003.822922
   BARNES CF, 1993, IEEE T INFORM THEORY, V39, P565, DOI 10.1109/18.212286
   DEERING S, 1993, MULT INT C EUR OCT
   GOLDBERG M, 1986, IEEE T COMMUN, V34, P703, DOI 10.1109/TCOM.1986.1096600
   *ISO IEC, 1999, 144961 ISOIEC IS
   KOURAS M, 2004, IEEE INT C AC SPEECH
   Li X, 1998, IEEE INFOCOM SER, P1062, DOI 10.1109/INFCOM.1998.662916
   LINDE Y, 1980, IEEE T COMMUN, V28, P84, DOI 10.1109/TCOM.1980.1094577
   McCanne S, 1997, IEEE J SEL AREA COMM, V15, P983, DOI 10.1109/49.611154
   MOURA JMF, 1992, IEEE T INFORM THEORY, V38, P334, DOI 10.1109/18.119691
   Schweizer SM, 2000, IEEE T INFORM THEORY, V46, P1855, DOI 10.1109/18.857796
   SHACHAM N, 1992, IEEE INFOCOM SER, P2107, DOI 10.1109/INFCOM.1992.263483
   Sodagar I, 1999, IEEE T CIRC SYST VID, V9, P244, DOI 10.1109/76.752092
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   TANAKITPARPA T, H 263 VIDEO CODEC
   van der Schaar M, 2001, IEEE T CIRC SYST VID, V11, P318, DOI 10.1109/76.911158
   Vicisano L, 1998, IEEE INFOCOM SER, P996, DOI 10.1109/INFCOM.1998.662909
   WOODS JW, 1972, IEEE T INFORM THEORY, V18, P232, DOI 10.1109/TIT.1972.1054786
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
NR 22
TC 3
Z9 4
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 19
EP 31
DI 10.1109/TMM.2005.861294
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000003
DA 2024-07-18
ER

PT J
AU Li, W
   Xue, XY
   Lu, PZ
AF Li, W
   Xue, XY
   Lu, PZ
TI Localized audio watermarking technique robust against time-scale
   modification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE localized watermarking; music content analysis; time-scale modification
   (TSM)
AB dSynchronization attacks like random cropping andtime-scale modification are very challenging problems to audio watermarking techniques. To combat these attacks, a novel content-dependent localized robust audio watermarking scheme is proposed. The basic idea is to first select steady high-energy local regions that represent music edges like note attacks, transitions or drum sounds by using different methods, then embed the watermark in these regions. Such regions are of great importance to the understanding of music and will not be changed much for maintaining high auditory quality. In this way, the embedded watermark has the potential to escape all kinds of distortions. Experimental results show strong robustness against common audio signal processing, time-domain synchronization attacks, and most distortions introduced in Stirmark for Audio.
C1 Fudan Univ, Dept Comp Sci & Engn, Shanghai 200433, Peoples R China.
C3 Fudan University
RP Fudan Univ, Dept Comp Sci & Engn, Shanghai 200433, Peoples R China.
EM weili-fudan@fudan.edu.cn; xyxue@fudan.edu.cn; pzlu@fudan.edu.cn
OI Li, Wei/0000-0002-4486-8341
CR DUXBURY C, 2001, P INT WORKSH DIG AUD
   HAMDY KN, 1997, P IEEE INT C AC SPEE, V1, P21
   Kennedy MP, 2000, SIGNAL PROCESS, V80, P1307, DOI 10.1016/S0165-1684(00)00038-4
   KIM HO, WAVELET BASED AUDIO
   Li W, 2003, COMPUT MUSIC J, V27, P58, DOI 10.1162/014892603322730505
   Mansour M., 2001, P IEEE INT C MULT EX, P76
   Mansour MF, 2001, INT CONF ACOUST SPEE, P1353, DOI 10.1109/ICASSP.2001.941179
   Tachibana R, 2001, P SOC PHOTO-OPT INS, V4314, P104, DOI 10.1117/12.435390
   TACHIBANA R, P IEEE PAC RIM C MUL, P647
   Tang CW, 2003, IEEE T SIGNAL PROCES, V51, P950, DOI 10.1109/TSP.2003.809367
   Wu CP, 2000, PROC SPIE, V3971, P382, DOI 10.1117/12.384992
NR 11
TC 108
Z9 125
U1 3
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 60
EP 69
DI 10.1109/TMM.2005.861291
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000006
DA 2024-07-18
ER

PT J
AU Feiten, B
   Wolf, I
   Oh, E
   Seo, J
   Kim, HK
AF Feiten, B
   Wolf, I
   Oh, E
   Seo, J
   Kim, HK
TI Audio adaptation according to usage environment and perceptual quality
   metrics
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE adaptation; audio; MPEG-21; perceptual metrics; quality of service; user
   preference
AB Future audio applications will need to handle a wide variety of end-devices and networks from low quality mobile services to very high quality home entertainment services. The MPEG-21 Digital Item Adaptation standard specifies a variety of tools that aim to enhance the user experience and maximize the perceived quality of service. This paper focuses on the audio-related tools, including audio usage environment descriptions and techniques for audio resource adaptation. Additionally, subjective and objective measures are discussed in the context of bit-rate adaptation. New quality measures, brightness, cleanness, and wideness are proposed for controlling the quality of the audio transmission.
C1 Deutsch Telekom, T Syst, D-10598 Berlin, Germany.
   Samsung Adv Inst Technol, Yongin, South Korea.
   Elect & Telecommun Res Inst, Taejon 305606, South Korea.
   Sejong Univ, Seoul, South Korea.
C3 Deutsche Telekom AG; Samsung; Electronics & Telecommunications Research
   Institute - Korea (ETRI); Sejong University
RP Feiten, B (corresponding author), Deutsch Telekom, T Syst, D-10598 Berlin, Germany.
EM bernhard.feiten@t-systems.com; wolfi@t-systems.com;
   oh@sait.samsung.co.kr; seoji@etri.re.kr; hkkim@sejong.ac.kr
RI kim, haekwang/G-1645-2013
CR Devillers S, 2005, IEEE T MULTIMEDIA, V7, P463, DOI 10.1109/TMM.2005.846794
   GARDNER WG, 1995, J AUDIO ENG SOC, V43, P127
   *ISO IEC, 2002, 210001 ISO IEC
   KEYHL M, 2000, P 109 AES C LOS ANG
   LEE SJ, 2000, P 112 AES C MUN GERM
   MARZINZIK M, 1996, PSYCHOACOUSTICS SPEE, P203
   MORRISON G, 2001, AQUAVIT ASSESSMENT Q
   MOULINES E, 1995, SPEECH COMMUN, V16, P175, DOI 10.1016/0167-6393(94)00054-E
   Mukherjee D, 2005, IEEE T MULTIMEDIA, V7, P454, DOI 10.1109/TMM.2005.846798
   *MUSHRA, 2001, BS1534 ITUR
   OH E, 2003, JTC1SC29WG11 ISO IEC
   Thiede T, 2000, J AUDIO ENG SOC, V48, P3
   WOLF I, 2003, IMPROVEMENTS AUDIO A
   WOLF I, 2004, 116 AES C BERL GERM
   2002, 159385 ISO IEC
   1994, METHOD SUBJECTIVE AS
   1999, 144961 ISO IEC
   1998, 1387 ITUR
NR 18
TC 14
Z9 17
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2005
VL 7
IS 3
BP 446
EP 453
DI 10.1109/TMM.2005.846793
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 949OO
UT WOS:000230798200007
DA 2024-07-18
ER

PT J
AU Wu, M
   Liu, BD
AF Wu, M
   Liu, BD
TI Data hiding in binary image for authentication and annotation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT 1st IEEE International Conference on Multimedia and Expo (ICME2000)
CY JUL 30-AUG 02, 2000
CL NEW YORK, NY
SP IEEE Circuits & Syst, IEEE Commun Soc, IEEE Comp Soc, IEEE Signal Processing Soc
DE annotation; authentication; binary image; data hiding; digital
   watermarking
ID WATERMARKING; VIDEO
AB This paper proposes a new method to embed data in binary images, including scanned text, figures, and signatures. The method manipulates "flippable" pixels to enforce specific block-based relationship in order to embed a significant amount of data without causing noticeable artifacts. Shuffling is applied before embedding to equalize the uneven embedding capacity from region to region. The hidden data can be extracted without using the original image, and can also be accurately extracted after high quality printing and scanning with the help of a few registration marks. The proposed data embedding method can be used to detect unauthorized use of a digitized signature, and annotate or authenticate binary documents. The paper also presents analysis and discussions on robustness and security issues.
C1 Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
   Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
C3 University System of Maryland; University of Maryland College Park;
   Princeton University
RP Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM minwu@eng.umd.edu; liu@princeton.edu
RI Wu, Min/P-2009-2019; Wu, Min/B-7501-2009
OI Wu, Min/0000-0001-7672-9357; 
CR [Anonymous], IEEE INT C MULT EXP
   Bhattacharjya A.K., 1999, P 1999 INT C, V2, P245
   Castleman K. R., 1996, Digital Image Processing
   Cox I., 2001, Digital Watermarking
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   FINKELSTEIN A, 1998, UNPUB COMMUNICATION
   Hartung F, 1999, P IEEE, V87, P1079, DOI 10.1109/5.771066
   Holliman M, 2000, IEEE T IMAGE PROCESS, V9, P432, DOI 10.1109/83.826780
   KOCH E, 1995, P INT C INT PROP RIG
   Liu Y, 1999, P SOC PHOTO-OPT INS, V3657, P317, DOI 10.1117/12.344682
   Matsuoka Y, 1994, J GLOBAL ENV ENG, V1, P1
   MAXEMCHUK NF, 1997, P IEEE ICIP 97
   Min Wu, 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P291, DOI 10.1109/ICIP.1999.821616
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Podilchuk CI, 1998, IEEE J SEL AREA COMM, V16, P525, DOI 10.1109/49.668975
   Schneier B., 1996, Applied Cryptography: Protocols, Algorithms, and Source Code in C
   Sedgewick R, 1990, ALGORITHMS C
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 2, P437, DOI 10.1109/ICIP.1998.723413
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P685, DOI 10.1109/TIP.2003.810588
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P696, DOI 10.1109/TIP.2003.810589
   WU M, 2001, THESIS PRINCETON U P
   Yeung MM, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL II, P680, DOI 10.1109/ICIP.1997.638587
NR 22
TC 220
Z9 276
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2004
VL 6
IS 4
BP 528
EP 538
DI 10.1109/tmm.2004.830814
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 838RC
UT WOS:000222729800002
DA 2024-07-18
ER

PT J
AU Cheung, G
   Tan, WT
   Yoshimura, T
AF Cheung, G
   Tan, WT
   Yoshimura, T
TI Double feedback streaming agent for real-time delivery of media over 3G
   wireless networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE multimedia communication; multimedia systems
AB A network agent located at the junction of wired and wireless networks can provide additional feedback information to streaming media servers to supplement feedbacks from clients. Specifically, it has been shown that feedbacks from the network agent have lower latency, and they can be used in conjunction with client feedbacks to effect proper congestion control. In this work, we propose the double-feedback streaming agent (DFSA) which further allows the detection of discrepancies in the transmission constraints of the wired and wireless networks. By working together with the streaming server and client, DFSA reduces overall packet losses by exploiting the excess capacity of the path with more capacity. We show how DFSA can be used to support three modes of operation tailored for different delay requirements of streaming applications. Simulation results under high wireless latency show significant improvement of media quality using DFSA over non-agent-based and earlier agent-based streaming systems.
C1 Hewlett Packard Labs, Tokyo 1680072, Japan.
   Hewlett Packard Labs, Palo Alto, CA 94304 USA.
   NTT DoCoMo Inc, Yokosuka, Kanagawa 2398536, Japan.
C3 Hewlett-Packard; Hewlett-Packard; NTT Docomo
RP Hewlett Packard Labs, Tokyo 1680072, Japan.
EM gene-cs.cheung@hp.com; dtan@hpl.hp.com; yoshi@spg.yrp.nttdocomo.co.jp
RI Cheung, Gene/AAB-9284-2020
OI Cheung, Gene/0000-0002-5571-4137
CR *3GPP TS, 2001, 26233 3GPP TS
   *3GPP TS, 2001, 26234 3GPP TS
   Balakrishnan H, 1997, IEEE ACM T NETWORK, V5, P756, DOI 10.1109/90.650137
   Chakareski J, 2002, PROCEEDINGS OF THE 2002 IEEE WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P49
   Chakareski J, 2003, IEEE DATA COMPR CONF, P203
   CHAKARESKI J, 2002, AS C SIGN SYST COMP, V2, P1310
   Cheung G, 2002, IEEE IMAGE PROC, P529
   Cheung G, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, pA81
   CHOU PA, 2001, MSRTR200135, V3
   Dahlman E, 1998, IEEE T VEH TECHNOL, V47, P1105, DOI 10.1109/25.728481
   Dogan S, 2002, IEEE T CIRC SYST VID, V12, P453, DOI 10.1109/TCSVT.2002.800308
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   GUNTER M, 2002, IEEE NETWORK MAG MAY, P22
   Holma H., 2002, WCDMA UMTS RADIO ACC, V3rd
   Inamura H., 2003, 3481 RFC
   KELLER R, 2000, INFOCOM, P1137
   Larzon L.-A., 1999, Technical report HPL-IRI-1999-001
   LEE A, 2001, GLOBECOM, V3, P25
   LIU B, 2002, IEEE GLOBECOM 02, V3, P2128
   Margaritidis M, 2000, IEEE PERS COMMUN, V7, P36, DOI 10.1109/98.892257
   MATSUOKA H, 2002, AS INT MOB COMP C AM
   MONTENEGRO G, 2000, 2757 IETF RFC
   OTT J, 2003, EXTENDED RTP PROFILE
   ROSENBERG J, 1999, 2733 IETF RFC
   SCHULZRINE H, 1996, 1889 IEFT RFC
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   WU R, 2002, QVOISE UMTS WCDMA ME
   YOSHIMURA T, 2002, IEEE INT C COMMUNICA, V4, P2513
   Zhang Q, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P579, DOI 10.1109/ICIP.2001.958184
   2003, NETWORK SIMULATOR NS
NR 30
TC 8
Z9 10
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 304
EP 314
DI 10.1109/TMM.2003.822794
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400009
DA 2024-07-18
ER

PT J
AU Sun, XY
   Wu, F
   Li, SP
   Gao, W
   Zhang, YQ
AF Sun, XY
   Wu, F
   Li, SP
   Gao, W
   Zhang, YQ
TI Seamless switching of scalable video bitstreams for efficient streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bitstream switching; fine granularity scalable video coding; scalable
   video coding; SP frame; video streaming
ID FINE GRANULARITY SCALABILITY; INTERNET
AB Efficient adaptation to channel bandwidth is broadly required for effective streaming video over the Internet. To address this requirement, a novel seamless switching scheme among scalable video bitstreams is proposed in this paper. It can significantly improve the performance of video streaming over a broad range of bit rates by fully taking advantage of both the high coding efficiency of nonscalable bitstreams and the flexibility of scalable bitstreams, where small channel bandwidth fluctuations are accommodated by the scalability of a single scalable bitstream, whereas large channel bandwidth fluctuations are tolerated by flexible switching between different scalable bitstreams. Two main techniques for switching between video bitstreams are proposed in this paper. Firstly, a novel coding scheme is proposed to enable drift-free switching at any frame from the current scalable bitstream to one operated at lower rates without sending any overhead bits. Secondly, an switching-frame coding scheme is proposed to greatly re-duce the number of extra bits needed for switching from the current scalable bitstream to one operated at higher rates. Compared with existing approaches, such as switching between nonscalable bitstreams and streaming with a single scalable bitstream, our experimental results clearly show that the proposed scheme brings higher efficiency and more flexibility in video streaming.
C1 Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
   Microsoft Res Asia, Beijing 100080, Peoples R China.
   Inst Comp Technol, Beijing 100080, Peoples R China.
C3 Harbin Institute of Technology; Microsoft Research Asia; Microsoft
RP Microsoft Res Asia, Beijing 100080, Peoples R China.
EM t-xysun@microsoft.com; fengwu@microsoft.com; spli@microsoft.com;
   wgao@ict.ac.cn; yzhang@microsoft.com
RI Li, Shipeng/AAA-3374-2020
OI Li, Shipeng/0000-0001-5368-4256
CR Civanlar MR, 2001, IEEE T CIRC SYST VID, V11, P265, DOI 10.1109/TCSVT.2001.911154
   Conklin GJ, 2001, IEEE T CIRC SYST VID, V11, P269, DOI 10.1109/76.911155
   FARBER N, 1997, P IEEE INT C IM PROC, V2, P73
   Gallant M, 2001, IEEE T CIRC SYST VID, V11, P357, DOI 10.1109/76.911161
   GIROD B, 1997, P 31 AS C SIGN SYST, V1, P357
   Huang HC, 2002, IEEE T CIRC SYST VID, V12, P372, DOI 10.1109/TCSVT.2002.800314
   KARCZEWISZ M, 2001, VCEGL27 ITU T Q 6 SG
   KURCERENR, 2001, VCEGM73 ITU T Q 6 SG
   LI W, 2000, ISCAS 2000, V1, P299
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   LING F, 1999, P SPIE VCIP 99 SAN J
   Lu J, 2000, PROC SPIE, V3974, P246, DOI 10.1117/12.382918
   PENG WS, 2001, ICIP 2001, P993
   Reibman AR, 2001, IEEE DATA COMPR CONF, P351, DOI 10.1109/DCC.2001.917166
   SCHAAR M, 2001, P IEEE 4 WORKSH MULT, P453
   SUN X, 2001, ICME 2001 TOK JAP AU
   SUN X, 2002, JOINT VIDEO TEAM JVT
   Tan WT, 2001, IEEE T CIRC SYST VID, V11, P373, DOI 10.1109/76.911162
   van der Schaar M, 2001, IEEE T CIRC SYST VID, V11, P318, DOI 10.1109/76.911158
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Wu F, 2001, IEEE T CIRC SYST VID, V11, P332, DOI 10.1109/76.911159
   Wu F, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P556, DOI 10.1109/ICIP.2000.899504
NR 22
TC 23
Z9 78
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 291
EP 303
DI 10.1109/TMM.2003.822818
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400008
OA Bronze
DA 2024-07-18
ER

PT J
AU Rodríguez, A
   Guil, N
   Shotton, DM
   Trelles, O
AF Rodríguez, A
   Guil, N
   Shotton, DM
   Trelles, O
TI Automatic analysis of the content of cell biological videos and database
   organization of their metadata descriptors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE biological videos; content analysis; content recognition; semantic
   metadata
ID GUEST EDITORS INTRODUCTION; SEMANTIC CONTENT-ANALYSIS; IMAGES
AB We present a video content analysis and metadata organizational system for research videos arising from biological microscopy of living cells. Automated procedures are described to determine the position, size, shape and orientation of cells in each video frame. From the temporal changes in the values of these simple metadata parameters, high-level descriptors are derived that describe the semantic content of the video. This content information (specific intrinsic metadata) is of high information value, since it describes the behavior of cells and the timing of events within the video, including changes in environmental conditions experienced by the cells. When such metadata are properly organized in a searchable database, a content-based video query and retrieval system may be developed to locate particular objects, events or behaviors. Moreover, the availability of such semantic contents in the formal and generic format we propose will allow the application of data mining techniques and the amassing of moire elaborate knowledge, e.g., species classification depending on behavior, patterns in response to environment changes, etc. The suitability and functionality of the proposed metadata model is demonstrated by the automated analysis of five different types of biological experiments, recording epithelial wound healing, bacterial multiplication, the rotations of tethered bacteria, and the swimming of motile bacteria and of human sperm.
C1 Univ Malaga, Comp Architecture Dept, Malaga 29017, Spain.
   Univ Oxford, Dept Zool, Image Bioinformat Lab, Oxford OX1 3PS, England.
C3 Universidad de Malaga; University of Oxford
EM andresr@ac.uma.es; nico@ac.uma.es; david.shotton@zoo.ox.ac.uk;
   ots@ac.uma.es
RI Guil, Nicolas/AAM-6160-2020; Moreno, Andrés Rodríguez/AAB-5176-2020
OI Guil, Nicolas/0000-0003-3431-6516; Moreno, Andrés
   Rodríguez/0000-0002-0431-2322; Trelles Salazar,
   Oswaldo/0000-0003-1554-8987
CR BALLARD DH, 1982, COMPUTER VISION, P143
   Boudier T, 1999, J STRUCT BIOL, V125, P133, DOI 10.1006/jsbi.1999.4097
   Carazo JM, 1999, J STRUCT BIOL, V125, P97, DOI 10.1006/jsbi.1999.4103
   Del Bimbo A, 1999, COMPUT VIS IMAGE UND, V75, P1, DOI 10.1006/cviu.1999.0775
   Guil N, 1999, PATTERN RECOGN, V32, P1025, DOI 10.1016/S0031-3203(98)00127-7
   GUPTA A, 1997, VISUAL INFORMATION R
   Hampapur A, 1997, P SOC PHOTO-OPT INS, V3022, P188, DOI 10.1117/12.263407
   Hobson G., 1996, HOBSON TRACKER USER
   Hunter J, 1999, COMPUT NETW, V31, P1431, DOI 10.1016/S1389-1286(99)00053-5
   HUNTER J, 1999, DDL WORKING DRAFT 1
   IN N, 2000, DOI HDB
   LEWIS JW, 1995, P ROY MICROSCOPY SOC, V30, P1
   LEWIS JW, 1994, EFFECTS COLCHICINE B
   LEWIS JW, 1994, P 4 ANN M EUR TISS R, P230
   LINDSAY A, 1999, MPEG 7 APPL DOC, V9
   Machtynger J, 2002, J MICROSC-OXFORD, V205, P43, DOI 10.1046/j.0022-2720.2001.00967.x
   Manson MD, 1998, J BACTERIOL, V180, P1009, DOI 10.1128/JB.180.5.1009-1022.1998
   Nack F, 1999, IEEE MULTIMEDIA, V6, P65, DOI 10.1109/93.790612
   Panchanathan S, 1996, J VIS COMMUN IMAGE R, V7, P305, DOI 10.1006/jvci.1996.0026
   Paskin N, 1999, P IEEE, V87, P1208, DOI 10.1109/5.771073
   RODRIGUEZ A, 2000, P RIAO 2000 6 C CONT
   RUST G, 1998, METADATA RIGHT APPRO
   RUST G, 1999, INTRO INDECS METADAT
   SALEMBIER P, 2000, IEEE INT C MULT EXP
   SALEMBIER PS, 2000, IEEE INT C IM PROC I
   Shotton DM, 1998, P NATL ACAD SCI USA, V95, P15571, DOI 10.1073/pnas.95.26.15571
   Shotton DM, 2002, J MICROSC-OXFORD, V205, P33, DOI 10.1046/j.0022-2720.2001.00966.x
   SHOTTON DM, 2000, P ICPR 2000 15 INT C
   Sussman JL, 1998, ACTA CRYSTALLOGR D, V54, P1078, DOI 10.1107/S0907444998009378
   VANBEEK P, 2000, MPEG 7 MULTIMEDIA DE
   PROPOSED SMPTE STAND
   2000, OVERVIEW MPEG 7 STAN
NR 32
TC 8
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 119
EP 128
DI 10.1109/TMM.2003.819581
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200009
DA 2024-07-18
ER

PT J
AU Leung, YW
   Chan, TKC
AF Leung, YW
   Chan, TKC
TI Design of an interactive video-on-demand system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE broadcast delivery paradigm; client-server program; video-on-demand
ID NETWORKS; DELIVERY; SERVICE
AB We design an interactive video-on-demand (VOD) system using both the client-server paradigm and the broadcast delivery, paradigm. Between the VOD warehouse and the customers, we adopt a client-server paradigm to provide an interactive service. Within the VOD warehouse, we adopt a broadcast delivery paradigm to support many concurrent customers. In particular, we exploit the enormous bandwidth of optical fibers for broadcast delivery, so that the system can provide many video program and maintain a small access delay, In addition, we design and adopt an interleaved broadcast delivery scheme, so that every video stream only requires a small buffer size for temporary storage. A simple proxy is allocated to each ongoing customer, and it retrieves video from the optical channels and delivers the video to the customer through an information network. The proposed VOD system is suitable for large scale applications with many customers, and it has several desirable features: 1) it can be scaled up to serve. more concurrent customers and provide more video programs, 2) it provides interactive operations, 3) it only requires point-to-point communication between the VOD warehouse and the customer and it does not involve any network control, 4) it has a small access delay, and 5) it requires a small buffer size for each video stream.
C1 Hong Kong Baptist Univ, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
C3 Hong Kong Baptist University
RP Leung, YW (corresponding author), Hong Kong Baptist Univ, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
CR AHUJA RK, 1993, NETWORK FLOWS THEORY, P87
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   ANDERSON DP, 1993, ACM T COMPUT SYST, V11, P226, DOI 10.1145/152864.152866
   BANKER RO, 1994, Patent No. 5357276
   Borella MS, 1997, P IEEE, V85, P1274, DOI 10.1109/5.622506
   BOWEN TF, 1992, COMMUN ACM, V35, P71, DOI 10.1145/138859.138868
   BRACKETT CA, 1990, IEEE J SEL AREA COMM, V8, P948, DOI 10.1109/49.57798
   Chatterjee S, 1999, COMMUN ACM, V42, P74, DOI 10.1145/303849.303865
   CHIUEH T, 1995, SPIE, V2615, P162
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   DAN A, 1995, J PARALLEL DISTR COM, V30, P168, DOI 10.1006/jpdc.1995.1135
   DELODDERE D, 1994, IEEE COMMUN MAG, V32, P82, DOI 10.1109/35.281582
   Furht B., 1995, Real-Time Imaging, V1, P319, DOI 10.1006/rtim.1995.1033
   Green PE, 1996, IEEE J SEL AREA COMM, V14, P764, DOI 10.1109/49.510902
   GROSS D, 1985, FUNDAMENTALS QUEUEIN, P294
   HU TC, 1982, COMBINATORIAL ALGORI, P209
   LIAO WJ, 1997, P IEEE INFOCOM JAP
   Nemhauser G., 1988, INTEGER COMBINATORIA, DOI DOI 10.1002/9781118627372
   Ozden B, 1996, MULTIMEDIA SYST, V4, P40, DOI 10.1007/s005300050011
   Reisslein M, 1998, IEEE NETWORK, V12, P46, DOI 10.1109/65.752644
   Scholl A, 1997, COMPUT OPER RES, V24, P627, DOI 10.1016/S0305-0548(96)00082-2
   Serpanos DN, 1998, IEEE T CIRC SYST VID, V8, P13, DOI 10.1109/76.660824
   TEWARI R, 1995, 20020 IBM RC
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
   Wilfong G, 1999, J LIGHTWAVE TECHNOL, V17, P1732, DOI 10.1109/50.793743
   WONG JW, 1988, P IEEE, V76, P1566, DOI 10.1109/5.16350
   YAKOWITZ S, 1989, INTRO NUMERICAL COMP
   1993, SEAGATE BARRACUDA DI
NR 28
TC 17
Z9 21
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI NEW YORK
PA 345 E 47TH ST, NEW YORK, NY 10017-2394 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2003
VL 5
IS 1
BP 130
EP 140
DI 10.1109/TMM.2003.808818
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 675HP
UT WOS:000182688200011
DA 2024-07-18
ER

PT J
AU Deng, QY
   Li, Q
   Cao, J
   Liu, YF
   Sun, ZN
AF Deng, Qiyao
   Li, Qi
   Cao, Jie
   Liu, Yunfan
   Sun, Zhenan
TI Semantic-Aware Noise Driven Portrait Synthesis and Manipulation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face manipulation; face synthesis; semantic noise
ID GENERATIVE ADVERSARIAL NETWORKS; IMAGE SYNTHESIS
AB Semantic portrait synthesis has drawn consistent attention and has made significant progress, yet achieving style diversity and semantic controllability simultaneously is still a challenge. Existing methods either 1) directly take a semantic label map as input, ignoring various possibilities of semantic styles, or 2) sample global noise as input, ignoring controllability of local semantics. To fill this gap, we propose semantic-aware noise, a simple but effective input that tackles both issues and shows improved results over baselines. Semantic-aware noise introduces semantic information into noise, and each semantic is sampled from the noise separately, combining the semantic controllability and the noise sampling diversity. To further expand and manipulate real images, we propose a novel ternary network structure, allowing simultaneous diverse semantic image synthesis and real image manipulation in a unified framework. Extensive experiments demonstrate that the proposed method achieves quantitatively superior and perceptually pleasing results compared to state-of-the-art methods. We also analyze the performance of our method with respect to different noise structures and real-life applications in diverse synthesis, interactive manipulation, and extreme pose scenarios.
C1 [Deng, Qiyao; Li, Qi; Cao, Jie; Liu, Yunfan; Sun, Zhenan] Chinese Acad Sci, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Deng, Qiyao; Liu, Yunfan; Sun, Zhenan] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Li, Q; Sun, ZN (corresponding author), Chinese Acad Sci, Ctr Res Intelligent Percept & Comp, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
EM dengqiyao@cripac.ia.ac.cn; qli@nlpr.ia.ac.cn; jie.cao@cripac.ia.ac.cn;
   yunfan.liu@cripac.ia.ac.cn; znsun@nlpr.ia.ac.cn
RI Liu, Yunfan/JAA-9308-2023
OI Liu, Yunfan/0000-0001-8929-4866; li, qi/0000-0002-7905-2860; cao,
   jie/0000-0001-6368-4495
FU National Key Research and Development Program of China [2020AAA0140002];
   Natural Science Foundation of China [U1836217, 62076240]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020AAA0140002, and in part by
   the Natural Science Foundation of China under Grants U1836217 and
   62076240.
CR Alharbi Y, 2020, PROC CVPR IEEE, P5133, DOI 10.1109/CVPR42600.2020.00518
   Cao J, 2018, ADV NEUR IN, V31
   Cao J, 2019, IEEE T INF FOREN SEC, V14, P2028, DOI 10.1109/TIFS.2019.2891116
   Chen QF, 2017, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2017.168
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Collins E, 2020, PROC CVPR IEEE, P5770, DOI 10.1109/CVPR42600.2020.00581
   Deng QY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P502
   Deng QY, 2021, IEEE T INF FOREN SEC, V16, P1410, DOI 10.1109/TIFS.2020.3033184
   Deng Y, 2020, PROC CVPR IEEE, P5153, DOI 10.1109/CVPR42600.2020.00520
   Esser P, 2021, PROC CVPR IEEE, P12868, DOI 10.1109/CVPR46437.2021.01268
   Ghosh S, 2019, NATL CONF COMMUN, DOI 10.1109/ncc.2019.8732250
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ghosh S, 2017, IET IMAGE PROCESS, V11, P317, DOI 10.1049/iet-ipr.2016.0331
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   He ZL, 2019, IEEE T IMAGE PROCESS, V28, P5464, DOI 10.1109/TIP.2019.2916751
   Hensel M, 2017, ADV NEUR IN, V30
   Hong Sarah Jane, 2020, P ADV NEUR INF PROC, V33
   Huang JL, 2022, IEEE T MULTIMEDIA, V24, P1435, DOI 10.1109/TMM.2021.3065230
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jie Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P404, DOI 10.1007/978-3-030-58529-7_24
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Karras Tero, 2020, IEEE C COMP VIS PATT
   Kingma D. P., 2014, arXiv
   Ngo LM, 2022, IEEE T MULTIMEDIA, V24, P377, DOI 10.1109/TMM.2021.3050672
   Lee CH, 2020, PROC CVPR IEEE, P5548, DOI 10.1109/CVPR42600.2020.00559
   Li Q., 2020, P AAAI C ART INT, p11 378
   Li ZY, 2021, IEEE T MULTIMEDIA, V23, P2694, DOI 10.1109/TMM.2020.3015015
   Liu YF, 2020, Arxiv, DOI arXiv:2011.09699
   Mao Q, 2019, PROC CVPR IEEE, P1429, DOI 10.1109/CVPR.2019.00152
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Park Taesung, 2020, Advances in Neural Information Processing Systems, V33, P7198
   Qi XJ, 2018, PROC CVPR IEEE, P8808, DOI 10.1109/CVPR.2018.00918
   Shen Yujun, 2020, P IEEE CVF C COMP VI, P9243, DOI DOI 10.1109/CVPR42600.2020.00926
   Shoshan A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14063, DOI 10.1109/ICCV48922.2021.01382
   Shuyang Gu, 2019, 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P3431, DOI 10.1109/CVPR.2019.00355
   Song JK, 2022, IEEE T MULTIMEDIA, V24, P791, DOI 10.1109/TMM.2021.3059336
   Song LS, 2019, AAAI CONF ARTIF INTE, P2506
   Tan ZT, 2021, PROC CVPR IEEE, P7958, DOI 10.1109/CVPR46437.2021.00787
   Tan ZT, 2022, IEEE T PATTERN ANAL, V44, P4852, DOI 10.1109/TPAMI.2021.3076487
   Tewari A, 2020, PROC CVPR IEEE, P6141, DOI 10.1109/CVPR42600.2020.00618
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang Y, 2020, PROC CVPR IEEE, P5093, DOI 10.1109/CVPR42600.2020.00514
   Yang D., 2019, PROC INT C LEARN REP
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, ADV NEUR IN, V30
   Zhu PH, 2020, PROC CVPR IEEE, P5103, DOI 10.1109/CVPR42600.2020.00515
   Zhu Z, 2020, PROC CVPR IEEE, P5466, DOI 10.1109/CVPR42600.2020.00551
NR 52
TC 1
Z9 1
U1 17
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2799
EP 2811
DI 10.1109/TMM.2022.3151507
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600027
DA 2024-07-18
ER

PT J
AU Han, XJ
   Song, XM
   Dong, XN
   Wei, YW
   Liu, M
   Nie, LQ
AF Han, Xianjing
   Song, Xuemeng
   Dong, Xingning
   Wei, Yinwei
   Liu, Meng
   Nie, Liqiang
TI DBiased-P: Dual-Biased Predicate Predictor for Unbiased Scene Graph
   Generation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Head; Tail; Decoding; Visualization; Predictive models; Detectors;
   Training data; Scene graph generation; vision and language; Re-weighting
   classification; KL-divergence
AB Scene Graph Generation (SGG) is to abstract the objects and their semantic relationships within a given image. Current SGG performance is mainly limited by the biased predicate prediction caused by the long-tailed data distribution. Though many unbiased SGG methods have emerged to enhance the prediction of the tail predicates, their improvements on the tail predicates are often accompanied by the deterioration on the head ones, leading the prediction overly debiased. Toward this end, in this work, we propose a Dual-Biased Predicate Predictor (DBiased-P) to boost the unbiased SGG, which comprises a re-weighted primary classifier and an unweighted auxiliary classifier. The former classifier is tail-biased and used for the final predicate prediction, while the latter one is head-biased and designed to boost the head predicate prediction of the primary classifier by a head-oriented soft regularization. Experiments conducted on Visual Genome and Open Image datasets indicate the superiority of our DBiased-P in unbiased SGG, which significantly improves the recall@50 of the state-of-the-art unbiased SGG method DT2-ACBS from 23.3% to 55.5% as well as the mean recall@50 from 35.9% to 37.7%.
C1 [Han, Xianjing; Song, Xuemeng; Dong, Xingning; Nie, Liqiang] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Wei, Yinwei] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Liu, Meng] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
C3 Shandong University; National University of Singapore; Shandong Jianzhu
   University
RP Nie, LQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM hanxianjing2018@gmail.com; sxmustc@gmail.com; pass1463365882@gmail.com;
   weiyinwei@hotmail.com; mengliu.sdu@gmail.com; nieliqiang@gmail.com
RI Wei, Yinwei/JHX-9398-2023
OI Wei, Yinwei/0000-0003-1791-3159; Han, Xianjing/0000-0001-7867-3190; Liu,
   Meng/0000-0002-1582-5764
FU National Key Research and Development Project of New Generation
   Artificial Intelligence [2018AAA0102502]; National Natural Science
   Foundation of China [61772310, 61702300, 62006142, U1936203]; Natural
   Science Foundation of Shandong Province [ZR2019JQ23]; Shandong
   Provincial Key Research and Development Program [2019JZZY010118];
   Shandong Provincial Natural Science Foundation for Distinguished Young
   Scholars [ZR2021JQ26]; Major Basic Research Project of Natural Science
   Foundation of Shandong Province [ZR2021ZD15]; Science and Technology
   Innovation Program for Distinguished Young Scholars of Shandong Province
   Higher Education Institutions [2021KJ036]; Innovation Teams in Colleges
   and Universities in Jinan [2018GXRC014]
FX This work was supported in part by the National Key Research and
   Development Project of New Generation Artificial Intelligence under
   Grant 2018AAA0102502, in part by the National Natural Science Foundation
   of China under Grants 61772310, 61702300, 62006142, and U1936203, in
   part by the Natural Science Foundation of Shandong Province under Grant
   ZR2019JQ23, in part by the Shandong Provincial Key Research and
   Development Program under Grant 2019JZZY010118, in part by the Shandong
   Provincial Natural Science Foundation for Distinguished Young Scholars
   under Grant ZR2021JQ26, in part by the Major Basic Research Project of
   Natural Science Foundation of Shandong Province under Grant ZR2021ZD15,
   in part by the Science and Technology Innovation Program for
   Distinguished Young Scholars of Shandong Province Higher Education
   Institutions under Grant 2021KJ036, and in part by the Innovation Teams
   in Colleges and Universities in Jinan under Grant 2018GXRC014.
CR Bin Y, 2019, AAAI CONF ARTIF INTE, P8110
   Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632
   Chen VS, 2019, IEEE I CONF COMP VIS, P2580, DOI [10.1109/iccv.2019.00267, 10.1109/ICCV.2019.00267]
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Cui Z, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1475, DOI 10.1145/3240508.3240668
   Desai A, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P15384, DOI 10.1109/ICCV48922.2021.01512
   Elkan C., 2001, IJCAI 2001, V17, P973, DOI DOI 10.5555/1642194.1642224
   Gu JX, 2019, PROC CVPR IEEE, P1969, DOI 10.1109/CVPR.2019.00207
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Kolesnikov A, 2019, IEEE INT CONF COMP V, P1749, DOI 10.1109/ICCVW.2019.00217
   Krishna R., 2016, Visual genome: Connecting language and vision using crowdsourced dense image annotations, V123, P32
   Kuznetsova A, 2020, INT J COMPUT VISION, V128, P1956, DOI 10.1007/s11263-020-01316-z
   Lertnattee V, 2004, IEEE INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES 2004 (ISCIT 2004), PROCEEDINGS, VOLS 1 AND 2, P1171
   Li RJ, 2021, PROC CVPR IEEE, P11104, DOI 10.1109/CVPR46437.2021.01096
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Liao WT, 2019, IEEE COMPUT SOC CONF, P444, DOI 10.1109/CVPRW.2019.00058
   Lin X, 2020, PROC CVPR IEEE, P3743, DOI 10.1109/CVPR42600.2020.00380
   Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12
   Mikolov Tomas, 2013, Advances in Neural Information Processing Systems, P3111, DOI DOI 10.48550/ARXIV.1310.4546
   Ouyang NL, 2021, IEEE T MULTIMEDIA, V24, P3405, DOI 10.1109/TMM.2021.3097502
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Qi MS, 2019, PROC CVPR IEEE, P3952, DOI 10.1109/CVPR.2019.00408
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sharifzadeh S, 2021, AAAI CONF ARTIF INTE, V35, P5025
   Suhail M, 2021, PROC CVPR IEEE, P13931, DOI 10.1109/CVPR46437.2021.01372
   Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556
   Tajrobehkar M, 2022, IEEE T MULTIMEDIA, V24, P1266, DOI 10.1109/TMM.2021.3062543
   Tang KH, 2020, PROC CVPR IEEE, P3713, DOI 10.1109/CVPR42600.2020.00377
   Tang KH, 2019, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2019.00678
   Ting KM, 2002, IEEE T KNOWL DATA EN, V14, P659, DOI 10.1109/TKDE.2002.1000348
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang WB, 2019, PROC CVPR IEEE, P8180, DOI 10.1109/CVPR.2019.00838
   Wu J, 2021, IEEE T MULTIMEDIA, V23, P2413, DOI 10.1109/TMM.2020.3011317
   Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330
   Yan ST, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P265, DOI 10.1145/3394171.3413722
   Yang JW, 2018, LECT NOTES COMPUT SC, V11205, P690, DOI 10.1007/978-3-030-01246-5_41
   Yang M, 2019, IEEE T MULTIMEDIA, V21, P1047, DOI 10.1109/TMM.2018.2869276
   Yu J., 2021, INT JOINT C ARTIFICI, P1274
   Yu J, 2020, IEEE T MULTIMEDIA, V22, P3196, DOI 10.1109/TMM.2020.2972830
   Yu RC, 2017, IEEE I CONF COMP VIS, P1068, DOI 10.1109/ICCV.2017.121
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P642, DOI 10.1007/978-3-030-58592-1_38
   Zareian Alireza, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P606, DOI 10.1007/978-3-030-58592-1_36
   Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611
   Zhang J, 2019, PROC CVPR IEEE, P11527, DOI 10.1109/CVPR.2019.01180
   Zhou H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P30, DOI 10.1145/3343031.3351024
NR 46
TC 3
Z9 3
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5319
EP 5329
DI 10.1109/TMM.2022.3190135
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300050
DA 2024-07-18
ER

PT J
AU Hong, YC
   Lyu, Y
   Li, S
   Cao, G
   Shi, BX
AF Hong, Yuchen
   Lyu, Youwei
   Li, Si
   Cao, Gang
   Shi, Boxin
TI Reflection Removal With NIR and RGB Image Feature Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Reflection; Imaging; Glass; Cameras; Pipelines; Computer vision;
   Visualization; Reflection removal; deep learning; feature fusion;
   near-infrared image
ID STRUCTURAL SIMILARITY; SEPARATION
AB Removing undesirable reflections in photographs benefits both human perceptions and downstream computer vision tasks, but it is a highly ill-posed problem based on a single RGB image. Different from RGB images, near-infrared (NIR) images captured by an active NIR camera are less likely to be affected by reflections when glass and camera planes form certain angles, while textures on objects could "vanish" in some situations. Based on this observation, we propose a cascaded reflection removal network with an image feature fusion strategy to utilize auxiliary information in active NIR images. To tackle the insufficiency of training data, we propose a data generation pipeline to approximate perceptual properties and the reflection-suppressing nature of active NIR images. We further build a dataset with synthetic and real images to facilitate the research. Experimental results show that the proposed method outperforms state-of-the-art reflection removal methods in both quantitative metrics and visual quality.
C1 [Hong, Yuchen; Shi, Boxin] Peking Univ, Natl Engn Res Ctr Visual Technol, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Lyu, Youwei; Li, Si] Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
   [Cao, Gang] Beijing Acad Artificial Intelligence, Beijing 100084, Peoples R China.
   [Shi, Boxin] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Peking University; Beijing University of Posts & Telecommunications;
   Peng Cheng Laboratory
RP Li, S (corresponding author), Beijing Univ Posts & Telecommun, Sch Artificial Intelligence, Beijing 100876, Peoples R China.
EM yuchenhong.cn@gmail.com; youweilv@gmail.com; lisi@bupt.edu.cn;
   caogang@baai.ac.cn; shiboxin@pku.edu.cn
OI Lyu, Youwei/0000-0002-6723-3517; Hong, Yuchen/0000-0003-2772-217X
FU National Key R&D Program of China [2020AAA0105200]; National Natural
   Science Foundation of China [62136001]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020AAA0105200 and in part by the National Natural Science
   Foundation of China under Grant 62136001.
CR Brown M, 2011, PROC CVPR IEEE, P177, DOI 10.1109/CVPR.2011.5995637
   Chang YK, 2020, INT J COMPUT VISION, V128, P1673, DOI 10.1007/s11263-019-01276-z
   Cheng Z, 2019, IEEE I CONF COMP VIS, P2521, DOI 10.1109/ICCV.2019.00261
   Choe G, 2018, IEEE ROBOT AUTOM LET, V3, P1808, DOI 10.1109/LRA.2018.2801390
   Choe GM, 2014, PROC CVPR IEEE, P3922, DOI 10.1109/CVPR.2014.501
   Dong Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4997, DOI 10.1109/ICCV48922.2021.00497
   Fan QN, 2017, IEEE I CONF COMP VIS, P3258, DOI 10.1109/ICCV.2017.351
   Fankhauser P, 2015, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P388, DOI 10.1109/ICAR.2015.7251485
   Feng C, 2013, IEEE IMAGE PROC, P2363, DOI 10.1109/ICIP.2013.6738487
   Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hecht E., 2019, OPTICS, V5th
   Hong Y., 2020, P INT C MULT EXP, P1
   Huynh-Thu Q, 2008, ELECTRON LETT, V44, P800, DOI 10.1049/el:20080522
   Jolicoeur-Martineau A., 2018, The relativistic discriminator: A key element missing from standard GAN
   Kim C, 2015, LECT NOTES ELECTR EN, V352, P111, DOI 10.1007/978-3-662-47487-7_17
   Kim S, 2020, PROC CVPR IEEE, P5163, DOI 10.1109/CVPR42600.2020.00521
   Kingma D. P., 2014, arXiv
   Kong NJ, 2014, IEEE T PATTERN ANAL, V36, P209, DOI 10.1109/TPAMI.2013.45
   Lei CY, 2020, PROC CVPR IEEE, P1747, DOI 10.1109/CVPR42600.2020.00182
   Levin A, 2004, PROC CVPR IEEE, P306
   Levin A., 2002, Adv. Neural lnf. Process. Syst., P1271
   Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106
   Li C, 2020, PROC CVPR IEEE, P3562, DOI 10.1109/CVPR42600.2020.00362
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Lyu Youwei, 2019, P ADV NEURAL INFORM, V32, P14559
   Paszke A, 2019, ADV NEUR IN, V32
   Rüfenacht D, 2014, IEEE T PATTERN ANAL, V36, P1672, DOI 10.1109/TPAMI.2013.229
   Salamati N, 2012, LECT NOTES COMPUT SC, V7584, P461, DOI 10.1007/978-3-642-33868-7_46
   Shih YC, 2015, PROC CVPR IEEE, P3193, DOI 10.1109/CVPR.2015.7298939
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655
   Sun J, 2019, IEEE SIGNAL PROC LET, V26, P1011, DOI 10.1109/LSP.2019.2915560
   Sun SH, 2014, IEEE IMAGE PROC, P4482, DOI 10.1109/ICIP.2014.7025909
   Wan RJ, 2020, IEEE T PATTERN ANAL, V42, P2969, DOI 10.1109/TPAMI.2019.2921574
   Wan RJ, 2018, PROC CVPR IEEE, P4777, DOI 10.1109/CVPR.2018.00502
   Wan RJ, 2018, IEEE T IMAGE PROCESS, V27, P2927, DOI 10.1109/TIP.2018.2808768
   Wan RJ, 2017, IEEE I CONF COMP VIS, P3942, DOI 10.1109/ICCV.2017.423
   Wan RJ, 2016, IEEE IMAGE PROC, P21, DOI 10.1109/ICIP.2016.7532311
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wei KX, 2019, PROC CVPR IEEE, P8170, DOI 10.1109/CVPR.2019.00837
   Wen Q, 2019, PROC CVPR IEEE, P3766, DOI 10.1109/CVPR.2019.00389
   Wieschollek Patrick, 2018, P EUROPEAN C COMPUTE
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Yoo JC, 2009, CIRC SYST SIGNAL PR, V28, P819, DOI [10.1007/s00034-009-9130-7, 10.1007/S00034-009-9130-7]
   Zhang HD, 2020, IEEE T MULTIMEDIA, V22, P2012, DOI 10.1109/TMM.2019.2951461
   Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503
NR 47
TC 4
Z9 4
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7101
EP 7112
DI 10.1109/TMM.2022.3217446
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000029
DA 2024-07-18
ER

PT J
AU Hua, GL
   Liu, H
   Li, WH
   Zhang, Q
   Ding, RW
   Xu, X
AF Hua, Guoliang
   Liu, Hong
   Li, Wenhao
   Zhang, Qian
   Ding, Runwei
   Xu, Xin
TI Weakly-Supervised 3D Human Pose Estimation With Cross-View U-Shaped
   Graph Convolutional Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3D human pose estimation; cross-view; graph convolutional network;
   weakly-supervised learning
AB Although monocular 3D human pose estimation methods have made significant progress, it is far from being solved due to the inherent depth ambiguity. Instead, exploiting multi-view information is a practical way to achieve absolute 3D human pose estimation. In this paper, we propose a simple yet effective pipeline for weakly-supervised cross-view 3D human pose estimation. By only using two camera views, our method can achieve state-of-the-art performance in a weakly-supervised manner, requiring no 3D ground truth but only 2D annotations. Specifically, our method contains two steps: triangulation and refinement. First, given the 2D keypoints that can be obtained through any classic 2D detection methods, triangulation is performed across two views to lift the 2D keypoints into coarse 3D poses. Then, a novel cross-view U-shaped graph convolutional network (CV-UGCN), which can explore the spatial configurations and cross-view correlations, is designed to refine the coarse 3D poses. In particular, the refinement progress is achieved through weakly-supervised learning, in which geometric and structure-aware consistency checks are performed. We evaluate our method on the standard benchmark dataset, Human3.6M. The Mean Per Joint Position Error on the benchmark dataset is 27.4 mm, which outperforms existing state-of-the-art methods remarkably (27.4 mm vs 30.2 mm).
C1 [Hua, Guoliang; Liu, Hong; Li, Wenhao; Zhang, Qian; Ding, Runwei] Peking Univ, Key Lab Machine Percept, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
   [Xu, Xin] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
C3 Peking University; National University of Defense Technology - China
RP Liu, H (corresponding author), Peking Univ, Key Lab Machine Percept, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM glhua@pku.edu.cn; hongliu@pku.edu.cn; wenhaoli@pku.edu.cn;
   qian.zhang@pku.edu.cn; dingrunwei@pku.edu.cn; xinxu@nudt.edu.cn
RI 徐, 昕/JNS-1298-2023
OI 徐, 昕/0000-0003-3238-745X; Hua, Guoliang/0000-0003-4666-8551; Liu,
   Hong/0000-0002-7498-6541
FU National Natural Science Foundation of China [62073004, 61825305];
   Shenzhen Fundamental Research Program
   [GXWD20201231165807007-20200807164903001, JCYJ20190808182209321,
   JCYJ20200109140410340]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62073004 and 61825305, and in part by
   Shenzhen Fundamental Research Program under Grants
   GXWD20201231165807007-20200807164903001, JCYJ20190808182209321, and
   JCYJ20200109140410340.
CR Ailing Zeng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P507, DOI 10.1007/978-3-030-58568-6_30
   Cai YJ, 2019, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2019.00236
   Chen X., 2014, Advances in neural information processing systems, P1736, DOI DOI 10.1109/CVPR.2018.00742
   Chen XP, 2019, PROC CVPR IEEE, P10887, DOI 10.1109/CVPR.2019.01115
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Ching-Hang Chen, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P5759, DOI 10.1109/CVPR.2017.610
   Dabral R, 2018, LECT NOTES COMPUT SC, V11213, P679, DOI 10.1007/978-3-030-01240-3_41
   Fabbri M, 2020, PROC CVPR IEEE, P7202, DOI 10.1109/CVPR42600.2020.00723
   Fang HS, 2018, AAAI CONF ARTIF INTE, P6821
   He Y., 2020, P IEEE C COMP VIS PA, P7779
   Hossain MRI, 2018, LECT NOTES COMPUT SC, V11214, P69, DOI 10.1007/978-3-030-01249-6_5
   Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248
   Iqbal U, 2020, PROC CVPR IEEE, P5242, DOI 10.1109/CVPR42600.2020.00529
   Iskakov K, 2019, IEEE I CONF COMP VIS, P7717, DOI 10.1109/ICCV.2019.00781
   Jingbo Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12358), P764, DOI 10.1007/978-3-030-58601-0_45
   Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117
   Kundu JN, 2020, PROC CVPR IEEE, P6151, DOI 10.1109/CVPR42600.2020.00619
   Lee K, 2018, LECT NOTES COMPUT SC, V11211, P123, DOI 10.1007/978-3-030-01234-2_8
   Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23
   Li W., 2022, P IEEE C COMP VIS PA, p19 447
   Li W., 2022, IEEE Transactions on Multimedia
   Liu RX, 2020, PROC CVPR IEEE, P5063, DOI 10.1109/CVPR42600.2020.00511
   Martinez J, 2017, IEEE I CONF COMP VIS, P2659, DOI 10.1109/ICCV.2017.288
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Pavlakos G, 2017, PROC CVPR IEEE, P1263, DOI 10.1109/CVPR.2017.139
   Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138
   Pavllo D, 2019, PROC CVPR IEEE, P7745, DOI 10.1109/CVPR.2019.00794
   Qiu HB, 2019, IEEE I CONF COMP VIS, P4341, DOI 10.1109/ICCV.2019.00444
   Remelli E, 2020, PROC CVPR IEEE, P6039, DOI 10.1109/CVPR42600.2020.00608
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Tianlang Chen, 2022, IEEE Transactions on Circuits and Systems for Video Technology, V32, P198, DOI 10.1109/TCSVT.2021.3057267
   Tome D, 2018, INT CONF 3D VISION, P474, DOI 10.1109/3DV.2018.00061
   Tome D, 2017, PROC CVPR IEEE, P5689, DOI 10.1109/CVPR.2017.603
   Trumble M., 2017, P BRIT MACH VIS C, V2, P1, DOI 10.5244/C.31.14
   Wandt B, 2019, PROC CVPR IEEE, P7774, DOI 10.1109/CVPR.2019.00797
   Wang CY, 2019, IEEE I CONF COMP VIS, P743, DOI 10.1109/ICCV.2019.00083
   Xu TH, 2021, PROC CVPR IEEE, P16100, DOI 10.1109/CVPR46437.2021.01584
   Zhou XY, 2017, IEEE I CONF COMP VIS, P398, DOI 10.1109/ICCV.2017.51
NR 40
TC 10
Z9 10
U1 8
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1832
EP 1843
DI 10.1109/TMM.2022.3171102
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, W
   Zhou, YT
   Cheung, YM
   Zhang, P
   Zha, YF
   Pang, M
AF Huang, Wei
   Zhou, Yintao
   Cheung, Yiu-ming
   Zhang, Peng
   Zha, Yufei
   Pang, Meng
TI Facial Expression Guided Diagnosis of Parkinson's Disease via
   High-Quality Data Augmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Training data; Faces; Predictive models; Diseases;
   Deep learning; Neural networks; Data augmentation; deep learning;
   multi-domain adversarial learning; Parkinson's disease diagnosis
ID PARKINSONS-DISEASE; GAIT; FRAMEWORK; FEATURES; FACES
AB Parkinson's disease (PD) is a neurodegenerative disease which is prevalent among the elder population and severely affects the life quality of patients and their families. Therefore, it is important to conduct an early diagnosis for potential patients with PD, so as to promote prompt treatment and avoid the aggravation of the disease. Recently, the in-vitro PD diagnosis based on facial expressions has received increasing attention because of its distinguishability (i.e., PD patients always possess the characteristics of "masked face") and affordability. However, the performance of the existing facial expression-based PD diagnosis approaches is limited by: 1) the small-scale training data on PD patients' facial expressions, and 2) the weak prediction model. To address these two problems, we propose a new facial expression guided PD diagnosis method based on high-quality training data augmentation and deep neural network prediction. Specifically, the proposed method consists of three stages: Firstly, we synthesize virtual facial expression images with 6 basic emotions (i.e., anger, disgust, fear, happiness, sadness, and surprise) based on multi-domain adversarial learning to approximate the premorbid expressions of PD patients. Secondly, we introduce three facial image quality assessment (FIQA) criteria to measure the quality of these synthesized facial expression images and design a fusion screening strategy that shortlists the high-quality ones to augment the training data. Finally, we train a deep neural network prediction model based on the original and synthesized high-quality facial expression images for PD diagnosis. To show real-world impacts and evaluate the proposed method under different facial expressions, we also create a (currently largest) multiple facial expressions-based PD face dataset in collaboration with a hospital. Extensive experiments are performed to demonstrate the effectiveness of the multi-domain adversarial learning-based facial expression synthesis and the fusion screening strategy, particularly the superior performance of the proposed method for PD diagnosis.
C1 [Huang, Wei; Zhou, Yintao; Pang, Meng] Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
   [Cheung, Yiu-ming] Hong Kong Baptist Univ, Dept Comp Sci, Hong Kong 999077, Peoples R China.
   [Zhang, Peng; Zha, Yufei] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Zhang, Peng; Zha, Yufei] Northwestern Polytech Univ, Ningbo Inst, Ningbo 315048, Peoples R China.
   [Pang, Meng] Nanchang Univ, Inst Math & Interdisciplinary Sci, Nanchang 330031, Peoples R China.
C3 Nanchang University; Hong Kong Baptist University; Northwestern
   Polytechnical University; Northwestern Polytechnical University;
   Nanchang University
RP Pang, M (corresponding author), Nanchang Univ, Sch Math & Comp Sci, Nanchang 330031, Peoples R China.
EM n060101@e.ntu.edu.sg; yintaozhou@email.ncu.edu.cn; ymc@comp.hkbu.edu.hk;
   zh0036ng@nwpu.edu.cn; yufeizha@nwpu.edu.cn; pangmeng1992@gmail.com
RI Cheung, Yiu-ming/E-2050-2015; Li, Chun/KBC-9591-2024
OI Cheung, Yiu-ming/0000-0001-7629-4648; Huang, Wei/0000-0002-0541-8612;
   Pang, Meng/0000-0001-7184-2043; Zha, yufei/0000-0001-5013-2501; Zhou,
   Yintao/0000-0001-8411-9313
FU National Natural Science Foundation of China [62271239, 61862043,
   61971352]; Natural Science Foundation of Ningbo [2021J048, 2021J049];
   NSFC/Research Grants Council (RGC) Joint Research Scheme [N_HKBU214/21];
   General Research Fund of RGC [12201321]; Hong Kong Baptist University
   (HKBU) [RC-FNRA-IG/18-19/SCI/03]
FX The work of Wei Huang, Peng Zhang, and Yufei Zha wassupported in part by
   the National Natural Science Foundation of China underGrants 62271239,
   61862043, and 61971352, and in part by the Natural Science Foundation of
   Ningbo under Grants 2021J048 and 2021J049. The work ofYiu-ming Cheung
   was supported in part by NSFC/Research Grants Council(RGC) Joint
   Research Scheme under Grant N_HKBU214/21, in part by the General
   Research Fund of RGC under Grant 12201321, and in part by the Hong Kong
   Baptist University (HKBU) underGrant RC-FNRA-IG/18-19/SCI/03.
CR Abdulhay E, 2018, FUTURE GENER COMP SY, V83, P366, DOI 10.1016/j.future.2018.02.009
   Ali MR, 2021, NPJ DIGIT MED, V4, DOI 10.1038/s41746-021-00502-8
   Balaji E, 2021, MED ENG PHYS, V91, P54, DOI 10.1016/j.medengphy.2021.03.005
   Bandini A, 2017, J NEUROSCI METH, V281, P7, DOI 10.1016/j.jneumeth.2017.02.006
   Barth J, 2011, IEEE ENG MED BIO, P868, DOI 10.1109/IEMBS.2011.6090226
   Brewer BR, 2009, IEEE T NEUR SYS REH, V17, P568, DOI 10.1109/TNSRE.2009.2034461
   Cao Q, 2018, IEEE INT CONF AUTOMA, P67, DOI 10.1109/FG.2018.00020
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chén OY, 2020, IEEE T BIO-MED ENG, V67, P3491, DOI 10.1109/TBME.2020.2988942
   Chen X, 2013, IEEE T MULTIMEDIA, V15, P1049, DOI 10.1109/TMM.2013.2245319
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Dauer W, 2003, NEURON, V39, P889, DOI 10.1016/S0896-6273(03)00568-3
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dorsey ER, 2018, J PARKINSON DIS, V8, pS3, DOI 10.3233/JPD-181474
   Ferrara M, 2012, IEEE T INF FOREN SEC, V7, P1204, DOI 10.1109/TIFS.2012.2198643
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goswami G, 2018, AAAI CONF ARTIF INTE, P6829
   Grammatikopoulou A, 2019, 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO ASSISTIVE ENVIRONMENTS (PETRA 2019), P517, DOI 10.1145/3316782.3322756
   Guo R, 2022, IEEE T MULTIMEDIA, V24, P1583, DOI 10.1109/TMM.2021.3068609
   Guo R, 2020, IEEE T NEUR SYS REH, V28, P2837, DOI 10.1109/TNSRE.2020.3039297
   Guo YD, 2016, LECT NOTES COMPUT SC, V9907, P87, DOI 10.1007/978-3-319-46487-9_6
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hernandez-Ortega J, 2019, INT CONF BIOMETR
   Hou Xinyao, 2022, Proceedings of 2021 Chinese Intelligent Automation Conference. Lecture Notes in Electrical Engineering (801), P249, DOI 10.1007/978-981-16-6372-7_29
   Hsu SC, 2017, J ACOUST SOC AM, V141, pEL293, DOI 10.1121/1.4978342
   Huang W, 2021, IEEE T MULTIMEDIA, V24, P3327, DOI 10.1109/TMM.2021.3096068
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   James SL, 2019, LANCET NEUROL, V18, P56, DOI [10.1016/S1474-4422(18)30499-X, 10.1016/S1474-4422(18)30415-0, 10.1016/S1474-4422(19)30034-1]
   Jankovic J, 2008, J NEUROL NEUROSUR PS, V79, P368, DOI 10.1136/jnnp.2007.131045
   Jin B, 2020, J MED INTERNET RES, V22, DOI 10.2196/18697
   Kingma D. P., 2014, arXiv
   Laganas C, 2022, IEEE T BIO-MED ENG, V69, P1573, DOI 10.1109/TBME.2021.3116935
   Langner O, 2010, COGNITION EMOTION, V24, P1377, DOI 10.1080/02699930903485076
   Lilin Her, 2019, 2019 IEEE 4th International Conference on Image, Vision and Computing (ICIVC), P93, DOI 10.1109/ICIVC47709.2019.8980980
   Liu Z, 2022, PROC CVPR IEEE, P11966, DOI 10.1109/CVPR52688.2022.01167
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Masur H., 2000, Neurol. Sci., V21
   Noella RSN, 2019, PROCEDIA COMPUT SCI, V165, P428, DOI 10.1016/j.procs.2020.01.002
   Odena A, 2017, PR MACH LEARN RES, V70
   Pagán FL, 2012, AM J MANAG CARE, V18, pS176
   Pang M, 2022, IEEE T INF FOREN SEC, V17, P1544, DOI 10.1109/TIFS.2022.3164215
   Pang M, 2023, IEEE T NEUR NET LEAR, V34, P867, DOI 10.1109/TNNLS.2021.3103194
   Pang M, 2021, IEEE T INF FOREN SEC, V16, P2246, DOI 10.1109/TIFS.2021.3050055
   Pang YX, 2022, IEEE T MULTIMEDIA, V24, P3859, DOI 10.1109/TMM.2021.3109419
   Prateek GV, 2018, IEEE T BIO-MED ENG, V65, P2152, DOI 10.1109/TBME.2017.2785625
   Quan CQ, 2021, IEEE ACCESS, V9, P10239, DOI 10.1109/ACCESS.2021.3051432
   Rajnoha M, 2018, PROC 10 INT C ULTRA, P1
   Ricciardi L, 2020, EUR J NEUROL, V27, P2422, DOI 10.1111/ene.14452
   Sakar CO, 2019, APPL SOFT COMPUT, V74, P255, DOI 10.1016/j.asoc.2018.10.022
   Schlett T., 2020, ACM Comput. Surv., V54, P1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shukla Alok Kumar, 2019, Engineering Vibration, Communication and Information Processing. ICoEVCI 2018, India. Lecture Notes in Electrical Engineering (LNEE 478), P407, DOI 10.1007/978-981-13-1642-5_37
   Simons G, 2004, J INT NEUROPSYCH SOC, V10, P521, DOI 10.1017/S135561770410413X
   Stolze H, 2008, NERVENARZT, V79, P485, DOI 10.1007/s00115-007-2406-x
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Tolosa E, 2006, LANCET NEUROL, V5, P75, DOI 10.1016/S1474-4422(05)70285-4
   Tolosa E, 2021, LANCET NEUROL, V20, P385, DOI 10.1016/S1474-4422(21)00030-2
   Touvron H, 2021, PR MACH LEARN RES, V139, P7358
   Tuncer T, 2020, BIOCYBERN BIOMED ENG, V40, P211, DOI 10.1016/j.bbe.2019.05.006
   Ullrich M, 2021, IEEE T NEUR SYS REH, V29, P2103, DOI 10.1109/TNSRE.2021.3119390
   Wu P, 2014, COMPUT MATH METHOD M, V2014, DOI 10.1155/2014/427826
   Yan Y, 2020, IEEE T MULTIMEDIA, V22, P2792, DOI 10.1109/TMM.2019.2962317
   Yang T, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0231304
   Yi D, 2014, Arxiv, DOI arXiv:1411.7923
   Zhang XK, 2022, IEEE T MULTIMEDIA, V24, P1990, DOI 10.1109/TMM.2021.3074807
   Zhao GY, 2011, IMAGE VISION COMPUT, V29, P607, DOI 10.1016/j.imavis.2011.07.002
   Zhou DW, 2021, IEEE T MULTIMEDIA, V24, P3469, DOI 10.1109/TMM.2021.3099297
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 71
TC 1
Z9 1
U1 2
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7037
EP 7050
DI 10.1109/TMM.2022.3216961
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000024
DA 2024-07-18
ER

PT J
AU Li, C
   Song, L
   Chen, S
   Xie, R
   Zhang, WJ
AF Li, Chen
   Song, Li
   Chen, Shuai
   Xie, Rong
   Zhang, Wenjun
TI Deep Online Video Stabilization Using IMU Sensors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE LSTM; IMU; online video stabilization
ID TIME-SERIES
AB In this paper, we propose a deep learning based sensor-driven method for online video stabilization. This method utilizes the Euler angles and acceleration values estimated from the gyroscope and accelerator to assist stable video reconstruction. We introduce two simple sub-networks for trajectory optimization. The first network exploits real unstable trajectories and camera acceleration values to detect shooting scenarios. This network also generates an attention mask to adaptively choose scenario-specific features. Then the second network predicts smooth camera paths based on real unstable trajectories using long short-term memory (LSTM) under the supervision of the above mask. The output of the trajectory optimization network is filtered with a two-step modification process to guarantee smoothness. The real and smoothed camera paths are then utilized as guidance to generate stable frames in a projective manner. We also capture videos with sensor data covering seven typical shooting scenarios and design a ground truth generation method to construct pseud-labels. Moreover, the trajectory smoothing network allows the use of 3- or 10-frame buffers as future information to construct a lookahead filter. Experimental results show that our online method could outperform other state-of-the-art offline methods in several shaky video clips with fewer buffer frames for both general and low-quality videos. Furthermore, our method could effectively reduce running times without performing image content analysis, and the stabilization efficiency reaches 25 fps on 1080p videos.
C1 [Li, Chen; Song, Li; Xie, Rong; Zhang, Wenjun] Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
   [Song, Li] Shanghai Jiao Tong Univ, AI Inst, MoE Key Lab Artificial Intelligence, Shanghai 200240, Peoples R China.
   [Chen, Shuai] Huawei Technol, Beijing 100095, Peoples R China.
   [Chen, Shuai] Univ Oxford, Oxford OX1 3PJ, Oxon, England.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Huawei
   Technologies; University of Oxford
RP Song, L (corresponding author), Shanghai Jiao Tong Univ, Inst Image Commun & Network Engn, Shanghai 200240, Peoples R China.
EM lcjurrivh@sjtu.edu.cn; song_li@sjtu.edu.cn; shuaic@robots.ox.ac.uk;
   xierong@sjtu.edu.cn; zhangwenjun@sjtu.edu.cn
RI Zhang, Wenjun/GNH-2095-2022
OI Zhang, Wenjun/0000-0002-5282-3725; Chen, Shuai/0000-0002-9215-7050; Li,
   Chen/0000-0003-4059-2058; Song, Li/0000-0002-7124-5182
FU MoE-China Mobile Research Fund Project [MCM20180702]; Chinese National
   Science Funding [62132006]; Shanghai Key Laboratory of Digital Media
   Processing and Transmissions; Huawei Technologies
FX This work was supported in part by MoE-China Mobile Research Fund
   Project under Grant MCM20180702, in part by Chinese National Science
   Funding under Grant 62132006, and in part by Shanghai Key Laboratory of
   Digital Media Processing and Transmissions. The work of Shuai Chen was
   supported by Huawei Technologies.
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Alomar ML, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3917892
   Alt H, 2009, LECT NOTES COMPUT SC, V5760, P235, DOI 10.1007/978-3-642-03456-5_16
   [Anonymous], 2011, 201103 CSTR STANF U
   [Anonymous], 2015, Information Granularity, Big Data, and Computational Intelligence, DOI DOI 10.1007/978-3-319-08254-719
   Battiato S, 2010, IEEE T MULTIMEDIA, V12, P622, DOI 10.1109/TMM.2010.2060474
   Bell S, 2014, LECT NOTES COMPUT SC, V8692, P294, DOI 10.1007/978-3-319-10593-2_20
   Busseti E., 2012, 229 CS STANF U DEP C
   CHAKRABORTY K, 1992, NEURAL NETWORKS, V5, P961, DOI 10.1016/S0893-6080(05)80092-9
   Chandra R, 2012, NEUROCOMPUTING, V86, P116, DOI 10.1016/j.neucom.2012.01.014
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi J, 2020, ACM T GRAPHIC, V39, DOI 10.1145/3363550
   Forssén PE, 2010, PROC CVPR IEEE, P507, DOI 10.1109/CVPR.2010.5540173
   Gao Y, 2016, IEEE INT CONF ROBOT, P536, DOI 10.1109/ICRA.2016.7487176
   Goldstein A, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2231816.2231824
   Grundmann M, 2011, PROC CVPR IEEE, P225, DOI 10.1109/CVPR.2011.5995525
   Ho N., 2014, SIMPLE VIDEO STABILI
   Hu SM, 2013, VISUAL COMPUT, V29, P393, DOI 10.1007/s00371-013-0792-6
   Huang CH, 2019, Arxiv, DOI arXiv:1907.10283
   IRANI M, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P454, DOI 10.1109/CVPR.1994.323866
   Jiang WC, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1307, DOI 10.1145/2733373.2806333
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   Lee HT, 2009, ACM T WEB, V3, DOI 10.1145/1541822.1541823
   Liang C.-K., FUSED VIDEO STABILIZ
   Liu F, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899408
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Liu J, 2014, ADV INTELL SYST, V255, P261, DOI 10.1007/978-81-322-1759-6_32
   Liu SC, 2016, LECT NOTES COMPUT SC, V9910, P800, DOI 10.1007/978-3-319-46466-4_48
   Liu SC, 2014, PROC CVPR IEEE, P4209, DOI 10.1109/CVPR.2014.536
   Liu S, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461995
   Liu SC, 2012, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2012.6247662
   Lu SP, 2013, IEEE T VIS COMPUT GR, V19, P1218, DOI 10.1109/TVCG.2012.145
   Lv YS, 2015, IEEE T INTELL TRANSP, V16, P865, DOI 10.1109/TITS.2014.2345663
   Matsushita Y, 2006, IEEE T PATTERN ANAL, V28, P1150, DOI 10.1109/TPAMI.2006.141
   Morimoto C., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P284, DOI 10.1109/ICPR.1996.546956
   MURRAY MP, 1964, J BONE JOINT SURG AM, V46, P335, DOI 10.2106/00004623-196446020-00009
   Oth L, 2013, PROC CVPR IEEE, P1360, DOI 10.1109/CVPR.2013.179
   Romeu P, 2013, LECT NOTES COMPUT SC, V8131, P451, DOI 10.1007/978-3-642-40728-4_57
   Schulz M, 2014, MAR ENVIRON RES, V98, P14, DOI 10.1016/j.marenvres.2014.03.014
   Tang ZY, 2016, INT CONF ACOUST SPEE, P5900, DOI 10.1109/ICASSP.2016.7472809
   Turner J. T., 2014, THESIS U MARYLAND CA
   Wang M, 2019, IEEE T IMAGE PROCESS, V28, P2283, DOI 10.1109/TIP.2018.2884280
   Xu YF, 2022, Arxiv, DOI arXiv:2011.14574
   Yu J., 2020, P IEEE CVF C COMP VI, P8159
   Yu JY, 2019, PROC CVPR IEEE, P3795, DOI 10.1109/CVPR.2019.00392
   Zhang FL, 2018, IEEE T MULTIMEDIA, V20, P1987, DOI 10.1109/TMM.2018.2790163
   Zhang GF, 2009, VISUAL COMPUT, V25, P997, DOI 10.1007/s00371-009-0310-z
   Zhang L, 2017, IEEE T IMAGE PROCESS, V26, P2219, DOI 10.1109/TIP.2017.2676354
   Zhao MD, 2020, IEEE T IMAGE PROCESS, V29, P3582, DOI 10.1109/TIP.2019.2963380
   Zhou ZH, 2013, PROC CVPR IEEE, P2299, DOI 10.1109/CVPR.2013.298
   Zhu JJ, 2008, J SYST ENG ELECTRON, V19, P228, DOI 10.1016/S1004-4132(08)60071-7
NR 51
TC 3
Z9 3
U1 4
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2047
EP 2060
DI 10.1109/TMM.2022.3142429
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100036
DA 2024-07-18
ER

PT J
AU Li, X
   Zhang, D
   Li, M
   Lee, DJ
AF Li, Xiao
   Zhang, Dong
   Li, Ming
   Lee, Dah-Jye
TI Accurate Head Pose Estimation Using Image Rectification and a
   Lightweight Convolutional Neural Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks; head pose estimation; image
   rectification; perspective transformation
ID WEAK PERSPECTIVE
AB Head pose estimation is an important step for many human-computer interaction applications such as face detection, facial recognition, and facial expression classification. Accurate head pose estimation benefits these applications that require face images as the input. Most head pose estimation methods suffer from perspective distortion because the users do not always align their face perfectly with the camera. This paper presents a new approach that uses image rectification to reduce the negative effect of perspective distortion and a lightweight convolutional neural network to obtain highly accurate head pose estimations. The proposed method calculates the angle between the optical axis of the camera and the projection vector of the center of the face. The face image is rectified using this estimated angle through perspective transformation. A lightweight network that is only 0.88 MB in size is designed to take the rectified face image as the input to perform head pose estimation. The output of the network, the head pose estimation of the rectified face image, is transformed back to the camera coordinate system as the final head pose estimation. Experiments on public benchmark datasets show that the proposed image rectification method and the newly designed lightweight network improve the accuracy of head pose estimation remarkably. Compared with state-of-the-art methods, our approach achieves both higher accuracy and faster processing speed.
C1 [Li, Xiao; Zhang, Dong] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
   [Li, Ming] Duke Kunshan Univ, Kunshan 215316, Peoples R China.
   [Lee, Dah-Jye] Brigham Young Univ, Dept Elect & Comp Engn, Provo, UT 84602 USA.
C3 Sun Yat Sen University; Duke Kunshan University; Brigham Young
   University
RP Zhang, D (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM lixiao37@mail2.sysu.edu.cn; zhangd@mail.sysu.edu.cn;
   ming.li369@dukekunshan.edu.cn; djlee@byu.edu
OI Zhang, Dong/0000-0003-0825-3400; Lee, Dah-Jye/0000-0003-1752-8146; Li,
   Xiao/0000-0002-9606-5292
FU National Natural Science Foundation of China [62173353, 62171207];
   Guangzhou Municipal People's Livelihood Science and Technology Plan
   [201903010040]; Science and Technology Program of Guangzhou, China
   [202007030011]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62173353 and 62171207, inpart by the
   Guangzhou Municipal People's Livelihood Science and Technology Plan
   under Grant 201903010040, and in part by the Science and Technology
   Program of Guangzhou, China under Grant 202007030011.
CR Albiero Vitor, 2021, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), P7613, DOI 10.1109/CVPR46437.2021.00753
   Alioua N, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0103-z
   [Anonymous], 2015, P 3 INT C LEARN REPR
   Barra P, 2020, IEEE T IMAGE PROCESS, V29, P5457, DOI 10.1109/TIP.2020.2984373
   Belhumeur PN, 2013, IEEE T PATTERN ANAL, V35, P2930, DOI 10.1109/TPAMI.2013.23
   Bulat A, 2017, IEEE I CONF COMP VIS, P1021, DOI 10.1109/ICCV.2017.116
   Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544
   Chang FJ, 2017, IEEE INT CONF COMP V, P1599, DOI 10.1109/ICCVW.2017.188
   Chen PF, 2019, PR MACH LEARN RES, V97
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Deng JK, 2020, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR42600.2020.00525
   Dollár P, 2010, PROC CVPR IEEE, P1078, DOI 10.1109/CVPR.2010.5540094
   Fanelli Gabriele, 2011, Pattern Recognition. Proceedings 33rd DAGM Symposium, P101, DOI 10.1007/978-3-642-23123-0_11
   Fanelli G, 2013, INT J COMPUT VISION, V101, P437, DOI 10.1007/s11263-012-0549-0
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gu JW, 2017, PROC CVPR IEEE, P1531, DOI 10.1109/CVPR.2017.167
   Horaud R, 1997, INT J COMPUT VISION, V22, P173, DOI 10.1023/A:1007940112931
   Howard A, 2019, IEEE I CONF COMP VIS, P1314, DOI 10.1109/ICCV.2019.00140
   Hsu HW, 2019, IEEE T MULTIMEDIA, V21, P1035, DOI 10.1109/TMM.2018.2866770
   Hu YX, 2004, SIXTH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, PROCEEDINGS, P651
   HUANG TS, 1995, IEEE T PATTERN ANAL, V17, P1220, DOI 10.1109/34.476515
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Koestinger M., 2011, IEEE INT C COMP VIS, P2144, DOI DOI 10.1109/ICCVW.2011.6130513
   Krinidis M, 2009, IEEE T CIRC SYST VID, V19, P261, DOI 10.1109/TCSVT.2008.2009261
   Lee BG, 2012, IEEE SENS J, V12, P2416, DOI 10.1109/JSEN.2012.2190505
   Lee D, 2015, PROC CVPR IEEE, P4204, DOI 10.1109/CVPR.2015.7299048
   Li J, 2019, PROC CVPR IEEE, P5055, DOI 10.1109/CVPR.2019.00520
   Martin Manuel, 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P641, DOI 10.1109/3DV.2014.54
   Martins P, 2008, IEEE INT CONF AUTOMA, P831
   Messer K., 1999, Xm2vtsdb: The extended m2vts database, P965
   Meyer GP, 2015, IEEE I CONF COMP VIS, P3649, DOI 10.1109/ICCV.2015.416
   Mukherjee SS, 2015, IEEE T MULTIMEDIA, V17, P2094, DOI 10.1109/TMM.2015.2482819
   Murphy-Chutorian E, 2007, 2007 IEEE INTELLIGENT TRANSPORTATION SYSTEMS CONFERENCE, VOLS 1 AND 2, P1049
   Murphy-Chutorian E, 2009, IEEE T PATTERN ANAL, V31, P607, DOI 10.1109/TPAMI.2008.106
   Ng J., 1999, Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378), P14, DOI 10.1109/RATFG.1999.799218
   Northcutt CG, 2021, J ARTIF INTELL RES, V70, P1373
   Patacchiola M, 2017, PATTERN RECOGN, V71, P132, DOI 10.1016/j.patcog.2017.06.009
   Rae R, 1998, IEEE T NEURAL NETWOR, V9, P257, DOI 10.1109/72.661121
   Ranjan R, 2017, IEEE INT CONF AUTOMA, P17, DOI 10.1109/FG.2017.137
   Ruiz N, 2018, IEEE COMPUT SOC CONF, P2155, DOI 10.1109/CVPRW.2018.00281
   Sagonas C, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P397, DOI 10.1109/ICCVW.2013.59
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sugano Y, 2014, PROC CVPR IEEE, P1821, DOI 10.1109/CVPR.2014.235
   Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141
   Xiong XH, 2013, PROC CVPR IEEE, P532, DOI 10.1109/CVPR.2013.75
   Yang TY, 2019, PROC CVPR IEEE, P1087, DOI 10.1109/CVPR.2019.00118
   Yu X, 2016, IEEE T PATTERN ANAL, V38, P2212, DOI 10.1109/TPAMI.2015.2509999
   Zhang FF, 2018, PROC CVPR IEEE, P3359, DOI 10.1109/CVPR.2018.00354
   Zhang GY, 2017, IEEE SIGNAL PROC LET, V24, P1666, DOI 10.1109/LSP.2017.2731952
   Zhang HW, 2018, INT C PATT RECOG, P2202, DOI 10.1109/ICPR.2018.8546220
   Zhou EJ, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P386, DOI 10.1109/ICCVW.2013.58
   Zhu XX, 2012, PROC CVPR IEEE, P2879, DOI 10.1109/CVPR.2012.6248014
   Zhu XY, 2016, PROC CVPR IEEE, P146, DOI 10.1109/CVPR.2016.23
   Zingoni A, 2019, IEEE AERO EL SYS MAG, V34, P38, DOI 10.1109/MAES.2018.170099
NR 54
TC 5
Z9 5
U1 8
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2239
EP 2251
DI 10.1109/TMM.2022.3144893
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100050
DA 2024-07-18
ER

PT J
AU Li, YK
   Wang, P
   Chan, CY
AF Li, Yuke
   Wang, Pin
   Chan, Ching-Yao
TI RESTEP Into the Future: Relational Spatio-Temporal Learning for
   Multi-Person Action Forecasting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-person action forecasting; spatiotemporal dependencies; graph
   neural network; weakly-supervised learning
ID MODEL
AB Multi-person action forecasting is an emerging topic in the computer vision field, and it is a pivotal step toward video understanding at a semantic level. This task is difficult due to the complexity of spatial and temporal dependencies. Yet, the state-of-the-art literature does not seem to be adequately responsive to this challenge. Hence, how to better foresee the forthcoming actions per actor has to be further pursued. Toward this end, we put forth a novel RElational Spatio-TEmPoral learning approach (RESTEP) for multi-person action forecasting. Our RESTEP explores the key that inherently characterizes actions from a perspective of incorporating the spatial and temporal information in a single pass (spatio-temporal dependencies) by extending relational reasoning. As a result, the RESTEP enables simultaneously predicting the actions of all actors in the scene. Our proposal significantly differs from mainstream works that heavily rely on independently processing the spatial and temporal dependencies. The proposed RESTEP first perceives a graph building upon the historical observations, then reasons the relational spatio-temporal context to extrapolate future actions. In order to augment the comprehension of individual actions that might vary over time, we further delve deeper into the essence behind this point - the evolution of spatio-temporal dependencies via optimizing the corresponding mutual information. We assess the RESTEP method on the large-scale Atomic Visual Actions (AVA) dataset, Activities in Extended Videos (ActEV/VIRAT) dataset and Joint-annotated Human Motion Data Base (J-HMDB). The experimental outcomes reveal that RESTEP can introduce considerable improvements with respect to recent leading studies.
C1 [Li, Yuke; Wang, Pin; Chan, Ching-Yao] Univ Calif Berkeley, Calif PATH Partners Adv Transportat Technol, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Wang, P (corresponding author), Univ Calif Berkeley, Calif PATH Partners Adv Transportat Technol, Berkeley, CA 94720 USA.
EM leesunfreshing@gmail.com; pin_wang@berkeley.edu; cychan@berkeley.edu
RI Wang, Bo/JLM-5172-2023; Li, Huaxiong/AAR-8881-2020; Wang, Bo/A-7073-2012
OI Li, Huaxiong/0000-0003-0395-1525; 
CR Abdulnabi AH, 2015, IEEE T MULTIMEDIA, V17, P1949, DOI 10.1109/TMM.2015.2477680
   Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110
   Aliakbarian MS, 2017, IEEE I CONF COMP VIS, P280, DOI 10.1109/ICCV.2017.39
   Armeni I, 2019, IEEE I CONF COMP VIS, P5663, DOI 10.1109/ICCV.2019.00576
   Awad George., 2018, Trecvid 2018: Benchmarking video activity detection, video captioning and matching, video storytelling linking and video search
   Ballas N., 2016, INT C LEARNING REPRE
   Battaglia PW, 2018, ARXIV
   Belghazi MI, 2018, PR MACH LEARN RES, V80
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Casanova Arantxa, 2018, INT C LEARNING REPRE
   Castrejon L, 2019, IEEE I CONF COMP VIS, P7607, DOI 10.1109/ICCV.2019.00770
   Chen L, 2019, IEEE I CONF COMP VIS, P4612, DOI 10.1109/ICCV.2019.00471
   Choi C, 2019, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2019.00101
   Gammulle H, 2019, IEEE I CONF COMP VIS, P5561, DOI 10.1109/ICCV.2019.00566
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gong LY, 2019, PROC CVPR IEEE, P9203, DOI 10.1109/CVPR.2019.00943
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gopalakrishnan A, 2019, PROC CVPR IEEE, P12108, DOI 10.1109/CVPR.2019.01239
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Gupta A, 2018, PROC CVPR IEEE, P2255, DOI 10.1109/CVPR.2018.00240
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hjelm R. Devon, 2018, LEARNING DEEP REPRES
   Hu RH, 2019, IEEE I CONF COMP VIS, P10293, DOI 10.1109/ICCV.2019.01039
   Huang DA, 2014, LECT NOTES COMPUT SC, V8695, P489, DOI 10.1007/978-3-319-10584-0_32
   Huang YF, 2019, IEEE I CONF COMP VIS, P6281, DOI 10.1109/ICCV.2019.00637
   Ivanovic B, 2019, IEEE I CONF COMP VIS, P2375, DOI 10.1109/ICCV.2019.00246
   Jhuang HH, 2013, IEEE I CONF COMP VIS, P3192, DOI 10.1109/ICCV.2013.396
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lan T, 2014, LECT NOTES COMPUT SC, V8691, P689, DOI 10.1007/978-3-319-10578-9_45
   Li JN, 2018, IEEE T MULTIMEDIA, V20, P985, DOI 10.1109/TMM.2017.2759508
   Li YK, 2019, PROC CVPR IEEE, P294, DOI 10.1109/CVPR.2019.00038
   Li YK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P211, DOI 10.1145/3240508.3240551
   Li YK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P235, DOI 10.1145/3123266.3123287
   Li YK, 2018, IEEE T MULTIMEDIA, V20, P3289, DOI 10.1109/TMM.2018.2834873
   Liang JW, 2019, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR.2019.00587
   Mao W, 2019, IEEE I CONF COMP VIS, P9488, DOI 10.1109/ICCV.2019.00958
   Minderer M., 2019, ADV NEURAL INFORM PR
   Minderer M, 2019, ADV NEUR IN, V32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Singh G., 2018, P EUROPEAN C COMPUTE, P106
   Singh G., 2017, P IEEE INT C COMP VI
   Soomro K, 2019, IEEE T PATTERN ANAL, V41, P459, DOI 10.1109/TPAMI.2018.2797266
   Sun C, 2019, PROC CVPR IEEE, P273, DOI 10.1109/CVPR.2019.00036
   Tacchetti A., 2019, PROC INT C LEARN REP
   Tsai YHH, 2019, PROC CVPR IEEE, P10416, DOI 10.1109/CVPR.2019.01067
   Velickovic Petar, 2019, P INT C LEARNING REP
   Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18
   Wang XY, 2019, IEEE I CONF COMP VIS, P6970, DOI 10.1109/ICCV.2019.00707
   Wichers N, 2018, PR MACH LEARN RES, V80
   Wu F, 2019, PR MACH LEARN RES, V97
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Ye Y., 2019, P IEEE INT C COMP VI
   Zeng KH, 2017, IEEE I CONF COMP VIS, P3018, DOI 10.1109/ICCV.2017.326
   Zhao H, 2019, IEEE I CONF COMP VIS, P7002, DOI 10.1109/ICCV.2019.00710
NR 60
TC 9
Z9 9
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1954
EP 1963
DI 10.1109/TMM.2021.3088303
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100029
DA 2024-07-18
ER

PT J
AU Liu, DZ
   Fang, X
   Hu, W
   Zhou, P
AF Liu, Daizong
   Fang, Xiang
   Hu, Wei
   Zhou, Pan
TI Exploring Optical-Flow-Guided Motion and Detection-Based Appearance for
   Temporal Sentence Grounding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Grounding; Semantics; Cognition; Task analysis;
   Three-dimensional displays; Proposals; Appearance context; motion
   context, optical flow; temporal sentence grounding
ID LANGUAGE
AB Temporal sentence grounding aims to localize a target segment in an untrimmed video semantically according to a given sentence query. Most previous works focus on learning frame-level features of each whole frame in the entire video, and directly match them with the textual information. Such frame-level feature extraction leads to the obstacles of these methods in distinguishing ambiguous video frames with complicated contents and subtle appearance differences, thus limiting their performance. In order to differentiate fine-grained appearance similarities among consecutive frames, some state-of-the-art methods additionally employ a detection model like Faster R-CNN to obtain detailed object-level features in each frame for filtering out the redundant background contents. However, these methods suffer from missing motion analysis since the object detection module in Faster R-CNN lacks temporal modeling. To alleviate the above limitations, in this paper, we propose a novel Motion- and Appearance-guided 3D Semantic Reasoning Network (MA3SRN), which incorporates optical-flow-guided motion-aware, detection-based appearance-aware, and 3D-aware object-level features to better reason the spatial-temporal object relations for accurately modelling the activity among consecutive frames. Specifically, we first develop three individual branches for motion, appearance, and 3D encoding separately to learn fine-grained motion-guided, appearance-guided, and 3D-aware object features, respectively. Then, both motion and appearance information from corresponding branches are associated to enhance the 3D-aware features for the final precise grounding. Extensive experiments on three challenging datasets (ActivityNet Caption, Charades-STA and TACoS) demonstrate that the proposed MA3SRN model achieves a new state-of-the-art.
C1 [Liu, Daizong; Hu, Wei] Peking Univ, Wangxuan Inst Comp Technol, Beijing 100091, Peoples R China.
   [Fang, Xiang; Zhou, Pan] Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Hubei, Peoples R China.
C3 Peking University; Huazhong University of Science & Technology
RP Hu, W (corresponding author), Peking Univ, Wangxuan Inst Comp Technol, Beijing 100091, Peoples R China.; Zhou, P (corresponding author), Huazhong Univ Sci & Technol, Hubei Engn Res Ctr Big Data Secur, Sch Cyber Sci & Engn, Hubei Key Lab Distributed Syst Secur, Wuhan 430074, Hubei, Peoples R China.
EM daizongliu1996@gmail.com; xfang9508@gmail.com; forhuwei@pku.edu.cn;
   panzhou@hust.edu.cn
OI Fang, Xiang/0000-0003-3231-5771; liu, daizong/0000-0001-8179-4508
FU National Natural Science Foundation of China
FX No Statement Available
CR Blattmann A, 2021, PROC CVPR IEEE, P5167, DOI 10.1109/CVPR46437.2021.00513
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cao M, 2021, 2021 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2021), P9810
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen JY, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P162
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P10551
   Chen SX, 2019, AAAI CONF ARTIF INTE, P8199
   Chung J., 2014, NIPS 2014 WORKSH DEE, Vabs/1412.3555, P1, DOI DOI 10.48550/ARXIV.1412.3555
   Deng JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P1749, DOI 10.1109/ICCV48922.2021.00179
   Devlin J, 2019, 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES (NAACL HLT 2019), VOL. 1, P4171
   Dorkenwald M, 2021, PROC CVPR IEEE, P3741, DOI 10.1109/CVPR46437.2021.00374
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563
   Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032
   Hahn M, 2020, Arxiv, DOI arXiv:1904.09936
   He K., 2017, IEEE C COMP VIS PATT, P2961, DOI DOI 10.1109/ICCV.2017.322
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618
   Hu RH, 2016, PROC CVPR IEEE, P4555, DOI 10.1109/CVPR.2016.493
   Hu YP, 2021, IEEE T IMAGE PROCESS, V30, P4667, DOI 10.1109/TIP.2021.3073867
   Jonghwan Mun, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10807, DOI 10.1109/CVPR42600.2020.01082
   Kangaspunta J, 2021, IEEE COMPUT SOC CONF, P1602, DOI 10.1109/CVPRW53098.2021.00176
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Li QM, 2018, AAAI CONF ARTIF INTE, P3538
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DZ, 2022, AAAI CONF ARTIF INTE, P1674
   Liu DZ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4070, DOI 10.1145/3394171.3414026
   Liu DZ, 2021, PROC CVPR IEEE, P11230, DOI 10.1109/CVPR46437.2021.01108
   Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003
   Liu W., 2019, Neural Inf. Process. Syst., P536
   Liu ZG, 2023, IEEE T PATTERN ANAL, V45, P681, DOI 10.1109/TPAMI.2021.3139918
   Lu CJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5144
   Lu JS, 2019, ADV NEUR IN, V32
   Lu JS, 2016, ADV NEUR IN, V29
   Kipf TN, 2017, Arxiv, DOI arXiv:1609.02907
   Nan GS, 2021, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR46437.2021.00279
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Pramono RRA, 2022, IEEE T MULTIMEDIA, V24, P625, DOI 10.1109/TMM.2021.3056892
   Qu XY, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P4280, DOI 10.1145/3394171.3414053
   Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rodriguez-Opazo C, 2020, IEEE WINT CONF APPL, P2453, DOI [10.1109/WACV45572.2020.9093328, 10.1109/wacv45572.2020.9093328]
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Sun C, 2022, IEEE T MULTIMEDIA, V24, P274, DOI 10.1109/TMM.2021.3050067
   Tang HY, 2022, IEEE T MULTIMEDIA, V24, P1338, DOI 10.1109/TMM.2021.3063631
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang JY, 2021, IEEE T MULTIMEDIA, V24, P3369, DOI 10.1109/TMM.2021.3097171
   Wang JW, 2020, AAAI CONF ARTIF INTE, V34, P12168
   Woo S., 2022, arXiv
   Woo S, 2021, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR46437.2021.00273
   Wu J, 2020, AAAI CONF ARTIF INTE, V34, P12386
   Xu HJ, 2019, AAAI CONF ARTIF INTE, P9062
   Xu HJ, 2019, IEEE T PATTERN ANAL, V41, P2319, DOI 10.1109/TPAMI.2019.2921539
   Xu M., 2020, CVPR, P10156
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Yang L, 2020, IEEE T IMAGE PROCESS, V29, P8535, DOI 10.1109/TIP.2020.3016486
   Yang ZL, 2019, ADV NEUR IN, V32
   Yu T, 2021, IEEE T CIRC SYST VID, V31, P931, DOI 10.1109/TCSVT.2020.2995959
   Yu T, 2020, IEEE T IMAGE PROCESS, V29, P1204, DOI 10.1109/TIP.2019.2940677
   Yu Z, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1114
   Yuan YT, 2019, AAAI CONF ARTIF INTE, P9159
   Zeng R., 2020, CVPR
   Zeng YW, 2021, PROC CVPR IEEE, P2215, DOI 10.1109/CVPR46437.2021.00225
   Zhai YH, 2019, IEEE IMAGE PROC, P3696, DOI [10.1109/ICIP.2019.8803447, 10.1109/icip.2019.8803447]
   Zhang D, 2019, PROC CVPR IEEE, P1247, DOI 10.1109/CVPR.2019.00134
   Zhang Hao, 2020, P 58 ANN M ASS COMPU, P6543
   Zhang SY, 2020, AAAI CONF ARTIF INTE, V34, P12870
   Zhang YQ, 2019, AAAI CONF ARTIF INTE, P9235
   Zhang Z, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1069
   Zhang Z, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P655, DOI 10.1145/3331184.3331235
   Zhang Zhu, 2020, P IEEECVF C COMPUTER, P10668
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 76
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8539
EP 8553
DI 10.1109/TMM.2023.3238514
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, LF
   Xie, HT
   Liu, CB
   Zhang, YD
AF Ma, Lingfeng
   Xie, Hongtao
   Liu, Chuanbin
   Zhang, Yongdong
TI Learning Cross-Channel Representations for Semantic Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantic segmentation; channel-wise informa- tion; inter-channel
   relationship
AB Semantic segmentation is a fundamental problem in multimedia which requires delicate per-pixel predictions of object categories. Recently, many researchers strive to refine the pixel-wise feature with spatial-contextual information. However, many of them still neglect the invisible hand of cross-channel information which provides inherent semantics to facilitate the segmentation performance. On the one hand, in the feature extraction stage, enhancing informative channels and suppressing trivial ones contribute to the acquisition of valuable semantic features, and thus improving the segmentation accuracy. On the other hand, in the prediction stage, we can predict the complete objects more clearly by finding the connections and complements between different channels, which can also contribute to the pixel prediction. And based on this idea, we propose a novel Channel-Adaptive Network for semantic segmentation, which is capable of enhancing the features from the perspective of channels in both feature extraction stage and prediction stage. Specifically, we propose two modules: (i) the Comprehensive Information Channel Attention (CiCA) module that addresses the shortcomings of existing channel attention by learning both low and high frequency components within each channel for emphasizing the informative channels; (ii) the Inter-Channel Relationship Reasoning (iCRR) module which is applied on the top of the feature extractor to adaptively enhance the interdependent channels by mining the complementary associations between them. Besides, our Channel-Adaptive Network is highly flexible, with a plug-and-play design. Extensive experiments have demonstrated that our method achieves the state-of-the-art segmentation performance on three challenging datasets, including Cityscapes (82.1%), ADE20K (46.51%) and PASCAL Context (55.0%).
C1 [Ma, Lingfeng; Xie, Hongtao; Liu, Chuanbin; Zhang, Yongdong] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xie, HT (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
EM mlf123@mail.ustc.edu.cn; htxie@ustc.edu.cn; lcb592@mail.ustc.edu.cn;
   zhyd73@ustc.edu.cn
RI Liu, Chuanbin/JLL-9341-2023
OI Liu, Chuanbin/0000-0002-2840-6235
FU National Nature Science Foundation of China [62121002, 62022076,
   U1936210]; China Postdoctoral Science Foundation [2021M703081]
FX & nbsp;This work was supported in part by the National Nature Science
   Foundation of China under Grants 62121002, 62022076, and U1936210, and
   in part by the China Postdoctoral Science Foundation under Grant
   2021M703081.& nbsp;& nbsp;
CR Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cao XC, 2015, IEEE T IMAGE PROCESS, V24, P2746, DOI 10.1109/TIP.2015.2428055
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen JJ, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P2381, DOI 10.1145/3474085.3475402
   Chen L.Z, 2017, CORR ABS170605587
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen YP, 2018, ADV NEUR IN, V31
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Ding HH, 2020, IEEE T IMAGE PROCESS, V29, P3520, DOI 10.1109/TIP.2019.2962685
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Glorot X., 2011, JMLR Proceedings, V15, P315, DOI DOI 10.1002/ECS2.1832
   Gu ZX, 2021, IEEE T MULTIMEDIA, V23, P3738, DOI 10.1109/TMM.2020.3035231
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2020, PROC CVPR IEEE, P4002, DOI 10.1109/CVPR42600.2020.00406
   Hu J, 2018, ADV NEURAL INFORM PR, P9401, DOI DOI 10.5555/3327546.3327612
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Li X., 2020, P AAAI C ART INT, V34, p11 418
   Li X., 2020, P IEEE CVF C COMP VI, P8950, DOI 10.1109/CVPR42600.2020.00897
   Li Y, 2018, ADV NEUR IN, V31
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lingfeng Ma, 2021, Image and Graphics: 11th International Conference, ICIG 2021, Proceedings. Lecture Notes in Computer Science, Image Processing, Computer Vision, Pattern Recognition, and Graphics (12889), P523, DOI 10.1007/978-3-030-87358-5_42
   Liu MY, 2021, PROC CVPR IEEE, P9721, DOI 10.1109/CVPR46437.2021.00960
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Shi HC, 2018, IEEE T MULTIMEDIA, V20, P2670, DOI 10.1109/TMM.2018.2812600
   Velickovicet P., 2018, GRAPH ATTENTION NETW
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang JD, 2021, IEEE T PATTERN ANAL, V43, P3349, DOI 10.1109/TPAMI.2020.2983686
   Wang Q, 2020, INT SYM QUAL ELECT, P1, DOI [10.1109/isqed48828.2020.9137057, 10.1109/ISQED48828.2020.9137057, 10.1109/CVPR42600.2020.01155]
   Wang XL, 2018, LECT NOTES COMPUT SC, V11209, P413, DOI 10.1007/978-3-030-01228-1_25
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weijian Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P266, DOI 10.1007/978-3-030-58545-7_16
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiangtai Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12362), P435, DOI 10.1007/978-3-030-58520-4_26
   Xiao TT, 2018, LECT NOTES COMPUT SC, V11209, P432, DOI 10.1007/978-3-030-01228-1_26
   Xie HT, 2019, PATTERN RECOGN, V85, P109, DOI 10.1016/j.patcog.2018.07.031
   Xing MT, 2022, IEEE T MULTIMEDIA, V24, P3129, DOI 10.1109/TMM.2021.3093727
   Yang GR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12815
   Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388
   Yu Changqian, 2020, PROC IEEECVF C COMPU
   Zhang H, 2020, Arxiv, DOI arXiv:2004.08955
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang H, 2016, PROC CVPR IEEE, P1105, DOI 10.1109/CVPR.2016.125
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 52
TC 19
Z9 19
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2774
EP 2787
DI 10.1109/TMM.2022.3151145
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600025
DA 2024-07-18
ER

PT J
AU Meng, M
   Lan, MC
   Yu, J
   Wu, JG
   Liu, LG
AF Meng, Min
   Lan, Mengcheng
   Yu, Jun
   Wu, Jigang
   Liu, Ligang
TI Dual-Level Adaptive and Discriminative Knowledge Transfer for
   Cross-Domain Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distribution adaptation; domain adaptation; semi-supervised learning;
   structural risk minimization
ID REGULARIZATION; FRAMEWORK; ALGORITHM
AB Unsupervised domain adaptation is an appealing technique to learn robust classifiers for unlabeled target domain by borrowing knowledge from well-established source domain. However, previous works mainly suffer from two limitations: 1) the classifier trained on labeled source data may be prone to overfitting the source distribution, lowering its performance on the target domain; 2) the adaptation process will be misled by conditional distribution matching using hard pseudo labels of target samples. This paper presents a Dual-Level Adaptive and Discriminative (DLAD) classifier learning framework, in which transfer classifier and distribution adaptation can be mutually beneficial for effective knowledge transfer. Specifically, we aim to achieve a domain-level adaptive classifier by considering structural risk minimization (SRM) on both domains and performing weighted distribution adaptation, which facilitates joint classifier learning in a semi-supervised manner. To further achieve a class-level discriminative classifier, we explicitly leverage unlabeled target data to promote classifier learning based on class probabilities, which refines the decision boundary to be more discriminative for unlabeled target data. To the best of our knowledge, DLAD is the first attempt to consider the principle of SRM on the target domain, which significantly boosts the discriminative power of transfer classifier and yields a tighter generalization bound. Experimental evaluations on several standard cross-domain datasets show that DLAD significantly outperforms other competitive methods.
C1 [Meng, Min; Lan, Mengcheng; Wu, Jigang] Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.
   [Yu, Jun] Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Peoples R China.
C3 Guangdong University of Technology; Hangzhou Dianzi University; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Wu, JG (corresponding author), Guangdong Univ Technol, Sch Comp Sci & Technol, Guangzhou 510006, Peoples R China.; Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci, Hangzhou 310018, Peoples R China.
EM mengmin1985@gmail.com; lanmengchengds@gmail.com; yujun@hdu.edu.cn;
   asjgwucn@outlook.com; lgliu@ustc.edu.cn
OI Lan, Mengcheng/0000-0002-3311-0295; Meng, Min/0000-0002-5107-5585
FU National Natural Science Foundation of China [62172109, 62072118,
   62025207, 62125201, 62020106007, 61836002]; Natural Science Foundation
   of Guangdong Province [2020A1515011361]; High-level Talents Programme of
   Guangdong Province [2017GC010556]; Guangdong Basic and Applied Basic
   Research Foundation [2021B1515120010]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62172109, 62072118, 62025207, 62125201,
   62020106007, and 61836002, in part by the Natural Science Foundation of
   Guangdong Province under Grant 2020A1515011361, in part by the
   High-level Talents Programme of Guangdong Province under Grant
   2017GC010556, and in part by the Guangdong Basic and Applied Basic
   Research Foundation under Grant 2021B1515120010.
CR Aljundi R, 2015, PROC CVPR IEEE, P56, DOI 10.1109/CVPR.2015.7298600
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Ben-David S., 2007, ADV NEURAL INFORM PR, V19
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bruzzone L, 2010, IEEE T PATTERN ANAL, V32, P770, DOI 10.1109/TPAMI.2009.57
   Cao XD, 2013, IEEE I CONF COMP VIS, P3208, DOI 10.1109/ICCV.2013.398
   Cao Y, 2018, AAAI CONF ARTIF INTE, P2795
   Chen M., 2012, PROC 29 INT C MACH L
   Chen YM, 2020, IEEE T IMAGE PROCESS, V29, P199, DOI 10.1109/TIP.2019.2928630
   Ding ZM, 2018, LECT NOTES COMPUT SC, V11206, P36, DOI 10.1007/978-3-030-01216-8_3
   Donahue J, 2014, PR MACH LEARN RES, V32
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   FUKUNAGA K, 1975, IEEE T COMPUT, VC 24, P750, DOI 10.1109/T-C.1975.224297
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gretton A, 2006, Adv Neural Inf Process Syst, V19
   Griffin G., 2007, CALTECH 256 OBJECT C
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li S, 2021, IEEE T PATTERN ANAL, V43, P2329, DOI 10.1109/TPAMI.2020.2964173
   Li S, 2018, IEEE T IMAGE PROCESS, V27, P4260, DOI 10.1109/TIP.2018.2839528
   Liang J, 2019, PROC CVPR IEEE, P2970, DOI 10.1109/CVPR.2019.00309
   Liu H, 2019, 36 INT C MACHINE LEA, V97
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274
   Long MS, 2014, IEEE T KNOWL DATA EN, V26, P1076, DOI 10.1109/TKDE.2013.111
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Meng M, 2019, IEEE T IMAGE PROCESS, V28, P1824, DOI 10.1109/TIP.2018.2881926
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Prettenhofer P, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P1118
   Quanz B., 2009, PROC 18 ACM C INF KN, P1327, DOI DOI 10.1145/1645953.1646121
   Roy S, 2019, PROC CVPR IEEE, P9463, DOI 10.1109/CVPR.2019.00970
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Schölkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27
   Si S, 2010, IEEE T KNOWL DATA EN, V22, P929, DOI 10.1109/TKDE.2009.126
   Si YN, 2021, IEEE ACCESS, V9, P2283, DOI 10.1109/ACCESS.2020.3047448
   Sugiyama Masashi, 2008, Advances in Neural Information Processing Systems, P1433
   Sun BC, 2016, AAAI CONF ARTIF INTE, P2058
   Tzeng E, 2014, Arxiv, DOI [arXiv:1412.3474, 10.48550/arXiv.1412.3474, DOI 10.48550/ARXIV.1412.3474]
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   VN V., 1998, STAT LEARNING THEORY, V16
   Wang J.a.o., Everything about Transfer Learning and Domain Adapation
   Wang JD, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P402, DOI 10.1145/3240508.3240512
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Xie SA, 2018, PR MACH LEARN RES, V80
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang F, 2021, IEEE T MULTIMEDIA, V23, P1681, DOI 10.1109/TMM.2020.3001522
   Yang J., 2007, P 15 ACM INT C MULT, P188
   Zhang J, 2017, PROC CVPR IEEE, P5150, DOI 10.1109/CVPR.2017.547
   Zhang XX, 2020, IEEE T MULTIMEDIA, V22, P1692, DOI 10.1109/TMM.2019.2959433
   Zhang Z, 2018, PROC CVPR IEEE, P3437, DOI 10.1109/CVPR.2018.00362
   Zheng YH, 2019, IEEE T MULTIMEDIA, V21, P2292, DOI 10.1109/TMM.2019.2900166
NR 57
TC 7
Z9 7
U1 5
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2266
EP 2279
DI 10.1109/TMM.2022.3145235
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100052
DA 2024-07-18
ER

PT J
AU Meng, QL
   Zhang, SP
   Li, ZL
   Wang, CY
   Zhang, WG
   Huang, QM
AF Meng, Quanling
   Zhang, Shengping
   Li, Zonglin
   Wang, Chenyang
   Zhang, Weigang
   Huang, Qingming
TI Automatic Shadow Generation via Exposure Fusion
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Exposure fusion; image composition; self-attention; shadow generation
ID IMAGE FUSION
AB Shadow generation aims to generate a plausible shadow for the inserted foreground object in a composite image. Besides the composite image and the associated mask of the inserted foreground object, existing methods also require a mask of all background objects as well as their shadows as an auxiliary input, which is laborious in practical applications. Meanwhile, most existing methods use a linear illumination transformation to darken the shadow region, which is prone to produce unrealistic shadows especially when background illumination is complex. To address these problems, this paper proposes an automatic shadow generation method, which avoids the laborious acquisition of the background object masks while harmonizing the shadow region to achieve plausible shadow effects. Specifically, to implicitly exploit background illumination to infer the shadow shape of the inserted foreground object, we first propose a Hierarchy Attention U-Net (HAU-Net) to sequentially build global interactions between the foreground object and background across spatial and channel dimensions. Since the spatial-variant property of the shadow, we formulate shadow harmonization as an exposure fusion problem and propose an Illumination-Aware Fusion Network (IFNet), which uses an improved illumination model with a double linear transformation to produce multiple under-exposure images of the shadow region. IFNet then learns pixel-wise fusion kernels that consider the local smoothness of the shadow to fuse the composite image with these under-exposure images to generate the realistic shadow of the foreground object. Extensive experiments on the DESOBA and Shadow-AR datasets demonstrate that our method achieves state-of-the-art performance for shadow generation on both the BOS and BOS-free test images.
C1 [Meng, Quanling; Zhang, Shengping; Li, Zonglin; Wang, Chenyang; Zhang, Weigang] Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
C3 Harbin Institute of Technology; Chinese Academy of Sciences; University
   of Chinese Academy of Sciences, CAS
RP Zhang, SP (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Weihai 264209, Peoples R China.
EM quanling.meng@hit.edu.cn; s.zhang@hit.edu.cn; zonglin.li@hit.edu.cn;
   c.wang@stu.hit.edu.cn; wgzhang@hit.edu.cn; qmhuang@ucas.ac.cn
RI Wang, Chenyang/GRF-3385-2022; Li, Zonglin/AHB-2013-2022
OI Wang, Chenyang/0000-0001-6210-4129; Li, Zonglin/0000-0002-4181-310X;
   Zhang, Weigang/0000-0003-0042-7074; Meng, Quanling/0000-0003-3902-7510
FU National Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 2022, P IEEE CVF C COMP VI, P18542
   Barrow Harry, 1978, Comput. Vis. Syst, V2, P2
   Chen BC, 2019, PROC CVPR IEEE, P8407, DOI 10.1109/CVPR.2019.00861
   Cheng BW, 2022, PROC CVPR IEEE, P1280, DOI 10.1109/CVPR52688.2022.00135
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dwibedi D, 2017, IEEE I CONF COMP VIS, P1310, DOI 10.1109/ICCV.2017.146
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Gardner MA, 2019, IEEE I CONF COMP VIS, P7174, DOI 10.1109/ICCV.2019.00727
   Garon M, 2019, PROC CVPR IEEE, P6901, DOI 10.1109/CVPR.2019.00707
   Goshtasby AA, 2005, IMAGE VISION COMPUT, V23, P611, DOI 10.1016/j.imavis.2005.02.004
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hold-Geoffroy Y, 2019, PROC CVPR IEEE, P6920, DOI 10.1109/CVPR.2019.00709
   Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255
   Hong Y, 2022, AAAI CONF ARTIF INTE, P914
   Hu XW, 2019, IEEE I CONF COMP VIS, P2472, DOI 10.1109/ICCV.2019.00256
   Huawei, 2020, MindSpore
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Karsch K, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2602146
   Karsch K, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024191
   Lalonde JF, 2012, INT J COMPUT VISION, V98, P123, DOI 10.1007/s11263-011-0501-8
   Le H, 2019, IEEE I CONF COMP VIS, P8577, DOI 10.1109/ICCV.2019.00867
   Li ST, 2013, IEEE T IMAGE PROCESS, V22, P2864, DOI 10.1109/TIP.2013.2244222
   Liao B, 2019, COMPUT GRAPH-UK, V82, P53, DOI 10.1016/j.cag.2019.05.007
   Liu, 2020, P IEEE CVF C COMP VI, P8136, DOI DOI 10.1109/CVPR42600.2020.00816
   Liu B, 2017, J COMPUT SCI TECH-CH, V32, P430, DOI 10.1007/s11390-017-1734-y
   Ma KD, 2018, IEEE T COMPUT IMAG, V4, P60, DOI 10.1109/TCI.2017.2786138
   Mertens T, 2007, PACIFIC GRAPHICS 2007: 15TH PACIFIC CONFERENCE ON COMPUTER GRAPHICS AND APPLICATIONS, P382, DOI 10.1109/PG.2007.17
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Niu L, 2024, Arxiv, DOI arXiv:2106.14490
   Ouyang X, 2018, Arxiv, DOI arXiv:1804.02047
   Paszke A, 2019, ADV NEUR IN, V32
   Prabhakar KR, 2017, IEEE I CONF COMP VIS, P4724, DOI 10.1109/ICCV.2017.505
   Qi Y, 2021, INFORM FUSION, V66, P18, DOI 10.1016/j.inffus.2020.08.012
   Remez T, 2018, LECT NOTES COMPUT SC, V11211, P39, DOI 10.1007/978-3-030-01234-2_3
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sheng Y., 2021, P IEEE CVF C COMP VI, P4380
   Sheng YC, 2022, LECT NOTES COMPUT SC, V13683, P240, DOI 10.1007/978-3-031-20050-2_15
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang HN, 2022, AAAI CONF ARTIF INTE, P2441
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weber H, 2018, INT CONF 3D VISION, P199, DOI 10.1109/3DV.2018.00032
   Weng S., 2020, P IEEE CVF C COMP VI, P7741
   Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI [10.1109/CSTIC.2018.8369274, 10.1007/s11263-019-01198-w]
   Xu H, 2020, IEEE T IMAGE PROCESS, V29, P7203, DOI 10.1109/TIP.2020.2999855
   Yin JL, 2020, IEEE INT CON MULTI, DOI 10.1109/icme46284.2020.9102832
   Zamir SW, 2022, PROC CVPR IEEE, P5718, DOI 10.1109/CVPR52688.2022.00564
   Zhan F., 2022, P IEEE CVF C COMP VI, P18280
   Zhan F., 2020, P AS C COMP VIS, P234
   Zhang H, 2020, AAAI CONF ARTIF INTE, V34, P12797
   Zhang JS, 2019, PROC CVPR IEEE, P10150, DOI 10.1109/CVPR.2019.01040
   Zhang SY, 2019, COMPUT VIS MEDIA, V5, P105, DOI 10.1007/s41095-019-0136-1
   Zhang SH, 2020, COMPUT VIS MEDIA, V6, P79, DOI 10.1007/s41095-020-0158-8
NR 53
TC 1
Z9 1
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9044
EP 9056
DI 10.1109/TMM.2023.3244398
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200035
DA 2024-07-18
ER

PT J
AU Nong, LP
   Peng, J
   Zhang, WH
   Lin, JM
   Qiu, HB
   Wang, JY
AF Nong, Liping
   Peng, Jie
   Zhang, Wenhui
   Lin, Jiming
   Qiu, Hongbing
   Wang, Junyi
TI Adaptive Multi-Hypergraph Convolutional Networks for 3D Object
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hypergraph learning; hypergraph convolution operator; partially
   absorbing random walks; multi-modal fusion; 3D object classification
ID ATTENTION NETWORK; NEURAL-NETWORKS; RETRIEVAL
AB 3D object classification is an important task in computer vision. In order to explore the high-order and multi-modal correlations among 3D data, we propose an adaptive multi-hypergraph convolutional networks (AMHCN) framework to enhance 3D object classification performance. The proposed network improves the current hypergraph neural networks in two aspects. Firstly, existing networks rely on hyperedge constrained neighborhoods for feature aggregation, which may introduce noise or ignore positive information outside the hyperedges. To this end, we develop the partially absorbing random walks (PARW) to hypergraph for capturing optimal vertex neighborhoods from hypergraph globally. Then, based on the PARW on hypergraph, we design a new hypergraph convolution operator to learn deep embeddings from the optimized high-order correlation, which enables effective information propagation among the most relevant vertices. Secondly, concerning the multi-modal representations in practice, the current multi-modal hypergraph learning models either treat all modalities equally or introduce abundant parameters to learn weights of different modalities. To overcome these shortcomings, we propose a simple but effective dynamic weighting strategy for combining multi-modal representations, in which the importance of each modality can be adjusted adaptively by the loss function. We apply the proposed model to 3D object classification, and the experimental results on two 3D benchmark datasets demonstrate that our method outperforms the state-of-the-art methods, testifying to the effectiveness of both our convolution method and multi-modality fusion strategy.
C1 [Nong, Liping; Peng, Jie] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
   [Nong, Liping] Guangxi Normal Univ, Coll Phys & Technol, Guilin 541004, Peoples R China.
   [Zhang, Wenhui] Guilin Univ Elect Technol, Sch Comp Sci & Informat Secur, Guilin 541004, Peoples R China.
   [Lin, Jiming; Qiu, Hongbing; Wang, Junyi] Guilin Univ Elect Technol, Sch Informat & Commun, Guilin 541004, Peoples R China.
C3 Xidian University; Guangxi Normal University; Guilin University of
   Electronic Technology; Guilin University of Electronic Technology
RP Wang, JY (corresponding author), Guilin Univ Elect Technol, Sch Informat & Commun, Guilin 541004, Peoples R China.
EM nongliping@stu.xidian.edu.cn; jiepengxidian@qq.com; zhangwh@guet.edu.cn;
   linjm@guet.edu.cn; qiuhb@guet.edu.cn; wangjy@guet.edu.cn
OI Peng, Jie/0000-0002-2786-6221
FU National Natural Science Foundation of China [61966007]; Guangxi Key
   Laboratory of Wireless Wideband Communication and Signal Processing,
   Guilin University of Electronic Technology [GXKL06190204, GXKL06200116,
   GXKL06190117]; Key Laboratory of Cognitive Radio and Information
   Processing, Ministry of Education (Guilin University of Electronic
   Technology) [CRKL180201, CRKL180106]; Guangxi Natural Science Foundation
   [2020GXNSFAA159105]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61966007, in part by the Guangxi Key
   Laboratory of Wireless Wideband Communication and Signal Processing,
   Guilin University of Electronic Technology under Grants GXKL06190204,
   GXKL06200116, and GXKL06190117, in part by the Key Laboratory of
   Cognitive Radio and Information Processing, Ministry of Education
   (Guilin University of Electronic Technology) under Grants CRKL180201 and
   CRKL180106, and in part by Guangxi Natural Science Foundation under
   Grant 2020GXNSFAA159105.
CR Bai JJ, 2021, IEEE T IMAGE PROCESS, V30, P5327, DOI 10.1109/TIP.2021.3082765
   Bai S, 2021, PATTERN RECOGN, V110, DOI 10.1016/j.patcog.2020.107637
   Bandyopadhyay S, 2020, Arxiv, DOI arXiv:2002.03392
   Banerjee A, 2021, LINEAR ALGEBRA APPL, V614, P82, DOI 10.1016/j.laa.2020.01.012
   Bello SA, 2021, NEUROCOMPUTING, V461, P55, DOI 10.1016/j.neucom.2021.07.044
   Carletti T., 2021, J. Phys. Complexity, V2, P1
   Carletti T, 2020, PHYS REV E, V101, DOI 10.1103/PhysRevE.101.022308
   Chen CF, 2020, IEEE INT CONF TRUST, P1560, DOI 10.1109/TrustCom50675.2020.00215
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Cui C., 2021, arXiv
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Ding KZ, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P4927
   Eriksson A, 2021, COMMUN PHYS-UK, V4, DOI 10.1038/s42005-021-00634-z
   Feng YF, 2019, AAAI CONF ARTIF INTE, P3558
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao YB, 2023, IEEE T INTELL TRANSP, V24, P2158, DOI 10.1109/TITS.2022.3140355
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Garasuie Mustafa Mohammadi, 2020, 2020 11th International Conference on Information and Knowledge Technology (IKT), P67, DOI 10.1109/IKT51791.2020.9345609
   Hayashi K, 2020, CIKM '20: PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT, P495, DOI 10.1145/3340531.3412034
   Jiang JW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2635
   Jiang JW, 2019, AAAI CONF ARTIF INTE, P8513
   Jin TS, 2019, INFORM SCIENCES, V501, P708, DOI 10.1016/j.ins.2019.03.012
   Kim Eun-Sol, 2020, P IEEE CVF C COMP VI, P14581
   Kingma D. P., 2014, arXiv
   Kipf TN, 2017, INT C LEARN REPR
   Liu AA, 2021, INFORM SCIENCES, V547, P984, DOI 10.1016/j.ins.2020.09.057
   Liu MX, 2017, MED IMAGE ANAL, V36, P123, DOI 10.1016/j.media.2016.11.002
   Liu SY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P782
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Mulas R., 2021, arXiv
   Muzahid AAM, 2021, NEUROCOMPUTING, V460, P20, DOI 10.1016/j.neucom.2021.06.091
   Nie WZ, 2021, IEEE T IMAGE PROCESS, V30, P4371, DOI 10.1109/TIP.2021.3071687
   Nong LP, 2021, NEUROCOMPUTING, V463, P580, DOI 10.1016/j.neucom.2021.08.006
   Ozcan C, 2012, PROC TECH, V1, P50, DOI 10.1016/j.protcy.2012.02.011
   Qi CR, 2017, ADV NEUR IN, V30
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Sawhney R, 2021, AAAI CONF ARTIF INTE, V35, P497
   Shao JZ, 2019, IEEE IMAGE PROC, P2144, DOI [10.1109/icip.2019.8803207, 10.1109/ICIP.2019.8803207]
   Shi HY, 2019, IEEE T NEUR NET LEAR, V30, P2963, DOI 10.1109/TNNLS.2018.2869747
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun K, 2021, IEEE T IMAGE PROCESS, V30, P868, DOI 10.1109/TIP.2020.3039378
   Wang Jianling, 2021, P 2021 SIAM INT C DA, P82
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wolf M. M., 2016, P IEEE HIGH PERF EXT, P1
   Wu Q, 2016, 2016 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2016), P240, DOI [10.1109/WI.2016.41, 10.1109/WI.2016.0042]
   Wu X. M., 2012, NIPS, P3077
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xia Z., 2021, Comput. Intell. Neurosci., P1
   Xie ZY, 2020, NEUROCOMPUTING, V402, P245, DOI 10.1016/j.neucom.2020.03.086
   Yadati N., 2019, Advances in Neural Information Processing Systems, P1509
   Yang CQ, 2020, Arxiv, DOI arXiv:2005.04843
   Zhang ZZ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3162
   Zhang ZZ, 2018, LECT NOTES COMPUT SC, V11164, P38, DOI 10.1007/978-3-030-00776-8_4
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhou HY, 2020, IEEE T MULTIMEDIA, V22, P1496, DOI 10.1109/TMM.2019.2943740
   Zhu JJ, 2019, IEEE INT CON MULTI, P610, DOI 10.1109/ICME.2019.00111
NR 57
TC 2
Z9 2
U1 13
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4842
EP 4855
DI 10.1109/TMM.2022.3183388
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300017
OA hybrid
DA 2024-07-18
ER

PT J
AU Ople, JJM
   Huang, TM
   Chiu, MC
   Chen, YL
   Hua, KL
AF Ople, Jose Jaena Mari
   Huang, Tai-Ming
   Chiu, Ming-Chih
   Chen, Yi-Ling
   Hua, Kai-Lung
TI Adjustable Model Compression Using Multiple Genetic Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Genetic algorithms; Generative adversarial networks; Image coding;
   Statistics; Sociology; Computational modeling; Training; Evolutionary
   pruning; genetic algorithm; model compression
ID IMAGE; GAN
AB Generative Adversarial Networks (GAN) is a popular machine learning method that possesses powerful image generation ability, which is useful for different multimedia applications (e.g., photographic filters, image editing). However, typical GAN models have a large memory footprint that limits their practical applications for resource-constrained devices (e.g., smartphones). To deploy GAN models on devices with various hardware constraints, we propose our method, AdjustableGAN, which can compress a pretrained GAN model to different compression ratios. Our method compresses GAN by performing filter-wise pruning that follows these objectives: (1) deactivate convolutional filters for minimal performance decrease, (2) reactivate convolutional filters for maximal performance increase. We implement multiple Genetic Algorithms (GA) to perform each of these objectives- Downsize GA for best filter deactivations, while Upsize GA searches for best filter reactivations. By selective utilization of Upsize/Downsize GA, we could explicitly control the compression ratio of the model. For finalization, we fine-tune the compressed output model using the training dataset of the original input model. Our experimental results show that our method can reliably compress generative networks with minimal accuracy drop compared to other state-of-the-art compression algorithms.
C1 [Ople, Jose Jaena Mari; Huang, Tai-Ming; Chiu, Ming-Chih; Chen, Yi-Ling; Hua, Kai-Lung] Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
C3 National Taiwan University of Science & Technology
RP Hua, KL (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
EM d10815808@mail.ntust.edu.tw; mickey23405383@gmail.com;
   ndob.chiu@gmail.com; yiling@mail.ntust.edu.tw; hua@mail.ntust.edu.tw
OI Hua, Kai-Lung/0000-0002-7735-243X; Huang, Tai-Ming/0000-0003-0525-0466;
   Chiu, Ming-Chih/0000-0002-3486-6713
FU Ministry of~ Science and Technology of Taiwan
   [MOST109-2221-E-011-125-MY3, MOST110-2622-8-011-008-TE2,
   MOST110-2923-E-011-004]; Wang Jhan-Yang Charitable Trust Fund [WJY
   2020-HR-01]
FX This work was supported in part by the Ministry of Science and
   Technology of Taiwan under Grants MOST109-2221-E-011-125-MY3,
   MOST110-2622-8-011-008-TE2, and MOST110-2923-E-011-004; andin part by
   the Wang Jhan-Yang Charitable Trust Fund under Grant WJY 2020-HR-01. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Prof. Yadong Mu.
CR Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Han XJ, 2020, IEEE T MULTIMEDIA, V22, P1619, DOI 10.1109/TMM.2019.2945197
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Ho TT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3396237
   Karras T, 2018, Arxiv, DOI [arXiv:1710.10196, DOI 10.48550/ARXIV.1710.10196]
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Li RF, 2020, IEEE T MULTIMEDIA, V22, P3075, DOI 10.1109/TMM.2020.2972856
   Liu Y, 2019, IEEE T MULTIMEDIA, V21, P2209, DOI 10.1109/TMM.2019.2897897
   Luo JH, 2020, PROC CVPR IEEE, P1455, DOI 10.1109/CVPR42600.2020.00153
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Mishra R, 2020, Arxiv, DOI arXiv:2010.03954
   Pan JT, 2019, PROC CVPR IEEE, P3728, DOI 10.1109/CVPR.2019.00385
   Paszke A, 2019, ADV NEUR IN, V32
   Peng F, 2020, IEEE T MULTIMEDIA, V22, P2511, DOI 10.1109/TMM.2019.2959443
   Radford A, 2016, Arxiv, DOI [arXiv:1511.06434, DOI 10.48550/ARXIV.1511.06434]
   Shu H, 2019, IEEE I CONF COMP VIS, P3234, DOI 10.1109/ICCV.2019.00333
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan DS, 2021, IEEE T CIRC SYST VID, V31, P1526, DOI 10.1109/TCSVT.2020.3005311
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2273, DOI 10.1109/TMM.2020.3009484
   Wang YH, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2476, DOI 10.1145/3219819.3219970
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yu J., 2019, PROC 7 INT C LEARN R
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 30
TC 7
Z9 7
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1125
EP 1132
DI 10.1109/TMM.2021.3139215
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA D7WI3
UT WOS:000970791100008
DA 2024-07-18
ER

PT J
AU Tang, H
   Zhao, GS
   Wu, YX
   Qian, XM
AF Tang, Hao
   Zhao, Guoshuai
   Wu, Yuxia
   Qian, Xueming
TI Multisample-Based Contrastive Loss for Top-K Recommendation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Business process re-engineering; Training; Task analysis; Faces;
   Entropy; Convolution; Measurement; Contrastive loss; recommendation
   system; data augmentation; graph convolution network
AB Top-k recommendation is a fundamental task in recommendation systems that is generally learned by comparing positive and negative pairs. The contrastive loss (CL) is the key in contrastive learning that has recently received more attention, and we find that it is well suited for top-k recommendations. However, CL is problematic because it treats the importance of the positive and negative samples the same. On the one hand, CL faces the imbalance problem of one positive sample and many negative samples. On the other hand, there are so few positive items in sparser datasets that their importance should be emphasized. Moreover, the other important issue is that the sparse positive items are still not sufficiently utilized in recommendations. Consequently, we propose a new data augmentation method by using multiple positive items (or samples) simultaneously with the CL loss function. Therefore, we propose a multisample-based contrastive loss (MSCL) function that solves the two problems by balancing the importance of positive and negative samples and data augmentation. Based on the graph convolution network (GCN) method, experimental results demonstrate the state-of-the-art performance of MSCL. The proposed MSCL is simple and can be applied in many methods.
C1 [Tang, Hao; Wu, Yuxia] Jiaotong Univ, Sch Informat & Commun Engn, Xian 710049, Peoples R China.
   [Zhao, Guoshuai] Jiaotong Univ, Sch Software Engn, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, Sch Informat & Communica t Engn, Key Lab Intelligent Networks and Network Security, Minist Educ, Xian 710049, Peoples R China.
   [Qian, Xueming] Xi An Jiao Tong Univ, SMILES LAB, Xian 710049, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Zhao, GS (corresponding author), Jiaotong Univ, Sch Software Engn, Xian 710049, Peoples R China.
EM th1002@stu.xjtu.edu.cn; guoshuai.zhao@xjtu.edu.cn;
   wuyuxia@stu.xjtu.edu.cn; qianxm@mail.xjtu.edu.cn
RI Tang, Hao/KMX-1377-2024
OI Tang, Hao/0000-0002-2200-6249
FU NSFC, China [61902309, 61701391, 61772407]; ShaanXi Province
   [2018JM6092]; Fundamental Research Funds for the Central Universities,
   China [xxj022019003]; China Postdoctoral Science Foundation
   [2020M683496]; National Postdoctoral Innovative Talents Support Program,
   China [BX20190273]
FX This work was supported in part by NSFC, China, under Grants 61902309,
   61701391, and 61772407, in part by ShaanXi Province under Grant
   2018JM6092, in part by the Fundamental Research Funds for the Central
   Universities, China under Grant xxj022019003, in part by China
   Postdoctoral Science Foundation under Grant 2020M683496, and in part by
   National Postdoctoral Innovative Talents Support Program, China, under
   Grant BX20190273. The Associate Editor coordinating the review of this
   manuscript and approving it for publication was Prof. Liqiang Nie.
CR [Anonymous], 2006, PROC IEEE COMPUT SOC
   Bachman P, 2019, ADV NEUR IN, V32
   Chechik G, 2010, J MACH LEARN RES, V11, P1109
   Chen L, 2020, AAAI CONF ARTIF INTE, V34, P27
   Chen T, 2020, PR MACH LEARN RES, V119
   Chen T, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P767, DOI 10.1145/3097983.3098202
   Chen W, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2662, DOI 10.1145/3292500.3330652
   Chen XS, 2021, IEEE T MULTIMEDIA, V23, P484, DOI 10.1109/TMM.2020.2978618
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chuang Ching-Yao, 2020, ADV NEURAL INFORM PR, V33, P8765, DOI DOI 10.48550/ARXIV.2007.00224
   Donkers T, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P152, DOI 10.1145/3109859.3109877
   Grill J.-B., 2020, PROC C NEURAL INF PR
   Hao JM, 2021, IEEE T MULTIMEDIA, V24, P3381, DOI 10.1109/TMM.2021.3097186
   Hassani K, 2020, PR MACH LEARN RES, V119
   He XN, 2020, PROCEEDINGS OF THE 43RD INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '20), P639, DOI 10.1145/3397271.3401063
   He XN, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2227
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   Hjelm R. D., 2019, PROC INT C LEARN REP
   Hu Y, 2021, Arxiv, DOI [arXiv:2106.04051, DOI 10.48550/ARXIV.2106.04051]
   Huang SR, 2016, IEEE T MULTIMEDIA, V18, P287, DOI 10.1109/TMM.2015.2510333
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Khosla P., 2020, ADV NEURAL INF PROCE, V33, P18661
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li J., 2021, PROC INT C LEARN REP
   Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150
   Liu Y, 2022, IEEE T PATTERN ANAL, V44, P3688, DOI 10.1109/TPAMI.2021.3053577
   Miao YQ, 2021, IEEE T IMAGE PROCESS, V30, P7554, DOI 10.1109/TIP.2021.3106805
   Qiu JZ, 2020, KDD '20: PROCEEDINGS OF THE 26TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1150, DOI 10.1145/3394486.3403168
   Rendle S., 2009, P 25 C UNCERTAINTY A, P452
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sohn K, 2016, ADV NEUR IN, V29
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Tang H, 2021, KNOWL-BASED SYST, V228, DOI 10.1016/j.knosys.2021.107251
   Thoma J., 2020, Advances in Neural Information Processing Systems, V33, P11119
   Tong X., 2021, P 30 INT JOINT C ART, P1593
   van den Berg R, 2017, Arxiv, DOI arXiv:1706.02263
   van den Oord A, 2019, Arxiv, DOI arXiv:1807.03748
   Wang X, 2019, PROCEEDINGS OF THE 42ND INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR '19), P165, DOI 10.1145/3331184.3331267
   Wei YW, 2022, IEEE T MULTIMEDIA, V24, P2701, DOI 10.1109/TMM.2021.3088307
   Wei YW, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5382, DOI 10.1145/3474085.3475665
   Wei YW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1437, DOI 10.1145/3343031.3351034
   Weinberger KQ, 2009, J MACH LEARN RES, V10, P207
   Wu GS, 2019, IEEE T IMAGE PROCESS, V28, P1993, DOI 10.1109/TIP.2018.2882155
   Wu JC, 2021, SIGIR '21 - PROCEEDINGS OF THE 44TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P726, DOI 10.1145/3404835.3462862
   Wu YX, 2022, IEEE T KNOWL DATA EN, V34, P1944, DOI 10.1109/TKDE.2020.3002531
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiao J, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3119
   Xie X, 2021, Arxiv, DOI arXiv:2010.14395
   Xie Z, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P449, DOI 10.1145/3442381.3449873
   Xu ZX, 2017, IEEE T MULTIMEDIA, V19, P1933, DOI 10.1109/TMM.2017.2688928
   Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890
   You Y, 2020, ADV NEURAL INFORM PR, V33, P5812, DOI [10.48550/arXiv.2010.13902, DOI 10.48550/ARXIV.2010.13902]
   Yu SS, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P3886, DOI 10.1145/3447548.3467151
   Zhao GS, 2021, IEEE T KNOWL DATA EN, V33, P3160, DOI 10.1109/TKDE.2020.2966971
   Zhao GS, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105849
   Zhao GS, 2019, IEEE T MULTIMEDIA, V21, P771, DOI 10.1109/TMM.2018.2863598
   Zhao GS, 2016, IEEE T KNOWL DATA EN, V28, P3382, DOI 10.1109/TKDE.2016.2607172
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao T, 2021, AAAI CONF ARTIF INTE, V35, P11015
   Zhu YQ, 2021, PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE 2021 (WWW 2021), P2069, DOI 10.1145/3442381.3449802
NR 60
TC 17
Z9 17
U1 7
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 339
EP 351
DI 10.1109/TMM.2021.3126146
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9B8PL
UT WOS:000934993800001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Wang, YM
   Chang, DX
   Fu, ZQ
   Wen, J
   Zhao, Y
AF Wang, Yiming
   Chang, Dongxia
   Fu, Zhiqiang
   Wen, Jie
   Zhao, Yao
TI Graph Contrastive Partial Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Clustering methods; Generative adversarial networks; Task
   analysis; Semantics; Representation learning; Media; Contrastive
   learning; multi-view learning; partial multi-view clustering
AB With the diversity of information acquisition, data is stored and transmitted in an increasing number of modalities. Nevertheless, it is not unusual for parts of the data to be lost in some views due to unavoidable acquisition, transmission or storage errors. In this paper, we propose an augmentation-free graph contrastive learning framework to solve the problem of partial multi-view clustering. Notably, we suppose that the representations of similar samples (i.e., belonging to the same cluster) should be similar. This is distinct from the general unsupervised contrastive learning that assumes an image and its augmentations share a similar representation. Specifically, relation graphs are constructed using the nearest neighbors to identify existing similar samples, then the constructed inter-instance relation graphs are transferred to the missing views to build graphs on the corresponding missing data. Subsequently, two main components, within-view graph contrastive learning and cross-view graph consistency learning, are devised to maximize the mutual information of different views within a cluster. The proposed approach elevates instance-level contrastive learning and missing data inference to the cluster-level, effectively mitigating the impact of individual missing data on clustering. Experiments on several challenging datasets demonstrate the superiority of our proposed methods.
C1 [Wang, Yiming; Chang, Dongxia; Fu, Zhiqiang; Zhao, Yao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
   [Wang, Yiming; Chang, Dongxia; Fu, Zhiqiang; Zhao, Yao] Beijing Jiaotong Univ, Beijing Key Lab Adv Informat Sci & Network Techno, Beijing 100044, Peoples R China.
   [Wen, Jie] Harbin Inst Technol, Shenzhen Key Lab Visual Object Detect & Recognit, Shenzhen 518055, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Harbin
   Institute of Technology
RP Chang, DX (corresponding author), Beijing Jiaotong Univ, Inst Informat Sci, Beijing 100044, Peoples R China.
EM wangym@bjtu.edu.cn; dxchang@bjtu.edu.cn; zhiqiangfu@bjtu.edu.cn;
   jiewen_pr@126.com; yzhao@bjtu.edu.cn
RI Wen, Jie/AAH-8083-2020; Wen, Jie/G-7235-2015
OI Zhao, Yao/0000-0002-8581-9554; Wen, Jie/0000-0001-9554-2379; Yiming,
   Wang/0000-0002-8765-7640
FU National Natural Science Foundation of China [62272035]; National Key
   Research and Development Program of China [2018AAA0102100]; Fundamental
   Research Funds for the Central Universities [2021YJS027]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272035, in part by the National Key
   Research and Development Program of China under Grant 2018AAA0102100,
   and in part by the Fundamental Research Funds for the Central
   Universities under Grant 2021YJS027. The Associate Editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Jiebo Luo.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Chao GQ, 2019, INFORM SCIENCES, V494, P278, DOI 10.1016/j.ins.2019.04.039
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen T, 2020, PR MACH LEARN RES, V119
   Giorgi J, 2021, 59TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 11TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1 (ACL-IJCNLP 2021), P879
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo RQ, 2020, PR MACH LEARN RES, V119
   Hadsell R, 2006, IEEE C COMP VIS PATT, P1735, DOI DOI 10.1109/CVPR.2006.100
   Hu ML, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2262
   Huang ZY, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2563
   Jiang Y., 2019, NEURIPS, P5880
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li RH, 2019, IEEE I CONF COMP VIS, P8171, DOI 10.1109/ICCV.2019.00826
   Li SY, 2014, AAAI CONF ARTIF INTE, P1968
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Lin YJ, 2021, PROC CVPR IEEE, P11169, DOI 10.1109/CVPR46437.2021.01102
   Liu XW, 2021, IEEE T PATTERN ANAL, V43, P2634, DOI 10.1109/TPAMI.2020.2974828
   Liu XW, 2019, IEEE T PATTERN ANAL, V41, P2410, DOI 10.1109/TPAMI.2018.2879108
   Liu XW, 2016, AAAI CONF ARTIF INTE, P1888
   Malkov YA, 2020, IEEE T PATTERN ANAL, V42, P824, DOI 10.1109/TPAMI.2018.2889473
   Peng X, 2019, PR MACH LEARN RES, V97
   Qian Qian B. B., 2016, P INT C DIG IM COMP, P1
   Shao WX, 2015, LECT NOTES ARTIF INT, V9284, P318, DOI 10.1007/978-3-319-23528-8_20
   Sharma V, 2020, IEEE INT CONF AUTOMA, P109, DOI 10.1109/FG47880.2020.00011
   Simonyan K., 2014, CORR
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3677
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P1771, DOI 10.1109/TIP.2020.3048626
   Wang QQ, 2021, IEEE T IMAGE PROCESS, V30, P305, DOI 10.1109/TIP.2020.3036717
   Wang SW, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3778
   Wen J, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P3753, DOI 10.1145/3394171.3413807
   Wen J, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3230
   Wen J, 2021, IEEE T MULTIMEDIA, V23, P2493, DOI 10.1109/TMM.2020.3013408
   Wen J, 2019, AAAI CONF ARTIF INTE, P5393
   Xia W, 2021, IEEE T MULTIMEDIA, V24, P3182, DOI 10.1109/TMM.2021.3094296
   Xiao XL, 2021, IEEE T MULTIMEDIA, V23, P4555, DOI 10.1109/TMM.2020.3045259
   Xu C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3933
   Yan F, 2015, PROC CVPR IEEE, P3441, DOI 10.1109/CVPR.2015.7298966
   Yang B, 2022, IEEE T CIRC SYST VID, V32, P6200, DOI 10.1109/TCSVT.2022.3162575
   Yin M, 2020, AAAI CONF ARTIF INTE, V34, P6688
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2022, IEEE T PATTERN ANAL, V44, P2402, DOI 10.1109/TPAMI.2020.3037734
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang C, 2019, PROC CVPR IEEE, P9444, DOI 10.1109/CVPR.2019.00968
   Zhang L, 2014, IEEE T KNOWL DATA EN, V26, P2745, DOI 10.1109/TKDE.2014.2313866
   Zhao H., 2016, IJCAI, P2392
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   Zhao XR, 2014, PATTERN RECOGN LETT, V41, P73, DOI 10.1016/j.patrec.2013.12.003
   Zhong HS, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9204, DOI 10.1109/ICCV48922.2021.00909
NR 51
TC 7
Z9 7
U1 12
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6551
EP 6562
DI 10.1109/TMM.2022.3210376
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500065
DA 2024-07-18
ER

PT J
AU Wang, Y
   Zhang, T
   Zhou, CW
   Cui, Z
   Yang, J
AF Wang, Yun
   Zhang, Tong
   Zhou, Chuanwei
   Cui, Zhen
   Yang, Jian
TI Instance-Aware Deep Graph Learning for Multi-Label Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Adaptation models; Task analysis; Feature extraction; Image
   recognition; Convolutional neural networks; Sports; Graph convolutional
   neural network; image-dependent label correlation matrix; regions of
   interests; variational inference
ID IMAGE CLASSIFICATION
AB Graph convolutional neural network (GCN) has effectively boosted the multi-label image recognition task by modeling correlation among labels. In previous methods, label correlation is computed based on statistical information through label diffusion, and therefore the same for all samples. This, however, makes graph inference on labels insufficient to handle huge variations among numerous image instances. In this paper, we propose an instance-aware graph convolutional neural network (IA_GCN) framework for the multi-label classification. As a whole, two fused branches of sub-networks are involved in the framework: a global branch modeling the whole image and a local branch exploring dependencies among regions of interests (ROIs). For both the branches, an image-dependent label correlation matrix (ID_LCM), fusing both the statistical label correlation matrix (LCM) and an individual one of each image instance, is constructed to inject adaptive information of label-awareness into the learned features of the model through graph convolution. Specifically, the individual LCM of each image is obtained by mining the label dependencies based on the predicted label scores of those detected ROIs. In this process, considering the contribution differences of ROIs to multi-label classification, variational inference is introduced to learn adaptive scaling factors for those ROIs by considering their complex distribution. Finally, extensive experiments on MS-COCO and VOC datasets show that our proposed approach outperforms existing state-of-the-art methods.
C1 [Wang, Yun; Zhang, Tong; Zhou, Chuanwei; Cui, Zhen; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab, Key Lab Intelligent Percept & Syst High Dimens Inf, Nanjing 210094, Peoples R China.
   [Wang, Yun; Zhang, Tong; Zhou, Chuanwei; Cui, Zhen; Yang, Jian] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology
RP Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, PCA Lab, Key Lab Intelligent Percept & Syst High Dimens Inf, Nanjing 210094, Peoples R China.; Cui, Z (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Social, Nanjing 210094, Peoples R China.
EM yun.wang@njust.edu.cn; tong.zhang@njust.edu.cn; cwzhou@njust.edu.cn;
   zhen.cui@njust.edu.cn; csjyang@njust.edu.cn
RI li, xiaomin/KCX-9845-2024
FU Natural Science Foundation of Jiangsu Province [BK20190019, BK20190452];
   Natural Science Foundation of Shandong Province [ZR2020LZH008]; National
   Natural Science Foundation of China [62072244, 61906094]; Fundamental
   Research Funds for the Central Universities [30921011104]; State Key
   Laboratory of High-end Server & Storage Technology
FX Manuscript received 2 February 2021; revised 15 July 2021 and 1
   September 2021; accepted 3 October 2021. Date of publication 26 October
   2021; date of current version 13 January 2023. This work was supported
   in part by the Natural Science Foundation of Jiangsu Province under
   Grants BK20190019 and BK20190452, in part by the Natural Science
   Foundation of Shandong Province under Grant ZR2020LZH008, in part by the
   National Natural Science Foundation of China under Grants 62072244 and
   61906094, in part by the Fundamental Research Funds for the Central
   Universities under Grant 30921011104, and in part by the State Key
   Laboratory of High-end Server & Storage Technology. The Associate Editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Ramanathan Subramanian. (Yun Wang and Tong Zhang
   contributed equally to this work.) (Corresponding author: Zhen Cui.)
CR Alemi A., 2017, PROC INT C LEARN REP
   Barber D, 2004, ADV NEUR IN, V16, P201
   Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009
   Chalk M., 2016, Adv Neural Inf Process Syst
   Chen TS, 2019, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2019.00061
   Chen TS, 2018, AAAI CONF ARTIF INTE, P6730
   Chen ZM, 2019, PROC CVPR IEEE, P5172, DOI 10.1109/CVPR.2019.00532
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding XM, 2016, IEEE T MULTIMEDIA, V18, P1616, DOI 10.1109/TMM.2016.2572000
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Gao BB, 2021, IEEE T IMAGE PROCESS, V30, P5920, DOI 10.1109/TIP.2021.3088605
   Gao SH, 2014, IEEE T MULTIMEDIA, V16, P762, DOI 10.1109/TMM.2014.2299516
   Ge WF, 2018, PROC CVPR IEEE, P1277, DOI 10.1109/CVPR.2018.00139
   Ge ZY, 2018, Arxiv, DOI arXiv:1807.07247
   Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266
   Hamilton WL, 2017, ADV NEUR IN, V30
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang Y, 2015, IEEE T MULTIMEDIA, V17, P1923, DOI 10.1109/TMM.2015.2476658
   Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133
   Kingma D. P., 2014, arXiv
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Lei CY, 2016, IEEE T MULTIMEDIA, V18, P687, DOI 10.1109/TMM.2015.2477277
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li Q, 2016, PROC CVPR IEEE, P2977, DOI 10.1109/CVPR.2016.325
   Li X, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P430
   Li YN, 2016, LECT NOTES COMPUT SC, V9910, P684, DOI 10.1007/978-3-319-46466-4_41
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin ZH, 2020, PROC CVPR IEEE, P1797, DOI 10.1109/CVPR42600.2020.00187
   Mohamed S, 2015, ADV NEUR IN, V28
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935
   Shirian A, 2022, IEEE T MULTIMEDIA, V24, P780, DOI 10.1109/TMM.2021.3059169
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Thekumparampil K. K., 2018, PROC INT C LEARN REP
   Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1, DOI DOI 10.4018/JDWM.2007070101
   Velickovic P, 2018, PROC INT C LEARN REP
   Walker J, 2016, LECT NOTES COMPUT SC, V9911, P835, DOI 10.1007/978-3-319-46478-7_51
   Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251
   Wang Y, 2020, AAAI CONF ARTIF INTE, V34, P12265
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Wang ZX, 2017, IEEE I CONF COMP VIS, P464, DOI 10.1109/ICCV.2017.58
   Wei YC, 2016, IEEE T PATTERN ANAL, V38, P1901, DOI 10.1109/TPAMI.2015.2491929
   Xu JJ, 2014, IEEE T MULTIMEDIA, V16, P403, DOI 10.1109/TMM.2013.2291218
   Xu K., 2019, PROC INT C LEARN REP
   Xu N, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2926
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2016, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2016.37
   You RC, 2020, AAAI CONF ARTIF INTE, V34, P12709
   Zhang JJ, 2018, IEEE T MULTIMEDIA, V20, P2801, DOI 10.1109/TMM.2018.2812605
   Zhang ML, 2021, IEEE T KNOWL DATA EN, V33, P2057, DOI 10.1109/TKDE.2019.2951561
   Zhang YS, 2020, IEEE T MULTIMEDIA, V22, P2844, DOI 10.1109/TMM.2020.2966887
   Zhu F, 2017, PROC CVPR IEEE, P2027, DOI 10.1109/CVPR.2017.219
NR 56
TC 3
Z9 3
U1 7
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 90
EP 99
DI 10.1109/TMM.2021.3121559
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400007
DA 2024-07-18
ER

PT J
AU Wang, ZH
   Jiang, QP
   Zhao, SS
   Feng, WS
   Lin, WS
AF Wang, Zhihua
   Jiang, Qiuping
   Zhao, Shanshan
   Feng, Wensen
   Lin, Weisi
TI Deep Blind Image Quality Assessment Powered by Online Hard Example
   Mining
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment; network pruning; hard example mining;
   co-evolution
AB Recently, blind image quality assessment (BIQA) models based on deep neural networks (DNNs) have achieved impressive performance on existing datasets. However, due to the intrinsic imbalance property of the training set, not all distortions or images are handled equally well. Online hard example mining (OHEM) is a promising way to alleviate this issue. Inspired by the recent finding that network pruning disproportionately hampers the model's memorization of a tractable subset, e.g., atypical, low-quality, long-tailed samples, which are hard-to-memorize during training and easily "forgotten" during pruning, we propose an effective "plug-and-play" OHEM pipeline for generalizable deep BIQA. Specifically, we train two parallel weight-sharing branches simultaneously, where one is full model and other is a "self-competitor" generated from the full model online by network pruning. Then, we leverage the prediction disagreement between the full model and its pruned variant (i.e., the self-competitor) to expose easily "forgettable" samples, which are therefore regarded as the hard ones. We enforce the prediction consistency between the full model and its pruned variant to implicitly put more focus on these hard samples, which benefits the full model to recover forgettable information introduced by pruning. Extensive experiments across multiple datasets and BIQA models demonstrate that the proposed OHEM can improve the model performance and generalizability as measured by correlation numbers and group maximum differentiation (gMAD) competition.
C1 [Wang, Zhihua] Shenzhen MSU BIT Univ, Guangdong Lab Machine Percept & Intelligent Comp, Shenzhen 51817, Peoples R China.
   [Jiang, Qiuping] Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Zhao, Shanshan] JD Explore Acad, Beijing 9618, Peoples R China.
   [Feng, Wensen] Tsinghua Univ, Shenzhen Grad Sch, Shenzhen 30013, Peoples R China.
   [Lin, Weisi] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Ningbo University; Tsinghua University; Nanyang Technological University
RP Jiang, QP (corresponding author), Ningbo Univ, Sch Informat Sci & Engn, Ningbo 315211, Peoples R China.
EM zhihua.wang@my.cityu.edu.hk; jiangqiuping@nbu.edu.c;
   sshan.zhao00@gmail.com; fengwensen@huawei.com; wslin@ntu.edu.sg
RI Zhihua, Wang/AFO-5263-2022; Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Qiuping, Jiang/0000-0002-6025-9343; Zhihua, WANG/0000-0002-4398-536X;
   Lin, Weisi/0000-0001-9866-1947
FU Natural Science Foundation of China
FX No Statement Available
CR [Anonymous], 1996, Learning and Example Selection for Object and Pattern Detection
   [Anonymous], 1990, Adv Neural Inform Process Syst.
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Bucher M, 2016, LECT NOTES COMPUT SC, V9915, P524, DOI 10.1007/978-3-319-49409-8_45
   Cai WB, 2013, IEEE DATA MINING, P51, DOI 10.1109/ICDM.2013.104
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Frankle J., 2018, ARXIV180303635
   Frankle J, 2020, Arxiv, DOI arXiv:1903.01611
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Golestaneh S Alireza, 2022, P IEEE CVF WINT C AP, P3209
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Gu Jinjin, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P633, DOI 10.1007/978-3-030-58621-8_37
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Han PC, 2021, ENG APPL ARTIF INTEL, V106, DOI 10.1016/j.engappai.2021.104489
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   Hooker S, 2021, Arxiv, DOI [arXiv:1911.05248, DOI 10.48550/ARXIV.1911.05248]
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Jiang Ziyu, 2021, INT C MACH LEARN, P4927
   Jin S, 2018, LECT NOTES COMPUT SC, V11217, P316, DOI 10.1007/978-3-030-01261-8_19
   Ke JJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5128, DOI 10.1109/ICCV48922.2021.00510
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li MN, 2017, COMM COM INF SC, V773, P166, DOI 10.1007/978-981-10-7305-2_15
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu J., 2022, IEEE T MULTIMEDIA EA
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Liu YT, 2020, IEEE T CIRC SYST VID, V30, P929, DOI 10.1109/TCSVT.2019.2900472
   Liu Z., 2019, PROC INT C LEARN REP
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Loshchilov I., 2016, P INT C LEARN REPR W, P1
   Ma KD, 2019, IEEE IMAGE PROC, P2344, DOI [10.1109/ICIP.2019.8803390, 10.1109/icip.2019.8803390]
   Ma KD, 2020, IEEE T PATTERN ANAL, V42, P851, DOI 10.1109/TPAMI.2018.2889948
   Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P3951, DOI 10.1109/TIP.2017.2708503
   Ming-Feng Tsai, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P383, DOI 10.1145/1277741.1277808
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152
   Ou FZ, 2022, IEEE T MULTIMEDIA, V24, P4197, DOI 10.1109/TMM.2021.3114551
   Ring M.B., 1994, PhD thesis
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shi HL, 2016, LECT NOTES COMPUT SC, V9905, P732, DOI 10.1007/978-3-319-46448-0_44
   Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89
   Shuyang Gu, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P369, DOI 10.1007/978-3-030-58621-8_22
   Simonyan K., 2014, CORR
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Wang ZH, 2021, PROC CVPR IEEE, P16251, DOI 10.1109/CVPR46437.2021.01599
   Wang ZH, 2022, IEEE T PATTERN ANAL, V44, P4577, DOI 10.1109/TPAMI.2021.3071759
   Xia X., 2020, PROC INT C LEARN REP, P1
   You H., 2019, P INT C LEARN REPR, P1
   Zhang CY, 2021, COMMUN ACM, V64, P107, DOI 10.1145/3446776
   Zhang WX, 2021, IEEE T IMAGE PROCESS, V30, P3474, DOI 10.1109/TIP.2021.3061932
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
   Zhang ZY, 2021, INT C MACHINE LEARNI, V139
   Zhu LY, 2021, PROC CVPR IEEE, P12532, DOI 10.1109/CVPR46437.2021.01235
NR 61
TC 1
Z9 1
U1 7
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4774
EP 4784
DI 10.1109/TMM.2023.3257564
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP5R1
UT WOS:001133278300004
DA 2024-07-18
ER

PT J
AU Xu, L
   Lan, CL
   Zeng, WJ
   Lu, CW
AF Xu, Liang
   Lan, Cuiling
   Zeng, Wenjun
   Lu, Cewu
TI Skeleton-Based Mutually Assisted Interacted Object Localization and
   Human Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton-based action recognition; interacted object localization; joint
   learning
ID ENSEMBLE
AB Skeleton data carries valuable motion information and is widely explored in human action recognition. However, not only the motion information but also the interaction with the environment provides discriminative cues to recognize the action of persons. In this paper, we propose a joint learning framework for mutually assisted "interacted object localization" and "human action recognition" based on skeleton data. The two tasks are serialized together and collaborate to promote each other, where preliminary action type derived from skeleton alone helps improve interacted object localization, which in turn provides valuable cues for the final human action recognition. Besides, we explore the temporal consistency of interacted object as constraint to better localize the interacted object with the absence of ground-truth labels. Extensive experiments on the datasets of SYSU-3D, NTU60 RGB+D, Northwestern-UCLA and UAV-Human show that our method achieves the best or competitive performance with the state-of-the-art methods for human action recognition. Visualization results show that our method can also provide reasonable interacted object localization results.
C1 [Xu, Liang; Lu, Cewu] Shanghai Jiao Tong Univ, Dept Elect & Comp Engn, Shanghai 200240, Peoples R China.
   [Lan, Cuiling] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Zeng, Wenjun] Eastern Inst Adv Study, Ningbo 315200, Peoples R China.
C3 Shanghai Jiao Tong University; Microsoft Research Asia; Microsoft
RP Lan, CL (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM liangxuy96@gmail.com; culan@microsoft.com; wenjunzeng@eias.ac.cn;
   lucewu@sjtu.edu.cn
RI Lan, Cuiling/KCK-5597-2024
OI Xu, Liang/0000-0002-6441-4443
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Authors P., 2019, Paddledetection, object detection and instance segmentation toolkit based on paddlepaddle
   Avola D, 2020, IEEE T MULTIMEDIA, V22, P2481, DOI 10.1109/TMM.2019.2960588
   Baradel F., 2018, P BRIT MACH VIS C, P1
   Baradel F, 2018, PROC CVPR IEEE, P469, DOI 10.1109/CVPR.2018.00056
   Baradel F, 2017, IEEE INT CONF COMP V, P604, DOI 10.1109/ICCVW.2017.77
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644
   Cao CQ, 2019, IEEE T CIRC SYST VID, V29, P3247, DOI 10.1109/TCSVT.2018.2879913
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Das Srijan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P72, DOI 10.1007/978-3-030-58545-7_5
   Das S, 2019, IEEE I CONF COMP VIS, P833, DOI 10.1109/ICCV.2019.00092
   Das S, 2019, IEEE WINT CONF APPL, P71, DOI 10.1109/WACV.2019.00015
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan YB, 2020, IEEE ACCESS, V8, P15280, DOI 10.1109/ACCESS.2020.2968054
   Feichtenhofer C., 2016, P INT C NEUR INF PRO, P3468
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Gkioxari G, 2018, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2018.00872
   Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633
   Gupta P, 2021, INT J COMPUT VISION, V129, P2097, DOI 10.1007/s11263-021-01470-y
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   Hou R, 2017, IEEE I CONF COMP VIS, P5823, DOI 10.1109/ICCV.2017.620
   Hu GY, 2020, IEEE T MULTIMEDIA, V22, P2207, DOI 10.1109/TMM.2019.2953325
   Hu JF, 2015, PROC CVPR IEEE, P5344, DOI 10.1109/CVPR.2015.7299172
   Hu Y., 2019, P INT C MACH LEARN W
   Hussein, 2013, INT JOINT C ART INT
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kay W, 2017, Arxiv, DOI arXiv:1705.06950
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Ke Q, 2017, PROC CVPR IEEE, P4570, DOI 10.1109/CVPR.2017.486
   Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781
   Kim S, 2016, LECT NOTES COMPUT SC, V9912, P143, DOI 10.1007/978-3-319-46484-8_9
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kun Su, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9628, DOI 10.1109/CVPR42600.2020.00965
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li C, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P786
   Li C, 2017, IEEE INT CONF MULTI
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Li TJ, 2021, PROC CVPR IEEE, P16261, DOI 10.1109/CVPR46437.2021.01600
   Li YL, 2020, PROC CVPR IEEE, P379, DOI [10.1109/CVPR42600.2020.00046, 10.1109/ICEMME51517.2020.00080]
   Li YL, 2019, PROC CVPR IEEE, P3580, DOI 10.1109/CVPR.2019.00370
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2017, Arxiv, DOI arXiv:1705.08106
   Liu J, 2018, IEEE T IMAGE PROCESS, V27, P1586, DOI 10.1109/TIP.2017.2785279
   Liu J, 2018, IEEE T PATTERN ANAL, V40, P3007, DOI 10.1109/TPAMI.2017.2771306
   Liu J, 2017, PROC CVPR IEEE, P3671, DOI 10.1109/CVPR.2017.391
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu K, 2021, IEEE T MULTIMEDIA, V23, P64, DOI 10.1109/TMM.2020.2974323
   Liu MY, 2018, PROC CVPR IEEE, P1159, DOI 10.1109/CVPR.2018.00127
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Materzynska J, 2020, PROC CVPR IEEE, P1046, DOI 10.1109/CVPR42600.2020.00113
   Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rahmani H, 2016, PROC CVPR IEEE, P1506, DOI 10.1109/CVPR.2016.167
   Rahmani H, 2015, PROC CVPR IEEE, P2458, DOI 10.1109/CVPR.2015.7298860
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shao S, 2019, IEEE I CONF COMP VIS, P8429, DOI 10.1109/ICCV.2019.00852
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2018, IEEE INT CON MULTI
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Tang J., 2020, COMPUTER VISION ECCV, P71
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11
   Tianjiao Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P420, DOI 10.1007/978-3-030-58621-8_25
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Veeriah V, 2015, IEEE I CONF COMP VIS, P4041, DOI 10.1109/ICCV.2015.460
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang JY, 2021, IEEE T MULTIMEDIA, V23, P883, DOI 10.1109/TMM.2020.2990082
   Zhang PF, 2020, PROC CVPR IEEE, P1109, DOI 10.1109/CVPR42600.2020.00119
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang PF, 2018, LECT NOTES COMPUT SC, V11213, P136, DOI 10.1007/978-3-030-01240-3_9
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang T, 2020, IEEE T MULTIMEDIA, V22, P2926, DOI 10.1109/TMM.2020.2966878
   Zhang X., 2020, P IEEE CVF C COMP VI, P14333, DOI DOI 10.1109/CVPR42600202001434
   Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15
   Zhu KJ, 2020, IEEE T MULTIMEDIA, V22, P2977, DOI 10.1109/TMM.2019.2962304
NR 98
TC 4
Z9 4
U1 12
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4415
EP 4425
DI 10.1109/TMM.2022.3175374
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200024
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yan, H
   Zhang, HJ
   Liu, LL
   Zhou, DL
   Xu, XF
   Zhang, Z
   Yan, SC
AF Yan, Han
   Zhang, Haijun
   Liu, Linlin
   Zhou, Dongliang
   Xu, Xiaofei
   Zhang, Zhao
   Yan, Shuicheng
TI Toward Intelligent Design: An AI-Based Fashion Designer Using Generative
   Adversarial Networks Aided by Sketch and Rendering Generators
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fashion design; generative adversarial network; image translation;
   fashion data
AB The traditional fashion industry is heavily dependent on designers whose talent and vision have a significant impact on their innovative designs. Through taking advantage of recent advances in image-to-image translation by generative adversarial networks (GANs), marked improvement in designers' efficiency is now possible. Considering both randomness and controllability in the design process, this article presents a novel artificial intelligence (AI)-based framework for fashion design. Under this framework, a sketch-generation module which is based on latent space is firstly introduced for designing various sketches. Secondly, a rendering-generation module is proposed to learn mapping between textures and sketches to complete the task of fashion design. In order to achieve effectiveness in synthesizing semantic-aware textures on sketches, a multi-conditional feature interaction module is developed in the rendering-generation model. Moreover, two different training schemes are introduced to optimize both the sketch-generation module and the rendering-generation module. In order to evaluate the performance of our proposed models, we built a large-scale dataset which consists of 115,584 pairs of fashion item images. Experimental results demonstrate the effectiveness of our proposed method, and indicate that our model can facilitate designers' design process by taking full advantage of the controllability of different conditions (e.g., sketch and texture) and the randomness of latent space.
C1 [Yan, Han; Zhang, Haijun; Liu, Linlin; Zhou, Dongliang; Xu, Xiaofei] Xili Univ Town, Harbin Inst Technol Shenzhen, Dept Comp Sci, Shenzhen 518055, Peoples R China.
   [Zhang, Zhao] Hefei Univ Technol, Dept Comp Sci, Hefei 230000, Peoples R China.
   [Yan, Shuicheng] Sea AI Lab SAIL, Singapore 11758, Singapore.
   [Yan, Shuicheng] Natl Univ Singapore, Singapore 117583, Singapore.
C3 Harbin Institute of Technology; Hefei University of Technology; National
   University of Singapore
RP Zhang, HJ (corresponding author), Xili Univ Town, Harbin Inst Technol Shenzhen, Dept Comp Sci, Shenzhen 518055, Peoples R China.
EM 20b351014@stu.hit.edu.cn; hjzhang@hit.edu.cn; liulinlin@stu.hit.edu.cn;
   zhou_dongliang@stu.hit.edu.cn; xiaofei@hit.edu.cn; cszzhang@gmail.com;
   yansc@sea.com
RI Zhang, Haijun/N-8470-2015; Xu, Xiaolong/U-2547-2019; Yan,
   Shuicheng/HCI-1431-2022; Bilal, Muhammad/F-5225-2019; Zhou,
   Dongliang/AAY-4577-2021; Xu, Xiaofei/IQS-7571-2023; S,
   Vimal/E-9551-2016; Zhang, Zhao/B-5136-2010
OI Xu, Xiaolong/0000-0003-4879-9803; Bilal, Muhammad/0000-0003-4221-0877;
   Zhou, Dongliang/0000-0003-0361-8597; S, Vimal/0000-0002-1467-1206;
   Zhang, Zhao/0000-0002-5703-7969
FU National Natural Science Foundation of China [61972112, 61832004];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515020088];
   Shenzhen Science and Technology Program [JCYJ20210324131203009];
   HITSZ-J&A Joint Laboratory of Digital Design and Intelligent Fabrication
   [HITSZ-JA-2021A01]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61972112 and 61832004, in part by the
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2021B1515020088, in part by the Shenzhen Science and Technology Program
   under Grant JCYJ20210324131203009, and in part by the HITSZ-J&A Joint
   Laboratory of Digital Design and Intelligent Fabrication under Grant
   HITSZ-J&A-2021A01.
CR Bradski G., 2008, LEARNING OPENCV COMP, DOI DOI 10.1109/MRA.2009.933612
   Brock A., 2019, INT C LEARN REPR
   Cheng WH, 2021, ACM COMPUT SURV, V54, DOI [10.1145/3447239, 10.1145/3552468.3554360]
   Cui YR, 2018, COMPUT GRAPH FORUM, V37, P109, DOI 10.1111/cgf.13552
   Dong JF, 2021, IEEE T IMAGE PROCESS, V30, P8410, DOI 10.1109/TIP.2021.3115658
   Dosovitskiy A., 2016, Advances in Neural Information Processing Systems, P658
   Gatys L., 2015, NIPS
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hensel M, 2017, ADV NEUR IN, V30
   Huang HB, 2018, ADV NEUR IN, V31
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Gulrajani I, 2017, ADV NEUR IN, V30
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karnewar Animesh, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7796, DOI 10.1109/CVPR42600.2020.00782
   Karras Tero, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8107, DOI 10.1109/CVPR42600.2020.00813
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kingma D. P., 2014, AUTOENCODING VARIATI, P3581
   Kodali N, 2017, Arxiv, DOI [arXiv:1705.07215, DOI 10.48550/ARXIV.1705.07215]
   Kukiev B., 2019, Eur. J. Res. Reflect. Educ. Sci., V7, P49
   Lee J, 2020, PROC CVPR IEEE, P5800, DOI 10.1109/CVPR42600.2020.00584
   Liu LL, 2020, IEEE T NEUR NET LEAR, V31, P3540, DOI 10.1109/TNNLS.2019.2944979
   Liu LL, 2019, NEUROCOMPUTING, V341, P156, DOI 10.1016/j.neucom.2019.03.011
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Nakanishi Y., 1996, PROC ARTIF LIFE POST, P147
   Norouzi M., 2012, ADV NEURAL INFORM PR
   Ntu Xia-mu, 2008, Acta Electronica Sinica, V36, P1405
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Peng J, 2022, IEEE T MULTIMEDIA, V24, P4356, DOI 10.1109/TMM.2021.3116416
   Radford A., 2015, ARXIV
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Salimans T, 2016, ADV NEUR IN, V29
   Shelly G. B., 2009, Adobe Photoshop CS4: Comprehensive Concepts and Techniques
   Song SJ, 2021, IEEE T PATTERN ANAL, V43, P4161, DOI 10.1109/TPAMI.2020.2992105
   Sun TH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P683, DOI 10.1145/3343031.3351041
   Virtusio JJ, 2021, IEEE T MULTIMEDIA, V23, P2245, DOI 10.1109/TMM.2021.3087026
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xian WQ, 2018, PROC CVPR IEEE, P8456, DOI 10.1109/CVPR.2018.00882
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xinrui Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8087, DOI 10.1109/CVPR42600.2020.00811
   Yildirim G, 2018, Arxiv, DOI arXiv:1806.07819
   Yuan ML, 2013, IEEE T MULTIMEDIA, V15, P1958, DOI 10.1109/TMM.2013.2280560
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhao F., 2021, P IEEE CVF INT C COM, P13239
NR 47
TC 24
Z9 24
U1 37
U2 85
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2323
EP 2338
DI 10.1109/TMM.2022.3146010
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100056
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Yin, YF
   Deng, JJ
   Zhou, WA
   Li, L
   Li, HQ
AF Yin, Yufei
   Deng, Jiajun
   Zhou, Wengang
   Li, Li
   Li, Houqiang
TI FI-WSOD: Foreground Information Guided Weakly Supervised Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Object detection; weakly supervised learning
AB Existing solutions for weakly supervised object detection (WSOD) generally follow the multiple instance learning (MIL) paradigm to formulate WSOD as a multi-class classification problem over a set of region proposals. However, without the supervision signal of ground-truth boxes, the training objective of multi-class classification makes the detectors devote main efforts to finding the most common pattern of each class, as the common pattern is always the most discriminative evidence for classification. In addition, although learning from distinguishing multiple foreground classes, the detectors can still ignore to differentiate foreground regions from the background ones, which causes false alarm in prediction. These two points account for the limited localization capability of MIL-based WSOD methods. To this end, we propose foreground information guided WSOD (FI-WSOD), a novel framework that introduces an extra foreground-background binary classification (F-BBC) sub-task to the original MIL-based WSOD paradigm. At the training stage, the involvement of F-BBC task not only improves the feature representation of the network, but also provides extra information from the foreground-background perspective. By leveraging the learnt foreground information, a Foreground Guided Self-Training (FGST) module is further proposed to filter out noisy samples, and to mine representative seeds from the remaining proposals. Moreover, a Multi-Seed Training strategy is performed to reduce the impact of noisy labels when training the self-training networks in FGST. We have conducted extensive experiments on the prevalent Pascal VOC 2007, Pascal VOC 2012 and MSCOCO datasets, and report a series of state-of-the-art records achieved by our proposed framework.
C1 [Yin, Yufei; Deng, Jiajun; Zhou, Wengang; Li, Li; Li, Houqiang] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230026, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Zhou, WA; Li, HQ (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, CAS Key Lab Technol Geospatial Informat Proc & App, Hefei 230026, Peoples R China.
EM yinyufei@mail.ustc.edu.cn; dengjj@mail.ustc.edu.cn; zhwg@ustc.edu.cn;
   lil1@ustc.edu.cn; lihq@ustc.edu.cn
RI Deng, Jiajun/KIK-3592-2024; Li, Houqiang Li/B-6259-2013
OI Yin, Yufei/0000-0001-9643-4601; Deng, Jiajun/0000-0001-9624-7451
CR Ahn J, 2019, PROC CVPR IEEE, P2204, DOI 10.1109/CVPR.2019.00231
   Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523
   Arun A, 2019, PROC CVPR IEEE, P9424, DOI 10.1109/CVPR.2019.00966
   Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Cheng D, 2022, IEEE T IMAGE PROCESS, V31, P3334, DOI 10.1109/TIP.2022.3169693
   Cheng G, 2020, IEEE T IMAGE PROCESS, V29, P5794, DOI 10.1109/TIP.2020.2987161
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diba A, 2017, PROC CVPR IEEE, P5131, DOI 10.1109/CVPR.2017.545
   Duan KW, 2019, IEEE I CONF COMP VIS, P6568, DOI 10.1109/ICCV.2019.00667
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fan Junsong, 2020, EUR C COMP VIS
   Feng XX, 2021, IEEE T GEOSCI REMOTE, V59, P6946, DOI 10.1109/TGRS.2020.3030990
   Gao MF, 2018, LECT NOTES COMPUT SC, V11205, P155, DOI 10.1007/978-3-030-01246-5_10
   Gao Y, 2019, IEEE I CONF COMP VIS, P9833, DOI 10.1109/ICCV.2019.00993
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Huang T. S., 2016, P 24 ACM INT C MULT, P516, DOI 10.1145/2964284.2967274
   Jie ZQ, 2017, PROC CVPR IEEE, P4294, DOI 10.1109/CVPR.2017.457
   Kantorov V, 2016, LECT NOTES COMPUT SC, V9909, P350, DOI 10.1007/978-3-319-46454-1_22
   Ke Yang, 2019, 2019 IEEE/CVF International Conference on Computer Vision (ICCV). Proceedings, P8371, DOI 10.1109/ICCV.2019.00846
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Li XY, 2019, IEEE I CONF COMP VIS, P9734, DOI 10.1109/ICCV.2019.00983
   Lin CH, 2020, AAAI CONF ARTIF INTE, V34, P11482
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   Shen YH, 2019, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2019.00079
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang P, 2018, LECT NOTES COMPUT SC, V11215, P370, DOI 10.1007/978-3-030-01252-6_22
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P176, DOI 10.1109/TPAMI.2018.2876304
   Tang P, 2017, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2017.326
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wan F, 2019, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2019.00230
   Wan F, 2018, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2018.00141
   Wei Y., 2018, P EUR C COMP VIS, P434
   Xie EZ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P8372, DOI 10.1109/ICCV48922.2021.00828
   Xu YQ, 2021, IEEE T IMAGE PROCESS, V30, P3029, DOI 10.1109/TIP.2021.3056887
   Yang K, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1688, DOI 10.1145/3394171.3413835
   Yin YF, 2021, AAAI CONF ARTIF INTE, V35, P3190
   Ze Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12992, DOI 10.1109/CVPR42600.2020.01301
   Zeng ZY, 2019, IEEE I CONF COMP VIS, P8291, DOI 10.1109/ICCV.2019.00838
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P5866, DOI 10.1109/TPAMI.2021.3074313
   Zhang DW, 2022, IEEE T PATTERN ANAL, V44, P3349, DOI 10.1109/TPAMI.2020.3046647
   Zhongzheng Ren, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10595, DOI 10.1109/CVPR42600.2020.01061
NR 51
TC 4
Z9 4
U1 2
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1890
EP 1902
DI 10.1109/TMM.2022.3198018
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100024
DA 2024-07-18
ER

PT J
AU Yuan, HJ
   Chu, Q
   Zhu, F
   Zhao, R
   Liu, B
   Yu, NH
AF Yuan, Haojie
   Chu, Qi
   Zhu, Feng
   Zhao, Rui
   Liu, Bin
   Yu, Nenghai
TI AutoMA: Towards Automatic Model Augmentation for Transferable
   Adversarial Attacks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adversarial attack; AutoML; transferability
ID ROBUSTNESS; EXAMPLES
AB Recent adversarial attack works attempt to improve the transferability by applying various differentiable transformations on input images. Considering the differentiable transformations and the original model together as a new model, these methods can be regarded as model augmentation that effectively derives an ensemble of models from the single original model. Despite their impressive performance, the model augmentation policies used in these methods are manually designed by experimental attempts, leaving the design of model augmentation policy an open question. In this paper, we propose an Automatic Model Augmentation (AutoMA) approach to find a strong model augmentation policy for transferable adversarial attacks. Specifically, we design a discrete search space that contains various diffierentiable transformations with different parameters and adopt reinforcement learning to search for the strong augmentation policy. The sampled augmentation policies together with the rewards they obtain during the searching process reveal several valuable observations for designing more powerful attacks using model augmentation policy: 1) Augmentation transformations on color space are less effective; 2) The transformation type diversity matters; and 3) Using small distortion for geometric transformations while larger distortion for intensity transformations. Extensive experiments show that the augmentation policy found by AutoMA achieves superior performance than existing manually designed policies in a wide range of cases.
C1 [Yuan, Haojie; Chu, Qi; Liu, Bin; Yu, Nenghai] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
   [Zhu, Feng; Zhao, Rui] SenseTime Ltd, Shenzhen 265000, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Chu, Q (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
EM doubihj@mail.ustc.edu.cn; qchu@ustc.edu.cn; zhufengx@mail.ustc.edu.cn;
   fantasticzr2011@gmail.com; flowice@ustc.edu.cn; ynh@ustc.edu.cn
RI Sun, Xinyu/JXX-2281-2024; Huang, YQ/JOK-7580-2023; ZHANG,
   YINGFANG/JQW-2816-2023; li, qing/JEF-9044-2023; Chu, Qi/AAQ-5998-2020;
   Wang, lingyu/JLM-2013-2023; Zhang, Junran/JRY-8660-2023; Wang,
   Yuchen/JPW-9345-2023; Zeng, Yun/JFK-6190-2023; liu, wenli/JRW-0517-2023;
   Chen, Chao/JHS-6563-2023; Wang, Xingyu/JNE-0602-2023; Zhang,
   Xiaoxi/KBP-8753-2024; sun, huan/JEO-7152-2023; chen, wang/KGK-5932-2024;
   Zhang, Chi/JSK-0744-2023; Wang, Siying/KHX-1894-2024; Zhang,
   Xiaoyue/JFS-9880-2023; WANG, Bin/JGM-2639-2023; zhang, ly/JMB-7214-2023
FU National Natural Science Foundation of China [62002336, U20B2047];
   SenseTime Research Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62002336 and U20B2047, and in part by
   the SenseTime Research Fund for Young Scholars.
CR Amini S, 2020, IEEE T MULTIMEDIA, V22, P1889, DOI 10.1109/TMM.2020.2969784
   [Anonymous], 2016, P EUR C COMP VIS
   Bai T., 2020, PROC IEEECVF C COMPU, P860
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chao Li, 2019, NeurIPS, P10791
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cohen G., 2020, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, P14453
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Du YL, 2019, IEEE T MULTIMEDIA, V21, P555, DOI 10.1109/TMM.2018.2887018
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gao LL, 2022, IEEE T MULTIMEDIA, V24, P2329, DOI 10.1109/TMM.2021.3079723
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Inkawhich N, 2019, PROC CVPR IEEE, P7059, DOI 10.1109/CVPR.2019.00723
   Inoue H, 2018, Arxiv, DOI arXiv:1801.02929
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Junhua Zou, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P563, DOI 10.1007/978-3-030-58542-6_34
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li MS, 2020, PROC CVPR IEEE, P638, DOI 10.1109/CVPR42600.2020.00072
   Li YW, 2020, AAAI CONF ARTIF INTE, V34, P11458
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Lin J., 2019, PROC INT C LEARN REP, P4608
   Liu Y., 2017, PROC INT C LEARN REP, P1944
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Iandola FN, 2016, Arxiv, DOI arXiv:1602.07360
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Shafahi A, 2019, ADV NEUR IN, V32
   Shasha Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P396, DOI 10.1007/978-3-030-58592-1_24
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutton RS, 2000, ADV NEUR IN, V12, P1057
   Szegedy C., 2014, P ICLR
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Tramer F., 2018, PROC 6 INT C LEARN R
   Wang JW, 2022, IEEE T MULTIMEDIA, V24, P230, DOI 10.1109/TMM.2021.3050057
   Wu WB, 2020, PROC CVPR IEEE, P1158, DOI 10.1109/CVPR42600.2020.00124
   Xie C., 2018, PROC INT C LEARN REP, P2808
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yang EK, 2020, IEEE T CYBERNETICS, V50, P1473, DOI 10.1109/TCYB.2018.2882908
   Zhang JM, 2021, IEEE T MULTIMEDIA, V23, P2575, DOI 10.1109/TMM.2020.3013376
   Zhang X, 2020, 8 INT C LEARNING REP
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zheng HZ, 2020, PROC CVPR IEEE, P1178, DOI 10.1109/CVPR42600.2020.00126
   Zhou W, 2018, LECT NOTES COMPUT SC, V11218, P471, DOI 10.1007/978-3-030-01264-9_28
NR 57
TC 6
Z9 6
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 203
EP 213
DI 10.1109/TMM.2021.3124083
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 9E8KM
UT WOS:000937028400015
DA 2024-07-18
ER

PT J
AU Yuan, ZK
   Cheng, JD
   Yang, X
AF Yuan, Zikang
   Cheng, Junda
   Yang, Xin
TI CR-LDSO: Direct Sparse LiDAR-Assisted Visual Odometry With Cloud Reusing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE SLAM; sensor fusion; localization
ID DEPTH PREDICTION; MONOCULAR SLAM; ROBUST; EFFICIENT; VERSATILE
AB LiDAR-assisted visual odometry (VO) is a widely-used solution for pose estimation and mapping. However, most existing LiDAR-assisted VO systems could suffer from the problems of 1) lacking distinctive and evenly distributed pixels for tracking due to the sparsity of LiDAR points and limited FOV overlap between a camera and LiDAR, and 2) nontrivial errors when processing LiDAR point clouds. To address above problems, we present CR-LDSO, a direct sparse LiDAR-assisted VO with the core parts being: 1) a novel cloud reusing method with point extraction/re-extraction to increase both the camera-LiDAR FOV overlap and the number of high-quality tracking pixels and 2) an occlusion removal method to exclude mismatching pixels due to occluded 3D object from sliding-window optimization and a point extraction strategy without depth interpolation. Extensive experimental results on public datasets demonstrates the superiority of our method to the existing state-of-the-art methods.
C1 [Yuan, Zikang] Huazhong Univ Sci & Technol, Inst Artificial Intelligence, Wuhan 430074, Peoples R China.
   [Cheng, Junda; Yang, Xin] Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Huazhong University of
   Science & Technology
RP Yang, X (corresponding author), Huazhong Univ Sci & Technol, Elect Informat & Commun, Wuhan 430074, Peoples R China.
EM zikangyuan@hust.edu.cn; cjd@hust.edu.cn; xinyang2014@hust.edu.cn
FU National Natural Science Foundation of China
FX No Statement Available
CR Campos C, 2021, IEEE T ROBOT, V37, P1874, DOI 10.1109/TRO.2021.3075644
   Dai WC, 2022, IEEE T PATTERN ANAL, V44, P373, DOI 10.1109/TPAMI.2020.3010942
   Engel J, 2018, IEEE T PATTERN ANAL, V40, P611, DOI 10.1109/TPAMI.2017.2658577
   Engel J, 2014, LECT NOTES COMPUT SC, V8690, P834, DOI 10.1007/978-3-319-10605-2_54
   Gee AP, 2006, LECT NOTES COMPUT SC, V4292, P354
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Graeter J, 2018, IEEE INT C INT ROBOT, P7872, DOI 10.1109/IROS.2018.8594394
   Huang SS, 2020, IEEE INT CONF ROBOT, P1091, DOI [10.1109/ICRA40945.2020.9196613, 10.1109/icra40945.2020.9196613]
   Klein G, 2008, LECT NOTES COMPUT SC, V5303, P802, DOI 10.1007/978-3-540-88688-4_59
   Lemaire T, 2007, INT J COMPUT VISION, V74, P343, DOI 10.1007/s11263-007-0042-3
   Liao YY, 2023, IEEE T PATTERN ANAL, V45, P3292, DOI 10.1109/TPAMI.2022.3179507
   Luo HC, 2019, IEEE T MULTIMEDIA, V21, P470, DOI 10.1109/TMM.2018.2859034
   Micusik B, 2021, INT J COMPUT VISION, V129, P2011, DOI 10.1007/s11263-021-01462-y
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671
   Pradeep V, 2012, INT J COMPUT VISION, V98, P202, DOI 10.1007/s11263-011-0504-5
   Pumarola A., 2017, P 2017 IEEE INT C RO, P4503, DOI DOI 10.1109/ICRA.2017.7989522
   Qin T, 2018, IEEE T ROBOT, V34, P1004, DOI 10.1109/TRO.2018.2853729
   Shin YS, 2020, AUTON ROBOT, V44, P115, DOI 10.1007/s10514-019-09881-0
   Sim R, 2007, INT J COMPUT VISION, V74, P303, DOI 10.1007/s11263-006-0021-0
   Smith Paul, 2006, Bellbird, V1, P17
   Wang R, 2017, IEEE I CONF COMP VIS, P3923, DOI 10.1109/ICCV.2017.421
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Yang X, 2019, IEEE T MULTIMEDIA, V21, P2701, DOI 10.1109/TMM.2019.2912121
   Yu C, 2018, IEEE INT C INT ROBOT, P1168, DOI 10.1109/IROS.2018.8593691
   Yuan ZK, 2022, IEEE T MULTIMEDIA, V24, P4092, DOI 10.1109/TMM.2021.3114546
   Yuan ZK, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1410, DOI 10.1145/3343031.3351079
   Zhang J, 2015, IEEE INT CONF ROBOT, P2174, DOI 10.1109/ICRA.2015.7139486
   Zhang J, 2017, AUTON ROBOT, V41, P31, DOI [10.1007/s10514-015-9525-1, 10.1109/MWSYM.2015.7167049]
NR 29
TC 2
Z9 2
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9397
EP 9409
DI 10.1109/TMM.2023.3252161
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200026
DA 2024-07-18
ER

PT J
AU Yue, HH
   Guo, JC
   Yin, XJ
   Zhang, Y
   Zheng, SD
AF Yue, Huihui
   Guo, Jichang
   Yin, Xiangjun
   Zhang, Yi
   Zheng, Sida
TI Deep Label Prior: Pre-Training-Free Salient Object Detection Network
   Based on Label Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Salient object detection; deep label prior; morphology operation; deep
   image prior
ID ATTENTION
AB Due to the excellent semantics extraction capabilities, deep learning methods have significantly progressed in salient object detection (SOD). However, these methods often require time-consuming pre-training and large training datasets with ground truth. To address these issues, by referring to the framework known as "deep image prior (DIP)," we propose a SOD method called deep label prior network (DLPNet), which consists of A-stream and B-stream. The A-stream includes two cascaded UNets and a simple CNNs module to extract the initial saliency map, while the B-stream contains only two cascaded UNets, which refines the extracted initial saliency map. Unlike most of the current deep learning methods, DLPNet views the SOD task as a conditional image generation problem, relying on only the internal prior of the input itself to generate the saliency map. Hence, our DLPNet does not require pre-training or large annotated / unannotated datasets. Furthermore, we propose a morphology operation scheme, which creates rich pseudo-labels for facilitating the updating of network weights. Extensive experiments demonstrate that our method outperforms state-of-the-art unsupervised techniques and is even comparable to state-of-the-art supervised and weakly supervised methods on different evaluation metrics.
C1 [Yue, Huihui; Guo, Jichang; Yin, Xiangjun; Zhang, Yi; Zheng, Sida] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Guo, JC (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM yuehuihui@tju.edu.cn; jcguo@tju.edu.cn; yinxiangjun@tju.edu.cn;
   zhangyi123@tju.edu.cn; zhengsida@tju.edu.cn
RI Guo, Jichang/GQY-5798-2022
OI Guo, Jichang/0000-0003-3130-1685
CR Baguer DO, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/aba415
   Bi HB, 2021, NEUROCOMPUTING, V439, P63, DOI 10.1016/j.neucom.2020.12.125
   Chen ZY, 2020, AAAI CONF ARTIF INTE, V34, P10599
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fatemi Narges, 2019, 2019 4th International Conference on Pattern Recognition and Image Analysis (IPRIA), P32, DOI 10.1109/PRIA.2019.8785974
   Feng MY, 2019, PROC CVPR IEEE, P1623, DOI 10.1109/CVPR.2019.00172
   Gandelsman Y, 2019, PROC CVPR IEEE, P11018, DOI 10.1109/CVPR.2019.01128
   Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688
   Hu ZW, 2020, PROC CVPR IEEE, P4423, DOI 10.1109/CVPR42600.2020.00448
   Ke YY, 2022, IEEE WINT CONF APPL, P1360, DOI 10.1109/WACV51458.2022.00143
   Kingma D. P., 2014, arXiv
   Kong ZM, 2021, IEEE ACCESS, V9, P51179, DOI 10.1109/ACCESS.2021.3069581
   Liang YH, 2021, NEUROCOMPUTING, V422, P22, DOI 10.1016/j.neucom.2020.09.033
   Liu JJ, 2019, PROC CVPR IEEE, P3912, DOI 10.1109/CVPR.2019.00404
   Liu N, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4702, DOI 10.1109/ICCV48922.2021.00468
   Liu NA, 2020, IEEE T IMAGE PROCESS, V29, P6438, DOI 10.1109/TIP.2020.2988568
   Liu N, 2022, IEEE T PATTERN ANAL, V44, P9026, DOI 10.1109/TPAMI.2021.3122139
   Liu NA, 2022, IEEE T PATTERN ANAL, V44, P8321, DOI 10.1109/TPAMI.2021.3107872
   Liu YX, 2021, IEEE T IMAGE PROCESS, V30, P4423, DOI 10.1109/TIP.2021.3071691
   Nguyen TH., 2019, NEURIPS
   Piao YR, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P4116, DOI 10.1109/ICCV48922.2021.00410
   Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Quan R, 2018, IEEE T MULTIMEDIA, V20, P1101, DOI 10.1109/TMM.2017.2763780
   Ren QH, 2021, IEEE T MULTIMEDIA, V23, P1442, DOI 10.1109/TMM.2020.2997178
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sidorov O, 2019, IEEE INT CONF COMP V, P3844, DOI 10.1109/ICCVW.2019.00477
   Sun H, 2021, IEEE ACCESS, V9, P52378, DOI 10.1109/ACCESS.2021.3069236
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Ullah A, 2021, NEUROCOMPUTING, V435, P321, DOI 10.1016/j.neucom.2019.12.151
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   Unser M, 2017, SIAM REV, V59, P769, DOI 10.1137/16M1061199
   Van Veen D, 2020, Arxiv, DOI arXiv:1806.06438
   Vu T, 2021, PHOTOACOUSTICS, V22, DOI 10.1016/j.pacs.2021.100266
   Wang B, 2020, AAAI CONF ARTIF INTE, V34, P12128
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang WG, 2022, IEEE T PATTERN ANAL, V44, P3239, DOI 10.1109/TPAMI.2021.3051099
   Wang WG, 2020, IEEE T PATTERN ANAL, V42, P1913, DOI 10.1109/TPAMI.2019.2905607
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P5961, DOI 10.1109/CVPR.2019.00612
   Wang WG, 2019, IEEE T PATTERN ANAL, V41, P1531, DOI 10.1109/TPAMI.2018.2840724
   Wu YJ, 2019, IEEE ACCESS, V7, P30659, DOI 10.1109/ACCESS.2019.2903125
   Xia C., 2018, IEEE Photon. J., V10, P1
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yu SY, 2021, AAAI CONF ARTIF INTE, V35, P3234
   Yun-Chun Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P442, DOI 10.1007/978-3-030-58523-5_26
   Zeng Y, 2019, IEEE I CONF COMP VIS, P7233, DOI 10.1109/ICCV.2019.00733
   Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang J., 2020, P IEEE CVF C COMP VI, P8579, DOI DOI 10.1109/CVPR42600.2020.00861
   Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941
   Zhang Jing, 2020, P IEEECVF C COMPUTER, P12546, DOI DOI 10.1109/CVPR42600.2020.01256
   Zhang M, 2020, PROC CVPR IEEE, P3469, DOI 10.1109/CVPR42600.2020.00353
   Zhang YF, 2021, NEUROCOMPUTING, V423, P463, DOI 10.1016/j.neucom.2020.10.079
   Zhang Z, 2021, IEEE T IMAGE PROCESS, V30, P1949, DOI 10.1109/TIP.2021.3049959
   Zhao K, 2022, IEEE T PATTERN ANAL, V44, P4793, DOI 10.1109/TPAMI.2021.3077129
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zheng XY, 2021, IEEE T CIRC SYST VID, V31, P4370, DOI 10.1109/TCSVT.2021.3049408
   Zhou XF, 2022, IEEE T GEOSCI REMOTE, V60, DOI 10.1109/TGRS.2021.3091312
NR 60
TC 0
Z9 0
U1 5
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6067
EP 6078
DI 10.1109/TMM.2022.3204440
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500031
DA 2024-07-18
ER

PT J
AU Zhong, X
   Gu, C
   Ye, M
   Huang, WX
   Lin, CW
AF Zhong, Xian
   Gu, Cheng
   Ye, Mang
   Huang, Wenxin
   Lin, Chia-Wen
TI Graph Complemented Latent Representation for Few-Shot Image
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Few-shot learning; graph network; meta-learning; representation
   deficiency; variational inference
ID NETWORK
AB Few-shot learning is a tough topic to solve since obtaining a large number of training samples in real applications is challenging. It has attracted increasing attention recently. Meta-learning is a prominent way to address this issue, intending to adapt predictors as base-learners to new tasks swiftly. However, a key challenge of meta-learning is its lack of expressive capacity, which stems from the difficulty of extracting general information from a small number of training samples. As a result, the generalizability of meta-learners trained from high-dimensional parameter spaces is frequently limited. To learn a better representation, we propose a graph complemented latent representation (GCLR) network for few-shot image classification. In particular, we embed the representation into a latent space, in which the latent codes are reconstructed using variational information to enrich the representation. In this way, the latent representation can achieve better generalizability. Another benefit is that, because the latent space is formed using variational inference, it cooperates well with various base-learners, boosting robustness. To make full use of the relation between samples in each category, a graph neural network (GNN) is also incorporated to improve relation mining. Consequently, our end-to-end framework delivers competitive performance on three few-shot learning benchmarks for image classification.
C1 [Zhong, Xian] Wuhan Univ Technol, Sch Comp Sci & Artificial Intelligence, Wuhan 430070, Peoples R China.
   [Zhong, Xian] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100091, Peoples R China.
   [Gu, Cheng] ZhongNeng Power Tech Dev Co Ltd, Beijing 100034, Peoples R China.
   [Ye, Mang] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Huang, Wenxin] Hubei Univ, Sch Comp Sci & Informat Engn, Wuhan 430062, Peoples R China.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu 30013, Taiwan.
   [Lin, Chia-Wen] Natl Tsing Hua Univ, Inst Commun Engn, Hsinchu 30013, Taiwan.
C3 Wuhan University of Technology; Peking University; Wuhan University;
   Hubei University; National Tsing Hua University; National Tsing Hua
   University
RP Ye, M (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
EM zhongx@whut.edu.cn; 20066133@chnenergy.com.cn; mangye16@gmail.com;
   wenxinhuang_wh@163.com; cwlin@ee.nthu.edu.tw
RI Huang, Wenxin/AFN-5558-2022; Ye, Mang/AAT-6142-2020; Lin,
   Chia-Wen/ABH-6075-2020
OI Ye, Mang/0000-0003-3989-7655; Zhong, Xian/0000-0002-5242-0467; Gu,
   Cheng/0000-0003-2489-6131
FU Department of Science and Technology, Hubei Provincial People's
   Government [2021CFB513, 2021CFB281]; Ministry of Science and Technology,
   Taiwan [MOST 109-2634-F-007-013]; CAAI-Huawei MindSpore Open Fund
FX This work was supported in part by the Department of Science and
   Technology, Hubei Provincial People's Government under Grants 2021CFB513
   and 2021CFB281, in part by the Ministry of Science and Technology,
   Taiwan, under Grant MOST 109-2634-F-007-013, and in part by the
   CAAI-Huawei MindSpore Open Fund.
CR Afrasiyabi Arman, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12350), P18, DOI 10.1007/978-3-030-58558-7_2
   [Anonymous], 2020, MindSpore
   Ao Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P346, DOI 10.1007/978-3-030-58610-2_21
   Bateni Peyman, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14481, DOI 10.1109/CVPR42600.2020.01450
   Bertinetto Luca, 2019, PROC INT C LEARN REP
   Chen MT, 2020, AAAI CONF ARTIF INTE, V34, P10559
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chu WH, 2019, PROC CVPR IEEE, P6244, DOI 10.1109/CVPR.2019.00641
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Defferrard M, 2016, ADV NEUR IN, V29
   Dhillon G. S., 2020, INT C LEARN REPR
   Duvenaudt D, 2015, ADV NEUR IN, V28
   Dvornik Nikita, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P769, DOI 10.1007/978-3-030-58607-2_45
   Finn C., 2018, PROC OPENREVIEW INT, P1
   Finn C, 2018, ADV NEUR IN, V31
   Finn C, 2017, PR MACH LEARN RES, V70
   Gidaris S, 2019, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2019.00011
   Gilmer J, 2017, PR MACH LEARN RES, V70
   Gori M, 2005, IEEE IJCNN, P729
   Guo-Sen Xie, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12349), P562, DOI 10.1007/978-3-030-58548-8_33
   Gusi Te, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P258, DOI 10.1007/978-3-030-58610-2_16
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hu MS, 2022, IEEE T CIRC SYST VID, V32, P3390, DOI 10.1109/TCSVT.2021.3110796
   Hu YF, 2021, IEEE T MULTIMEDIA, V23, P4285, DOI 10.1109/TMM.2020.3039329
   Huang HX, 2021, IEEE T MULTIMEDIA, V23, P1666, DOI 10.1109/TMM.2020.3001510
   Hui BY, 2019, IEEE INT CONF MULTI, P198, DOI 10.1109/ICMEW.2019.00041
   Jiang K, 2021, IEEE T IMAGE PROCESS, V30, P7404, DOI 10.1109/TIP.2021.3102504
   Jiang X., 2019, PROC OPENREVIEW INT, P1
   Krizhevsky A., 2010, Cifar-10 (canadian institute for advanced research)
   Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091
   Lee Y, 2018, PR MACH LEARN RES, V80
   Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79
   Li JJ, 2021, IEEE T PATTERN ANAL, V43, P3918, DOI 10.1109/TPAMI.2020.2991050
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li WB, 2019, PROC CVPR IEEE, P7253, DOI 10.1109/CVPR.2019.00743
   Li Y., 2016, ICLR, P1, DOI DOI 10.48550/ARXIV.1511.05493
   Liu Y., 2019, PROC OPENREVIEW INT, P1
   Lu GY, 2017, IEEE T MULTIMEDIA, V19, P2117, DOI 10.1109/TMM.2017.2731044
   Mishra N., 2018, INT C LEARN REPR
   Oreshkin BN, 2018, ADV NEUR IN, V31
   Ravi S, 2016, PROC INT C LEARN REP
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rusu A. A., 2019, INT C LEARN REPR
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schönfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844
   Shi W., 2020, P IEEE CVF C COMP VI, P1711, DOI DOI 10.1109/CVPR42600.2020.00178
   Silver T, 2020, AAAI CONF ARTIF INTE, V34, P10251
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Snell J, 2017, ADV NEUR IN, V30
   Sukhbaatar S, 2016, ADV NEUR IN, V29
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Vinyals O, 2016, 30 C NEURAL INFORM P, V29
   Welling M., 2014, PROC OPENREVIEW INT, P1
   Yao HX, 2020, AAAI CONF ARTIF INTE, V34, P6656
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P924, DOI 10.1109/TPAMI.2020.3013379
   Ye M, 2022, IEEE T INF FOREN SEC, V17, P386, DOI 10.1109/TIFS.2021.3139224
   Ye M, 2022, IEEE T IMAGE PROCESS, V31, P379, DOI 10.1109/TIP.2021.3131937
   Ye M, 2020, PROC CVPR IEEE, P5456, DOI 10.1109/CVPR42600.2020.00550
   Ye M, 2016, IEEE T MULTIMEDIA, V18, P2553, DOI 10.1109/TMM.2016.2605058
   Yoon J, 2018, ADV NEUR IN, V31
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zeng ZL, 2023, IEEE T MULTIMEDIA, V25, P2176, DOI 10.1109/TMM.2022.3144066
   Zhang G, 2019, PR MACH LEARN RES, V97
   Zhong X, 2022, IEEE T CIRC SYST VID, V32, P1418, DOI 10.1109/TCSVT.2021.3072171
   Zhong X, 2021, INT C PATT RECOG, P2677, DOI 10.1109/ICPR48806.2021.9412416
   Zhou F., 2019, P ADV NEUR INF PROC, P7013
   Zhu YH, 2021, IEEE T MULTIMEDIA, V23, P1200, DOI 10.1109/TMM.2020.2993952
   Zou YX, 2020, IEEE T MULTIMEDIA, V22, P3166, DOI 10.1109/TMM.2020.2972128
NR 70
TC 24
Z9 24
U1 5
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1979
EP 1990
DI 10.1109/TMM.2022.3141886
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100031
DA 2024-07-18
ER

PT J
AU Zhou, JQ
   Fu, ZH
   Huang, QY
   Liu, QJ
   Wang, YH
AF Zhou, Jiaqi
   Fu, Zehua
   Huang, Qiuyu
   Liu, Qingjie
   Wang, Yunhong
TI LgNet: A Local-Global Network for Action Recognition and Beyond
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video action recognition; self-supervised learning; supervised learning;
   local-global modeling
AB This work addresses the task of action recognition in video sequences. In real world applications, this task is quite challenging due to the complex background of video content, the similarities between different types of actions, the dependence on a large amount of annotated data, and so on. Most of the existing methods fail to distinguish similar actions with the same static appearance and motion pattern. We attempt to address this issue from the perspective of a local-global view, considering videos as combinations of a set of action units (local semantic information) and their relations along temporal dimension (global relation information). To achieve this end, we propose a novel Local-global Networks (LgNet) to enhance recognition of similar action. Besides, we propose an end-to-end training method to decrease the reliance on annotated data. It combines self-supervised learning and supervised learning, which not only enables the model to learn video representations from a large number unannotated data but also avoids subsequent finetuning. The proposed training method can be flexibly equipped to a wide array of vision tasks. Experiments on several benchmark datasets show that our proposed model and training method achieve state-of-the-art performance.
C1 [Zhou, Jiaqi; Huang, Qiuyu; Liu, Qingjie; Wang, Yunhong] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Zhou, Jiaqi; Huang, Qiuyu; Liu, Qingjie; Wang, Yunhong] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
   [Fu, Zehua] Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
C3 Beihang University; Beihang University; Beihang University
RP Liu, QJ (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Liu, QJ (corresponding author), Beihang Univ, Hangzhou Innovat Inst, Hangzhou 310051, Peoples R China.
EM gracciechou@buaa.edu.cn; zehua_fu@buaa.edu.cn; huangqiuyu@buaa.edu.cn;
   qingjie.liu@buaa.edu.cn; yhwang@buaa.edu.cn
RI Liu, Qingjie/IVH-7937-2023
OI Liu, Qingjie/0000-0003-2789-7113
FU National Key Research and Development Program of China [2018YFB1701600];
   National Natural Science Foundation of China [U20B2069]
FX The work was supported in part by the National Key Research and
   Development Program of China under Grant 2018YFB1701600 and in part by
   the National Natural Science Foundation of China under Grant U20B2069.
CR Abu-El-Haija Sami, 2016, arXiv
   Benaim S., 2020, CVPR, P9919
   Büchler U, 2018, LECT NOTES COMPUT SC, V11219, P797, DOI 10.1007/978-3-030-01267-0_47
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen X, 2021, AUTOPHAGY, V17, P2054, DOI 10.1080/15548627.2020.1810918
   Chen YP, 2018, LECT NOTES COMPUT SC, V11205, P364, DOI 10.1007/978-3-030-01246-5_22
   Diba A, 2019, IEEE I CONF COMP VIS, P6191, DOI 10.1109/ICCV.2019.00629
   Diba A, 2018, LECT NOTES COMPUT SC, V11208, P299, DOI 10.1007/978-3-030-01225-0_18
   Diba A, 2017, PROC CVPR IEEE, P1541, DOI 10.1109/CVPR.2017.168
   Ding XH, 2019, IEEE I CONF COMP VIS, P1911, DOI 10.1109/ICCV.2019.00200
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C., 2016, ADV NEURAL INFORM PR, P3468
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Fernando B, 2017, PROC CVPR IEEE, P5729, DOI 10.1109/CVPR.2017.607
   Girdhar R, 2017, PROC CVPR IEEE, P3165, DOI 10.1109/CVPR.2017.337
   Goyal Priya, 2017, CoRR abs/1706.02677
   Han TD, 2019, IEEE INT CONF COMP V, P1483, DOI 10.1109/ICCVW.2019.00186
   Han Tengda, 2020, Adv. Neural Inf. Process. Syst., NIPS, V33, P5679
   Hara K, 2018, PROC CVPR IEEE, P6546, DOI 10.1109/CVPR.2018.00685
   He DL, 2019, AAAI CONF ARTIF INTE, P8401
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G. E., 2012, 12070580 ARXIV
   Hjelm R.D., 2019, P 7 INT C LEARNING R
   Jing L., 2018, arXiv
   Jinhyung Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12100, DOI 10.1109/CVPR42600.2020.01212
   Kaiming He, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9726, DOI 10.1109/CVPR42600.2020.00975
   Kay W., 2017, ARXIV170506950
   Kim D, 2019, AAAI CONF ARTIF INTE, P8545
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan ZZ, 2017, IEEE COMPUT SOC CONF, P1219, DOI 10.1109/CVPRW.2017.161
   Lee HY, 2017, IEEE I CONF COMP VIS, P667, DOI 10.1109/ICCV.2017.79
   Li Y, 2020, PROC CVPR IEEE, P906, DOI 10.1109/CVPR42600.2020.00099
   Li YH, 2019, AAAI CONF ARTIF INTE, P8674
   Lin J, 2019, IEEE I CONF COMP VIS, P7082, DOI 10.1109/ICCV.2019.00718
   Loshchilov I., 2017, P INT C LEARN REPR
   Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Rodríguez P, 2020, IEEE T MULTIMEDIA, V22, P502, DOI 10.1109/TMM.2019.2928494
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Sun Chen., 2019, arXiv
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tengda Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P312, DOI 10.1007/978-3-030-58580-8_19
   Tran D, 2019, IEEE I CONF COMP VIS, P5551, DOI 10.1109/ICCV.2019.00565
   Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675
   vandenOord Aaron, 2018, ARXIV180703748
   Velickovic Petar, 2019, ICLR
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19
   Xinyu Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12351), P275, DOI 10.1007/978-3-030-58539-6_17
   Xu DJ, 2019, PROC CVPR IEEE, P10326, DOI 10.1109/CVPR.2019.01058
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Zhang W, 2020, IEEE T MULTIMEDIA, V22, P515, DOI 10.1109/TMM.2019.2928998
NR 65
TC 3
Z9 3
U1 1
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 5192
EP 5205
DI 10.1109/TMM.2022.3189253
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300042
DA 2024-07-18
ER

PT J
AU Zhou, WJ
   Yang, EQ
   Lei, JS
   Wan, J
   Yu, L
AF Zhou, Wujie
   Yang, Enquan
   Lei, Jingsheng
   Wan, Jian
   Yu, Lu
TI PGDENet: Progressive Guided Fusion and Depth Enhancement Network for
   RGB-D Indoor Scene Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-level information; depth enhancement; modality-specific fusion;
   progressive guided fusion; RGB-D indoor scene parsing
ID INFORMATION
AB Scene parsing is a fundamental task in computer vision. Various RGB-D (color and depth) scene parsing methods based on fully convolutional networks have achieved excellent performance. However, color and depth information are different in nature and existing methods cannot optimize the cooperation of high-level and low-level information when aggregating modal information, which introduces noise or loss of key information in the aggregated features and generates inaccurate segmentation maps. The features extracted from the depth branch are weak because of the low quality of the depth map, which results in unsatisfactory feature representation. To address these drawbacks, we propose a progressive guided fusion and depth enhancement network (PGDENet) for RGB-D indoor scene parsing. First, high-quality RGB images are used to improve depth data through a depth enhancement module, in which the depth maps are strengthened in terms of channel and spatial correlations. Then, we integrate information from the RGB and enhance depth modalities using a progressive complementary fusion module, in which we start with high-level semantic information and move down layerwise to guide the fusion of adjacent layers while reducing hierarchy-based differences. Extensive experiments are conducted on two public indoor scene datasets, and the results show that the proposed PGDENet outperforms state-of-the-art methods in RGB-D scene parsing.
C1 [Zhou, Wujie; Yang, Enquan; Lei, Jingsheng; Wan, Jian] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
   [Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310023, Peoples R China.
C3 Zhejiang University of Science & Technology; Zhejiang University
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
EM wujiezhou@163.com
OI zhou, wujie/0000-0002-3055-2493
FU National Natural Science Foundation of China [61502429, 61972357,
   61972358, 62071427]; Zhejiang Provincial Natural Science Foundation of
   China [LY18F020012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61502429, 61972357, 61972358, and
   62071427 and in part by the Zhejiang Provincial Natural Science
   Foundation of China under Grant LY18F020012.~
CR Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LZ, 2021, IEEE T IMAGE PROCESS, V30, P2313, DOI 10.1109/TIP.2021.3049332
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Couprie C, 2013, Arxiv, DOI arXiv:1301.3572
   Dal Mutto C, 2012, IEEE J-STSP, V6, P505, DOI 10.1109/JSTSP.2012.2194474
   Deng L., 2019, arXiv
   Deng-Ping Fan, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P275, DOI 10.1007/978-3-030-58610-2_17
   Ding HH, 2019, IEEE I CONF COMP VIS, P6818, DOI 10.1109/ICCV.2019.00692
   Fayyaz M, 2017, LECT NOTES COMPUT SC, V10116, P493, DOI 10.1007/978-3-319-54407-6_33
   Fu H, 2018, PATTERN RECOGN, V84, P226, DOI 10.1016/j.patcog.2018.07.020
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Fu Jun, 2019, IEEE Trans Image Process, DOI 10.1109/TIP.2019.2895460
   Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He JJ, 2019, IEEE I CONF COMP VIS, P3561, DOI 10.1109/ICCV.2019.00366
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2017, PROC CVPR IEEE, P7158, DOI 10.1109/CVPR.2017.757
   Hu XX, 2019, IEEE IMAGE PROC, P1440, DOI [10.1109/ICIP.2019.8803025, 10.1109/icip.2019.8803025]
   Huang NAC, 2022, IEEE T MULTIMEDIA, V24, P1651, DOI 10.1109/TMM.2021.3069297
   Huang NAC, 2021, IEEE T MULTIMEDIA, V23, P2428, DOI 10.1109/TMM.2020.3011327
   Huang Z, 2021, NEUROCOMPUTING, V452, P200, DOI 10.1016/j.neucom.2021.04.053
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Huang ZL, 2020, IEEE T IMAGE PROCESS, V29, P2066, DOI 10.1109/TIP.2019.2941644
   Jiang B, 2021, IEEE T MULTIMEDIA, V23, P1343, DOI 10.1109/TMM.2020.2997184
   Jiang JD, 2018, Arxiv, DOI arXiv:1806.01054
   Li GY, 2021, IEEE T IMAGE PROCESS, V30, P3528, DOI 10.1109/TIP.2021.3062689
   Li XL, 2022, IEEE T CYBERNETICS, V52, P9352, DOI 10.1109/TCYB.2021.3050558
   Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34
   Lin D, 2017, IEEE I CONF COMP VIS, P1320, DOI 10.1109/ICCV.2017.147
   Lin D, 2020, IEEE T CYBERNETICS, V50, P1120, DOI 10.1109/TCYB.2018.2885062
   Liu FY, 2017, IEEE T IMAGE PROCESS, V26, P2127, DOI 10.1109/TIP.2017.2675166
   Liu H, 2018, MULTIMED TOOLS APPL, V77, P22475, DOI 10.1007/s11042-018-6056-8
   Liu SP, 2022, IEEE T MULTIMEDIA, V24, P2392, DOI 10.1109/TMM.2021.3080076
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Ma LN, 2017, IEEE INT C INT ROBOT, P598, DOI 10.1109/IROS.2017.8202213
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5
   Qi XJ, 2017, IEEE I CONF COMP VIS, P5209, DOI 10.1109/ICCV.2017.556
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Seichter D, 2021, Arxiv, DOI arXiv:2011.06961
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Takikawa T, 2019, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2019.00533
   Tang Q, 2022, IEEE T INTELL TRANSP, V23, P7008, DOI 10.1109/TITS.2021.3066401
   Valada A, 2020, INT J COMPUT VISION, V128, P1239, DOI 10.1007/s11263-019-01188-y
   Wang W., 2020, P EUR C COMP VIS, P135
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Xiao JX, 2013, IEEE I CONF COMP VIS, P1625, DOI 10.1109/ICCV.2013.458
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Xiong ZT, 2020, PROC CVPR IEEE, P3991, DOI 10.1109/CVPR42600.2020.00405
   Youwei Pang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9410, DOI 10.1109/CVPR42600.2020.00943
   Yuan JZ, 2019, IEEE ACCESS, V7, P169350, DOI 10.1109/ACCESS.2019.2955101
   Yuan YH, 2021, Arxiv, DOI arXiv:1809.00916
   Zhang GD, 2021, IEEE SIGNAL PROC LET, V28, P658, DOI 10.1109/LSP.2021.3066071
   Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747
   Zhang PP, 2020, IEEE T IMAGE PROCESS, V29, P5010, DOI 10.1109/TIP.2020.2978339
   Zhang PP, 2020, IEEE T IMAGE PROCESS, V29, P4556, DOI 10.1109/TIP.2019.2957915
   Zhang WH, 2021, IEEE INFOCOM SER, DOI 10.1109/INFOCOM42981.2021.9488834
   Zhao HS, 2018, LECT NOTES COMPUT SC, V11213, P270, DOI 10.1007/978-3-030-01240-3_17
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao YF, 2019, IEEE I CONF COMP VIS, P9176, DOI 10.1109/ICCV.2019.00927
   Zhou WJ, 2021, IEEE T IMAGE PROCESS, V30, P7790, DOI 10.1109/TIP.2021.3109518
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2526, DOI 10.1109/TMM.2021.3086618
   Zhou WJ, 2021, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2021.3105484
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou WJ, 2021, IEEE T MULTIMEDIA, V23, P3388, DOI 10.1109/TMM.2020.3025166
   Zhou WJ, 2021, IEEE INTELL SYST, V36, P73, DOI 10.1109/MIS.2020.2999462
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
   Zhou WJ, 2018, IEEE T IMAGE PROCESS, V27, P2086, DOI 10.1109/TIP.2018.2794207
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
   Zhu CB, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3319368
   Zhu Z, 2019, IEEE I CONF COMP VIS, P593, DOI 10.1109/ICCV.2019.00068
NR 74
TC 30
Z9 30
U1 4
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3483
EP 3494
DI 10.1109/TMM.2022.3161852
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200042
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zou, WB
   Chen, L
   Wu, Y
   Zhang, YC
   Xu, YX
   Shao, J
AF Zou, Wenbin
   Chen, Liang
   Wu, Yi
   Zhang, Yunchen
   Xu, Yuxiang
   Shao, Jun
TI Joint Wavelet Sub-Bands Guided Network for Single Image Super-Resolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Single image super-resolution; convolutional neural network; wavelet
   transform (WT)
ID QUALITY ASSESSMENT; INTERPOLATION; RESOLUTION; TRANSFORM
AB Since deep convolutional neural network (CNN) has achieved excellent results in single image super-resolution (SISR), an increasing number of methods based on CNN have been proposed. Most CNN-based methods are devoted to finding mapping based on pixel intensity while ignoring the importance of frequency information, which can reflect semantic information of images on different bands. This leads to less effectiveness in the reconstruction of high-frequency details. To address this problem, we propose a novel CNN-based super-resolution method named joint wavelet sub-bands guided network (JWSGN). We separate the different frequency information of the image by the WT and then recover this information by a multi-branch network. To recover finer edge details, we propose an edge extraction module, which estimates an edge feature map by using the similarity of all high-frequency sub-bands and then corrects the high-frequency features recovered from each branch by exploiting the edge feature map. Furthermore, we use the complementary relationship between different frequencies to calibrate the high-frequency sub-bands. Finally, the high-resolution image is obtained by inverse wavelet transform. Both qualitative and quantitative experiments show that our method performs excellent performance with the guidance of the edge extraction module.
C1 [Zou, Wenbin; Chen, Liang; Wu, Yi; Zhang, Yunchen; Xu, Yuxiang; Shao, Jun] Fujian Normal Univ, Fujian Prov Engn Technol Res Ctr Photoelect Sensin, Key Lab Optoelect Sci & Technol Med, Fujian Prov Key Lab Photon Technol,Minist Educ, Fuzhou 350117, Peoples R China.
C3 Fujian Normal University
RP Chen, L (corresponding author), Fujian Normal Univ, Fujian Prov Engn Technol Res Ctr Photoelect Sensin, Key Lab Optoelect Sci & Technol Med, Fujian Prov Key Lab Photon Technol,Minist Educ, Fuzhou 350117, Peoples R China.
EM alexzou14@foxmail.com; cl_0827@126.com; wuyi@fjnu.edu.cn;
   cydiachen@cydiachen.tech; yuxiangxu@88.com; shaojun95@126.com
RI 邹, 文斌/HWQ-8314-2023
OI 邹, 文斌/0000-0002-7156-0115
FU National Nature Science Foundation of China [61901117, U1805262,
   61971165]; Natural Science Foundation of Fujian Province [2019J05060,
   2019J01271]; Special Fund for Marine Economic Development of Fujian
   Province [ZHHY-2020-3]; Research program of Fujian Province [2018H6007];
   Special Funds of the Central Government Guiding Local Science and
   Technology Development [2017L3009]; National Key Research and
   Development Program of China [2016YFB1001001]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grants 61901117, U1805262, and 61971165, in
   part by the Natural Science Foundation of Fujian Province under Grants
   2019J05060 and 2019J01271, in part by the Special Fund for Marine
   Economic Development of Fujian Province under Grant ZHHY-2020-3, in part
   by the Research program of Fujian Province under Grant 2018H6007, in
   part by the Special Funds of the Central Government Guiding Local
   Science and Technology Development under Grant 2017L3009, and in part by
   the National Key Research and Development Program of China under Grant
   2016YFB1001001.
CR Aballe A, 1999, ELECTROCHIM ACTA, V44, P4805, DOI 10.1016/S0013-4686(99)00222-4
   Abbate A, 1995, ULTRASON, P751, DOI 10.1109/ULTSYM.1995.495677
   Ahn N, 2018, LECT NOTES COMPUT SC, V11214, P256, DOI 10.1007/978-3-030-01249-6_16
   Akbarzadeh S, 2015, 2015 9TH IRANIAN CONFERENCE ON MACHINE VISION AND IMAGE PROCESSING (MVIP), P111, DOI 10.1109/IranianMVIP.2015.7397516
   Anwar S, 2022, IEEE T PATTERN ANAL, V44, P1192, DOI 10.1109/TPAMI.2020.3021088
   Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135
   Birare S., 2010, Int. J. Eng. Sci. Technol., V2, P7363
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Chavez-Roman H, 2014, IEEE GEOSCI REMOTE S, V11, P1777, DOI 10.1109/LGRS.2014.2308905
   Cheng Ma, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7766, DOI 10.1109/CVPR42600.2020.00779
   Dai T, 2019, PROC CVPR IEEE, P11057, DOI 10.1109/CVPR.2019.01132
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Fang FM, 2020, IEEE T IMAGE PROCESS, V29, P4656, DOI 10.1109/TIP.2020.2973769
   Freedman G, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944852
   Gunturk BK, 2003, IEEE T IMAGE PROCESS, V12, P597, DOI 10.1109/TIP.2003.811513
   Guo TT, 2017, IEEE COMPUT SOC CONF, P1100, DOI 10.1109/CVPRW.2017.148
   Haris M, 2021, IEEE T PATTERN ANAL, V43, P4323, DOI 10.1109/TPAMI.2020.3002836
   He XY, 2019, PROC CVPR IEEE, P1732, DOI 10.1109/CVPR.2019.00183
   Hu J, 2020, IEEE T PATTERN ANAL, V42, P2011, DOI 10.1109/TPAMI.2019.2913372
   Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187
   Huang JB, 2015, PROC CVPR IEEE, P5197, DOI 10.1109/CVPR.2015.7299156
   Huang YF, 2021, IEEE T IMAGE PROCESS, V30, P2325, DOI 10.1109/TIP.2021.3050856
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kumar N, 2017, PATTERN RECOGN LETT, V90, P65, DOI 10.1016/j.patrec.2017.03.014
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lim B, 2017, IEEE COMPUT SOC CONF, P1132, DOI 10.1109/CVPRW.2017.151
   Liu PJ, 2018, IEEE COMPUT SOC CONF, P886, DOI 10.1109/CVPRW.2018.00121
   Liu Y., 2020, IEEE Trans. Multimedia, V24, P2259
   Ma W, 2019, IEEE T GEOSCI REMOTE, V57, P3512, DOI 10.1109/TGRS.2018.2885506
   MALLAT S, 1992, IEEE T INFORM THEORY, V38, P617, DOI 10.1109/18.119727
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Matsui Y, 2017, MULTIMED TOOLS APPL, V76, P21811, DOI 10.1007/s11042-016-4020-z
   Mei YQ, 2021, PROC CVPR IEEE, P3516, DOI 10.1109/CVPR46437.2021.00352
   Michelini P. N., 2018, P EUR C COMP VIS WOR
   Nazeri K, 2019, IEEE INT CONF COMP V, P3275, DOI 10.1109/ICCVW.2019.00409
   Niu B., 2020, EUR C COMP VIS, P191, DOI [10.1007/978-3-030-58610-2_47, DOI 10.1007/978-3-030-58610-2_12]
   Pan JS, 2019, IEEE T PATTERN ANAL, V41, P1412, DOI 10.1109/TPAMI.2018.2832125
   Paszke Adam, 2017, NIPS 2017 WORKSH AUT
   Romano Y, 2014, IEEE T IMAGE PROCESS, V23, P3085, DOI 10.1109/TIP.2014.2325774
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shi WZ, 2013, LECT NOTES COMPUT SC, V8151, P9, DOI 10.1007/978-3-642-40760-4_2
   Sun WJ, 2020, IEEE T IMAGE PROCESS, V29, P4027, DOI 10.1109/TIP.2020.2970248
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Tai Y, 2017, PROC CVPR IEEE, P2790, DOI 10.1109/CVPR.2017.298
   Thornton MW, 2006, INT J REMOTE SENS, V27, P473, DOI 10.1080/01431160500207088
   Tian CW, 2020, NEURAL NETWORKS, V121, P461, DOI 10.1016/j.neunet.2019.08.022
   Timofte R, 2017, IEEE COMPUT SOC CONF, P1110, DOI 10.1109/CVPRW.2017.149
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Wang TW, 2018, IEEE GEOSCI REMOTE S, V15, P769, DOI 10.1109/LGRS.2018.2810893
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wei Z, 2013, IEEE T IMAGE PROCESS, V22, P4271, DOI 10.1109/TIP.2013.2271849
   Xin JW, 2022, IEEE T NEUR NET LEAR, V33, P707, DOI 10.1109/TNNLS.2020.3028688
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2957, DOI 10.1109/TMM.2019.2914883
   Yan YT, 2022, IEEE T MULTIMEDIA, V24, P1473, DOI 10.1109/TMM.2021.3065731
   Yang JC, 2013, PROC CVPR IEEE, P1059, DOI 10.1109/CVPR.2013.141
   Yang JX, 2019, INT GEOSCI REMOTE SE, P2770, DOI [10.1109/igarss.2019.8898813, 10.1109/IGARSS.2019.8898813]
   Yang MC, 2013, IEEE T MULTIMEDIA, V15, P498, DOI 10.1109/TMM.2012.2232646
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhong Z., 2018, Joint Sub-Bands Learning With Clique Structures for Wavelet Domain Super-Resolution, P165
   Zhou S., 2020, P ADV NEUR INF PROC, V33, P3499
   Zhu ZL, 2014, IEEE T MULTIMEDIA, V16, P2178, DOI 10.1109/TMM.2014.2364976
   Zou WWW, 2012, IEEE T IMAGE PROCESS, V21, P327, DOI 10.1109/TIP.2011.2162423
NR 69
TC 7
Z9 7
U1 5
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4623
EP 4637
DI 10.1109/TMM.2022.3179926
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X3FI3
UT WOS:001097340300002
DA 2024-07-18
ER

PT J
AU Cheng, Q
   Shan, HG
   Zhuang, WH
   Yu, L
   Zhang, ZY
   Quek, TQS
AF Cheng, Qi
   Shan, Hangguan
   Zhuang, Weihua
   Yu, Lu
   Zhang, Zhaoyang
   Quek, Tony Q. S.
TI Design and Analysis of MEC- and Proactive Caching-Based 360° Mobile VR
   Video Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 360-degree mobile virtual reality video streaming; computation
   offloading; end-to-end delay; field-of-view prediction; mobile edge
   computing; proactive caching
ID VIRTUAL-REALITY; COOPERATIVE COMMUNICATIONS; LOW-LATENCY; NETWORKS;
   SYSTEMS
AB Recently, 360-degree mobile virtual reality video (MVRV) has become increasingly popular because it can provide users with an immersive experience. However, MVRV is usually recorded in a high resolution and is sensitive to latency, which indicates that broadband, ultra-reliable, and low-latency communication is necessary to guarantee the users' quality of experience. In this paper, we propose a mobile edge computing (MEC)-based 360-degree MVRV streaming scheme with field-of-view (FoV) prediction, which jointly considers video coding, proactive caching, computation offloading, and data transmission. To meet the requirement of stringent end-to-end (E2E) latency, the user's viewpoint prediction is utilized to cache video data proactively, and computing tasks are partially offloaded to the MEC server. In addition, we propose an analytical model based on diffusion process to study the packet transmission process of 360-degree MVRV in multihop wired/wireless networks and analyze the performance of the MEC-enabled scheme. The simulation results verify the accuracy of the analysis and the effectiveness of the proposed MVRV streaming scheme in reducing the E2E delay. Furthermore, the analytical framework sheds some light on the impacts of system parameters, e.g., FoV prediction accuracy and transmission rate, on the balance between computation delay and communication delay.
C1 [Cheng, Qi; Shan, Hangguan; Yu, Lu; Zhang, Zhaoyang] Zhejiang Univ, Zhejiang Prov Key Lab Informat Pressing Commun Ne, Hangzhou 310027, Peoples R China.
   [Cheng, Qi; Shan, Hangguan; Yu, Lu; Zhang, Zhaoyang] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.
   [Cheng, Qi; Shan, Hangguan; Yu, Lu; Zhang, Zhaoyang] Zhejiang Univ, SUTD ZJU IDEA, Hangzhou 310027, Peoples R China.
   [Zhuang, Weihua] Univ Waterloo, Dept Elect & Comp Engn, Waterloo, ON N2L 3G1, Canada.
   [Quek, Tony Q. S.] Singapore Univ Technol & Design, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
C3 Zhejiang University; Zhejiang University; Zhejiang University;
   University of Waterloo; Singapore University of Technology & Design
RP Shan, HG (corresponding author), Zhejiang Univ, Zhejiang Prov Key Lab Informat Pressing Commun Ne, Hangzhou 310027, Peoples R China.; Shan, HG (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou 310027, Peoples R China.; Shan, HG (corresponding author), Zhejiang Univ, SUTD ZJU IDEA, Hangzhou 310027, Peoples R China.
EM 3150104884@zju.edu.cn; hshan@zju.edu.cn; wzhuang@bbcr.uwaterloo.ca;
   yul@zju.edu.cn; ning_ming@zju.edu.cn; tonyquek@sutd.edu.sg
RI Quek, Tony Q. S./A-1578-2016; Zhuang, Weihua/AAH-2576-2020; zhang,
   zihan/JHU-2592-2023; Zhang, Zhaoyang/C-1446-2015
OI Quek, Tony Q. S./0000-0002-4037-3149; Zhuang,
   Weihua/0000-0003-0488-511X; Cheng, Qi/0000-0002-6168-4747; Zhang,
   Zhaoyang/0000-0003-2346-6228
FU National Key Research and Development Program of China [2018YFB1801104];
   National Natural Science Foundation Program of China (NSFC) [61771427];
   Zhejiang Provincial Key Project of Research and Development
   [2021C01119]; SUTD-ZJU IDEA Grant for Visiting Professor [ZJUVP1800104];
   SUTD Growth Plan Grant for AI; Huawei Technologies Company Ltd.
   [YBN2018115223]
FX This work was supported in part by theNationalKey Research and
   Development Program of China under Grant 2018YFB1801104, in part by the
   National Natural Science Foundation Program of China (NSFC) under Grant
   61771427, in part by the Zhejiang Provincial Key Project of Research and
   Development under Grant 2021C01119, in part by the SUTD-ZJU IDEA Grant
   for Visiting Professor ZJUVP1800104, in part by the SUTD Growth Plan
   Grant for AI, and in part by the Huawei Technologies Company Ltd. under
   Grant YBN2018115223. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Abderrahim
   Benslimane.
CR Alameddine HA, 2019, IEEE J SEL AREA COMM, V37, P668, DOI 10.1109/JSAC.2019.2894306
   [Anonymous], 2018, PROC IEEE INT C COMM
   [Anonymous], 2017, P 27 NOSSDAV, DOI DOI 10.1145/3083165.3083180
   [Anonymous], 1977, THEORY STOCHASTIC PR
   [Anonymous], 230091 ISOIEC
   Bazzani L., 2017, INT C LEARN REPR
   Chakareski J, 2017, VR/AR NETWORK '17: PROCEEDINGS OF THE 2017 WORKSHOP ON VIRTUAL REALITY AND AUGMENTED REALITY NETWORK, P36, DOI 10.1145/3097895.3097902
   Chen MZ, 2018, IEEE T COMMUN, V66, P5621, DOI 10.1109/TCOMM.2018.2850303
   Chen YF, 2009, IEEE J SEL AREA COMM, V27, P5, DOI 10.1109/JSAC.2009.090102
   David EJ, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P432, DOI 10.1145/3204949.3208139
   Dodge SF, 2018, IEEE T IMAGE PROCESS, V27, P4080, DOI 10.1109/TIP.2018.2834826
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Duda A., 1983, P ACM SIGMETRICS C M, P118
   Elbamby MS, 2018, IEEE NETWORK, V32, P78, DOI 10.1109/MNET.2018.1700268
   Erol-Kantarci M., 2018, AD HOC NETW
   Ge XH, 2017, IEEE T MULTIMEDIA, V19, P2345, DOI 10.1109/TMM.2017.2733461
   GoPro,, SWIMM WILD DOLPH OC
   Huang TY, 2014, ACM SIGCOMM COMP COM, V44, P187, DOI 10.1145/2740070.2626296
   Huawei iLab,, CLOUD VR SOL WHIT
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kleinrock L., 2006, QUEUEING SYSTEMS COM, V2
   Kummerer M., 2015, INT C LEARN REPR ICL
   LaValle SM, 2014, IEEE INT CONF ROBOT, P187, DOI 10.1109/ICRA.2014.6906608
   Lee K., 2015, ANAL CLOUD GAMING PL, P151
   Louchard G., 1983, PROBABILITY THEORY C
   Luan TH, 2010, IEEE T MULTIMEDIA, V12, P64, DOI 10.1109/TMM.2009.2036294
   Lucas B., 1981, Proc. DARPA Image Understanding Workshop, P121
   Nasrabadi AT, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1689, DOI 10.1145/3123266.3123414
   Nguyen DV, 2019, IEEE J EM SEL TOP C, V9, P29, DOI 10.1109/JETCAS.2019.2899488
   Pang HT, 2019, IEEE INFOCOM SER, P991, DOI 10.1109/INFOCOM.2019.8737395
   Park J, 2018, IEEE WIREL COMMUN LE, V7, P776, DOI 10.1109/LWC.2018.2823761
   Park J, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P447, DOI 10.1145/3343031.3351021
   Qian F., 2016, Proceedings of the 5th Workshop on All Things Cellular: Operations, Applications and Challenges, P1, DOI DOI 10.1145/2980055.2980056
   Ren JK, 2018, IEEE T WIREL COMMUN, V17, P5506, DOI 10.1109/TWC.2018.2845360
   Soret B, 2014, IEEE GLOBE WORK, P1391, DOI 10.1109/GLOCOMW.2014.7063628
   Sukhmani S, 2019, IEEE MULTIMEDIA, V26, P21, DOI 10.1109/MMUL.2018.2879591
   Sun YP, 2019, IEEE T COMMUN, V67, P7573, DOI 10.1109/TCOMM.2019.2920594
   Le TT, 2018, IEEE ACCESS, V6, P66576, DOI 10.1109/ACCESS.2018.2878519
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wong V. W. S., 2017, KEY TECHNOLOGIES 5 G
   Xie L, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P315, DOI 10.1145/3123266.3123291
   Xu M, 2019, IEEE T PATTERN ANAL, V41, P2693, DOI 10.1109/TPAMI.2018.2858783
   Yang J, 2017, IEEE T VEH TECHNOL, V66, P3280, DOI 10.1109/TVT.2016.2587101
   Yang X, 2018, IEEE ACCESS, V6, P16665, DOI 10.1109/ACCESS.2018.2817288
   Yin CQ, 2018, PR IEEE I C PROGR IN, P209, DOI 10.1109/PIC.2018.8706277
   Zhang L., 2015, P IEEE INT S BROADB, P1
   Zhang YX, 2019, IEEE INFOCOM SER, P1252, DOI [10.1109/INFOCOM.2019.8737361, 10.1109/infocom.2019.8737361]
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
   Zink M, 2019, P IEEE, V107, P639, DOI 10.1109/JPROC.2019.2894817
NR 49
TC 35
Z9 35
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1529
EP 1544
DI 10.1109/TMM.2021.3067205
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200022
DA 2024-07-18
ER

PT J
AU Guan, DY
   Huang, JX
   Xiao, AR
   Lu, SJ
   Cao, YP
AF Guan, Dayan
   Huang, Jiaxing
   Xiao, Aoran
   Lu, Shijian
   Cao, Yanpeng
TI Uncertainty-Aware Unsupervised Domain Adaptation in Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Uncertainty; Object detection; Entropy; Feature extraction;
   Detectors; Convolutional neural networks; Unsupervised domain
   adaptation; object detection; adversarial learning; curriculum learning
AB Unsupervised domain adaptive object detection aims to adapt detectors from a labelled source domain to an unlabelled target domain. Most existing works take a two-stage strategy that first generates region proposals and then detects objects of interest, where adversarial learning is widely adopted to mitigate the inter-domain discrepancy in both stages. However, adversarial learning may impair the alignment of well-aligned samples as it merely aligns the global distributions across domains. To address this issue, we design an uncertainty-aware domain adaptation network (UaDAN) that introduces conditional adversarial learning to align well-aligned and poorly-aligned samples separately in different manners. Specifically, we design an uncertainty metric that assesses the alignment of each sample and adjusts the strength of adversarial learning for well-aligned and poorly-aligned samples adaptively. In addition, we exploit the uncertainty metric to achieve curriculum learning that first performs easier image-level alignment and then more difficult instance-level alignment progressively. Extensive experiments over four challenging domain adaptive object detection datasets show that UaDAN achieves superior performance as compared with state-of-the-art methods.
C1 [Guan, Dayan; Huang, Jiaxing; Xiao, Aoran; Lu, Shijian] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Cao, Yanpeng] Zhejiang Univ, Sch Mech Engn, Hangzhou 310027, Peoples R China.
C3 Nanyang Technological University; Zhejiang University
RP Lu, SJ (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM dayan.guan@ntu.edu.sg; jiaxing.huang@ntu.edu.sg; aoran.xiao@ntu.edu.sg;
   Shijian.Lu@ntu.edu.sg; caoyp@zju.edu.cn
RI Lu, Shijian/AAU-4831-2021; Xiao, Aoran/GMW-8725-2022; Guan,
   Dayan/IUM-9090-2023
OI Lu, Shijian/0000-0002-6766-2506; Xiao, Aoran/0000-0002-2956-0613; Guan,
   Dayan/0000-0001-9752-1520
FU Singtel Cognitive and Artificial Intelligence Laboratory for Enterprises
   (SCALE@NTU); Singapore Telecommunications Limited (Singtel); Nanyang
   Technological University (NTU) - Singapore Government through the
   Industry Alignment Fund -Industry Collaboration Projects Grant
FX This work was supported in part by Singtel Cognitive and Artificial
   Intelligence Laboratory for Enterprises (SCALE@NTU), in part by
   Singapore Telecommunications Limited (Singtel), and in part by the
   Nanyang Technological University (NTU) that is funded by the Singapore
   Government through the Industry Alignment Fund -Industry Collaboration
   Projects Grant. The associate editor coordinating the reviewof this
   manuscript and approving it for publication was Prof. Liqiang Nie.
CR [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], Bell System Technical Journal
   Azzam M, 2021, IEEE T MULTIMEDIA, V23, P3318, DOI 10.1109/TMM.2020.3023792
   Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4
   Bengio Y., 2009, P 26 ANN INT C MACH, V60, P6, DOI [DOI 10.1145/1553374.1553380, 10.1145/1553374.1553380]
   Cai Q, 2019, PROC CVPR IEEE, P11449, DOI 10.1109/CVPR.2019.01172
   Cai ZW, 2021, IEEE T PATTERN ANAL, V43, P1483, DOI 10.1109/TPAMI.2019.2956516
   Chang XY, 2020, PATTERN RECOGN, V108, DOI 10.1016/j.patcog.2020.107569
   Chen T, 2018, IEEE T PATTERN ANAL, V40, P2522, DOI 10.1109/TPAMI.2017.2756936
   Chen Y, 2018, PROC CVPR IEEE, P3339, DOI 10.1109/CVPR.2018.00352
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JF, 2016, ADV NEUR IN, V29
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dollár P, 2014, IEEE T PATTERN ANAL, V36, P1532, DOI 10.1109/TPAMI.2014.2300479
   Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5
   Fan Q., 2019, PROC CVPR IEEE
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganlong Zhao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12363), P86, DOI 10.1007/978-3-030-58523-5_6
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gong C, 2016, IEEE T IMAGE PROCESS, V25, P3249, DOI 10.1109/TIP.2016.2563981
   Grandvalet Y., 2005, CAP, V367, P281
   Guan DY, 2021, PATTERN RECOGN, V112, DOI 10.1016/j.patcog.2020.107764
   Guo S, 2018, LECT NOTES COMPUT SC, V11214, P139, DOI 10.1007/978-3-030-01249-6_9
   He K., IEEE I CONF COMP VIS
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Huang J., 2021, PROC IEEECVF C COMPU, P4897
   Huang J., 2021, PROC IEEECVF C COMPU, P4913
   Huang J., 2020, P EUR C COMP VIS, P705
   Huang YG, 2020, PROC CVPR IEEE, P5900, DOI 10.1109/CVPR42600.2020.00594
   Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Johnson-Roberson Matthew, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P746, DOI 10.1109/ICRA.2017.7989092
   Khan Faisal, 2011, ADV NEURAL INFORM PR, V24
   Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45
   Lee C.-H., 2007, Advances in Neural Information Processing Systems, P793
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu YF, 2023, IEEE T PATTERN ANAL, V45, P7035, DOI 10.1109/TPAMI.2020.3001940
   Matiisen T, 2020, IEEE T NEUR NET LEAR, V31, P3732, DOI 10.1109/TNNLS.2019.2934906
   Minghao Xu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12352, DOI 10.1109/CVPR42600.2020.01237
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   Neuhold G, 2017, IEEE I CONF COMP VIS, P5000, DOI 10.1109/ICCV.2017.534
   Nie LQ, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1192, DOI 10.1145/3123266.3123313
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Pang JM, 2019, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2019.00091
   Papageorgiou CP, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P555, DOI 10.1109/ICCV.1998.710772
   Pawan Kumar M., 2010, NIPS
   Pentina A, 2015, PROC CVPR IEEE, P5492, DOI 10.1109/CVPR.2015.7299188
   Qing Yu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P438, DOI 10.1007/978-3-030-58610-2_26
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Saito K, 2019, PROC CVPR IEEE, P6949, DOI 10.1109/CVPR.2019.00712
   Sakaridis C, 2018, INT J COMPUT VISION, V126, P973, DOI 10.1007/s11263-018-1072-8
   Sarafianos N, 2018, PATTERN RECOGN, V80, P94, DOI 10.1016/j.patcog.2018.02.028
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tian Y., 2017, PR MACH LEARN RES
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang ZR, 2019, PROC CVPR IEEE, P11285, DOI 10.1109/CVPR.2019.01155
   Williams C.K.I., PASCAL VISUAL OBJECT
   Xu C.-D., 2020, PROC IEEECVF C COMPU
   Yan HL, 2020, IEEE T MULTIMEDIA, V22, P2420, DOI 10.1109/TMM.2019.2953375
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P64, DOI 10.1109/TMM.2014.2375793
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhan FN, 2019, IEEE I CONF COMP VIS, P9104, DOI 10.1109/ICCV.2019.00920
   Zhang C, 2022, IEEE T MULTIMEDIA, V24, P2246, DOI 10.1109/TMM.2021.3078141
   Zhang DW, 2019, INT J COMPUT VISION, V127, P363, DOI 10.1007/s11263-018-1112-4
   Zhang J., 2021, ARXIV 210317084
   Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223
   Zheng Y., 2020, PROC IEEECVF C COMPU, p13 766
   Zhenwei He, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P309, DOI 10.1007/978-3-030-58586-0_19
   Zhuang CF, 2020, AAAI CONF ARTIF INTE, V34, P13122
NR 81
TC 59
Z9 61
U1 7
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2502
EP 2514
DI 10.1109/TMM.2021.3082687
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Huang, XM
   Zhang, YJ
AF Huang, Xiaoming
   Zhang, Yu-Jin
TI Fast Video Saliency Detection via Maximally Stable Region Motion and
   Object Repeatability
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Saliency detection; salient object detection; maximally stable region;
   motion; object repeatability
ID SEGMENTATION; OPTIMIZATION; CASCADE; FUSION
AB Motion information is one important cue in unsupervised video salient object detection. In order to estimate motion in videos, most of the methods adopt time-consuming algorithms such as large displacement optical flow estimation (needs more than 8-40s with 640X480 size per frame), which leads to saliency detection with only 0.01-0.1 FPS speed and limits its application. In human visual system, the motion of one object is usually considered as a whole. Therefore, we need not compute the motion of each pixel. Instead, it is desirable to estimate the probability of each pixel belonging to a well identifiable object, which is proposed as maximally stable region (MSR) in recent work, and compute object-level motion. Motivated by this intuition, we firstly propose one fast object-level video motion model based on MSR, which only needs 49 ms for 640X480 size frame. Next, we present spatial-temporal boundary connectivity (BndCon) and spatial-temporal Minimum Barrier Distance (MBD) to estimate background probability and saliency. Then, we propose the repeatability saliency which means the frequency of the object recurs in all video sequences. Besides, we propose one simple yet effective method to combine our unsupervised method and deep learning model to further boost performance. Compared with the state-of-the-art unsupervised methods, our method shows significantly better performance with 12 FPS speed on normal CPU hardware.
C1 [Huang, Xiaoming] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100192, Peoples R China.
   [Zhang, Yu-Jin] Tsinghua Univ, Dept Elect Engn, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Beijing Information Science & Technology University; Tsinghua University
RP Huang, XM (corresponding author), Beijing Informat Sci & Technol Univ, Comp Sch, Beijing 100192, Peoples R China.
EM huangxm0556@163.com; zhang-yj@mail.tsinghua.edu.cn
FU National Nature Science Foundation [61171118, 61673234, U1636124];
   Scientific Research Project of Beijing Educational Committee
   [KM202011232014]
FX This work was supported in part by the National Nature Science
   Foundation under Grants 61171118, 61673234, and U1636124, and in part by
   the Scientific Research Project of Beijing Educational Committee under
   Grant KM202011232014.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143
   Chen CLZ, 2021, IEEE T IMAGE PROCESS, V30, P2350, DOI 10.1109/TIP.2021.3052069
   Chen CLZ, 2020, IEEE T IMAGE PROCESS, V29, P1090, DOI 10.1109/TIP.2019.2934350
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Chen CZ, 2017, IEEE T IMAGE PROCESS, V26, P3156, DOI 10.1109/TIP.2017.2670143
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Cong RM, 2019, IEEE T IMAGE PROCESS, V28, P4819, DOI 10.1109/TIP.2019.2910377
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Guo F, 2020, IEEE T CIRC SYST VID, V30, P4887, DOI 10.1109/TCSVT.2019.2906226
   Huang XM, 2020, IEEE T IMAGE PROCESS, V29, P1384, DOI 10.1109/TIP.2019.2941663
   Huang XM, 2018, PATTERN RECOGN, V76, P95, DOI 10.1016/j.patcog.2017.10.027
   Huang XM, 2017, IEEE T IMAGE PROCESS, V26, P4243, DOI 10.1109/TIP.2017.2710636
   Kim H, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2425544
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Li HF, 2019, IEEE I CONF COMP VIS, P7273, DOI 10.1109/ICCV.2019.00737
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li SX, 2018, IEEE T MULTIMEDIA, V20, P155, DOI 10.1109/TMM.2017.2721544
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1153, DOI 10.1109/TMM.2019.2940851
   LIU C., 2009, Ph.D. Thesis
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Ma C, 2017, IEEE T MULTIMEDIA, V19, P2415, DOI 10.1109/TMM.2017.2694219
   Ochs P, 2014, IEEE T PATTERN ANAL, V36, P1187, DOI 10.1109/TPAMI.2013.242
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pan YW, 2017, PROC CVPR IEEE, P984, DOI 10.1109/CVPR.2017.111
   Peng QM, 2019, IEEE T MULTIMEDIA, V21, P3083, DOI 10.1109/TMM.2019.2918730
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Strand R, 2013, COMPUT VIS IMAGE UND, V117, P429, DOI 10.1016/j.cviu.2012.10.011
   Tang Y, 2019, IEEE T CIRC SYST VID, V29, P1973, DOI 10.1109/TCSVT.2018.2859773
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wang XH, 2021, IEEE T IMAGE PROCESS, V30, P458, DOI 10.1109/TIP.2020.3037470
   Wei YC, 2012, LECT NOTES COMPUT SC, V7574, P29, DOI 10.1007/978-3-642-33712-3_3
   Xi T, 2017, IEEE T IMAGE PROCESS, V26, P3425, DOI 10.1109/TIP.2016.2631900
   Xu MZ, 2020, IEEE T CIRC SYST VID, V30, P2191, DOI 10.1109/TCSVT.2019.2920652
   Xu MZ, 2019, IEEE T MULTIMEDIA, V21, P2790, DOI 10.1109/TMM.2019.2914889
   Yan PX, 2019, IEEE I CONF COMP VIS, P7283, DOI 10.1109/ICCV.2019.00738
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 52
TC 4
Z9 4
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4458
EP 4470
DI 10.1109/TMM.2021.3094356
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400001
DA 2024-07-18
ER

PT J
AU Huang, Y
   Yang, XS
   Gao, JY
   Xu, CS
AF Huang, Yi
   Yang, Xiaoshan
   Gao, Junyun
   Xu, Changsheng
TI Holographic Feature Learning of Egocentric-Exocentric Videos for
   Multi-Domain Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Feature extraction; Visualization; Task analysis; Computational
   modeling; Target recognition; Prototypes; Egocentric videos; exocentric
   videos; holographic feature; multi-domain; action recognition
ID DOMAIN ADAPTATION; NETWORKS
AB Though existing cross-domain action recognition methods successfully improve the performance on videos of one view (e.g., egocentric videos) by transferring the knowledge from videos of another view (e.g., exocentric videos), they have limitations in generality because the source and target domains need to be fixed aforehand. In this paper, we propose to solve a more practical task of multi-domain action recognition on egocentric-exocentric videos, which aims to learn a single model to recognize test videos from either egocentric perspective or exocentric perspective by transferring knowledge between two domains. Though previous cross-domain methods can also transfer knowledge from one domain to another one by learning view-invariant representations of two video domains, they are not suitable for the multi-domain action recognition task because they always suffer from the problem of losing view-specific visual information. As a solution to the multi-domain action recognition task, we propose to map a video from either egocentric perspective or exocentric perspective to a global feature space (we call it holographic feature space) that shares both view-invariant and view-specific visual knowledge of two views. Specially, we decompose the video feature into view-invariant component and view-specific component, where view-specific component is written into memory networks for saving view-specific visual knowledge. The final holographic feature combines view-invariant feature and view-specific features of two views based on the memory networks. We demonstrate the effectiveness of the proposed method with extensive experimental results on two public datasets. Moreover, the good performances under the semi-supervised setting show the generality of our model.
C1 [Huang, Yi; Yang, Xiaoshan; Gao, Junyun; Xu, Changsheng] Chinese Acad Sci, Inst Automat, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Huang, Yi; Yang, Xiaoshan; Gao, Junyun; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Huang, Yi; Yang, Xiaoshan; Gao, Junyun; Xu, Changsheng] Peng Cheng Lab, Shenzhen 518066, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Peng Cheng Laboratory
RP Xu, CS (corresponding author), Chinese Acad Sci, Inst Automat, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Xu, CS (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
EM yi.huang@nlpr.ia.ac.cn; xiaoshan.yang@nlpr.ia.ac.cn; junyu.gao@ia.ac.cn;
   csxu@nlpr.ia.ac.cn
RI xu, cj/HJZ-3488-2023; Xu, Chang/GQP-7280-2022
OI Yang, Xiaoshan/0000-0001-5453-9755; xu, chang sheng/0000-0001-8343-9665
FU National Key Research and Development Program of China [2018AAA0100604];
   National Natural Science Foundation of China [61720106006, 61721004,
   62072455, U1836220, U1705262, 61872424]; Key Research Program of
   Frontier Sciences of CAS [QYZDJ-SSW-JSC039]; Beijing Natural Science
   Foundation [L201001]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0100604, in part by the
   National Natural Science Foundation of China under Grants 61720106006,
   61721004, 62072455, U1836220, U1705262, and 61872424, in part by Key
   Research Program of Frontier Sciences of CAS under Grant
   QYZDJ-SSW-JSC039, and in part by Beijing Natural Science Foundation
   under Grant L201001. The associate editor coordinating the review of
   this manuscript, and approving it for publication was Dr. F. Sohel.
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   [Anonymous], 2015, PROC AAAI C ARTIF IN
   Ardeshir S, 2018, COMPUT VIS IMAGE UND, V171, P61, DOI 10.1016/j.cviu.2018.05.005
   Ba J, 2016, ADV NEUR IN, V29
   Baradel F, 2018, LECT NOTES COMPUT SC, V11217, P106, DOI 10.1007/978-3-030-01261-8_7
   Bousmalis G., 2016, Advances in Neural Information Processing Systems, V29, P1
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Cai M., 2016, PROC C ROB SCI SYST, V3, P311
   Cao KD, 2018, PROC CVPR IEEE, P5187, DOI 10.1109/CVPR.2018.00544
   Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542
   Chen MH, 2019, IEEE I CONF COMP VIS, P6330, DOI 10.1109/ICCV.2019.00642
   Damen D, 2018, LECT NOTES COMPUT SC, V11208, P753, DOI 10.1007/978-3-030-01225-0_44
   Daumé H, 2006, J ARTIF INTELL RES, V26, P101, DOI 10.1613/jair.1872
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ganin Y., 2015, ICML
   Ganin Y, 2016, J MACH LEARN RES, V17
   Gao JY, 2019, AAAI CONF ARTIF INTE, P8303
   Ghosh S, 2020, IEEE T CIRC SYST VID, V30, P2015, DOI 10.1109/TCSVT.2019.2916589
   Ghosh S, 2018, IEEE SIGNAL PROC LET, V25, P1555, DOI 10.1109/LSP.2018.2866949
   Gong B., 2013, P INT C MACH LEARN, P222
   Graves A., 2014, ARXIV 14105401
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang Y, 2019, IEEE I CONF COMP VIS, P5773, DOI 10.1109/ICCV.2019.00587
   Joshi M., 2012, P 2012 JOINT C EMPIR, P1302
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kumano S, 2017, IEEE T MULTIMEDIA, V19, P107, DOI 10.1109/TMM.2016.2608002
   Lapin M, 2018, IEEE T PATTERN ANAL, V40, P1533, DOI 10.1109/TPAMI.2017.2751607
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Li Y, 2015, PROC CVPR IEEE, P287, DOI 10.1109/CVPR.2015.7298625
   Liu WY, 2016, PR MACH LEARN RES, V48
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Lv F, 2006, LECT NOTES COMPUT SC, V3954, P359
   Ma C, 2018, PROC CVPR IEEE, P6975, DOI 10.1109/CVPR.2018.00729
   Mansour Y, 2009, LECT NOTES ARTIF INT, V5809, P4, DOI 10.1007/978-3-642-04414-4_4
   Meagher M, 2007, IEEE INT CONF INF VI, P601
   Munkhdalai Tsendsuren, 2017, Proc Mach Learn Res, V70, P2554
   Myeongjin Kim, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12972, DOI 10.1109/CVPR42600.2020.01299
   Pei WJ, 2019, PROC CVPR IEEE, P8339, DOI 10.1109/CVPR.2019.00854
   Perrett T, 2019, PROC CVPR IEEE, P7844, DOI 10.1109/CVPR.2019.00804
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Santoro A, 2016, PR MACH LEARN RES, V48
   Schoenauer-Sebag A., 2019, PROC INT C LEARN REP
   Sigurdsson GA, 2018, PROC CVPR IEEE, P7396, DOI 10.1109/CVPR.2018.00772
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Snell J, 2017, ADV NEUR IN, V30
   Soran B, 2015, LECT NOTES COMPUT SC, V9007, P178, DOI 10.1007/978-3-319-16814-2_12
   Su Z, 2018, PROC CVPR IEEE, P7736, DOI 10.1109/CVPR.2018.00807
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Thurau C., 2008, CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587721
   Varini P, 2017, IEEE T MULTIMEDIA, V19, P2832, DOI 10.1109/TMM.2017.2705915
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang H, 2019, PROC CVPR IEEE, P3522, DOI 10.1109/CVPR.2019.00364
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang JB, 2018, PROC CVPR IEEE, P7512, DOI 10.1109/CVPR.2018.00784
   Weinland D, 2006, COMPUT VIS IMAGE UND, V104, P249, DOI 10.1016/j.cviu.2006.07.013
   Weston J., 2015, PROC INT C LEARN REP
   Xiong CM, 2016, PR MACH LEARN RES, V48
   Xu TT, 2016, IMAGE VISION COMPUT, V55, P127, DOI 10.1016/j.imavis.2016.01.001
   Yang Y., 2015, PROC INT C LEARN REP, P1
   Yu HY, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1358, DOI 10.1145/3343031.3350896
   Yu TZ, 2019, IEEE T MULTIMEDIA, V21, P2504, DOI 10.1109/TMM.2019.2907060
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang YF, 2018, IEEE T MULTIMEDIA, V20, P1038, DOI 10.1109/TMM.2018.2808769
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
NR 69
TC 4
Z9 4
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2273
EP 2286
DI 10.1109/TMM.2021.3078882
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600005
DA 2024-07-18
ER

PT J
AU Li, H
   Wei, P
   Hu, P
AF Li, Huan
   Wei, Ping
   Hu, Ping
TI AVN: An Adversarial Variation Network Model for Handwritten Signature
   Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Handwritten signature; variation consistency; adversarial enhancement;
   neural network
ID CLASSIFIER; DISTANCE
AB Handwritten signature verification is a crucial yet challenging problem. While previous studies have made great progress in this problem, they learn signature features passively from given existing data. In this paper, we propose a novel adversarial variation network (AVN) model for handwritten signature verification which mines effective features by actively varying existing data and generating new data. Powered by a proposed novel variation consistency mechanism, the AVN contains three different types of modules unified under one end-to-end framework: the extractor seeks to extract deep discriminative features of handwritten signatures, the discriminator aims to make verification decisions based on the extracted features, and the variator is designed to actively generate signature variants for constructing a more discriminative model. The proposed model is trained in an adversarial way with a min-max loss function, by which the three modules cooperate and compete to enhance the entire model's ability and therefore the signature verification performance is improved. We test the proposed method on four challenging signature datasets of different languages: CEDAR, BHSig-Hindi, BHSig-Bengali, and GPDS Synthetic Signature. Extensive experiments with in-depth discussions validate the effectiveness of the proposed method.
C1 [Li, Huan; Wei, Ping] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China.
   [Hu, Ping] Xi An Jiao Tong Univ, Sch Management, Xian, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University
RP Wei, P (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China.
EM lh875056558@stu.xjtu.edu.cn; pingwei@xjtu.edu.cn; helenhu@xjtu.edu.cn
FU National Natural Science Foundation of China [61876149]; China
   Postdoctoral Science Foundation [2018M643657]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61876149 and in part by China
   Postdoctoral Science Foundation under Grant 2018M643657.
CR Alaei A, 2017, IEEE T INF FOREN SEC, V12, P2360, DOI 10.1109/TIFS.2017.2707332
   Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   [Anonymous], 2010, P 1 INT C INTELLIGEN, DOI DOI 10.1145/1963564.1963610
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Baltzakis H, 2001, ENG APPL ARTIF INTEL, V14, P95, DOI 10.1016/S0952-1976(00)00064-6
   Bharathi RK, 2013, 2013 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P2063, DOI 10.1109/ICACCI.2013.6637499
   Bhunia AK, 2019, NEURAL COMPUT APPL, V31, P8737, DOI 10.1007/s00521-019-04220-x
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Buslaev A, 2020, INFORMATION, V11, DOI 10.3390/info11020125
   Chen SY, 2006, INT C PATT RECOG, P869
   Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202
   Chuang Li, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P166, DOI 10.1109/ICDAR.2019.00035
   Das S. D., 2019, ICDSMLA 2019 P 1 INT
   Dey S., 2017, Pattern Recognition Letters
   Dutta A, 2016, INT C PATT RECOG, P3422, DOI 10.1109/ICPR.2016.7900163
   El-Yacoubi A, 2000, NEURAL NETWORKS FOR SIGNAL PROCESSING X, VOLS 1 AND 2, PROCEEDINGS, P859, DOI 10.1109/NNSP.2000.890166
   Engin D, 2020, IEEE COMPUT SOC CONF, P3518, DOI 10.1109/CVPRW50498.2020.00412
   Ferrer MA, 2015, IEEE T PATTERN ANAL, V37, P667, DOI 10.1109/TPAMI.2014.2343981
   Ferrer MA, 2012, IEEE T INF FOREN SEC, V7, P966, DOI 10.1109/TIFS.2012.2190281
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guerbai Y, 2015, PATTERN RECOGN, V48, P103, DOI 10.1016/j.patcog.2014.07.016
   Gumusbas D., 2019, P IEEE INT S INN INT, DOI [DOI 10.1109/INISTA.2019.8778228, 10.1109/INISTA.2019.8778228]
   Hafemann LG, 2020, IEEE T INF FOREN SEC, V15, P1735, DOI 10.1109/TIFS.2019.2949425
   Hafemann LG, 2019, IEEE T INF FOREN SEC, V14, P2153, DOI 10.1109/TIFS.2019.2894031
   Hafemann LG, 2017, PATTERN RECOGN, V70, P163, DOI 10.1016/j.patcog.2017.05.012
   Hafemann LG, 2016, IEEE IJCNN, P2576, DOI 10.1109/IJCNN.2016.7727521
   Hamadene A, 2016, IEEE T INF FOREN SEC, V11, P1226, DOI 10.1109/TIFS.2016.2521611
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Kalera MK, 2004, INT J PATTERN RECOGN, V18, P1339, DOI 10.1142/S0218001404003630
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuang D., 2019, P SIMUL SYNTH MED IM
   Kumar R, 2012, PATTERN RECOGN LETT, V33, P301, DOI 10.1016/j.patrec.2011.10.009
   Lai SX, 2018, INT CONF FRONT HAND, P175, DOI 10.1109/ICFHR-2018.2018.00039
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Liu Y, 2017, IEEE I CONF COMP VIS, P2943, DOI 10.1109/ICCV.2017.318
   Liu Y, 2019, PATTERN RECOGN, V93, P365, DOI 10.1016/j.patcog.2019.05.008
   Luo Y., 2020, P NEURAL INF PROCESS
   Maergner P, 2019, PATTERN RECOGN LETT, V125, P527, DOI 10.1016/j.patrec.2019.06.024
   Mirza Mehdi, 2014, COMPUTER SCI
   Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pal S, 2016, PROCEEDINGS OF 12TH IAPR WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, (DAS 2016), P72, DOI 10.1109/DAS.2016.48
   Peng X, 2018, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2018.00237
   Radford A., 2015, ARXIV
   Ren S, 2015, FASTER R CNN REAL TI, P91
   Rezaei N. N. Mohammad, 2017, P NAT C NEW RES EL C
   Ruiz V, 2020, NEUROCOMPUTING, V374, P30, DOI 10.1016/j.neucom.2019.09.041
   Ruiz-Del-Solar J, 2008, LECT NOTES COMPUT SC, V5197, P22, DOI 10.1007/978-3-540-85920-8_3
   Serdouk Y., 2015, P INT C TEL, P75
   Serdouk Y, 2017, IMAGE VISION COMPUT, V66, P26, DOI 10.1016/j.imavis.2017.08.004
   Serdouk Y, 2014, 10TH INTERNATIONAL CONFERENCE ON SIGNAL-IMAGE TECHNOLOGY AND INTERNET-BASED SYSTEMS SITIS 2014, P84, DOI 10.1109/SITIS.2014.36
   Shariatmadari S, 2019, INT J DOC ANAL RECOG, V22, P375, DOI 10.1007/s10032-019-00331-2
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Tiao L. C., 2018, P INT C MACH LEARN W
   Wei P, 2019, PROC CVPR IEEE, P5757, DOI 10.1109/CVPR.2019.00591
   Yi Y, 2020, IEEE T MULTIMEDIA, V22, P2454, DOI 10.1109/TMM.2019.2955300
   Yilmaz MB, 2020, ADV INTELL SYST COMP, V1070, P417, DOI 10.1007/978-3-030-32523-7_29
   Yilmaz MB, 2018, IEEE COMPUT SOC CONF, P639, DOI 10.1109/CVPRW.2018.00094
   Yilmaz MB, 2016, INFORM FUSION, V32, P109, DOI 10.1016/j.inffus.2016.02.003
   Yuan L, 2020, IEEE T MULTIMEDIA, V22, P2711, DOI 10.1109/TMM.2019.2959451
   Yuan MK, 2020, IEEE T MULTIMEDIA, V22, P1955, DOI 10.1109/TMM.2019.2951463
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang H., 2018, P INT C MACH LEARN
   Zhang ZH, 2016, INT SYM COMPUT INTEL, P103, DOI [10.1109/ISCID.2016.2033, 10.1109/ISCID.2016.138]
   Zhao A, 2019, PROC CVPR IEEE, P8535, DOI 10.1109/CVPR.2019.00874
   Zhao FQ, 2019, LECT NOTES COMPUT SC, V11767, P475, DOI 10.1007/978-3-030-32251-9_52
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zois Elias N., 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13245, DOI 10.1109/CVPR42600.2020.01326
   Zois EN, 2019, EXPERT SYST APPL, V125, P14, DOI 10.1016/j.eswa.2019.01.058
   Zois EN, 2018, IEEE COMPUT SOC CONF, P545, DOI 10.1109/CVPRW.2018.00084
NR 74
TC 13
Z9 13
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 594
EP 608
DI 10.1109/TMM.2021.3056217
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100007
OA hybrid
DA 2024-07-18
ER

PT J
AU Nie, WZ
   Chang, RH
   Ren, MJ
   Su, YT
   Liu, AA
AF Nie, Weizhi
   Chang, Rihao
   Ren, Minjie
   Su, Yuting
   Liu, Anan
TI I-GCN: Incremental Graph Convolution Network for Conversation Emotion
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Emotion detection; Sentiment analysis; Incremental
   GCN; GCN
AB Sentiment analysis and emotion detection in conversation are becoming hot topics in regard to several applications. With the development of the social robot, social network, and intelligent voice assistant, emotion detection is attracting more attention as a key component in these research fields. Many approaches have been proposed to handle this problem in recent years. However, these previous approaches focus on either the temporal change information of the conversation or the semantic correlation information of the dialogue but ignore the combination of temporal information and semantic correlation information. In this paper, we propose an incremental graph convolution network (I-GCN) to handle emotion detection in conversation. We first utilize the graph structure to represent conversation at different times, which can represent the semantic correlation information of utterances. Then, we apply the incremental graph structure to imitate the process of dynamic conversation, which can preserve the temporal change information of conversation. Especially, for the first step of the process, we creatively propose utterance-level GCN (U-GCN) and speaker-level GCN (S-GCN) to learn the features of utterances for emotion detection. U-GCN focuses on the correlations among utterances and applies the multi-head attention model to find latent correlation information among utterances, which aims to further enhance the guidance of semantic relevance for feature learning. S-GCN focuses on the correlation between speaker and utterances, which can provide a different angle to guide the feature learning of utterances. In the learning of model parameters, we constantly utilize the new utterances to fine-tune the parameters of GNN for enhancement of the contribution of temporal change information. Detailed evaluations of the proposed method on three published conversation corpuses demonstrate the great effectiveness of our approach over several conventional competitive baselines.
C1 [Nie, Weizhi; Chang, Rihao; Ren, Minjie; Su, Yuting; Liu, Anan] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University
RP Ren, MJ; Liu, AA (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM weizhinie@tju.edu.cn; kevinc@tju.edu.cn; renminjie@tju.edu.cn;
   ytsu@tju.edu.cn; anan0422@gmail.com
RI lu, lala/GQQ-3784-2022
OI lu, lala/0000-0002-6080-8074; nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61702471,
   61772359]; 2019 Tianjin New Generation Artificial Intelligence Major
   Program [18ZXZNGX00150, 19ZXZNGX00110]; Open Project Program of the
   State Key Laboratory of CAD & CG, Zhejiang University [A2005, A2012];
   Tianjin Science Foundation for Young Scientists of China
   [19JC-QNJC00500]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872267, 61702471, and 61772359, in
   part by the grant of 2019 Tianjin New Generation Artificial Intelligence
   Major Program under Grants 18ZXZNGX00150 and 19ZXZNGX00110, in part by
   the Open Project Program of the State Key Laboratory of CAD & CG,
   Zhejiang University under Grants A2005 and A2012, and in part by the
   Tianjin Science Foundation for Young Scientists of China under Grant
   19JC-QNJC00500.
CR Bruna J., 2014, ABS13126203 CORR, P1, DOI [10.48550/arXiv.1312.6203, DOI 10.48550/ARXIV.1312.6203]
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Chen SY, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1597
   Choi YJ, 2021, BIG DATA-US, V9, P279, DOI 10.1089/big.2020.0274
   Cui H, 2020, IEEE T IMAGE PROCESS, V29, P1271, DOI 10.1109/TIP.2019.2940693
   Ghosal D, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2470
   Ghosal D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P154
   Han YH, 2020, IEEE T CIRC SYST VID, V30, P875, DOI 10.1109/TCSVT.2019.2897604
   Han YH, 2015, IEEE T IMAGE PROCESS, V24, P5114, DOI 10.1109/TIP.2015.2479917
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang XD, 2021, IEEE MULTIMEDIA, V28, P76, DOI 10.1109/MMUL.2021.3065678
   Huang ZH, 2015, Arxiv, DOI arXiv:1508.01991
   Ishiwatari T, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P7360
   Jiang P., 2020, PROC AAAI C ARTIF IN, V34, p11 109
   Jiao WX, 2020, AAAI CONF ARTIF INTE, V34, P8002
   Khare SK, 2021, IEEE T NEUR NET LEAR, V32, P2901, DOI 10.1109/TNNLS.2020.3008938
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Kipf TN, 2017, INT C LEARN REPR
   Li, 2017, P 8 INT JOINT C NAT, V1, P986
   Li C, 2020, INFORM PROCESS MANAG, V57, DOI 10.1016/j.ipm.2019.102185
   Li W, 2021, Arxiv, DOI arXiv:2006.00492
   Lian Z, 2021, IEEE-ACM T AUDIO SPE, V29, P985, DOI 10.1109/TASLP.2021.3049898
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Manessi F, 2020, PATTERN RECOGN, V97, DOI 10.1016/j.patcog.2019.107000
   Mao YZ, 2020, Arxiv, DOI [arXiv:2010.07637, DOI 10.48550/ARXIV.2010.07637]
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Pareja A, 2020, AAAI CONF ARTIF INTE, V34, P5363
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Vaswani A, 2017, ADV NEUR IN, V30
   Battaglia PW, 2018, Arxiv, DOI [arXiv:1806.01261, DOI 10.48550/ARXIV.1806.01261, 10.48550/arXiv.1806.01261]
   Wang KX, 2020, NEUROCOMPUTING, V398, P257, DOI 10.1016/j.neucom.2020.02.085
   Wang Q., 2020, PROC AAAI C ARTIF IN, V34, p12 200
   Weston J, 2015, Arxiv, DOI arXiv:1410.3916
   Wu F, 2019, PR MACH LEARN RES, V97
   Xing SL, 2022, IEEE T AFFECT COMPUT, V13, P1426, DOI 10.1109/TAFFC.2020.3005660
   Xu D., 2020, PROC 8 INT C LEARN R
   Zadeh A., 2017, P 2017 C EMPIRICAL M, P1114
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhu L, 2020, ACM T INTEL SYST TEC, V11, DOI 10.1145/3365841
   Zhu L, 2020, IEEE T IMAGE PROCESS, V29, P4643, DOI 10.1109/TIP.2020.2974065
NR 44
TC 14
Z9 14
U1 15
U2 43
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4471
EP 4481
DI 10.1109/TMM.2021.3118881
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 7B6NT
UT WOS:000899248400002
DA 2024-07-18
ER

PT J
AU Ren, MJ
   Huang, XD
   Li, WH
   Song, D
   Nie, WZ
AF Ren, Minjie
   Huang, Xiangdong
   Li, Wenhui
   Song, Dan
   Nie, Weizhi
TI LR-GCN: Latent Relation-Aware Graph Convolutional Network for
   Conversational Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Correlation; Emotion recognition; Task analysis; Context modeling;
   Computer architecture; Transformers; Social networking (online); Emotion
   recognition in conversations; multi-head attention; graph convolutional
   network
AB As an intersection of artificial intelligence and human communication analysis, Emotion Recognition in Conversation (ERC) has attracted much research attention in recent years. Existing studies, however, are limited in adequately exploiting latent relations among the constituent utterances. In this paper, we address this issue by proposing a novel approach named Latent Relation-Aware Graph Convolutional Network (LR-GCN), where both speaker dependency of the interlocutors is leveraged and latent correlations among the utterances are captured for ERC. Specifically, we first establish a graph model to incorporate the context information and speaker dependency of the conversation. Afterward, the multi-head attention mechanism is introduced to explore the latent correlations among the utterances and generate a set of all-linked graphs. Here, aiming to simultaneously exploit the original modeled speaker dependency and the explored correlation information, we introduce a dense connection layer to capture more structural information of the generated graphs. Through a multi-branch graph network, we achieve a unified representation of each utterance for final prediction. Detailed evaluations on two benchmark datasets demonstrate LR-GCN outperforms the state-of-the-art approaches.
C1 [Ren, Minjie; Huang, Xiangdong; Li, Wenhui; Song, Dan; Nie, Weizhi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Ren, Minjie] Hefei Comprehens Natl Sci Ctr, Inst Artificial Intelligence, Hefei, Peoples R China.
C3 Tianjin University
RP Li, WH (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
EM renminjie@tju.edu.cn; xdhuang@tju.edu.cn; liwenhui@tju.edu.cn;
   dan.song@tju.edu.cn; weizhinie@tju.edu.cn
RI LU, lpp pp/JFJ-9011-2023; Zeng, Yun/JFK-6190-2023; wu,
   meng/JPK-1930-2023; Lu, Wang/JVO-0416-2024; LI, Wenhui/JCD-9947-2023;
   lu, lala/GQQ-3784-2022
OI lu, lala/0000-0002-6080-8074; nie, weizhi/0000-0002-0578-8138
FU National Natural Science Foundation of China [61872267, 61702471,
   61772359, 19ZXZNGX00110]; Open Project Program of the State Key Lab of
   CAD & CG, Zhejiang University [A2005, A2012]; Tianjin Science Foundation
   for Young Scientists of China [19JCQNJC00500]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61872267, 61702471, and 61772359, in
   part by the grant of 2019 Tianjin New Generation Artificial Intelligence
   Major Program under Grants 19ZXZNGX00110, in part by the Open Project
   Program of the State Key Lab of CAD & CG, Zhejiang University under
   Grants A2005 and A2012, and in part by the Tianjin Science Foundation
   for Young Scientists of China under Grant 19JCQNJC00500. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Liqiang Nie.
CR Bradbury James, 2017, INT C LEARN REPR
   Bruna J., 2014, 2 INT C LEARN REPRES
   Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6
   Cambria E, 2017, IEEE INTELL SYST, V32, P74, DOI 10.1109/MIS.2017.4531228
   Chen SY, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1597
   Chung J, 2014, P NIPS 2014 WORKSHOP
   Ghosal D, 2020, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, EMNLP 2020, P2470
   Ghosal D, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P154
   Guo ZJ, 2019, T ASSOC COMPUT LING, V7, P297, DOI 10.1162/tacl_a_00269
   Hazarika D., EMOTION RECOGNITION
   Hazarika D, 2018, 2018 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018), P2594
   Hazarika Devamanyu, 2018, Proc Conf, V2018, P2122, DOI 10.18653/v1/n18-1193
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Jiao WX, 2020, AAAI CONF ARTIF INTE, V34, P8002
   Kim Y, 2014, ARXIV PREPRINT ARXIV, DOI 10.3115/v1/D14-1181
   Kipf Thomas N., 2017, 5 INT C LEARN REPRES
   Li CC, 2020, IEEE T MULTIMEDIA, V22, P1634, DOI 10.1109/TMM.2019.2946477
   Li W, 2021, Arxiv, DOI arXiv:2006.00492
   Lian Z, 2021, IEEE-ACM T AUDIO SPE, V29, P985, DOI 10.1109/TASLP.2021.3049898
   Lin ZJ, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P121
   Liu YH, 2019, Arxiv, DOI arXiv:1907.11692
   Ma H, 2021, NEURAL COMPUT APPL, V33, P2685, DOI 10.1007/s00521-020-05063-7
   Majumder N, 2019, AAAI CONF ARTIF INTE, P6818
   Mao YZ, 2020, Arxiv, DOI [arXiv:2010.07637, DOI 10.48550/ARXIV.2010.07637]
   Nguyen D, 2022, IEEE T MULTIMEDIA, V24, P1313, DOI 10.1109/TMM.2021.3063612
   Nie WZ, 2021, IEEE T MULTIMEDIA, V23, P3793, DOI 10.1109/TMM.2020.3032037
   Picard RW, 2010, IEEE T AFFECT COMPUT, V1, P11, DOI 10.1109/T-AFFC.2010.10
   Poria S, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P527
   Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081
   Rashkin H, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5370
   Richards JM, 2004, CURR DIR PSYCHOL SCI, V13, P131, DOI 10.1111/j.0963-7214.2004.00291.x
   Runnan Li, 2019, ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Proceedings, P6675, DOI 10.1109/ICASSP.2019.8682154
   Ruusuvuori J., 2012, The handbook of conversation analysis, P330, DOI [DOI 10.1002/9781118325001.CH16, DOI 10.1002/9781118325001]
   Schlichtkrull Michael, 2018, PROC EUR SEMANTIC WE, P593
   Sukhbaatar S, 2015, ADV NEUR IN, V28
   Vaswani A, 2017, ADV NEUR IN, V30
   Veličkovic P, 2018, Arxiv, DOI arXiv:1710.10903
   Battaglia PW, 2018, Arxiv, DOI [arXiv:1806.01261, DOI 10.48550/ARXIV.1806.01261, 10.48550/arXiv.1806.01261]
   Wang JZ, 2020, COMPUT VIS IMAGE UND, V192, DOI 10.1016/j.cviu.2019.102898
   Xing SL, 2022, IEEE T AFFECT COMPUT, V13, P1426, DOI 10.1109/TAFFC.2020.3005660
   Yang X., 2020, IEEE T MULTIMEDIA, P1
   Young T, 2018, AAAI CONF ARTIF INTE, P4970
   Zadeh A, 2018, AAAI CONF ARTIF INTE, P5634
   Zhang D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5415
NR 44
TC 12
Z9 12
U1 2
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4422
EP 4432
DI 10.1109/TMM.2021.3117062
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000004
DA 2024-07-18
ER

PT J
AU Thuseethan, S
   Rajasegarar, S
   Yearwood, J
AF Thuseethan, Selvarajah
   Rajasegarar, Sutharshan
   Yearwood, John
TI Deep Continual Learning for Emerging Emotion Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion recognition; Task analysis; Feature extraction; Learning
   systems; Transfer learning; Training; Databases; Continual learning;
   emotion recognition; deep convolutional neural network; knowledge
   distillation; unknown emotions
ID FACIAL EXPRESSION
AB Understanding an unknown facial emotion that emerges in the future underpins significant impacts in various domains. Knowing the fact that emotional states grow in vocabulary, new emotional states need to be adapted while the existing knowledge of known emotional states is preserved. While human beings spontaneously perform this task, the challenge is, how to devise a deep learning technique that can effectively recognize an unknown emotion category in the future. Although the deep convolutional neural network has shown excellent emotion recognition performances in the past, it is conventionally a predefined multi-way classifier showing little resilience towards adding a new emotion class. Considering the aforementioned challenge, in this paper, we propose a generic deep convolutional neural network-based architecture that constantly absorbs the upcoming emotion categories and recognizes them effectively. We further propose an indicator loss, which is associated with the distillation mechanism that preserves the existing knowledge. In order to demonstrate the feasibility of our proposed method, we evaluated our model using benchmark emotion datasets. The results confirm that the proposed approach is superior in recognizing unknown emotional states compared to continual learning benchmarks. Further, our proposed method demonstrates higher accuracy, compared to the transfer learning baselines.
C1 [Thuseethan, Selvarajah; Rajasegarar, Sutharshan; Yearwood, John] Deakin Univ, Sch Informat Technol, Geelong, Vic 3220, Australia.
   [Thuseethan, Selvarajah] Sabaragamuwa Univ Sri Lanka, Dept Comp & Informat Syst, Belihuloya 70140, Sri Lanka.
C3 Deakin University; Sabaragamuwa University of Sri Lanka
RP Thuseethan, S (corresponding author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3220, Australia.
EM tselvarajah@deakin.edu.au; srajas@deakin.edu.au;
   john.yearwood@deakin.edu.au
RI Yearwood, John/HPB-5213-2023
OI Rajasegarar, Sutharshan/0000-0002-6559-6736; Selvarajah,
   Thuseethan/0000-0001-6378-9940
CR [Anonymous], 2015, EXPRESSION EMOTIONS
   Chen Z., 2016, SUSTAINABILITY-BASEL, V10, P1, DOI [10.2200/S00737ED1V01Y201610AIM033, DOI 10.3390/SU10103378]
   Davison AK, 2018, IEEE T AFFECT COMPUT, V9, P116, DOI 10.1109/TAFFC.2016.2573832
   Diethe T, 2019, Arxiv, DOI arXiv:1903.05202
   Douillard Arthur, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P86, DOI 10.1007/978-3-030-58565-5_6
   Du SC, 2014, P NATL ACAD SCI USA, V111, pE1454, DOI 10.1073/pnas.1322355111
   Duran J.I., 2017, The Science of Facial Expression
   EKMAN P, 1993, AM PSYCHOL, V48, P384, DOI 10.1037/0003-066X.48.4.384
   Fernández-Dols JM, 2013, EMOT REV, V5, P24, DOI 10.1177/1754073912457229
   Fujii K, 2021, IEEE T MULTIMEDIA, V23, P3892, DOI 10.1109/TMM.2020.3033125
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Gorbova J, 2019, MULTIMED TOOLS APPL, V78, P23161, DOI 10.1007/s11042-019-7658-5
   Hasani B, 2017, IEEE COMPUT SOC CONF, P2278, DOI 10.1109/CVPRW.2017.282
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hayes TL, 2020, IEEE COMPUT SOC CONF, P887, DOI 10.1109/CVPRW50498.2020.00118
   Hayes Tyler L, 2020, P EUR C COMP VIS ECC
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hou SH, 2019, PROC CVPR IEEE, P831, DOI 10.1109/CVPR.2019.00092
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jain RakeshKumar A., 2019, CVPR WORKSH, P12
   Jaiswal S, 2016, IEEE WINT CONF APPL
   Kaya H, 2017, IMAGE VISION COMPUT, V65, P66, DOI 10.1016/j.imavis.2017.01.012
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Kosti R, 2017, PROC CVPR IEEE, P1960, DOI 10.1109/CVPR.2017.212
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecoutre A., 2017, ASIAN C MACHINE LEAR, P327
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Liu YY, 2021, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR46437.2021.00257
   Long M., 2017, ADV NEUR IN, V30, P1
   Lopes AT, 2017, PATTERN RECOGN, V61, P610, DOI 10.1016/j.patcog.2016.07.026
   Lucey P., 2010, IEEE COMP SOC C COMP, P94, DOI 10.1109/CVPRW.2010.5543262
   Maltoni D, 2019, NEURAL NETWORKS, V116, P56, DOI 10.1016/j.neunet.2019.03.010
   Fernandez PDM, 2019, Arxiv, DOI arXiv:1902.03284
   Mavani V, 2017, IEEE INT CONF COMP V, P2783, DOI 10.1109/ICCVW.2017.327
   MEHRABIAN A, 1995, GENET SOC GEN PSYCH, V121, P339
   Mici L, 2018, NEUROCOMPUTING, V307, P14, DOI 10.1016/j.neucom.2018.04.015
   Mollahosseini A, 2016, IEEE WINT CONF APPL
   Mollahosseini A, 2019, IEEE T AFFECT COMPUT, V10, P18, DOI 10.1109/TAFFC.2017.2740923
   National Institute of Mental Health, 2016, 19MH8079 NIH
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Roy D, 2020, NEURAL NETWORKS, V121, P148, DOI 10.1016/j.neunet.2019.09.010
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Rusu AA, 2018, Arxiv, DOI arXiv:1610.04286
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Siqueira H, 2020, AAAI CONF ARTIF INTE, V34, P5800
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Thuseethan S., 2019, IEEE IJCNN, P1
   Thuseethan S, 2020, IEEE ACCESS, V8, P147711, DOI 10.1109/ACCESS.2020.3015917
   Thuseethan S, 2019, LECT NOTES COMPUT SC, V11955, P449, DOI 10.1007/978-3-030-36718-3_38
   Thuseethan S, 2019, LECT NOTES ARTIF INT, V11441, P387, DOI 10.1007/978-3-030-16142-2_30
   Vo TH, 2020, IEEE ACCESS, V8, P131988, DOI 10.1109/ACCESS.2020.3010018
   Wang K, 2020, PROC CVPR IEEE, P6896, DOI 10.1109/CVPR42600.2020.00693
   Wellman HM., 1992, The childs theory of mind
   Xiaoyu Tao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12180, DOI 10.1109/CVPR42600.2020.01220
   Zhang HM, 2021, IEEE T MULTIMEDIA, V23, P2033, DOI 10.1109/TMM.2020.3007352
NR 56
TC 11
Z9 11
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 4367
EP 4380
DI 10.1109/TMM.2021.3116434
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 6A4RT
UT WOS:000880645000001
DA 2024-07-18
ER

PT J
AU Yu, LT
   Zhang, J
   Wu, Q
AF Yu, Litao
   Zhang, Jian
   Wu, Qiang
TI Dual Attention on Pyramid Feature Maps for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Decoding; Task analysis; Semantics; Feature extraction;
   Two dimensional displays; Context modeling; Image captioning; dual
   attention; pyramid attention
AB Generating natural sentences from images is a fundamental learning task for visual-semantic understanding in multimedia. In this paper, we propose to apply dual attention on pyramid image feature maps to fully explore the visual-semantic correlations and improve the quality of generated sentences. Specifically, with the full consideration of the contextual information provided by the hidden state of the RNN controller, the pyramid attention can better localize the visually indicative and semantically consistent regions in images. On the other hand, the contextual information can help re-calibrate the importance of feature components by learning the channel-wise dependencies, to improve the discriminative power of visual features for better content description. We conducted comprehensive experiments on three well-known datasets: Flickr8K, Flickr30 K and MS COCO, which achieved impressive results in generating descriptive and smooth natural sentences from images. Using either convolution visual features or more informative bottom-up attention features, the composite model can boost the performance of image-to-sentence translation, with a limited computational resource overhead. The proposed pyramid attention and dual attention methods are highly modular, which can be inserted into various image captioning modules to further improve the performance.
C1 [Yu, Litao; Zhang, Jian; Wu, Qiang] Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
C3 University of Technology Sydney
RP Zhang, J (corresponding author), Univ Technol Sydney, Global Big Data Technol Ctr, Ultimo, NSW 2007, Australia.
EM litao.yu@uts.edu.au; jian.zhang@uts.edu.au; qiang.wu@uts.edu.au
OI Zhang, Jian/0000-0002-7240-3541; Wu, Qiang/0000-0001-5641-2483
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   Aneja J, 2018, PROC CVPR IEEE, P5561, DOI 10.1109/CVPR.2018.00583
   [Anonymous], 2016, P 24 ACM INT C MULT
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Bin Y, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1345, DOI 10.1145/3123266.3123391
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cheng Y., 2019, Joint Training for Neural Machine Translation, P11, DOI DOI 10.1007/978-981-32-9748-7_2
   Cornia M, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3177745
   Cui Y, 2018, PROC CVPR IEEE, P5804, DOI 10.1109/CVPR.2018.00608
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gao LL, 2020, IEEE T PATTERN ANAL, V42, P1112, DOI 10.1109/TPAMI.2019.2894139
   Gao LL, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1742, DOI 10.1145/3240508.3240687
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Guo LT, 2020, IEEE T MULTIMEDIA, V22, P2149, DOI 10.1109/TMM.2019.2951226
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang L, 2019, IEEE I CONF COMP VIS, P4633, DOI 10.1109/ICCV.2019.00473
   Huang L, 2019, ADV NEUR IN, V32
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Li H., 2018, P IEEE BRIT MACH VIS
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Li XP, 2019, AAAI CONF ARTIF INTE, P8658
   Li XY, 2019, IEEE T MULTIMEDIA, V21, P2117, DOI 10.1109/TMM.2019.2896516
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Loshchilov I., 2019, ARXIV171105101PROC
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Luo RT, 2018, PROC CVPR IEEE, P6964, DOI 10.1109/CVPR.2018.00728
   Mai L, 2017, P CVPR, P4718
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Qin Y, 2019, PROC CVPR IEEE, P8359, DOI 10.1109/CVPR.2019.00856
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Shen T, 2018, AAAI CONF ARTIF INTE, P5446
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun DJ, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, COMMUNICATIONS AND COMPUTING (ICSPCC)
   Vaswani A, 2017, ADV NEUR IN, V30
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wu J., 2020, IEEE T MULTIMEDIA
   Xu J, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P537, DOI 10.1145/3123266.3123448
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu WR, 2021, IEEE T MULTIMEDIA, V23, P1772, DOI 10.1109/TMM.2020.3002669
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yao T, 2019, IEEE I CONF COMP VIS, P2621, DOI 10.1109/ICCV.2019.00271
   Yao T, 2018, LECT NOTES COMPUT SC, V11218, P711, DOI 10.1007/978-3-030-01264-9_42
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, Transactions of the Association for Computational Linguistics, V2, P67
   Zhang B, 2020, IEEE T PATTERN ANAL, V42, P154, DOI 10.1109/TPAMI.2018.2876404
   Zhang H, 2022, IEEE COMPUT SOC CONF, P2735, DOI 10.1109/CVPRW56347.2022.00309
   Zhang ZJ, 2019, IEEE T MULTIMEDIA, V21, P1681, DOI 10.1109/TMM.2018.2888822
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 59
TC 27
Z9 27
U1 4
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1775
EP 1786
DI 10.1109/TMM.2021.3072479
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200003
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Chen, FY
   Shao, J
   Zhang, YH
   Xu, X
   Shen, HT
AF Chen, Feiyu
   Shao, Jie
   Zhang, Yonghui
   Xu, Xing
   Shen, Heng Tao
TI Interclass-Relativity-Adaptive Metric Learning for Cross-Modal Matching
   and Beyond
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Loss measurement; Task analysis; Benchmark testing; Image retrieval;
   Sampling methods; Semantics; Cross-modal matching; metric learning;
   image retrieval; sample weighting; interclass relativity
ID SHOT IMAGE
AB Training under supervision of triplet ranking loss is a dominant methodology for cross-modal matching models, while good-performing losses in this domain are immensely under-explored since the majority of advanced metric losses are inapplicable due to the particularity of cross-modal setting. Current prominent approaches of metric learning have developed various weighting schemes that assign weights to separate positive or negative samples. It is the interclass relative order in a triplet, however, that matters. In this work, we propose a new Interclass-Relativity-Adaptive (IRA) loss that assigns weights to the relative similarities between positive and negative pairs instead of separate pairs, which allows us to regard a whole triplet as a weighable entity and achieve maximum utilization of sole positive under cross-modal setting. Our method outperforms the baselines by a large margin and obtains competitive results on two video-text matching benchmarks and two image-text matching benchmarks. We also further extend our method to two unimodal image retrieval benchmarks to test its generality and achieve new state-of-the-art results.
C1 [Chen, Feiyu; Shao, Jie; Zhang, Yonghui; Xu, Xing; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Shao, Jie; Shen, Heng Tao] Sichuan Artificial Intelligence Res Inst, Yibin 644000, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Shao, J (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM chenfeiyu@std.uestc.edu.cn; shaojie@uestc.edu.cn;
   zyoohv@std.uestc.edu.cn; xing.xu@uestc.edu.cn; shenhengtao@hotmail.com
RI zhang, ying/HJB-1230-2022; z, y/HPC-0477-2023; Zhang,
   Yonghui/AGY-9072-2022; Shen, Heng Tao/ABD-5331-2021
FU National Natural Science Foundation of China [61832001, 61672133,
   61632007]; Sichuan Science and Technology Program [2019YFG0535]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61832001, 61672133, and 61632007, and
   in part by Sichuan Science and Technology Program 2019YFG0535.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2015, P 3 INT C LEARN REPR
   [Anonymous], 2006, PROC IEEE COMPUT SOC
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Cakir F, 2019, PROC CVPR IEEE, P1861, DOI 10.1109/CVPR.2019.00196
   Chen BH, 2019, AAAI CONF ARTIF INTE, P8134
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Dong JF, 2019, PROC CVPR IEEE, P9338, DOI 10.1109/CVPR.2019.00957
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Faghri F., 2018, P BMVC
   Ge WF, 2018, LECT NOTES COMPUT SC, V11210, P272, DOI 10.1007/978-3-030-01231-1_17
   Gu JX, 2018, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR.2018.00750
   Harwood B, 2017, IEEE I CONF COMP VIS, P2840, DOI 10.1109/ICCV.2017.307
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   Huang X, 2017, IEEE INT CON MULTI, P943, DOI 10.1109/ICME.2017.8019340
   Huang Y, 2019, AAAI CONF ARTIF INTE, P8489
   Huang Y, 2018, PROC CVPR IEEE, P6163, DOI 10.1109/CVPR.2018.00645
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kim W, 2018, LECT NOTES COMPUT SC, V11205, P760, DOI 10.1007/978-3-030-01246-5_45
   Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83
   Lee KH, 2018, LECT NOTES COMPUT SC, V11208, P212, DOI 10.1007/978-3-030-01225-0_13
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Yang, 2019, BMVC
   Mithun NC, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P19, DOI 10.1145/3206025.3206064
   Musgrave K, 2020, ARXIV PREPRINT ARXIV
   Peng YX, 2018, IEEE T MULTIMEDIA, V20, P405, DOI 10.1109/TMM.2017.2742704
   Pini S, 2019, MULTIMED TOOLS APPL, V78, P14007, DOI 10.1007/s11042-018-7040-z
   Roth K, 2019, IEEE I CONF COMP VIS, P7999, DOI 10.1109/ICCV.2019.00809
   Sanderson M, 2010, NAT LANG ENG, V16, P100, DOI 10.1017/S1351324909005129
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Shizhe Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10635, DOI 10.1109/CVPR42600.2020.01065
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sohn K, 2016, ADV NEUR IN, V29
   Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI [10.1109/ICCV.2017.283, 10.1109/ICCV.2017.65]
   Wang LW, 2019, IEEE T PATTERN ANAL, V41, P394, DOI 10.1109/TPAMI.2018.2797921
   Wang XS, 2019, PROC CVPR IEEE, P5202, DOI 10.1109/CVPR.2019.00535
   Wang XS, 2019, AAAI CONF ARTIF INTE, P5361
   Wang X, 2019, PROC CVPR IEEE, P5017, DOI 10.1109/CVPR.2019.00516
   Wu CY, 2017, IEEE I CONF COMP VIS, P2859, DOI 10.1109/ICCV.2017.309
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Xu X, 2017, IEEE T IMAGE PROCESS, V26, P2494, DOI 10.1109/TIP.2017.2676345
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Yu R, 2018, LECT NOTES COMPUT SC, V11220, P196, DOI 10.1007/978-3-030-01270-0_12
   Zhang BW, 2018, LECT NOTES COMPUT SC, V11217, P385, DOI 10.1007/978-3-030-01261-8_23
   Zheng WZ, 2019, PROC CVPR IEEE, P72, DOI 10.1109/CVPR.2019.00016
NR 53
TC 13
Z9 13
U1 2
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3073
EP 3084
DI 10.1109/TMM.2020.3019710
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000010
DA 2024-07-18
ER

PT J
AU Jiang, YC
   Leung, FHF
AF Jiang, Yuechi
   Leung, Frank H. F.
TI Vector-Based Feature Representations for Speech Signals: From
   Supervector to Latent Vector
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Acoustics; Probabilistic logic; Computational modeling; Adaptation
   models; Computational efficiency; Task analysis; Speaker recognition;
   Acoustic and speech signal processing; gaussian supervector; i-vector;
   supervector and latent vector; vector-based feature representation
ID SPEAKER RECOGNITION; ACOUSTIC SCENES; MIXTURE; CLASSIFICATION;
   ADAPTATION; MACHINES; DISTANCE; KERNEL
AB There are two basic types of feature representations for speech signals. The first type refers to probabilistic models, such as the Gaussian mixture model (GMM). The second type refers to vector-based feature representations, such as the Gaussian supervector (GSV). Since vector-based feature representations are easier to use and process, they are more popular than probabilistic model-based feature representations. In this paper, we begin by explaining the rationale behind two widely used vector-based feature representations, viz. GSV and the i-vector, and then make extensions. GSV is a supervector (SV) based on maximum a posteriori (MAP) adaptation. Its computation is simple and fast, but its dimensionality is high and fixed. While the i-vector is a latent vector (LV) based on factor analysis (FA). Although the computation can be time-consuming because of additional model parameters, its dimensionality is changeable. To generalize GSV, we propose the MAP SV, which is also based on MAP adaptation but can have an even higher dimensionality and thus carry more information. To boost the computational efficiency of the i-vector, we adopt the concept of the mixture of factor analyzers (MFA) and propose the MFA LV, which exhibits a similar flexibility in dimensionality but is faster in computation. The experimental results for speaker identification and verification tasks demonstrate that, MAP SV can be more robust than GSV, and MFALV is comparable to or even better than the i-vector in effectiveness and meanwhile maintains a higher computational efficiency. With a powerful backend, GSV and MAP SV are comparable to the i-vector and MFALV, but the latter two are more flexible in dimensionality.
C1 [Jiang, Yuechi; Leung, Frank H. F.] Hong Kong Polytech Univ, Dept Elect & Informat Engn, Kowloon, Hong Kong 999077, Peoples R China.
C3 Hong Kong Polytechnic University
RP Jiang, YC (corresponding author), Hong Kong Polytech Univ, Dept Elect & Informat Engn, Kowloon, Hong Kong 999077, Peoples R China.
EM yuechi.jiang@connect.polyu.hk; frank-h-f.leung@polyu.edu.hk
RI Leung, F.H.F./AAQ-1956-2021
OI Leung, F.H.F./0000-0003-3921-7074; Jiang, Yuechi/0000-0002-1715-2540
FU Hong Kong Polytechnic University [RUG7]
FX This work was supported by a grant from The Hong Kong Polytechnic
   University (Project Account Code: RUG7).
CR [Anonymous], 2001, SPOKEN LANGUAGE PROC
   Bahari MH, 2014, IEEE-ACM T AUDIO SPE, V22, P1117, DOI 10.1109/TASLP.2014.2319159
   Bishop C., 2006, PATTERN RECOGN, V1st, P559
   Campbell WM, 2014, INTERSPEECH, P676
   Campbell WM, 2006, IEEE SIGNAL PROC LET, V13, P308, DOI 10.1109/LSP.2006.870086
   Chandrakala S, 2020, IEEE T MULTIMEDIA, V22, P3, DOI 10.1109/TMM.2019.2925956
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Cumani S, 2017, INT CONF ACOUST SPEE, P5435, DOI 10.1109/ICASSP.2017.7953195
   Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307
   Do MN, 2003, IEEE SIGNAL PROC LET, V10, P115, DOI 10.1109/LSP.2003.809034
   Ferrer L, 2019, J MACH LEARN RES, V20
   Garcia-Romero D, 2010, INT CONF ACOUST SPEE, P1806, DOI 10.1109/ICASSP.2010.5495407
   Ghahramani Z., 1996, Tech. Rep.
   Hansen JHL, 2015, IEEE SIGNAL PROC MAG, V32, P74, DOI 10.1109/MSP.2015.2462851
   Hasan T, 2013, IEEE T AUDIO SPEECH, V21, P842, DOI 10.1109/TASL.2012.2226161
   Inoue N, 2012, IEEE T MULTIMEDIA, V14, P1196, DOI 10.1109/TMM.2012.2191395
   Istiyanto, 2017, TELKOMNIKA, V15, P1976
   Jiang Y, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P1678
   Jiang Y, 2018, POLYMERS-BASEL, V10, DOI 10.3390/polym10111237
   Jiang YC, 2019, IEEE T INF FOREN SEC, V14, P2875, DOI 10.1109/TIFS.2019.2911175
   Jing LP, 2017, IEEE T MULTIMEDIA, V19, P2637, DOI 10.1109/TMM.2017.2703939
   Kenny P, 2005, IEEE T SPEECH AUDI P, V13, P345, DOI 10.1109/TSA.2004.840940
   KingLine Data Center, 2013, AM ENGL SPEECH REC C
   Kinnunen T., 2018, P OD 2018 SPEAK LANG, P357
   Kinnunen T, 2010, SPEECH COMMUN, V52, P12, DOI 10.1016/j.specom.2009.08.009
   Li P, 2012, IEEE T PATTERN ANAL, V34, P144, DOI 10.1109/TPAMI.2011.104
   Li YX, 2020, IEEE T MULTIMEDIA, V22, P1385, DOI 10.1109/TMM.2019.2947199
   Li YX, 2018, IEEE T INF FOREN SEC, V13, P965, DOI 10.1109/TIFS.2017.2774505
   Liu LQ, 2016, IEEE T MULTIMEDIA, V18, P64, DOI 10.1109/TMM.2015.2500730
   Ma B, 2012, P OD, P338
   Mesaros A, 2018, IEEE-ACM T AUDIO SPE, V26, P379, DOI 10.1109/TASLP.2017.2778423
   Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950
   Ortega-Garcia J, 2000, SPEECH COMMUN, V31, P255, DOI 10.1016/S0167-6393(99)00081-3
   Perronnin F, 2007, PROC CVPR IEEE, P2272
   Perveen N, 2018, IEEE T IMAGE PROCESS, V27, P5575, DOI 10.1109/TIP.2018.2856373
   REYNOLDS DA, 1995, IEEE T SPEECH AUDI P, V3, P72, DOI 10.1109/89.365379
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Sell G, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3031, DOI 10.1109/ICASSP.2018.8462122
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Snyder D, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5329
   Snyder D, 2017, INTERSPEECH, P999, DOI 10.21437/Interspeech.2017-620
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Tan P. N., 2016, INTRO DATA MINING
   Variani Ehsan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4052, DOI 10.1109/ICASSP.2014.6854363
   Vestman V, 2019, INT CONF ACOUST SPEE, P5781, DOI 10.1109/ICASSP.2019.8683272
   Xu LT, 2018, IEEE-ACM T AUDIO SPE, V26, P749, DOI 10.1109/TASLP.2018.2793670
   Yamamoto H, 2005, IEICE T INF SYST, VE88D, P418, DOI 10.1093/ietisy/e88-d.3.418
   You CH, 2009, IEEE SIGNAL PROC LET, V16, P49, DOI 10.1109/LSP.2008.2006711
   Yun Lei, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1695, DOI 10.1109/ICASSP.2014.6853887
NR 50
TC 0
Z9 0
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2641
EP 2655
DI 10.1109/TMM.2020.3014559
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600008
DA 2024-07-18
ER

PT J
AU Lu, Y
   Stathopoulou, T
   Vasiloglou, MF
   Christodoulidis, S
   Stanga, Z
   Mougiakakou, S
AF Lu, Ya
   Stathopoulou, Thomai
   Vasiloglou, Maria F.
   Christodoulidis, Stergios
   Stanga, Zeno
   Mougiakakou, Stavroula
TI An Artificial Intelligence-Based System to Assess Nutrient Intake for
   Hospitalised Patients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Databases; Estimation; Image segmentation; Artificial intelligence;
   Hospitals; Training data; Visualization; Artificial Intelligence;
   nutrient intake assessment; few-shot learning
ID FOOD RECOGNITION
AB Regular monitoring of nutrient intake in hospitalised patients plays a critical role in reducing the risk of disease-related malnutrition. Although several methods to estimate nutrient intake have been developed, there is still a clear demand for a more reliable and fully automated technique, as this could improve data accuracy and reduce both the burden on participants and health costs. In this paper, we propose a novel system based on artificial intelligence (AI) to accurately estimate nutrient intake, by simply processing RGB Depth (RGB-D) image pairs captured before and after meal consumption. The system includes a novel multi-task contextual network for food segmentation, a few-shot learning-based classifier built by limited training samples for food recognition, and an algorithm for 3D surface construction. This allows sequential food segmentation, recognition, and estimation of the consumed food volume, permitting fully automatic estimation of the nutrient intake for each meal. For the development and evaluation of the system, a dedicated new database containing images and nutrient recipes of 322 meals is assembled, coupled to data annotation using innovative strategies. Experimental results demonstrate that the estimated nutrient intake is highly correlated (>0.91) to the ground truth and shows very small mean relative errors (<20%), outperforming existing techniques proposed for nutrient intake assessment.
C1 [Lu, Ya; Stathopoulou, Thomai; Vasiloglou, Maria F.; Christodoulidis, Stergios; Mougiakakou, Stavroula] Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3008 Bern, Switzerland.
   [Stanga, Zeno] Univ Hosp, Inselspital, Univ Hosp Diabetol Endocrinol Nutr Med & Metab UD, CH-3010 Bern, Switzerland.
C3 University of Bern; University of Bern; University Hospital of Bern
RP Mougiakakou, S (corresponding author), Univ Bern, ARTORG Ctr Biomed Engn Res, CH-3008 Bern, Switzerland.
EM ya.lu@artorg.unibe.ch; thomai.stathopoulou@artorg.unibe.ch;
   maria.vasiloglou@artorg.unibe.ch;
   stestergios.christodoulidis@artorg.unibe.ch; zeno.stanga@insel.ch;
   stavroula.mougiakakou@artorg.unibe.ch
RI Vasiloglou, Maria/IAN-3045-2023
OI Christodoulidis, Stergios/0000-0002-8773-1070; Mougiakakou,
   Stavroula/0000-0002-6355-9982; Vasiloglou, Maria/0000-0002-2013-2858
FU SV Foundation
FX This work was supported by SV Foundation.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Aguilar E, 2018, IEEE T MULTIMEDIA, V20, P3266, DOI 10.1109/TMM.2018.2831627
   Aguilar E, 2017, LECT NOTES COMPUT SC, V10485, P213, DOI 10.1007/978-3-319-68548-9_20
   Allegra D, 2017, LECT NOTES COMPUT SC, V10590, P471, DOI 10.1007/978-3-319-70742-6_46
   [Anonymous], 2013, IEEE 13 INT C BIOINF
   [Anonymous], PED SURG DIAGN MAN
   Anthimopoulos MM, 2014, IEEE J BIOMED HEALTH, V18, P1261, DOI 10.1109/JBHI.2014.2308928
   Antoniou A., 2018, ARXIV PREPRINT ARXIV
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Barker LA, 2011, INT J ENV RES PUB HE, V8, P514, DOI 10.3390/ijerph8020514
   Benesty J., 2009, NOISE REDUCTION SPEE, P1, DOI [10.1007/978-3-642-00296-0_5, DOI 10.1007/978-3-642-00296-05]
   Bossard L, 2014, LECT NOTES COMPUT SC, V8694, P446, DOI 10.1007/978-3-319-10599-4_29
   Chen M, 2009, IEEE IMAGE PROC, P289, DOI 10.1109/ICIP.2009.5413511
   Ciocca G, 2017, IEEE J BIOMED HEALTH, V21, P588, DOI 10.1109/JBHI.2016.2636441
   Dehais J, 2017, IEEE T MULTIMEDIA, V19, P1090, DOI 10.1109/TMM.2016.2642792
   Dehais J, 2016, MADIMA'16: PROCEEDINGS OF THE 2ND INTERNATIONAL WORKSHOP ON MULTIMEDIA ASSISTED DIETARY MANAGEMENT, P23, DOI 10.1145/2986035.2986047
   Ege T, 2019, 2019 2ND IEEE CONFERENCE ON MULTIMEDIA INFORMATION PROCESSING AND RETRIEVAL (MIPR 2019), P274, DOI 10.1109/MIPR.2019.00056
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Giavarina D, 2015, BIOCHEM MEDICA, V25, P141, DOI 10.11613/BM.2015.015
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Horiguchi S, 2018, IEEE T MULTIMEDIA, V20, P2836, DOI 10.1109/TMM.2018.2814339
   Imoberdorf R, 2010, CLIN NUTR, V29, P38, DOI 10.1016/j.clnu.2009.06.005
   Kandiah J, 2006, J AM DIET ASSOC, V106, P1663, DOI 10.1016/j.jada.2006.07.015
   Kaur P., 2019, P IEEE C COMP VIS PA
   Kawano Y, 2015, LECT NOTES COMPUT SC, V8927, P3, DOI 10.1007/978-3-319-16199-0_1
   Kawasaki Y, 2016, CLIN NUTR, V35, P1543, DOI 10.1016/j.clnu.2016.04.006
   KRAHENBUHL P, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuanar S, 2019, IEEE IMAGE PROC, P1351, DOI [10.1109/ICIP.2019.8803037, 10.1109/icip.2019.8803037]
   Kuanar S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2576, DOI 10.1109/ICASSP.2018.8462243
   Lo FPW, 2018, NUTRIENTS, V10, DOI 10.3390/nu10122005
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu Y., 2018, P JOINT WORKSH MULT, P152
   Lu Y, 2019, IEEE ENG MED BIO, P5696, DOI [10.1109/embc.2019.8856889, 10.1109/EMBC.2019.8856889]
   Marin J, 2021, IEEE T PATTERN ANAL, V43, P187, DOI 10.1109/TPAMI.2019.2927476
   Martin CK, 2014, J HUM NUTR DIET, V27, P72, DOI 10.1111/jhn.12014
   Martinel N, 2018, IEEE WINT CONF APPL, P567, DOI 10.1109/WACV.2018.00068
   Monacelli F, 2017, J NUTR HEALTH AGING, V21, P614, DOI 10.1007/s12603-016-0814-y
   Myers A, 2015, IEEE I CONF COMP VIS, P1233, DOI 10.1109/ICCV.2015.146
   Ofei KT, 2019, PUBLIC HEALTH NUTR, V22, P1203, DOI 10.1017/S1368980018001064
   Pirlich M, 2006, CLIN NUTR, V25, P563, DOI 10.1016/j.clnu.2006.03.005
   Rhyner D, 2016, J MED INTERNET RES, V18, DOI 10.2196/jmir.5567
   Sandfort V, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-52737-x
   Sauer AC, 2019, JPEN-PARENTER ENTER, V43, P918, DOI 10.1002/jpen.1499
   Shroff G, 2008, TWELFTH IEEE INTERNATIONAL SYMPOSIUM ON WEARABLE COMPUTERS, PROCEEDINGS, P119, DOI 10.1109/ISWC.2008.4911602
   Snell J., 2017, ADV NEURAL INFORM PR, V30, P4077
   Sullivan SC, 2016, NUTRIENTS, V8, DOI 10.3390/nu8070412
   Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131
   Tan CQ, 2018, LECT NOTES COMPUT SC, V11141, P270, DOI 10.1007/978-3-030-01424-7_27
   Vasiloglou MF, 2018, NUTRIENTS, V10, DOI 10.3390/nu10060741
   Villalon L, 2011, CAN J DIET PRACT RES, V72, P162, DOI 10.3148/72.4.2011.162
   Vinyals O., 2016, ADV NEURAL INFORM PR, P3630, DOI DOI 10.48550/ARXIV.1606.04080
   Williams P., 2011, e-SPEN, the European e-Journal of Clinical Nutrition and Metabolism, V6, pe235
   Yu F., 2017, PROC CVPR IEEE, P472, DOI [DOI 10.1109/CVPR.2017.75, 10.1109/CVPR.2017.75]
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
NR 56
TC 30
Z9 32
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1136
EP 1147
DI 10.1109/TMM.2020.2993948
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shermin, T
   Lu, GJ
   Teng, SW
   Murshed, M
   Sohel, F
AF Shermin, Tasfia
   Lu, Guojun
   Teng, Shyh Wei
   Murshed, Manzur
   Sohel, Ferdous
TI Adversarial Network With Multiple Classifiers for Open Set Domain
   Adaptation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptation models; Training; Loss measurement; Generators; Computational
   modeling; Data models; Task analysis; Open set domain adaptation;
   adversarial domain networks; multi-classifier based weighting module
AB Domain adaptation aims to transfer knowledge from a domain with adequate labeled samples to a domain with scarce labeled samples. Prior research has introduced various open set domain adaptation settings in the literature to extend the applications of domain adaptation methods in real-world scenarios. This paper focuses on the type of open set domain adaptation setting where the target domain has both private ('unknown classes') label space and the shared ('known classes') label space. However, the source domain only has the 'known classes' label space. Prevalent distribution-matching domain adaptation methods are inadequate in such a setting that demands adaptation from a smaller source domain to a larger and diverse target domain with more classes. For addressing this specific open set domain adaptation setting, prior research introduces a domain adversarial model that uses a fixed threshold for distinguishing known from unknown target samples and lacks at handling negative transfers. We extend their adversarial model and propose a novel adversarial domain adaptation model with multiple auxiliary classifiers. The proposed multi-classifier structure introduces a weighting module that evaluates distinctive domain characteristics for assigning the target samples with weights which are more representative to whether they are likely to belong to the known and unknown classes to encourage positive transfers during adversarial training and simultaneously reduces the domain gap between the shared classes of the source and target domains. A thorough experimental investigation shows that our proposed method outperforms existing domain adaptation methods on a number of domain adaptation datasets.
C1 [Shermin, Tasfia; Lu, Guojun; Teng, Shyh Wei; Murshed, Manzur] Federat Univ Australia, Sch Engn Informat Technol & Phys Sci, Churchill, Vic 3842, Australia.
   [Sohel, Ferdous] Murdoch Univ, Murdoch, WA 6150, Australia.
C3 Federation University Australia; Murdoch University
RP Shermin, T (corresponding author), Federat Univ Australia, Sch Engn Informat Technol & Phys Sci, Churchill, Vic 3842, Australia.
EM t.shermin@federation.edu.au; guojun.lu@federation.edu.au;
   shyh.wei.teng@federation.edu.au; manzur.murshed@federation.edu.au;
   f.sohel@murdoch.edu.au
RI Sohel, Ferdous/C-2428-2013; Shermin, Tasfia/AAD-4048-2021
OI Sohel, Ferdous/0000-0003-1557-4907; Murshed, Manzur/0000-0001-7079-9717;
   Lu, Guojun/0000-0003-2523-7576; Shermin, Tasfia/0000-0001-9999-5383
FU Federation University Australia
FX The work of Tasfia Shermin was supported by the PhD scholarship by
   Federation University Australia.
CR Bendale A, 2016, PROC CVPR IEEE, P1563, DOI 10.1109/CVPR.2016.173
   Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18
   Busto PP, 2017, IEEE I CONF COMP VIS, P754, DOI 10.1109/ICCV.2017.88
   Cao ZJ, 2018, LECT NOTES COMPUT SC, V11212, P139, DOI 10.1007/978-3-030-01237-3_9
   Cao ZJ, 2019, PROC CVPR IEEE, P2980, DOI 10.1109/CVPR.2019.00310
   Cao ZJ, 2018, PROC CVPR IEEE, P2724, DOI 10.1109/CVPR.2018.00288
   Carlucci FM, 2017, IEEE I CONF COMP VIS, P5077, DOI 10.1109/ICCV.2017.542
   Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Ge Zongyuan, 2017, BMVC
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Haeusser P, 2017, IEEE I CONF COMP VIS, P2784, DOI 10.1109/ICCV.2017.301
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Hoffman J, 2017, ADV COMPUT VIS PATT, P173, DOI 10.1007/978-3-319-58347-1_9
   Huang SW, 2018, LECT NOTES COMPUT SC, V11213, P731, DOI 10.1007/978-3-030-01240-3_44
   Jain LP, 2014, LECT NOTES COMPUT SC, V8691, P393, DOI 10.1007/978-3-319-10578-9_26
   Li JJ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P747, DOI 10.1145/3343031.3350902
   Li JJ, 2019, IEEE T IMAGE PROCESS, V28, P6103, DOI 10.1109/TIP.2019.2924174
   Li JJ, 2019, IEEE T NEUR NET LEAR, V30, P1381, DOI 10.1109/TNNLS.2018.2868854
   Li JJ, 2019, IEEE T CYBERNETICS, V49, P2144, DOI 10.1109/TCYB.2018.2820174
   Li Y., 2017, ICLR WORKSH
   Liu H, 2019, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2019.00304
   Liu MY, 2017, ADV NEUR IN, V30
   Liu YC, 2018, PROC CVPR IEEE, P8867, DOI 10.1109/CVPR.2018.00924
   Long M., 2016, Advances in neural information processing systems, V29
   Long M., 2017, Proc Mach Learn Res, V70, P2208
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Ma XH, 2019, IEEE T MULTIMEDIA, V21, P2419, DOI 10.1109/TMM.2019.2902100
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Odena A, 2017, PR MACH LEARN RES, V70
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191
   Peng XC, 2018, IEEE COMPUT SOC CONF, P2102, DOI 10.1109/CVPRW.2018.00271
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Saito K., 2018, EUR C COMP VIS, P153
   Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392
   Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887
   Shermin T, 2019, LECT NOTES COMPUT SC, V11854, P142, DOI 10.1007/978-3-030-34879-3_12
   Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taigman Y., 2016, ARXIV PREPRINT 16110
   Tzeng E., 2014, ARXIV14123474
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572
   Volpi R, 2018, PROC CVPR IEEE, P5495, DOI 10.1109/CVPR.2018.00576
   Wang X., 2014, Advances in neural information processing systems, V27, P1898
   You KC, 2019, PROC CVPR IEEE, P2715, DOI 10.1109/CVPR.2019.00283
   Zellinger Werner, 2017, 5 INT C LEARN REPR I, P2
   Zhang J, 2018, PROC CVPR IEEE, P8156, DOI 10.1109/CVPR.2018.00851
   Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 59
TC 45
Z9 47
U1 17
U2 134
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2732
EP 2744
DI 10.1109/TMM.2020.3016126
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600015
OA Green Published, Green Submitted
DA 2024-07-18
ER

PT J
AU Su, HS
   Zhao, X
   Lin, TW
   Liu, SM
   Hu, ZL
AF Su, Haisheng
   Zhao, Xu
   Lin, Tianwei
   Liu, Shuming
   Hu, Zhilan
TI Transferable Knowledge-Based Multi-Granularity Fusion Network for Weakly
   Supervised Temporal Action Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Task analysis; Training; Object detection; Kernel;
   Convolution; Visualization; Boundary regression; cascaded dilated
   classification block; class activation sequence; knowledge transfer;
   temporal action detection; weak supervision
ID ACTION RECOGNITION; ATTENTION
AB Despite remarkable progress, temporal action detection is still limited for real application due to the great amount of manual annotations. This issue motivates interest in addressing this task under weak supervision, namely, locating the action instances using only video-level class labels. Many current works on this task are mainly based on the Class Activation Sequence (CAS), which is generated by the video classification network to describe the probability of each snippet being in a specific action class of the video. However, the CAS generated by a simple classification network can only focus on local discriminative parts instead of locating the entire interval of target actions. In this paper, we present a novel framework to handle this issue. Specifically, we propose to utilize convolutional kernels with varied dilation rates to enlarge the receptive fields, which can transfer the discriminative information to the surrounding non-discriminative regions. Then, we design a cascaded module with the proposed Online Adversarial Erasing (OAE) mechanism to further mine more relevant regions of target actions by feeding the erased-feature maps of discovered regions back into the system. In addition, inspired by the transfer learning method, we adopt an additional module to transfer the knowledge from trimmed videos to untrimmed videos to promote the classification performance on untrimmed videos. Finally, we employ a boundary regression module embedded with Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries based on the enhanced CAS. Extensive experiments are conducted on two challenging datasets, THUMOS14 and ActivityNet-1.3, and the experimental results clearly demonstrate the superiority of our unified framework.
C1 [Su, Haisheng; Zhao, Xu; Lin, Tianwei; Liu, Shuming] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
   [Zhao, Xu] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.
   [Hu, Zhilan] Huawei Co Ltd, Cent Media Technol Inst, Shenzhen 518129, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; Huawei
   Technologies
RP Zhao, X (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
EM suhaisheng@sjtu.edu.cn; zhaoxu@sjtu.edu.cn; wzmsltw@sjtu.edu.cn;
   shumingliu@sjtu.edu.cn; huzhilan@huawei.com
OI Su, Haisheng/0000-0002-4228-7439
FU NSFC [61673269, 61273285]; Institute of Medical Robotics at Shanghai
   Jiao Tong University
FX This work was supported in part by NSFC (61673269, 61273285) and in part
   by the project funding of the Institute of Medical Robotics at Shanghai
   Jiao Tong University. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. M. Murshed.
CR Bilen H, 2016, PROC CVPR IEEE, P2846, DOI 10.1109/CVPR.2016.311
   Bodla N, 2017, IEEE I CONF COMP VIS, P5562, DOI 10.1109/ICCV.2017.593
   Buch S., 2017, P BRIT MACH VIS C BM
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Gao Jiyang, 2017, BMVC
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Guo DS, 2018, IEEE T MULTIMEDIA, V20, P3428, DOI 10.1109/TMM.2018.2839534
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heilbron FC, 2017, PROC CVPR IEEE, P3175, DOI 10.1109/CVPR.2017.338
   Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Jing PG, 2017, IEEE T MULTIMEDIA, V19, P1050, DOI 10.1109/TMM.2016.2644866
   Li D, 2019, IEEE T MULTIMEDIA, V21, P416, DOI 10.1109/TMM.2018.2862341
   Lin T., 2017, P IEEE INT C COMP VI
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu DC, 2019, PROC CVPR IEEE, P1298, DOI 10.1109/CVPR.2019.00139
   Oneata D., 2014, ECCV THUMOS WORKSH
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Richard A, 2016, PROC CVPR IEEE, P3131, DOI 10.1109/CVPR.2016.341
   Shou Z, 2018, LECT NOTES COMPUT SC, V11220, P162, DOI 10.1007/978-3-030-01270-0_10
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh G., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Singh KK, 2017, IEEE I CONF COMP VIS, P3544, DOI 10.1109/ICCV.2017.381
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Su H., 2018, P AS C COMP VIS, P558
   Su HS, 2018, LECT NOTES COMPUT SC, V11304, P426, DOI 10.1007/978-3-030-04212-7_37
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Tao QY, 2019, IEEE T MULTIMEDIA, V21, P1135, DOI 10.1109/TMM.2018.2875597
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang L., 2016, P ECCV
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang PC, 2018, IEEE T MULTIMEDIA, V20, P1051, DOI 10.1109/TMM.2018.2818329
   Wang R., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Wang X., 2017, PROC CVPR IEEE, P4322
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759
   Xiong Y., 2017, CoRR
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yan CG, 2019, IEEE T MULTIMEDIA, V21, P2675, DOI 10.1109/TMM.2019.2903448
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Yunchao Wei, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P6488, DOI 10.1109/CVPR.2017.687
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou Z, 2015, IEEE T MULTIMEDIA, V17, P512, DOI 10.1109/TMM.2015.2404779
   Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26
NR 57
TC 8
Z9 8
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1503
EP 1515
DI 10.1109/TMM.2020.2999184
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA SJ9EZ
UT WOS:000655830300004
DA 2024-07-18
ER

PT J
AU Tian, X
   Ng, WWY
   Wang, H
   Kwong, S
AF Tian, Xing
   Ng, Wing W. Y.
   Wang, Hui
   Kwong, Sam
TI Complementary Incremental Hashing With Query-Adaptive Re-Ranking for
   Image Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Training; Semantics; Hamming distance; Computer
   science; Laplace equations; Image Retrieval; Hashing; Non-stationary
   Environment; Concept Drift; Re-ranking
ID SEARCH; GRAPH
AB Concept drift is prevalent in non-stationary data environments but is rarely researched in image retrieval. Therefore, more research is needed on image retrieval in non-stationary data environments so that highly relevant images can still be retrieved when concept drifts happen. Hashing is a key technique to allow efficient image retrieval, so incremental hashing technique emerges in recent years for image retrieval in non-stationary environments. A state-of-the-art method is Incremental Hashing (ICH). ICH trains new hash tables on new data without considering the performance of previous hash tables, so the dependency of successive hash tables is ignored. To make use of this dependency in order to improve the performance of image retrieval in non-stationary environments, Complementary Incremental Hashing with query-adaptive Re-ranking (CIHR) is proposed in this paper. CIHR trains multiple hash tables incrementally, one for each data chunk of images. A new hash table is trained on a new data chunk of images as well as those images badly hashed by previous hash tables, thus the new hash table is complementary to the previous hash tables. To use the hash tables more effectively, a query-adaptive re-ranking method is used to weight all hash functions in each hash table according to their retrieval performance with respect to a given query. Weighted Hamming distance is finally used to evaluate the similarity between the query and the images in the database, as the basis of image retrieval. Experimental results on simulated non-stationary scenarios show that the proposed CIHR method achieves higher retrieval accuracy than all methods being compared, thus setting a new state of the art in image retrieval in non-stationary data environments.
C1 [Tian, Xing; Ng, Wing W. Y.] South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
   [Tian, Xing; Wang, Hui] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Kwong, Sam] Ulster Univ, Sch Comp, Belfast BT15 1ED, Antrim, North Ireland.
C3 South China University of Technology; City University of Hong Kong;
   Ulster University
RP Ng, WWY (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cy, Guangzhou 510006, Peoples R China.
EM shawntian123@gmail.com; wingng@ieee.org; h.wang@ulster.ac.uk;
   cssamk@cityu.edu.hk
RI Kwong, Sam/C-9319-2012; wang, yue/ISA-4119-2023; wang,
   hui/HSG-6135-2023; Wang, Hui/HMU-9512-2023; TIAN, XING/L-8374-2018
OI Kwong, Sam/0000-0001-7484-7261; TIAN, XING/0000-0002-7546-1018; Wang,
   Hui/0000-0003-2633-6015; Ng, Wing W. Y./0000-0003-0783-3585
FU National Natural Science Foundation of China [61876066, 61772344,
   61672443]; Guangzhou Science and Technology Plan Project [201804010245];
   Guangdong Province Science and Technology Plan Project (Collaborative
   Innovation and Platform Environment Construction) [2019A050510006]; Hong
   Kong RGC General Research Funds [9042489 (CityU 11206317), 9042816
   (CityU 11209819), 9042322 (CityU 11200116)]; EU Horizon 2020 Programme
   [700381]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876066, 61772344, and 61672443, in
   part by Guangzhou Science and Technology Plan Project underGrant
   201804010245, in part by Guangdong Province Science and Technology Plan
   Project (Collaborative Innovation and Platform Environment Construction)
   under Grant 2019A050510006, in part by Hong Kong RGC General Research
   Funds under Grants 9042489 (CityU 11206317), 9042816 (CityU 11209819),
   and 9042322 (CityU 11200116), and in part by EU Horizon 2020 Programme
   (700381, ASGARD). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. MengWang.
CR [Anonymous], 2009, NEURIPS
   Cakir F, 2017, IEEE I CONF COMP VIS, P437, DOI 10.1109/ICCV.2017.55
   Cakir F, 2015, IEEE I CONF COMP VIS, P1044, DOI 10.1109/ICCV.2015.125
   Cakir F, 2015, IEEE IMAGE PROC, P2606, DOI 10.1109/ICIP.2015.7351274
   Chen BC, 2015, IEEE T MULTIMEDIA, V17, P804, DOI 10.1109/TMM.2015.2420374
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Elwell R, 2011, IEEE T NEURAL NETWOR, V22, P1517, DOI 10.1109/TNN.2011.2160459
   Gorisse D, 2012, IEEE T PATTERN ANAL, V34, P402, DOI 10.1109/TPAMI.2011.193
   Guo YC, 2017, IEEE T IMAGE PROCESS, V26, P1344, DOI 10.1109/TIP.2017.2652730
   Heo JP, 2015, IEEE T PATTERN ANAL, V37, P2304, DOI 10.1109/TPAMI.2015.2408363
   Hu D, 2019, IEEE T IMAGE PROCESS, V28, P1080, DOI 10.1109/TIP.2018.2875312
   Huang LK, 2018, IEEE T NEUR NET LEAR, V29, P2309, DOI 10.1109/TNNLS.2017.2689242
   Huang Long-Kai., 2013, IJCAI, P1422
   Jiang K, 2015, PROC CVPR IEEE, P4933, DOI 10.1109/CVPR.2015.7299127
   Kong WH, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P45, DOI 10.1145/2348283.2348293
   Lai HJ, 2016, IEEE T IMAGE PROCESS, V25, P2469, DOI 10.1109/TIP.2016.2545300
   Leng C, 2015, PROC CVPR IEEE, P2503, DOI 10.1109/CVPR.2015.7298865
   Li P, 2013, NEUROCOMPUTING, V120, P83, DOI 10.1016/j.neucom.2012.07.053
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu L, 2016, IEEE T CYBERNETICS, V46, P2548, DOI 10.1109/TCYB.2015.2480966
   Liu WJ, 2011, E-POLYMERS
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P4514, DOI 10.1109/TIP.2016.2593344
   Liu XL, 2016, IEEE T IMAGE PROCESS, V25, P907, DOI 10.1109/TIP.2015.2505180
   Liu XL, 2013, PROC CVPR IEEE, P1570, DOI 10.1109/CVPR.2013.206
   Liu XM, 2018, IEEE T IMAGE PROCESS, V27, P3222, DOI 10.1109/TIP.2018.2799704
   Ng WWY, 2019, IEEE T CYBERNETICS, V49, P3844, DOI 10.1109/TCYB.2018.2846760
   Ng WWY, 2018, NEUROCOMPUTING, V275, P916, DOI 10.1016/j.neucom.2017.09.042
   Ng WWY, 2017, IEEE T CYBERNETICS, V47, P3814, DOI 10.1109/TCYB.2016.2582530
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638
   Wang D, 2016, IEEE T IMAGE PROCESS, V25, P4540, DOI 10.1109/TIP.2016.2592800
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang M, 2015, IEEE T CYBERNETICS, V45, P1561, DOI 10.1109/TCYB.2014.2356136
   Wang M, 2012, IEEE T IMAGE PROCESS, V21, P4649, DOI 10.1109/TIP.2012.2207397
   Wang SN, 2019, IEEE T CYBERNETICS, V49, P1896, DOI 10.1109/TCYB.2018.2816791
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   WENG Z, 2019, IEEE ACCESS, V7
   Wu CX, 2013, IEEE T KNOWL DATA EN, V25, P1380, DOI 10.1109/TKDE.2012.76
   Xu H, 2011, IEEE I CONF COMP VIS, P1631, DOI 10.1109/ICCV.2011.6126424
   Ye RZ, 2016, IEEE T CYBERNETICS, V46, P718, DOI 10.1109/TCYB.2015.2414299
   Zhang ST, 2015, IEEE T PATTERN ANAL, V37, P803, DOI 10.1109/TPAMI.2014.2346201
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhen Y, 2016, IEEE T CYBERNETICS, V46, P27, DOI 10.1109/TCYB.2015.2392052
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
NR 47
TC 9
Z9 9
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1210
EP 1224
DI 10.1109/TMM.2020.2994509
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XO9CW
UT WOS:000730475700001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, FB
   Yang, T
   Liu, LF
   Liang, B
   Bai, Y
   Li, J
AF Zhang, Fangbing
   Yang, Tao
   Liu, LinFeng
   Liang, Bang
   Bai, Yi
   Li, Jing
TI Image-Only Real-Time Incremental UAV Image Mosaic for Multi-Strip Flight
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Real-time systems; Robustness; Task analysis; Optimization; Feature
   extraction; Atmospheric modeling; Cameras; UAV images; image mosaic;
   local optimization; homography estimation
ID ALGORITHM
AB Limited by aircraft flight altitude and camera parameters, it is necessary to obtain wide-angle panoramas quickly by stitching aerial images, which is helpful in rapid disaster investigation, recovery after earthquakes, and aerial reconnaissance. However, most existing stitching algorithms do not simultaneously meet practical real-time, robustness, and accuracy requirements, especially in the case of a long-distance multistrip flight. In this paper, we propose a novel image-only real-time UAV image mosaic framework for long-distance multistrip flights that does not require any auxiliary information, such as GPS or GCPs. The framework has a complete structure, mainly consisting of the three tasks of automatic initialization, current frame tracking, and real-time mosaic generation. The stitching plane is determined in the initialization process, the homography transformation of the current image is estimated in the tracking task, and the image is mapped to the stitching plane to generate and update the panorama in the real-time mosaic process. The core idea is that, in the tracking task, we introduce and develop a keyframe insertion strategy to generate a keyframe list and, on this basis, design a homography matrix estimation based on a local optimization strategy to reduce the accumulated error when continuously stitching image sequences collected online by UAVs and to realize real-time, effective UAV image mosaic construction. In addition, this framework has good scalability, which is not limited to a specific algorithm. To evaluate the effectiveness of the proposed framework, we carry out a large number of experiments on the AirSim simulation platform and present an exhaustive evaluation in some sequences from a popular dataset. Qualitative and quantitative experimental results in simulation and real environments demonstrate that our algorithm can obtain an effective and robust mosaic image in real-time. Through strategy comparison experiments, it is proven that the keyframe insertion strategy and the local optimization strategy both improve the stitching performance. Compared with five state-of-art image stitching approaches, the mosaic effect of the proposed method is comparable or better. In terms of algorithm speed, its performance is superior to them. Additionally, experiments of illumination change and feature replacement in the framework verify the good adaptability and scalability of the algorithm.
C1 [Zhang, Fangbing; Yang, Tao; Liu, LinFeng; Liang, Bang; Bai, Yi] Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big, SAIIP, Xian 710129, Peoples R China.
   [Li, Jing] Xidian Univ, Sch Telecommun Engn, Xian 710071, Peoples R China.
C3 Northwestern Polytechnical University; Xidian University
RP Yang, T (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Natl Engn Lab Integrated Aerosp Ground Ocean Big, SAIIP, Xian 710129, Peoples R China.
EM fangbing_zhang@mail.nwpu.edu.cn; tyang@nwpu.edu.cn;
   linfengliu@mail.nwpu.edu.cn; kn1000b@mail.nwpu.edu.cn;
   seafire@mail.nwpu.edu.cn; jinglixd@mail.xidian.edu.cn
FU  [61672429];  [61502364]
FX Thisworkwas supported by theNationalNatural Science Foundation ofChina
   underGrants 61672429 and 61502364. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Chang-Su Kim.
CR Avola D, 2020, IEEE T SYST MAN CY-S, V50, P2139, DOI 10.1109/TSMC.2018.2804766
   Avola D, 2017, LECT NOTES COMPUT SC, V10484, P694, DOI 10.1007/978-3-319-68560-1_62
   Bajpai P, 2018, PROCEEDINGS OF THE 2018 APWG SYMPOSIUM ON ELECTRONIC CRIME RESEARCH (ECRIME), P35
   Bertel T, 2019, IEEE T VIS COMPUT GR, V25, P1828, DOI 10.1109/TVCG.2019.2898799
   Bu SH, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4564, DOI 10.1109/IROS.2016.7759672
   Chailloux C, 2011, IEEE J OCEANIC ENG, V36, P627, DOI 10.1109/JOE.2011.2141850
   Chen J, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081898
   Chen J, 2018, CHIN CONTR CONF, P4265, DOI 10.23919/ChiCC.2018.8483513
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Deng HW, 2018, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2018.00028
   DeTone D., 2016, ARXIV160603798
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Fang FM, 2019, IEEE GEOSCI REMOTE S, V16, P1115, DOI 10.1109/LGRS.2019.2893210
   Fu ZX, 2011, LECT NOTES ARTIF INT, V7003, P273, DOI 10.1007/978-3-642-23887-1_34
   Gaddam VR, 2016, IEEE T MULTIMEDIA, V18, P1819, DOI 10.1109/TMM.2016.2586304
   Ge Y, 2016, INT C INTEL HUM MACH, P90, DOI 10.1109/IHMSC.2016.110
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hugin, PANORAMA PHOTOSTITCH
   Ruiz JJ, 2018, IEEE ROBOT AUTOM LET, V3, P2838, DOI 10.1109/LRA.2018.2844304
   Lee MS, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P121, DOI 10.1109/ICME.2004.1394140
   Li J, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11101241
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Liu JC, 2017, IEEE T GEOSCI REMOTE, V55, P4089, DOI 10.1109/TGRS.2017.2688385
   Liu SG, 2019, IEEE T CONSUM ELECTR, V65, P303, DOI 10.1109/TCE.2019.2893644
   Liu SG, 2019, IEEE T MULTIMEDIA, V21, P690, DOI 10.1109/TMM.2018.2864576
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2019, PROC CVPR IEEE, P2522, DOI 10.1109/CVPR.2019.00263
   Madhusudana PC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2921858
   Melekhov I, 2019, IEEE WINT CONF APPL, P1034, DOI 10.1109/WACV.2019.00115
   Meng XY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P261, DOI 10.1145/2733373.2806225
   Park S, 2017, IEEE T CONSUM ELECTR, V63, P178, DOI 10.1109/TCE.2017.014847
   Patel MS, 2015, 2015 INTERNATIONAL SYMPOSIUM ON ADVANCED COMPUTING AND COMMUNICATION (ISACC), P213, DOI 10.1109/ISACC.2015.7377344
   Pei HJ, 2017, INT GEOSCI REMOTE SE, P5904, DOI 10.1109/IGARSS.2017.8128353
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Tsao P, 2018, INT CONF DAT MIN WOR, P616, DOI 10.1109/ICDMW.2018.00096
   Nguyen T, 2018, IEEE ROBOT AUTOM LET, V3, P2346, DOI 10.1109/LRA.2018.2809549
   Wang H, 2014, INT GEOSCI REMOTE SE, P2633, DOI 10.1109/IGARSS.2014.6947014
   Xiong YG, 2010, IEEE T CONSUM ELECTR, V56, P298, DOI 10.1109/TCE.2010.5505931
   Yang T, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10111829
   Yang YC, 2019, REMOTE SENS-BASEL, V11, DOI 10.3390/rs11111365
   Ye JS, 2019, PUBLIC HEALTH NUTR, V22, P1048, DOI [10.1017/s1368980018003129, 10.1017/S1368980018003129]
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Yong HW, 2019, IEEE T IMAGE PROCESS, V28, P3162, DOI 10.1109/TIP.2019.2894940
   Yu L, 2017, IEEE GEOSCI REMOTE S, V14, P729, DOI 10.1109/LGRS.2017.2676438
   Zhang GF, 2016, IEEE T IMAGE PROCESS, V25, P3099, DOI 10.1109/TIP.2016.2535225
   Zhang WL, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18041214
   Zhang WP, 2018, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-018-0323-5
   Zhang Yu, 2017, Journal of Computer Aided Design & Computer Graphics, V29, P2317, DOI 10.3724/SP.J.1089.2017.16744
   Zheng J, 2019, IEEE T MULTIMEDIA, V21, P2561, DOI 10.1109/TMM.2019.2905692
   Zhou H, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE RCAR), P561, DOI 10.1109/RCAR.2016.7784091
NR 51
TC 17
Z9 18
U1 5
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1410
EP 1425
DI 10.1109/TMM.2020.2997193
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200018
DA 2024-07-18
ER

PT J
AU Zhang, Q
   Nie, YW
   Zhu, L
   Xiao, CX
   Zheng, WS
AF Zhang, Qing
   Nie, Yongwei
   Zhu, Lei
   Xiao, Chunxia
   Zheng, Wei-Shi
TI Enhancing Underexposed Photos Using Perceptually Bidirectional
   Similarity
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lighting; Estimation; Image color analysis; Visualization; Image
   enhancement; Distortion; Computer science; Underexposed photo
   enhancement; perceptually bidirectional similarity; illumination
   estimation
ID ILLUMINATION ESTIMATION; IMAGE-ENHANCEMENT; OBJECT DETECTION; RETINEX
AB Although remarkable progress has been made, existing methods for enhancing underexposed photos tend to produce visually unpleasing results due to the existence of visual artifacts (e.g., color distortion, loss of details and uneven exposure). We observed that this is because they fail to ensure the perceptual consistency of visual information between the source underexposed image and its enhanced output. To obtain high-quality results free of these artifacts, we present a novel underexposed photo enhancement approach that is able to maintain the perceptual consistency. We achieve this by proposing an effective criterion, referred to as perceptually bidirectional similarity, which explicitly describes how to ensure the perceptual consistency. Particularly, we adopt the Retinex theory and cast the enhancement problem as a constrained illumination estimation optimization, where we formulate perceptually bidirectional similarity as constraints on illumination and solve for the illumination which can recover the desired artifact-free enhancement results. In addition, we describe a video enhancement framework that adopts the presented illumination estimation for handling underexposed videos. To this end, a probabilistic approach is introduced to propagate illuminations of sampled keyframes to the entire video by tackling a Bayesian Maximum A Posteriori problem. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods.
C1 [Zhang, Qing; Zheng, Wei-Shi] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.
   [Nie, Yongwei] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhu, Lei] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Xiao, Chunxia] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Zheng, Wei-Shi] Peng Cheng Lab, Shenzhen 518005, Peoples R China.
   [Zheng, Wei-Shi] Sun Yat Sen Univ, Key Lab Machine Intelligence & Adv Comp, Minist Educ, Guangzhou 510275, Peoples R China.
C3 Sun Yat Sen University; South China University of Technology; Chinese
   University of Hong Kong; Wuhan University; Peng Cheng Laboratory; Sun
   Yat Sen University
RP Zheng, WS (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou 510006, Peoples R China.; Zheng, WS (corresponding author), Peng Cheng Lab, Shenzhen 518005, Peoples R China.
EM zhangqing.whu.cs@gmail.com; nieyongwei@scut.edu.cn;
   lzhu@cse.cuhk.edu.hk; cxxiao@whu.edu.cn; wszheng@ieee.org
RI WU, ZHEN/GRN-7688-2022; Zhang, Qing/ABB-1569-2021; zheng,
   wei/IQT-9639-2023
OI WU, ZHEN/0000-0001-8719-057X; Zhu, Lei/0000-0003-3871-663X
FU National Key Research and Development Program of China [2016YFB1001001];
   NSFC [61802453, U1911401, U1811461, 61902275]; Fundamental Research
   Funds for the Central Universities [19lgpy216, D2190670]; Guangdong
   Province Science and Technology Innovation Leading Talents
   [2016TX03X157]; Guangdong NSF [2018B030312002, 2019A1515010860];
   Guangzhou Research Project [201902010037]; Research Projects of Zhejiang
   Lab [2019KD0AB03]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2016YFB1001001, in part by NSFC
   under Grants 61802453, U1911401, U1811461, and 61902275, in part by the
   Fundamental Research Funds for the Central Universities (19lgpy216,
   D2190670), in part by Guangdong Province Science and Technology
   Innovation Leading Talents under Grant 2016TX03X157, in part by
   Guangdong NSF Projects 2018B030312002 and 2019A1515010860, in part by
   Guangzhou Research Project 201902010037, and in part by the Research
   Projects of Zhejiang Lab 2019KD0AB03.
CR [Anonymous], 2012, ACM T GRAPH TOG
   Bennett EP, 2005, ACM T GRAPHIC, V24, P845, DOI 10.1145/1073204.1073272
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Cai BL, 2017, IEEE I CONF COMP VIS, P4020, DOI 10.1109/ICCV.2017.431
   Cai JR, 2018, IEEE T IMAGE PROCESS, V27, P2049, DOI 10.1109/TIP.2018.2794218
   Celik T, 2011, IEEE T IMAGE PROCESS, V20, P3431, DOI 10.1109/TIP.2011.2157513
   Chen C, 2018, PROC CVPR IEEE, P3291, DOI 10.1109/CVPR.2018.00347
   Chen YS, 2018, PROC CVPR IEEE, P6306, DOI 10.1109/CVPR.2018.00660
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Dong X, 2011, IEEE INT CON MULTI
   Farbman Z, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360666
   Fu G, 2019, IEEE INT CON MULTI, P175, DOI 10.1109/ICME.2019.00038
   Fu XY, 2016, PROC CVPR IEEE, P2782, DOI 10.1109/CVPR.2016.304
   Fu XY, 2016, SIGNAL PROCESS, V129, P82, DOI 10.1016/j.sigpro.2016.05.031
   Fu XY, 2015, IEEE T IMAGE PROCESS, V24, P4965, DOI 10.1109/TIP.2015.2474701
   Gao YY, 2018, IEEE T MULTIMEDIA, V20, P335, DOI 10.1109/TMM.2017.2740025
   Gharbi M, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073592
   Guo XJ, 2017, IEEE T IMAGE PROCESS, V26, P982, DOI 10.1109/TIP.2016.2639450
   Hwang SJ, 2012, LECT NOTES COMPUT SC, V7572, P569, DOI 10.1007/978-3-642-33718-5_41
   Jiang Y., 2019, ARXIV190606972
   Jobson DJ, 1997, IEEE T IMAGE PROCESS, V6, P965, DOI 10.1109/83.597272
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kopf J, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1276377.1276497, 10.1145/1239451.1239547]
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   Li HL, 2013, IEEE T MULTIMEDIA, V15, P1896, DOI 10.1109/TMM.2013.2271476
   Li L, 2015, IEEE IMAGE PROC, P3730, DOI 10.1109/ICIP.2015.7351501
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Lore KG, 2017, PATTERN RECOGN, V61, P650, DOI 10.1016/j.patcog.2016.06.008
   Ma K, 2015, IEEE T IMAGE PROCESS, V24, P3345, DOI 10.1109/TIP.2015.2442920
   Maggioni M, 2012, IEEE T IMAGE PROCESS, V21, P3952, DOI 10.1109/TIP.2012.2199324
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Nascimento JC, 2006, IEEE T MULTIMEDIA, V8, P761, DOI 10.1109/TMM.2006.876287
   Park J, 2018, PROC CVPR IEEE, P5928, DOI 10.1109/CVPR.2018.00621
   Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412
   Wang RX, 2019, PROC CVPR IEEE, P6842, DOI 10.1109/CVPR.2019.00701
   Wang SQ, 2016, IEEE T MULTIMEDIA, V18, P219, DOI 10.1109/TMM.2015.2510326
   Wang SH, 2013, IEEE T IMAGE PROCESS, V22, P3538, DOI 10.1109/TIP.2013.2261309
   Wei C., 2018, P BRIT MACHINE VISIO, P155
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Xu L, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024208
   Yan JZ, 2014, PROC CVPR IEEE, P2987, DOI 10.1109/CVPR.2014.382
   Yan ZC, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2790296
   Yang X, 2018, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2018.00193
   Ye GZ, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601135
   Ye Z, 2007, SE SYM SYS THRY, P315
   Ying ZQ, 2017, IEEE INT CONF COMP V, P3015, DOI 10.1109/ICCVW.2017.356
   Yu RS, 2018, ADV NEUR IN, V31
   Yuan L, 2012, LECT NOTES COMPUT SC, V7575, P771, DOI 10.1007/978-3-642-33765-9_55
   Zhang Q, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P582, DOI 10.1145/3240508.3240595
   Zhang Q, 2019, COMPUT GRAPH FORUM, V38, P243, DOI 10.1111/cgf.13833
   Zhang Q, 2016, IEEE T VIS COMPUT GR, V22, P1773, DOI 10.1109/TVCG.2015.2461157
   Zhang YH, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1632, DOI 10.1145/3343031.3350926
   Zuiderveld K., 1994, Graphics Gems, P474, DOI 10.1016/B978-0-12-336156-1.50061-6
NR 55
TC 16
Z9 18
U1 2
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 189
EP 202
DI 10.1109/TMM.2020.2982045
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600015
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Zhou, Y
   Wang, RL
   Li, HR
   Kung, SY
AF Zhou, Yuan
   Wang, Ruolin
   Li, Hongru
   Kung, Sun-Yuan
TI Temporal Action Localization Using Long Short-Term Dependency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Videos; Feature extraction; Proposals; Task analysis; Three-dimensional
   displays; Recurrent neural networks; Spatiotemporal phenomena; Action
   localization; convolutional neural networks; video content analysis
ID ACTION RECOGNITION; VIDEOS; FUSION
AB Temporal action localization in untrimmed videos is an important but difficult task. Difficulties are encountered in the application of existing methods when modeling the temporal structures of videos. In the present study, we develop a novel method, referred to as the Gemini Network, for effective modeling of temporal structures and achieving high-performance temporal action localization. The significant improvements afforded by the proposed method are due to three major factors. First, temporal dependencies are explicitly distinguished as long-term temporal dependencies and short-term temporal dependencies and are separately captured by two dedicated subnets. Second, a long-range temporal dependency capture module combined with a self-adaptive pooling module is proposed to capture long-term temporal dependency. Third, the proposed method uses auxiliary supervision, with the auxiliary classifier losses affording additional constraints for improving the modeling capability of the network. As a demonstration of its effectiveness, the Gemini Network is used to achieve a state-of-the-art temporal action localization performance on two challenging datasets, namely, THUMOS14 and ActivityNet.
C1 [Zhou, Yuan; Wang, Ruolin; Li, Hongru] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Kung, Sun-Yuan] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.
C3 Tianjin University; Princeton University
RP Zhou, Y (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM zhouyuan@tju.edu.cn; szx2048067511@163.com; lihongru@tju.edu.cn;
   kung@princeton.edu
FU National Natural Science Foundation of China [U2006211]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant U2006211. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. F. Porikli.
CR Alwassel H, 2018, LECT NOTES COMPUT SC, V11213, P253, DOI 10.1007/978-3-030-01240-3_16
   [Anonymous], 2018, P IEEE CVF C COMP VI
   Buch S., 2017, P BRIT MACH VIS C BM
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698
   Chao YW, 2018, PROC CVPR IEEE, P1130, DOI 10.1109/CVPR.2018.00124
   Chen C, 2015, IEEE T HUM-MACH SYST, V45, P51, DOI 10.1109/THMS.2014.2362520
   Dai XY, 2017, IEEE I CONF COMP VIS, P5727, DOI 10.1109/ICCV.2017.610
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gao J., 2017, ARXIV170704818
   Gao JY, 2017, IEEE I CONF COMP VIS, P3648, DOI 10.1109/ICCV.2017.392
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heilbron FC, 2018, LECT NOTES COMPUT SC, V11215, P212, DOI 10.1007/978-3-030-01252-6_13
   Heilbron FC, 2016, PROC CVPR IEEE, P1914, DOI 10.1109/CVPR.2016.211
   Hou JY, 2018, IEEE T MULTIMEDIA, V20, P1537, DOI 10.1109/TMM.2017.2771462
   Hussein, 2013, INT JOINT C ART INT
   Jiang Y.-G., 2014, THUMOS challenge: Action recognition with a large number of classes
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Khandelwal U, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P284
   Kim J.-H., IEEE ACCESS, V7
   Li NN, 2018, IEEE ACCESS, V6, P59126, DOI 10.1109/ACCESS.2018.2872759
   Lin TW, 2019, IEEE I CONF COMP VIS, P3888, DOI 10.1109/ICCV.2019.00399
   Lin TW, 2018, LECT NOTES COMPUT SC, V11208, P3, DOI 10.1007/978-3-030-01225-0_1
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu JY, 2019, IEEE T MULTIMEDIA, V21, P887, DOI 10.1109/TMM.2018.2871418
   Liu L, 2016, AAAI CONF ARTIF INTE, P1266
   Liu Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1617
   Oneata D, 2013, IEEE I CONF COMP VIS, P1817, DOI 10.1109/ICCV.2013.228
   Paul S, 2018, LECT NOTES COMPUT SC, V11208, P588, DOI 10.1007/978-3-030-01225-0_35
   Nguyen P, 2018, PROC CVPR IEEE, P6752, DOI 10.1109/CVPR.2018.00706
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Roy D, 2019, IEEE T MULTIMEDIA, V21, P1672, DOI 10.1109/TMM.2018.2887021
   Sánchez J, 2013, IMAGE PROCESS ON LIN, V3, P137, DOI 10.5201/ipol.2013.26
   Shah M., 2017, P BMVC, P7
   Shi YM, 2017, IEEE I CONF COMP VIS, P716, DOI 10.1109/ICCV.2017.84
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Simonyan K, 2014, ADV NEUR IN, V27
   Singh B., 2016, P C COMP VIS PATT RE
   Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216
   Singh G., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Song H, 2019, IEEE T MULTIMEDIA, V21, P717, DOI 10.1109/TMM.2018.2866370
   Sun L, 2017, IEEE I CONF COMP VIS, P2166, DOI 10.1109/ICCV.2017.236
   Sun SY, 2018, PROC CVPR IEEE, P1390, DOI 10.1109/CVPR.2018.00151
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang KV, 2013, IEEE I CONF COMP VIS, P2696, DOI 10.1109/ICCV.2013.335
   Wang JZ, 2020, IEEE T CIRC SYST VID, V30, P117, DOI 10.1109/TCSVT.2018.2887061
   Wang LM, 2019, IEEE T PATTERN ANAL, V41, P2740, DOI 10.1109/TPAMI.2018.2868668
   Wang R., 2016, ActivityNet Large Scale Activity Recognition Challenge
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wu CY, 2018, PROC CVPR IEEE, P6026, DOI 10.1109/CVPR.2018.00631
   Xiong Y., 2017, CoRR
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang HT, 2018, PROC CVPR IEEE, P1450, DOI 10.1109/CVPR.2018.00157
   Yuan ZH, 2017, PROC CVPR IEEE, P3215, DOI 10.1109/CVPR.2017.342
   Zeng RH, 2019, IEEE I CONF COMP VIS, P7093, DOI 10.1109/ICCV.2019.00719
   Zhen XT, 2017, IEEE T MULTIMEDIA, V19, P2056, DOI 10.1109/TMM.2017.2700204
   Zheng JY, 2019, IEEE ACCESS, V7, P183860, DOI 10.1109/ACCESS.2019.2933360
NR 61
TC 11
Z9 11
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4363
EP 4375
DI 10.1109/TMM.2020.3042077
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XA2YU
UT WOS:000720519900035
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lin, WY
   He, XY
   Han, XT
   Liu, D
   See, J
   Zou, JN
   Xiong, HK
   Wu, F
AF Lin, Weiyao
   He, Xiaoyi
   Han, Xintong
   Liu, Dong
   See, John
   Zou, Junni
   Xiong, Hongkai
   Wu, Feng
TI Partition-Aware Adaptive Switching Neural Networks for Post-Processing
   in HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Neural networks; Training; Image coding; Encoding;
   Degradation; Video coding; High Efficiency Video Coding; convolutional
   neural network; post-processing
ID EFFICIENCY; ARTIFACTS; FILTER
AB This article addresses neural network based post-processing for the state-of-the-art video coding standard, High Efficiency Video Coding (HEVC). We first propose a partition-aware convolution neural network (CNN) that utilizes the partition information produced by the encoder to assist in the post-processing. In contrast to existing CNN-based approaches, which only take the decoded frame as input, the proposed approach considers the coding unit (CU) size information and combines it with the distorted decoded frame such that the artifacts introduced by HEVC are efficiently reduced. We further introduce an adaptive-switching neural network (ASN) that consists of multiple independent CNNs to adaptively handle the variations in content and distortion within compressed-video frames, providing further reduction in visual artifacts. Additionally, an iterative training procedure is proposed to train these independent CNNs attentively on different local patch-wise classes. Experiments on benchmark sequences demonstrate the effectiveness of our partition-aware and adaptive-switching neural networks.
C1 [Lin, Weiyao; He, Xiaoyi; Xiong, Hongkai] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai, Peoples R China.
   [Han, Xintong] Huya Inc, Guangzhou, Peoples R China.
   [See, John] Multimedia Univ, Fac Comp & Informat, Cyberjaya, Malaysia.
   [Zou, Junni] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai, Peoples R China.
   [Liu, Dong; Wu, Feng] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei, Peoples R China.
C3 Shanghai Jiao Tong University; Multimedia University; Shanghai Jiao Tong
   University; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Lin, WY (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai, Peoples R China.
EM wylin@sjtu.edu.cn; 515974418@sjtu.edu.cn; xintong@umd.edu;
   dongeliu@ustc.edu.cn; johnsee@mmu.edu.my; zou-jn@cs.sjtu.edu.cn;
   xionghongkai@sjtu.edu.cn; fengwu@ustc.edu.cn
RI Wu, Feng/KCY-3017-2024; liu, dong/GRJ-9115-2022; See, John/C-8633-2013;
   lin, yuxi/HKF-6212-2023
OI See, John/0000-0003-3005-4109; Lin, Weiyao/0000-0001-8307-7107; Liu,
   Dong/0000-0001-9100-2906; Xiong, Hongkai/0000-0003-4552-0029
FU China Major Project for New Generation of AI [2018AAA0100400]; National
   Natural Science Foundation of China [61971277, 61772483]; CREST Malaysia
   [T03C1-17]
FX The paper is supported in part by the China Major Project for New
   Generation of AI Grant 2018AAA0100400, National Natural Science
   Foundation of China under Grants 61971277, 61772483, CREST Malaysia
   Grant T03C1-17. The basic idea of this paper appeared in our conference
   version. In this version, we extend our approach by introducing an
   adaptive-switching scheme, carry out detailed analysis, and present more
   performance results.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Bjontegaard G., 2001, VCEGM33
   Bossen F., 2013, JCTVCL1100, V12
   Chang HB, 2014, IEEE T SIGNAL PROCES, V62, P718, DOI 10.1109/TSP.2013.2290508
   Dai YY, 2017, LECT NOTES COMPUT SC, V10132, P28, DOI 10.1007/978-3-319-51811-4_3
   Dong C, 2015, IEEE I CONF COMP VIS, P576, DOI 10.1109/ICCV.2015.73
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Gross Sam, 2016, Facebook AI Res.
   Han QL, 2015, IEEE IMAGE PROC, P1905, DOI 10.1109/ICIP.2015.7351132
   He XY, 2018, IEEE IMAGE PROC, P216, DOI 10.1109/ICIP.2018.8451086
   Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9
   Jung C, 2012, SIGNAL PROCESS-IMAGE, V27, P663, DOI 10.1016/j.image.2012.03.002
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Li S, 2016, IEEE T CIRC SYST VID, V26, P117, DOI 10.1109/TCSVT.2015.2450131
   Liew AWC, 2004, IEEE T CIRC SYST VID, V14, P450, DOI 10.1109/TCSVT.2004.825555
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Park Park W.-S. W.-S., 2016, P IM VID MULT SIGN P, P1
   SCHWARZ H, 2005, JOINT VIDEO TEAM JVT
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang C, 2013, SIGNAL PROCESS-IMAGE, V28, P522, DOI 10.1016/j.image.2013.01.006
   Wang HL, 2018, IEEE T MULTIMEDIA, V20, P2935, DOI 10.1109/TMM.2018.2830120
   Wang TT, 2018, IEEE DATA COMPR CONF, P197, DOI 10.1109/DCC.2018.00028
   Wang TT, 2017, IEEE DATA COMPR CONF, P410, DOI 10.1109/DCC.2017.42
   Wang YB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON VISUAL COMMUNICATIONS AND IMAGE PROCESSING (IEEE VCIP)
   Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302
   Yang Ren, 2017, ARXIV170906734
   Zhang J, 2016, IEEE DATA COMPR CONF, P91, DOI 10.1109/DCC.2016.105
   Zhang YD, 2012, IEEE T MULTIMEDIA, V14, P510, DOI 10.1109/TMM.2012.2190391
NR 33
TC 32
Z9 32
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 2749
EP 2763
DI 10.1109/TMM.2019.2962310
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lou, YH
   Duan, LY
   Luo, Y
   Chen, ZQ
   Liu, TL
   Wang, SQ
   Gao, W
AF Lou, Yihang
   Duan, Ling-Yu
   Luo, Yong
   Chen, Ziqian
   Liu, Tongliang
   Wang, Shiqi
   Gao, Wen
TI Towards Efficient Front-End Visual Sensing for Digital Retina: A
   Model-Centric Paradigm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Biological system modeling; Retina; Adaptation models; Visualization;
   Brain modeling; Computational modeling; Sensors; Digital retina; model
   reuse; model communication; visual sensing
AB The digital retina excels at providing enhanced visual sensing and analysis capability for city brain in smart cities, and can feasibly convert the visual data from visual sensors into semantic features. With the deployment of deep learning or handcrafted models, these features are extracted on front-end devices, then delivered to back-end servers for advanced analysis. In this scenario, we propose a model generation, utilization and communication paradigm, aiming at strong front-end sensing capabilities for establishing better artificial visual systems in smart cities. In particular, we propose an integrated multiple deep learning models reuse and prediction strategy, which dramatically increases the feasibility of the digital retina in large-scale visual data analysis in smart cities. The proposed multi-model reuse scheme aims to reuse the knowledge from models cached and transmitted in digital retina to obtain more discriminative capability. To efficiently deliver these newly generated models, a model prediction scheme is further proposed by encoding and reconstructing model differences. Extensive experiments have been conducted to demonstrate the effectiveness of proposed model-centric paradigm.
C1 [Lou, Yihang; Duan, Ling-Yu; Luo, Yong; Chen, Ziqian; Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
   [Lou, Yihang; Duan, Ling-Yu; Luo, Yong; Chen, Ziqian; Gao, Wen] China & Peng Cheng Lab, Shenzhen 518066, Peoples R China.
   [Liu, Tongliang] Univ Sydney, Sydney, NSW 2006, Australia.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong 999077, Peoples R China.
C3 Peking University; University of Sydney; City University of Hong Kong
RP Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM yihanglou@pku.edu.cn; lingyu@pku.edu.cn; yongluo@pku.edu.cn;
   wzziqian@pku.edu.cn; tongliang.liu@sydney.edu.au; shiqwang@cityu.edu.hk;
   wgao@pku.edu.cn
RI Liu, Tongliang/AAA-1506-2021
OI Liu, Tongliang/0000-0002-9640-6472
FU National Natural Science Foundation of China [U1611461, 61425025];
   Australian Research Council [DE-1901014738]; Hong Kong RGC Early Career
   Scheme [9048122, CityU 21211018]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U1611461 and 61425025, and in part by
   the Australian Research Council Project under Grant DE-1901014738, and
   in part by the Hong Kong RGC Early Career Scheme under Grants 9048122
   and CityU 21211018. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Marta Mrak
CR [Anonymous], IEEE MULTIMEDIA
   [Anonymous], 2014, ARXIV14124446
   [Anonymous], 2008, P 2008 SIAM INT C DA
   [Anonymous], 2017, 31 AAAI C ART INT
   Bai Y, 2018, IEEE T MULTIMEDIA, V20, P2385, DOI 10.1109/TMM.2018.2796240
   Bao L, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31958-6
   Bhattarai B, 2016, PROC CVPR IEEE, P4226, DOI 10.1109/CVPR.2016.458
   Chen WL, 2015, PR MACH LEARN RES, V37, P2285
   Chen ZQ, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1625, DOI 10.1145/3240508.3240654
   Denil M., 2013, P 26 INT C NEUR INF, P2148
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P44, DOI 10.1109/MMUL.2018.2873844
   Duan LY, 2016, IEEE T IMAGE PROCESS, V25, P179, DOI 10.1109/TIP.2015.2500034
   Duan LY, 2019, IEEE MULTIMEDIA, V26, P8, DOI 10.1109/MMUL.2018.2873564
   [高文 Gao Wen], 2018, [中国科学. 信息科学, Scientia Sinica Informationis], V48, P1076
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541
   Han SY, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511104
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Huang G, 2018, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2018.00291
   Jaderberg M, 2014, P BRIT MACH VIS C, P1
   Jha R., 2018, NAACL HLT, V3, P153
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leibe B., 2017, ARXIV170307737CS
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Liu HY, 2016, PROC CVPR IEEE, P2167, DOI 10.1109/CVPR.2016.238
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Lou YH, 2019, PROC CVPR IEEE, P3230, DOI 10.1109/CVPR.2019.00335
   Lou YH, 2019, IEEE INT CON MULTI, P19, DOI 10.1109/ICME.2019.00012
   Lou YH, 2019, IEEE T IMAGE PROCESS, V28, P3794, DOI 10.1109/TIP.2019.2902112
   Lou YH, 2017, IEEE DATA COMPR CONF, P420, DOI 10.1109/DCC.2017.31
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   N. R. Council, 2000, How People Learn: Brain, Mind, Experience, and School: Expanded Edition
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sterling P, 2015, PRINCIPLES NEURAL DE
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Thorndike EL, 1901, PSYCHOL REV, V8, P247, DOI 10.1037/h0071363
   Wässle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Westphal C., 2015, U.S. Patent, Patent No. 9002921
   Wu DP, 2019, IEEE WIREL COMMUN, V26, P117, DOI 10.1109/MWC.2019.1800323
   Wu JR, 2018, PR MACH LEARN RES, V80
   WU XZ, 2019, P 36 INT C MACH LEAR, P6840
   Yang Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3287
   Yu L, 2009, SIGNAL PROCESS-IMAGE, V24, P247, DOI 10.1016/j.image.2009.02.003
   Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809
   Zhang Xiangyu, 2017, CoRRabs/1711.08184
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
NR 52
TC 6
Z9 6
U1 1
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2020
VL 22
IS 11
BP 3002
EP 3013
DI 10.1109/TMM.2020.2966885
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OJ8YR
UT WOS:000584239900019
DA 2024-07-18
ER

PT J
AU Liu, H
   Sun, PH
   Zhang, JQ
   Wu, SP
   Yu, ZH
   Sun, XH
AF Liu, Hao
   Sun, Penghui
   Zhang, Jiaqiang
   Wu, Suping
   Yu, Zhenhua
   Sun, Xuehong
TI Similarity-Aware and Variational Deep Adversarial Learning for Robust
   Facial Age Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Training; Face; Aging; Measurement; Generators; Convergence;
   Facial age estimation; deep learning; generative adversarial network;
   variational auto-encoder; biometrics
ID FRAMEWORK
AB In this paper, we propose a similarity-aware deep adversarial learning (SADAL) approach for facial age estimation. Instead of making full access to the limited training samples which likely leads to bias age prediction, our SADAL aims to seek batches of unobserved hard-negative samples based on existing training samples, which typically reinforces the discriminativeness of the learned feature representation for facial ages. Motivated by the fact that age labels are usually correlated in real-world scenarios, we carefully develop a similarity-aware function to well measure the distance of each face pair based on the age value gaps. Consequently, the age-difference information is exploited in the synthetic feature space for robust age estimation. During the learning process, we jointly optimize both procedures of generating hard negatives and learning discriminative age ranker via a sequence of adversarial-game iterations. Another major issue lies on that existing methods only enforce the indiscriminativeness within each class, which is probably trapped into model overfitting and thus the generation capacity is limited particularly on unseen age classes with many individuals. To circumvent this problem, we propose a variational deep adversarial learning (VDAL) paradigm, which learns to encode each face sample in two factorized parts, i.e., the intra-class variance distribution and the intra-class invariant class center. Moreover, our VDAL principally optimizes the variational confidence lower bound on the variational factorized feature representation. To better enhance the discriminativeness of the age representation, our VDAL further learns to encode the ordinal relationship among age labels in the reconstructed subspace. Experimental results on folds of widely-evaluated benchmarking datasets demonstrate that our approach achieves promising performance in contrast to most state-of-the-art age estimation methods.
C1 [Liu, Hao; Sun, Penghui; Zhang, Jiaqiang; Wu, Suping; Yu, Zhenhua; Sun, Xuehong] Ningxia Univ, Sch Informat Engn, Yinchuan 750021, Ningxia, Peoples R China.
   [Liu, Hao; Sun, Penghui; Zhang, Jiaqiang; Wu, Suping; Yu, Zhenhua; Sun, Xuehong] Collaborat Innovat Ctr Ningxia Big Data & Artific, Yinchuan 750021, Ningxia, Peoples R China.
C3 Ningxia University
RP Liu, H (corresponding author), Ningxia Univ, Sch Informat Engn, Yinchuan 750021, Ningxia, Peoples R China.
EM liuhao@nxu.edu.cn; penghui.sun_nxu@outlook.com;
   zhangjiaqiang_nxu@outlook.com; pswuu@nxu.edu.cn; zhyu@nxu.edu.cn;
   sunxh@nxu.edu.cn
RI Liu, Hao/AAE-2455-2020; sun, Xue-Hong/J-2695-2012; Zhang,
   Jiaqiang/AAA-3936-2020
OI Liu, Hao/0000-0003-0954-5405; Yu, Zhenhua/0000-0001-6526-6991
FU National Science Foundation of China [61806104, 61862050, 61662059];
   Natural Science Foundation of Ningxia [2018AAC03035]; Scientific
   Research Projects of Colleges and Universities of Ningxia [NGY2018050];
   Youth Science and Technology Talents Enrollment Projects of Ningxia
   [TJGC2018028]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61806104, 61862050, and 61662059, in part by the
   Natural Science Foundation of Ningxia under Grant 2018AAC03035, in part
   by the Scientific Research Projects of Colleges and Universities of
   Ningxia under Grant NGY2018050, and in part by the Youth Science and
   Technology Talents Enrollment Projects of Ningxia under Grant
   TJGC2018028.
CR Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244
   [Anonymous], 2016, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2016.532
   Antipov G, 2017, IEEE IMAGE PROC, P2089, DOI 10.1109/ICIP.2017.8296650
   Chang KY, 2015, IEEE T IMAGE PROCESS, V24, P785, DOI 10.1109/TIP.2014.2387379
   Chang KY, 2011, PROC CVPR IEEE, P585, DOI 10.1109/CVPR.2011.5995437
   CHEN K, 2013, PROC CVPR IEEE, P2467, DOI [DOI 10.1109/CVPR.2013.319, 10.1109/CVPR.2013.319]
   Chen SX, 2018, IEEE T MULTIMEDIA, V20, P2209, DOI 10.1109/TMM.2017.2786869
   Chen Xi, 2016, Advances in Neural Information Processing Systems (NIPS), V29
   Dibeklioglu H, 2015, IEEE T IMAGE PROCESS, V24, P1928, DOI 10.1109/TIP.2015.2412377
   Dong Y, 2016, NEUROCOMPUTING, V187, P4, DOI 10.1016/j.neucom.2015.09.115
   Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294
   Escalera S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P243, DOI 10.1109/ICCVW.2015.40
   Feng SH, 2017, IEEE T MULTIMEDIA, V19, P136, DOI 10.1109/TMM.2016.2608786
   Feng ZH, 2018, PROC CVPR IEEE, P2235, DOI 10.1109/CVPR.2018.00238
   Fu Y, 2010, IEEE T PATTERN ANAL, V32, P1955, DOI 10.1109/TPAMI.2010.36
   Gao C, 2018, PROCEEDINGS 2018 33RD YOUTH ACADEMIC ANNUAL CONFERENCE OF CHINESE ASSOCIATION OF AUTOMATION (YAC), P712, DOI 10.1109/YAC.2018.8406464
   Geng X, 2007, IEEE T PATTERN ANAL, V29, P2234, DOI 10.1109/TPAMI.2007.70733
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280
   Guo GD, 2014, IMAGE VISION COMPUT, V32, P761, DOI 10.1016/j.imavis.2014.04.011
   Guo GD, 2011, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2011.5995404
   Guo GD, 2009, PROC CVPR IEEE, P112, DOI 10.1109/CVPRW.2009.5206681
   Guo J, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL V, P71
   Gürpinar F, 2016, IEEE COMPUT SOC CONF, P785, DOI 10.1109/CVPRW.2016.103
   Han H, 2018, IEEE T PATTERN ANAL, V40, P2597, DOI 10.1109/TPAMI.2017.2738004
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3846, DOI 10.1109/TIP.2017.2655445
   Hu ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3087, DOI 10.1109/TIP.2016.2633868
   Iandola F. N., 2016, ARXIV160207360, DOI 10.1007/978-3-319-24553-9
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Keinert F, 2019, IEEE T IMAGE PROCESS, V28, P2785, DOI 10.1109/TIP.2018.2890312
   Kingma D. P., 2014, arXiv
   Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuang ZH, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P338, DOI 10.1109/ICCVW.2015.52
   Kuang-Yu Chang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3396, DOI 10.1109/ICPR.2010.829
   Lanitis A, 2002, IEEE T PATTERN ANAL, V24, P442, DOI 10.1109/34.993553
   Levi Gil, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P34, DOI 10.1109/CVPRW.2015.7301352
   Li CS, 2012, PROC CVPR IEEE, P2570, DOI 10.1109/CVPR.2012.6247975
   Li K, 2018, PROC CVPR IEEE, P399, DOI 10.1109/CVPR.2018.00049
   Li PP, 2018, INT C PATT RECOG, P1073, DOI 10.1109/ICPR.2018.8545119
   Li WH, 2019, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2019.00514
   Lin X, 2018, 2018 5TH INTERNATIONAL CONFERENCE ON INFORMATION SCIENCE AND CONTROL ENGINEERING (ICISCE 2018), P718, DOI 10.1109/ICISCE.2018.00154
   Liu H, 2019, IEEE T CIRC SYST VID, V29, P486, DOI 10.1109/TCSVT.2017.2782709
   Liu H, 2018, IEEE T INF FOREN SEC, V13, P292, DOI 10.1109/TIFS.2017.2746062
   Liu H, 2017, PATTERN RECOGN, V66, P82, DOI 10.1016/j.patcog.2016.10.026
   Liu H, 2016, AAAI CONF ARTIF INTE, P1258
   Liu M.-Y., 2016, P ADV NEUR INF PROC, P469
   Liu SY, 2017, IEEE INT CONF AUTOMA, P947, DOI 10.1109/FG.2017.117
   Liu X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P258, DOI 10.1109/ICCVW.2015.42
   Lou ZY, 2018, IEEE T PATTERN ANAL, V40, P365, DOI 10.1109/TPAMI.2017.2679739
   Lu JW, 2015, IEEE T IMAGE PROCESS, V24, P5356, DOI 10.1109/TIP.2015.2481327
   Lu JW, 2010, IEEE IMAGE PROC, P1593, DOI 10.1109/ICIP.2010.5650873
   Lu YX, 2017, PROC CVPR IEEE, P1131, DOI 10.1109/CVPR.2017.126
   Miao LS, 2012, INT C PATT RECOG, P967
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554
   Parkhi OM, 2015, Proceedings of the British Machine Vision Conference, DOI DOI 10.5244/C.29.41
   Qi G.-J., 2017, CoRR
   Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341
   Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3
   Shen W, 2018, PROC CVPR IEEE, P2304, DOI 10.1109/CVPR.2018.00245
   Shen Wei, 2017, Advances in Neural Information Processing Systems, V30, P834
   Shu XB, 2015, IEEE I CONF COMP VIS, P3970, DOI 10.1109/ICCV.2015.452
   Sun PH, 2019, IEEE INT CON MULTI, P260, DOI 10.1109/ICME.2019.00053
   Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316
   van der Maaten L, 2014, J MACH LEARN RES, V15, P3221
   Wang XL, 2015, IEEE WINT CONF APPL, P534, DOI 10.1109/WACV.2015.77
   Weng R, 2013, 2013 10 IEEE INT C W, P1
   Wu X, 2019, AAAI CONF ARTIF INTE, P9005
   Xing JH, 2017, PATTERN RECOGN, V66, P106, DOI 10.1016/j.patcog.2017.01.005
   Yan SC, 2009, IEEE T IMAGE PROCESS, V18, P202, DOI 10.1109/TIP.2008.2006400
   Yang H.-F., 2015, P BRIT MACH VIS C
   Yang HY, 2018, PROC CVPR IEEE, P31, DOI 10.1109/CVPR.2018.00011
   Yang X, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P344, DOI 10.1109/ICCVW.2015.53
   Yi D., LECT NOTES COMPUTER, V9005
   Yi ZL, 2017, IEEE I CONF COMP VIS, P2868, DOI 10.1109/ICCV.2017.310
   Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360
   Zhang Y, 2010, PROC CVPR IEEE, P2622, DOI 10.1109/CVPR.2010.5539975
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
NR 81
TC 20
Z9 20
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1808
EP 1822
DI 10.1109/TMM.2020.2969793
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500013
DA 2024-07-18
ER

PT J
AU Xu, YH
   Dai, WR
   Qi, YY
   Zou, JN
   Xiong, HK
AF Xu, Yuhui
   Dai, Wenrui
   Qi, Yingyong
   Zou, Junni
   Xiong, Hongkai
TI Iterative Deep Neural Network Quantization With Lipschitz Constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quantization (signal); Neural networks; Convolution; Computational
   modeling; Semantics; Object detection; Image coding; Network
   compression; iterative quantization; Lipschitz constraint
AB Network quantization offers an effective solution to deep neural network compression for practical usage. Existing network quantization methods cannot theoretically guarantee the convergence. This paper proposes a novel iterative framework for network quantization with arbitrary bit-widths. We present two Lipschitz constraint based quantization strategies, namely width-level network quantization (WLQ) and multi-level network quantization (MLQ), for high-bit and extremely low-bit (ternary) quantization, respectively. In WLQ, Lipschitz based partition is developed to divide parameters in each layer into two groups: one for quantization and the other for re-training to eliminate the quantization loss. WLQ is further extended to MLQ by introducing layer partition to suppress the quantization loss for extremely low bit-widths. The Lipschitz based partition is proven to guarantee the convergence of the quantized networks. Moreover, the proposed framework is complementary to network compression methods such as activation quantization, pruning and efficient network architectures. The proposed framework is evaluated over extensive state-of-the-art deep neural networks, i.e., AlexNet, VGG-16, GoogleNet and ResNet18. Experimental results show that the proposed framework improves the performance of tasks like classification, object detection and semantic segmentation.
C1 [Xu, Yuhui; Xiong, Hongkai] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Dai, Wenrui; Zou, Junni] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Qi, Yingyong] Univ Calif Irvine, Dept Math, Irvine, CA 92697 USA.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University; University
   of California System; University of California Irvine
RP Xiong, HK (corresponding author), Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
EM yuhuixu@sjtu.edu.cn; daiwenrui@sjtu.edu.cn; yqi@uci.edu;
   zou-jn@cs.sjtu.edu.cn; xionghongkai@sjtu.edu.cn
RI Xu, Yuhui/AAW-6061-2021
OI Xiong, Hongkai/0000-0003-4552-0029; Xu, Yuhui/0000-0002-7109-7140
FU National Natural Science Foundation of China [61971285, 61529101,
   61831018, 61425011, 61622112, 61720106001, 61932022, 61931023]; Program
   of Shanghai Academic Research [17XD1401900]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61971285, 61529101, 61831018, 61425011,
   61622112, 61720106001, 61932022, and 61931023; and in part by the
   Program of Shanghai Academic Research under Grant 17XD1401900.
CR Bartlett P. L., 2018, P ADV NEUR INF PROC, P6240
   Cai ZW, 2017, PROC CVPR IEEE, P5406, DOI 10.1109/CVPR.2017.574
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Choi Jungwook, 2018, PACT PARAMETERIZED C
   Dean J., 2015, NIPS DEEP LEARNING R
   Faraone J, 2018, PROC CVPR IEEE, P4300, DOI 10.1109/CVPR.2018.00452
   Gong Y., 2014, INT C LEARN REPR ICL
   Graf, 2017, ARXIV160808710, P1, DOI DOI 10.48550/ARXIV.1608.08710
   Guo Y., 2016, Advances in neural information processing systems, P1379
   Gysel P, 2018, IEEE T NEUR NET LEAR, V29, P5784, DOI 10.1109/TNNLS.2018.2808319
   Gysel PM., 2016, Ristretto: Hardware-oriented approximation of convolutional neural networks
   Han, 2015, LEARNING BOTH WEIGHT
   Han  S., 2015, ARXIV151000149
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Huang Z., 2017, ARXIV170701219
   Jaderberg M., 2014, CORR
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1
   Li Yuxi, 2018, arXiv
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Long J., 2015, P IEEE C COMP VIS PA, P6810
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Lyu HL, 2015, CHIN CONT DECIS CONF, P2885
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shademani A, 2018, CLINICAL APPLICATIONS OF MAGNETIC NANOPARTICLES: DESIGN TO DIAGNOSIS MANUFACTURING TO MEDICINE, P365
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song JK, 2018, AAAI CONF ARTIF INTE, P394
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tang W, 2017, AAAI CONF ARTIF INTE, P2625
   Xu YH, 2018, AAAI CONF ARTIF INTE, P4335
   Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579
   Zhou A., 2017, ARXIV170203044
   Zhou Shuchang, 2016, ARXIV160606160
   Zhu C, 2016, ARXIV PREPRINT ARXIV
NR 46
TC 13
Z9 14
U1 2
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1874
EP 1888
DI 10.1109/TMM.2019.2949857
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500018
DA 2024-07-18
ER

PT J
AU Li, CC
   Wang, JL
   Wang, HW
   Zhao, M
   Li, WJ
   Deng, XT
AF Li, Chenchen
   Wang, Jialin
   Wang, Hongwei
   Zhao, Miao
   Li, Wenjie
   Deng, Xiaotie
TI Visual-Texual Emotion Analysis With Deep Coupled Video and Danmu Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Danmu; deep multimodal learning; emotion analysis
ID CANONICAL CORRELATION-ANALYSIS
AB User emotion analysis toward videos is to automatically recognize the general emotional status of viewers from the multimedia content embedded in the online video stream. Existing works fall into two categories: 1) visual-based methods, which focus on visual content and extract a specific set of features of videos. However, it is generally hard to learn a mapping function from low-level video pixels to high-level emotion space due to great intra-class variance. 2) textual-based methods, which focus on the investigation of user-generated comments associated with videos. The learned word representations by traditional linguistic approaches typically lack emotion information and the global comments usually reflect viewers' high-level understandings rather than instantaneous emotions. To address these limitations, in this paper, we propose to jointly utilize video content and user-generated texts simultaneously for emotion analysis. In particular, we introduce exploiting a new type of user-generated texts, i.e., "danmu," which are real-time comments floating on the video and contain rich information to convey viewers' emotional opinions. To enhance the emotion discriminativeness of words in textual feature extraction, we propose Emotional Word Embedding (EWE) to learn text representations by jointly considering their semantics and emotions. Afterward, we propose a novel visual-textual emotion analysis model with Deep Coupled Video and Danmu Neural networks (DCVDN), in which visual and textual features are synchronously extracted and fused to form a comprehensive representation by deep-canonically-correlated-autoencoder-based multi-view learning. Through extensive experiments on a self-crawled real-world video-danmu dataset, we prove that DCVDN significantly outperforms the state-of-the-art baselines.
C1 [Li, Chenchen; Wang, Hongwei] Shanghai Jiao Tong Univ, Dept Comp Sci, Shanghai 200240, Peoples R China.
   [Wang, Jialin; Zhao, Miao; Li, Wenjie] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Deng, Xiaotie] Peking Univ, Sch Elect Engn & Comp Sci, Beijing 100871, Peoples R China.
C3 Shanghai Jiao Tong University; Hong Kong Polytechnic University; Peking
   University
RP Li, CC (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci, Shanghai 200240, Peoples R China.
EM lcc1992@sjtu.edu.cn; wangjialin@hust.edu.cn; wanghongwei55@gmail.com;
   mzhao.ny@gmail.com; cswjli@comp.polyu.edu.hk; xiaotie@pku.edu.cn
RI WANG, HONGWEI/D-6507-2016; Li, Wenjie/AAQ-7622-2020; Li,
   Wenjie/IZQ-0727-2023
OI Li, Wenjie/0000-0002-7360-8864; Li, Wenjie/0000-0002-7360-8864; Li,
   Chenchen/0000-0002-9401-765X; Deng, Xiaotie/0000-0002-5282-6467
FU National Key Research and Development Program of China [2017YFB0701900]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2017YFB0701900.
CR [Anonymous], 2011, P ICML
   [Anonymous], 2001, EMOTIONS SOCIAL PSYC
   [Anonymous], 2015, P 1 INT WORKSH AFF S, DOI 10.1145/2813524.2813530
   [Anonymous], 2016, P ADV NEUR INF PROC
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Blitzer J., 2007, Proceedings of the 45th annual meeting of the association of computational linguistics, V45, P440
   Borth D., 2013, P 21 ACM INT C MULT, P459
   Campos V, 2017, IMAGE VISION COMPUT, V65, P15, DOI 10.1016/j.imavis.2017.01.011
   Cao DL, 2016, MULTIMEDIA SYST, V22, P479, DOI 10.1007/s00530-014-0407-8
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   El Rahman SA, 2019, 2019 INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION SCIENCES (ICCIS), P336, DOI 10.1109/iccisci.2019.8716464
   Frome Andrea, 2013, DEVISE DEEP VISUAL S, P1, DOI DOI 10.5555/2999792.2999849
   Gao L, 2018, IEEE T IMAGE PROCESS, V27, P1951, DOI 10.1109/TIP.2017.2765820
   Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2016, IEEE T MULTIMEDIA, V18, P1363, DOI 10.1109/TMM.2016.2558463
   Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kim HR, 2018, IEEE T MULTIMEDIA, V20, P2980, DOI 10.1109/TMM.2018.2827782
   Kiros R, 2014, Arxiv, DOI arXiv:1411.2539
   Ko E, 2015, PROCEEDINGS OF 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC), P156, DOI 10.1109/ICCI-CC.2015.7259380
   Kouloumpis E., 2011, TWITTER SENTIMENT AN, P538
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Li FT, 2010, AAAI CONF ARTIF INTE, P1371
   Liu SD, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P82, DOI 10.1145/2964284.2967187
   Liu Y, 2015, AAAI CONF ARTIF INTE, P2418
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Rasiwasia N., 2010, P 18 INT C MULT FIR, P251, DOI 10.1145/1873951.1873987
   Reisinger J., 2010, HLT-NAACL, P109
   Siersdorfer S., 2010, ACM MM, P715
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Socher R, 2010, PROC CVPR IEEE, P966, DOI 10.1109/CVPR.2010.5540112
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tang DY, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1555
   Wang HW, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P347, DOI 10.1145/3132847.3132889
   Wang Jingwen., 2016, IJCAI, P3484
   Wang JZ, 2018, IEEE T MULTIMEDIA, V20, P2578, DOI 10.1109/TMM.2018.2855081
   Wang M., 2014, P INT C INT MULT COM, P76, DOI [10.1145/2632856.2632912, DOI 10.1145/2632856.2632912]
   Wang WR, 2015, PR MACH LEARN RES, V37, P1083
   Yan HC, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL II, PROCEEDINGS, P379, DOI 10.1109/AICI.2009.446
   Yang JF, 2018, IEEE T MULTIMEDIA, V20, P2513, DOI 10.1109/TMM.2018.2803520
   You QZ, 2017, AAAI CONF ARTIF INTE, P231
   You QZ, 2016, AAAI CONF ARTIF INTE, P308
   You QZ, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P13, DOI 10.1145/2835776.2835779
   You QZ, 2015, AAAI CONF ARTIF INTE, P381
   Yuan J., 2013, Proceedings of the Second International Workshop on Issues of Sentiment Discovery and Opinion Mining, page, P10
   Zhang HM, 2018, IEEE T MULTIMEDIA, V20, P2824, DOI 10.1109/TMM.2018.2808760
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
NR 51
TC 12
Z9 12
U1 3
U2 74
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2020
VL 22
IS 6
BP 1634
EP 1646
DI 10.1109/TMM.2019.2946477
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LU8YA
UT WOS:000538033100021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Song, WF
   Li, S
   Liu, J
   Hao, AM
   Zhao, QP
   Qin, H
AF Song, Wenfeng
   Li, Shuai
   Liu, Ji
   Hao, Aimin
   Zhao, Qinping
   Qin, Hong
TI Contextualized CNN for Scene-Aware Depth Estimation From Single RGB
   Image
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Semantics; Training; Task analysis; Feature extraction;
   Decoding; Convolution; Depth Estimation; CNN; Single RGB Image;
   Contextualization; Scene-Aware Algorithm
AB Directly benefited from deep learning techniques, depth estimation from single image has gained great momentum in recent years. However, most of the existing approaches treat depth prediction as an isolated problem without taking into consideration high-level semantic context information, which results in inefficient utilization of training dataset and unavoidably requires a large number of captured depth data during the training phase. To ameliorate, this paper develops a novel scene-aware contextualized convolution neural network (CCNN), which characterizes the semantic context relationship at the class-level and refines depth at the pixel-level. Our newly-proposed CCNN is built upon the intrinsic exploitation of context-dependent depth association, including inner-object continuous depth and inter-object depth change priors nearby. Specifically, rather than conducting regression on depth in single CNN, we make the first attempt to integrate both class-level and pixel-level conditional random fields (CRFs) based probabilistic graphical model into the powerful CNN framework to simultaneously learn different-level features within the same CNN layer. With our CCNN, the former model will guide the latter one to learn the contextualized RGB-Depth mapping. Hence, CCNN has desirable properties in both class-level integrity and pixel-level discrimination, which makes it ideal to share such two-level convolutional features in parallel during the end-to-end training with the commonly-used back-propagation algorithm. We conduct extensive experiments and comprehensive evaluations on public benchmarks involving various indoor and outdoor scenes, and all the experiments confirm that, our method outperforms the state-of-the-art depth estimation methods, especially for the cases where only small-scale training data are readily available.
C1 [Song, Wenfeng; Li, Shuai; Liu, Ji; Hao, Aimin; Zhao, Qinping] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Shuai] Beihang Univ, Qingdao Res Inst, Qingdao 266000, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Beihang University; Beihang University; State University of New York
   (SUNY) System; State University of New York (SUNY) Stony Brook
RP Li, S (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.
EM songwenfenga@gmail.com; lishuai@buaa.edu.cn; liujiu@buaa.edu.cn;
   ham@buaa.edu.cn; zhaoqp@buaa.edu.cn; qin@cs.stonybrook.edu
OI QIN, HONG/0000-0001-7699-1355
FU National Key R&D Program of China [2017YFF0106407, 2017YFB1002602];
   National Natural Science Foundation of China [61672077, 61532002];
   Applied Basic Research Program of Qingdao [161013xx]; National Science
   Foundation of USA [IIS-0949467, IIS-1047715, IIS-1715985, IIS-1049448];
   Capital Health Research and Development of Special [2016-1-4011];
   Fundamental Research Funds for the Central Universities; Beijing Natural
   Science Foundation-Haidian Primitive Innovation Joint Fund [L182016]
FX This work was supported in part by National Key R&D Program of China
   (2017YFB1002602), in part by National Key R&D Program of China
   (2017YFF0106407), in part by National Natural Science Foundation of
   China (61672077 and 61532002), in part by Applied Basic Research Program
   of Qingdao (161013xx), in part by National Science Foundation of USA
   (IIS-0949467, IIS-1047715, IIS-1715985, and IIS-1049448), in part by
   Capital Health Research and Development of Special 2016-1-4011, in part
   by Fundamental Research Funds for the Central Universities, and in part
   by Beijing Natural Science Foundation-Haidian Primitive Innovation Joint
   Fund (L182016). The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Mohammed Daoudi.
CR [Anonymous], 2010, CVPR, DOI DOI 10.1109/CVPR.2010.5539823
   [Anonymous], P COMP VIS PATT REC
   Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642
   Cao YZH, 2018, IEEE T CIRC SYST VID, V28, P3174, DOI 10.1109/TCSVT.2017.2740321
   Chakrabarti Ayan, 2016, Advances in Neural Information Processing Systems, P2658
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen Weifeng, 2016, ADV NEURAL INFORM PR, V29, P730, DOI DOI 10.5555/3157096.3157178
   Dong WS, 2017, IEEE T MULTIMEDIA, V19, P293, DOI 10.1109/TMM.2016.2613824
   Eigen D, 2014, ADV NEUR IN, V27
   Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304
   Gaidon A, 2016, PROC CVPR IEEE, P4340, DOI 10.1109/CVPR.2016.470
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Güney F, 2015, PROC CVPR IEEE, P4165, DOI 10.1109/CVPR.2015.7299044
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56
   Kendall Alex, 2017, ADV NEURAL INFORM PR, V30, DOI DOI 10.5555/3295222.3295309
   Krahenbuhl P., 2012, NIPS
   Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19
   Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32
   Lee JH, 2018, PROC CVPR IEEE, P330, DOI 10.1109/CVPR.2018.00042
   Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715
   Li CH, 2019, IEEE T MULTIMEDIA, V21, P1464, DOI 10.1109/TMM.2018.2882085
   Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152
   Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   MENZE M, 2015, PROC CVPR IEEE, P3061, DOI DOI 10.1109/CVPR.2015.7298925
   Pan Y, 2017, IEEE T MULTIMEDIA, V19, P685, DOI 10.1109/TMM.2016.2646179
   Qi CR, 2017, ADV NEUR IN, V30
   Ranftl R, 2016, PROC CVPR IEEE, P4058, DOI 10.1109/CVPR.2016.440
   Roy A, 2016, PROC CVPR IEEE, P5506, DOI 10.1109/CVPR.2016.594
   Saxena A, 2008, INT J COMPUT VISION, V76, P53, DOI 10.1007/s11263-007-0071-y
   Saxena A, 2007, IEEE I CONF COMP VIS, P1
   Shen FL, 2017, PROC CVPR IEEE, P5178, DOI 10.1109/CVPR.2017.550
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang L, 2014, IEEE T MULTIMEDIA, V16, P1905, DOI 10.1109/TMM.2014.2341599
   Wang P, 2016, ADV NEUR IN, V29
   Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897
   Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652
   Xu D, 2018, PROC CVPR IEEE, P675, DOI 10.1109/CVPR.2018.00077
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Zhang ZY, 2015, IEEE I CONF COMP VIS, P2614, DOI 10.1109/ICCV.2015.300
   Zheng CX, 2018, LECT NOTES COMPUT SC, V11211, P798, DOI 10.1007/978-3-030-01234-2_47
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
   Zhou TH, 2015, IEEE I CONF COMP VIS, P3469, DOI 10.1109/ICCV.2015.396
NR 49
TC 17
Z9 18
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1220
EP 1233
DI 10.1109/TMM.2019.2941776
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200009
OA hybrid
DA 2024-07-18
ER

PT J
AU Xiang, T
   Yang, Y
   Guo, SW
AF Xiang, Tao
   Yang, Ying
   Guo, Shangwei
TI Blind Night-Time Image Quality Assessment: Subjective and Objective
   Approaches
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Measurement; Distortion; Feature extraction; Image quality;
   Visualization; Image databases; Blind image quality assessment; natural
   night-time images; superpixel; ranking-based weighting; gray-level
   co-occurrence matrix
ID DISTORTED IMAGES; GRADIENT; STATISTICS; DATABASE; COLOR
AB Blind image quality assessment (BIQA) aims to develop quantitative measures to automatically and accurately estimate the visual quality of an image without any prior information about its reference image. This issue has been attracting a great deal of attention for a long time; however, little work has been done on night-time images, which are crucially important for consumer photography and practical applications such as automated driving systems. In this paper, to the best of our knowledge, we conduct the first exploration on subjective and objective quality assessment of night-time images. First, we build a large-scale natural night-time image database (NNID) containing 2240 images with 448 different image contents captured by different photographic equipment in real-world scenarios. Subsequently, we carry out a subjective experiment to evaluate the perceptual quality of all the images in the NNID database. Thereafter, we perform objective assessment of night-time images by proposing a blind night-time image quality assessment metric using brightness and texture features (BNBT). Finally, extensive experiments are conducted to evaluate the performance and efficiency of the proposed BNBT metric on the NNID database. The experimental results demonstrate that this metric outperforms existing state-of-the-art BIQA methods in terms of all evaluation criteria and has an acceptable computational cost at the same time. We have made the NNID database publicly available for downloading at https://sites.google.com/site/xiangtaooo/.
C1 [Xiang, Tao; Yang, Ying] Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
   [Guo, Shangwei] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Chongqing University; Nanyang Technological University
RP Xiang, T (corresponding author), Chongqing Univ, Coll Comp Sci, Chongqing 400044, Peoples R China.
EM txiang@cqu.edu.cn; yingyang@cqu.edu.cn; shangwei.guo@ntu.edu.sg
RI Xiang, Tao/N-3706-2016
OI Xiang, Tao/0000-0002-9439-4623; Guo, Shangwei/0000-0002-6443-5308;
   Xiang, Tao/0000-0002-0022-3082
FU National Natural Science Foundation of China [61672118, 61932006]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61672118 and 61932006.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment
   [Anonymous], 2002, METH SUBJ ASS QUAL T
   Chow LS, 2016, BIOMED SIGNAL PROCES, V27, P145, DOI 10.1016/j.bspc.2016.02.006
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Dutta J, 2013, THERANOSTICS, V3, P741, DOI 10.7150/thno.6815
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fang YM, 2018, IEEE T IMAGE PROCESS, V27, P1600, DOI 10.1109/TIP.2017.2781307
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Gangapure VN, 2018, IEEE T CIRC SYST VID, V28, P1263, DOI 10.1109/TCSVT.2017.2662743
   Geusebroek JM, 2001, IEEE T PATTERN ANAL, V23, P1338, DOI 10.1109/34.977559
   Geusebroek JM, 2000, LECT NOTES COMPUT SC, V1842, P331
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T CYBERNETICS, V47, P4559, DOI 10.1109/TCYB.2016.2575544
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T BROADCAST, V61, P520, DOI 10.1109/TBC.2015.2459851
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   Hu B, 2017, SIGNAL PROCESS-IMAGE, V58, P165, DOI 10.1016/j.image.2017.08.003
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jin XD, 2017, IEEE T GEOSCI REMOTE, V55, P4285, DOI 10.1109/TGRS.2017.2690445
   Kalayeh MM, 2013, IEEE T NUCL SCI, V60, P1609, DOI 10.1109/TNS.2013.2257183
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Liu LX, 2014, SIGNAL PROCESS-IMAGE, V29, P856, DOI 10.1016/j.image.2014.06.006
   Liu TJ, 2018, IEEE T IMAGE PROCESS, V27, P1138, DOI 10.1109/TIP.2017.2771422
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Nakhaie AA, 2011, CAN CON EL COMP EN, P121, DOI 10.1109/CCECE.2011.6030422
   Peng CL, 2017, IEEE T CIRC SYST VID, V27, P288, DOI 10.1109/TCSVT.2015.2502861
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10
   Saad M.A., 2015, Colour and Visual Computing Symposium, P1, DOI DOI 10.1109/CVCS.2015.7274887
   Saad M. A, 2016, PROC INT S ELECT IMA, P1
   Saad MA, 2015, IEEE SIGNAL PROC LET, V22, P1516, DOI 10.1109/LSP.2015.2406861
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Shao F, 2018, IEEE T SYST MAN CY-S, V48, P1521, DOI 10.1109/TSMC.2017.2676180
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang Z, 2011, IEEE SIGNAL PROC MAG, V28, P137, DOI 10.1109/MSP.2011.942295
   Wu QB, 2015, J VIS COMMUN IMAGE R, V32, P205, DOI 10.1016/j.jvcir.2015.08.009
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xu M, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P362, DOI 10.1109/SIPROCESS.2016.7888285
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Yang GY, 2017, IEEE ACCESS, V5, P23146, DOI 10.1109/ACCESS.2017.2764126
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P4408, DOI 10.1109/TIP.2015.2465145
   Yue GH, 2019, IEEE T IND ELECTRON, V66, P3784, DOI 10.1109/TIE.2018.2851984
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang J, 2011, SIGNAL PROCESS-IMAGE, V26, P13, DOI 10.1016/j.image.2010.11.003
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhu X, 2010, IEEE T IMAGE PROCESS, V19, P3116, DOI 10.1109/TIP.2010.2052820
NR 56
TC 23
Z9 24
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2020
VL 22
IS 5
BP 1259
EP 1272
DI 10.1109/TMM.2019.2938612
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA LJ3VZ
UT WOS:000530097200012
DA 2024-07-18
ER

PT J
AU Krasula, L
   Baveye, Y
   Le Callet, P
AF Krasula, Lukas
   Baveye, Yoann
   Le Callet, Patrick
TI Training Objective Image and Video Quality Estimators Using Multiple
   Databases
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Quality assessment; Databases; Training; Video recording; Measurement;
   Visualization; Biological neural networks; Image quality assessment;
   video quality assessment; objective quality metrics; machine learning
ID OPTIMIZATION; MODELS; AREAS
AB Machine learning (ML) is an essential part of recent advances in computer science. To fully exploit its potential, ML-based algorithms require a considerable amount of annotated data to be used for training. This represents a severe limitation in the field of image and video quality assessment since obtaining large-scale annotated databases is time-consuming and expensive. Moreover, the resulting quality estimators are mainly restricted only to the usecases included in the dataset used for their training. This paper proposes a strategy allowing for combination of multiple databases for training of objective image and video quality assessment algorithms. Using this strategy, the algorithms can be trained using all of the existing relevant databases together which allows to increase the amount of data-points and usecases in orders of magnitude. The potential of the proposed method is demonstrated by re-training the combination of features from Video Multimethod Assessment Fusion (VMAF) algorithm resulting in the significant improvement of its performance with respect to 20 video databases.
C1 [Krasula, Lukas; Le Callet, Patrick] Univ Nantes, IPI, LS2N CNRS UMR 6004, F-44306 Nantes, France.
   [Baveye, Yoann] Capacites SAS, F-44200 Nantes, France.
C3 Nantes Universite
RP Krasula, L (corresponding author), Univ Nantes, IPI, LS2N CNRS UMR 6004, F-44306 Nantes, France.
EM lukas.krasula@univ-nantes.fr; yoann.baveye@capacites.fr;
   patrick.lecallet@univ-nantes.fr
RI Le Callet, Patrick/F-5772-2010
CR [Anonymous], 2012, P SPIE HUMAN VISION
   [Anonymous], [No title captured]
   [Anonymous], 1996, Signal detection theory and ROC analysis in psychology and diagnostics: Collected papers
   [Anonymous], P WORKSH QOE MULT CO
   [Anonymous], 2015, 2015 7 INT WORKSHOP, DOI DOI 10.1109/QOMEX.2015.7148114
   [Anonymous], 2004, METHOD SPECIFYING AC
   [Anonymous], 2016, P IEEE INT C QUAL MU
   [Anonymous], 2012, METHODOLOGY SUBJECTI
   [Anonymous], P 5 INT WORKSH VID P
   [Anonymous], 3 INT WORKSH IM MED
   Babu RV, 2007, SIGNAL PROCESS, V87, P1493, DOI 10.1016/j.sigpro.2006.12.014
   Barkowsky M., 2012, QOEMCS 2012 3 WORKSH, P1
   Barri A, 2017, IEEE J-STSP, V11, P196, DOI 10.1109/JSTSP.2016.2637164
   Bishop C. M., 1995, NEURAL NETWORKS PATT
   Cheon M, 2018, IEEE T CIRC SYST VID, V28, P1467, DOI 10.1109/TCSVT.2017.2683504
   DELONG ER, 1988, BIOMETRICS, V44, P837, DOI 10.2307/2531595
   Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521
   Gastaldo P, 2002, IEEE T NEURAL NETWOR, V13, P939, DOI 10.1109/TNN.2002.1021894
   Gastaldo P, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-54
   Gu K, 2019, IEEE T IND ELECTRON, V66, P3176, DOI 10.1109/TIE.2018.2840515
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Jakhetiya V, 2019, IEEE T IND INFORM, V15, P4120, DOI 10.1109/TII.2018.2888861
   Krasula L., 2017, ACAD PRESS LIB SIGNA, V6
   Krasula L, 2017, IEEE T IMAGE PROCESS, V26, P1496, DOI 10.1109/TIP.2017.2651374
   Le Callet P, 2006, IEEE T NEURAL NETWOR, V17, P1316, DOI 10.1109/TNN.2006.879766
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Li Z., 2016, TECH REP
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu TJ, 2013, IEEE T IMAGE PROCESS, V22, P1793, DOI 10.1109/TIP.2012.2236343
   Min XK, 2019, IEEE T INTELL TRANSP, V20, P2879, DOI 10.1109/TITS.2018.2868771
   Min XK, 2018, IEEE T MULTIMEDIA, V20, P2049, DOI 10.1109/TMM.2017.2788206
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Min XK, 2018, IEEE T BROADCAST, V64, P508, DOI 10.1109/TBC.2018.2816783
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2012, IEEE J-STSP, V6, P652, DOI 10.1109/JSTSP.2012.2212417
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narwaria M, 2010, IEEE T NEURAL NETWOR, V21, P515, DOI 10.1109/TNN.2010.2040192
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Pechard S, 2006, IEEE IMAGE PROC, P409, DOI 10.1109/ICIP.2006.312480
   Pitrey Y., 2010, P SPIE APPL DIGITAL, V7798, P1
   Powell M J D., 2007, Cambridge University Technical Report DAMTP 2007/NA03
   POWELL MJD, 1993, MATH APPL, V275, P51
   RATCLIFF R, 1990, PSYCHOL REV, V97, P285, DOI 10.1037/0033-295X.97.2.285
   Rios LM, 2013, J GLOBAL OPTIM, V56, P1247, DOI 10.1007/s10898-012-9951-y
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Seshadrinathan K, 2010, IEEE T IMAGE PROCESS, V19, P1427, DOI 10.1109/TIP.2010.2042111
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Stütz T, 2014, IEEE T MULTIMEDIA, V16, P1337, DOI 10.1109/TMM.2014.2310595
   Sun X, 2014, IEEE SIGNAL PROC LET, V21, P1389, DOI 10.1109/LSP.2014.2337313
   Suresh S, 2009, APPL SOFT COMPUT, V9, P541, DOI 10.1016/j.asoc.2008.07.005
   TAYLOR JMG, 1987, BIOMETRICS, V43, P409, DOI 10.2307/2531822
   Vapnik V, 1997, ADV NEUR IN, V9, P281
   Wales DJ, 1997, J PHYS CHEM A, V101, P5111, DOI 10.1021/jp970984n
   Wang JM, 2003, COMP AID CH, V15, P1
   Xu Long., 2015, Visual quality assessment by machine learning
NR 59
TC 6
Z9 7
U1 0
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 961
EP 969
DI 10.1109/TMM.2019.2935687
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400011
DA 2024-07-18
ER

PT J
AU Fan, XT
   Lei, JJ
   Fang, YM
   Huang, QM
   Ling, N
   Hou, CP
AF Fan, Xiaoting
   Lei, Jianjun
   Fang, Yuming
   Huang, Qingming
   Ling, Nam
   Hou, Chunping
TI Stereoscopic Image Stitching via Disparity-Constrained Warping and
   Blending
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Stereo image processing; Distortion; Two dimensional displays;
   Visualization; Minimization methods; Shape; Feature extraction;
   Stereoscopic image; image stitching; disparity consistency;
   multi-constraint warping; seam-cutting and blending
AB As a significant branch of virtual reality, stereoscopic image stitching aims to generating wide perspectives and natural-looking scenes. Existing 2D image stitching methods cannot be successfully applied to the stereoscopic images without considering the disparity consistency of stereoscopic images. To address this issue, this paper presents a stereoscopic image stitching method based on disparity-constrained warping and blending, which could avoid visual distortion and preserve disparity consistency. First, a point-line-driven homography based disparity minimization method is designed to pre-align the left and right images and reduce vertical disparity. Afterwards, a multi-constraint warping is proposed to further align the left and right images, where the initial disparity map is introduced to control the consistency of disparities. Finally, a disparity consistency seam-cutting and blending method is presented to determine the optimal seam and conduct stereoscopic image stitching. Experimental results demonstrate that the proposed method achieves competitive performance compared with other state-of-the-art methods.
C1 [Fan, Xiaoting; Lei, Jianjun; Hou, Chunping] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Informat Management, Nanchang 330032, Jiangxi, Peoples R China.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp Sci & Technol, Beijing 101408, Peoples R China.
   [Huang, Qingming] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; Jiangxi University of Finance & Economics; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Santa Clara University
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM xtfan@tju.edu.cn; jjlei@tju.edu.cn; fa0001ng@e.ntu.edu.sg;
   qmhuang@ucas.ac.cn; nling@scu.edu; hcp@tju.edu.cn
RI Fan, Xiaoting/IWM-7573-2023; Lei, Jianjun/P-2539-2018
OI Fan, Xiaoting/0000-0002-5437-3625; 
FU National Key R&D Program of China [2017YFB1002900]; National Natural
   Science Foundation of China [61722112, 61520106002, 61620106009,
   U1636214]; Natural Science Foundation of Tianjin [18ZXZNGX00110,
   18JCJQJC45800]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002900, in part by the National Natural Science
   Foundation of China under Grants 61722112, 61520106002, 61620106009, and
   U1636214, and in part by the Natural Science Foundation of Tianjin under
   Grants 18ZXZNGX00110 and 18JCJQJC45800. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Balakrishnan Prabhakaran.
CR Allène C, 2008, INT C PATT RECOG, P2539
   Bellavia F, 2018, IEEE T IMAGE PROCESS, V27, P735, DOI 10.1109/TIP.2017.2757262
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Brown M, 2007, INT J COMPUT VISION, V74, P59, DOI 10.1007/s11263-006-0002-3
   Buades A, 2015, SIAM J IMAGING SCI, V8, P888, DOI 10.1137/140984269
   Chai Q., 2016, P IEEE INT C MULTIME, P1
   Chang CH, 2014, PROC CVPR IEEE, P3254, DOI 10.1109/CVPR.2014.422
   Chen YS, 2016, LECT NOTES COMPUT SC, V9909, P186, DOI 10.1007/978-3-319-46454-1_12
   Dubrofsky E., 2011, P INT S VIS COMP, P202
   Eden A., 2006, P 2006 IEEE COMP SOC, VVolume 2, P2498
   Fang YM, 2019, IEEE T IMAGE PROCESS, V28, P5253, DOI 10.1109/TIP.2019.2916766
   Fang YM, 2012, IEEE T IMAGE PROCESS, V21, P3888, DOI 10.1109/TIP.2012.2199126
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Gao JH, 2011, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2011.5995433
   Gurrieri L. E., 2013, P SOC PHOTO-OPT INS, V8648, P1
   Jiang YT, 2015, Proceedings of the Second International Symposium - Management, Innovation and Development, P42
   Joo K, 2015, IEEE IMAGE PROC, P1175, DOI 10.1109/ICIP.2015.7350985
   Lei JJ, 2018, IEEE T CIRC SYST VID, V28, P3333, DOI 10.1109/TCSVT.2017.2749146
   Lei JJ, 2018, IEEE T CIRC SYST VID, V28, P706, DOI 10.1109/TCSVT.2016.2617332
   Lei JJ, 2015, IEEE T MULTIMEDIA, V17, P457, DOI 10.1109/TMM.2015.2400823
   Li C, 2016, NEUROCOMPUTING, V214, P829, DOI 10.1016/j.neucom.2016.07.004
   Li J, 2018, IEEE T MULTIMEDIA, V20, P1672, DOI 10.1109/TMM.2017.2777461
   Li J, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P1, DOI 10.1109/ACPR.2015.7486454
   Li N, 2018, SIGNAL IMAGE VIDEO P, V12, P967, DOI 10.1007/s11760-018-1241-9
   Li N, 2018, IEEE T MULTIMEDIA, V20, P1365, DOI 10.1109/TMM.2017.2771566
   Li SW, 2015, IEEE I CONF COMP VIS, P4283, DOI 10.1109/ICCV.2015.487
   Lin CC, 2015, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2015.7298719
   Lin KM, 2016, LECT NOTES COMPUT SC, V9907, P370, DOI 10.1007/978-3-319-46487-9_23
   Lin WY, 2011, PROC CVPR IEEE, P345, DOI 10.1109/CVPR.2011.5995314
   Liu F, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531350
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Peleg S, 2001, IEEE T PATTERN ANAL, V23, P279, DOI 10.1109/34.910880
   Plotz T., 2016, P IEEE INT C COMP VI, P2030
   Richardt C, 2013, PROC CVPR IEEE, P1256, DOI 10.1109/CVPR.2013.166
   Szeliski R, 2006, FOUND TRENDS COMPUT, V2, P1, DOI 10.1561/0600000009
   Tang H, 2012, IEEE T CIRC SYST VID, V22, P295, DOI 10.1109/TCSVT.2011.2178729
   Wagenmaker AJ, 2017, IEEE IMAGE PROC, P1457, DOI 10.1109/ICIP.2017.8296523
   Wang ZJ, 2018, IEEE T CIRC SYST VID, V28, P3053, DOI 10.1109/TCSVT.2017.2706197
   Weng YL, 2006, VISUAL COMPUT, V22, P653, DOI 10.1007/s00371-006-0054-y
   Xiang TZ, 2016, INT C PATT RECOG, P4178, DOI 10.1109/ICPR.2016.7900289
   Xu B, 2017, IEEE IMAGE PROC, P1467, DOI 10.1109/ICIP.2017.8296525
   Yan WQ, 2017, IEEE T CIRC SYST VID, V27, P1934, DOI 10.1109/TCSVT.2016.2564838
   Yang JY, 2015, IEEE T CYBERNETICS, V45, P913, DOI 10.1109/TCYB.2014.2340032
   Yang JY, 2014, IEEE T IMAGE PROCESS, V23, P3443, DOI 10.1109/TIP.2014.2329776
   Yuan H, 2014, IEEE T CIRC SYST VID, V24, P443, DOI 10.1109/TCSVT.2013.2280071
   Zaragoza J., 2013, PATTERN ANAL MACHINE, P1
   Zhang F, 2015, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2015.7298811
   Zhang F, 2014, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2014.423
NR 48
TC 17
Z9 18
U1 2
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 655
EP 665
DI 10.1109/TMM.2019.2932573
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700007
DA 2024-07-18
ER

PT J
AU Jiang, QR
   Li, S
   Zhu, ZL
   Bai, H
   He, XX
   de Lamare, RC
AF Jiang, Qianru
   Li, Sheng
   Zhu, Zhillui
   Bai, Huang
   He, Xiongxiong
   de Lamare, Rodrigo C.
TI Design of Compressed Sensing System With Probability-Based Prior
   Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sensors; Dictionaries; Sparse matrices; Matching pursuit algorithms;
   Optimization; Coherence; Noise measurement; Compressed sensing; prior
   information; probability; sensing matrix; sparse recovery; optimization
   techniques
ID SPARSE REPRESENTATION; OPTIMIZED PROJECTIONS; MATRIX; RECONSTRUCTION;
   DICTIONARIES; FRAMES
AB This paper deals with the design of a sensing matrix along with a sparse recovery algorithm by utilizing the probability-based prior information for compressed sensing systems. With the knowledge of the probability for each atom of the dictionary being used, a diagonal weighted matrix is obtained and then the sensing matrix is designed by minimizing a weighted function such that the Gram of the equivalent dictionary is as close to the Gram of dictionary as possible. An analytical solution for the corresponding sensing matrix is derived that requires low computational complexity. We also exploit this prior information through the sparse recovery stage and propose a probability-driven orthogonal matching pursuit algorithm that improves the accuracy of the recovery. Simulations for synthetic data and application scenarios of video streaming are carried out to compare the performance of the proposed methods with some existing algorithms. The results reveal that the proposed compressed sensing (CS) approach outperforms existing CS systems.
C1 [Jiang, Qianru; Li, Sheng; He, Xiongxiong] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310014, Peoples R China.
   [Zhu, Zhillui] Johns Hopkins Univ, Ctr Imaging Sci, Math Inst Data Sci, Baltimore, MD 21218 USA.
   [Bai, Huang] Hangzhou Normal Univ, Coll Informat Sci & Engn, Hangzhou 311121, Peoples R China.
   [de Lamare, Rodrigo C.] Univ York, Dept Elect Engn, York YO10 5DD, N Yorkshire, England.
   [de Lamare, Rodrigo C.] Pontificia Univ Catolica Rio de Janeiro, CETUC, BR-22451900 Rio De Janeiro, Brazil.
C3 Zhejiang University of Technology; Johns Hopkins University; Hangzhou
   Normal University; University of York - UK
RP Li, S (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310014, Peoples R China.
EM jqr08141989@163.com; shengli@zjut.edu.cn; zzhu29@jhu.edu;
   bh667770@163.com; hxx@zjut.edu.cn; rcd1500@ohm.york.ac.uk
RI Zhu, Zhihui/AAR-5029-2020; Li, Sheng/AAA-1540-2022
OI Zhu, Zhihui/0000-0002-3856-0375; Li, Sheng/0000-0003-2144-958X; He,
   Xiongxiong/0000-0002-5806-1047; de Lamare, Rodrigo/0000-0003-2322-6451;
   Bai, Huang/0000-0003-1875-468X
FU National Science Foundation of China [61503339, 61801159, 61873239,
   61771430, 61675183]; Zhejiang National Science Foundation [LY18F010023];
   ConselhoNacional deDesenvolvimento Cientifico e Tecnologico (CNPq)
   Foundation; Fundacao de Amparo a Pesquisa do Estado do Rio de Janeiro
   (FAPERJ) Foundation
FX This work was supported in part by the National Science Foundation of
   China under Grant 61503339, Grant 61801159, Grant 61873239, Grant
   61771430, and Grant 61675183, in part by the Zhejiang National Science
   Foundation under Grant LY18F010023, in part by the ConselhoNacional
   deDesenvolvimento Cientifico e Tecnologico (CNPq) Foundation, and in
   part by the Fundacao de Amparo a Pesquisa do Estado do Rio de Janeiro
   (FAPERJ) Foundation. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Xiaoqing Zhu.
CR Abolghasemi V, 2012, SIGNAL PROCESS, V92, P999, DOI 10.1016/j.sigpro.2011.10.012
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], [No title captured]
   [Anonymous], [No title captured]
   [Anonymous], PETS 2006 BENCHMARK
   Bai H, 2016, IEEE T MULTIMEDIA, V18, P2040, DOI 10.1109/TMM.2016.2595261
   Bai H, 2015, IEEE T SIGNAL PROCES, V63, P1581, DOI 10.1109/TSP.2015.2399864
   Bajwa W, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P134
   Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979
   Candès EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731
   Chen W, 2012, IEEE SIGNAL PROC LET, V19, P8, DOI 10.1109/LSP.2011.2173675
   Cleju N, 2014, APPL COMPUT HARMON A, V36, P495, DOI 10.1016/j.acha.2013.08.005
   Cui MS, 2016, PATTERN RECOGN LETT, V84, P120, DOI 10.1016/j.patrec.2016.08.017
   Ding X, 2017, IEEE T SIGNAL PROCES, V65, P3632, DOI 10.1109/TSP.2017.2699639
   Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582
   Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100
   Duarte MF, 2011, IEEE T SIGNAL PROCES, V59, P4053, DOI 10.1109/TSP.2011.2161982
   Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459
   Elad M, 2007, IEEE T SIGNAL PROCES, V55, P5695, DOI 10.1109/TSP.2007.900760
   Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624
   Horn R. A., 2012, MATRIX ANAL
   Jain S, 2013, CONF REC ASILOMAR C, P163, DOI 10.1109/ACSSC.2013.6810251
   Ji SH, 2008, IEEE T SIGNAL PROCES, V56, P2346, DOI 10.1109/TSP.2007.914345
   Lee K, 2011, IEEE T MED IMAGING, V30, P1076, DOI 10.1109/TMI.2010.2097275
   Li B, 2017, SIGNAL PROCESS, V135, P36, DOI 10.1016/j.sigpro.2016.11.024
   Li G, 2018, DIGIT SIGNAL PROCESS, V73, P62, DOI 10.1016/j.dsp.2017.10.023
   Li G, 2015, IEEE T IMAGE PROCESS, V24, P5389, DOI 10.1109/TIP.2015.2479474
   Li G, 2013, IEEE T SIGNAL PROCES, V61, P2887, DOI 10.1109/TSP.2013.2253776
   MacKay D., 2003, INFORM THEORY INFERE
   Mairal J., 2009, P 26 ANN INT C MACHI, P689, DOI 10.1145/1553374.1553463
   Metzler CA, 2016, IEEE T INFORM THEORY, V62, P5117, DOI 10.1109/TIT.2016.2556683
   Minaee S, 2019, IEEE T IMAGE PROCESS, V28, P3192, DOI 10.1109/TIP.2019.2894966
   Minaee S, 2017, IEEE INT SYMP CIRC S
   Minaee S, 2016, IEEE J EM SEL TOP C, V6, P573, DOI 10.1109/JETCAS.2016.2597701
   Miosso CJ, 2013, IEEE T SIGNAL PROCES, V61, P2150, DOI 10.1109/TSP.2012.2231076
   Mota JFC, 2017, IEEE T INFORM THEORY, V63, P4472, DOI 10.1109/TIT.2017.2695614
   Palangi H, 2017, SIGNAL PROCESS, V131, P181, DOI 10.1016/j.sigpro.2016.07.006
   PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465
   Pudlewski S, 2013, IEEE T MULTIMEDIA, V15, P2072, DOI 10.1109/TMM.2013.2280245
   Scarlett J, 2013, IEEE T SIGNAL PROCES, V61, P427, DOI 10.1109/TSP.2012.2225051
   Strohmer T, 2003, APPL COMPUT HARMON A, V14, P257, DOI 10.1016/S1063-5203(03)00023-X
   Tan J, 2015, IEEE T SIGNAL PROCES, V63, P2085, DOI 10.1109/TSP.2015.2408558
   Quan TM, 2018, IEEE T MED IMAGING, V37, P1488, DOI 10.1109/TMI.2018.2820120
   Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018
   Wang NY, 2013, IEEE I CONF COMP VIS, P657, DOI 10.1109/ICCV.2013.87
   Xu SC, 2015, IEEE SIGNAL PROC LET, V22, P1311, DOI 10.1109/LSP.2015.2400372
   Yan C., IEEE T MULTIMEDIA
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Zelnik-Manor L, 2011, IEEE T SIGNAL PROCES, V59, P4300, DOI 10.1109/TSP.2011.2159211
   Zhang SS, 2018, IEEE ANN INT CONF CY, P617, DOI 10.1109/CYBER.2018.8688295
   Zhang Z, 2015, IEEE ACCESS, V3, P490, DOI 10.1109/ACCESS.2015.2430359
   Zhu ZH, 2018, SIAM J IMAGING SCI, V11, P1717, DOI 10.1137/17M1148426
   Zhu ZH, 2017, J FOURIER ANAL APPL, V23, P1263, DOI 10.1007/s00041-016-9498-2
NR 53
TC 15
Z9 18
U1 1
U2 32
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 594
EP 609
DI 10.1109/TMM.2019.2931400
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700003
DA 2024-07-18
ER

PT J
AU Ma, C
   Gong, C
   Li, X
   Huang, XL
   Liu, W
   Yang, J
AF Ma, Chao
   Gong, Chen
   Li, Xiang
   Huang, Xiaolin
   Liu, Wei
   Yang, Jie
TI Toward Making Unsupervised Graph Hashing Discriminative
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary codes; Principal component analysis; Hamming distance; Videos;
   Manifolds; Probabilistic logic; Data models; Unsupervised hashing;
   graph-based method; discrimination; probabilistic model
ID IMAGE RETRIEVAL; QUANTIZATION; REPRESENTATION; TREES
AB Recently, hashing has attracted much attention in visual information retrieval due to its low storage cost and fast query speed. The goal of hashing is to map original high-dimensional data into a low-dimensional binary-code space where the similar data points are assigned similar hash codes and dissimilar points are far away from each other. Existing unsupervised hashing methods mainly focus on recovering the pairwise similarity of the original data in hash space, but do not take specific measures to make the generated binary codes to be discriminative. To address this problem, this paper proposes a novel unsupervised hashing method, named "Discriminative Unsupervised Graph Hashing" (DUGH), which takes both similarity and dissimilarity of original data into consideration to learn discriminative binary codes. In particular, a probabilistic model is utilized to learn the encoding of original data in low-dimensional space, which models the original neighbor structure through both positive and negative edges in the KNN graph and then maximizes the likelihood of observing these edges. To efficiently and accurately measure the neighbor structure for large-scale datasets, we propose an effective KNN graph construction algorithm based on the random projection tree and neighbor exploring techniques. The experimental results on one synthetic dataset and four typical real-world image datasets demonstrate that the proposed method significantly outperforms the state-of-the-art unsupervised hashing methods.
C1 [Ma, Chao; Li, Xiang] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
   [Gong, Chen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Huang, Xiaolin; Yang, Jie] Shanghai Jiao Tong Univ, Inst Med Robot, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
   [Liu, Wei] Tencent AI Lab, Shenzhen 518172, Peoples R China.
C3 Shanghai Jiao Tong University; Nanjing University of Science &
   Technology; Shanghai Jiao Tong University; Tencent
RP Huang, XL; Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Med Robot, Inst Image Proc & Pattern Recognit, Shanghai 200240, Peoples R China.
EM sjtu_machao@sjtu.edu.cn; chen.gong@njust.edu.cn; xx.lee@sjtu.edu.cn;
   xiaolinhuang@sjtu.edu.cn; wliu@ee.columbia.edu; jieyang@sjtu.edu.cn
RI Liu, Wei/L-1951-2019; GONG, CHEN/JDW-5727-2023; Yang, Jie/JCD-9867-2023
OI Liu, Wei/0000-0002-3865-8145; Huang, Xiaolin/0000-0003-4285-6520
FU NSFC, China [61602246, 61876107, U1803261, 61603248]; Committee of
   Science and Technology, Shanghai, China [19510711200]; 973 Plan, China
   [2015CB856004]; 1000-Talent Plan (Young Program); NSF of Jiangsu
   Province [BK20171430]; Fundamental Research Funds for the
   CentralUniversities [30918011319]; State Key Laboratory of Integrated
   Services Networks (Xidian University) [ISN19-03]; Summit of the Six Top
   Talents Program [DZXX-027]; Innovative and Entrepreneurial Doctor
   Program of Jiangsu Province; Young Elite Scientists Sponsorship Program
   by Jiangsu Province; Young Elite Scientists Sponsorship Program by CAST
   [2018QNRC001]
FX This work was supported in part by NSFC, China (No: 61602246, 61876107,
   U1803261, 61603248), in part by Committee of Science and Technology,
   Shanghai, China (No. 19510711200) and 973 Plan, China (No.
   2015CB856004), in part by 1000-Talent Plan (Young Program), in part by
   NSF of Jiangsu Province (No: BK20171430), in part by the Fundamental
   Research Funds for the CentralUniversities (No: 30918011319), in part by
   the open project of State Key Laboratory of Integrated Services Networks
   (Xidian University, ID: ISN19-03), in part by the Summit of the Six Top
   Talents Program (No: DZXX-027), in part by the Innovative and
   Entrepreneurial Doctor Program of Jiangsu Province, in part by the Young
   Elite Scientists Sponsorship Program by Jiangsu Province, and in part by
   the Young Elite Scientists Sponsorship Program by CAST (No:
   2018QNRC001).
CR [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], IEEE T CIRCUITS SYST
   [Anonymous], 2015, P CVPR BOST MA US
   [Anonymous], 2008, INTERACTIVE TECHNIQU, DOI DOI 10.1145/1394669.1394685
   [Anonymous], 2009, NEURIPS
   [Anonymous], 2018, P IEEE C COMP VIS PA
   [Anonymous], 2018, IEEE T CYBERN
   BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chua T.-S., 2009, CIVR, P1, DOI 10.1145/1646396.1646452
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dasgupta S, 2008, ACM S THEORY COMPUT, P537
   Dong X, 2011, INSECT MOL BIOL, V20, P577, DOI 10.1111/j.1365-2583.2011.01088.x
   Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745
   Gao LL, 2017, MULTIMEDIA SYST, V23, P303, DOI 10.1007/s00530-015-0494-1
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong C, 2019, IEEE T CYBERNETICS, V49, P388, DOI 10.1109/TCYB.2017.2773562
   Gong C, 2017, IEEE T NEUR NET LEAR, V28, P1452, DOI 10.1109/TNNLS.2016.2514360
   Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024
   Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147
   Huang C, 2016, PROC CVPR IEEE, P5175, DOI 10.1109/CVPR.2016.559
   Irie G, 2014, PROC CVPR IEEE, P2123, DOI 10.1109/CVPR.2014.272
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   Kulis B, 2009, IEEE T PATTERN ANAL, V31, P2143, DOI 10.1109/TPAMI.2009.151
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li X, 2018, IEEE IMAGE PROC, P490, DOI 10.1109/ICIP.2018.8451183
   Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133
   Liu L, 2017, INT J COMPUT VISION, V122, P439, DOI 10.1007/s11263-016-0931-4
   Liu W., 2010, PROC ICML, P679
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Mao XJ, 2017, IEEE T MULTIMEDIA, V19, P382, DOI 10.1109/TMM.2016.2614858
   Mikolov T., 2013, ADV NEURAL INFORM PR, V26, P3111, DOI DOI 10.5555/2999792.2999959
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Recht B., 2011, ADV NEURAL INFORM PR, P693
   SCHONEMA.PH, 1966, PSYCHOMETRIKA, V31, P1, DOI 10.1007/BF02289451
   Shen FM, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1522, DOI 10.1145/3123266.3123345
   Shen FM, 2018, IEEE T PATTERN ANAL, V40, P3034, DOI 10.1109/TPAMI.2018.2789887
   Shen FM, 2017, IEEE T MULTIMEDIA, V19, P2022, DOI 10.1109/TMM.2017.2699863
   Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Shen FM, 2013, PROC CVPR IEEE, P1562, DOI 10.1109/CVPR.2013.205
   Silpa-Anan C, 2008, PROC CVPR IEEE, P2308
   Song JK, 2018, IEEE T IMAGE PROCESS, V27, P3210, DOI 10.1109/TIP.2018.2814344
   Song JK, 2018, PATTERN RECOGN, V75, P175, DOI 10.1016/j.patcog.2017.03.021
   Song JK, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P827, DOI 10.1145/2733373.2806341
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Song JK, 2016, IEEE T MULTIMEDIA, V18, P484, DOI 10.1109/TMM.2016.2515990
   Song JK, 2014, IEEE T CYBERNETICS, V44, P1225, DOI 10.1109/TCYB.2013.2289351
   Tang J, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P287, DOI 10.1145/2872427.2883041
   Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang M, 2016, IEEE T KNOWL DATA EN, V28, P1864, DOI 10.1109/TKDE.2016.2535367
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Xiao H., 2017, arXiv
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhang PC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P173, DOI 10.1145/2600428.2609600
   Zhu XF, 2017, IEEE T MULTIMEDIA, V19, P2033, DOI 10.1109/TMM.2017.2703636
NR 67
TC 7
Z9 7
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 760
EP 774
DI 10.1109/TMM.2019.2931808
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700015
DA 2024-07-18
ER

PT J
AU Verhack, R
   Sikora, T
   Van Wallendael, G
   Lambert, P
AF Verhack, Ruben
   Sikora, Thomas
   Van Wallendael, Glenn
   Lambert, Peter
TI Steered Mixture-of-Experts for Light Field Images and Video:
   Representation and Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Kernel; Encoding; Cameras; Image coding; Solid modeling; Image
   reconstruction; Image resolution; Mixture of experts; light fields;
   mixture models; sparse representation; bayesian modeling
ID QUALITY ASSESSMENT; MULTIVIEW
AB Research in light field (LF) processing has heavily increased over the last decade. This is largely driven by the desire to achieve the same level of immersion and navigational freedom for camera-captured scenes as it is currently available for CGI content. Standardization organizations such as MPEG and JPEG continue to follow conventional coding paradigms in which viewpoints are discretely represented on 2-D regular grids. These grids are then further decorrelated through hybrid DPCM/transform techniques. However, these 2-D regular grids are less suited for high-dimensional data, such as LFs. We propose a novel coding framework for higher-dimensional image modalities, called Steered Mixture-of-Experts (SMoE). Coherent areas in the higher-dimensional space are represented by single higher-dimensional entities, called kernels. These kernels hold spatially localized information about light rays at any angle arriving at a certain region. The global model consists thus of a set of kernels which define a continuous approximation of the underlying plenoptic function. We introduce the theory of SMoE and illustrate its application for 2-D images, 4-D LF images, and 5-D LF video. We also propose an efficient coding strategy to convert the model parameters into a bitstream. Even without provisions for high-frequency information, the proposed method performs comparable to the state of the art for low-to-mid range bitrates with respect to subjective visual quality of 4-D LF images. In case of 5-D LF video, we observe superior decorrelation and coding performance with coding gains of a factor of 4x in bitrate for the same quality. At least equally important is the fact that our method inherently has desired functionality for LF rendering which is lacking in other state-of-the-art techniques: (1) full zero-delay random access, (2) light-weight pixel-parallel view reconstruction, and (3) intrinsic view interpolation and super-resolution.
C1 [Verhack, Ruben; Van Wallendael, Glenn; Lambert, Peter] Univ Ghent, IDLab, IMEC, B-9052 Ghent, Belgium.
   [Verhack, Ruben; Sikora, Thomas] Tech Univ Berlin, Commun Syst Grp, D-10623 Berlin, Germany.
C3 Ghent University; IMEC; Technical University of Berlin
RP Verhack, R (corresponding author), Univ Ghent, IDLab, IMEC, B-9052 Ghent, Belgium.
EM ruben.verhack@ugent.be; sikora@nue.tu-berlin.de;
   glenn.vanwallendael@ugent.be; peter.lambert@ugent.be
RI Verhack, Ruben/AAU-9848-2021; Van Wallendael, Glenn/H-8315-2015;
   Lambert, Peter/D-7776-2016
OI Verhack, Ruben/0000-0001-6636-629X; Van Wallendael,
   Glenn/0000-0001-9530-3466; Lambert, Peter/0000-0001-5313-4158
FU IDLab (Ghent University - imec); Communication Systems Group (Technische
   Universitat Berlin); Flanders Innovation & Entrepreneurship (VLAIO);
   Fund for Scientific Research Flanders (FWO Flanders); European Union
FX This work was supported in part by IDLab (Ghent University - imec), in
   part by Communication Systems Group (Technische Universitat Berlin), in
   part by Flanders Innovation & Entrepreneurship (VLAIO), in part by the
   Fund for Scientific Research Flanders (FWO Flanders), and in part by the
   European Union. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Mea Wang.
CR Adelson E.H., 1991, Computational Models of Visual Processing, P3
   Ahmad W, 2017, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2017.8297145
   [Anonymous], 2016, 8 INT C QUAL MULT EX
   [Anonymous], 2014, JCTVCS1002
   [Anonymous], 2015, LIGHT FIELD TOOLBOX
   [Anonymous], 2012, TECH REP
   [Anonymous], 2006, THESIS
   [Anonymous], 2004, GAUSSIAN MIXTURE REG
   [Anonymous], [No title captured]
   Avramelos V, 2020, J REAL-TIME IMAGE PR, V17, P931, DOI 10.1007/s11554-018-0843-3
   Bochinski E, 2018, IEEE IMAGE PROC, P3873, DOI 10.1109/ICIP.2018.8451823
   Bugmann G, 1998, NEUROCOMPUTING, V20, P97, DOI 10.1016/S0925-2312(98)00027-7
   Ceulemans B, 2018, IEEE T MULTIMEDIA, V20, P2235, DOI 10.1109/TMM.2018.2802646
   Chen J, 2018, IEEE T IMAGE PROCESS, V27, P314, DOI 10.1109/TIP.2017.2750413
   Conti C, 2018, IEEE T MULTIMEDIA, V20, P2905, DOI 10.1109/TMM.2018.2825882
   de Carvalho MB, 2018, IEEE IMAGE PROC, P435, DOI 10.1109/ICIP.2018.8451684
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Domanski M, 2017, INT CONF SYST SIGNAL, DOI 10.1109/IWSSIP.2017.7965623
   Ebrahimi T, 2016, IEEE MULTIMEDIA, V23, P14, DOI 10.1109/MMUL.2016.64
   Georgiev T, 2013, PROC SPIE, V8667, DOI 10.1117/12.2013581
   Ghahramani Z., 1994, ADV NEURAL INFORM PR, P120
   Hinds AT, 2017, IEEE INT CON MULTI, P1171, DOI 10.1109/ICME.2017.8019543
   Hog M, 2017, IEEE J-STSP, V11, P1187, DOI 10.1109/JSTSP.2017.2738619
   Ihrke I, 2016, IEEE SIGNAL PROC MAG, V33, P59, DOI 10.1109/MSP.2016.2582220
   Jiang LG, 2009, FLOW IN POROUS MEDIA - FROM PHENOMENA TO ENGINEERING AND BEYOND, P611
   Johannsen O, 2016, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2016.355
   Johannsen O, 2016, LECT NOTES COMPUT SC, V9796, P207, DOI 10.1007/978-3-319-45886-1_17
   Jordan MI, 1995, NEURAL NETWORKS, V8, P1409, DOI 10.1016/0893-6080(95)00014-3
   Lalush DS, 1998, PHYS MED BIOL, V43, P875, DOI 10.1088/0031-9155/43/4/015
   Lange L, 2016, PICT COD SYMP
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li L, 2017, IEEE DATA COMPR CONF, P131, DOI 10.1109/DCC.2017.10
   Lucas LER, 2014, EUR SIGNAL PR CONF, P11
   Moon TK, 1996, IEEE SIGNAL PROC MAG, V13, P47, DOI 10.1109/79.543975
   MUMFORD D, 1989, COMMUN PUR APPL MATH, V42, P577, DOI 10.1002/cpa.3160420503
   Peixeiro JP, 2018, IEEE T MULTIMEDIA, V20, P282, DOI 10.1109/TMM.2017.2742701
   Perra C, 2016, IEEE INT CONF MULTI
   Prandoni P, 1999, PHILOS T R SOC A, V357, P2573, DOI 10.1098/rsta.1999.0449
   Sato M, 2000, NEURAL COMPUT, V12, P407, DOI 10.1162/089976600300015853
   Sculley D., 2010, P INT C WORLD WID WE, V19, P1177, DOI [DOI 10.1145/1772690.1772862, 10.1145/1772690.1772862]
   Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88
   Tech G, 2016, IEEE T CIRC SYST VID, V26, P35, DOI 10.1109/TCSVT.2015.2477935
   Tok M, 2018, PICT COD SYMP, P273, DOI 10.1109/PCS.2018.8456250
   Vagharshakyan S, 2018, IEEE T PATTERN ANAL, V40, P133, DOI 10.1109/TPAMI.2017.2653101
   Verhack R, 2017, IEEE INT CON MULTI, P1183, DOI 10.1109/ICME.2017.8019442
   Verhack R, 2016, IEEE IMAGE PROC, P2142, DOI 10.1109/ICIP.2016.7532737
   Verhack R, 2014, IEEE IMAGE PROC, P4807, DOI 10.1109/ICIP.2014.7025974
   Viola I, 2018, INT WORK QUAL MULTIM, P189
   Viola I, 2016, PICT COD SYMP
   Viola I, 2017, IEEE J-STSP, V11, P1092, DOI 10.1109/JSTSP.2017.2740167
   Wang TC, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073614
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Yuksel SE, 2012, IEEE T NEUR NET LEAR, V23, P1177, DOI 10.1109/TNNLS.2012.2200299
   Yun Li, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P539, DOI 10.1109/ICASSP.2014.6853654
NR 54
TC 21
Z9 21
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 579
EP 593
DI 10.1109/TMM.2019.2932614
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700002
OA hybrid
DA 2024-07-18
ER

PT J
AU Cascianelli, S
   Costante, G
   Devo, A
   Ciarfuglia, TA
   Valigi, P
   Fravolini, ML
AF Cascianelli, Silvia
   Costante, Gabriele
   Devo, Alessandro
   Ciarfuglia, Thomas A.
   Valigi, Paolo
   Fravolini, Mario L.
TI The Role of the Input in Natural Language Video Description
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video description; multimodal data; input preprocessing
ID IMAGE; ATTENTION; TEXT
AB Natural language video description (NLVD) has recently received strong interest in the computer vision, natural language processing (NLP), multimedia, and autonomous robotics communities. The state-of-the-art (SotA) approaches obtained remarkable results when tested on the benchmark datasets. However, those approaches poorly generalize to new datasets. In addition, none of the existing works focus on the processing of the input to the NLVD systems, which is both visual and textual. In this paper, an extensive study is presented to deal with the role of the visual input, evaluated with respect to the overall NLP performance. This is achieved by performing data augmentation of the visual component, applying common transformations to model camera distortions, noise, lighting, and camera positioning that are typical in real-world operative scenarios. A t-SNE-based analysis is proposed to evaluate the effects of the considered transformations on the overall visual data distribution. For this study, the English subset of the Microsoft Research Video Description (MSVD) dataset is considered, which is used commonly for NLVD. It was observed that this dataset contains a relevant amount of syntactic and semantic errors. These errors have been amended manually, and the new version of the dataset (called MSVD-v2) is used in the experimentation. The MSVD-v2 dataset is released to help to gain insight into the NLVD problem.
C1 [Cascianelli, Silvia; Costante, Gabriele; Devo, Alessandro; Ciarfuglia, Thomas A.; Valigi, Paolo; Fravolini, Mario L.] Univ Perugia, Dept Engn, I-06123 Perugia, Italy.
C3 University of Perugia
RP Cascianelli, S (corresponding author), Univ Perugia, Dept Engn, I-06123 Perugia, Italy.
EM silvia.cascianelli@unipg.it; gabriele.costante@unipg.it;
   alessandro.devo@studenti.unipg.it; thomas.ciarfuglia@unipg.it;
   paolo.valigi@unipg.it; mario.fravolini@unipg.it
RI Valigi, Paolo/AAI-8912-2020; Devo, Alessandro/AAH-1470-2020; fravolini,
   mario luca/G-5192-2012; Ciarfuglia, Thomas Alessandro/JNS-8830-2023
OI Valigi, Paolo/0000-0002-0486-7678; Devo, Alessandro/0000-0001-7522-6264;
   Ciarfuglia, Thomas Alessandro/0000-0001-8646-8197; Cascianelli,
   Silvia/0000-0001-7885-6050
CR Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24
   [Anonymous], ARXIX170802478
   [Anonymous], 2016, ARXIV160401729
   [Anonymous], 2017, P 2017 C EMPIRICAL M
   [Anonymous], 2017, P 8 INT JOINT C NATU
   [Anonymous], 2017, ARXIV170506830
   [Anonymous], 2016, ARXIV PREPRINT ARXIV
   [Anonymous], PATTERN RECOGNIT LET
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Baraldi L, 2017, PROC CVPR IEEE, P3185, DOI 10.1109/CVPR.2017.339
   Baraldi L, 2017, IEEE T MULTIMEDIA, V19, P955, DOI 10.1109/TMM.2016.2644872
   Bin Y, 2019, IEEE T CYBERNETICS, V49, P2631, DOI 10.1109/TCYB.2018.2831447
   Cascianelli S, 2018, IEEE ROBOT AUTOM LET, V3, P841, DOI 10.1109/LRA.2018.2793345
   Chen D., 2011, P 49 ANN M ASS COMP, P190
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen TH, 2016, ASIAN TEST SYMPOSIUM, P269, DOI 10.1109/ATS.2016.26
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Chung Junyoung, 2014, ARXIV14123555
   Cui P, 2018, IEEE T MULTIMEDIA, V20, P198, DOI 10.1109/TMM.2017.2724843
   DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409
   Dong JF, 2018, IEEE T MULTIMEDIA, V20, P3377, DOI 10.1109/TMM.2018.2832602
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Engstrom L., 2016, Fast style transfer
   Fadaee Marzieh, 2017, ARXIV170500440
   Fellbaum C, 2010, THEORY AND APPLICATIONS OF ONTOLOGY: COMPUTER APPLICATIONS, P231, DOI 10.1007/978-90-481-8847-5_10
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Guo YY, 2019, WORLD WIDE WEB, V22, P735, DOI 10.1007/s11280-018-0530-0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu YT, 2018, IEEE T MULTIMEDIA, V20, P927, DOI 10.1109/TMM.2017.2760101
   Jackson P. T., 2018, ARXIV180905375
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kofler C, 2014, IEEE T MULTIMEDIA, V16, P1973, DOI 10.1109/TMM.2014.2347937
   Kotsiantis SB, 2006, J COMPUT, V1, P30, DOI 10.4304/jcp.1.4.30-37
   Krishnadas N, 2013, PRINCIPLES, METHODOLOGIES, AND SERVICE-ORIENTED APPROACHES FOR CLOUD COMPUTING, P1, DOI 10.4018/978-1-4666-2854-0.ch001
   Krizhevsky A., 2012, ADV NEURAL INF PROCE, V25, P1097
   Li LJ, 2019, IEEE WINT CONF APPL, P339, DOI 10.1109/WACV.2019.00042
   Li LH, 2018, IEEE T MULTIMEDIA, V20, P726, DOI 10.1109/TMM.2017.2751140
   Li W, 2018, PATTERN RECOGN LETT, V105, P23, DOI 10.1016/j.patrec.2017.10.012
   Li WX, 2017, IEEE T MULTIMEDIA, V19, P367, DOI 10.1109/TMM.2016.2616279
   Li XP, 2019, WORLD WIDE WEB, V22, P621, DOI 10.1007/s11280-018-0531-z
   Li Y, 2018, P EUR C COMP VIS ECC, P453
   Lin Chin-Yew, 2004, P WORKSH TEXT SUMM B, V8
   Nawi NM, 2013, PROC TECH, V11, P32, DOI 10.1016/j.protcy.2013.12.159
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Pasunuru R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1273, DOI 10.18653/v1/P17-1117
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Rohrbach A, 2017, INT J COMPUT VISION, V123, P94, DOI 10.1007/s11263-016-0987-1
   Rohrbach A, 2015, PROC CVPR IEEE, P3202, DOI 10.1109/CVPR.2015.7298940
   Song H, 2018, IEEE T MULTIMEDIA, V20, P1088, DOI 10.1109/TMM.2017.2763322
   Teng CM, 1999, MACHINE LEARNING, PROCEEDINGS, P239
   Thomason J., 2014, COLING, P1218
   Torabi Atousa., 2015, ARXIV150301070
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Venugopalan Subhashini, 2014, P 2015 C N AM CHAPT
   Wang BR, 2018, PROC CVPR IEEE, P7622, DOI 10.1109/CVPR.2018.00795
   Wang X, 2018, PROC CVPR IEEE, P4213, DOI 10.1109/CVPR.2018.00443
   Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029
   Xie HT, 2014, IEEE T MULTIMEDIA, V16, P1104, DOI 10.1109/TMM.2014.2305909
   Xu J, 2016, PROC CVPR IEEE, P5288, DOI 10.1109/CVPR.2016.571
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Xu YJ, 2018, IEEE T IMAGE PROCESS, V27, P4933, DOI 10.1109/TIP.2018.2846664
   Yang XS, 2018, IEEE T MULTIMEDIA, V20, P2360, DOI 10.1109/TMM.2018.2807588
   Yang ZW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P146, DOI 10.1145/3123266.3123327
   Youngjae Yu, 2018, Computer Vision - ECCV 2018. 15th European Conference. Proceedings: Lecture Notes in Computer Science (LNCS 11211), P487, DOI 10.1007/978-3-030-01234-2_29
   Zhang Xiang., 2015, Text Understanding From Scratch
   Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11
NR 71
TC 2
Z9 2
U1 1
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 271
EP 283
DI 10.1109/TMM.2019.2924598
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Kuang, ZZ
   Yu, J
   Zhu, SG
   Li, ZM
   Fan, JP
AF Kuang, Zhenzhong
   Yu, Jun
   Zhu, Suguo
   Li, Zongmin
   Fan, Jianping
TI Effective 3-D Shape Retrieval by Integrating Traditional Descriptors and
   Pointwise Convolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D Shape retrieval and recognition; isometric variation; intrinsic
   point descriptor; pointwise convolution; parallel knowledge transfer
ID CLOUD RECOGNITION; 3D SHAPES
AB The applications of isometric 3-D objects have recently received sufficient attention and, thus, it is very attractive to retrieve such isometric 3-D objects from large-scale collections. Although existing approaches have presented some interesting ideas, their performance is limited to their ability on feature representation. To improve the performance of 3-D object (shape) recognition, some recent algorithms prefer using complicated deep neural networks to learn discriminative features, but they consume huge amounts of computing resources. Instead, this paper presents a more effective solution by seamlessly integrating the traditional local descriptor with a deep pointwise convolutional network to extract 1-D features for shape recognition and retrieval. To reduce the costs of designing a complicated deep network, the first step of our algorithm is to describe the shape deformation by sampling a set of intrinsic point descriptors. Then, we introduce a simple yet effective pointwise convolutional network to integrate these descriptors as a global feature and the learning process can be significantly accelerated with the help of downsampling. Furthermore, a knowledge transfer strategy is used to upgrade our feature by compensating for information loss. Finally, we carry out experimental evaluations over popular shape benchmarks, and the results suggest that our approach exhibits superior accuracy rates and robustness on shape recognition and retrieval.
C1 [Kuang, Zhenzhong; Yu, Jun; Zhu, Suguo] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
   [Li, Zongmin] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Peoples R China.
   [Fan, Jianping] Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.
C3 Hangzhou Dianzi University; China University of Petroleum; University of
   North Carolina; University of North Carolina Charlotte
RP Yu, J (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Key Lab Complex Syst Modeling & Simulat, Hangzhou 310018, Peoples R China.
EM zzkuang@hdu.edu.cn; yujun@hdu.edu.cn; zsg2016@hdu.edu.cn;
   lizongmin@upc.edu.cn; jfan@uncc.edu
FU National Natural Science Foundation of China [61806063, 61836002,
   61772161, 61622205, 61472110]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61806063, 61836002, 61772161, 61622205, and 61472110.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Zhu Li.
CR Agathos A., 2009, 3DOR, P29
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], P EUR WORKSH 3D OBJ
   [Anonymous], 2018, 2018 IEEE INT C MULT
   Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693
   Bronstein AM, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1899404.1899405
   Bronstein MM, 2011, IEEE T PATTERN ANAL, V33, P1065, DOI 10.1109/TPAMI.2010.210
   Bu SH, 2017, NEUROCOMPUTING, V259, P183, DOI 10.1016/j.neucom.2016.06.088
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Chu WT, 2018, IEEE T MULTIMEDIA, V20, P2491, DOI 10.1109/TMM.2018.2801718
   Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Elad A, 2003, IEEE T PATTERN ANAL, V25, P1285, DOI 10.1109/TPAMI.2003.1233902
   Fukui Akira, 2016, P C EMP METH NAT LAN
   Gao F, 2018, IEEE T MULTIMEDIA, V20, P2774, DOI 10.1109/TMM.2018.2818012
   Hinton G., 2015, COMPUT SCI, V2
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Kokkinos I, 2012, PROC CVPR IEEE, P159, DOI 10.1109/CVPR.2012.6247671
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuang ZZ, 2018, LECT NOTES COMPUT SC, V11165, P89, DOI 10.1007/978-3-030-00767-6_9
   Kuang ZZ, 2018, PATTERN RECOGN, V78, P198, DOI 10.1016/j.patcog.2018.01.027
   Kuang ZZ, 2015, COMPUT GRAPH-UK, V46, P209, DOI 10.1016/j.cag.2014.09.033
   Kuang ZZ, 2015, COMPUT AIDED DESIGN, V58, P13, DOI 10.1016/j.cad.2014.08.004
   Lavoue G., 2011, Eurographics Conference on 3D Object Retrieval, P41, DOI DOI 10.2312/3DOR/3DOR11/041-048
   Lavoué G, 2012, VISUAL COMPUT, V28, P931, DOI 10.1007/s00371-012-0724-x
   Levy B, 2006, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2006, PROCEEDINGS, P66
   Li YH, 2017, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2017.229
   Lian Z., 2010, Eurographics Workshop on 3D Object Retrieval, V10, P101, DOI [10.2312/3DOR/3DOR10/101-108, 10.1109/CVPR.2014.491, DOI 10.2312/3DOR/3DOR10/101-108]
   Lian Z., 2015, PROC 8 EUROGRAPHICS, P107
   Lian ZH, 2013, PATTERN RECOGN, V46, P449, DOI 10.1016/j.patcog.2012.07.014
   Liang J, 2012, PROC CVPR IEEE, P214, DOI 10.1109/CVPR.2012.6247678
   Liu RS, 2018, INT J DIGIT MULTIMED, V2018, DOI [10.1155/2018/7543875, 10.1109/TMM.2018.2812605]
   Liu YJ, 2011, IEEE T PATTERN ANAL, V33, P1502, DOI 10.1109/TPAMI.2010.221
   Liu YS, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-480
   Luciano L, 2018, PATTERN RECOGN LETT, V105, P182, DOI 10.1016/j.patrec.2017.05.011
   Luo Y, 2011, 2011 INTERNATIONAL FORUM ON BIOMEDICAL TEXTILE MATERIALS, PROCEEDINGS, P90
   Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002
   Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648
   Papadakis P, 2014, PATTERN RECOGN, V47, P216, DOI 10.1016/j.patcog.2013.06.024
   Qi C., 2017, NeurIPS, P5105, DOI [10.1109/ CVPR.2017.16, DOI 10.1109/CVPR.2017.16]
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Qiu HJ, 2007, IEEE T PATTERN ANAL, V29, P1873, DOI 10.1109/TPAMI.2007.1103
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Tabia H, 2014, PROC CVPR IEEE, P4185, DOI 10.1109/CVPR.2014.533
   Tang J., 2018, ARXIV181112013
   Te GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P746, DOI 10.1145/3240508.3240621
   Toldo R, 2010, VISUAL COMPUT, V26, P1257, DOI 10.1007/s00371-010-0519-x
   van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4
   Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362
   Xie J, 2017, IEEE T PATTERN ANAL, V39, P1335, DOI 10.1109/TPAMI.2016.2596722
   Xie ZG, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12740
   You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702
   Yu J, 2017, IEEE T CYBERNETICS, V47, P4014, DOI 10.1109/TCYB.2016.2591583
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhu F, 2016, LECT NOTES COMPUT SC, V9911, P305, DOI 10.1007/978-3-319-46478-7_19
   Zhu ZT, 2016, NEUROCOMPUTING, V204, P41, DOI 10.1016/j.neucom.2015.08.127
NR 60
TC 10
Z9 10
U1 1
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3164
EP 3177
DI 10.1109/TMM.2019.2918729
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200015
DA 2024-07-18
ER

PT J
AU Xiao, XY
   Wang, LF
   Ding, K
   Xiang, SM
   Pan, CH
AF Xiao, Xinyu
   Wang, Lingfeng
   Ding, Kun
   Xiang, Shiming
   Pan, Chunhong
TI Deep Hierarchical Encoder-Decoder Network for Image Captioning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Semantics; Hidden Markov models; Decoding; Logic gates;
   Training; Computer architecture; Deep hierarchical structure;
   encoder-decoder; LSTM; image captioning; retrieval; vision-sentence
AB Encoder-decoder models have been widely used in image captioning, and most of them are designed via single long short term memory (LSTM). The capacity of single-layer network, whose encoder and decoder are integrated together, is limited for such a complex task of image captioning. Moreover, how to effectively increase the "vertical depth" of encoder-decoder remains to be solved. To deal with these problems, a novel deep hierarchical encoder-decoder network is proposed for image captioning, where a deep hierarchical structure is explored to separate the functions of encoder and decoder. This model is capable of efficiently exerting the representation capacity of deep networks to fuse high level semantics of vision and language in generating captions. Specifically, visual representations in top levels of abstraction are simultaneously considered, and each of these levels is associated to one LSTM. The bottom-most LSTM is applied as the encoder of textual inputs. The application of the middle layer in encoder-decoder is to enhance the decoding ability of top-most LSTM. Furthermore, depending on the introduction of semantic enhancement module of image feature and distribution combine module of text feature, variants of architectures of our model are constructed to explore the impacts and mutual interactions among the visual representation, textual representations, and the output of the middle LSTM layer. Particularly, the framework is training under a reinforcement learning method to address the exposure bias problem between the training and the testing by the policy gradient optimization. Qualitative analyses indicate the process that our model "translates" image to sentence and further visualization presents the evolution of the hidden states from different hierarchical LSTMs over time. Extensive experiments demonstrate that our model outperforms current state-of-the-art models on three benchmark datasets: Flickr8K, Flickr30K, and MSCOCO. On both image captioning and retrieval tasks, our method achieves the best results. On MSCOCO captioning Leaderboard, our method also achieves superior performance.
C1 [Xiao, Xinyu; Wang, Lingfeng; Ding, Kun; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Xiao, Xinyu; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS
RP Wang, LF (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM xinyu.xiao@nlpr.ia.ac.cn; lfwang@nlpr.ia.ac.cn; kding1225@gmail.com;
   smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
RI DING, KUN/HNJ-1709-2023
OI wang, ling feng/0000-0003-3707-0267
FU National Natural Science Foundation of China [91646207, 61773377,
   61573352]; Beijing Natural Science Foundation [L172053]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 91646207, 61773377, and 61573352, and
   in part by the Beijing Natural Science Foundation under Grant L172053.
   The associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Rita Cucchiara.
CR Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2014, Transactions of the Association for Computational Linguistics
   [Anonymous], 2013, PREPRINT ARXIV 1308
   [Anonymous], COMPUT RES REPOSITOR
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], P INT C LEARN REPR
   [Anonymous], 2012, ARXIV E PRINTS
   [Anonymous], COMPUT RES REPOSITOR
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2013, P 2013 C EMP METH NA
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   [Anonymous], 2016, P 24 ACM INT C MULT
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Chen H, 2018, AAAI CONF ARTIF INTE, P6706
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Choi Y, 2011, P 15 C COMPUTATIONAL, P220
   Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Devlin J, 2015, ROUTL APPR HIST, P106
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Dong L, 2016, IEEE T MULTIMEDIA, V18, P714, DOI 10.1109/TMM.2016.2530399
   Eisenschtat A, 2017, PROC CVPR IEEE, P1855, DOI 10.1109/CVPR.2017.201
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Gan Z, 2017, PROC CVPR IEEE, P1141, DOI 10.1109/CVPR.2017.127
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   González-Díaz I, 2017, IEEE T MULTIMEDIA, V19, P544, DOI 10.1109/TMM.2016.2616298
   Gu JX, 2018, AAAI CONF ARTIF INTE, P6837
   Guadarrama S, 2013, IEEE I CONF COMP VIS, P2712, DOI 10.1109/ICCV.2013.337
   Karpathy A, 2014, ADV NEUR IN, V27
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kiros R, 2014, PR MACH LEARN RES, V32, P595
   Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, AAAI CONF ARTIF INTE, P4176
   Liu F, 2011, MECHANIKA, P449
   Liu SQ, 2017, IEEE I CONF COMP VIS, P873, DOI 10.1109/ICCV.2017.100
   Liu Y, 2017, LECT NOTES COMPUT SC, V10132, P416, DOI 10.1007/978-3-319-51811-4_34
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao Junhua, 2015, P INT C LEARN REPR
   Luong MT, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P11
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281
   Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140
   Rashtchian C., 2010, P NAACL HLT 2010 WOR, V2010, P139, DOI DOI 10.5555/1866696.1866717
   Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Spratling MW, 2004, J COGNITIVE NEUROSCI, V16, P219, DOI 10.1162/089892904322984526
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutton R. S., 1998, Adaptive Computation and Machine Learning
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tan Y, 2016, PALGR MAC STUD BANK, P105, DOI 10.1057/978-1-137-49376-7_5
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29
   Wu Yonghui, 2016, P C ASS MACH TRANSL
   Xu X, 2015, IEEE ICC, P2048, DOI 10.1109/ICC.2015.7248627
   Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379
   Yang Z, 2016, INT CONF ACOUST SPEE, P3236, DOI 10.1109/ICASSP.2016.7472275
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zhang T, 2016, IEEE T MULTIMEDIA, V18, P2528, DOI 10.1109/TMM.2016.2598092
   Zhou J., 2016, T ASS COMPUT LING, V4, P371, DOI [DOI 10.1162/TACL_A_00105, 10.1162/tacl_a_00105]
   Zhou LW, 2017, PROCEEDINGS OF THE THEMATIC WORKSHOPS OF ACM MULTIMEDIA 2017 (THEMATIC WORKSHOPS'17), P305, DOI 10.1145/3126686.3126717
NR 76
TC 50
Z9 53
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2019
VL 21
IS 11
BP 2942
EP 2956
DI 10.1109/TMM.2019.2915033
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA JJ7VX
UT WOS:000494363000020
DA 2024-07-18
ER

PT J
AU Liu, Y
   Chen, W
   Liu, L
   Lew, MS
AF Liu, Yu
   Chen, Wei
   Liu, Li
   Lew, Michael S.
TI SwapGAN: A Multistage Generative Approach for Person-to-Person Fashion
   Style Transfer
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
AB Fashion style transfer has attracted significant attention because it both has interesting scientific challenges and it is also important to the fashion industry. This paper focuses on addressing a practical problem in fashion style transfer, person-to-person clothing swapping, which aims to visualize what the person would look like with the target clothes worn on another person instead of dressing them physically. This problem remains challenging due to varying pose deformations between different person images. In contrast to traditional nonparametric methods that blend or warp the target clothes for the reference person, in this paper we propose a multistage deep generative approach named SwapGAN that exploits three generators and one discriminator in a unified framework to fulfill the task end-to-end. The first and second generators are conditioned on a human pose map and a segmentation map, respectively, so that we can simultaneously transfer the pose style and the clothes style. In addition, the third generator is used to preserve the human body shape during the image synthesis process. The discriminator needs to distinguish two fake image pairs from the real image pair. The entire SwapCAN is trained by integrating the adversarial loss and the mask-consistency loss. The experimental results on the DeepFashion dataset demonstrate the improvements of SwapGAN over other existing approaches through both quantitative and qualitative evaluations. Moreover, we conduct ablation studies on SwapGAN and provide a detailed analysis about its effectiveness.
C1 [Liu, Yu; Chen, Wei; Lew, Michael S.] Leiden Univ, Leiden Inst Adv Comp Sci, NL-2311 Leiden, Netherlands.
   [Liu, Li] Natl Univ Def Technol, Coll Syst Engn, Changsha 410073, Hunan, Peoples R China.
   [Liu, Li] Univ Oulu, Ctr Machine Vis & Signal Anal, Oulu 90014, Finland.
C3 Leiden University; Leiden University - Excl LUMC; National University of
   Defense Technology - China; University of Oulu
RP Lew, MS (corresponding author), Leiden Univ, Leiden Inst Adv Comp Sci, NL-2311 Leiden, Netherlands.
EM y.liu@liacs.leidenuniv.nl; w.chen@liacs.leidenuniv.nl; li.liu@oulu.fi;
   m.s.k.lew@liacs.leidenuniv.nl
RI Bueno, Regis Cortez/AAG-3852-2020
OI Bueno, Regis Cortez/0000-0002-2923-4930; Liu, li/0000-0002-2011-2873
FU National Natural Science Foundation of China [61872379]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61872379. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Lei Zhang.
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Al-Halah Z, 2017, IEEE I CONF COMP VIS, P388, DOI 10.1109/ICCV.2017.50
   [Anonymous], P INT C MACH LEARN
   [Anonymous], 2017, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2017.632
   [Anonymous], P SIGGRAPH AS
   [Anonymous], P ACM C KNOWL DISC D
   [Anonymous], P IEEE INT C COMP VI
   [Anonymous], 2014, ARXIV PREPRINT ARXIV
   [Anonymous], P ACM SIGGRAPH
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Denton E.L., 2015, CoRR, P1486
   Dong H., 2018, NeurIPS, P474
   Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guan P, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185531
   Gültepe U, 2014, COMPUT GRAPH-UK, V43, P31, DOI 10.1016/j.cag.2014.06.001
   Han XT, 2018, PROC CVPR IEEE, P7543, DOI 10.1109/CVPR.2018.00787
   Hauswiesner S, 2013, IEEE T VIS COMPUT GR, V19, P1552, DOI 10.1109/TVCG.2013.67
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoffman J, 2018, PR MACH LEARN RES, V80
   Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202
   Jetchev N, 2017, IEEE INT CONF COMP V, P2287, DOI 10.1109/ICCVW.2017.269
   Jiang SH, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3721
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kanamori Y, 2016, LECT NOTES COMPUT SC, V9550, P1, DOI 10.1007/978-3-662-49247-5_1
   Kiapour MH, 2015, IEEE I CONF COMP VIS, P3343, DOI 10.1109/ICCV.2015.382
   Kingma D. P., 2015, P INT C LEARN REPR I, P1
   Li YC, 2017, IEEE T MULTIMEDIA, V19, P1946, DOI 10.1109/TMM.2017.2690144
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liu DL, 2017, 2017 INTERNATIONAL CONFERENCE ON SMART GRID AND ELECTRICAL AUTOMATION (ICSGEA), P406, DOI 10.1109/ICSGEA.2017.74
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Ma YH, 2017, AAAI CONF ARTIF INTE, P38
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Odena A., 2016, DISTILL, V1, P3, DOI [10.23915/distill.00003., DOI 10.23915/DISTILL, 10.23915/distill.00003, DOI 10.23915/DISTILL.00003]
   Pons-Moll G, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073711
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Salimans T, 2016, ADV NEUR IN, V29
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   Wang B, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON CLOUD COMPUTING AND BIG DATA ANALYSIS (ICCCBDA), P589, DOI 10.1109/ICCCBDA.2018.8386584
   Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47
   Yang S., 2016, Detailed garment recovery from a single-view image
   Yoo D, 2016, LECT NOTES COMPUT SC, V9912, P517, DOI 10.1007/978-3-319-46484-8_31
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang LM, 2018, IEEE T MULTIMEDIA, V20, P1462, DOI 10.1109/TMM.2017.2769799
   Zhang XS, 2017, IEEE T MULTIMEDIA, V19, P2533, DOI 10.1109/TMM.2017.2696825
   Zhao B, 2016, IEEE T MULTIMEDIA, V18, P1111, DOI 10.1109/TMM.2016.2537783
   Zheng Zhaoheng, 2017, [Computational Visual Media, 计算可视媒体], V3, P337
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zhu SZ, 2017, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2017.186
NR 51
TC 46
Z9 46
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2019
VL 21
IS 9
BP 2209
EP 2222
DI 10.1109/TMM.2019.2897897
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IT6VX
UT WOS:000483015200005
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Cong, RM
   Lei, JJ
   Fu, HZ
   Huang, QM
   Cao, XC
   Ling, N
AF Cong, Runmin
   Lei, Jianjun
   Fu, Huazhu
   Huang, Qingming
   Cao, Xiaochun
   Ling, Nam
TI HSCS: Hierarchical Sparsity Based Co-saliency Detection for RGBD Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Co-saliency detection; RGBD images; global sparsity reconstruction;
   pairwise sparsity reconstruction; energy function refinement
ID OBJECT DETECTION; SEGMENTATION; OPTIMIZATION
AB Co-saliency detection aims to discover common and salient objects in an image group containing more than two relevant images. Moreover, depth information has been demonstrated to be effective for many computer vision tasks. In this paper, we propose a novel co-saliency detection method for RGBD images based on hierarchical sparsity reconstruction and energy function refinement. With the assistance of the intrasaliency map, the inter-image correspondence is formulated as a hierarchical sparsity reconstruction framework. The global sparsity reconstruction model with a ranking scheme focuses on capturing the global characteristics among the whole image group through a common foreground dictionary. The pairwise sparsity reconstruction model aims to explore the corresponding relationship between pairwise images through a set of pairwise dictionaries. In order to improve the intra-image smoothness and inter-image consistency, an energy function refinement model is proposed, which includes the unary data term, spatial smooth term, and holistic consistency term. Experiments on two RGBD co-saliency detection benchmarks demonstrate that the proposed method outperforms the state-of-the-art algorithms both qualitatively and quantitatively.
C1 [Cong, Runmin; Lei, Jianjun] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Fu, Huazhu] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Huang, Qingming] Univ Chinese Acad Sci, Sch Comp & Control Engn, Beijing 100190, Peoples R China.
   [Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
   [Cao, Xiaochun] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Ling, Nam] Santa Clara Univ, Dept Comp Engn, Santa Clara, CA 95053 USA.
C3 Tianjin University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of
   Information Engineering, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Santa Clara University
RP Lei, JJ (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM rmcong@tju.edu.cn; jjlei@tju.edu.cn; huazhufu@gmail.com;
   qmhuang@ucas.ac.cn; caoxiaochun@iie.ac.cn; nling@scu.edu
RI Wang, Meng/ITR-8699-2023; Lei, Jianjun/P-2539-2018; Fu,
   Huazhu/A-1411-2014
OI Fu, Huazhu/0000-0002-9702-5524; CONG, RUNMIN/0000-0003-0972-4008
FU National Natural Science Foundation of China [61520106002, 61722112,
   61731003, 61332016, 61620106009, U1636214, 61602345]; Key Research
   Program of Frontier Sciences, Chinese Academy of Sciences
   [QYZDJ-SSW-SYS013]; Technology Research and Development Program of
   Tianjin [15ZXHLGX00130]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61520106002, 61722112, 61731003,
   61332016, 61620106009, U1636214, 61602345, in part by the Key Research
   Program of Frontier Sciences, Chinese Academy of Sciences under Grant
   QYZDJ-SSW-SYS013, and in part by the Technology Research and Development
   Program of Tianjin under Grant 15ZXHLGX00130.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Cao XC, 2016, IEEE T NEUR NET LEAR, V27, P1253, DOI 10.1109/TNNLS.2015.2488637
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Cheng MM, 2021, INT J COMPUT VISION, V129, P2622, DOI [10.1007/s11263-021-01490-8, 10.1109/ICCV.2017.487]
   Cong RM, 2016, IEEE SIGNAL PROC LET, V23, DOI 10.1109/LSP.2016.2557347
   Cong RM, 2019, IEEE T CIRC SYST VID, V29, P2941, DOI 10.1109/TCSVT.2018.2870832
   Cong RM, 2019, IEEE T CYBERNETICS, V49, P233, DOI 10.1109/TCYB.2017.2771488
   Cong RM, 2018, IEEE T IMAGE PROCESS, V27, P568, DOI 10.1109/TIP.2017.2763819
   Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684
   Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698
   Fan DP, 2018, LECT NOTES COMPUT SC, V11219, P196, DOI 10.1007/978-3-030-01267-0_12
   Fu HZ, 2015, IEEE T IMAGE PROCESS, V24, P3415, DOI 10.1109/TIP.2015.2442915
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   FU HZ, 2015, PROC CVPR IEEE, P4428, DOI DOI 10.1109/CVPR.2015
   Ge CJ, 2016, SIGNAL PROCESS-IMAGE, V44, P69, DOI 10.1016/j.image.2016.03.005
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   Han JW, 2018, IEEE T CIRC SYST VID, V28, P2473, DOI 10.1109/TCSVT.2017.2706264
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Han JW, 2018, IEEE T IMAGE PROCESS, V27, P1639, DOI 10.1109/TIP.2017.2781424
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hu XW, 2018, AAAI CONF ARTIF INTE, P6943
   Huang R, 2017, IEEE SIGNAL PROC LET, V24, P569, DOI 10.1109/LSP.2017.2681687
   Ju R, 2015, SIGNAL PROCESS-IMAGE, V38, P115, DOI 10.1016/j.image.2015.07.002
   Kim J, 2016, IEEE T IMAGE PROCESS, V25, P9, DOI 10.1109/TIP.2015.2495122
   Lei JJ, 2017, IEEE T MULTIMEDIA, V19, P1442, DOI 10.1109/TMM.2017.2660440
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Leung T., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1010, DOI 10.1109/ICCV.1999.790379
   Li CY, 2016, IEEE T IMAGE PROCESS, V26, P5664, DOI 10.1109/TIP.2016.2612882
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Meng FM, 2013, IEEE T MULTIMEDIA, V15, P2186, DOI 10.1109/TMM.2013.2280893
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Ni M, 2017, IEEE ACCESS, V5, P26666, DOI 10.1109/ACCESS.2017.2773141
   Pang YW, 2016, IEEE T IND ELECTRON, V63, P5592, DOI 10.1109/TIE.2016.2564938
   Peng HW, 2017, IEEE T PATTERN ANAL, V39, P818, DOI 10.1109/TPAMI.2016.2562626
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren JR, 2018, J VIS COMMUN IMAGE R, V50, P227, DOI 10.1016/j.jvcir.2017.12.002
   Rui Huang, 2015, 2015 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT). Proceedings, P1, DOI 10.1109/ISGT.2015.7131826
   Song HK, 2017, IEEE T IMAGE PROCESS, V26, P4204, DOI 10.1109/TIP.2017.2711277
   Song HK, 2016, IEEE SIGNAL PROC LET, V23, P1722, DOI 10.1109/LSP.2016.2615293
   Tan ZY, 2013, INT CONF ACOUST SPEE, P2114, DOI 10.1109/ICASSP.2013.6638027
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tao ZQ, 2017, AAAI CONF ARTIF INTE, P4285
   Toshev A., 2007, IEEE C COMPUTER VISI, P1
   Tsai CC, 2017, IEEE INT CON MULTI, P523, DOI 10.1109/ICME.2017.8019413
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang WG, 2015, IEEE T IMAGE PROCESS, V24, P4185, DOI 10.1109/TIP.2015.2460013
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Wu LS, 2018, MULTIMED TOOLS APPL, V77, P21185, DOI 10.1007/s11042-017-5576-y
   Yang JY, 2018, IEEE J-STSP, V12, P1420, DOI 10.1109/JSTSP.2018.2873990
   Yang JY, 2015, IEEE T CYBERNETICS, V45, P913, DOI 10.1109/TCYB.2014.2340032
   Yang J, 2016, IEEE INT CONF MULTI
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Yuan YC, 2018, IEEE T CIRC SYST VID, V28, P1130, DOI 10.1109/TCSVT.2016.2646720
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2015, IEEE I CONF COMP VIS, P594, DOI 10.1109/ICCV.2015.75
   Zhang DW, 2015, PROC CVPR IEEE, P2994, DOI 10.1109/CVPR.2015.7298918
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhang YX, 2018, ACM T INTEL SYST TEC, V9, DOI 10.1145/3125645
   Zhou L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2438546
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 76
TC 79
Z9 81
U1 0
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1660
EP 1671
DI 10.1109/TMM.2018.2884481
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700004
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Luo, FL
   Wang, SS
   Wang, SQ
   Zhang, XF
   Ma, SW
   Gao, W
AF Luo, Falei
   Wang, Shanshe
   Wang, Shiqi
   Zhang, Xinfeng
   Ma, Siwei
   Gao, Wen
TI GPU-Based Hierarchical Motion Estimation for High Efficiency Video
   Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE GPU; motion estimation; High Efficiency Video Coding
ID INTER CU DECISION; SEARCH ALGORITHM; HEVC; SIZE
AB Motion estimation (ME) plays a crucial role in removing the temporal redundancy for video compression. However, during the encoding process a substantial computational burden is imposed by ME due to the exhaustive evaluations of possible candidates within the searching window. In view of the increasing computing capacity of GPU, we propose a GPU-based low delay parallel ME scheme for high efficiency video coding (HEVC). In particular, considering the quadtree coding structure of HEVC, we achieve the parallelization in a hierarchical way by optimizing the ME process in a coding tree unit (CTU), prediction unit (PU), and motion vector (MV) layers. Specifically, in the CTU layer, a novel motion vector predictor determination scheme is proposed to alleviate the side effects of inaccurate MV prediction due to the removal of the CTU-level dependency. In the PU layer, a novel indexing table is particularly designed to realize an efficient cost derivation strategy. As such, the cost of each PU can be computed in a convenient and efficient manner. In an MV layer, we propose a compact descriptor to represent MV and its corresponding cost as a whole, such that the redundant branches can be further avoided in the searching process. With such an optimization strategy, the proposed scheme can completely save the encoding time for ME on CPU. Experimental results demonstrate that the proposed scheme can achieve 41% encoding time savings with the ME acceleration up to 12.7 times, and the incurred BD-BR loss is only 0.52% on average. Moreover, further experimental results show that the proposed GPU-based ME can achieve up to 200 times acceleration compared to the full search ME on CPU.
C1 [Luo, Falei] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Luo, Falei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Luo, Falei; Wang, Shanshe; Ma, Siwei; Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Wang, Shiqi] City Univ Hong Kong, Dept Comp Sci, Kowloon, Hong Kong 999077, Peoples R China.
   [Zhang, Xinfeng] Univ Southern Calif, Ming Hsieh Dept Elect Engn, Los Angeles, CA 90089 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Peking University; City University of Hong Kong; University of
   Southern California
RP Ma, SW (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM falei.luo@vipl.ict.ac.cn; sswang@pku.edu.cn; shiqwang@cityu.edu.hk;
   xin-fengz@usc.edu; swma@pku.edu.cn; wgao@pku.edu.cn
RI Zhang, Xinfeng/X-8148-2019
OI Zhang, Xinfeng/0000-0002-7517-3868
FU National Basic Research Program of China (973 Program) [2015CB351800];
   National Natural Science Foundation of China [61632001]; National
   Postdoctoral Program for Innovative Talents [BX201600006]; Top-Notch
   Young Talents Program of China; Hong Kong RGC Early Career Scheme
   [9048122 (CityU 21211018)]; City University of Hong Kong [7200539/CS];
   NVIDIA NVAIL program; Shenzhen Peacock Plan
FX This work was supported in part by the National Basic Research Program
   of China (973 Program, 2015CB351800), by the National Natural Science
   Foundation of China (61632001), by the National Postdoctoral Program for
   Innovative Talents (BX201600006), by the Top-Notch Young Talents Program
   of China, by the Hong Kong RGC Early Career Scheme 9048122 (CityU
   21211018), by the City University of Hong Kong under Grant 7200539/CS,
   by the NVIDIA NVAIL program, and by the Shenzhen Peacock Plan. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Joern Ostermann. (Corresponding
   author: Siwei Ma.)
CR [Anonymous], 1981, P NAT TEL C NEW ORL
   [Anonymous], 2018, NVIDIA CUDA C programming guide
   [Anonymous], 2007, ISMM
   Bjontegaard G., 2008, VCEGAI11 ITUT SG16 Q
   Bossen F., 2013, JCTVCL1100 ITUTISOIE
   Bross B., 2013, JCTVCL1003 ITUTISOIE
   Chen WN, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P697
   de Souza DF, 2017, IEEE T MULTIMEDIA, V19, P459, DOI 10.1109/TMM.2016.2625261
   Duan LY, 2018, IEEE T IMAGE PROCESS, V27, P2201, DOI 10.1109/TIP.2018.2794203
   Fan HF, 2016, IEEE T MULTIMEDIA, V18, P537, DOI 10.1109/TMM.2016.2515365
   Fan R, 2017, IEEE T MULTIMEDIA, V19, P893, DOI 10.1109/TMM.2016.2642786
   Gao Y, 2016, IEEE T MULTIMEDIA, V18, P2321, DOI 10.1109/TMM.2016.2598481
   Hu Q, 2016, IEEE T MULTIMEDIA, V18, P379, DOI 10.1109/TMM.2015.2512799
   JCT-VC, 2013, JCTVCL1002 ITUTISOIE
   JCT-VC, 2018, HEVC TEST MOD HM
   Kim T. S., 2017, INVESTIGATION ANTIMI, V99, P1
   Li X, 2014, CHINA-EU LAW SER, V1, P1, DOI 10.1007/978-3-642-41024-6
   Li XF, 2015, IEEE INT SYMP CIRC S, P2784, DOI 10.1109/ISCAS.2015.7169264
   LI Y, 2015, PROC VIS COMMUN IMAG, P1
   Liao ZT, 2015, INT SOC DESIGN CONF, P267, DOI 10.1109/ISOCC.2015.7401750
   Luo FL, 2015, IEEE INT SYMP CIRC S, P1122, DOI 10.1109/ISCAS.2015.7168835
   Ma JC, 2014, 2014 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING CONFERENCE, P282, DOI 10.1109/VCIP.2014.7051559
   Ma SW, 2013, IEEE DATA COMPR CONF, P73, DOI 10.1109/DCC.2013.15
   Momcilovic S, 2014, IEEE T MULTIMEDIA, V16, P108, DOI 10.1109/TMM.2013.2284892
   Ohm JR, 2012, IEEE T CIRC SYST VID, V22, P1669, DOI 10.1109/TCSVT.2012.2221192
   Pan ZQ, 2013, INT CONF ACOUST SPEE, P1389, DOI 10.1109/ICASSP.2013.6637879
   Po LM, 1996, IEEE T CIRC SYST VID, V6, P313, DOI 10.1109/76.499840
   Purnachand N., 2012, 2012 IEEE Second International Conference on Consumer Electronics - Berlin (ICCE-Berlin), P34, DOI 10.1109/ICCE-Berlin.2012.6336494
   Purnachand N., 2012, 2012 International Conference on Systems, Signals and Image Processing (IWSSIP), P388
   Qin Yu, 2012, 2012 Asia-Pacific Signal and Information Processing Association 2012 Annual Summit and Conference (APSIPA ASC 2012)
   Radicke S, 2014, IEEE T CONSUM ELECTR, V60, P728, DOI 10.1109/TCE.2014.7027349
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Stone JE, 2010, COMPUT SCI ENG, V12, P66, DOI 10.1109/MCSE.2010.69
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sun W, 2017, IEEE IMAGE PROC, P1122, DOI 10.1109/ICIP.2017.8296456
   Tohidypour HR, 2016, IEEE T MULTIMEDIA, V18, P182, DOI 10.1109/TMM.2015.2510332
   Wang SS, 2013, IEEE IMAGE PROC, P2005, DOI 10.1109/ICIP.2013.6738413
   Wang X., 2013, IEEE INT C MULT EXP, P1, DOI DOI 10.1109/ICMEW.2013.6618412
   Xiao W, 2015, IEEE T CIRC SYST VID, V25, P1830, DOI 10.1109/TCSVT.2015.2406199
   Xiong J, 2015, IEEE T MULTIMEDIA, V17, P2147, DOI 10.1109/TMM.2015.2491018
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P2141, DOI 10.1109/TMM.2014.2356795
   Xiong J, 2014, IEEE T MULTIMEDIA, V16, P559, DOI 10.1109/TMM.2013.2291958
   Zhang XF, 2017, IEEE T CIRC SYST VID, V27, P2177, DOI 10.1109/TCSVT.2016.2581618
   Zhu C, 2002, IEEE T CIRC SYST VID, V12, P349, DOI 10.1109/TCSVT.2002.1003474
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
NR 45
TC 19
Z9 19
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 851
EP 862
DI 10.1109/TMM.2018.2867260
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700004
DA 2024-07-18
ER

PT J
AU Yang, MM
   Liu, JC
   Li, ZG
AF Yang, Minmin
   Liu, Jianchang
   Li, Zhengguo
TI Superpixel-Based Single Nighttime Image Haze Removal
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Nighttime image haze removal; glow decomposition; morphologic artifacts;
   superpixel segmentation; weighted guided image filtering
ID VISION
AB Haze removal is important to improve performance of outdoor vision systems. However, it is challenging to remove haze from a single nighttime haze image. In this paper, a novel superpixel-based single image haze removal algorithm is proposed for nighttime haze images. The input nighttime image is first decomposed into a glow image and a glow-free nighttime haze image using their relative smoothness. A superpixel-based method is then introduced to compute the value of the atmospheric light and dark channel for each pixel in the glow-free haze image. The transmission map is decomposed from the dark channel of the glow-free haze image by the weighted guided image filter. Since superpixels usually adhere to the boundaries of objects well, a smaller local window size can be selected. As such, details in areas of fine structures are preserved better. In addition, to avoid noticeable noise in the sky area, an adaptive threshold is added to the transmission map when the nighttime haze image is restored. Experiments show that our method produces better results than the existing haze removal algorithms for nighttime haze images.
C1 [Yang, Minmin; Liu, Jianchang] Northeastern Univ, State Key Lab Synthet Automat Proc Ind, Shenyang 110819, Liaoning, Peoples R China.
   [Yang, Minmin; Liu, Jianchang] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Liaoning, Peoples R China.
   [Li, Zhengguo] Inst Infocomm Res, Signal Proc Dept, Singapore 138632, Singapore.
C3 Northeastern University - China; Northeastern University - China; Agency
   for Science Technology & Research (A*STAR); A*STAR - Institute for
   Infocomm Research (I2R)
RP Yang, MM (corresponding author), Northeastern Univ, State Key Lab Synthet Automat Proc Ind, Shenyang 110819, Liaoning, Peoples R China.; Yang, MM (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Liaoning, Peoples R China.
EM ymm_2011@163.com; liujianchang@ise.neu.edu.cn; ezgli@i2r.a-star.edu.sg
RI wen, Wen/KBB-1727-2024
OI Li, Zhengguo/0000-0002-4525-1204
FU National Natural Science Foundation of China (NSFC) [61773106, 61703086]
FX This work was supported by the National Natural Science Foundation of
   China (NSFC) (No. 61773106, No. 61703086).
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Ancuti C, 2016, IEEE IMAGE PROC, P2256, DOI 10.1109/ICIP.2016.7532760
   Choi LK, 2015, IEEE T IMAGE PROCESS, V24, P3888, DOI 10.1109/TIP.2015.2456502
   He KM, 2011, IEEE T PATTERN ANAL, V33, P2341, DOI 10.1109/TPAMI.2010.168
   He KM, 2013, IEEE T PATTERN ANAL, V35, P1397, DOI 10.1109/TPAMI.2012.213
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Kou F, 2015, IEEE T IMAGE PROCESS, V24, P4528, DOI 10.1109/TIP.2015.2468183
   Lai YH, 2015, IEEE T CIRC SYST VID, V25, P1, DOI 10.1109/TCSVT.2014.2329381
   Li Y, 2015, IEEE I CONF COMP VIS, P226, DOI 10.1109/ICCV.2015.34
   Li Y, 2014, PROC CVPR IEEE, P2752, DOI 10.1109/CVPR.2014.346
   Li ZG, 2015, INT CONF ACOUST SPEE, P1608, DOI 10.1109/ICASSP.2015.7178242
   Li ZG, 2018, IEEE T IMAGE PROCESS, V27, P442, DOI 10.1109/TIP.2017.2750418
   Li ZG, 2016, INT CONF ACOUST SPEE, P1756, DOI 10.1109/ICASSP.2016.7471978
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P5432, DOI 10.1109/TIP.2015.2482903
   Li ZG, 2015, IEEE T IMAGE PROCESS, V24, P120, DOI 10.1109/TIP.2014.2371234
   Liu FY, 2016, IEEE T PATTERN ANAL, V38, P2024, DOI 10.1109/TPAMI.2015.2505283
   Narasimhan SG, 2003, PROC CVPR IEEE, P665
   Narasimhan SG, 2002, INT J COMPUT VISION, V48, P233, DOI 10.1023/A:1016328200723
   Pei SC, 2012, IEEE IMAGE PROC, P957, DOI 10.1109/ICIP.2012.6467020
   Schechner YY, 2003, APPL OPTICS, V42, P511, DOI 10.1364/AO.42.000511
   Shih KT, 2016, IEEE T MULTIMEDIA, V18, P300, DOI 10.1109/TMM.2015.2503918
   Wang WC, 2017, IEEE T MULTIMEDIA, V19, P1142, DOI 10.1109/TMM.2017.2652069
   Xu HT, 2014, IEEE T MULTIMEDIA, V16, P68, DOI 10.1109/TMM.2013.2283453
   Yang MM, 2016, CHIN CONT DECIS CONF, P1965, DOI 10.1109/CCDC.2016.7531305
   Zhang J, 2017, PROC CVPR IEEE, P5226, DOI 10.1109/CVPR.2017.555
   Zhang J, 2014, IEEE IMAGE PROC, P4557, DOI 10.1109/ICIP.2014.7025924
   Zhu QS, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P113, DOI 10.1109/ROBIO.2014.7090316
NR 27
TC 45
Z9 47
U1 1
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3008
EP 3018
DI 10.1109/TMM.2018.2820327
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800011
DA 2024-07-18
ER

PT J
AU Wang, JZ
   Wang, WM
   Gao, W
AF Wang, Jinzhuo
   Wang, Wenmin
   Gao, Wen
TI Multiscale Deep Alternative Neural Network for Large-Scale Video
   Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video classification; deep alternative neural network; multi-scale deep
   network; human activity; natural disaster; new dataset
ID ACTION RECOGNITION; DENSE
AB With the rapid increase in the amount of multimedia data, video classification has become a demanding and challenging research topic. Compared with image classification, video classification requires mapping a video that contains hundreds of frames to semantic tags, which poses many challenges to the direct use of advanced models originally designed for image-oriented tasks. On the other hand, continuous frames in a video also give us more visual clues that we can leverage to achieve better classification. One of the most important clues is the context in the spatiotemporal domain. In this paper, we introduce the multiscale deep alternative neural network (DANN), a novel architecture combining the strengths of both convolutional neural network and recurrent neural networks to achieve a deep network that can collect rich context hierarchies for video classification. In particular, the DANN is stacked with alternative layers, each of which consists of a volumetric convolutional layer followed by a recurrent layer. The former acts as a local feature learner, whereas the latter is used to collect contexts. Compared with popular deep feed-forward neural networks, the DANN learns local features and their contexts from the very beginning. This setting enables preserving context evolutions, which we show to be essential for improving the accuracy of video classification. To release the full potential of the DANN, we develop a deeper version with stochastic-layer skip-connections and construct a multiscale DANN to incorporate contexts at different scales. We show how to apply the multiscale DANN for video classification with carefully designed configurations in terms of both input-output settings and training-testing methods. The DANN is shown to be robust to not only human-centric videos, but also natural videos. As there are few large-scale natural disaster video datasets, we construct a new large-scale one and make it publicly available. Experiments on four datasets show the effectiveness of our method for both human actions and natural events.
C1 [Wang, Jinzhuo; Wang, Wenmin; Gao, Wen] Peking Univ, Dept Elect & Comp Engn, Beijing 100080, Peoples R China.
C3 Peking University
RP Wang, WM (corresponding author), Peking Univ, Dept Elect & Comp Engn, Beijing 100080, Peoples R China.
EM cr7or9@163.com; wangwm@ece.pku.edu.cn; wgao@pku.edu.cn
RI Wang, Wenmin/W-3511-2019
OI Wang, Wenmin/0000-0003-2664-4413; Wang, Jinzhuo/0000-0002-9464-4426
FU Shenzhen Peacock Plan [20130408-183003656]; Shenzhen Key Laboratory for
   Intelligent Multimedia and Virtual Reality [ZDSYS201703031405467];
   National Natural Science Foundation of China [U1613209]
FX This work was supported in part by the Shenzhen Peacock Plan under Grant
   20130408-183003656, in part by the Shenzhen Key Laboratory for
   Intelligent Multimedia and Virtual Reality under Grant
   ZDSYS201703031405467, and in part by the National Natural Science
   Foundation of China under Grant U1613209. The associate editor
   coordinating the review of this manuscript and approving it for
   publicationwas Shibasaki Ryosuke.
CR [Anonymous], 2017, ARXIV170400389
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Collobert R., 2011, P BIGLEARN NIPS WORK
   Cui P, 2012, IEEE T MULTIMEDIA, V14, P102, DOI 10.1109/TMM.2011.2176110
   Czarnecki WM, 2017, ADV NEUR IN, V30
   de Souza Cesar Roberto, 2017, P IEEE C COMP VIS PA, P4757
   Dean J., 2015, NIPS DEEP LEARNING R
   DIMITROVA N, 1995, ACM T INFORM SYST, V13, P408, DOI 10.1145/211430.211433
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eigen D., 2013, arXiv:1312.1847
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   HEILBRON FC, 2015, PROC CVPR IEEE, P961, DOI DOI 10.1109/CVPR.2015.7298698
   HUANG G, 2017, PROC CVPR IEEE, P2261, DOI DOI 10.1109/CVPR.2017.243
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe S, 2015, P INT C MACH LEARN, V2015, P1
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kantorov V, 2014, PROC CVPR IEEE, P2593, DOI 10.1109/CVPR.2014.332
   Kar A, 2017, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR.2017.604
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Klaser A., 2008, P BMVC, P275, DOI DOI 10.5244/C.22.99
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Krüger B, 2017, IEEE T MULTIMEDIA, V19, P797, DOI 10.1109/TMM.2016.2635030
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Lan ZZ, 2017, IEEE COMPUT SOC CONF, P1219, DOI 10.1109/CVPRW.2017.161
   Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756
   Lazebnik S., 2006, P IEEE CVF C COMP VI, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/CVPR.2006.68]
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Liang M., 2015, Advances in Neural Information Processing Systems, P937
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   NIYOGI SA, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P469, DOI 10.1109/CVPR.1994.323868
   Patrona F, 2016, IEEE T MULTIMEDIA, V18, P967, DOI 10.1109/TMM.2016.2535357
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38
   Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11
   Pouyanfar S, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3150226
   Pouyanfar S, 2016, PROCEEDINGS OF 2016 IEEE 17TH INTERNATIONAL CONFERENCE ON INFORMATION REUSE AND INTEGRATION (IEEE IRI), P556, DOI 10.1109/IRI.2016.82
   Qiu ZF, 2017, IEEE I CONF COMP VIS, P5534, DOI 10.1109/ICCV.2017.590
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sadanand S, 2012, PROC CVPR IEEE, P1234, DOI 10.1109/CVPR.2012.6247806
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Scovanner P., 2007, P 15 ACM INT C MULT, P357, DOI DOI 10.1145/1291233.1291311
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Sharma A., 2014, ADV NEURAL INFORM PR, V27, P2447
   Shi YM, 2017, IEEE I CONF COMP VIS, P716, DOI 10.1109/ICCV.2017.84
   Shi YM, 2017, IEEE T MULTIMEDIA, V19, P1510, DOI 10.1109/TMM.2017.2666540
   Simonyan K., 2014, P 27 INT C NEUR INF, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smoliar S. W., 1994, IEEE Multimedia, V1, P62, DOI 10.1109/93.311653
   Socher R., 2012, NIPS, V3, P8
   Soomro Khurram, 2012, UCF101 DATASET 101 H
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2015, ADV NEURAL INFORM PR, P2377, DOI DOI 10.48550/ARXIV.1505.00387
   Sun L, 2017, IEEE I CONF COMP VIS, P2166, DOI 10.1109/ICCV.2017.236
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Wang H., 2009, BMVC
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang J, 2011, PROC CVPR IEEE, DOI 10.1109/CVPR.2011.5995493
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang LM, 2016, INT J COMPUT VISION, V119, P254, DOI 10.1007/s11263-015-0859-0
   Wang LM, 2014, LECT NOTES COMPUT SC, V8693, P565, DOI 10.1007/978-3-319-10602-1_37
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Wang X., 2017, PROC CVPR IEEE, P2097, DOI [DOI 10.1109/CVPR.2017.369, 10.1109/CVPR.2017.369]
   WATERS RL, 1972, J ANAT, V111, P191
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48
   Wu ZX, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P791, DOI 10.1145/2964284.2964328
   Xiong W, 1998, COMPUT VIS IMAGE UND, V71, P166, DOI 10.1006/cviu.1998.0711
   Xu WR, 2017, IEEE T MULTIMEDIA, V19, P1494, DOI 10.1109/TMM.2017.2674622
   Yang YM, 2018, IEEE T MULTIMEDIA, V20, P1024, DOI 10.1109/TMM.2017.2760623
   Yi Zhu, 2016, Computer Vision - ECCV 2016. 14th European Conference: Workshops. Proceedings: LNCS 9913, P668, DOI 10.1007/978-3-319-46604-0_47
   Yu G, 2011, IEEE T MULTIMEDIA, V13, P507, DOI 10.1109/TMM.2011.2128301
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhou B., 2014, CORR, V1412, P6856
   Zhu WJ, 2016, PROC CVPR IEEE, P1991, DOI 10.1109/CVPR.2016.219
   Zuo Z, 2015, 2015 IEEE C COMP VIS, P18, DOI [10.1109/CVPRW.2015.7301268, DOI 10.1109/CVPRW.2015.7301268]
NR 91
TC 21
Z9 22
U1 3
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2578
EP 2592
DI 10.1109/TMM.2018.2855081
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000004
DA 2024-07-18
ER

PT J
AU Buitelaar, P
   Wood, ID
   Negi, S
   Arcan, M
   McCrae, JP
   Abele, A
   Robin, C
   Andryushechkin, V
   Ziad, H
   Sagha, H
   Schmitt, M
   Schuller, BW
   Sánchez-Rada, JF
   Iglesias, CA
   Navarro, C
   Giefer, A
   Heise, N
   Masucci, V
   Danza, FA
   Caterino, C
   Smrz, P
   Hradis, M
   Povolny, F
   Klimes, M
   Matejka, P
   Tummarello, G
AF Buitelaar, Paul
   Wood, Ian D.
   Negi, Sapna
   Arcan, Mihael
   McCrae, John P.
   Abele, Andrejs
   Robin, Cecile
   Andryushechkin, Vladimir
   Ziad, Housam
   Sagha, Hesam
   Schmitt, Maximilian
   Schuller, Bjoern W.
   Fernando Sanchez-Rada, J.
   Iglesias, Carlos A.
   Navarro, Carlos
   Giefer, Andreas
   Heise, Nicolaus
   Masucci, Vincenzo
   Danza, Francesco A.
   Caterino, Ciro
   Smrz, Pavel
   Hradis, Michal
   Povolny, Filip
   Klimes, Marek
   Matejka, Pavel
   Tummarello, Giovanni
TI MixedEmotions: An Open-Source Toolbox for Multimodal Emotion Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion analysis; open source toolbox; affective computing; linked data;
   audio processing; text processing; video processing
ID RECOGNITION; SPEECH; MODEL
AB Recently, there is an increasing tendency to embed functionalities for recognizing emotions from user-generated media content in automated systems such as call-centre operations, recommendations, and assistive technologies, providing richer and more informative user and content profiles. However, to date, adding these functionalities was a tedious, costly, and time-consuming effort, requiring identification and integration of diverse tools with diverse interfaces as required by the use case at hand. The MixedEmotions Toolbox leverages the need 14 such functionalities by providing tools for text, audio, video, and linked data processing within an easily integrable plug-and-play platform. These functionalities include: 1) for text processing: emotion and sentiment recognition; 2) for audio processing: emotion, age, and gender recognition; 3) for video processing: face detection and tracking, emotion recognition, facial landmark localization, head pose estimation, face alignment, and body pose estimation; and 4) for linked data: knowledge graph integration. Moreover, the MixedEmotions Toolbox is open-source and free. In this paper, we present this toolbox in the context of the existing landscape, and provide a range of detailed benchmarks on standard test-beds showing its state-of-the-art performance. Furthermore, three real-world use cases show its effectiveness, namely, emotion-driven smart TV, call center monitoring, and brand reputation analysis.
C1 [Buitelaar, Paul; Wood, Ian D.; Negi, Sapna; Arcan, Mihael; McCrae, John P.; Abele, Andrejs; Robin, Cecile; Andryushechkin, Vladimir; Ziad, Housam] Natl Univ Ireland Galway, Galway, Ireland.
   [Sagha, Hesam; Schmitt, Maximilian; Schuller, Bjoern W.] Univ Passau, Chair Complex & Intelligent Syst, D-94032 Passau, Germany.
   [Sagha, Hesam] AudEERING GmbH, D-82205 Gilching, Germany.
   [Schuller, Bjoern W.] Imperial Coll London, Dept Comp, London SW7 2AZ, England.
   [Fernando Sanchez-Rada, J.; Iglesias, Carlos A.] GSI Univ Politecn Madrid, Madrid 28040, Spain.
   [Navarro, Carlos] Paradigma Digital, Madrid 28224, Spain.
   [Giefer, Andreas; Heise, Nicolaus] Deutsch Welle, D-53113 Bonn, Germany.
   [Masucci, Vincenzo; Danza, Francesco A.; Caterino, Ciro] Expert Syst, I-41123 Modena, Italy.
   [Smrz, Pavel; Hradis, Michal] Brno Univ Technol, Brno 60190, Czech Republic.
   [Povolny, Filip; Klimes, Marek; Matejka, Pavel] Phonexia, Brno 61200, Czech Republic.
   [Tummarello, Giovanni] Siren Solut, Dublin, Ireland.
C3 Ollscoil na Gaillimhe-University of Galway; University of Passau;
   Imperial College London; Brno University of Technology
RP Sagha, H (corresponding author), AudEERING GmbH, D-82205 Gilching, Germany.
EM paul.buitelaar@insight-centre.org; ian.wood@insight-centre.org;
   sapna.negi@insight-centre.org; mihael.arcan@insight-centre.org;
   John.McCrae@insight-centre.org; andrejs.abele@insight-centre.org;
   cecile.robin@insight-centre.org;
   vladimir.andryushechkin@insight-centre.org;
   housam.ziad@insight-centre.org; hesamsga81@gmail.com;
   maximilian.schmitt@uni-passau.de; bjoern.schuller@imperial.ac.uk;
   jf.sanchez@upm.es; cif@dit.upm.es; cnavarro@paradigmadigital.com;
   andreas.giefer@dw.com; nicolaus.heise@dw.com; vmasucci@expertsystem.com;
   fadanza@gmail.com; ccaterino@expertsystem.com; smrz@fit.vutbr.cz;
   ihradis@fit.vutbr.cz; filip.povolny@phonexia.com; klimes@phonexia.com;
   matejka@phonexia.com; giovanni@siren.solutions
RI Schmitt, Maximilian/ABD-4551-2020; Wood, Ian/N-1901-2019; Matejka,
   Pavel/AAP-7982-2020; Smrz, Pavel/A-4763-2016; Negi, Sapna/HGA-5265-2022;
   McCrae, John P/P-8625-2016; IGLESIAS, CARLOS A./I-2181-2015; Hradiš,
   Michal/G-9365-2016; Schuller, Björn Wolfgang/D-3241-2011
OI Schmitt, Maximilian/0000-0001-7453-5612; Wood, Ian/0000-0002-6094-0358;
   Matejka, Pavel/0000-0002-0404-8672; IGLESIAS, CARLOS
   A./0000-0002-1755-2712; Hradiš, Michal/0000-0002-6364-129X; Schuller,
   Björn Wolfgang/0000-0002-6478-8699; Smrz, Pavel/0000-0002-5638-1362;
   Arcan, Mihael/0000-0002-3116-621X; Sagha, Hesam/0000-0002-8644-9591;
   Ziad, Housam/0000-0001-6132-4082
FU European Unions Horizon 2020 Programme research and innovation programme
   [644632]; H2020 - Industrial Leadership [644632] Funding Source: H2020 -
   Industrial Leadership
FX This work was supported by the European Unions Horizon 2020 Programme
   research and innovation programme under Grant 644632 (MixedEmotions).
CR Alhabash S, 2015, NEW MEDIA SOC, V17, P1317, DOI 10.1177/1461444814523726
   [Anonymous], 2016, INT CONF ACOUST SPEE
   [Anonymous], 2017, IEEE T MULTIMEDIA, DOI DOI 10.1109/TMM.2017.2692650
   [Anonymous], 2016, P 8 GLOB WORDNET C 2
   [Anonymous], 2015, CEUR Workshop Proceedings
   [Anonymous], 2016, P 7 WORKSH COMP APPR
   [Anonymous], 2016, P IEEE WINT C APPL C
   [Anonymous], 2016, 26 INT C COMP LING C
   [Anonymous], 2013, Journal of Data Analysis and Information Processing, DOI [10.4236/jdaip.2013.13004, DOI 10.4236/JDAIP.2013.13004]
   [Anonymous], 2012, Introducing the Knowledge Graph: things, not strings
   [Anonymous], P ACII SAN ANT TX US
   [Anonymous], 2013, 2 JOINT C LEX COMP S
   [Anonymous], P 4 WORKSH LINK DAT
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2016, P 5 JOINT C LEX COMP
   [Anonymous], 2007, Technical report
   [Anonymous], P 3 INT WORKSH AFF S
   [Anonymous], SIGNALS COMMUNICATIO
   [Anonymous], 2011, P 4 INT WORKSHOP SOC
   [Anonymous], 2012, P C N AM ASS COMP LI
   [Anonymous], 2017, P 8 WORKSH COMP APPR
   [Anonymous], 2016, Tech. Rep.
   [Anonymous], P 1 INT C GLOB WORDN
   [Anonymous], 2014, Association for Computing Machinery SIGKDD Explorations Newsletter, DOI [DOI 10.1145/2641190.2641195, 10.1145/2641190.2641195]
   [Anonymous], 2013, 2013 10 IEEE INT C W, DOI [10.1109/FG.2013.6553805, DOI 10.1109/FG.2013.6553805]
   [Anonymous], 2017, P 8 WORKSH COMP APPR
   [Anonymous], 1987, THESIS U TWENTE ENSC
   [Anonymous], INTERNET ENG TASK FO
   [Anonymous], 1983, Conceptual structures: information processing in mind and machine
   [Anonymous], 2013, P 2013 C EMP METH NA
   Arapakis I, 2009, IEEE INT CON MULTI, P1440, DOI 10.1109/ICME.2009.5202773
   Auer S, 2007, LECT NOTES COMPUT SC, V4825, P722, DOI 10.1007/978-3-540-76298-0_52
   Bizer C, 2011, SEMANTIC SERVICES, INTEROPERABILITY AND WEB APPLICATIONS: EMERGING CONCEPTS, P205, DOI 10.4018/978-1-60960-593-3.ch008
   Bo Pang, 2008, Foundations and Trends in Information Retrieval, V2, P1, DOI 10.1561/1500000001
   Bradley M.M., 1999, PSYCHOLOGY
   Buechel S., 2017, 15 C EUR CHAPT ASS C, P578, DOI 10.18653/V1/E17-2092
   Cao Z., 2017, P IEEE C COMP VIS PA, P7291
   Cheng J, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P925, DOI 10.1145/2566486.2567997
   De Domenico M, 2013, SCI REP-UK, V3, DOI 10.1038/srep02980
   EKMAN P, 1971, J PERS SOC PSYCHOL, V17, P124, DOI 10.1037/h0030377
   Esuli Andrea., 2006, LREC 2006 Proceedings, 2006, S, P417
   Eyben F., 2013, P 21 ACM INT C MULT, P835, DOI DOI 10.1145/2502081.2502224
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Feraru SM, 2015, INT CONF AFFECT, P125, DOI 10.1109/ACII.2015.7344561
   Sánchez-Rada JF, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P735, DOI 10.1109/DSAA.2016.79
   Sánchez-Rada JF, 2016, INFORM PROCESS MANAG, V52, P99, DOI 10.1016/j.ipm.2015.03.007
   Fontaine JRJ, 2007, PSYCHOL SCI, V18, P1050, DOI 10.1111/j.1467-9280.2007.02024.x
   Go Alec., 2009, CS224N project report 1.12
   Goodfellow IJ, 2015, NEURAL NETWORKS, V64, P59, DOI 10.1016/j.neunet.2014.09.005
   Graves A, 2005, LECT NOTES COMPUT SC, V3697, P799
   Hantke S, 2016, LREC 2016 - TENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P2156
   Hellmann S, 2013, LECT NOTES COMPUT SC, V8219, P98, DOI 10.1007/978-3-642-41338-4_7
   Hoede C., 1994, Journal of Computer Assisted Learning, V10, P104, DOI 10.1111/j.1365-2729.1994.tb00287.x
   Hu M., 2004, P 10 ACM SIGKDD INT, P168, DOI DOI 10.1145/1014052.1014073
   Hu X., 2013, WSDM
   Hutto C., 2014, P INT AAAI C WEB SOC, V8, P216, DOI [DOI 10.1609/ICWSM.V8I1.14550, 10.1609/icwsm.v8i1.14550]
   Kazemi V, 2014, PROC CVPR IEEE, P1867, DOI 10.1109/CVPR.2014.241
   Köstinger M, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Kramer ADI, 2014, P NATL ACAD SCI USA, V111, P8788, DOI 10.1073/pnas.1320040111
   Kunpeng Zhang, 2013, 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM), P223
   Lanthaler M., 2012, Third International Workshop on RESTful Design, P25, DOI [10.1145/2307819.2307827, DOI 10.1145/2307819.2307827]
   Lewis RJ, 2014, J COMMUN, V64, P397, DOI 10.1111/jcom.12101
   LIN LI, 1989, BIOMETRICS, V45, P255, DOI 10.2307/2532051
   Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010
   Mao QR, 2014, IEEE T MULTIMEDIA, V16, P2203, DOI 10.1109/TMM.2014.2360798
   Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4
   McKeown G, 2010, IEEE INT CON MULTI, P1079, DOI 10.1109/ICME.2010.5583006
   Mencattini A, 2017, IEEE T AFFECT COMPUT, V8, P314, DOI 10.1109/TAFFC.2016.2531664
   Mohammad Saif., 2012, Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval'12, P246
   Negoescu RA, 2010, IEEE T MULTIMEDIA, V12, P399, DOI 10.1109/TMM.2010.2050649
   Oliver MB, 2011, J COMMUN, V61, P984, DOI 10.1111/j.1460-2466.2011.01585.x
   Osgood C. E., 1957, The measurement of meaning
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Parkhi OM, 2015, DEEP FACE RECOGNITIO
   Pavelková A, 2015, IEEE INT C INTELL TR, P1545, DOI 10.1109/ITSC.2015.252
   Plutchik R., 1980, Theories of Emotion, P3, DOI DOI 10.1016/B978-0-12-558701-3.50007-7
   Popková A, 2016, LECT NOTES ARTIF INT, V9924, P426, DOI 10.1007/978-3-319-45510-5_49
   Posner J, 2005, DEV PSYCHOPATHOL, V17, P715, DOI 10.1017/S0954579405050340
   Rawat S., 2013, Proc. Interspeech, P2929
   Redondo J, 2007, BEHAV RES METHODS, V39, P600, DOI 10.3758/BF03193031
   Ringeval Fabien, 2017, P 7 ANN WORKSHOP AUD, P3, DOI DOI 10.1145/3133944.3133953
   Rosenthal S., 2015, P 9 INT WORKSH SEM E, P451
   Russell JA, 2003, PSYCHOL REV, V110, P145, DOI 10.1037/0033-295X.110.1.145
   Sagha H, 2016, INTERSPEECH, P2949, DOI 10.21437/Interspeech.2016-333
   Sagha H, 2017, WIRES DATA MIN KNOWL, V7, DOI 10.1002/widm.1212
   Sagha H, 2016, INT CONF ACOUST SPEE, P5800, DOI 10.1109/ICASSP.2016.7472789
   Scherer KR, 2005, SOC SCI INFORM, V44, P695, DOI 10.1177/0539018405058216
   Schmitt M. and Schuller B., 2017, The Journal of Machine Learning Research, V18, P3370, DOI 10.5555/3122009.3176840
   Schmitt M, 2016, INTERSPEECH, P495, DOI 10.21437/Interspeech.2016-1124
   Schröder M, 2011, LECT NOTES COMPUT SC, V6974, P316, DOI 10.1007/978-3-642-24600-5_35
   Schuller B, 2015, WIRES DATA MIN KNOWL, V5, P255, DOI 10.1002/widm.1159
   Shepstone SE, 2014, IEEE T MULTIMEDIA, V16, P1999, DOI 10.1109/TMM.2014.2337845
   Speriosu M., 2011, WORKSH UNS LEARN NLP, P53
   Strapparava C., 2004, Lrec, Volume, V4, P1083
   Strapparava C., 2007, P 4 INT WORKSH SEM E, P70
   Tan Chenhao., 2011, P 17 ACM SIGKDD INT, P1397, DOI DOI 10.1145/2020408.2020614
   Tarvainen J, 2014, IEEE T MULTIMEDIA, V16, P2085, DOI 10.1109/TMM.2014.2357688
   Tkalcic M, 2013, IEEE T MULTIMEDIA, V15, P391, DOI 10.1109/TMM.2012.2229970
   Trigeorgis G, 2016, INT CONF ACOUST SPEE, P5200, DOI 10.1109/ICASSP.2016.7472669
   Tufis Dan, 2004, Romanian Journal of Information Science and Technology, V7, P9
   Valstar M, 2016, PROCEEDINGS OF THE 6TH INTERNATIONAL WORKSHOP ON AUDIO/VISUAL EMOTION CHALLENGE (AVEC'16), P3, DOI 10.1145/2988257.2988258
   Vassiliadis P, 2009, INT J DATA WAREHOUS, V5, P1, DOI 10.4018/jdwm.2009070101
   Vossen Piek., 1998, EuroWordNet: A Multilingual Database with Lexical Semantic Networks
   Weninger F, 2015, J MACH LEARN RES, V16, P547
   Wood E, 2015, IEEE I CONF COMP VIS, P3756, DOI 10.1109/ICCV.2015.428
   Wood I.Sebastian Ruder., 2016, P EMOTION SENTIMENT, P76
   Wood ID, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P1197
   Yang J., 2011, P 4 ACM INT C WEB SE, P177, DOI DOI 10.1145/1935826.1935863
   Yu L, 2016, ANN ALLERTON CONF, P540, DOI 10.1109/ALLERTON.2016.7852278
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhang ZX, 2016, INTERSPEECH, P3593, DOI 10.21437/Interspeech.2016-998
NR 113
TC 21
Z9 26
U1 0
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2454
EP 2465
DI 10.1109/TMM.2018.2798287
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200017
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Kim, M
   Sung, M
   Kim, M
   Ro, WW
AF Kim, Minsik
   Sung, Minyong
   Kim, Minwoo
   Ro, Won Woo
TI Exploiting Pseudo-Quadtree Structure for Accelerating HEVC Spatial
   Resolution Downscaling Transcoder
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Open-loop transcoding; spatial resolution downscaling; high efficiency
   video coding (HEVC)
ID H.264/AVC
AB In this paper, a novel method to accelerate the spatial resolution downscaling transcoding operation for high efficiency video coding (HEVC) is proposed. The proposed transcoder first extracts the information about the coding unit (CU) structure during the decoding process and analyzes it to construct a pseudo-quadtree of the target resolution. By utilizing the constructed pseudo-quadtree, the encoder process is accelerated by searching only the optimal depth in the quadtree structure. To evaluate the performance of the proposed transcoding method, its transcoding time and bitrate loss are measured and compared with those of the HEVC reference encoder and decoder. Also, a comparison with state-of-the-art fast CU size decision methods and spatial resolution downscaling transcoding methods for HEVC is presented. The proposed method achieves an average speedup of 1.65 with an average bitrate loss of 2.06%.
C1 [Kim, Minsik; Ro, Won Woo] Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.
   [Sung, Minyong; Kim, Minwoo] Samsung Elect Co Ltd, Suwon 16677, South Korea.
C3 Yonsei University; Samsung Electronics; Samsung
RP Ro, WW (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.
EM minsik.kim@yonsei.ac.kr; minyong.sung@samsung.com;
   minwoo87.kim@samsung.com; wro@yonsei.ac.kr
RI Kim, Minsik/HRC-8251-2023
OI Kim, Minsik/0000-0001-6177-522X
FU Institute for Information & Communications Technology Promotion (IITP) -
   Korean government (MSIT) [2016-0-00140]; Digital Media and Communication
   Research and Development Team, Samsung Electronics Company Ltd.; Suwon,
   South Korea; Graduate School of YONSEI University
FX This work was supported in part by the Institute for Information &
   Communications Technology Promotion (IITP) grant funded by the Korean
   government (MSIT) (2016-0-00140, Development of Application Program
   Optimization Tools for High Performance Computing Systems); in part by
   the Digital Media and Communication Research and Development Team,
   Samsung Electronics Company Ltd.; Suwon, South Korea; and in part by the
   Graduate School of YONSEI University Research Scholarship Grants in
   2017.
CR Ahmad I, 2005, IEEE T MULTIMEDIA, V7, P793, DOI 10.1109/TMM.2005.854472
   [Anonymous], 2014, 18 IEEE INT S CONS E
   [Anonymous], 2001, CALC AV PSNR DIFF RD
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Bosson F., 2013, JCTVCL1100 ITUTISOIE
   Boyce J., 2014, JCTVCR1013 ITUTISOIE
   Boyce J. M., IEEE T CIRCUITS SYST, V26, P20
   Choi K, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.3.030502
   Correa G, 2015, IEEE T CIRC SYST VID, V25, P660, DOI 10.1109/TCSVT.2014.2363753
   De Cock J, 2008, IEEE IMAGE PROC, P1208, DOI 10.1109/ICIP.2008.4711978
   Dong Zhang, 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P651, DOI 10.1109/ICME.2012.112
   Jiang W, 2014, MULTIMED TOOLS APPL, V73, P2179, DOI 10.1007/s11042-013-1675-6
   Jie Leng, 2011, 2011 International Conference on Multimedia and Signal Processing (CMSP), P56, DOI 10.1109/CMSP.2011.167
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim HS, 2016, IEEE T CIRC SYST VID, V26, P130, DOI 10.1109/TCSVT.2015.2444672
   Kim J, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P449, DOI 10.1109/PCS.2012.6213251
   Lambert P, 2006, IEEE T CIRC SYST VID, V16, P134, DOI 10.1109/TCSVT.2005.857783
   Van LP, 2016, IEEE T MULTIMEDIA, V18, P364, DOI 10.1109/TMM.2015.2512231
   Van LP, 2015, IEEE T CONSUM ELECTR, V61, P507, DOI 10.1109/TCE.2015.7389806
   Nguyen VA, 2015, IEEE INT SYMP CIRC S, P1286, DOI 10.1109/ISCAS.2015.7168876
   Peixoto E, 2014, IEEE T CIRC SYST VID, V24, P99, DOI 10.1109/TCSVT.2013.2273651
   Peixoto E, 2012, IEEE IMAGE PROC, P737, DOI 10.1109/ICIP.2012.6466965
   Segall A., 2012, JCTVCK1008 ITUTISOIE
   Shanableh T, 2013, IEEE T CIRC SYST VID, V23, P1191, DOI 10.1109/TCSVT.2013.2241352
   Shen HF, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P2017, DOI 10.1109/ICME.2006.262609
   Shen LQ, 2013, IEEE T MULTIMEDIA, V15, P465, DOI 10.1109/TMM.2012.2231060
   Shen T, 2013, IEEE DATA COMPR CONF, P241, DOI 10.1109/DCC.2013.32
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Tan YP, 2004, IEEE T CONSUM ELECTR, V50, P887, DOI 10.1109/TCE.2004.1341696
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Wang R.-J., IEEE T CIRCUITS SYST, V24, P1957
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zhang P, 2004, IEEE IMAGE PROC, P2781
   Zhang YF, 2013, IEEE DATA COMPR CONF, P53, DOI 10.1109/DCC.2013.13
NR 35
TC 2
Z9 2
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2018
VL 20
IS 9
BP 2262
EP 2275
DI 10.1109/TMM.2018.2804765
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GR1ZX
UT WOS:000442358200003
DA 2024-07-18
ER

PT J
AU Liu, JY
   Yang, WH
   Sun, XY
   Zeng, WJ
AF Liu, Jiaying
   Yang, Wenhan
   Sun, Xiaoyan
   Zeng, Wenjun
TI Photo Stylistic Brush: Robust Style Transfer via Superpixel-Based
   Bipartite Graph
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image stylization; superpixel; bipartite graph; stylistic brush
ID COLOR TRANSFER; NATURAL IMAGES; SEGMENTATION; TEXTURE
AB With the rapid development of social network and multimedia technology, customized image and video stylization have been widely used for various social-media applications. In this paper, we explore the problem of exemplar-based photo style transfer, which provides a flexible and convenient way to invoke fantastic visual impression. Rather than investigating some fixed artistic patterns to represent certain styles as was done in some previous works, our work emphasizes styles related to a series of visual effects in the photograph (e.g., color, tone, and contrast). We propose a photo stylistic brush, an automatic robust style transfer approach based on Superpixel-based BI partite Graph (SuperBIG). A two-step bipartite graph algorithm with different granularity levels is employed to aggregate pixels into superpixels and find their correspondences. In the first step, with the extracted hierarchical features, a bipartite graph is constructed to describe the content similarity for pixel partition to produce superpixels. In the second step, superpixels in the input/reference image are rematched to form a new superpixel-based bipartite graph, and superpixel-level correspondences are generated by bipartite matching. Finally, the refined correspondence guides SuperBIG to perform the transformation in a decorrelated color space. Extensive experimental results demonstrate the effectiveness and robustness of the proposed method for transferring various styles of exemplar images, even for some challenging cases, such as night images.
C1 [Liu, Jiaying; Yang, Wenhan] Peking Univ, Inst Comp Sci & Technol, Beijing 100080, Peoples R China.
   [Sun, Xiaoyan; Zeng, Wenjun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Peking University; Microsoft; Microsoft Research Asia
RP Sun, XY (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM liujiaying@pku.edu.cn; yangwenhan@pku.edu.cn; xysun@microsoft.com;
   wezeng@microsoft.com
RI Liu, JY/GYJ-0138-2022
OI Liu, Jiaying/0000-0002-0468-9576
FU National Natural Science Foundation of China [61772043]; Microsoft
   Research Asia [FY17-RES-THEME-013]; CCF-Tencent Open Research Fund
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61772043, in part by Microsoft Research
   Asia Project ID FY17-RES-THEME-013, and in part by CCF-Tencent Open
   Research Fund. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Sen-Ching Samson
   Cheung.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   [Anonymous], ACM T GRAPHICS
   [Anonymous], 2012, MATRIX COMPUTATIONS
   [Anonymous], 1950, The Print: Contact Printing and Enlarging
   [Anonymous], ADOBE PHOTOSHOP REST
   [Anonymous], 2015, ARXIV150904232
   [Anonymous], 2010, P NPAR
   [Anonymous], 2008, CVPR
   [Anonymous], NEGATIVE EXPOSURE DE
   [Anonymous], ACM T GRAPHICS
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Bychkovsky V, 2011, PROC CVPR IEEE, P97
   Champandard AJ, ARXIV160301768
   Cheng WG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P943, DOI 10.1145/2733373.2806370
   Chia AYS, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024190
   Curtis C. J., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P421, DOI 10.1145/258734.258896
   Fern XZ, 2004, P 21 INT C MACH LEAR, P36, DOI [10.1145/1015330.1015414, DOI 10.1145/1015330.1015414]
   Gatys L. A., 2015, J VISUAL-JAPAN
   Gooch B., 2002, 2 INT S NONPHOTOREAL, P83
   Gupta R. K., 2012, P 20 ACM INT C MULT, P369, DOI DOI 10.1145/2393347.2393402
   He KM, 2010, LECT NOTES COMPUT SC, V6311, P1
   Hertzmann A, 2003, IEEE COMPUT GRAPH, V23, P70, DOI 10.1109/MCG.2003.1210867
   Hertzmann A, 2001, COMP GRAPH, P327, DOI 10.1145/383259.383295
   Hertzmann A, 2001, COMPUTER GRAPHICS INTERNATIONAL 2001, PROCEEDINGS, P47, DOI 10.1109/CGI.2001.934657
   Hongyuan Zha, 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P25
   Huang TW, 2009, IEEE I CONF COMP VIS, P199, DOI 10.1109/ICCV.2009.5459165
   Hwang Y, 2014, PROC CVPR IEEE, P3342, DOI 10.1109/CVPR.2014.427
   Ironi R., 2005, RENDERING TECHNIQUES, P201
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kuhn HW, 2005, NAV RES LOG, V52, P7, DOI 10.1002/nav.20053
   Kyprianidis JE, 2013, IEEE T VIS COMPUT GR, V19, P866, DOI 10.1109/TVCG.2012.160
   Kyprianidis JE, 2009, COMPUT GRAPH FORUM, V28, P1955, DOI 10.1111/j.1467-8659.2009.01574.x
   Levin A, 2004, ACM T GRAPHIC, V23, P689, DOI 10.1145/1015706.1015780
   Li W, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360700
   Li YZ, 2005, ACM T GRAPHIC, V24, P836, DOI 10.1145/1073204.1073271
   Li ZG, 2012, PROC CVPR IEEE, P789, DOI [10.1109/CVPR.2012.6247750, 10.1109/ISRA.2012.6219309]
   Lin L., 2010, Proc. NPAR '10, P73, DOI DOI 10.1145/1809939.1809948
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Margulis D., 2005, Photoshop LAB Color: The Canyon Conundrum and Other Adventures in the Most Powerful Colorspace
   Mobahi H, 2011, INT J COMPUT VISION, V95, P86, DOI 10.1007/s11263-011-0444-0
   Orzan A, 2007, NPAR 2007: 5TH INTERNATIONAL SYMPOSIUM ON NON-PHOTOREALISTIC ANIMATION AND RENDERING, PROCEEDINGS, P103
   Pitie A., 2007, 4 EUR C VIS MED PROD, DOI DOI 10.1049/CP:20070055
   Pitié F, 2005, IEEE I CONF COMP VIS, P1434
   Pitié F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011
   Pouli T., 2010, Proceedings ofNPAR, P81
   Pouli T, 2011, COMPUT GRAPH-UK, V35, P67, DOI 10.1016/j.cag.2010.11.003
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Revaud J., 2015, Deep Convolutional Matching
   Selim A, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925968
   Shih YC, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601137
   Shih YC, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508419
   Tai YW, 2007, IEEE T PATTERN ANAL, V29, P1520, DOI 10.1109/TPAMI.2007.1168
   Wang BY, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866172
   Wang XG, 2009, IEEE T PATTERN ANAL, V31, P1955, DOI 10.1109/TPAMI.2008.222
   Welsh T, 2002, ACM T GRAPHIC, V21, P277, DOI 10.1145/566570.566576
   Winkenbach G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P91, DOI 10.1145/192161.192184
   Xu XM, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409070
   Yang AY, 2008, COMPUT VIS IMAGE UND, V110, P212, DOI 10.1016/j.cviu.2007.07.005
   Yang S, 2016, AER ADV ENG RES, V45, P1
   Yi-Chin Huang, 2005, 13th Annual ACM International Conference on Multimedia, P351, DOI 10.1145/1101149.1101223
   Yuan JY, 2015, IEEE T IMAGE PROCESS, V24, P3488, DOI 10.1109/TIP.2015.2446948
   Zhang W, 2013, IEEE T MULTIMEDIA, V15, P1594, DOI 10.1109/TMM.2013.2265675
NR 62
TC 26
Z9 26
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1724
EP 1737
DI 10.1109/TMM.2017.2780761
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, R
   Maugey, T
   Frossard, P
AF Ma, Rui
   Maugey, Thomas
   Frossard, Pascal
TI Optimized Data Representation for Interactive Multiview Navigation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multiview navigation; interactivity; navigation segment; multiview image
   compression
ID VIDEO CODING HEVC; COMPRESSION; 3DTV
AB In contrary to traditional media streaming services where a unique media content is delivered to different users, interactive multiview navigation applications enable users to choose their own viewpoints and freely navigate in a three-dimensional scene. The interactivity brings new challenges in addition to the classical rate-distortion tradeoff, which considers only the compression performance and viewing quality. On one hand, interactivity necessitates sufficient viewpoints for richer navigation; on the other hand, it requires to provide low bandwidth and delay costs for smooth navigation during view transitions. In this paper, we formally describe the novel tradeoffs posed by the navigation interactivity and classical rate-distortion criterion. Based on an original formulation, we look for the optimal design of the data representation by introducing novel rate and distortion models and practical solving algorithms. Experiments show that the proposed data representation method outperforms the baseline solution by providing lower resource consumptions and higher visual quality in all navigation configurations, which certainly confirms the potential of the proposed data representation in practical interactive navigation systems.
C1 [Ma, Rui] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
   [Maugey, Thomas] Inria Rennes Bretagne Atlantique Res Ctr, F-35042 Rennes, France.
   [Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
C3 Hong Kong University of Science & Technology; Universite de Rennes;
   Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Ma, R (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Hong Kong, Peoples R China.
EM rmaaa@connect.ust.hk; thomas.maugey@inria.fr; pascal.frossard@epfl.ch
RI Frossard, Pascal/AAF-2268-2019
OI Frossard, Pascal/0000-0002-4010-714X
CR [Anonymous], 2009, P 17 INY PACK VID WO
   Bauermann I, 2008, IEEE T IMAGE PROCESS, V17, P724, DOI 10.1109/TIP.2008.920501
   Bauermann I, 2008, IEEE T IMAGE PROCESS, V17, P709, DOI 10.1109/TIP.2008.918962
   Bjontegaard G., 2001, Document VCEG-M33
   Chen Y., 2009, EURASIP J APPL SIG P, V2009, P8
   Cheung G., 2009, P APSIPA ANN SUMM C, P498
   Cheung N. M., 2009, P PICT COD S MAY, P1
   De Abreu A, 2015, J VIS COMMUN IMAGE R, V33, P255, DOI 10.1016/j.jvcir.2015.09.010
   Fehn C, 2004, PROC SPIE, V5291, P93, DOI 10.1117/12.524762
   FREDMAN ML, 1987, J ACM, V34, P596, DOI 10.1145/28869.28874
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kubota A, 2007, IEEE SIGNAL PROC MAG, V24, P10, DOI 10.1109/MSP.2007.905873
   Kurutepe E, 2007, IEEE T CIRC SYST VID, V17, P1558, DOI 10.1109/TCSVT.2007.903664
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Liu YW, 2010, J VIS COMMUN IMAGE R, V21, P523, DOI 10.1016/j.jvcir.2010.02.004
   Ma R., 2017, ARXIV E PRINTS
   Ma R, 2013, IEEE IMAGE PROC, P1714, DOI 10.1109/ICIP.2013.6738353
   Martull S., 2012, P ICPR WORKSHOP TRAK, V111, P117
   Maugey T, 2016, IEEE T IMAGE PROCESS, V25, P1808, DOI 10.1109/TIP.2016.2530303
   Maugey T, 2015, IEEE IMAGE PROC, P1702, DOI 10.1109/ICIP.2015.7351091
   Maugey T, 2015, IEEE T IMAGE PROCESS, V24, P1573, DOI 10.1109/TIP.2015.2400817
   Maugey T, 2013, IEEE T IMAGE PROCESS, V22, P3459, DOI 10.1109/TIP.2013.2270183
   Maugey T, 2012, IEEE IMAGE PROC, P2717, DOI 10.1109/ICIP.2012.6467460
   Maugey T, 2011, IEEE IMAGE PROC, P589, DOI 10.1109/ICIP.2011.6116618
   Müller K, 2013, IEEE T IMAGE PROCESS, V22, P3366, DOI 10.1109/TIP.2013.2264820
   Müller K, 2011, P IEEE, V99, P643, DOI 10.1109/JPROC.2010.2091090
   Na ST, 2008, IEEE INT SYMP CIRC S, P1400, DOI 10.1109/ISCAS.2008.4541689
   Peris M, 2012, INT C PATT RECOG, P1038
   Petrazzuoli G, 2011, IEEE IMAGE PROC, P597, DOI 10.1109/ICIP.2011.6116620
   Shum HY, 2003, IEEE T CIRC SYST VID, V13, P1020, DOI 10.1109/TCSVT.2003.817360
   Sullivan GJ, 2013, IEEE J-STSP, V7, P1001, DOI 10.1109/JSTSP.2013.2283657
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Takyar U, 2014, IEEE SIGNAL PROC LET, V21, P22, DOI 10.1109/LSP.2013.2288014
   Tanimoto M, 2006, SIGNAL PROCESS-IMAGE, V21, P454, DOI 10.1016/j.image.2006.03.009
   Tanimoto M, 2011, IEEE SIGNAL PROC MAG, V28, P67, DOI 10.1109/MSP.2010.939077
   Tech G., 2013, JCT3VE1001
   Tekalp AM, 2007, IEEE SIGNAL PROC MAG, V24, P77, DOI 10.1109/MSP.2007.905878
   Toni L, 2015, IEEE T MULTIMEDIA, V17, P1604, DOI 10.1109/TMM.2015.2450020
   Xiu X., 2011, P IEEE INT WORKSH HO, P1
   Xiu XY, 2012, IEEE T MULTIMEDIA, V14, P1109, DOI 10.1109/TMM.2012.2191267
NR 40
TC 12
Z9 12
U1 1
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1595
EP 1609
DI 10.1109/TMM.2017.2779039
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100001
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, LH
   Tang, S
   Zhang, YD
   Deng, LX
   Tian, Q
AF Li, Linghui
   Tang, Sheng
   Zhang, Yongdong
   Deng, Lixi
   Tian, Qi
TI GLA: Global-Local Attention for Image Description
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; recurrent neural network; image
   description; natural language processing
ID NETWORKS
AB In recent years, the task of automatically generating image description has attracted a lot of attention in the field of artificial intelligence. Benefitting from the development of convolutional neural networks (CNNs) and recurrent neural networks (RNNs), many approaches based on the CNN-RNN framework have been proposed to solve this task and achieved remarkable process. However, two problems remain to be tackled in which the most existing methods use only the image-level representation. One problem is object missing, in which some important objects may he missing when generating the image description and the other is misprediction, when one object may be recognized in a wrong category. In this paper, to address these two problems, we propose a new method called global-local attention (GLA) for generating image description. The proposed GLA model utilizes an attention mechanism to integrate object-level features with image-level feature. Through this manner, our model can selectively pay attention to objects and context information concurrently. Therefore, our proposed GLA method can generate more relevant image description sentences and achieve the state-of-the-art performance on the well-known Microsoft COCO caption dataset with several popular evaluation metrics-CIDEr, METEOR, ROUGE-L and BLEU-1, 2,3, 4.
C1 [Li, Linghui; Tang, Sheng; Zhang, Yongdong; Deng, Lixi] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
   [Li, Linghui; Tang, Sheng; Zhang, Yongdong; Deng, Lixi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University of Texas System; University of Texas at San Antonio
   (UTSA)
RP Tang, S (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.
EM lilinghui@ict.ac.cn; ts@ict.ac.cn; zhyd@ict.ac.cn; denglixi@ict.ac.cn;
   qitian@cs.utsa.edu
RI Tang, Sheng/L-5792-2013
OI Tang, Sheng/0000-0003-3573-2407
FU National Key Research and Development Program of China [2017YFB1002202];
   Beijing Natural Science Foundation [4152050]; Beijing Advanced
   Innovation Center for Imaging Technology [BAICIT-2016009]; ARO
   [W911NF-15-1-0290]; National Natural Science Foundation of China
   [61525206, 61572472, 61429201]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61525206, Grant 61572472, and Grant
   61429201; in part by the National Key Research and Development Program
   of China (2017YFB1002202); in part by the Beijing Natural Science
   Foundation under Grant 4152050; and in part by the Beijing Advanced
   Innovation Center for Imaging Technology under Grant BAICIT-2016009. The
   work of Q. Tian was supported in part by the ARO Grant W911NF-15-1-0290
   and in part by the Faculty Research Gift Awards by NEC Laboratories of
   America and Blippar.
CR [Anonymous], 2015, International_Journal_of_Computer Vision
   [Anonymous], 2015, ARXIV150904942
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, FINITE ELEMENT METHO
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2015, P N AM CHAPT ASS COM
   [Anonymous], PROC CVPR IEEE
   [Anonymous], IEEE T PATTERN ANAL
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Banerjee S, 2005, ACL WORKSHOP INTRINS, P65
   Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223
   Bin Y, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P436, DOI 10.1145/2964284.2967258
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Graves A., 2013, Generating sequences with recurrent neural networks
   He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI [arXiv:1406.4729, 10.1007/978-3-319-10578-9_23]
   Hodosh M, 2013, J ARTIF INTELL RES, V47, P853, DOI 10.1613/jair.3994
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162
   Kuznetsova Polina, 2012, Association for Computational Linguistics
   Li LH, 2017, AAAI CONF ARTIF INTE, P4133
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Mao Junhua, 2015, P INT C LEARN REPR
   Mason R, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P592
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Mitchell M, 2012, P 13 C EUR CHAPT ASS, P747
   Ouyang WL, 2015, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2015.7298854
   Pan PB, 2016, PROC CVPR IEEE, P1029, DOI 10.1109/CVPR.2016.117
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Radenovic F, 2016, LECT NOTES COMPUT SC, V9905, P3, DOI 10.1007/978-3-319-46448-0_1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rohrbach M, 2013, IEEE I CONF COMP VIS, P433, DOI 10.1109/ICCV.2013.61
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I, 2014, ADV NEUR IN, V27
   Sutskever Ilya, 2011, P 28 INT C MACH LEAR, P1017
   Szegedy C., 2016, uS Patent, Patent No. [9,275,308, 9275308]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang S, 2017, IEEE T MULTIMEDIA, V19, P2105, DOI 10.1109/TMM.2017.2729786
   Tang S, 2012, IEEE T MULTIMEDIA, V14, P43, DOI 10.1109/TMM.2011.2168198
   Thomason J., 2014, COLING, P1218
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Xu K., 2015, COMPUTER SCI, P2048
   Yang Y., 2011, P C EMP METH NAT LAN, P444
   Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Young P., 2014, T ASSOC COMPUT LING, V2, P67, DOI [DOI 10.1162/TACL_A_00166, 10.1162/tacl_a_00166]
   Zaremba W., 2014, ARXIV
NR 59
TC 85
Z9 94
U1 1
U2 39
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2018
VL 20
IS 3
BP 726
EP 737
DI 10.1109/TMM.2017.2751140
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FW5ZH
UT WOS:000425397500017
DA 2024-07-18
ER

PT J
AU Ge, C
   Wang, N
   Foster, G
   Wilson, M
AF Ge, Chang
   Wang, Ning
   Foster, Gerry
   Wilson, Mick
TI Toward QoE-Assured 4K Video-on-Demand Delivery Through Mobile Edge
   Virtualization With Adaptive Prefetching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile edge computing; MPEG-DASH; network function virtualization;
   prefetching; quality of experience (QoE); video on demand (VoD)
ID EFFICIENCY
AB Internet video streaming applications have been demanding more bandwidth and higher video quality, especially with the advent of virtual reality and augmented reality applications. While adaptive strea ming protocols like MPEG-DASH (dynamic adaptive streaming over HTTP) allows video quality to be flexibly adapted, e.g., degraded when mobile network condition deteriorates, this is not an option if the application itself requires guaranteed 4K quality at all time. On the other hand, conventional end-to-end transmission control protocol (TCP) has been struggling in supporting 4K video delivery across long-distance Internet paths containing both fixed and mobile network segments with heterogeneous characteristics. In this paper, we present a novel and practically feasible system architecture named MVP(mobile edge virtualization with adaptive prefetching), which enables content providers to embed their content intelligence as a virtual network function into the mobile network operator's infrastructure edge. Based on this architecture, we present a context-aware adaptive video prefetching scheme in order to achieve quality of experience (QoE)-assured 4K video on demand (VoD) delivery across the global Internet. Through experiments based on a real LTE-A network infrastructure, we demonstrate that our proposed scheme is able to achieve QoE-assured 4K VoD streaming, especially when the video source is located remotely in the public Internet, in which case none of the state-of-the-art solutions is able to support such an objective at global Internet scale.
C1 [Ge, Chang; Wang, Ning; Foster, Gerry] Univ Surrey, Inst Commun Syst, 5GIC, Guildford GU2 7XH, Surrey, England.
   [Wilson, Mick] Fujitsu Labs Europe, Hayes UB4 8FE, England.
C3 University of Surrey; Fujitsu Ltd; Fujitsu Laboratories Ltd
RP Ge, C (corresponding author), Univ Surrey, Inst Commun Syst, 5GIC, Guildford GU2 7XH, Surrey, England.
EM C.Ge@surrey.ac.uk; N.Wang@surrey.ac.uk; G.Foster@surrey.ac.uk;
   mick.wilson@uk.fujitsu.com
FU EPSRC CONCERT [EP/L018683/1]; KCN [EP/L026120/1]; EPSRC [EP/L026120/1,
   EP/L018683/1] Funding Source: UKRI
FX This work was supported by the EPSRC CONCERT (EP/L018683/1) and KCN
   (EP/L026120/1) projects. The guest editor coordinating the review of
   this manuscript and approving it for publication was Dr. Mahbub Hassan.
   (Corresponding author: Chang Ge.)
CR [Anonymous], P 7 INT C MULT SYST
   [Anonymous], CACH DET CLOUD CDN D
   [Anonymous], SPRINT NETWORK PERFO
   [Anonymous], 2016, WATCHING VIDEO 4K UL
   [Anonymous], 2011, Proc. second annu. acm conf. multimed. syst.-mmsys'11, DOI DOI 10.1145/1943552.1943574
   [Anonymous], HUAWEI HUAWEI LAMPSI
   [Anonymous], MOBILE EDGE COMPUTIN
   [Anonymous], 2015, OPTIMAL ADAPTIVE STR
   [Anonymous], 38913 3GPP
   [Anonymous], CISC VNI MOB FOR 201
   [Anonymous], 2015, MOBILE EDGE COMPUTIN
   [Anonymous], P 7 INT C MULT SYST
   Bouten N, 2014, IEEE T MULTIMEDIA, V16, P2281, DOI 10.1109/TMM.2014.2362856
   Bronzino Francesco, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P1092, DOI 10.1109/CCNC.2016.7444942
   Chang C.-Y., 2016, Proc. 2016 ACM the Workshop on Mobility in the Evolving Internet Architecture, P13
   Chen SQ, 2006, IEEE T MULTIMEDIA, V8, P243, DOI 10.1109/TMM.2005.864281
   Claeys M, 2015, INT CONF NETW SER, P310, DOI 10.1109/CNSM.2015.7367376
   Dong K, 2015, 2015 INTERNATIONAL CONFERENCE ON COMPUTING, NETWORKING AND COMMUNICATIONS (ICNC), P737, DOI 10.1109/ICCNC.2015.7069438
   El Essaili A, 2015, IEEE T CIRC SYST VID, V25, P988, DOI 10.1109/TCSVT.2014.2367355
   Ge C, 2016, PROCEEDINGS OF THE 2016 3RD ACM CONFERENCE ON INFORMATION-CENTRIC NETWORKING (ACM-ICN '16), P237, DOI 10.1145/2984356.2988522
   Huysegems R., 2012, PROC IEEE INT WORKSH, P1
   ISO/IEC, 2014, Rep. 23009-1. ISO/IEC JTCI/SC29/WG11
   Jiang JC, 2014, IEEE ACM T NETWORK, V22, P326, DOI 10.1109/TNET.2013.2291681
   Krishnamoorthi V, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P551, DOI 10.1145/2733373.2806270
   Krishnamoorthi V, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P317, DOI 10.1145/2647868.2654951
   Krishnamoorthi V, 2013, I S MOD ANAL SIM COM, P182, DOI 10.1109/MASCOTS.2013.26
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Master N, 2016, IEEE T WIREL COMMUN, V15, P3296, DOI 10.1109/TWC.2016.2519882
   Mok R.K., 2012, P 3 MULTIMEDIA SYSTE, P11
   Fajardo JO, 2015, IEEE NETWORK, V29, P40, DOI 10.1109/MNET.2015.7340423
   Thang TC, 2012, IEEE T CONSUM ELECTR, V58, P78, DOI 10.1109/TCE.2012.6170058
   Wang XF, 2014, IEEE COMMUN MAG, V52, P131, DOI 10.1109/MCOM.2014.6736753
   Wang X, 2013, IEEE WIREL COMMUN, V20, P72, DOI 10.1109/MWC.2013.6549285
   Wilk Stefan, 2016, 2016 13th IEEE Annual Consumer Communications & Networking Conference (CCNC), P429, DOI 10.1109/CCNC.2016.7444818
   Zhou C, 2016, IEEE T MULTIMEDIA, V18, P738, DOI 10.1109/TMM.2016.2522650
NR 35
TC 69
Z9 71
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2222
EP 2237
DI 10.1109/TMM.2017.2735301
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600008
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Hosseini, M
   Jiang, Y
   Berlin, RR
   Sha, L
   Song, HB
AF Hosseini, Mohammad
   Jiang, Yu
   Berlin, Richard R.
   Sha, Lui
   Song, Houbing
TI Toward Physiology-Aware DASH: Bandwidth-Compliant Prioritized Clinical
   Multimedia Communication in Ambulances
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile communication; multimedia communication; medical services; mobile
   applications; physiology
ID MANAGEMENT
AB The ultimate objective of medical cyber-physical systems is to enhance the safety and effectiveness of patient care. To ensure safe and effective care during emergency patient transfer from rural areas to center tertiary hospitals, reliable and real-time communication is essential. Unfortunately, real-time monitoring of patients involves transmission of various clinical multimedia data including videos, medical images, and vital signs, which requires use of mobile network with high-fidelity communication bandwidth. However, thewireless networks along the roads in rural areas range from 4G to 2G to lowspeed satellite links, which poses a significant challenge to transmit critical patient information. In this paper, we present a bandwidth-compliant criticality-aware system for transmission of massive clinical multimedia data adaptive to varying bandwidths during patient transport. Model-based clinical automata are used to determine the criticality of clinical multimedia data. We borrow concepts from DASH, and propose physiology-aware adaptation techniques to transmit more critical clinical data with higher fidelity in response to changes in disease, clinical states, and bandwidth condition. In collaboration with Carle's ambulance service center, we develop a bandwidth profiler, and use it as proof of concept to support our experiments. Our preliminary evaluation results show that our solutions ensure that most critical patient's clinical data are communicated with higher fidelity.
C1 [Hosseini, Mohammad; Berlin, Richard R.; Sha, Lui] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.
   [Jiang, Yu] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Berlin, Richard R.] Carle Fdn Hosp, Urbana, IL 61801 USA.
   [Song, Houbing] Embry Riddle Aeronaut Univ, Dept Elect Comp Software & Syst Engn, Daytona Beach, FL 32114 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   Tsinghua University; Embry-Riddle Aeronautical University
RP Hosseini, M (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.
EM shossen2@illinois.edu; jy1989@mail.tsinghua.edu.cn;
   richard.berlin@carle.com; lrs@illinois.edu; h.song@ieee.org
RI song, hu/JVO-3838-2024; Song, Houbing Herbert/E-3628-2010
OI Song, Houbing Herbert/0000-0003-2631-9223; Hosseini,
   Mohammad/0000-0003-1397-0375
FU NSF CNS [1329886, 1545002]; ONR [N00014-14-1-0717]; Division Of Computer
   and Network Systems; Direct For Computer & Info Scie & Enginr [1545002,
   1329886] Funding Source: National Science Foundation
FX This work was supported in part by NSF CNS under Grant 1329886 and Grant
   1545002, and in part by ONR under Grant N00014-14-1-0717. The guest
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Mahbub Hassan. (Corresponding author: Mohammad
   Hosseini.)
CR Alinejad A, 2011, IEEE ENG MED BIO, P1532, DOI 10.1109/IEMBS.2011.6090447
   [Anonymous], P WORKSH MOB VID DEL
   [Anonymous], P 24 INT C SOFTW TEL
   [Anonymous], P IEEE INT C COMP TO
   [Anonymous], P 14 INT WORKSH AD R
   [Anonymous], CLIN MANAGEMENT ALGO
   Bokani A, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Chen C, 2015, IEEE J-STSP, V9, DOI 10.1109/JSTSP.2014.2337277
   Cicalò S, 2016, IEEE T MULTIMEDIA, V18, P1988, DOI 10.1109/TMM.2016.2597001
   Cohen R, 2014, IEEE INFOCOM SER, P2427, DOI 10.1109/INFOCOM.2014.6848188
   De Vleeschauwer D, 2013, IEEE INFOCOM SER, P989
   DeVincenzi Anthony., 2011, P ACM 2011 C COMPUTE, P621
   Garcia-Morchon Oscar, 2010, 2010 8th IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops), P322, DOI 10.1109/PERCOMW.2010.5470649
   Görlitz RA, 2012, HEALTH POLICY TECHN, V1, P145, DOI 10.1016/j.hlpt.2012.07.008
   Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Gupta SKS, 2006, PERCOM 2006: FOURTH ANNUAL IEEE INTERNATIONAL CONFERENCE ON PERVASIVE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P251
   Hacke W, 2008, CEREBROVASC DIS, V25, P457, DOI 10.1159/000131083
   He ZF, 2016, IEEE T MULTIMEDIA, V18, P1401, DOI 10.1109/TMM.2016.2564104
   He ZF, 2014, IEEE GLOB COMM CONF, P1388, DOI 10.1109/GLOCOM.2014.7037002
   Hofmann BerndM., 2012, Perspectives in Medicine, V1, P73
   Hosseini M., 2017, PROC 5 IEEE INT C HE, P1
   Hosseini M, 2017, PROCEEDINGS OF THE 8TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'17), P158, DOI 10.1145/3083187.3083211
   Hosseini M, 2017, MULTIMEDIA SYST, V23, P421, DOI 10.1007/s00530-016-0511-z
   Hosseini M, 2016, IEEE INT SYM MULTIM, P107, DOI [10.1109/ISM.2016.45, 10.1109/ISM.2016.0028]
   Hosseini M, 2016, J MED SYST, V40, DOI 10.1007/s10916-016-0583-5
   Hosseini Mohammad., 2013, Proceedings of the 4th ACM Multimedia Systems Conference, MMSys '13, P1
   Hosseini Mohammad., 2015, Proceedings of the 7th ACM International Workshop on Mobile Video (MoVid'15), in conjunction with ACM Multimedia Systems (MMSys'15), P7
   Hosseini Mohammad., 2015, Proceedings of the 7th ACM International Workshop on Massively Multiuser Virtual Environments, MMVE'15, P13
   Hosseini Mohammad., 2012, Proceedings of the 20th ACM international conference on Multimedia, P1017
   Kim J, 2010, IEEE INT CON MULTI, P7, DOI 10.1109/ICME.2010.5583856
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Malindi Phumzile, 2008, 2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications, P499, DOI 10.1109/BROADCOM.2008.62
   Panzarasa S, 2006, NEUROL SCI, V27, pS245, DOI 10.1007/s10072-006-0628-5
   Peng BY, 2015, PROCEEDINGS OF THE 16TH ANNUAL MIDDLEWARE CONFERENCE, P149, DOI 10.1145/2814576.2814808
   Qiao L, 2011, IEEE T VEH TECHNOL, V60, P632, DOI 10.1109/TVT.2010.2095472
   Rehman IU, 2014, 2014 EAI 4TH INTERNATIONAL CONFERENCE ON WIRELESS MOBILE COMMUNICATION AND HEALTHCARE (MOBIHEALTH), P211, DOI [10.1109/MOBIHEALTH.2014.7015948, 10.4108/icst.mobihealth.2014.257415]
   *ROYAL COLL PHYS I, 2008, NAT CLIN GUID STROK
   Seferoglu H., 2010, P IEEE INT S NETW CO, P1
   Skorin-Kapov L, 2010, INT J TELEMED APPL, V2010, DOI 10.1155/2010/628086
   Song Y, 2008, PROCEEDINGS OF 2008 INTERNATIONAL SYMPOSIUM ON APPLIED COMPUTING AND COMPUTATIONAL SCIENCES, P1
   Stockhammer T., 2011, P 2 ANN ACM C MULTIM, P133
   Stoica I, 2001, ACM SIGCOMM COMP COM, V31, P149, DOI 10.1145/964723.383071
   Thakolsri S., 2010, P 18 ACM INT C MULT, P783
   Wu PL, 2015, PROCEEDINGS 41ST EUROMICRO CONFERENCE ON SOFTWARE ENGINEERING AND ADVANCED APPLICATIONS SEAA 2015, P464, DOI 10.1109/SEAA.2015.27
   Xie XF, 2007, 2007 IEEE SWARM INTELLIGENCE SYMPOSIUM, P190, DOI 10.1109/SIS.2007.368045
   Yadav RK, 2014, 2014 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS AND INFORMATICS (ICACCI), P80, DOI 10.1109/ICACCI.2014.6968544
NR 46
TC 6
Z9 6
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2017
VL 19
IS 10
BP 2307
EP 2321
DI 10.1109/TMM.2017.2733298
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5YG
UT WOS:000411247600014
OA hybrid
DA 2024-07-18
ER

PT J
AU Liu, YF
   Guo, JM
   An, LL
AF Liu, Yun-Fu
   Guo, Jing-Ming
   An, Lingling
TI Multimedia Classification Using Bipolar Relation Graphs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Classification; loss function; multimedia retrieval; optimization
ID RECOGNITION
AB Recent studies on category relations have shown the promising progress in addressing classification problems. Existing works independently consider the known relation and classifier optimization, and thus restrain the room for performance improvement. In this work, a new loss function is proposed to leverage the underlining relations among categories and classifiers. In addition, the bipolar relation (BR) graph is employed to formulate a general form for diverse relations. This bipolar graph is automatically learnt for reliving the constraints which may happen during the cost minimization. Extensive experiments on three benchmarks with various hypotheses and graphs demonstrate that our method can offer a significant performance improvement by jointly learning from both BR graph and hypothesis, in particular on a small training dataset scenario that suffers from severe overfitting problem.
C1 [Liu, Yun-Fu; Guo, Jing-Ming] Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 106, Taiwan.
   [An, Lingling] Xidian Univ, Sch Comp Sci & Technol, Xian 710126, Shaanxi, Peoples R China.
C3 National Taiwan University of Science & Technology; Xidian University
RP Guo, JM (corresponding author), Natl Taiwan Univ Sci & Technol, Dept Elect Engn, Taipei 106, Taiwan.
EM yunfuliu@gmail.com; jmguo@seed.net.tw; an.lingling@gmail.com
RI Liu, Yun-Fu/K-2506-2012
FU National Science Council, Taiwan [NSC 101-2221-E-011-136-MY3]; National
   Key Research and Development Program of China [2016YFB0800601]
FX This work was supported in part by the National Science Council, Taiwan,
   under Contract NSC 101-2221-E-011-136-MY3, and in part by the National
   Key Research and Development Program of China under Grant
   2016YFB0800601. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Yap-Peng Tan.
   (Corresponding Author: Jing-Ming Guo.)
CR [Anonymous], L BFGS MATLAB FUNCTI
   [Anonymous], 2011, P 17 ACM SIGKDD INT, DOI [DOI 10.1145/2020408, 10.1145/2020408.2020430]
   [Anonymous], THUMOS CHALLENGE
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2011, P 28 INT C INT C MAC
   [Anonymous], P 14 ECCV THUMOS CHA
   [Anonymous], THUMOS CHALLENGE
   Backstrom L., 2011, P 4 ACM INT C WEB SE, P635, DOI DOI 10.1145/1935826.1935914
   Bart E, 2008, PROC CVPR IEEE, P2166
   Bi W, 2014, IEEE T NEUR NET LEAR, V25, P2275, DOI 10.1109/TNNLS.2014.2309437
   Campadelli P, 2006, IEEE T MED IMAGING, V25, P1588, DOI 10.1109/TMI.2006.884198
   Chen LF, 2000, PATTERN RECOGN, V33, P1713, DOI 10.1016/S0031-3203(99)00139-9
   Chen XY, 2011, IEEE I CONF COMP VIS, P834, DOI 10.1109/ICCV.2011.6126323
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng J, 2014, LECT NOTES COMPUT SC, V8689, P48, DOI 10.1007/978-3-319-10590-1_4
   Fergus R, 2010, LECT NOTES COMPUT SC, V6311, P762, DOI 10.1007/978-3-642-15549-9_55
   FRIEDMAN JH, 1989, J AM STAT ASSOC, V84, P165, DOI 10.2307/2289860
   Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Liu YF, 2014, IEEE T INF FOREN SEC, V9, P1953, DOI 10.1109/TIFS.2014.2355495
   Liu YF, 2011, IEEE T IMAGE PROCESS, V20, P1077, DOI 10.1109/TIP.2010.2087765
   Moguerza JM, 2006, STAT SCI, V21, P322, DOI 10.1214/088342306000000493
   Pan J-Y, 2004, P 10 ACM SIGKDD INT, P653, DOI DOI 10.1145/1014052.1014135
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Tousch AM, 2012, PATTERN RECOGN, V45, P333, DOI 10.1016/j.patcog.2011.05.017
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang S, 2014, IEEE T MULTIMEDIA, V16, P289, DOI 10.1109/TMM.2013.2293060
   Xu Z., 2015, THUMOS Challenge
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Ye GG, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P471, DOI 10.1145/2733373.2806221
   Zhang L, 2016, IEEE T MULTIMEDIA, V18, P247, DOI 10.1109/TMM.2015.2510509
NR 36
TC 2
Z9 2
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2017
VL 19
IS 8
BP 1860
EP 1869
DI 10.1109/TMM.2017.2689922
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FH5VV
UT WOS:000411240400014
DA 2024-07-18
ER

PT J
AU Cai, ZC
   Lan, T
   Zheng, CM
AF Cai, Zhanchuan
   Lan, Ting
   Zheng, Caimu
TI Hierarchical MK Splines: Algorithm and Applications to Data Fitting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scattered data; data fitting; B-spline; hierarchical MK (HMK) splines;
   interpolation
ID SCATTERED DATA INTERPOLATION; MANY-CORE PROCESSORS; PARALLEL FRAMEWORK;
   MINIMUM CURVATURE; HEVC; RECONSTRUCTION; SURFACES; DESIGN
AB In the era of big data, it is very important to study large-scale data fitting methods. In order to ensure the calculation speed and accuracy, we propose a new kind of hierarchical many-knot splines (hereinafter called "hierarchical MK splines," generally abbreviated as HMK splines) in this paper. The HMK splines method produces a sequence of MK spline functions. These MK spline functions are constructed into one ideal interpolation function by the MK spline refinement. In the case of regular sampling data, HMK splines can achieve the purpose of accurate approximation for the given data points without solving systems of equations. Further, in order to deal with the issues of scattered data fitting, the use of least-squares method will lead to the necessary of solving a linear system of equations. Since the ill-conditioned systems of equations often lead to unacceptable deviation of calculation results, one tries to avoid it as much as possible. The HMK splines algorithm can meet this requirement; it can avoid the intolerable deviation caused by solving systems of equations. Experimental results show that large-scale scattered data fitting can be easily achieved by the HMK splines algorithm and the reconstruction of nonuniform samples has a high accuracy.
C1 [Cai, Zhanchuan; Lan, Ting; Zheng, Caimu] Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macau, Peoples R China.
C3 Macau University of Science & Technology
RP Cai, ZC (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Taipa 999078, Macau, Peoples R China.
EM zc-cai@must.edu.mo; lantingleo@gmail.com; caimuzheng@126.com
RI Lan, Ting/GWV-2392-2022; Lan, Ting/AGP-1142-2022
FU National Basic Research Program of China (973 Program) [2011CB302400];
   Science and Technology Development Fund of Macau [048/2016/A2,
   110/2014/A3, 084/2012/A3]; National Natural Science Foundation of China
   [61170320, 61272304]; State Key Lab of CAD&CG of Zhejiang University
   [A1513]
FX This work was supported in part by the National Basic Research Program
   of China (973 Program) under Grant 2011CB302400, in part by the Science
   and Technology Development Fund of Macau under Grant 048/2016/A2, Grant
   110/2014/A3, and Grant 084/2012/A3, in part by the National Natural
   Science Foundation of China under Grant 61170320 and Grant 61272304, and
   in part by the Open Project Program of the State Key Lab of CAD&CG of
   Zhejiang University under Grant A1513. The guest editor coordinating the
   review of this manuscript and approving it for publication was Dr. David
   Gotz. (Corresponding author: Zhanchuan Cai.)
CR Almeida N, 2016, IEEE T ULTRASON FERR, V63, P212, DOI 10.1109/TUFFC.2015.2507638
   [Anonymous], 1968, P 1968 ACM NAT C
   Bai Y, 2010, IEEE T FUZZY SYST, V18, P1016, DOI 10.1109/TFUZZ.2010.2064170
   BAJAJ CL, 1992, ACM T GRAPHIC, V11, P61, DOI 10.1145/102377.120081
   Billings SD, 2002, GEOPHYSICS, V67, P1810, DOI 10.1190/1.1527081
   BRIGGS IC, 1974, GEOPHYSICS, V39, P39, DOI 10.1190/1.1440410
   Chen S, 2013, IEEE T CIRCUITS-I, V60, P1584, DOI 10.1109/TCSI.2012.2226514
   Clough R, 1965, P C MATRIX METHODS S, P515
   Csébfalvi B, 2013, IEEE T VIS COMPUT GR, V19, P1455, DOI 10.1109/TVCG.2013.7
   FOGG DA, 1984, COMPUT VISION GRAPH, V28, P85, DOI 10.1016/0734-189X(84)90141-5
   FRANKE R, 1991, SYMB COMPUT, P131
   FRANKE R, 1982, MATH COMPUT, V38, P181, DOI 10.2307/2007474
   Franke R., 1979, NPS5379003 DEF TECH
   GOSHTASBY A, 1987, PATTERN RECOGN, V20, P525, DOI 10.1016/0031-3203(87)90079-3
   HARDY RL, 1971, J GEOPHYS RES, V76, P1905, DOI 10.1029/JB076i008p01905
   Huang JZ, 2015, IET COMPUT VIS, V9, P456, DOI 10.1049/iet-cvi.2014.0166
   Lee S, 1997, IEEE T VIS COMPUT GR, V3, P228, DOI 10.1109/2945.620490
   Li H., 1997, J IMAGE GRAPHICS, V2, P701
   Li Y., 1979, SPLINE FUNCTION METH, P194
   Nam J, 2005, IEEE T MULTIMEDIA, V7, P667, DOI 10.1109/TMM.2005.843362
   NIELSON GM, 1993, IEEE COMPUT GRAPH, V13, P60, DOI 10.1109/38.180119
   Ogawa T, 2011, IEEE T MULTIMEDIA, V13, P974, DOI 10.1109/TMM.2011.2161760
   Parra LC, 2000, IEEE T NUCL SCI, V47, P1543, DOI 10.1109/23.873014
   Qi D., 1982, COMPUT MATH APPL, V4, P244
   Qi D., 1981, 2238 U WISC MAD
   Qi DX, 1999, SCI CHINA SER E, V42, P383, DOI 10.1007/BF02916747
   Shea JD, 2012, IEEE T BIO-MED ENG, V59, P936, DOI 10.1109/TBME.2011.2176727
   Tang M., IEEE T MULTIME UNPUB
   Yan CG, 2014, IEEE T CIRC SYST VID, V24, P2077, DOI 10.1109/TCSVT.2014.2335852
   Yan CG, 2014, ELECTRON LETT, V50, P367, DOI 10.1049/el.2013.3235
   Yan CG, 2014, IEEE SIGNAL PROC LET, V21, P573, DOI 10.1109/LSP.2014.2310494
   Zhu SY, 2016, IEEE T MULTIMEDIA, V18, P1707, DOI 10.1109/TMM.2016.2593039
NR 32
TC 9
Z9 10
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2017
VL 19
IS 5
BP 921
EP 934
DI 10.1109/TMM.2016.2640759
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EY5XS
UT WOS:000404056000003
DA 2024-07-18
ER

PT J
AU Sun, LF
   Wang, XY
   Wang, Z
   Zhao, H
   Zhu, WW
AF Sun, Lifeng
   Wang, Xiaoyan
   Wang, Zhi
   Zhao, Hong
   Zhu, Wenwu
TI Social-Aware Video Recommendation for Online Social Groups
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Experiment; group recommendation; measurement; social media
AB Group recommendation plays a significant role in today's social media systems, where users form social groups to receive multimedia content together and interact with each other, instead of consuming the online content individually. Limitations of traditional group recommendation approaches are as follows. First, they usually infer group members' preferences by their historical behaviors, failing to capture inactive users' preferences from the sparse historical data. Second, relationships between group members are not studied by these approaches, which fail to capture the inherent personality of members in a group. To address these issues, we propose a social-aware group recommendation framework that jointly utilizes both social relationships and social behaviors to not only infer a group's preference, but also model the tolerance and altruism characteristics of group members. Based on the observation that the following relationship in the online social network reflects common interests of users, we propose a group preference model based on external experts of group members. Furthermore, we model users' tolerance (willingness to receive content not preferred) and altruism (willingness to receive content preferred by friends). Finally, based on the group preference model, we design recommendation algorithms for users under different social contexts. Experimental results demonstrate the effectiveness of our approach, which significantly improves the recommendation accuracy against traditional approaches, especially in the cases of inactive group members.
C1 [Sun, Lifeng; Wang, Xiaoyan; Zhu, Wenwu] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Sun, Lifeng; Wang, Xiaoyan; Zhao, Hong; Zhu, Wenwu] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
   [Sun, Lifeng; Wang, Xiaoyan; Zhu, Wenwu] Tsinghua Univ, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
   [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Zhao, Hong] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Zhao, Hong] Tsinghua Univ, State Key Lab Intelligent Technol & Syst, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua
   Shenzhen International Graduate School; Tsinghua University; Tsinghua
   University; Tsinghua University
RP Sun, LF (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.; Sun, LF (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.; Sun, LF (corresponding author), Tsinghua Univ, Beijing Key Lab Networked Multimedia, Beijing 100084, Peoples R China.
EM sunlf@tsinghua.edu.cn; muyushiok@gmail.com; wangzhi@sz.tsinghua.edu.cn;
   vzhao@tsinghua.edu.cn; wwzhu@tsinghua.edu.cn
CR Amer-Yahia S., 2009, PROC VLDB ENDOW, V2, P754, DOI DOI 10.14778/1687627.1687713
   [Anonymous], 2015, Preference-oriented social networks: Group recommendation and inference, DOI DOI 10.1145/2792838.2800190
   [Anonymous], 2009, P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC P 1 SIGMM WORKSH SOC
   Baluja S, 2008, WORLD WID WEB C, P895
   Basilico J., 2004, P 21 INT C MACH LEAR, P9, DOI DOI 10.1145/1015330.1015394
   Blei D.M., 2006, INT C MACHINE LEARNI, DOI [DOI 10.1145/1143844.1143859, 10.1145/1143844.1143859]
   Boratto L, 2010, STUD COMPUT INTELL, V324, P1
   Buss A.H., 2014, Social behavior and personality Psychology Revivals, DOI 10.4324/9781315738567
   Cesar Pablo, 2011, P NEM SUMM 2011, V2011, P94
   Chen KL, 2012, SIGIR 2012: PROCEEDINGS OF THE 35TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P661, DOI 10.1145/2348283.2348372
   Cui LZ, 2016, COMPUT OPER RES, V67, P155, DOI 10.1016/j.cor.2015.09.006
   Culotta A, 2015, AAAI CONF ARTIF INTE, P72
   Davidson James, 2010, P 4 ACM C REC SYST, P293, DOI [DOI 10.1145/1864708.1864770, 10.1145/1864708]
   Debnath S., 2008, Proceedings of the 17th international conference on World Wide Web, P1041
   Garcia I, 2011, LECT NOTES COMPUT SC, V6786, P547, DOI 10.1007/978-3-642-21934-4_45
   Gartrell M., 2010, Proceedings of the 16th ACM International Conference on Supporting Group Work, ACM, Sanibel Island, P97, DOI DOI 10.1145/1880071.1880087
   Guzzi Francesca, 2011, P 5 ACM C REC SYST, P265
   Hu H., 2014, proceeding of The IEEE International Conference on Multimedia and Expo (ICME), P1, DOI 10.1109/ICME.2014.6890134
   Isaacman S., 2011, Proc. Fifth ACM Conf. on Rec. Sys, P69
   Jameson A., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P596
   Krishnamurthy B., 2008, Proceedings of the rst workshop on Online social networks, P19, DOI [DOI 10.1145/1397735.1397741, 10.1145/1397735.1397741]
   Lee K, 2014, IEEE T MULTIMEDIA, V16, P1201, DOI 10.1109/TMM.2014.2311012
   Mao K, 2015, IEEE T MULTIMEDIA, V17, P396, DOI 10.1109/TMM.2015.2392562
   O'Connor M, 2001, ECSCW 2001: PROCEEDINGS OF THE SEVENTH EUROPEAN CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, P199
   Pan WK, 2016, INFORM SCIENCES, V332, P84, DOI 10.1016/j.ins.2015.10.044
   Schafer J. B., 2007, The Adaptive Web. Methods and Strategies of Web Personalization, P291
   Skowron P., 2014, CORR
   Ujjin S, 2003, PROCEEDINGS OF THE 2003 IEEE SWARM INTELLIGENCE SYMPOSIUM (SIS 03), P124, DOI 10.1109/SIS.2003.1202257
   Walter FE, 2008, AUTON AGENT MULTI-AG, V16, P57, DOI 10.1007/s10458-007-9021-x
   Wang XD, 2012, PROCEEDINGS OF THE ASME SUMMER BIOENGINEERING CONFERENCE, PTS A AND B, P37
   Wang Z, 2013, IEEE T MULTIMEDIA, V15, P698, DOI 10.1109/TMM.2012.2237022
   Wang Zhi., 2012, Proceedings of the 20th ACM international conference on Multimedia, MM '12, P29
   Yang Xiwang., 2012, P 18 ACM SIGKDD INT, P1267, DOI [10.1145/2339530.2339728, DOI 10.1145/2339530.2339728]
   Zhou Renjie, 2010, P 10 ACM SIGCOMM C I, P404, DOI DOI 10.1145/1879141.1879193
   Zhu WW, 2015, IEEE MULTIMEDIA, V22, P96, DOI 10.1109/MMUL.2015.66
   [No title captured]
NR 36
TC 63
Z9 66
U1 4
U2 48
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 609
EP 618
DI 10.1109/TMM.2016.2635589
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400015
DA 2024-07-18
ER

PT J
AU Wu, Y
   Shen, X
   Mei, T
   Tian, XM
   Yu, NH
   Rui, Y
AF Wu, Yue
   Shen, Xu
   Mei, Tao
   Tian, Xinmei
   Yu, Nenghai
   Rui, Yong
TI Monet: A System for Reliving Your Memories by Theme-Based Photo
   Storytelling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Personal photos; photo selection; storytelling; cinematic grammar
AB With the ever-increasing use of smartphones and digital cameras, people are now able to take photos anywhere and anytime. Most of these photos simply end up stored in the cloud without further interaction. This occurs because we lack intelligent services to organize these personal photos well. Therefore, there is an urgent need for such a system to enable people to relive their memories by turning their photos into stories. This paper presents a storytelling system named Monet, which automatically creates interesting stories from personal photos by mimicking cinematic knowledge based on a set of predesigned editing styles. The system consists of two stages: photo summarization, which selects a subset of the "best" photos to represent a photo collection, and story remixing, which generates a stylish music video from the selected photos. During photo summarization, photos are grouped into events based on multimodal features (time and location). The "best" photos are then selected according to visual quality, event representativeness, and diversity. The second stage, story remixing, automatically selects an appropriate theme-dependent editing style based on the photo content. Each selected photo is converted to a video clip by applying a virtual camera with appropriate motions. A series of video effects, color filters, shapes, and transitions are then applied to the video clips according to cinematic rules. The generated video is finally multiplexed with a music clip to generate the story. Evaluations show that our system achieves superior performance to state-of-the-art photo event detection and story generation systems.
C1 [Wu, Yue; Yu, Nenghai] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Peoples R China.
   [Shen, Xu; Tian, Xinmei] Univ Sci & Technol China, Chinese Acad Sci, Key Lab Technol Geospatial Informat Proc & Applic, Hefei 230027, Peoples R China.
   [Mei, Tao; Rui, Yong] Microsoft Res, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS; Microsoft
RP Mei, T (corresponding author), Microsoft Res, Beijing 100080, Peoples R China.
EM wye@mail.ustc.edu.cn; shenxu@mail.ustc.edu.cn; tmei@microsoft.com;
   xinmei@ustc.edu.cn; ynh@ustc.edu.cn; yongrui@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU National Natural Science Foundation of China [61572451, 61371192]; Youth
   Innovation Promotion Association Chinese Academy of Sciences
   [CX2100060016]; Fok Ying Tung Education Foundation
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61572451 and Grant 61371192, in part by
   the Youth Innovation Promotion Association Chinese Academy of Sciences
   under Grant CX2100060016, and in part by the Fok Ying Tung Education
   Foundation. The guest editor coordinating the review of this manuscript
   and approving it for publication was Prof. Yingcai Wu. (Y. Wu and X.
   Shen contributed equally to this work.) (Corresponding author: Tao Mei.)
CR Chen Jun-Cheng, 2006, P 14 ACM INT C MULTI, P25
   Chu Wei-Ta., 2008, P 16 ACM INT C MULTI, P829, DOI DOI 10.1145/1459359.1459498
   Cooper Matthew., 2005, ACM T MULTIM COMPUT, V1, P269, DOI [DOI 10.1145/1083314.1083317, 10.1145/1083314.1083317]
   Dong Z, 2014, IEEE SYS MAN CYBERN, P2859, DOI 10.1109/SMC.2014.6974363
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Girgensohn A., 2000, UIST. Proceedings of the 13th Annual ACM Symposium on User Interface Software and Technology, P81, DOI 10.1145/354401.354415
   Gong B, 2007, ICSC 2007: INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING, PROCEEDINGS, P71, DOI 10.1109/ICSC.2007.88
   Graham A., 2002, JCDL 2002. Proceedings of the Second ACM/IEEE-CS Joint Conference on Digital Libraries, P326, DOI 10.1145/544220.544301
   Hua XS, 2006, IEEE T CIRC SYST VID, V16, P803, DOI 10.1109/TCSVT.2006.877394
   Kavitha C., 2011, J COMPUTER APPL, V15, P33
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kuo TH, 2011, LECT NOTES COMPUT SC, V6523, P73
   Loui AC, 2000, 2000 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, PROCEEDINGS VOLS I-III, P1125, DOI 10.1109/ICME.2000.871558
   Luo YW, 2008, LECT NOTES COMPUT SC, V5304, P386
   Ma Y.F., 2003, P 11 ACM INT C MULT, P374, DOI DOI 10.1145/957013.957094
   Mei T, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1757, DOI 10.1109/ICME.2006.262891
   Mei T, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2536798
   Mei Tao., 2007, Proceedings of the 15th International Conference on Multimedia, P1075
   Platt JC, 2003, ICICS-PCM 2003, VOLS 1-3, PROCEEDINGS, P6
   Shen X, 2016, MULTIMED TOOLS APPL, V75, P2527, DOI 10.1007/s11042-015-2658-6
   Tong HH, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P17, DOI 10.1109/ICME.2004.1394114
   Ullas G, 2003, P 5 ACM SIGMM INT WO, P47
   Winder SAJ, 2007, PROC CVPR IEEE, P17
   Wu Y, 2015, IEEE T CIRC SYST VID, V25, P1941, DOI 10.1109/TCSVT.2015.2416554
   Xia T, 2010, J VIS COMMUN IMAGE R, V21, P826, DOI 10.1016/j.jvcir.2010.06.005
   Yang XY, 2016, ACM T MULTIM COMPUT, V12, DOI 10.1145/2818709
NR 27
TC 10
Z9 11
U1 1
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2016
VL 18
IS 11
BP 2206
EP 2216
DI 10.1109/TMM.2016.2614185
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EA9BX
UT WOS:000386936900008
DA 2024-07-18
ER

PT J
AU Dimoulas, CA
AF Dimoulas, Charalampos A.
TI Audiovisual Spatial-Audio Analysis by Means of Sound Localization and
   Imaging: A Multimedia Healthcare Framework in Abdominal Sound Mapping
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Abdominal sounds (AS); audiovisual content management; multimedia-based
   healthcare; multimodal monitoring; spatial audio analysis; sound
   imaging; sound localization
ID BIOACOUSTICS APPLICATION; FRACTAL DIMENSION; LONG-TERM; CLASSIFICATION;
   SEGMENTATION; WAVELETS; MPEG-7
AB This paper presents the novel sound-field localization and visualization techniques for audiovisual spatial audio analysis, content description, and management. The method focuses on topographic analysis and mapping of gastro-intestinal motility (GIM) through multichannel recording of abdominal sounds (AS). Related research has attempted to study GIM physiology patterns and diagnose specific abnormalities or diseases. In this context, a new AS source localization method is implemented and evaluated. Sound imaging and spatiotemporalmapping utilities are deployed next by means of sound level distribution images. Novel audio description and management is introduced for the analysis automation of prolonged AS recordings, facilitating smart content browsing with audiovisual summarization and highlighting. The proposed modalities can be utilized in multimedia healthcare applications, where multimodal monitoring of GIM and other psychophysiological parameters can be combined for the study of human digestive patterns and their relation to other factors (i.e., subjects' medical history, nutrition, medication, psychological state, and others). The adopted methodology of spatial audio analysis introduces a generic framework that can be efficiently applied to various sound localization and imaging tasks, which are encountered both in bioacoustics and in contemporary multimedia involving multichannel/3D audio.
C1 [Dimoulas, Charalampos A.] Aristotle Univ Thessaloniki, Lab Elect Media, Sch Journalism & Mass Commun, GR-54124 Thessaloniki, Greece.
   [Dimoulas, Charalampos A.] Aristotle Univ Thessaloniki, Lab Electroacoust & TV Syst, Sch Elect & Comp Engn, GR-54124 Thessaloniki, Greece.
C3 Aristotle University of Thessaloniki; Aristotle University of
   Thessaloniki
RP Dimoulas, CA (corresponding author), Aristotle Univ Thessaloniki, Lab Elect Media, Sch Journalism & Mass Commun, GR-54124 Thessaloniki, Greece.; Dimoulas, CA (corresponding author), Aristotle Univ Thessaloniki, Lab Electroacoust & TV Syst, Sch Elect & Comp Engn, GR-54124 Thessaloniki, Greece.
EM babis@eng.auth.gr
RI Dimoulas, Charalampos/ABU-1098-2022
OI Dimoulas, Charalampos/0000-0001-7923-9361
CR Abdel-Mottaleb M, 2004, IEEE T MULTIMEDIA, V6, P459, DOI 10.1109/TMM.2004.827500
   Al Mamun Khandaker A., 2016, Sensing and Bio-Sensing Research, V7, P84, DOI 10.1016/j.sbsr.2016.01.004
   Al Mamun KA, 2015, IEEE IMTC P, P779, DOI 10.1109/I2MTC.2015.7151367
   Barzelay Z, 2010, IEEE T MULTIMEDIA, V12, P108, DOI 10.1109/TMM.2009.2037387
   Cha Z, 2008, IEEE T MULTIMEDIA, V10, P538, DOI 10.1109/TMM.2008.917406
   Charleston-Villalobos S, 2004, MED BIOL ENG COMPUT, V42, P618, DOI 10.1007/BF02347543
   Chen JC, 2003, EURASIP J APPL SIG P, V2003, P359, DOI 10.1155/S1110865703212038
   Chien CH, 2009, BIOMED ENG-APP BAS C, V21, P333, DOI 10.4015/S1016237209001428
   Craine BL, 2002, DIGEST DIS SCI, V47, P1290, DOI 10.1023/A:1015318413638
   Dimoulas A, 2008, EURASIP J ADV SIG PR, DOI 10.1155/2008/792028
   Dimoulas C, 2008, EXPERT SYST APPL, V34, P26, DOI 10.1016/j.eswa.2006.08.014
   Dimoulas C, 2007, COMPUT BIOL MED, V37, P438, DOI 10.1016/j.compbiomed.2006.08.013
   Dimoulas C, 2006, BIOMED SIGNAL PROCES, V1, P177, DOI 10.1016/j.bspc.2006.08.004
   Dimoulas C., 2007, 122 AUD ENG SOC CONV
   Dimoulas C., 2009, 126 AUD ENG SOC CONV
   Dimoulas CA, 2011, EXPERT SYST APPL, V38, P13082, DOI 10.1016/j.eswa.2011.04.115
   Dimoulas C. A., 2006, THESIS
   Dimoulas CA, 2015, IEEE MULTIMEDIA, V22, P26, DOI 10.1109/MMUL.2015.33
   Drossman D A, 1999, Gut, V45 Suppl 2, pII1
   Funayama Y, 2014, 2014 10TH FRANCE-JAPAN/ 8TH EUROPE-ASIA CONGRESS ON MECATRONICS (MECATRONICS), P80, DOI 10.1109/MECATRONICS.2014.7018588
   Hadjileontiadis LJ, 2005, IEEE T BIO-MED ENG, V52, P1143, DOI 10.1109/TBME.2005.846706
   Izadinia H, 2013, IEEE T MULTIMEDIA, V15, P378, DOI 10.1109/TMM.2012.2228476
   Kim KS, 2012, INT CONF AWARE SCI, P185
   Kraman SS, 2006, IEEE T BIO-MED ENG, V53, P1711, DOI 10.1109/TBME.2006.873696
   Lai K, 2016, STUD COMPUT INTELL, V606, P143, DOI 10.1007/978-3-319-19147-8_8
   Li D, 2003, EURASIP J APPL SIG P, V2003, P321, DOI 10.1155/S1110865703212075
   Li PB, 2015, IEEE T BIO-MED ENG, V62, P2702, DOI 10.1109/TBME.2015.2444406
   Lu Q., 2013, ENGINEERING, V5, P73, DOI [10.4236/eng.2013.55B015, DOI 10.4236/ENG.2013.55B015]
   Minotto Vicente P., 2014, IEEE Transactions on Multimedia, V16, P1032, DOI 10.1109/TMM.2014.2305632
   Muhammad G, 2014, BIOMED SIGNAL PROCES, V11, P1, DOI 10.1016/j.bspc.2014.02.001
   Ntalampiras S, 2011, IEEE T MULTIMEDIA, V13, P713, DOI 10.1109/TMM.2011.2122247
   Ranta R, 2003, P ANN INT IEEE EMBS, V25, P2769, DOI 10.1109/IEMBS.2003.1280491
   Ranta R, 2010, IEEE T BIO-MED ENG, V57, P1507, DOI 10.1109/TBME.2010.2040081
   Reju VG, 2013, IEEE T MULTIMEDIA, V15, P1365, DOI 10.1109/TMM.2013.2264656
   Stowell D, 2015, IEEE T MULTIMEDIA, V17, P1733, DOI 10.1109/TMM.2015.2428998
   Ulusar UD, 2014, COMPUT BIOL MED, V51, P223, DOI 10.1016/j.compbiomed.2014.05.013
   Vryzas N., 2015, 10 AUD MOSTL C INT S
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Yin Y., 2015, 2015 40th International Conference on Infrared, Millimeter and Terahertz Waves (IRMMW-THz), P1, DOI 10.1109/IRMMW-THz.2015.7327663
NR 39
TC 16
Z9 18
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2016
VL 18
IS 10
BP 1969
EP 1976
DI 10.1109/TMM.2016.2594148
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DX8NC
UT WOS:000384644800005
DA 2024-07-18
ER

PT J
AU Wang, Y
   Li, S
   Kot, AC
AF Wang, Yan
   Li, Sheng
   Kot, Alex C.
TI On Branded Handbag Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Branded handbag recognition; discriminative; fine-grained object
   recognition
ID IMAGE; RETRIEVAL; ADAPTATION; ATTRIBUTES; SCALE
AB Manufacturing branded handbags is a big business in the fashion world. Shoppers' feedback showing photos of their purchased handbags in social networks or blogs is important for branding purposes. In this paper, we deal with handbag recognition. It is a challenging problem due to the inter-class style similarity and the intra-class color variation. We focus on developing discriminative representations of handbag style and color. For handbag style representation, two supervised mid-level patch selection procedures are proposed to select discriminative patches, regarding individual classes and pairwise classes. We also propose a low-level complementary feature, extracted from texture-enhanced mid-level patches, to capture the fine details of the mid-level patches. For handbag color representation, we propose to extract dominant color features to handle the illumination changes. The performance of our proposed method is evaluated on a newly built branded handbag dataset. The results show that our method performs favorably in recognizing handbags, with around 10% improvement in accuracy when compared with the existing fine-grained or generic object recognition methods.
C1 [Wang, Yan; Kot, Alex C.] Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search ROSE Lab, Singapore 637553, Singapore.
   [Li, Sheng] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 Nanyang Technological University; Shanghai University
RP Wang, Y (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Rapid Rich Object Search ROSE Lab, Singapore 637553, Singapore.
EM wang0696@e.ntu.edu.sg; shengli@shu.edu.cn; eackot@ntu.edu.sg
RI li, sheng/AAF-2381-2019
OI li, sheng/0000-0002-7932-9831
FU National Research Foundation, Singapore, under its Interactive Digital
   Media Strategic Research Programme
FX This work was carried out at the Rapid-Rich Object Search (ROSE)
   Laboratory at the Nanyang Technological University, Singapore. The ROSE
   Laboratory was supported by the National Research Foundation, Singapore,
   under its Interactive Digital Media Strategic Research Programme.
CR Angelova A, 2013, IEEE WORK APP COMP, P39, DOI 10.1109/WACV.2013.6474997
   [Anonymous], P TRECVID
   [Anonymous], CNSTR2007001 CALTECH
   [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2006, P IEEE COMPUTER SOC, DOI DOI 10.1109/CVPR.2006.95
   [Anonymous], 2010, CNSTR2010001 CALTECH
   [Anonymous], RECOGNITION OBJECT I
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2010, P NIPS
   [Anonymous], 2011, PROC CVPR WORKSHOP F
   [Anonymous], 2012, P ACM INT C MULT
   [Anonymous], IEEE C COMP VIS PATT
   Arandjelovic R, 2012, PROC CVPR IEEE, P2911, DOI 10.1109/CVPR.2012.6248018
   Berg T, 2013, PROC CVPR IEEE, P955, DOI 10.1109/CVPR.2013.128
   Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32
   Breiman L., 2001, Mach. Learn., V45, P5
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Duan K, 2012, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2012.6248089
   Endres I, 2013, PROC CVPR IEEE, P939, DOI 10.1109/CVPR.2013.126
   Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238
   Feng J, 2011, IEEE I CONF COMP VIS, P1028, DOI 10.1109/ICCV.2011.6126348
   Gavves E, 2013, IEEE I CONF COMP VIS, P1713, DOI 10.1109/ICCV.2013.215
   Gavves E, 2015, INT J COMPUT VISION, V111, P191, DOI 10.1007/s11263-014-0741-5
   Geusebroek JM, 2003, IEEE T IMAGE PROCESS, V12, P938, DOI 10.1109/TIP.2003.812429
   Gualdi G, 2008, IEEE T MULTIMEDIA, V10, P1142, DOI 10.1109/TMM.2008.2001378
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jia D, 2013, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2013.81
   Juneja M, 2013, PROC CVPR IEEE, P923, DOI 10.1109/CVPR.2013.124
   Khan FS, 2012, PROC CVPR IEEE, P3306, DOI 10.1109/CVPR.2012.6248068
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li HS, 2011, IEEE I CONF COMP VIS, P33, DOI 10.1109/ICCV.2011.6126222
   Li HS, 2010, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2010.5539776
   Li L.-J., 2007, PROC IEEE 11 INT C C, P1
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406
   Liu Luoqi, 2014, ACM T MULTIM COMPUT, V11, P1, DOI DOI 10.1145/2659234
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu YA, 2011, IEEE T MULTIMEDIA, V13, P280, DOI 10.1109/TMM.2010.2103931
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Shotton J, 2013, IEEE T PATTERN ANAL, V35, P2821, DOI 10.1109/TPAMI.2012.241
   Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316
   Singh S, 2012, LECT NOTES COMPUT SC, V7573, P73, DOI 10.1007/978-3-642-33709-3_6
   Stojic T, 2006, PHYSICA A, V367, P494, DOI 10.1016/j.physa.2005.11.030
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wah Catherine, 2011, Technical report
   Wang JJ, 2010, PROC CVPR IEEE, P3360, DOI 10.1109/CVPR.2010.5540018
   Wang Y, 2014, IEEE IMAGE PROC, P5896, DOI 10.1109/ICIP.2014.7026191
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Yang Shulin., 2012, NIPS, P3131
   Yao BP, 2012, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2012.6248088
   Yao BP, 2011, PROC CVPR IEEE, P1577, DOI 10.1109/CVPR.2011.5995368
   Yin WY, 2011, IEEE T MULTIMEDIA, V13, P432, DOI 10.1109/TMM.2011.2129501
   You QZ, 2015, IEEE INT C SEMANT CO, P173, DOI 10.1109/ICOSC.2015.7050803
   Zhang N, 2013, IEEE I CONF COMP VIS, P729, DOI 10.1109/ICCV.2013.96
   Zheng L, 2015, IEEE T MULTIMEDIA, V17, P648, DOI 10.1109/TMM.2015.2408563
NR 63
TC 8
Z9 9
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2016
VL 18
IS 9
BP 1869
EP 1881
DI 10.1109/TMM.2016.2581580
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DT4GI
UT WOS:000381437800016
DA 2024-07-18
ER

PT J
AU Fedorov, R
   Camerada, A
   Fraternali, P
   Tagliasacchi, M
AF Fedorov, Roman
   Camerada, Alessandro
   Fraternali, Piero
   Tagliasacchi, Marco
TI Estimating Snow Cover From Publicly Available Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Environmental monitoring; mountain identification; scene classification;
   snow cover index; snow estimation; user generated content (UGC)
ID TIME-LAPSE PHOTOGRAPHY; CLASSIFICATION; ACCUMULATION; RESOLUTION
AB In this paper, we study the problem of estimating snow cover in mountainous regions, that is, the spatial extent of the earth surface covered by snow. We argue that publicly available visual content, in the form of user-generated photographs and image feeds from outdoor webcams, can both be leveraged as additional measurement sources, complementing existing ground, satellite, and airborne sensor data. To this end, we describe two content acquisition and processing pipelines that are tailored to such sources, addressing the specific challenges posed by each of them, e.g., identifying the mountain peaks, filtering out images taken in bad weather conditions, and handling varying illumination conditions. The final outcome is summarized in a snow cover index, which indicates for a specific mountain and day of the year the fraction of visible area covered by snow, possibly at different elevations. We created a manually labeled dataset to assess the accuracy of the image snow covered area estimation, achieving 90.0% precision at 91.1% recall. In addition, we show that seasonal trends related to air temperature are captured by the snow cover index.
C1 [Fedorov, Roman; Camerada, Alessandro; Fraternali, Piero; Tagliasacchi, Marco] Politecn Milan, Dipartimento Elettron Informaz & Bioingn, I-20133 Milan, Italy.
C3 Polytechnic University of Milan
RP Fedorov, R; Camerada, A; Fraternali, P; Tagliasacchi, M (corresponding author), Politecn Milan, Dipartimento Elettron Informaz & Bioingn, I-20133 Milan, Italy.
EM roman.fedorov@polimi.it; alessandro.camerada@mail.polimi.it;
   piero.fraternali@polimi.it; marco.tagliasacchi@polimi.it
FU POR-FESR PROACTIVE Project; EU
FX This work was supported by the POR-FESR 2007-2013 PROACTIVE Project, and
   also by the EU FP7 CUbRIK Integrating Project. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Qi Tian.
CR [Anonymous], 2007, 2007 IEEE C COMP VIS, DOI DOI 10.1109/CVPR.2007.383198
   Baatz G, 2012, LECT NOTES COMPUT SC, V7573, P517, DOI 10.1007/978-3-642-33709-3_37
   Baboud L, 2011, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2011.5995727
   Bradley ES, 2011, CARTOGR GEOGR INF SC, V38, P3, DOI 10.1559/152304063813
   DeBeer CM, 2009, HYDROL PROCESS, V23, P2584, DOI 10.1002/hyp.7346
   Farinotti D, 2010, HYDROL PROCESS, V24, P2087, DOI 10.1002/hyp.7629
   Fedorov R., 2014, P 3 ACM INT WORKSH M, P7
   Fedorov R., 2013, 2 INT WORKSH SOC MED
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Floyd W, 2008, HYDROL PROCESS, V22, P4805, DOI 10.1002/hyp.7142
   Garvelmann J, 2013, HYDROL EARTH SYST SC, V17, P1415, DOI 10.5194/hess-17-1415-2013
   Grauman K, 2005, IEEE I CONF COMP VIS, P1458
   Hall DK, 2002, REMOTE SENS ENVIRON, V83, P181, DOI 10.1016/S0034-4257(02)00095-0
   Hinkler J, 2002, INT J REMOTE SENS, V23, P4669, DOI 10.1080/01431160110113881
   Laffly D, 2012, POLAR REC, V48, P11, DOI 10.1017/S0032247411000519
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lie WN, 2005, PATTERN RECOGN LETT, V26, P221, DOI 10.1016/j.patrec.2004.08.021
   Liu W.-H., 2014, ADV TECHNOLOGIES EMB, V260, P1115
   NOLIN AW, 1993, REMOTE SENS ENVIRON, V44, P231, DOI 10.1016/0034-4257(93)90018-S
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Parajka J, 2012, HYDROL PROCESS, V26, P3327, DOI 10.1002/hyp.8389
   Prokop A, 2008, COLD REG SCI TECHNOL, V54, P155, DOI 10.1016/j.coldregions.2008.07.002
   Rüfenacht D, 2014, PROCEEDINGS OF THE 2014 9TH INTERNATIONAL CONFERENCE ON COMPUTER VISION, THEORY AND APPLICATIONS (VISAPP 2014), VOL 2, P275
   Ruzon M. A., 1999, P IEEE COMP SOC C CO, V2, P2160
   Salomonson VV, 2004, REMOTE SENS ENVIRON, V89, P351, DOI 10.1016/j.rse.2003.10.016
   Salvatori R, 2011, ITAL J REMOTE SENS, V43, P137, DOI 10.5721/ItJRS201143211
   Sánchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x
   SHI JC, 1994, IEEE T GEOSCI REMOTE, V32, P152, DOI 10.1109/36.285197
   Xiao J., 2014, IJCV, P1
NR 29
TC 22
Z9 22
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1187
EP 1200
DI 10.1109/TMM.2016.2535356
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Akbari, M
   Cheng, H
AF Akbari, Mohammad
   Cheng, Howard
TI Real-Time Piano Music Transcription Based on Computer Vision
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Automatic music transcription; claVision; computer vision; multipitch
   estimation; piano
AB One important problem in musical information retrieval is automatic music transcription, which is an automated conversion process from played music to a symbolic notation such as MIDI file. Since the accuracy of previous audio-based transcription systems is not satisfactory, we propose an innovative computer vision-based automatic music transcription system named claVision to perform piano music transcription. Instead of processing the music audio, the system performs the transcription only from the video performance captured by a camera mounted over the piano keyboard. In this paper, we describe the architecture and the algorithms used in claVision. The claVision system has a high accuracy (F-1 score over 0.95) and a very low latency (about 7.0 ms) in real-time music transcription, even under different illumination conditions. This technology can also be used for other musical keyboard instruments.
C1 [Akbari, Mohammad; Cheng, Howard] Univ Lethbridge, Dept Math & Comp Sci, Lethbridge, AB T1K 3M4, Canada.
C3 University of Lethbridge
RP Akbari, M (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC V5A 1S6, Canada.
EM akbari@sfu.ca; howard.cheng@uleth.ca
RI akbari, mohammad/ADF-5801-2022
FU Natural Sciences and Engineering Research Council Discovery Grant
   Program; Alberta Innovates Technology Futures geekStarter Program
FX This work was supported by the Natural Sciences and Engineering Research
   Council Discovery Grant Program and by the Alberta Innovates Technology
   Futures geekStarter Program. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Jiebo
   Luo.
CR Akbari M., 2014, THESIS U LETHBRIDGE
   [Anonymous], 2008, THESIS
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   Benetos E, 2013, J INTELL INF SYST, V41, P407, DOI 10.1007/s10844-013-0258-3
   Collins N., 2005, AUDIO ENG SOC CONVEN, V118
   Dorf R.H., 1968, Electronic Musical Instruments
   Downie JS, 2003, ANNU REV INFORM SCI, V37, P295, DOI 10.1002/aris.1440370108
   Dressler K., 2012, P 8 MUS INF RETR EV, P1
   Frisson Christian., 2009, QPSR NUMEDIART RES P, V2, P67
   Gorodnichy D.O., 2006, The 3rd Canadian Conference on Computer and Robot Vision, P63
   Heckbert Paul S., 1989, THESIS
   Hough P.V., 1962, U.S. Patent, Patent No. 3069654
   Lee CT, 2012, IEEE T MULTIMEDIA, V14, P608, DOI 10.1109/TMM.2012.2191398
   MAHER RC, 1990, J AUDIO ENG SOC, V38, P956
   Moorer J.A., 1977, COMPUT MUSIC J, V1, P32
   Oka A, 2013, KOR-JPN JT WORKS FR, P1, DOI 10.1109/FCV.2013.6485449
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Paleari M, 2008, IEEE IMAGE PROC, P93, DOI 10.1109/ICIP.2008.4711699
   Peeling PH, 2011, IEEE J-STSP, V5, P1133, DOI 10.1109/JSTSP.2011.2158804
   Piszczalski M., 1977, Computer Music Journal, V1, P24
   Quested G., 2008, P INT COMP MUS C, V2008
   Reboursiere L., 2010, Proceedings of the Conference on New Interfaces for Musical Expression, P415
   SAMET H, 1988, IEEE T PATTERN ANAL, V10, P579, DOI 10.1109/34.3918
   Savard J. G., 2011, SIZE PIANO KEYBOARD
   Scarr J., 2010, 25th International Conference of Image and Vision Computing New Zealand (IVCNZ), P1
   Smaragdis P, 2003, 2003 IEEE WORKSHOP ON APPLICATIONS OF SIGNAL PROCESSING TO AUDIO AND ACOUSTICS PROCEEDINGS, P177, DOI 10.1109/ASPAA.2003.1285860
   Sotirios M., 2008, Proceedings of the 10th International Conference on Information Integration and Web-based Applications Services, iiWAS '08, P604
   Suteparuk P., 2014, DETECTION PIANO KEYS
   Tavares T. Fernandes, 2013, J BRAZ COMPUT SOC, V19, P589, DOI DOI 10.1007/s13173-013-0118-6
   Zhang Bingjun., 2007, P ACM INT C MULT, P521
NR 30
TC 16
Z9 17
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2113
EP 2121
DI 10.1109/TMM.2015.2473702
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500001
DA 2024-07-18
ER

PT J
AU Wang, SF
   Wang, J
   Wang, ZH
   Ji, Q
AF Wang, Shangfei
   Wang, Jun
   Wang, Ziheng
   Ji, Qiang
TI Multiple Emotion Tagging for Multimedia Data by Exploiting High-Order
   Dependencies Among Emotions
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Multi-emotion recognition; multi-label classification; multimedia
   tagging; three-layer restricted Boltzmann machine (TRBM)
ID CLASSIFICATION; AROUSAL
AB In this paper, a novel approach of multiple emotional multimedia tagging is proposed, which explicitly models the higher-order relations among emotions. First, multimedia features are extracted from the multimedia data. Second, a traditional multi-label classifier is used to obtain the measurements of the multi-emotion labels. Then, we propose a three-layer restricted Boltzmann machine (TRBM) model to capture the higher-order relations among emotion labels, as well as the relations between labels and measurements. Finally, the TRBM model is used to infer the samples' multi-emotion labels by combining the emotion measurements with the dependencies among multi-emotions. Experimental results on four databases demonstrate that our method is more effective than both feature-driven methods and current model-based methods, which capture the pairwise relations among labels by the Bayesian network (BN). Furthermore, the comparison of BN models and the proposed TRBM model verifies that the patterns captured by the latent units of TRBM contain not only all the dependencies captured by the BN but also many other dependencies that the BN cannot capture.
C1 [Wang, Shangfei; Wang, Jun] USTC, Sch Comp Sci & Technol, Hefei 270000, Peoples R China.
   [Wang, Ziheng; Ji, Qiang] Rensselaer Polytech Inst, Dept Elect Comp & Syst Engn, Troy, NY 12180 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Rensselaer Polytechnic Institute
RP Wang, SF (corresponding author), USTC, Sch Comp Sci & Technol, Hefei 270000, Peoples R China.
EM sfwang@ustc.edu.cn; junwong@mail.ustc.edu.cn; wangz10@rpi.edu;
   jiq@rpi.edu
FU National Natural Science Foundation of China [61175037, 61228304,
   61473270]; Anhui Science and Technology Agency [1106c0805008]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 61175037, Grant 61228304, and Grant 61473270, and by
   the Anhui Science and Technology Agency under Project 1106c0805008. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Klara Nahrstedt.
CR [Anonymous], THESIS
   [Anonymous], 2009, NIPS
   [Anonymous], 2010, P ACM SIGKDD
   [Anonymous], 2003, Proceedings of the International Symposium on Music Information Retrieval
   [Anonymous], P IEEE SOUTH
   [Anonymous], 2010, P ACM INT C IM VID R
   [Anonymous], 2011, P 1 INT ACM WORKSHOP, DOI DOI 10.1145/2072529.2072532
   [Anonymous], 2010, 11 INT SOC MUS INF R
   [Anonymous], MULTIMEDIA TOOLS APP
   [Anonymous], 2008, ISMIR
   [Anonymous], 2012, ACM T INTEL SYST TEC, DOI DOI 10.1145/2168752.2168754
   [Anonymous], 2013, EURASIP J IMAGE VIDE
   [Anonymous], 2013, ERA INTERACTIVE MEDI
   Arifin S., 2007, Proceedings of the 15th International Conference on Multimedia, P68
   Arifin S, 2007, ICSC 2007: INTERNATIONAL CONFERENCE ON SEMANTIC COMPUTING, PROCEEDINGS, P147, DOI 10.1109/ICSC.2007.22
   Arifin S, 2008, IEEE T MULTIMEDIA, V10, P1325, DOI 10.1109/TMM.2008.2004911
   Canini L, 2013, IEEE T CIRC SYST VID, V23, P636, DOI 10.1109/TCSVT.2012.2211935
   Canini L, 2009, IEEE IMAGE PROC, P1821, DOI 10.1109/ICIP.2009.5413556
   Chen Xiu Y., 2009, 2009 WRI World Congress on Computer Science and Information Engineering (CSIE 2009), P277, DOI 10.1109/CSIE.2009.982
   Cherman Everton Alvares, 2011, CLEIej, V14, P4
   Eerola T., 2009, P 10 INT SOC MUS INF, P621
   Ekman P., 1999, HDB COGNITION EMOTIO, V98, P16
   Fürnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8
   Godbole S, 2004, LECT NOTES ARTIF INT, V3056, P22
   GROSS JJ, 1995, COGNITION EMOTION, V9, P87, DOI 10.1080/02699939508408966
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Huang S. J., 2012, P 18 ACM SIGKDD INT, P525, DOI DOI 10.1145/2339530.2339615
   Irie G, 2010, IEEE T MULTIMEDIA, V12, P523, DOI 10.1109/TMM.2010.2051871
   Isola P, 2011, PROC CVPR IEEE, P145, DOI 10.1109/CVPR.2011.5995721
   Joshi D, 2011, IEEE SIGNAL PROC MAG, V28, P94, DOI 10.1109/MSP.2011.941851
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   Kang HB, 2003, IEEE IMAGE PROC, P721
   Kok-Meng Ong, 2009, Information and Media Technologies, V4, P903
   Li DG, 2001, PATTERN RECOGN LETT, V22, P533, DOI 10.1016/S0167-8655(00)00119-7
   Money AG, 2008, LECT NOTES COMPUT SC, V4868, P194, DOI 10.1007/978-3-540-85099-1_17
   Money AG, 2009, DISPLAYS, V30, P59, DOI 10.1016/j.displa.2008.12.003
   PHILIPPOT P, 1993, COGNITION EMOTION, V7, P171, DOI 10.1080/02699939308409183
   Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17
   Schaefer A, 2010, COGNITION EMOTION, V24, P1153, DOI 10.1080/02699930903274322
   Schuller B., 2010, EURASIP J AUDIO SPEE, V2010, P735
   Sun K, 2007, LECT NOTES COMPUT SC, V4738, P594
   Teixeira RMA, 2012, MULTIMED TOOLS APPL, V61, P21, DOI 10.1007/s11042-010-0702-0
   Trohidis K, 2011, EURASIP J AUDIO SPEE, DOI 10.1186/1687-4722-2011-426793
   Tsoumakas G, 2007, LECT NOTES ARTIF INT, V4701, P406
   Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34
   Wang CW, 2007, LECT NOTES COMPUT SC, V4351, P606
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang SF, 2005, LECT NOTES COMPUT SC, V3784, P490
   Wang SF, 2011, KANSEI ENGINEERING AND SOFT COMPUTING: THEORY AND PRACTICE, P126, DOI 10.4018/978-1-61692-797-4.ch007
   Wang SF, 2015, MULTIMED TOOLS APPL, V74, P1863, DOI 10.1007/s11042-013-1722-3
   Wang SF, 2014, MULTIMED TOOLS APPL, V72, P1257, DOI 10.1007/s11042-013-1450-8
   Wang WN, 2008, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2008.4711705
   Wang ZH, 2013, IEEE I CONF COMP VIS, P3304, DOI 10.1109/ICCV.2013.410
   Watanapa SC, 2008, IEICE T INF SYST, VE91D, P1562, DOI 10.1093/ietisy/e91-d.5.1562
   Wei CY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXP (ICME), VOLS 1-3, P831, DOI 10.1109/ICME.2004.1394329
   Wieczorkowska A, 2006, ADV SOFT COMP, P307
   Xiao JX, 2010, PROC CVPR IEEE, P3485, DOI 10.1109/CVPR.2010.5539970
   Xinmiao Ding, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P599, DOI 10.1007/978-3-642-37431-9_46
   Xu M., 2005, 2005 IEEE International Conference on Multimedia and Expo
   Xu M., 2008, Proceeding of the 16th ACM International Conference on Multimedia, P677, DOI DOI 10.1145/1459359.1459457
   Xu M, 2014, MULTIMED TOOLS APPL, V70, P757, DOI 10.1007/s11042-012-1046-8
   Xu M, 2013, SIGNAL PROCESS, V93, P2140, DOI 10.1016/j.sigpro.2012.06.026
   Xu M, 2010, LECT NOTES COMPUT SC, V6298, P43, DOI 10.1007/978-3-642-15696-0_5
   Yazdani J.-S. Lee., 2009, Proc. SIGMM Workshop on Social media, P81
   Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019
   Zhang ML, 2006, IEEE T KNOWL DATA EN, V18, P1338, DOI 10.1109/TKDE.2006.162
   Zhang SL, 2009, IEEE IMAGE PROC, P1853, DOI 10.1109/ICIP.2009.5413590
   Zhang SL, 2010, IEEE T MULTIMEDIA, V12, P510, DOI 10.1109/TMM.2010.2059634
   Zhang SL, 2008, 2008 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-4, P1369, DOI 10.1109/ICME.2008.4607698
   Zhao Sicheng, 2011, P 19 ACM INT C MULTI, P1473, DOI [10.1145/2072298.2072043, DOI 10.1145/2072298.2072043]
NR 70
TC 26
Z9 27
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2015
VL 17
IS 12
BP 2185
EP 2197
DI 10.1109/TMM.2015.2484966
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CW9JR
UT WOS:000365315500007
DA 2024-07-18
ER

PT J
AU Guruprasad, R
   Dey, S
AF Guruprasad, Ranjini
   Dey, Sujit
TI Battery Aware Video Delivery Techniques Using Rate Adaptation and Base
   Station Reconfiguration
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Base station reconfiguration; battery life; mobile video; power
   consumption; rate adaptation; user experience; video viewing time
AB With mobile video increasingly becoming an important driver of mobile device usage, the battery consumption of mobile devices will be dominated by video delivery and playback. In this paper, we develop battery efficient video download techniques that vary video download rate dynamically, including stopping video download at times, depending on mobile device buffer levels and the channel conditions experienced, to maximize battery life while ensuring no degradation in user experience. The proposed dynamic download rate adaptation techniques enable the base station to adapt the MIMO transceiver configurations to reduce battery load required by MIMO components on the mobile device. In order to further enhance battery life, we propose to utilize video bit rate adaptation, in addition to download rate adaptation and MIMO reconfiguration. The proposed battery aware bit rate adaptation techniques take into account the mobile device battery and buffer levels, and network load and channel conditions experienced, to maximize battery lifetime (hence video viewing time) while ensuring desired level of video experience (measured in terms of video quality and stalls experienced). We propose a new metric termed "video experience longevity (VEL)" which quantifies the performance of the proposed bit rate adaptation techniques in terms of video viewing time and video experience. Extensive experiments conducted under variable channel conditions and network load demonstrate that the proposed battery aware video delivery techniques can significantly outperform other video delivery techniques in terms of battery lifetime and VEL metric (for bit rate adaptation techniques) while ensuring desired level of video experience.
C1 [Guruprasad, Ranjini; Dey, Sujit] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
C3 University of California System; University of California San Diego
RP Guruprasad, R (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM rgurupra@ucsd.edu; dey@ucsd.edu
CR [Anonymous], 2010, 36211 TS
   [Anonymous], 2009, IIS SMOOTH STREAM
   [Anonymous], 2014, CISC VIS NETW IND GL
   [Anonymous], 2011, 26247 3GPP TS
   [Anonymous], 2012, US DIG VID BENCHM 20
   Atayero A. A., 2011, LECT NOTES ELECT ENG, V170, P15
   Begen AC, 2011, IEEE INTERNET COMPUT, V15, P54, DOI 10.1109/MIC.2010.155
   Carroll A., 2010, USENIX annual technical conference, P271
   Cermak G, 2011, IEEE T BROADCAST, V57, P258, DOI 10.1109/TBC.2011.2121650
   Chen M, 2006, IEEE T ENERGY CONVER, V21, P504, DOI 10.1109/TEC.2006.874229
   Cui SG, 2005, IEEE T WIREL COMMUN, V4, P2349, DOI 10.1109/TWC.2005.853882
   Dai H., 2011, Acta Horticulturae, P169, DOI 10.1145/1943552.1943575
   Garrett D, 2004, IEEE J SOLID-ST CIRC, V39, P1544, DOI 10.1109/JSSC.2004.831454
   Haykin S, 2005, IEEE J SEL AREA COMM, V23, P201, DOI 10.1109/JSAC.2004.839380
   Hormis R, 2009, IEEE T SIGNAL PROCES, V57, P3624, DOI 10.1109/TSP.2009.2020051
   Hossfeld Tobias, 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P494, DOI 10.1109/ISM.2011.87
   Kennedy M, 2010, C LOCAL COMPUT NETW, P843, DOI 10.1109/LCN.2010.5735821
   Kim H, 2009, IEEE T WIREL COMMUN, V8, P4264, DOI 10.1109/TWC.2009.081123
   Kim HS, 2010, IEEE T WIREL COMMUN, V9, P2820, DOI 10.1109/TWC.2010.062910.090983
   Le Wang, 2011, 2011 11th International Conference on ITS Telecommunications (ITST), P217, DOI 10.1109/ITST.2011.6060056
   Li CY, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P341
   Lin CC, 2005, IEEE T CIRCUITS-I, V52, P1148, DOI 10.1109/TCSI.2005.849106
   Ma KJ, 2011, IEEE COMMUN MAG, V49, P166, DOI 10.1109/MCOM.2011.5741161
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Ning Ding, 2013, Performance Evaluation Review, V41, P29
   Pantos R., 2010, HTTP Live Streaming. Internet-Draft draft-pantos-http-live-streaming-05
   Rakhmatov DN, 2001, ICCAD 2001: IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER AIDED DESIGN, DIGEST OF TECHNICAL PAPERS, P488, DOI 10.1109/ICCAD.2001.968687
   Shah S., 2002, P NAT C COMM FEB, P494
   Sheluhin OI, 2011, LECT NOTES ENG COMP, P572
   Tabrizi FM, 2013, IEEE T MOBILE COMPUT, V12, P995, DOI 10.1109/TMC.2012.56
   Tamai M., 2006, P 7 INT C MOB DAT MA, P58
   Tian G., 2012, P 8 INT C EM NETW EX, P109
   Wu H., 2011, P 4 INT ICST C SIMUL, P222
   Xiao Y, 2008, INT CONF NEXT GEN, P61, DOI 10.1109/NGMAST.2008.26
   Zhu TX, 2012, PROCEEDINGS OF THE 4TH (2012) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P279
NR 35
TC 7
Z9 7
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1630
EP 1645
DI 10.1109/TMM.2015.2436821
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000021
DA 2024-07-18
ER

PT J
AU Lee, KH
   Hwang, JN
AF Lee, Kuan-Hui
   Hwang, Jenq-Neng
TI On-Road Pedestrian Tracking Across Multiple Driving Recorders
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D visualization; multi-label classification; pedestrian tracking;
   visual surveillance
ID CAMERAS
AB In this paper, we propose a new framework to track on-road pedestrians across multiple driving recorders. The framework is built upon the results of tracking under a single driving recorder. More specifically, we treat the problem as a multi-label classification task and determine whether a specific pedestrian belongs to one or several cameras' field of views by considering association likelihood of the tracked pedestrians. The likelihood is calculated based on the pedestrians' motion cues and appearance features, which are necessarily transformed via brightness transfer functions obtained by some available spatially overlapping views for compensating diversity of the cameras. When a pedestrian is leaving a camera's field of view, the proposed framework predicts and interpolates its possible moving trajectories, facilitated by open map service which can provide routing information. Experimental results show the robustness and effectiveness of the proposed framework in tracking pedestrians across several recorded driving videos. Moreover, based on the GPS locations, we can also reconstruct a 3-D visualization on a 3-D virtual real-world environment, so as to show the dynamic scenes of the recorded videos.
C1 [Lee, Kuan-Hui] Univ Washington, Dept Elect Engn, Informat Proc Lab, Seattle, WA 98105 USA.
   [Hwang, Jenq-Neng] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
C3 University of Washington; University of Washington Seattle; University
   of Washington; University of Washington Seattle
RP Lee, KH (corresponding author), Univ Washington, Dept Elect Engn, Informat Proc Lab, Seattle, WA 98105 USA.
EM ykhlee@uw.edu; hwang@uw.edu
CR Alahi A, 2010, COMPUT VIS IMAGE UND, V114, P624, DOI 10.1016/j.cviu.2010.01.004
   [Anonymous], 2012, Gurobi optimizer reference manual
   Berclaz J, 2011, IEEE T PATTERN ANAL, V33, P1806, DOI 10.1109/TPAMI.2011.21
   Chu CT, 2014, IEEE T CIRC SYST VID, V24, P979, DOI 10.1109/TCSVT.2014.2302516
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Chu Chun-Te., 2011, Proceedings of ACM/IEEE International Conference on Distributed Smart Cameras ICDSC, P1
   D'Orazio T, 2009, 2009 THIRD ACM/IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED SMART CAMERAS, P365
   Ding C, 2012, IEEE T IMAGE PROCESS, V21, P3282, DOI 10.1109/TIP.2012.2188806
   Ess A, 2009, IEEE T PATTERN ANAL, V31, P1831, DOI 10.1109/TPAMI.2009.109
   Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926
   Fleuret F, 2008, IEEE T PATTERN ANAL, V30, P267, DOI 10.1109/TPAMI.2007.1174
   Forssen P.-E., 2007, IEEE International Conference on Computer Vision (ICCV), P1, DOI DOI 10.1109/CVPR.2007.383120
   Gerónimo D, 2010, IEEE T PATTERN ANAL, V32, P1239, DOI 10.1109/TPAMI.2009.122
   Gilbert A, 2006, LECT NOTES COMPUT SC, V3952, P125
   Gong SG, 2014, ADV COMPUT VIS PATT, P1, DOI 10.1007/978-1-4471-6296-4_1
   Gray D, 2008, LECT NOTES COMPUT SC, V5302, P262, DOI 10.1007/978-3-540-88682-2_21
   Hartley R, 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2
   Hirzer M, 2012, LECT NOTES COMPUT SC, V7577, P780, DOI 10.1007/978-3-642-33783-3_56
   Javed O, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P952
   Lee KH, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P2629, DOI 10.1109/ITSC.2014.6958111
   Leibe B, 2008, IEEE T PATTERN ANAL, V30, P1683, DOI 10.1109/TPAMI.2008.170
   Li W, 2013, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2013.461
   Li Zhang, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563097
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mignon A, 2012, PROC CVPR IEEE, P2666, DOI 10.1109/CVPR.2012.6247987
   Möller B, 2008, INT C PATT RECOG, P968
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Pirsiavash H, 2011, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2011.5995604
   Platt JC, 2000, ADV NEUR IN, P61
   Wang XQ, 2007, INT J THERM SCI, V46, P1, DOI 10.1016/j.ijthermalsci.2006.06.010
   Yu SI, 2013, PROC CVPR IEEE, P3714, DOI 10.1109/CVPR.2013.476
   Zheng WS, 2011, PROC CVPR IEEE, P649, DOI 10.1109/CVPR.2011.5995598
   Zou DP, 2013, IEEE T PATTERN ANAL, V35, P354, DOI 10.1109/TPAMI.2012.104
NR 33
TC 101
Z9 115
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1429
EP 1438
DI 10.1109/TMM.2015.2455418
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000004
DA 2024-07-18
ER

PT J
AU Lin, SS
   Hu, MC
   Lee, CH
   Lee, TY
AF Lin, Shih-Syun
   Hu, Min-Chun
   Lee, Chien-Han
   Lee, Tong-Yee
TI Efficient QR Code Beautification With High Quality Visual Content
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gauss-Jordan elimination; quick response (QR) code beautification;
   Reed-Solomon (RS) code
AB Quick response (QR) code is generally used for embedding messages such that people can conveniently use mobile devices to capture the QR code and acquire information through a QR code reader. In the past, the design of QR code generators only aimed to achieve high decodability and the produced QR codes usually look like random black-and-white patterns without visual semantics. In recent years, researchers have been tried to endow the QR code with aesthetic elements and QR code beautification has been formulated as an optimization problem that minimizes the visual perception distortion subject to acceptable decoding rate. However, the visual quality of the QR code generated by existing methods still leaves much to be desired. In this work, we propose a two-stage approach to generate QR code with high quality visual content. In the first stage, a baseline QR code with reliable decodability but poor visual quality is first synthesized based on the Gauss-Jordan elimination procedure. In the second stage, a rendering mechanism is designed to improve the visual quality while avoiding affecting the decodability of the QR code. The experimental results show that the proposed method substantially enhances the appearance of the QR code and the processing complexity is near real-time.
C1 [Lin, Shih-Syun; Hu, Min-Chun; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
   [Lee, Chien-Han] MACHVISION Inc, Hsinchu 30075, Taiwan.
C3 National Cheng Kung University
RP Lin, SS (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM catchylss@gmail.com; anita_hu@mail.ncku.edu.tw; eee1@hotmail.com;
   tonylee@mail.ncku.edu.tw
RI Lin, Shih-Syun/ABD-8570-2020; Hu, Min-Chun/AAX-1721-2020
OI Lin, Shih-Syun/0000-0002-8360-5819; Hu, Min-Chun/0000-0003-1917-2155
FU Headquarters of University Advancement, National Cheng Kung University;
   Ministry of Science and Technology of Taiwan
   [MOST-104-2221-E-006-044-MY3, MOST-103-2221-E-006-106-MY3,
   MOST-103-2221-E-006-157-MY3]
FX This work was supported in part by the Headquarters of University
   Advancement, National Cheng Kung University, and in part by the Ministry
   of Science and Technology of Taiwan under Contract
   MOST-104-2221-E-006-044-MY3, Contract MOST-103-2221-E-006-106-MY3, and
   Contract MOST-103-2221-E-006-157-MY3. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Alessandro Piva.
CR Anezaki T., 2011, P KOR JAP JOINT WORK, P1
   [Anonymous], 2009, P 8 INT C VIRT REAL
   Chu HK, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508408
   Cox R., 2012, Qartcodes
   Cox R., 2012, FINITE FIELD ARITHME
   Erol B., 2007, P ACM MM 07, P819
   FALCON A, 2013, 40 GORGEOUS QR CODE
   Haisler D., 2010, P ACM MM 10, P1529
   ISO, 2006, 180042006 ISOIEC
   KATO Hiroto, 2010, Barcodes for Mobile Devices
   Lin YS, 2013, COMPUT GRAPH FORUM, V32, P137, DOI 10.1111/cgf.12221
   Lin YH, 2013, IEEE T MULTIMEDIA, V15, P2198, DOI 10.1109/TMM.2013.2271745
   Nikolaos T., 2010, P ACM SIGGRAPH, P144
NR 13
TC 73
Z9 90
U1 1
U2 30
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1515
EP 1524
DI 10.1109/TMM.2015.2437711
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000012
DA 2024-07-18
ER

PT J
AU Xue, NN
   Chen, XL
   Gong, L
   Li, SH
   Hu, DY
   Zhu, ZQ
AF Xue, Nana
   Chen, Xiaoliang
   Gong, Long
   Li, Suoheng
   Hu, Daoyun
   Zhu, Zuqing
TI Demonstration of OpenFlow-Controlled Network Orchestration for Adaptive
   SVC Video Manycast
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manycast; openflow; scalable video coding (SVC); software-defined
   networking (SDN); video streaming
ID DELIVERY
AB Software defined networking (SDN) makes networks programmable and application-aware by decoupling network control and management (NC&M) from data forwarding and leveraging centralized NC&M to facilitate user-customized routing and switching. Inspired by these, this paper investigates how to realize the OpenFlow-controlled (OF-controlled) network orchestration that can facilitate efficient scalable video coding (SVC) streaming to heterogeneous clients. Specifically, we consider real-time SVC streaming and address the situation in which video sources reside in geographically-distributed servers and clients can join and leave the streaming services dynamically. We formulate this as a multi-source multi-destination manycast problem and realize the networking system with an OF-controlled SDN architecture. We first design the OF controller to enable efficient network operations. Then, we focus on solving the multi-source multi-destination SVC video manycast problem and design several algorithms. Initially, an integer linear programming (ILP) model is formulated to obtain the optimal solutions for small-scale problems. Next, we try to make the manycast algorithm suitable for practical implementation, and design two time-efficient heuristics. Simulation results indicate that the heuristics can provide close-to-optimal solutions. Finally, we build an OF network testbed that consists of OF switches, SVC video servers and clients, and perform SVC streaming experiments to demonstrate our design. Experimental results verify that the proposed scheme can allocate bandwidth intelligently and ensure high-quality video streaming. To the best of our knowledge, this is the first work that accomplishes experimental demonstration of OF-controlled network orchestration for adaptive SVC video manycast.
C1 [Xue, Nana; Chen, Xiaoliang; Gong, Long; Li, Suoheng; Hu, Daoyun; Zhu, Zuqing] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Xue, NN (corresponding author), Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230027, Peoples R China.
EM xuenana@mail.ustc.edu.cn; gonglong@mail.ustc.edu.cn; zqzhu@ieee.org
RI Gong, Long/AAM-7993-2021; Chen, Xiaoliang/U-2358-2019; Zhu,
   Zuqing/J-8431-2017
OI Gong, Long/0000-0002-3805-187X; Zhu, Zuqing/0000-0002-4251-788X; Chen,
   Xiaoliang/0000-0002-7805-6237
FU NCET program [NCET-11-0884]; NSFC Project [61371117]; Fundamental
   Research Funds for the Central Universities [WK2100060010]; Natural
   Science Research Project for Universities in Anhui [KJ2014ZD38];
   Strategic Priority Research Program of the CAS [XDA06011202]
FX This work was supported in part by the NCET program under Project
   NCET-11-0884, by the NSFC Project 61371117, by the Fundamental Research
   Funds for the Central Universities under Grant WK2100060010, by the
   Natural Science Research Project for Universities in Anhui under Grant
   KJ2014ZD38, and by the Strategic Priority Research Program of the CAS
   under Grant XDA06011202. The associate editor coordinating the review of
   this manuscript and approving it for publication was Prof. Tommaso
   Melodia.
CR [Anonymous], 2013, CISC VIS NETW IND FO
   Apostolopoulos J, 2002, IEEE INFOCOM SER, P1736, DOI 10.1109/INFCOM.2002.1019427
   Baier G, 2005, ALGORITHMICA, V42, P231, DOI 10.1007/s00453-005-1167-9
   Chen JC, 2004, IEEE J SEL AREA COMM, V22, P1920, DOI 10.1109/JSAC.2004.836000
   Egilmez H. E., 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2241, DOI 10.1109/ICIP.2011.6116083
   Egilmez HE, 2014, IEEE T MULTIMEDIA, V16, P1597, DOI 10.1109/TMM.2014.2325791
   Egilmez HE, 2013, IEEE T MULTIMEDIA, V15, P710, DOI 10.1109/TMM.2012.2232645
   Egilmez HE, 2012, IEEE IMAGE PROC, P2237, DOI 10.1109/ICIP.2012.6467340
   Goth G, 2011, IEEE INTERNET COMPUT, V15, P6, DOI 10.1109/MIC.2011.96
   Ho PH, 2004, IEEE ACM T NETWORK, V12, P1105, DOI 10.1109/TNET.2004.838592
   Iyer Aakash., 2014, Communication Systems and Networks (COMSNETS), 2014 Sixth International Conference on, P1
   Kotani D, 2012, 2012 IEEE/IPSJ 12TH INTERNATIONAL SYMPOSIUM ON APPLICATIONS AND THE INTERNET (SAINT), P60, DOI 10.1109/SAINT.2012.17
   Krishnamurthy A, 1996, MULTIMEDIA SYST, V4, P328, DOI 10.1007/s005300050034
   Li WP, 2001, IEEE T CIRC SYST VID, V11, P301, DOI 10.1109/76.911157
   McKeown N, 2008, ACM SIGCOMM COMP COM, V38, P69, DOI 10.1145/1355734.1355746
   PHAN KT, 2012, P ACM C CONEXT STUD, P00033
   She Q., 2007, P ANTS DEC, P1
   Song D, 2007, IEEE T CIRC SYST VID, V17, P1218, DOI 10.1109/TCSVT.2007.905531
   Striegel A, 2002, IEEE COMMUN MAG, V40, P82, DOI 10.1109/MCOM.2002.1007412
   Suoheng Li, 2012, 2012 International Conference on Computing, Networking and Communications (ICNC), P944, DOI 10.1109/ICCNC.2012.6167564
   Wang CY, 2013, IEEE INFOCOM SER, P540
   Yu Y., 2012, AISS, V4, P278
   Zhang WS, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-338
   Zhu ZQ, 2013, IEEE T MULTIMEDIA, V15, P758, DOI 10.1109/TMM.2013.2238908
   Zou JF, 2013, 2013 5TH IEEE INTERNATIONAL CONFERENCE ON BROADBAND NETWORK & MULTIMEDIA TECHNOLOGY (IC-BNMT), P124, DOI 10.1109/ICBNMT.2013.6823928
NR 25
TC 56
Z9 56
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1617
EP 1629
DI 10.1109/TMM.2015.2450014
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000020
DA 2024-07-18
ER

PT J
AU Zhang, LM
   Wang, M
   Nie, LQ
   Hong, L
   Rui, Y
   Tian, Q
AF Zhang, Luming
   Wang, Meng
   Nie, Liqiang
   Hong, Liang
   Rui, Yong
   Tian, Qi
TI Retargeting Semantically-Rich Photos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human perception; image tags; noisy; retargeting; semantically-rich;
   shrink
ID SALIENCY DETECTION; IMAGE; QUALITY; SCENE; GRADIENTS; MODEL
AB Semantically-rich photos contain a rich variety of semantic objects (e.g., pedestrians and bicycles). Retargeting these photos is a challenging task since each semantic object has fixed geometric characteristics. Shrinking these objects simultaneously during retargeting is prone to distortion. In this paper, we propose to retarget semantically-rich photos by detecting photo semantics from image tags, which are predicted by a multi-label SVM. The key technique is a generative model termed latent stability discovery (LSD). It can robustly localize various semantic objects in a photo by making use of the predicted noisy image tags. Based on LSD, a feature fusion algorithm is proposed to detect salient regions at both the low-level and high-level. These salient regions are linked into a path sequentially to simulate human visual perception. Finally, we learn the prior distribution of such paths from aesthetically pleasing training photos. The prior enforces the path of a retargeted photo to be maximally similar to those from the training photos. In the experiment, we collect 217 1600 x 1200 photos, each containing over seven salient objects. Comprehensive user studies demonstrate the competitiveness of our method.
C1 [Zhang, Luming; Wang, Meng] Hefei Univ Technol, Dept Elect Engn & Informat Syst, Hefei 230009, Peoples R China.
   [Nie, Liqiang] Natl Univ Singapore, Sch Comp, Singapore 119613, Singapore.
   [Hong, Liang] Wuhan Univ, Sch Informat Management, Wuhan 430072, Peoples R China.
   [Rui, Yong] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Hefei University of Technology; National University of Singapore; Wuhan
   University; Microsoft; Microsoft Research Asia; University of Texas
   System; University of Texas at San Antonio (UTSA)
RP Hong, L (corresponding author), Wuhan Univ, Sch Informat Management, Wuhan 430072, Peoples R China.
RI Lei, Ming/JAD-1050-2023; zhang, lu/GRO-2969-2022; Wang,
   Meng/ITR-8699-2023
FU National 973 Program of China [2014CB347600]; National Science
   Foundation of China (NSFC) [61272393, 61322201, 61432019, 61373077,
   61429201]; ARO [W911NF-12-1-0057]; Faculty Research Awards of the NEC
   Laboratories of America
FX This work was supported in part by the National 973 Program of China
   under Grant 2014CB347600, and by the National Science Foundation of
   China (NSFC) under Grant 61272393, Grant 61322201, Grant 61432019, Grant
   61373077, and Grant 61429201. The work of Q. Tian was supported in part
   by the ARO under Grant W911NF-12-1-0057 and in part by the Faculty
   Research Awards of the NEC Laboratories of America. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Sheng-Wei Chen. (Corresponding author: Liang Hong.)
CR Alexe B, 2010, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.2010.5540226
   [Anonymous], P SPIE
   [Anonymous], P SIGGRAPH AS
   [Anonymous], P ACM INT C MULT
   [Anonymous], P 17 ACM INT C MULT
   [Anonymous], 2010, ACM MULTIMEDIA
   [Anonymous], 2007, 2007 IEEE 11 INT C C, DOI DOI 10.1109/ICCV.2007.4409010
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Dhar S, 2011, PROC CVPR IEEE, P1657, DOI 10.1109/CVPR.2011.5995467
   Fang YM, 2012, IEEE T MULTIMEDIA, V14, P187, DOI 10.1109/TMM.2011.2169775
   Goferman S, 2010, PROC CVPR IEEE, P2376, DOI 10.1109/CVPR.2010.5539929
   Guo YW, 2009, IEEE T MULTIMEDIA, V11, P856, DOI 10.1109/TMM.2009.2021781
   Hou XD, 2012, IEEE T PATTERN ANAL, V34, P194, DOI 10.1109/TPAMI.2011.146
   Imamoglu N, 2013, IEEE T MULTIMEDIA, V15, P96, DOI 10.1109/TMM.2012.2225034
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jiang HZ, 2013, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2013.271
   Judd T., 2012, MIT CSAIL TR
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Ke Y., 2006, P IEEE COMP SOC C CO, V1, P419, DOI DOI 10.1109/CVPR.2006.303
   Kendall MG, 1940, BIOMETRIKA, V31, P324, DOI DOI 10.1093/BIOMET/31.3-4.324
   Lazebnik S., COMPUTER VISION PATT, V2, P2169
   Lee YJ, 2010, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2010.5540237
   Li J, 2013, IEEE T PATTERN ANAL, V35, P996, DOI 10.1109/TPAMI.2012.147
   Lin SS, 2013, IEEE T MULTIMEDIA, V15, P359, DOI 10.1109/TMM.2012.2228475
   Liu C, 2009, PROC CVPR IEEE, P1972, DOI 10.1109/CVPRW.2009.5206536
   Liu G., 2010, P INT C MACH LEARN, P663
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Luo W, 2011, IEEE I CONF COMP VIS, P2206, DOI 10.1109/ICCV.2011.6126498
   Marchesotti L, 2011, IEEE I CONF COMP VIS, P1784, DOI 10.1109/ICCV.2011.6126444
   Nishiyama M., 2009, P 17 ACM INT C MULTI, P669
   Nishiyama M, 2011, PROC CVPR IEEE, P33, DOI 10.1109/CVPR.2011.5995539
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Pritch Y, 2009, IEEE I CONF COMP VIS, P151, DOI 10.1109/ICCV.2009.5459159
   Ren TW, 2010, IEEE IMAGE PROC, P1569, DOI 10.1109/ICIP.2010.5653559
   Rubinstein M., 2009, P ACM SIGGRAPH
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Rubinstein M, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531329
   Rubinstein M, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360615
   Stricker M., P STOR RETR IM VID D, P381
   Sun J, 2013, INT J COMPUT VISION, V104, P135, DOI 10.1007/s11263-013-0618-z
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Vig E, 2014, PROC CVPR IEEE, P2798, DOI 10.1109/CVPR.2014.358
   Wang XC, 2011, IEEE T IMAGE PROCESS, V20, P2627, DOI 10.1109/TIP.2011.2114354
   Wang Y. -S., 2010, ACM TOG, V29
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang JM, 2012, PROC CVPR IEEE, P2296, DOI 10.1109/CVPR.2012.6247940
   Zhang JM, 2013, IEEE I CONF COMP VIS, P153, DOI 10.1109/ICCV.2013.26
   Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32
   Zhang LM, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P237, DOI 10.1145/2647868.2654903
   Zhang LM, 2014, IEEE T IMAGE PROCESS, V23, P2235, DOI 10.1109/TIP.2014.2311658
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P94, DOI 10.1109/TMM.2013.2286817
   Zhang LM, 2013, PROC CVPR IEEE, P1908, DOI 10.1109/CVPR.2013.249
   Zhang YF, 2008, COMPUT GRAPH FORUM, V27, P1797, DOI 10.1111/j.1467-8659.2008.01325.x
   Zhao Q, 2012, J VISION, V12, DOI 10.1167/12.6.22
NR 65
TC 25
Z9 25
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEP
PY 2015
VL 17
IS 9
SI SI
BP 1538
EP 1549
DI 10.1109/TMM.2015.2451954
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CP0RN
UT WOS:000359583000014
DA 2024-07-18
ER

PT J
AU Aulí-Llinàs, F
AF Auli-Llinas, Francesc
TI Context-Adaptive Binary Arithmetic Coding With Fixed-Length Codewords
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Context-adaptive binary arithmetic coding; fixed-length arithmetic codes
ID COMPRESSION; CODER
AB Context-adaptive binary arithmetic coding is a widespread technique in the field of image and video coding. Most state-of-the-art arithmetic coders produce a (long) codeword of a priori unknown length. Its generation requires a renormalization procedure to permit progressive processing. This paper introduces two arithmetic coders that produce multiple codewords of fixed length. Contrary to the traditional approach, the generation of fixed-length codewords does not require renormalization since the whole interval arithmetic is stored in the coder's internal registers. The proposed coders employ a new context-adaptive mechanism based on variable-size sliding window that estimates with high precision the probability of the symbols coded. Their integration in coding systems is straightforward as demonstrated within the framework of JPEG2000. Experimental tests indicate that the proposed coders are computationally simpler than the MQ coder of JPEG2000 and the M coder of HEVC while achieving superior coding efficiency.
C1 Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
C3 Autonomous University of Barcelona
RP Aulí-Llinàs, F (corresponding author), Univ Autonoma Barcelona, Dept Informat & Commun Engn, Bellaterra 08193, Spain.
EM fauli@deic.uab.cat
RI Auli-Llinas, Francesc/K-4395-2013
OI Auli-Llinas, Francesc/0000-0002-3208-9957
FU Spanish Government (MINECO) [RYC-2010-05671]; FEDER
   [TIN2012-38102-C03-03]; Catalan Government [2014SGR-691]
FX This work was supported in part by the Spanish Government (MINECO) under
   Grant RYC-2010-05671, by FEDER under Grant TIN2012-38102-C03-03, and by
   the Catalan Government under Grant 2014SGR-691. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Xiao-Ping Zhang.
CR Auli-Llinas F., 2014, CONTEXT ADAPTIVE BIN
   Aulí-Llinàs F, 2013, INFORM SCIENCES, V239, P266, DOI 10.1016/j.ins.2013.03.027
   Aulí-Llinàs F, 2014, IEEE T MULTIMEDIA, V16, P960, DOI 10.1109/TMM.2014.2307553
   Aulí-Llinàs F, 2012, IEEE T IMAGE PROCESS, V21, P1920, DOI 10.1109/TIP.2011.2176953
   Aulí-Llinàs F, 2011, IEEE T IMAGE PROCESS, V20, P2153, DOI 10.1109/TIP.2011.2114892
   Belyaev E, 2013, IEEE J-STSP, V7, P1053, DOI 10.1109/JSTSP.2013.2269272
   BONCELET CG, 1993, IEEE T INFORM THEORY, V39, P1546, DOI 10.1109/18.259639
   Chan DY, 2001, IEEE T CIRC SYST VID, V11, P581, DOI 10.1109/76.920188
   Chevion D., 1991, P DAT COMPR C, P43
   Intel, 2014, 248966030 INT
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Reavy MD, 2001, IEEE T IMAGE PROCESS, V10, P669, DOI 10.1109/83.918560
   Slattery MJ, 1998, IBM J RES DEV, V42, P767, DOI 10.1147/rd.426.0767
   TEUHOLA J, 1994, IEEE T INFORM THEORY, V40, P219, DOI 10.1109/18.272486
   Xie Y, 2003, IEEE DATA COMPR CONF, P382
NR 15
TC 16
Z9 18
U1 2
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1385
EP 1390
DI 10.1109/TMM.2015.2444797
PG 6
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000022
OA Green Published
DA 2024-07-18
ER

PT J
AU Liu, S
   Liang, XD
   Liu, LQ
   Lu, K
   Lin, L
   Cao, XC
   Yan, SC
AF Liu, Si
   Liang, Xiaodan
   Liu, Luoqi
   Lu, Ke
   Lin, Liang
   Cao, Xiaochun
   Yan, Shuicheng
TI Fashion Parsing With Video Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Information retrieval; professional communication
ID POSE ESTIMATION; SEGMENTATION; TRACKING
AB In this paper, we propose a novel semi-supervised learning strategy to address human parsing. Existing human parsing datasets are relatively small due to the required tedious human labeling. We present a general, affordable and scalable solution, which harnesses the rich contexts in those easily available web videos to boost any existing human parser. First, we crawl a large number of unlabeled videos from the web. Then for each video, the cross-frame contexts are utilized for human pose co-estimation, and then video co-parsing to obtain satisfactory human parsing results for all frames. More specifically, SIFT flow and super-pixel matching are used to build correspondences across different frames, and these correspondences then contextualize the pose estimation and human parsing in individual frames. Finally these parsed video frames are used as the reference corpus for the non-parametric human parsing component of the whole solution. To further improve the accuracy of video co-parsing, we propose an active learning method to incorporate human guidance, where the labelers are required to assess the accuracies of the pose estimation results of certain selected video frames. Then we take reliable frames as the seed frames to guide the video pose co-estimation. Our human parsing framework can then easily incorporate the human feedback to train a better fashion parser. Extensive experiments on two benchmark fashion datasets as well as a newly collected challenging Fashion Icon dataset well demonstrate the encouraging performance gain from our general pipeline for human parsing.
C1 [Liu, Si; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
   [Liu, Si] Natl Univ Singapore, Learning & Vis Grp, Singapore 100093, Singapore.
   [Liang, Xiaodan; Lin, Liang] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
   [Liu, Luoqi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Lu, Ke] Chinese Acad Sci, Grad Univ, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   National University of Singapore; Sun Yat Sen University; National
   University of Singapore; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Liu, S (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100093, Peoples R China.
RI l, j/HNC-5728-2023; zhang, cl/JDW-6549-2023; Lin, L/HKO-8213-2023; L,
   J/JEF-9564-2023; LU, LU/JEZ-4760-2023; Yan, Shuicheng/HCI-1431-2022; l,
   j/JVZ-8480-2024
FU National Natural Science Foundation of China [61422213, 61332012,
   61328205]; 100 Talents Programme of The Chinese Academy of Sciences;
   Hi-Tech Research and Development Program of China [2013AA013801];
   Guangdong Natural Science Foundation [S2013050014548]; Program of
   Guangzhou Zhujiang Star of Science and Technology [2013J2200067]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61422213, Grant 61332012, and Grant
   61328205, the 100 Talents Programme of The Chinese Academy of Sciences,
   the Hi-Tech Research and Development Program of China under Grant
   2013AA013801, the Guangdong Natural Science Foundation under Grant
   S2013050014548, and the Program of Guangzhou Zhujiang Star of Science
   and Technology under Grant 2013J2200067. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. K. Selcuk Candan.
CR [Anonymous], 2009, IEEE I CONF COMP VIS, DOI 10.1109/ICCV.2009.5459175
   [Anonymous], 2014, P INT C LEARN REPR A
   [Anonymous], 2008, IEEE C COMPUTER VISI
   [Anonymous], 2006, P CVPR, DOI 10.1109/CVPR.2006.81
   Badrinarayanan V, 2010, PROC CVPR IEEE, P3265, DOI 10.1109/CVPR.2010.5540054
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32
   Cherian A, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.302
   Dong J, 2013, IEEE I CONF COMP VIS, P3408, DOI 10.1109/ICCV.2013.423
   Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Ferrari V, 2008, PROC CVPR IEEE, DOI 10.1109/CVPR.2008.4587468
   Girshick R. B., 2011, Advances in Neural Information Processing Systems, P442
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guzman-Rivera A, 2014, JMLR WORKSH CONF PRO, V33, P284
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   Joulin A, 2010, PROC CVPR IEEE, P1943, DOI 10.1109/CVPR.2010.5539868
   Kohli P, 2008, INT J COMPUT VISION, V79, P285, DOI 10.1007/s11263-007-0120-6
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147
   Liu HR, 2012, INT J COMPUT VISION, V98, P65, DOI 10.1007/s11263-011-0496-1
   Liu S., 2011, Proc. Conf. Artif. Intell. (AAAI), San Francisco, P392
   Liu S, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P467
   Liu S, 2014, IEEE T MULTIMEDIA, V16, P253, DOI 10.1109/TMM.2013.2285526
   Liu S, 2012, PROC CVPR IEEE, P3330, DOI 10.1109/CVPR.2012.6248071
   Liu X., 2014, P COMP VIS PATT REC, P512
   Park D, 2011, IEEE I CONF COMP VIS, P2627, DOI 10.1109/ICCV.2011.6126552
   Pinheiro PO, 2014, PR MACH LEARN RES, V32
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Sapp B, 2011, PROC CVPR IEEE, P1281, DOI 10.1109/CVPR.2011.5995607
   Shrivastava A, 2012, LECT NOTES COMPUT SC, V7574, P369, DOI 10.1007/978-3-642-33712-3_27
   Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214
   Tran D, 2010, LECT NOTES COMPUT SC, V6314, P227, DOI 10.1007/978-3-642-15561-1_17
   Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36
   Wang N, 2011, IEEE I CONF COMP VIS, P1535, DOI 10.1109/ICCV.2011.6126412
   Yamaguchi K, 2013, IEEE I CONF COMP VIS, P3519, DOI 10.1109/ICCV.2013.437
   Yamaguchi K, 2012, PROC CVPR IEEE, P3570, DOI 10.1109/CVPR.2012.6248101
   Yang W, 2014, PROC CVPR IEEE, P3182, DOI 10.1109/CVPR.2014.407
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
   Yihang Bo, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2265, DOI 10.1109/CVPR.2011.5995609
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang TZ, 2012, LECT NOTES COMPUT SC, V7577, P470, DOI 10.1007/978-3-642-33783-3_34
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
NR 43
TC 41
Z9 49
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1347
EP 1358
DI 10.1109/TMM.2015.2443559
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000019
DA 2024-07-18
ER

PT J
AU Shen, IC
   Cheng, WH
AF Shen, I-Chao
   Cheng, Wen-Huang
TI Gestalt Rule Feature Points
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross domain image matching; Gestalt rules; graph-based ranking; local
   feature detector
ID OBJECT; SEGMENTATION; RECOGNITION; CONTOURS; SCENE
AB As the large online repositories of image and video data has emerged and continued to grow in number, the visual variations in such repositories has also increased dramatically. For example, the visual scene of a photograph can be changed into different colors by image editing tools or depicted by multiple representations, such as a painting and a hand-drawn sketch. The large visual variations tend to cause ambiguities for the existing computer vision algorithms to recognize the visual analogies of these images and often limit the potential of related applications. In this paper, therefore, we propose a new approach for detecting reliable visual features from images, with a particular focus on improving the repeatability of the local features in those images containing the same semantic contents (e. g., a landmark) but in different visual styles (e. g., a photo and a painting). We proposed a novel method for establishing visual correspondences between images based on the Gestalt theory, a psychological study of how human visions organize the visual perception. Experiments demonstrated the outperformance of our approach over the state-of-the-art local features in various computer vision tasks, such as cross domain image matching and retrieval.
C1 [Shen, I-Chao] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
   [Cheng, Wen-Huang] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
C3 University of British Columbia; Academia Sinica - Taiwan
RP Shen, IC (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
EM ichaos@cs.ubc.ca; whcheng@citi.sinica.edu.tw
RI Cheng, Wen-Huang/AAK-2774-2020; Shen, I-Chao/AHA-3605-2022
OI Shen, I-Chao/0000-0003-4201-3793
CR Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293
   [Anonymous], 2007, P 2007 IEEE C COMP V
   [Anonymous], Principles of Psychology
   [Anonymous], 2008, P 2008 IEEE C COMPUT, DOI DOI 10.1109/CVPR.2008.4587673
   Bae SM, 2006, ACM T GRAPHIC, V25, P637, DOI 10.1145/1141911.1141935
   Berg AC, 2001, PROC CVPR IEEE, P607
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Brown M, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1218
   Cheng WH, 2008, IEEE T CIRC SYST VID, V18, P1639, DOI 10.1109/TCSVT.2008.2005608
   Cheng WH, 2007, IEEE T CIRC SYST VID, V17, P43, DOI 10.1109/TCSVT.2006.885717
   Dalal N., 2005, PROC IEEE COMPUT SOC, V1, P886
   DeCarlo D, 2002, ACM T GRAPHIC, V21, P769, DOI 10.1145/566570.566650
   Eisemann E, 2004, ACM T GRAPHIC, V23, P673, DOI 10.1145/1015706.1015778
   Harris C., 1988, P 4 ALV VIS C, V15, P10
   Hauagge DC, 2012, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2012.6247677
   He JR, 2006, IEEE T IMAGE PROCESS, V15, P3170, DOI 10.1109/TIP.2006.877491
   Heinly J, 2012, LECT NOTES COMPUT SC, V7573, P759, DOI 10.1007/978-3-642-33709-3_54
   Ji RR, 2013, IEEE T MULTIMEDIA, V15, P153, DOI 10.1109/TMM.2012.2225035
   Jung C.-W., 2013, P SIGGRAPH AS 2013
   Kim S, 2008, PATTERN RECOGN, V41, P726, DOI 10.1016/j.patcog.2007.05.014
   KIRKPATRICK S, 1984, J STAT PHYS, V34, P975, DOI 10.1007/BF01009452
   Kuo YH, 2012, IEEE T MULTIMEDIA, V14, P1079, DOI 10.1109/TMM.2012.2190386
   Lazebnik S., 2009, Object Categorization: Computer and Human Vision Perspectives, P401
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935
   Liu C., 2011, IEEE T PATTERN ANAL, V33, P28
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mahamud S, 2003, IEEE T PATTERN ANAL, V25, P433, DOI 10.1109/TPAMI.2003.1190570
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Ming YS, 2012, PROC CVPR IEEE, P829, DOI 10.1109/CVPR.2012.6247755
   Nan LL, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024219
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977
   Shrivastava A, 2011, ACM T GRAPHIC, V30, DOI 10.1145/2024156.2024188
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Todd J. T., 2004, TRENDS COGN SCI, V8
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Wang ZH, 2011, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2011.6126294
   Wertheimer M, 1923, PSYCHOL FORSCH, V4, P301, DOI 10.1007/BF00410640
   Williams LR, 1999, INT J COMPUT VISION, V34, P81, DOI 10.1023/A:1008187804026
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yoon KJ, 2006, IEEE T PATTERN ANAL, V28, P650, DOI 10.1109/TPAMI.2006.70
   Zhou DY, 2004, ADV NEUR IN, V16, P169
   Zhou DY, 2004, ADV NEUR IN, V16, P321
   Zhu LF, 2012, ENG GEOL, V127, P1, DOI 10.1016/j.enggeo.2011.12.001
   Zitnick CL, 2011, IEEE I CONF COMP VIS, P359, DOI 10.1109/ICCV.2011.6126263
NR 48
TC 16
Z9 16
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 526
EP 537
DI 10.1109/TMM.2015.2405350
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300006
DA 2024-07-18
ER

PT J
AU Jiménez-Moreno, A
   Martínez-Enríquez, E
   Kumar, V
   Díaz-de-María, F
AF Jimenez-Moreno, Amaya
   Martinez-Enriquez, Eduardo
   Kumar, Vinay
   Diaz-de-Maria, Fernando
TI Standard-Compliant Low-Pass Temporal Filter to Reduce the Perceived
   Flicker Artifact
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Flicker artifact; flicker reduction; H.264/AVC; low-pass temporal
   filtering; motion-guided temporal filtering; on-the-fly filter strength
   control; standard compliant
ID VIDEO; FRAMES
AB Flicker is a common video-compression-related temporal artifact. It occurs when co-located regions of consecutive frames are not encoded in a consistent manner, especially when Intra frames are periodically inserted at low and medium bit rates. In this paper we propose a flicker reduction method which aims to make the luminance changes between pixels in the same area of consecutive frames less noticeable. To this end, a temporal low-pass filtering is proposed that smooths these luminance changes on a block-by-block basis. The proposed method has some advantages compared to another state-of-the-art methods. It has been designed to be compliant with conventional video coding standards, i.e., to generate a bitstream that is decodable by any standard decoder implementation. The filter strength is estimated on-the-fly to limit the PSNR loss and thus the appearance of a noticeable blurring effect. The proposed method has been implemented on the H. 264/AVC reference software and thoroughly assessed in comparison to a couple of state-of-the-art methods. The flicker reduction achieved by the proposed method (calculated using an objective measurement) is notably higher than that of compared methods: 18.78% versus 5.32% and 31.96% versus 8.34%, in exchange of some slight losses in terms of coding efficiency. In terms of subjective quality, the proposed method is perceived more than two times better than the compared methods.
C1 [Jimenez-Moreno, Amaya; Martinez-Enriquez, Eduardo; Diaz-de-Maria, Fernando] Carlos III Univ, Dept Signal Theory & Commun, Madrid 28911, Spain.
   [Kumar, Vinay] Thapar Univ, Dept Elect & Commun Engn, Patiala 147001, Punjab, India.
C3 Thapar Institute of Engineering & Technology
RP Jiménez-Moreno, A (corresponding author), Carlos III Univ, Dept Signal Theory & Commun, Madrid 28911, Spain.
EM ajimenez@tsc.uc3m.es; emenriquez@tsc.uc3m.es; fdiaz@tsc.uc3m.es
RI Martinez, Eduardo/ISS-3584-2023; de María, Fernando Díaz/E-8048-2011;
   Kumar, Vinay/AAB-8186-2019; Jimenez-Moreno, Amaya/AAA-7450-2021;
   Martinez-Enriquez, Eduardo/L-8332-2014
OI de María, Fernando Díaz/0000-0002-6437-914X; Kumar,
   Vinay/0000-0001-9086-4782; Martinez-Enriquez,
   Eduardo/0000-0001-7097-8846
FU Spanish Ministry of Science and Innovation [TEC2011-26807]
FX This work was supported in part by the Spanish Ministry of Science and
   Innovation under National Grant TEC2011-26807. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Feng Wu.
CR Becker A, 2004, IEEE DATA COMPR CONF, P252
   Chono K, 2006, IEEE IMAGE PROC, P1713, DOI 10.1109/ICIP.2006.312711
   Chun SS, 2006, IEEE IMAGE PROC, P2025, DOI 10.1109/ICIP.2006.312844
   Chun SS, 2006, IEEE T CONSUM ELECTR, V52, P1303, DOI 10.1109/TCE.2006.273149
   Hong SH, 2003, IEEE T BROADCAST, V49, P1, DOI 10.1109/TBC.2003.808912
   ISO/IEC MPEG and ITU-T VCEG Joint Video Team, 2002, ITUTSG16Q6
   Itani Y., 2005, IEICE TECH REP, V105
   Jiménez-Moreno A, 2013, INT CONF ACOUST SPEE, P1729, DOI 10.1109/ICASSP.2013.6637948
   Kanumuri S., 2008, P SPIE, V7073
   Kuge T, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P217, DOI 10.1109/ICIP.2002.1038944
   Kuszpet Y., 2007, P 16 PICT COD S PCS
   Leontaris A., 2006, P 2006 IEEE INT C AC, V2, pII
   Martínez-Enríquez E, 2010, IEEE T CONSUM ELECTR, V56, P826, DOI 10.1109/TCE.2010.5506008
   Matsumura A, 2005, PICTURE QUALITY APPL, P569
   Ortega A, 1998, IEEE SIGNAL PROC MAG, V15, P23, DOI 10.1109/79.733495
   Ren J, 2013, INT CONF ACOUST SPEE, P1631, DOI 10.1109/ICASSP.2013.6637928
   Shujuan Qiao, 2012, 2012 International Conference on Computer Science and Service System (CSSS), P1551, DOI 10.1109/CSSS.2012.388
   Vo DT, 2009, IEEE T IMAGE PROCESS, V18, P1166, DOI 10.1109/TIP.2009.2017341
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wu H., 2005, DIGITAL VIDEO IMAGE, V28
   Yang H, 2008, IEEE IMAGE PROC, P2868, DOI 10.1109/ICIP.2008.4712393
   Yang JX, 2010, IEEE T CIRC SYST VID, V20, P458, DOI 10.1109/TCSVT.2009.2035850
   Yang JY, 2006, PROC SPIE, V6391, DOI 10.1117/12.689901
NR 23
TC 9
Z9 10
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 1863
EP 1873
DI 10.1109/TMM.2014.2347257
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300006
OA Green Published
DA 2024-07-18
ER

PT J
AU Zhang, YD
   Zhang, L
   Tian, Q
AF Zhang, Yongdong
   Zhang, Lei
   Tian, Qi
TI A Prior-Free Weighting Scheme for Binary Code Ranking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Binary hashing; data-adaptive and query-sensitive weighting; similarity
   search; weighted Hamming distance
ID NEAREST-NEIGHBOR; SCALE
AB Fast similarity search has been a research focus in recent years. Binary hashing, which embeds high-dimensional data points into Hamming space, is a promising way to accelerate similarity search, since its search process can be performed in real-time by using Hamming distance as similarity metric. However, as Hamming distance is discrete and bounded by code length, its resolution is limited. In practice, there are often many results sharing the same Hamming distance to a query, which poses a critical issue for problems where ranking is important. This paper proposes a weighted Hamming distance ranking algorithm (WhRank) to give a better ranking of results with equal Hamming distances to a query. By assigning different bit-level weights to different bits, WhRank is able to distinguish between the relative importance of different bits, and to rank the results at a finer-grained hash code level rather than the original integer Hamming distance level. We show that an effective weight is not only data-adaptive but also query-sensitive, and give a simple yet effective prior-free weight learning algorithm. Evaluations on three large-scale image datasets containing up to one million points demonstrate the efficacy of the proposed algorithm.
C1 [Zhang, Yongdong; Zhang, Lei] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   University of Texas System; University of Texas at San Antonio (UTSA)
RP Zhang, YD (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing, Peoples R China.
EM zhyd@ict.ac.cn; zhanglei09@ict.ac.cn; qitian@cs.utsa.edu
OI Zhang, Lei/0000-0002-2839-8693
FU National High Technology Research and Development Program of China
   [2014AA015202]; National Nature Science Foundation of China [61303151,
   61271428]; National Key Technology Research and Development Program of
   China [2012BAH39B02]; National Science Foundation of China (NSFC)
   [61128007]; ARO [W911NF-12-1-0057]; NEC Laboratories of America; UTSA
   START-R Research Award
FX This work was supported by the National High Technology Research and
   Development Program of China (2014AA015202), the National Nature Science
   Foundation of China (61303151, 61271428), the National Key Technology
   Research and Development Program of China (2012BAH39B02), and the
   National Science Foundation of China (NSFC) 61128007. The work of Q.
   Tian was supported in part by ARO grant W911NF-12-1-0057, Faculty
   Research Awards by NEC Laboratories of America, and 2012 UTSA START-R
   Research Award, respectively. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Alan Hanjalic.
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   [Anonymous], INT 64 IA 32 ARCH SO
   [Anonymous], 2012, P 20 ACM MULTIMEDIA
   [Anonymous], P CVPR
   [Anonymous], 2012, Book A probabilistic model for multimodal hash function learning, DOI DOI 10.1145/2339530.2339678
   [Anonymous], 2009, NIPS
   [Anonymous], 2009, NEURIPS
   [Anonymous], P ICML
   [Anonymous], P INT C MULT RETR
   BENTLEY JL, 1990, PROCEEDINGS OF THE SIXTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY, P187, DOI 10.1145/98524.98564
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Gu XG, 2013, SIGNAL PROCESS, V93, P2244, DOI 10.1016/j.sigpro.2012.07.014
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Jégou H, 2010, INT J COMPUT VISION, V87, P316, DOI 10.1007/s11263-009-0285-2
   Ji J., 2012, 25 INT C NEUR INF PR, P108
   Jiang YG, 2013, IEEE T MULTIMEDIA, V15, P442, DOI 10.1109/TMM.2012.2231061
   Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219
   Li P, 2013, IEEE T MULTIMEDIA, V15, P141, DOI 10.1109/TMM.2012.2199970
   Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912
   Liu Wei, 2011, Reports in Parasitology, V1, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lv Q, 2007, P 33 INT C VER LARG, P950
   Norouzi M.E., 2011, ICML
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006
   Silpa-Anan C., 2008, P CVPR, P1
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Stein Benno, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P527, DOI 10.1145/1277741.1277832
   Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang Jun., 2010, ICML, P1127
   Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Weiss Y, 2012, LECT NOTES COMPUT SC, V7576, P340, DOI 10.1007/978-3-642-33715-4_25
   Zhang L, 2013, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2013.208
   Zhang Lei., 2013, ACM Multimedia, P123, DOI DOI 10.1145/2502081.2502091
   Zhang X, 2012, PROC CVPR IEEE, P2058, DOI 10.1109/CVPR.2012.6247910
   Zhou W., 2010, P 18 ACM INT C MULTI, P511
NR 39
TC 14
Z9 14
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2014
VL 16
IS 4
BP 1127
EP 1139
DI 10.1109/TMM.2014.2306392
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AJ8LN
UT WOS:000337955800019
DA 2024-07-18
ER

PT J
AU González-Díaz, I
   Baz-Hormigos, CE
   Díaz-de-María, F
AF Gonzalez-Diaz, Ivan
   Baz-Hormigos, Carlos E.
   Diaz-de-Maria, Fernando
TI A Generative Model for Concurrent Image Retrieval and ROI Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image retrieval; Object segmentation; Object recognition; Image
   Databases; Computer Vision
ID SCALE
AB This paper proposes a probabilistic generative model that concurrently tackles the problems of image retrieval and region-of-interest (ROI) segmentation. Specifically, the proposed model takes into account several properties of the matching process between two objects in different images, namely: objects undergoing a geometric transformation, typical spatial location of the region of interest, and visual similarity. In this manner, our approach improves the reliability of detected true matches between any pair of images. Furthermore, by taking advantage of the links to the ROI provided by the true matches, the proposed method is able to perform a suitable ROI segmentation. Finally, the proposed method is able to work when there is more than one ROI in the query image. Our experiments on two challenging image retrieval datasets proved that our approach clearly outperforms the most prevalent approach for geometrically constrained matching and compares favorably to most of the state-of-the-art methods. Furthermore, the proposed technique concurrently provided very good segmentations of the ROI. Furthermore, the capability of the proposed method to take into account several objects-of-interest was also tested on three experiments: two of them concerning image segmentation and object detection in multi-object image retrieval tasks, and another concerning multiview image retrieval. These experiments proved the ability of our approach to handle scenarios in which more than one object of interest is present in the query.
C1 [Gonzalez-Diaz, Ivan; Baz-Hormigos, Carlos E.; Diaz-de-Maria, Fernando] Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid 28911, Spain.
C3 Universidad Carlos III de Madrid
RP González-Díaz, I (corresponding author), Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid 28911, Spain.
EM igonzalez@tsc.uc3m.es; cebaz@tsc.uc3m.es; fdiaz@tsc.uc3m.es
RI de María, Fernando Díaz/E-8048-2011; Díaz, Iván González/L-5103-2014
OI de María, Fernando Díaz/0000-0002-6437-914X; Díaz, Iván
   González/0000-0003-4644-8479
FU project AFICUS; Spanish Ministry of Industry, Trade and Tourism;
   European Fund for Regional Development [TSI-020110-2009-103]; National
   Grant of the Spanish Ministry of Science and Innovation [TEC2011-26807]
FX This work was supported in by the project AFICUS, co-funded by the
   Spanish Ministry of Industry, Trade and Tourism, and the European Fund
   for Regional Development, with Ref.: TSI-020110-2009-103, and the
   National Grant TEC2011-26807 of the Spanish Ministry of Science and
   Innovation. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Selcuk Candan.
CR [Anonymous], P EUR C COMP VIS
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Chum O, 2009, PROC CVPR IEEE, P17, DOI 10.1109/CVPRW.2009.5206531
   Chum Ondrej, 2007, P 11 INT C COMP VIS
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Douze M, 2009, P ACM INT C IM VID R, P1, DOI DOI 10.1145/1646396.1646421
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Ferrari V, 2004, LECT NOTES COMPUT SC, V3021, P40
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Goldberger J, 2006, IEEE T PATTERN ANAL, V28, P463, DOI 10.1109/TPAMI.2006.47
   Gonzalez-Diaz I., 2012, P 7 INT WORKSH CONT
   Horster E., 2007, CIVR 07 P 2007 ACM I, P17, DOI DOI 10.1145/1282280.1282283
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Matas J., 2002, Electronic Proceedings of the 13th British Machine Vision Conference, P384
   Mikolajczyk K, 2004, INT J COMPUT VISION, V60, P63, DOI 10.1023/B:VISI.0000027790.02288.f2
   Muja M., 2009, P INT C COMP VIS THE
   Permuter H, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL III, PROCEEDINGS, P569
   PERRONNIN F, 2010, PROC CVPR IEEE, P3384, DOI DOI 10.1109/CVPR.2010.5540009
   Philbin J., OXFORD BUILDING DATA
   Philbin J., 2008, P CVPR, P1
   Philbin J, 2011, INT J COMPUT VISION, V95, P138, DOI 10.1007/s11263-010-0363-5
   Shen XH, 2012, PROC CVPR IEEE, P3013, DOI 10.1109/CVPR.2012.6248031
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Tong W., 2011, ICMR 11, P28, DOI DOI 10.1145/1991996.1992024
   Tong W., 2011, P 1 ACM INT C MULT R
   Turcot Panu, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P2109, DOI 10.1109/ICCVW.2009.5457541
   Wang X., 2007, ADV NEURAL INF PROCE, V20
   Williams C.K.I., PASCAL VISUAL OBJECT
   Wu Z, 2009, PROC CVPR IEEE, P25, DOI 10.1109/CVPRW.2009.5206566
   Yang LJ, 2012, IEEE T MULTIMEDIA, V14, P871, DOI 10.1109/TMM.2012.2187778
   Zhang YM, 2011, PROC CVPR IEEE, P809, DOI 10.1109/CVPR.2011.5995528
NR 33
TC 9
Z9 9
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 169
EP 183
DI 10.1109/TMM.2013.2286083
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100015
OA Green Accepted
DA 2024-07-18
ER

PT J
AU He, J
   Xue, Z
   Wu, D
   Wu, DO
   Wen, YG
AF He, Jian
   Xue, Zheng
   Wu, Di
   Wu, Dapeng Oliver
   Wen, Yonggang
TI CBM: Online Strategies on Cost-Aware Buffer Management for Mobile Video
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile video streaming; buffer management; energy consumption; bandwidth
   cost; Lyapunov optimization
AB Mobile video traffic, owing to the rapid adoption of smartphones and tablets, has been growing exponentially in recent years and started to dominate the mobile Internet. In reality, mobile video applications commonly adopt buffering techniques to handle bandwidth fluctuation and minimize the impact of stochastic wireless channels on user experiences. However, recent measurement work reveals that mobile users tend to abort more frequently than PC users during viewing videos. Such a high abortion rate results in a significant wastage of buffered video data, which is directly translated into monetary and energy cost for mobile users. In this paper, we propose an intelligent buffer management strategy called CBM (Cost-aware Buffer Management), for mobile video streaming applications. Our purpose is to minimize cost induced by un-consumed video data while respecting certain user experience requirements. To this objective, we formulate the problem into a constrained stochastic optimization problem, and apply the Lyapunov optimization theory to derive the corresponding online strategy for cost minimization. Different from conventional heuristic-based strategies, our proposed CBM strategy can provide provably performance guarantee with explicit bounds. We also conduct extensive simulations to validate the effectiveness of our proposed strategy and our experimental results show that CBM achieves significant gains over existing schemes.
C1 [He, Jian; Xue, Zheng; Wu, Di] Sun Yat Sen Univ, Dept Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
   [Wu, Dapeng Oliver] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
   [Wen, Yonggang] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 Sun Yat Sen University; State University System of Florida; University
   of Florida; Nanyang Technological University
RP Wu, D (corresponding author), Sun Yat Sen Univ, Dept Comp Sci, Guangzhou 510275, Guangdong, Peoples R China.
EM hejian9@mail2.sysu.edu.cn; xuezh@mail2.sysu.edu.cn;
   wudi27@mail.sysu.edu.cn; wu@ece.ufl.edu; ygwen@ntu.edu.sg
RI Wen, Yonggang/B-8848-2011; Wu, Di/HNP-3772-2023; Wen,
   Yonggang/P-9406-2017; wu, di/IYS-9217-2023
OI Wen, Yonggang/0000-0002-2751-5114; Wu, Dapeng/0000-0003-1755-0183
FU NSFC [61003242, 61272397]; Fundamental Research Funds for the Central
   Universities [12LGPY53]; Guangdong Natural Science Funds for
   Distinguished Young Scholar [S20120011187]; Program for New Century
   Excellent Talents in University [NCET-11-0542]; US National Science
   Foundation [ECCS-1002214, CNS-1116970]; Joint Research Fund for Overseas
   Chinese Young Scholars [61228101]; NTU SUG; MOE Tier 1 [RG 31/11];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1116970] Funding Source: National Science Foundation
FX This work was supported in part by the NSFC under Grant 61003242, Grant
   61272397, the Fundamental Research Funds for the Central Universities
   under Grant 12LGPY53, Guangdong Natural Science Funds for Distinguished
   Young Scholar under Grant S20120011187, the Program for New Century
   Excellent Talents in University under Grant NCET-11-0542, the US
   National Science Foundation under grant ECCS-1002214, CNS-1116970, the
   Joint Research Fund for Overseas Chinese Young Scholars under Grant
   61228101, NTU SUG and MOE Tier 1 (RG 31/11). The corresponding author is
   Di Wu. The associate editor coordinating the review of this manuscript
   and approving it for publication was Prof. Chia-Wen Lin.
CR Adams J., 2007, P IEEE INT C COMM IC
   [Anonymous], 2011, P ACM C MULT SYST, DOI DOI 10.1145/1943552.1943575
   [Anonymous], 2011, CISC VIS NETW IND FO
   Balasubramanian N., 2009, P ACM SIGCOMM C INT
   Ding N., P INF
   Finamore A., 2011, P ACM SIGCOMM C INT
   Ha S., 2012, P ACM SIGCOMM
   He J., 2013, CBM ONLINE STRATEGIE
   Hefeeda M., 2010, IEEE ACM T NETW, V18
   Hoque M. A., P ACM MOBICOM
   Ji G., 2009, P IEEE INT C COMM IC
   Li Y., 2011, P ACM SIGCOMM C INT
   Liu JY, 2008, MOBISYS'08: PROCEEDINGS OF THE SIXTH INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P146
   Liu Y., P ACM MULT
   Liu Y., P INFOCOM
   Liu Y., P ACM NOSSDAV
   Mastronarde N., P ICASSP
   Neely M. J., 2010, STOCHASTIC NETWORK O
   Pathak A., P EUROSYS
   Salodkar N, 2008, IEEE J SEL AREA COMM, V26, P732, DOI 10.1109/JSAC.2008.080514
   Sen S., 2012, PRICING DATA LOOK PR
   Su YF, 2009, IEEE T MULTIMEDIA, V11, P1331, DOI 10.1109/TMM.2009.2030543
   Tabrizi FM, 2013, IEEE T MOBILE COMPUT, V12, P995, DOI 10.1109/TMC.2012.56
   Tse D., 2005, Fundementals of Wireless Communications
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Zhu TX, 2012, PROCEEDINGS OF THE 4TH (2012) INTERNATIONAL CONFERENCE ON FINANCIAL RISK AND CORPORATE FINANCE MANAGEMENT, VOLS I AND II, P279
NR 26
TC 32
Z9 37
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2014
VL 16
IS 1
BP 242
EP 252
DI 10.1109/TMM.2013.2284894
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 279GJ
UT WOS:000328948100020
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Aizawa, K
   Maruyama, Y
   Li, H
   Morikawa, C
AF Aizawa, Kiyoharu
   Maruyama, Yuto
   Li, He
   Morikawa, Chamin
TI Food Balance Estimation by Using Personal Dietary Tendencies in a
   Multimedia Food Log
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bayesian estimation; food log; food record; image processing; lifelog;
   multimedia
ID RECOGNITION
AB We have investigated the "FoodLog" multimedia food-recording tool, whereby users upload photographs of their meals and a food diary is constructed using image-processing functions such as food-image detection and food-balance estimation. In this paper, following a brief introduction to FoodLog, we propose a Bayesian framework that makes use of personal dietary tendencies to improve both food-image detection and food-balance estimation. The Bayesian framework facilitates incremental learning. It incorporates three personal dietary tendencies that influence food analysis: likelihood, prior distribution, and mealtime category. In the evaluation of the proposed method using images uploaded to FoodLog, both food-image detection and food-balance estimation are improved. In particular, in the food-balance estimation, the mean absolute error is significantly reduced from 0.69 servings to 0.28 servings on average for two persons using more than 200 personal images, and 0.59 servings to 0.48 servings on average for four persons using 100 personal images. Among the works analyzing food images, this is the first to make use of statistical personal bias to improve the performance of the analysis.
C1 [Aizawa, Kiyoharu; Li, He] Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
   [Maruyama, Yuto] Hitachi Ltd, Ibaraki 3170073, Japan.
   [Morikawa, Chamin] Univ Tokyo, Intelligent Modeling Lab, Tokyo 1138654, Japan.
C3 University of Tokyo; Hitachi Limited; University of Tokyo
RP Aizawa, K (corresponding author), Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138656, Japan.
EM aizawa@hal.t.u-tokyo.ac.jp; maruyama@hal.t.u-tokyo.ac.jp;
   ehrlick@hal.t.u-tokyo.ac.jp; chamds@hal.t.u-tokyo.ac.jp
CR Aizawa K., 2009, J IPSJ, V50, P592
   [Anonymous], 2008, Proceeding of the 16th ACM international conference on Multimedia-MM'08, DOI [DOI 10.1145/1459359, 10.1145/1459359]
   [Anonymous], 2011, Proceedings of the 24th, DOI [DOI 10.1145/2047196.2047198, 10.1145/2047196]
   Bosch M, 2011, IEEE IMAGE PROC, P1789, DOI 10.1109/ICIP.2011.6115809
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Csurka G., 2004, Workshop on Statistical Learning in Computer Vision, ECCV, P59
   Freund Y., 1996, PROC COLT, P209
   Joutou T, 2009, IEEE IMAGE PROC, P285, DOI 10.1109/ICIP.2009.5413400
   Khanna Nitin, 2010, ISM, P290
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Martin CK, 2012, OBESITY, V20, P891, DOI 10.1038/oby.2011.344
   Martin CK, 2009, BRIT J NUTR, V101, P446, DOI 10.1017/S0007114508027438
   Messina V, 2003, CAN J DIET PRACT RES, V64, P82, DOI 10.3148/64.2.2003.82
   Ministry of Agriculture, FOOD BAL GUID
   Miyazaki T., 2011, Proceedings of the 2011 IEEE International Symposium on Multimedia (ISM 2011), P363, DOI 10.1109/ISM.2011.66
   SCHALKOFF R., 1992, PATTERN RECOGN
   Thompson FE, 2010, J AM DIET ASSOC, V110, P48, DOI 10.1016/j.jada.2009.10.008
   Wu W, 2009, IEEE INT CON MULTI, P1210, DOI 10.1109/ICME.2009.5202718
   Yang S, 2010, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2010.5539907
   Yao BP, 2012, PROC CVPR IEEE, P3466, DOI 10.1109/CVPR.2012.6248088
   Zepeda L, 2008, INT J CONSUM STUD, V32, P692, DOI 10.1111/j.1470-6431.2008.00725.x
   Zhu FQ, 2010, IEEE J-STSP, V4, P756, DOI 10.1109/JSTSP.2010.2051471
NR 22
TC 67
Z9 75
U1 1
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 2176
EP 2185
DI 10.1109/TMM.2013.2271474
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900036
DA 2024-07-18
ER

PT J
AU Nguyen, TV
   Ni, BB
   Liu, HR
   Xia, W
   Luo, JB
   Kankanhalli, M
   Yan, SC
AF Nguyen, Tam V.
   Ni, Bingbing
   Liu, Hairong
   Xia, Wei
   Luo, Jiebo
   Kankanhalli, Mohan
   Yan, Shuicheng
TI Image Re-Attentionizing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention retargeting; visual saliency.
ID ENERGY MINIMIZATION
AB In this paper, we propose a computational framework, called Image Re-Attentionizing, to endow the target region in an image with the ability of attracting human visual attention. In particular, the objective is to recolor the target patches by color transfer with naturalness and smoothness preserved yet visual attention augmented. We propose to approach this objective within the Markov Random Field (MRF) framework and an extended graph cuts method is developed to pursue the solution. The input image is first over-segmented into patches, and the patches within the target region as well as their neighbors are used to construct the consistency graphs. Within the MRF framework, the unitary potentials are defined to encourage each target patch to match the patches with similar shapes and textures from a large salient patch database, each of which corresponds to a high-saliency region in one image, while the spatial and color coherence is reinforced as pairwise potentials. We evaluate the proposed method on the direct human fixation data. The results demonstrate that the target region(s) successfully attract human attention and in the meantime both spatial and color coherence is well preserved.
C1 [Nguyen, Tam V.; Liu, Hairong; Xia, Wei; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
   [Ni, Bingbing] Adv Digital Sci Ctr, Singapore, Singapore.
   [Luo, Jiebo] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.
   [Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117548, Singapore.
C3 National University of Singapore; University of Rochester; National
   University of Singapore
RP Nguyen, TV (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117548, Singapore.
EM tamnguyen@nus.edu.sg; bingbing.ni@adsc.com.sg; elelhr@nus.edu.sg;
   weixia@nus.edu.sg; jluo@cs.rochester.edu; mohan@comp.nus.edu.sg;
   eleyans@nus.edu.sg
RI Liu, Hairong/I-6695-2012; Yan, Shuicheng/HCI-1431-2022; Luo,
   Jiebo/AAI-7549-2020; Kankanhalli, Mohan/Q-9284-2019; Nguyen,
   Tam/HSG-3007-2023; Nguyen, Tam/AAU-6504-2020
OI Kankanhalli, Mohan/0000-0002-4846-2015; Nguyen, Tam/0000-0003-0236-7992;
   Luo, Jiebo/0000-0002-4516-9729
FU Singapore National Research Foundation under its International Research
   Centre
FX This work was supported in part by the Singapore National Research
   Foundation under its International Research Centre.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2009, P IEEE INT C COMP VI
   [Anonymous], 2009, P 17 ACM INT C MULT
   [Anonymous], 2011, 2011 IEEE WORKSH APP
   [Anonymous], 2007, P IEEE C COMP VIS PA
   Attneave F., 1954, PSYCHOL REV
   Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114
   Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60
   Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5
   Cerf M., J VISION, V9, P1
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   Garcia V., 2008, P IEEE C COMP VIS PA
   Gonzalezand R. C., 2002, DIGITAL IMAGE PROCES
   Ho J, 2009, IEEE I CONF COMP VIS, P1335, DOI 10.1109/ICCV.2009.5459309
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jordan M.I, 1998, Learning in Graphical Models
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Murray N, 2011, PROC CVPR IEEE, P433, DOI 10.1109/CVPR.2011.5995506
   O'Hare L, 2011, I-PERCEPTION, V2
   Ramanathan S, 2010, LECT NOTES COMPUT SC, V6314, P30, DOI 10.1007/978-3-642-15561-1_3
   REINHARD E., 2001, IEEE COMPUT GRAPH AP
   Shotton J, 2009, INT J COMPUT VISION, V81, P2, DOI 10.1007/s11263-007-0109-1
   Su S. L., 2004, P MIT STUD OX WORKSH
   Velichkovsky B., 1996, P VIS ATT COGN
   Wong LK, 2009, IEEE IMAGE PROC, P997, DOI 10.1109/ICIP.2009.5413825
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
NR 32
TC 32
Z9 34
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2013
VL 15
IS 8
BP 1910
EP 1919
DI 10.1109/TMM.2013.2272919
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 257OI
UT WOS:000327393900015
DA 2024-07-18
ER

PT J
AU Mai, ZC
   Mansour, H
   Nasiopoulos, P
   Ward, RK
AF Mai, Zicong
   Mansour, Hassan
   Nasiopoulos, Panos
   Ward, Rabab Kreidieh
TI Visually Favorable Tone-Mapping With High Compression Performance in
   Bit-Depth Scalable Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bit-depth scalable; HDR video compression; high dynamic range imaging;
   tone-mapping
AB In bit-depth scalable video coding, the tone-mapping scheme used to convert high-bit-depth to eight-bit videos is an essential yet very often ignored component. In this paper, we demonstrate that an appropriate choice of a tone-mapping operator can improve the coding efficiency of bit-depth scalable encoders. We present a new tone-mapping scheme that delivers superior compression efficiency while adhering to a predefined base layer perceptual quality. We develop numerical models that estimate the base layer bit-rate (R-b), the enhancement layer bitrate (R-e), and the mismatch (Q(L)) between the resulting low dynamic range (LDR) base-layer signal and the predefined base layer representation. Our proposed tone curve is given by the solution of an optimization problem which minimizes a weighted sum of R-b, R-e, and Q(L). The problem formulation also considers the temporal effect of tone-mapping by adding a constraint to the optimization problem that suppresses flickering artifacts. We also propose a technique with which to tone-map a high-bit-depth video directly in a compression-friendly color space (e. g., one luma and two chroma channels) without converting to the RGB domain. Experimental results show that we can save up to 40% of the total bit-rate (or 3.5 dB PSNR improvement for the same bitrate), and, in general, about 20% bit-rate savings can be achieved.
C1 [Mai, Zicong; Nasiopoulos, Panos; Ward, Rabab Kreidieh] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6J 1L4, Canada.
   [Mansour, Hassan] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6J 1L4, Canada.
C3 University of British Columbia; University of British Columbia
RP Mai, ZC (corresponding author), Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC V6J 1L4, Canada.
EM zicongm@ece.ubc.ca; hassanm@cs.ubc.ca; panosn@ece.ubc.ca;
   rababw@ece.ubc.ca
CR Akyuz A., 2004, J ELECTRON IMAGING, V13, P126
   [Anonymous], 2005, High Dynamic Range Imaging: Acquisition, Display, and Image-Based Lighting (The Morgan Kaufmann Series in Computer Graphics
   Drago F, 2003, COMPUT GRAPH FORUM, V22, P419, DOI 10.1111/1467-8659.00689
   Fairchild MD, 2004, J ELECTRON IMAGING, V13, P126, DOI 10.1117/1.1635368
   Jing XA, 2006, IEEE INT SYMP CIRC S, P5019
   Kainz F., 2003, ACM SIGGRAPH TECHNIC
   Kuang JT, 2007, J VIS COMMUN IMAGE R, V18, P406, DOI 10.1016/j.jvcir.2007.06.003
   Larson G. W., 1998, Journal of Graphics Tools, V3, P15, DOI 10.1080/10867651.1998.10487485
   Larson GregW., 1992, GRAPHICS GEMS 2, P80, DOI [10.1016/B978-0-08-050754-5.50025-6, DOI 10.1016/B978-0-08-050754-5.50025-6]
   Mai ZC, 2011, IEEE T IMAGE PROCESS, V20, P1558, DOI 10.1109/TIP.2010.2095866
   Mantiuk R, 2004, ACM T GRAPHIC, V23, P733, DOI 10.1145/1015706.1015794
   Mantiuk R, 2009, COMPUT GRAPH FORUM, V28, P193, DOI 10.1111/j.1467-8659.2009.01358.x
   Mantiuk R, 2008, COMPUT GRAPH FORUM, V27, P699, DOI 10.1111/j.1467-8659.2008.01168.x
   Mantiuk R, 2006, ACM T GRAPHIC, V25, P713, DOI 10.1145/1141911.1141946
   Mantiuk R, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360667
   Reinhard E, 2002, ACM T GRAPHIC, V21, P267, DOI 10.1145/566570.566575
   Seetzen H, 2004, ACM T GRAPHIC, V23, P760, DOI 10.1145/1015706.1015797
   Segall A., 2007, SG16Q6 ISOIECJTC1SC2
   Segall A, 2007, IEEE IMAGE PROC, P1
   Segall A, 2008, IEEE IMAGE PROC, P2776, DOI 10.1109/ICIP.2008.4712370
   Spaulding K. E., 2003, PICS C, P307
   Sullivan GJ, 2007, IEEE IMAGE PROC, P13
   Toda M, 2009, IEEE IMAGE PROC, P1817, DOI 10.1109/ICIP.2009.5413360
   Topiwala P., 2005, SG16Q6 ISOIECJTC1SC2
   Tumblin J, 1999, COMP GRAPH, P83, DOI 10.1145/311535.311544
   Ward G, 2005, THIRTEENTH COLOR IMAGING CONFERENCE, FINAL PROGRAM AND PROCEEDINGS, P283
   Winken M., 2007, SG16Q6 ISOIECJTC1SC2
   Winken M, 2007, IEEE IMAGE PROC, P5
   Xie J, 2011, IEEE INT SYMP CIRC S, P2789
   Xu RF, 2005, IEEE COMPUT GRAPH, V25, P57, DOI 10.1109/MCG.2005.133
NR 30
TC 19
Z9 23
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2013
VL 15
IS 7
BP 1503
EP 1518
DI 10.1109/TMM.2013.2266633
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 236PQ
UT WOS:000325811800004
DA 2024-07-18
ER

PT J
AU Rudinac, S
   Larson, M
   Hanjalic, A
AF Rudinac, Stevan
   Larson, Martha
   Hanjalic, Alan
TI Learning Crowdsourced User Preferences for Visual Summarization of Image
   Collections
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Crowdsourcing; image aesthetic appeal; image content and context; image
   set evaluation; learning to rank; sentiment analysis; social media;
   user-informed visual summarization
AB In this paper we propose a novel approach to selecting images suitable for inclusion in the visual summaries. The approach is grounded in insights about how people summarize image collections. We utilize the Amazon Mechanical Turk crowdsourcing platform to obtain a large number of manually created visual summaries as well as information about criteria for image inclusion in the summary. Based on these large-scale user tests, we propose an automatic image selection approach, which jointly utilizes the analysis of image content, context, popularity, visual aesthetic appeal as well as the sentiment derived from the comments posted on the images. In our approach we do not describe images based on their properties only, but also in the context of semantically related images, which improves robustness and effectively enables propagation of sentiment, aesthetic appeal as well as various inherent attributes associated with a particular group of images. We discuss the phenomenon of a low inter-user agreement, which makes an automatic evaluation of visual summaries a challenging task and propose a solution inspired by the text summarization and machine translation communities. The experiments performed on a collection of geo-referenced Flickr images demonstrate the effectiveness of our image selection approach.
C1 [Rudinac, Stevan; Larson, Martha; Hanjalic, Alan] Delft Univ Technol, Multimedia Informat Retrieval Lab, Delft, Netherlands.
C3 Delft University of Technology
RP Rudinac, S (corresponding author), Delft Univ Technol, Multimedia Informat Retrieval Lab, Delft, Netherlands.
EM s.rudinac@tudelft.nl; m.a.larson@tudelft.nl; a.hanjalic@tudelft.nl
RI Larson, Martha/E-9983-2014
OI Hanjalic, Alan/0000-0002-5771-2549
FU European Commission [216444]
FX Manuscript received September 07, 2012; revised January 31, 2013 and
   April 18, 2013; accepted April 24, 2013. Date of publication May 03,
   2013; date of current version September 13, 2013. This work was
   supported by the European Commission's 7th Framework Programme (FP7)
   under grant agreement no. 216444 (NoE PetaMedia). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Daniel Gatica-Perez.
CR [Anonymous], THESIS TU DELFT DELF
   [Anonymous], IEEE T MULTIMEDIA
   [Anonymous], P TEXT AN C
   [Anonymous], DUC 06
   [Anonymous], P CSDM 11
   [Anonymous], IS T SPIE
   [Anonymous], CPBD SHARPNESS METRI
   Cao LL, 2010, INT CONF ACOUST SPEE, P2274, DOI 10.1109/ICASSP.2010.5495905
   Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P201, DOI 10.1007/s10791-009-9109-9
   Datta R, 2006, LECT NOTES COMPUT SC, V3953, P288, DOI 10.1007/11744078_23
   Denecke Kerstin, 2008, 2008 IEEE 24th International Conference on Data Engineering Workshop (ICDE Workshop), P507, DOI 10.1109/ICDEW.2008.4498370
   Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]
   Esuli Andrea., 2006, LREC 2006 Proceedings, 2006, S, P417
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Harman D., 2002, Proceedings of the Second International Conference on Human Language Technology Research, HLT '02, P44
   Hasler D, 2003, P SOC PHOTO-OPT INS, V5007, P87, DOI 10.1117/12.477378
   Joachims T, 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]
   Jochems B, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P490
   Kazai G, 2011, LECT NOTES COMPUT SC, V6611, P165, DOI 10.1007/978-3-642-20161-5_17
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   Lee YJ, 2012, PROC CVPR IEEE, P1346, DOI 10.1109/CVPR.2012.6247820
   Li Y., 2010, ACM International Conference on Multimedia, P851
   Lin CY, 2004, ROUGE: A Package for Automatic Evaluation of Summaries, P74, DOI DOI 10.1253/JCJ.34.1213
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748
   Moorthy AK, 2010, LECT NOTES COMPUT SC, V6315, P1, DOI 10.1007/978-3-642-15555-0_1
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Nenkova A, 2004, HLT-NAACL 2004: HUMAN LANGUAGE TECHNOLOGY CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE MAIN CONFERENCE, P145
   Nowak S., 2010, P INT C MULT INF RET, P557, DOI [10.1145/1743384.1743478, DOI 10.1145/1743384.1743478]
   Over P., 2008, Proc. of the 2nd ACM TRECVid Video Summarization Workshop, P1, DOI [DOI 10.1145/1463563.1463564, 10.1145/1463563.1463564]
   Pang YW, 2011, COMPUT VIS IMAGE UND, V115, P352, DOI 10.1016/j.cviu.2010.10.010
   Paolacci G, 2010, JUDGM DECIS MAK, V5, P411
   Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135
   Park YJ, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P11
   Pihur V, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-62
   Popescu A., 2009, ACM International Conference on Information and Knowledge Management, P1713, DOI DOI 10.1145/1645953.1646211
   Radev DR, 2002, COMPUT LINGUIST, V28, P399, DOI 10.1162/089120102762671927
   Ross Joel, 2010, CHI 10 EXTENDED ABST, DOI [DOI 10.1145/1753846.1753873, 10.1145/1753846.1753873]
   Savakis AE, 2000, P SOC PHOTO-OPT INS, V3959, P111, DOI 10.1117/12.387147
   Siersdorfer S., 2010, ACM MM, P715
   Siersdorfer S., 2010, Proceedings of the 19th International Conference on World Wide Web, WWW'10, P891, DOI DOI 10.1145/1772690.1772781
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Strehl A., 2002, J. Machine Learning Res, V3, P583
   Wactlar HD, 1996, COMPUTER, V29, P46, DOI 10.1109/2.493456
   Whissell C, 2009, PSYCHOL REP, V105, P509, DOI 10.2466/PR0.105.2.509-521
   Wilson T., 2005, P HUMAN LANGUAGE TEC, P347, DOI DOI 10.3115/1220575.1220619
   Winkler S, 2001, PROC SPIE, V4299, P114, DOI 10.1117/12.429540
NR 48
TC 27
Z9 31
U1 0
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2013
VL 15
IS 6
BP 1231
EP 1243
DI 10.1109/TMM.2013.2261481
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA 222XH
UT WOS:000324765400002
DA 2024-07-18
ER

PT J
AU Essid, S
   Févotte, C
AF Essid, Slim
   Fevotte, Cedric
TI Smooth Nonnegative Matrix Factorization for Unsupervised Audiovisual
   Document Structuring
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bag of features; content structuring; indexing; machine learning; matrix
   factorization; unsupervised classification; videos
ID ALGORITHMS; PARTS
AB This paper introduces a new paradigm for unsupervised audiovisual document structuring. In this paradigm, a novel Nonnegative Matrix Factorization (NMF) algorithm is applied on histograms of counts (relating to a bag of features representation of the content) to jointly discover latent structuring patterns and their activations in time. Our NMF variant employs the Kullback-Leibler divergence as a cost function and imposes a temporal smoothness constraint to the activations. It is solved by a majorization-minimization technique. The approach proposed is meant to be generic and is particularly well suited to applications where the structuring patterns may overlap in time. As such, it is evaluated on two person-oriented video structuring tasks (one using the visual modality and the second the audio). This is done using a challenging database of political debate videos. Our results outperform reference results obtained by a method using Hidden Markov Models. Further, we show the potential that our general approach has for audio speaker diarization.
C1 [Essid, Slim; Fevotte, Cedric] Telecom ParisTech, CNRS LTCI, F-75014 Paris, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris; Centre National de la Recherche Scientifique (CNRS)
RP Essid, S (corresponding author), Telecom ParisTech, CNRS LTCI, F-75014 Paris, France.
FU  [ANR-09-JCJC-0073-01]
FX Manuscript received June 28, 2011; revised December 14, 2011; accepted
   June 15, 2012. Date of publication November 20, 2012; date of current
   version January 15, 2013. This work was supported by project
   ANR-09-JCJC-0073-01 TANGERINE (Theory and applications of nonnegative
   matrix factorization). The associate editor coordinating the review of
   this manuscript and approving it for publication was Jia Li.
CR AIGRAIN P, 1997, INTELLIGENT MULTIMED, P159
   [Anonymous], P WORKSH SIGN PROC A
   [Anonymous], 2007, P IEEE 11 INT C COMP
   [Anonymous], 2009, NIST RICH TRANSCR 20
   [Anonymous], 2008, An Open and Portable Library of Computer Vision Algorithms
   Assfalg J, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P825, DOI 10.1109/ICME.2002.1035909
   Assfalg J, 2002, IEEE MULTIMEDIA, V9, P52, DOI 10.1109/93.998060
   Bishop C.M., 2008, Pattern Recognition and Machine Learning: A Matlab Companion
   Cemgil Ali Taylan, 2009, Comput Intell Neurosci, P785152, DOI 10.1155/2009/785152
   DEPIERRO AR, 1993, IEEE T MED IMAGING, V12, P328, DOI 10.1109/42.232263
   Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277
   Eggert J, 2004, IEEE IJCNN, P2529
   El Khoury E., 2010, PROC INT C MULTIMEDI, P295
   Févotte C, 2011, NEURAL COMPUT, V23, P2421, DOI 10.1162/NECO_a_00168
   Gaussier E., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P601, DOI 10.1145/1076034.1076148
   Guillamet D, 2002, INT C PATT RECOG, P116, DOI 10.1109/ICPR.2002.1048251
   Hofmann T., 1999, P 22 ANN INT ACM SIG
   Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DD, 2001, ADV NEUR IN, V13, P556
   Lefevre A., 2011, P INT C AC SPEECH SI
   Levy M., 2006, P IEEE INT C AC SPEE, V5
   Li SZ, 2001, PROC CVPR IEEE, P207
   Mairal J, 2010, J MACH LEARN RES, V11, P19
   Meignier S., 2001, P 2001 SPEAK OD SPEA
   Meignier Sylvain, 2010, P CMU SPUD WORKSH DA
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Reynolds D. A., 2005, P IEEE INT C AC SPEE, V5
   Sivic J, 2009, PROC CVPR IEEE, P1145, DOI 10.1109/CVPRW.2009.5206513
   Smeaton AF, 2010, COMPUT VIS IMAGE UND, V114, P411, DOI 10.1016/j.cviu.2009.03.011
   Trivedi P.K, 1998, ECONOMETRIC SOC MONO
   Vallet F., 2010, P IEEE INT C IM PROC
   Vallet F., 2011, TV CONTENT ANAL TECH
   Vendrig J, 2002, IEEE T MULTIMEDIA, V4, P492, DOI 10.1109/TMM.2002.802021
   Vinciarelli A., 2009, P IEEE INT WORKSH SO
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Virtanen T, 2008, INT CONF ACOUST SPEE, P1825, DOI 10.1109/ICASSP.2008.4517987
   Yeung M. M., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P375, DOI 10.1109/ICPR.1996.546973
   Yeung MM, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pA338
   Zelenak M., 2010, 6 JORN TECN HABL 2 I
NR 40
TC 37
Z9 41
U1 0
U2 15
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 415
EP 425
DI 10.1109/TMM.2012.2228474
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500016
DA 2024-07-18
ER

PT J
AU Mastronarde, N
   Kanoun, K
   Atienza, D
   Frossard, P
   van der Schaar, M
AF Mastronarde, Nicholas
   Kanoun, Karim
   Atienza, David
   Frossard, Pascal
   van der Schaar, Mihaela
TI Markov Decision Process Based Energy-Efficient On-Line Scheduling for
   Slice-Parallel Video Decoders on Multicore Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video decoding; multicore scheduling; dynamic voltage frequency scaling;
   energy-efficient scheduling; Quality-of-Service; Markov decision process
AB We consider the problem of energy-efficient on-line scheduling for slice-parallel video decoders on multicore systems with Dynamic Voltage Frequency Scaling (DVFS) enabled processors. In the past, scheduling and DVFS policies in multi-core systems have been formulated heuristically due to the inherent complexity of the on-line multicore scheduling problem. The key contribution of this paper is that we rigorously formulate the problem as a Markov decision process (MDP), which simultaneously takes into account the on-line scheduling and per-core DVFS capabilities; the power consumption of the processor cores and caches; and the loss tolerant and dynamic nature of the video decoder. The objective of the MDP is to minimize long-term power consumption subject to a minimum Quality of Service (QoS) constraint related to the decoder's throughput. We evaluate the proposed on-line scheduling algorithm in Matlab using realistic video decoding traces generated from a cycle-accurate multiprocessor ARM simulator.
C1 [Mastronarde, Nicholas] SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
   [Kanoun, Karim; Atienza, David; Frossard, Pascal] Ecole Polytech Fed Lausanne, Inst Elect Engn, CH-1015 Lausanne, Switzerland.
   [van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo; Swiss Federal Institutes of Technology Domain; Ecole
   Polytechnique Federale de Lausanne; University of California System;
   University of California Los Angeles
RP Mastronarde, N (corresponding author), SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
EM nmastron@buffalo.edu; karim.kanoun@epfl.ch; david.atienza@epfl.ch;
   pascal.frossard@epfl.ch; mihaela@ee.ucla.edu
RI Mastronarde, Nicholas/IZD-7746-2023; Alonso, David Atienza/F-3964-2011;
   Mastronarde, Nicholas/W-5332-2019; Frossard, Pascal/AAF-2268-2019
OI Mastronarde, Nicholas/0000-0002-8474-7237; Alonso, David
   Atienza/0000-0001-9536-4947; Mastronarde, Nicholas/0000-0002-8474-7237;
   van der schaar, Mihaela/0000-0003-3933-6049
FU National Science Foundation [CNS-0509522]; Swiss National Science
   Foundation [200021-127282]; CSEM SA; Swiss National Science Foundation
   (SNF) [200021_127282] Funding Source: Swiss National Science Foundation
   (SNF)
FX Manuscript received December 03, 2011; revised May 21, 2012; accepted
   July 08, 2012. Date of publication December 04, 2012; date of current
   version January 15, 2013. The work of M. van der Schaar and N.
   Mastronarde was supported in part by the National Science Foundation
   under Award CNS-0509522. The work of D. Atienza and K. Kanoun was
   supported in part by the Swiss National Science Foundation, under Grant
   200021-127282, and a research grant funded by CSEM SA. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Yiannis Andreopoulos.
CR [Anonymous], SIGOPS OPER SYST REV
   Aydin H., 2003, P 17 INT S PAR DISTR
   Benini L, 2005, J VLSI SIG PROC SYST, V41, P169, DOI 10.1007/s11265-005-6648-1
   Benini L, 1999, IEEE T COMPUT AID D, V18, P813, DOI 10.1109/43.766730
   Catthoor F., 1998, CUSTOM MEMORY MANAGE
   Cong J, 2009, DES AUT TEST EUROPE, P411
   Lee W. Y., 2009, P 2009 INT C HYBR IN, P273
   LEISERSON CE, 1991, ALGORITHMICA, V6, P5, DOI 10.1007/BF01759032
   Liu H, 2008, ECRTS 2008: PROCEEDINGS OF THE 20TH EUROMICRO CONFERENCE ON REAL-TIME SYSTEMS, P92, DOI 10.1109/ECRTS.2008.18
   Mastronarde N., MARKOV DECISION PROC
   Niyato D., 2009, P 9 IEEE ACM INT S C
   Roitzsch M., 2007, EMSOFT '07, P269
   Sutton R., 1998, Reinforcement Learning: An Introduction
   van der Tol EB, 2003, PROC SPIE, V5022, P707, DOI 10.1117/12.476234
   Wei Y.-H., 2010, Proceedings of ACM Symposium on Applied Computing (SAC), P258
   Xu R., THESIS U PITTSBURGH
   Xu Ruibin., P 28 IEEE INT REAL T, P25
   Yuan WH, 2006, IEEE T MOBILE COMPUT, V5, P799, DOI 10.1109/TMC.2006.98
   Zhang D., 2011, P 2011 IEEE INT C CO, V2, P666
NR 19
TC 22
Z9 23
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2013
VL 15
IS 2
BP 268
EP 278
DI 10.1109/TMM.2012.2231668
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 075HN
UT WOS:000313875500004
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Ji, RR
   Duan, LY
   Chen, J
   Xie, LX
   Yao, HX
   Gao, W
AF Ji, Rongrong
   Duan, Ling-Yu
   Chen, Jie
   Xie, Lexing
   Yao, Hongxun
   Gao, Wen
TI Learning to Distribute Vocabulary Indexing for Scalable Visual Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Distributed search; inverted indexing; parallel computing; visual
   search; visual vocabulary
AB In recent years, there is an ever-increasing research focus on Bag-of-Words based near duplicate visual search paradigm with inverted indexing. One fundamental yet unexploited challenge is how to maintain the large indexing structures within a single server subject to its memory constraint, which is extremely hard to scale up to millions or even billions of images. In this paper, we propose to parallelize the near duplicate visual search architecture to index millions of images over multiple servers, including the distribution of both visual vocabulary and the corresponding indexing structure. We optimize the distribution of vocabulary indexing from a machine learning perspective, which provides a "memory light" search paradigm that leverages the computational power across multiple servers to reduce the search latency. Especially, our solution addresses two essential issues: "What to distribute" and "How to distribute". "What to distribute" is addressed by a "lossy" vocabulary Boosting, which discards both frequent and indiscriminating words prior to distribution. "How to distribute" is addressed by learning an optimal distribution function, which maximizes the uniformity of assigning the words of a given query to multiple servers. We validate the distributed vocabulary indexing scheme in a real world location search system over 10 million landmark images. Comparing to the state-of-the-art alternatives of single-server search [5], [6], [16] and distributed search [23], our scheme has yielded a significant gain of about 200% speedup at comparable precision by distributing only 5% words. We also report excellent robustness even when partial servers crash.
C1 [Ji, Rongrong; Duan, Ling-Yu; Chen, Jie; Gao, Wen] Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
   [Xie, Lexing] Australian Natl Univ, Sch Comp Sci, Canberra, ACT 0200, Australia.
   [Yao, Hongxun] Harbin Inst Technol, Dept Comp Sci, Harbin 150001, Peoples R China.
C3 Peking University; Australian National University; Harbin Institute of
   Technology
RP Duan, LY (corresponding author), Peking Univ, Inst Digital Media, Beijing 100871, Peoples R China.
EM lingyu@pku.edu.cn; yhx@vilab.hit.edu.cn
OI Xie, Lexing/0000-0001-8319-0118
FU National Science Foundation of China [60902057, 61271311]; National
   Basic Research Program ("973") of China [2009CB320902]
FX This work was supported by the National Science Foundation of China
   (60902057, 61271311), in part by the National Basic Research Program
   ("973") of China (2009CB320902). The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr.
   Shin'ichi Satoh.
CR [Anonymous], 2009, IEEE TPAMI
   [Anonymous], P INT C COMP VIS
   [Anonymous], 2009, P INT C COMP VIS
   [Anonymous], 2010, P IEEE C COMP VIS PA
   [Anonymous], 2006, P IEEE COMPUTER SOC
   [Anonymous], 2007, CVPR
   [Anonymous], P EUR C COMP VIS
   [Anonymous], P IEEE C COMP VIS PA
   Badue C, 2001, EIGHTH SYMPOSIUM ON STRING PROCESSING AND INFORMATION RETRIEVAL, PROCEEDINGS, P10, DOI 10.1109/SPIRE.2001.989733
   Barroso LA, 2003, IEEE MICRO, V23, P22, DOI 10.1109/MM.2003.1196112
   Blanco R, 2010, ACM T INFORM SYST, V28, DOI 10.1145/1658377.1658378
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Buttcher S., 2006, P ANN INT ACM SIGIR
   Chandrasekhar V., 2009, P IEEE C COMP VIS PA
   Chen D., 2009, P DAT COMPR C
   COUVREUR TR, 1994, J AM SOC INFORM SCI, V45, P443, DOI 10.1002/(SICI)1097-4571(199408)45:7<443::AID-ASI1>3.0.CO;2-O
   Hofmann T., 2001, MACH LEARN J
   JEONG BS, 1995, IEEE T PARALL DISTR, V6, P142, DOI 10.1109/71.342125
   Ji R., 2010, P CVPR
   Ji R., 2012, INT J COMPUT VIS
   Ji R., 2011, P ACM MULT
   Ji R., 2009, P ICME
   Jiang Y.-G., 2007, P ACM C CONT BAS IM
   Jurie F., 2005, P INT C COMP VIS
   Kulis B., 1999, P INT C COMP VIS
   Lazebnik S, 2009, IEEE T PATTERN ANAL, V31, P1294, DOI 10.1109/TPAMI.2008.138
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Lucchese C., 2007, P INT C SCAL INF SYS
   Mairal J., 2008, ADV NEURAL INF PROCE
   Maree R., 2010, P ACM C MULT INF RET
   Marin M., 2007, P ACM C INF KNOWL MA
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Mikolajczyk K., 2005, IEEE C COMP VIS PATT
   Moffat A., 2006, P ACM SIGIR C RES DE
   Moffat A, 2007, INFORM RETRIEVAL, V10, P205, DOI 10.1007/s10791-006-9014-4
   Moosmann F., 2006, ADV NEURAL INF PROCE
   Ntoulas A., 2007, P ANN INT ACM SIGIR
   Salton G., 1988, INF PROCESS MANAGE
   Stanfill C., 1990, P ACM SIGIR C RES DE
   Weiss Y., 2008, ADV NEURAL INF PROCE, V21, P1
   Yan T., 2008, P ACM C EMB NETW SEN
   Yang J., 2007, P ACM C MULT INF RET
NR 42
TC 80
Z9 84
U1 0
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 153
EP 166
DI 10.1109/TMM.2012.2225035
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600013
DA 2024-07-18
ER

PT J
AU Schweiger, F
   Schroth, G
   Eichhorn, M
   Al-Nuaimi, A
   Cizmeci, B
   Fahrmair, M
   Steinbach, E
AF Schweiger, Florian
   Schroth, Georg
   Eichhorn, Michael
   Al-Nuaimi, Anas
   Cizmeci, Burak
   Fahrmair, Michael
   Steinbach, Eckehard
TI Fully Automatic and Frame-Accurate Video Synchronization Using Bitrate
   Sequences
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video signal processing; multimedia systems; video codecs
AB Video synchronization is an essential processing step in many multimedia applications, and various methods have been proposed in the literature each of which addresses the problem from a different point of vantage. In this article, we present an information theoretic approach to video synchronization, based on the state-of-the-art in hybrid video coding. Time series derived from the videos' instantaneous bitrate demand are correlated in a robust manner employing the recently published ConCor algorithm. We enhance ConCor with integrated normalization capabilities in order to improve its shape-oriented matching performance. Furthermore, we present a mathematical framework to derive the most suitable ConCor parameters given a specific class of input videos. In an extensive experimental analysis, we give an insight into the representation of synchronization-relevant scene changes with bitrate data, and examine the influence of encoding parameters on the synchronization performance. Experiments on diverse video input substantiate the reliable performance of our easy to implement, yet effective video synchronization algorithm which distinguishes itself in that it operates largely without manual intervention.
C1 [Schweiger, Florian; Schroth, Georg; Eichhorn, Michael; Al-Nuaimi, Anas; Cizmeci, Burak; Steinbach, Eckehard] Tech Univ Munich, Inst Media Technol, Munich, Germany.
   [Fahrmair, Michael] DOCOMO Euro Labs, Munich, Germany.
C3 Technical University of Munich; NTT Docomo
RP Schweiger, F (corresponding author), Tech Univ Munich, Inst Media Technol, Munich, Germany.
EM florian.schweiger@tum.de; schroth@tum.de; michael.eichhorn@tum.de;
   anas.alnuaimi@tum.de; burak.cizmeci@tum.de; fahrmair@docomolab-euro.com;
   eckehard.steinbach@tum.de
OI Steinbach, Eckehard/0000-0001-8853-2703
CR [Anonymous], P WORKSH VIS MOD DYN
   [Anonymous], P ADV CONC INT VIS S
   [Anonymous], 2005, P IEEE WORKSH MOT VI
   Ballan L, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778824
   Brito DN, 2008, SIBGRAPI, P37, DOI 10.1109/SIBGRAPI.2008.28
   Caspi Y, 2002, INT J COMPUT VISION, V48, P39, DOI 10.1023/A:1014803327923
   Caspi Y., 2000, P C COMP VIS PATT RE
   Caspi Y, 2006, INT J COMPUT VISION, V68, P53, DOI 10.1007/s11263-005-4842-z
   Dai CX, 2006, IEEE IMAGE PROC, P501, DOI 10.1109/ICIP.2006.312436
   Irani M., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P626, DOI 10.1109/ICCV.1999.791283
   Janssens G., 2009, 4D REPOSITORY PUBLIC
   KLOTZ J, 1973, ANN STAT, V1, P373, DOI 10.1214/aos/1176342377
   Klotz J., 1972, Proc. Sixth Berkeley Symp. Math. Statist. Prob, V4, P173
   Lei C, 2006, IEEE T IMAGE PROCESS, V15, P2473, DOI 10.1109/TIP.2006.877438
   Mikolajczyk K, 2005, INT J COMPUT VISION, V65, P43, DOI 10.1007/s11263-005-3848-x
   Pádua FLC, 2010, IEEE T PATTERN ANAL, V32, P304, DOI 10.1109/TPAMI.2008.301
   Pooley D., 2003, P INT C IM PROC BARC
   Raguse K., 2006, P ISPRS COMM 5 S IM, P254
   Rao C, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P939
   Reid I., 1996, Computer Vision - ECCV '96. 4th Eurpean Conference on Computer Proceedings, P647
   Schroth G, 2010, IEEE IMAGE PROC, P1549, DOI 10.1109/ICIP.2010.5653576
   Schweiger F., 2012, TU MUNCHEN LEHRSTUHL
   Schweiger F., 2011, P ACM MULT SCOTTSD A
   Schweiger F., 2009, P INT C MULT EXP NEW
   Sigal L, 2010, INT J COMPUT VISION, V87, P4, DOI 10.1007/s11263-009-0273-6
   Stein G., 1999, P C COMP VIS PATT RE
   Tresadern P., 2003, Proceedings of the 14th British Machine Vision Conference, P629
   Tuytelaars T., 2004, P C COMP VIS PATT RE
   Ukrainitz Y, 2006, LECT NOTES COMPUT SC, V3953, P538, DOI 10.1007/11744078_42
   Ushizaki M., 2006, P INT C PATT REC HON
   VideoLAN, 2011, X264 BEST H264 AVC E
   Wedge D, 2006, LECT NOTES COMPUT SC, V3852, P832
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
NR 33
TC 1
Z9 1
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2013
VL 15
IS 1
BP 1
EP 14
DI 10.1109/TMM.2012.2225038
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 058PS
UT WOS:000312646600001
DA 2024-07-18
ER

PT J
AU Deng, CW
   Lin, WS
   Cai, JF
AF Deng, Chenwei
   Lin, Weisi
   Cai, Jianfei
TI Content-Based Image Compression for Arbitrary-Resolution Display Devices
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-aware; discrete wavelet transform (DWT); image compression; seam
   carving (SC); spatial scalability
AB The existing image coding methods cannot support content-based spatial scalability with high compression. In mobile multimedia communications, image retargeting is generally required at the user end. However, content-based image retargeting (e. g., seam carving) is with high computational complexity and is not suitable for mobile devices with limited computing power. The work presented in this paper addresses the increasing demand of visual signal delivery to terminals with arbitrary resolutions, without heavy computational burden to the receiving end. In this paper, the principle of seam carving is incorporated into a wavelet codec (i.e., SPIHT [2]). For each input image, block-based seam energy map is generated in the pixel domain. In the mean-time, multilevel discrete wavelet transform (DWT) is performed. Different from the conventional wavelet-based coding schemes, DWT coefficients here are grouped and encoded according to the resultant seam energy map. The bitstream is then transmitted in energy descending order. At the decoder side, the end user has the ultimate choice for the spatial scalability without the need to examine the visual content; an image with arbitrary aspect ratio can be reconstructed in a content-aware manner based upon the side information of the seam energy map. Experimental results show that, for the end users, the received images with an arbitrary resolution preserve important content while achieving high coding efficiency for transmission.
C1 [Deng, Chenwei; Lin, Weisi; Cai, Jianfei] Nanyang Technol Univ, Sch Comp Engn SCE, Singapore 639798, Singapore.
C3 Nanyang Technological University
RP Deng, CW (corresponding author), Nanyang Technol Univ, Sch Comp Engn SCE, Singapore 639798, Singapore.
EM cwdeng@ntu.edu.sg; wslin@ntu.edu.sg; asjfcai@ntu.edu.sg
RI Cai, Jianfei/A-3691-2011
OI Cai, Jianfei/0000-0002-9444-3763
FU MoE AcRF Tire 2, Singapore [T208B1218]
FX This work was supported by MoE AcRF Tire 2, Singapore, under Grant
   T208B1218. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Charles D. (Chuck)
   Creusere.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 154441 IEEE ISOIEC
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], 2002, APPL NUM HARM ANAL
   [Anonymous], 2009, ACM INT C MULTIMEDIA
   [Anonymous], P IEEE INT C COMM
   [Anonymous], P ACM SIG GRAPH AUG
   Avidan S, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239461
   BODDEN E., 2007, Arithmetic coding revealed
   Caldwell Ben, 2008, WWW CONSORTIUM W3C, V290, P1
   Christopoulos C, 2000, IEEE SIGNAL PROC LET, V7, P247, DOI 10.1109/97.863146
   Cormen T.H., 2009, INTRO ALGORITHMS
   Dyer M, 2009, IEEE T CIRC SYST VID, V19, P215, DOI 10.1109/TCSVT.2008.2009245
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kasutani E, 2001, IEEE IMAGE PROC, P674, DOI 10.1109/ICIP.2001.959135
   Liu D, 2007, IEEE T CIRC SYST VID, V17, P1273, DOI 10.1109/TCSVT.2007.903663
   Liu YJ, 2011, COMPUT GRAPH FORUM, V30, P583, DOI 10.1111/j.1467-8659.2011.01881.x
   Ma L, 2011, SIGNAL PROCESS-IMAGE, V26, P162, DOI 10.1016/j.image.2011.02.002
   Mallat S.A., 1999, WAVELET TOUR SIGNAL
   Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424
   Marpe D, 2003, IEEE T CIRC SYST VID, V13, P620, DOI 10.1109/TCSVT.2003.815173
   Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847
   Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199
   Percival D.B., 2006, Wavelet Methods for Time Series Analysis, V4
   PLOTNIK E, 1992, IEEE T INFORM THEORY, V38, P66, DOI 10.1109/18.108250
   Rubinstein M, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866186
   Ruch DK, 2009, Wavelet theory: an elementary approach with applications
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Santa-Cruz D, 2002, SIGNAL PROCESS-IMAGE, V17, P113, DOI 10.1016/S0923-5965(01)00025-X
   Shamir Ariel, 2009, ACM SIG-GRAPH ASIA 2009 Courses, P11
   Simakov D, 2008, PROC CVPR IEEE, P3887
   Tanaka Y, 2010, IEEE IMAGE PROC, P1225, DOI 10.1109/ICIP.2010.5652414
   Tanaka Y, 2010, INT CONF ACOUST SPEE, P1322, DOI 10.1109/ICASSP.2010.5495433
   Taubman D.S., 2001, JPEG 2000: Image Compression Fundamentals, Standards and Practice
   Usevitch BE, 2001, IEEE SIGNAL PROC MAG, V18, P22, DOI 10.1109/79.952803
   Vo DT, 2010, IEEE T IMAGE PROCESS, V19, P399, DOI 10.1109/TIP.2009.2035845
   Wang YS, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409071
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Wiegand T, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P542, DOI 10.1109/ICIP.2001.958171
NR 39
TC 25
Z9 28
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2012
VL 14
IS 4
BP 1127
EP 1139
DI 10.1109/TMM.2012.2191270
PN 2
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 976QM
UT WOS:000306599400002
DA 2024-07-18
ER

PT J
AU Bouman, KL
   Abdollahian, G
   Boutin, M
   Delp, EJ
AF Bouman, Katherine L.
   Abdollahian, Golnaz
   Boutin, Mireille
   Delp, Edward J.
TI A Low Complexity Sign Detection and Text Localization Method for Mobile
   Applications
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Mobile devices; sign detection; text detection; text localization; text
   segmentation
ID RECOGNITION; IMAGES
AB We propose a low complexity method for sign detection and text localization in natural images. This method is designed for mobile applications (e.g., unmanned or handheld devices) in which computational and energy resources are limited. No prior assumption is made regarding the text size, font, language, or character set. However, the text is assumed to be located on a homogeneous background using a contrasting color. We have deployed our method on a Nokia N800 cellular phone as part of a system for automatic detection and translation of outdoor signs. This handheld device is equipped with a 0.3-megapixel camera capable of acquiring images of outdoor signs that typically contain enough details for the sign to be readable by a human viewer. Our experiments show that the text of these images can be accurately localized within the device in a fraction of a second.
C1 [Bouman, Katherine L.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
   [Abdollahian, Golnaz; Boutin, Mireille; Delp, Edward J.] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 University of Michigan System; University of Michigan; Purdue University
   System; Purdue University
RP Bouman, KL (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM klbouman@umich.edu; gabdollla@ecn.purdue.edu; mboutin@ecn.purdue.edu;
   ace@ecn.purdue.edu
RI Delp, Edward J/C-3616-2013
OI Boutin, Mireille/0000-0002-0837-6577; Delp, Edward/0000-0002-2909-7323
FU Next Wave Systems, LLC
FX Manuscript received March 31, 2010; revised October 19, 2010 and
   February 15, 2011; accepted April 14, 2011. Date of publication May 12,
   2011; date of current version September 16, 2011. This work was
   supported by Next Wave Systems, LLC. Part of this work was presented at
   ICASSP 2010. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Z. Jane Wang.
CR Agnihotri L, 1999, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES (CBAIVL'99) - PROCEEDINGS, P109, DOI 10.1109/IVL.1999.781133
   Chen XL, 2004, IEEE T IMAGE PROCESS, V13, P87, DOI 10.1109/TIP.2003.819223
   Chen XR, 2004, PROC CVPR IEEE, P366
   Cui YT, 1997, PROC CVPR IEEE, P502, DOI 10.1109/CVPR.1997.609372
   Dubey P., 2006, P 8 INT C SIGN PROC, V4
   EISENBURG A, 2002, NY TIMES
   Ezaki N, 2004, INT C PATT RECOG, P683, DOI 10.1109/ICPR.2004.1334351
   GAO J, 2001, P IEEE COMP SOC C CO, V2
   Gllavata J, 2003, ISPA 2003: PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON IMAGE AND SIGNAL PROCESSING AND ANALYSIS, PTS 1 AND 2, P611
   Haneda E, 2010, INT CONF ACOUST SPEE, P1042, DOI 10.1109/ICASSP.2010.5495328
   JAFRI SAR, 2008, P ICIP, P3196
   JAFRI SAR, 2008, P SPIE, V6821
   Jain AK, 1998, PATTERN RECOGN, V31, P2055, DOI 10.1016/S0031-3203(98)00067-3
   JIRATTITICHAREO.W, 2006, P IEEE AS PAC C CIRC, P1000
   JUNG KC, 2000, P IEEE REG 10 C SYST, V2, P176
   Kim JS, 2005, PROC INT CONF DOC, P655
   Kim KI, 2003, IEEE T PATTERN ANAL, V25, P1631, DOI 10.1109/TPAMI.2003.1251157
   Lienhart R, 2000, MULTIMEDIA SYST, V8, P69, DOI 10.1007/s005300050006
   LIU TIY, 2005, P 8 INT C DOC AN REC
   LIU Y, 2005, P 8 INT C DOC AN REC, V1, P399
   Lucas SM, 2005, PROC INT CONF DOC, P80, DOI 10.1109/ICDAR.2005.231
   NEIVA M, NEW LOOK COLOUR GREY
   Shivakumara P, 2011, IEEE T PATTERN ANAL, V33, P412, DOI 10.1109/TPAMI.2010.166
   Shivakumara P, 2008, PROCEEDINGS OF THE 8TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS, P307, DOI 10.1109/DAS.2008.17
   Sin BK, 2002, INT C PATT RECOG, P489, DOI 10.1109/ICPR.2002.1047983
   WU CA, 2005, THESIS U CALIFORNIA
   WU J, 2002, P INT C MACH LEARN C, V3, P1167
   Wu W, 2005, IEEE T INTELL TRANSP, V6, P378, DOI 10.1109/TITS.2005.858619
   Xiaodong Huang, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3216, DOI 10.1109/ICPR.2010.786
   Yang J., 2001, Proc. of PUI, P1
   Yu J, 2009, INTEGRATION, P137
   Yuan Q, 2001, PROC INT CONF DOC, P302, DOI 10.1109/ICDAR.2001.953803
NR 32
TC 26
Z9 29
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 922
EP 934
DI 10.1109/TMM.2011.2154317
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300008
DA 2024-07-18
ER

PT J
AU Lee, JS
   De Simone, F
   Ebrahimi, T
AF Lee, Jong-Seok
   De Simone, Francesca
   Ebrahimi, Touradj
TI Subjective Quality Evaluation via Paired Comparison: Application to
   Scalable Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bradley-Terry model; content distribution; multimedia quality
   assessment; paired comparison; scalable video coding; subjective test
ID MODEL; TIES
AB Scalable video coding is a powerful solution for content delivery in many interactive multimedia services due to its adaptability to varying terminal and network constraints. In order to successfully exploit such adaptability, it is necessary to understand users' preference among various scalability options and consequently develop an optimal bit rate adaptation strategy. In this paper, we present a study of subjective quality assessment of scalable video coding, which investigates the influence of the combination of scalability options on perceived quality with the goal of providing guidelines for an adaptive strategy that selects the optimal combination for a given bandwidth constraint. In particular, the study is based on paired comparison of stimuli that is suitable for our goal due to its simplicity and easiness. We propose a new method, called Paired Evaluation via Analysis of Reliability (PEAR), which analyzes paired comparison results and produces not only quality scores but also intuitive measures of confidence of the scores for significance analysis. Results and analysis of extensive subjective tests for two different scalable video codecs and high definition contents are described, from which general consistent conclusions are drawn. The video and subjective data used in the paper are publicly available to the research community.(1)
C1 [Lee, Jong-Seok; De Simone, Francesca; Ebrahimi, Touradj] Swiss Fed Inst Technol Lausanne EPFL, Inst Elect Engn, Multimeida Signal Proc Grp, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Lee, JS (corresponding author), Swiss Fed Inst Technol Lausanne EPFL, Inst Elect Engn, Multimeida Signal Proc Grp, CH-1015 Lausanne, Switzerland.
EM jong-seok.lee@epfl.ch; francesca.desimone@epfl.ch;
   touradj.ebrahimi@epfl.ch
RI Lee, Jong-Seok/AAF-5197-2020
OI Lee, Jong-Seok/0000-0001-5255-4425; Ebrahimi,
   Touradj/0000-0002-9900-3687; De Simone, Francesca/0000-0001-5272-9221
FU European Community [FP7/2007-2011, 216444]; Swiss NCCR Interactive
   Multimodal Information Management (IM2)
FX Manuscript received September 01, 2010; revised February 28, 2011;
   accepted May 09, 2011. Date of publication May 23, 2011; date of current
   version September 16, 2011. This work was supported in part by the
   European Community's Seventh Framework Programme (FP7/2007-2011) under
   grant agreement no. 216444 (PetaMedia), and in part by the Swiss NCCR
   Interactive Multimodal Information Management (IM2). The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Jin Li.
CR Adami N, 2007, IEEE T CIRC SYST VID, V17, P1238, DOI 10.1109/TCSVT.2007.906828
   [Anonymous], The SVT High Definition Multi Format Test Set
   [Anonymous], P910 ITUR
   [Anonymous], P INT WORKSH QUAL MU
   [Anonymous], P INT WORKSH VID PRO
   [Anonymous], 2020, INT TELECOMMUNICATIO
   [Anonymous], P INT S CIRC SYST
   [Anonymous], P INT C MULT EXP
   [Anonymous], P ST TS IM PROC IM Q
   BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.1093/biomet/39.3-4.324
   Chen KT, 2009, MATH COMPUT SCI ENG, P491, DOI 10.1145/1631272.1631339
   Cranley N, 2005, MULTIMEDIA SYST, V10, P392, DOI 10.1007/s00530-005-0168-5
   DAVIDSON RR, 1970, J AM STAT ASSOC, V65, P317, DOI 10.2307/2283595
   Glickman ME, 1999, J ROY STAT SOC C-APP, V48, P377, DOI 10.1111/1467-9876.00159
   Lee J.-S., 2010, Proceedings of ACM international conference on Multimedia, MULTIMEDIA '10, P65, DOI DOI 10.1145/1873951.1873981
   MORAN PAP, 1947, BIOMETRIKA, V34, P363, DOI 10.1093/biomet/34.3-4.363
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Peng WH, 2008, J VIS COMMUN IMAGE R, V19, P543, DOI 10.1016/j.jvcir.2008.08.002
   Raman N, 2009, SIGNAL PROCESS-IMAGE, V24, P510, DOI 10.1016/j.image.2009.02.008
   RAO PV, 1967, J AM STAT ASSOC, V62, P194, DOI 10.2307/2282923
   Reichel J., 2007, JOINT SCALABLE VIDEO
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
NR 23
TC 72
Z9 82
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2011
VL 13
IS 5
BP 882
EP 893
DI 10.1109/TMM.2011.2157333
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 821XI
UT WOS:000295007300005
OA Green Published
DA 2024-07-18
ER

PT J
AU Peng, WT
   Chu, WT
   Chang, CH
   Chou, CN
   Huang, WJ
   Chang, WY
   Hung, YP
AF Peng, Wei-Ting
   Chu, Wei-Ta
   Chang, Chia-Han
   Chou, Chien-Nan
   Huang, Wei-Jia
   Chang, Wen-Yan
   Hung, Yi-Ping
TI Editing by Viewing: Automatic Home Video Summarization by Viewing
   Behavior Analysis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention detection; editing by viewing; emotion recognition; Interest
   Meter (IM); video summarization
ID FRAMEWORK; GENERATION; FACES
AB In this paper, we propose the Interest Meter (IM), a system making the computer conscious of user's reactions to measure user's interest and thus use it to conduct video summarization. The IM takes account of users' spontaneous reactions when they view videos. To estimate user's viewing interest, quantitative interest measures are devised based on the perspectives of attention and emotion. For estimating attention states, variations of user's eye movement, blink, and head motion are considered. For estimating emotion states, facial expression is recognized as positive or neural emotion. By combining characteristics of attention and emotion by a fuzzy fusion scheme, we transform users' viewing behaviors into quantitative interest scores, determine interesting parts of videos, and finally concatenate them as video summaries. Experimental results show that the proposed concept "editing by viewing" works well and may provide a promising direction to consider the human factor in video summarization.
C1 [Peng, Wei-Ting; Hung, Yi-Ping] Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
   [Chu, Wei-Ta] Natl Chung Cheng Univ, Dept Comp Sci & Informat Engn, Chiayi 621, Taiwan.
   [Chang, Chia-Han; Chou, Chien-Nan; Huang, Wei-Jia] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 621, Taiwan.
   [Chang, Wen-Yan] Acad Sinica, Inst Informat Sci, Taipei 115, Taiwan.
C3 National Taiwan University; National Chung Cheng University; National
   Taiwan University; Academia Sinica - Taiwan
RP Peng, WT (corresponding author), Natl Taiwan Univ, Grad Inst Networking & Multimedia, Taipei 106, Taiwan.
EM d93944004@ntu.edu.tw; wtchu@cs.ccu.edu.tw; chiahan_chang@yahoo.com.tw;
   92502093@cc.ncu.edu.tw; aga3134@gmail.com; wychang@iis.sinica.edu.tw;
   hung@csie.ntu.edu.tw
RI Chu, Wei-Ta/AAE-8471-2022
OI Chu, Wei-Ta/0000-0001-5722-7239
FU National Science Council, Taiwan [NSC 98-2221-E-002-127-MY3, NSC
   98-2221-E-002-128-MY3]; National Taiwan University [99R80300]
FX This work was supported in part by the National Science Council, Taiwan,
   under Grant NSC 98-2221-E-002-127-MY3 and Grant NSC
   98-2221-E-002-128-MY3 and by the Excellent Research Projects of National
   Taiwan University, under Grant 99R80300. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Zicheng Liu.
CR ALOAYEDI A, 1999, P INT C IM PROC ITS, V2, P625
   [Anonymous], THESIS U BRISTOL BRI
   [Anonymous], 2005, BOST U COMPUT SCI
   [Anonymous], P C BRIT MACH VIS MU
   Argyle Michael, 1975, BODILY COMMUNICATION
   ASTERIADIS S, 2006, P INT S CONTR COMM S
   Bai L, 2006, INT C PATT RECOG, P511
   Bello JP, 2005, IEEE T SPEECH AUDI P, V13, P1035, DOI 10.1109/TSA.2005.851998
   *BIOID TECHN RES, 2001, BIOIE FAC DAT
   Campadelli P., 2006, BRIT MACHINE VISION, P187
   Castrillón M, 2007, J VIS COMMUN IMAGE R, V18, P130, DOI 10.1016/j.jvcir.2006.11.004
   CHANDLER G, 2006, CUT CUT EDITING YOUR
   Chang WY, 2007, LECT NOTES COMPUT SC, V4844, P621
   CHEN HW, 2004, P ACM SIGMM INT WORK, P251
   *CYBERLINK CORP IN, CYBERLINK POWERDIREC
   Dixon S., 2006, P 9 INT C DIGITAL AU, P133
   Ekman P., 1975, UNMASKING FACE
   FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692
   Foote J., 2002, P ACM MULTIMEDIA 200, P553, DOI DOI 10.1145/641007.641119
   GOLDSTEIN RB, 2004, P C VIS SCI SOC
   Goodman R.M., 2002, Editing digital video
   GU H, 2003, P IM VIS COMP C IVCN, P154
   Hamouz M, 2005, IEEE T PATTERN ANAL, V27, P1490, DOI 10.1109/TPAMI.2005.179
   Hua X. -S., 2004, PROC 12 ANN ACM INT, P472
   Jesorsky O., 1992, AUDIO VIDEO BIOMETRI, P90
   JOHO H, 2009, P INT C IM VID RETR
   Money AG, 2008, J VIS COMMUN IMAGE R, V19, P121, DOI 10.1016/j.jvcir.2007.04.002
   Money AG, 2009, DISPLAYS, V30, P59, DOI 10.1016/j.displa.2008.12.003
   Mulhem P, 2003, IEEE MULTIMEDIA, V10, P28, DOI 10.1109/MMUL.2003.1195159
   *MUVEE TECHN PTE L, MUVEE AUTOPRODUCER
   Peng WT, 2010, IEEE INT CON MULTI, P849, DOI 10.1109/ICME.2010.5582606
   Peng WT, 2009, LECT NOTES COMPUT SC, V5371, P484, DOI 10.1007/978-3-540-92892-8_48
   Picard R. W., 1997, AFFECTIVE COMPUTING
   Shipman F, 2008, ACM T MULTIM COMPUT, V5, DOI 10.1145/1413862.1413868
   Sirohey SA, 2001, PATTERN RECOGN, V34, P1367, DOI 10.1016/S0031-3203(00)00082-0
   TAKAGI T, 1985, IEEE T SYST MAN CYB, V15, P116, DOI 10.1109/TSMC.1985.6313399
   TURKAN M, 2007, COMPUT VIS THEORY AP, P410
   Valenti Roberto., 2008, 2008 IEEE Conference on Computer Vision and Pattern Recognition, P1
   Vezhnevets V., 2003, P GRAPHICON, P81
   Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb
   Wang JJ, 2007, IEEE T MULTIMEDIA, V9, P576, DOI 10.1109/TMM.2006.888013
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
   Yoon JC, 2009, MULTIMED TOOLS APPL, V41, P197, DOI 10.1007/s11042-008-0225-0
   You JY, 2007, IEEE T CIRC SYST VID, V17, P273, DOI 10.1109/TCSVT.2007.890857
   YUILLE AL, 1992, INT J COMPUT VISION, V8, P99, DOI 10.1007/BF00127169
   Zettl H., 1998, Sight, Sound, Motion: Applied Media Aesthetics, V3rd
NR 46
TC 63
Z9 68
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2011
VL 13
IS 3
BP 539
EP 550
DI 10.1109/TMM.2011.2131638
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765UI
UT WOS:000290733700013
DA 2024-07-18
ER

PT J
AU Wang, SF
   Liu, ZL
   Lv, SL
   Lv, YP
   Wu, GB
   Peng, P
   Chen, F
   Wang, XF
AF Wang, Shangfei
   Liu, Zhilei
   Lv, Siliang
   Lv, Yanpeng
   Wu, Guobing
   Peng, Peng
   Chen, Fei
   Wang, Xufa
TI A Natural Visible and Infrared Facial Expression Database for Expression
   Recognition and Emotion Inference
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emotion inference; expression recognition; facial expression; infrared
   image; spontaneous database; visible image
ID EXPERIENCE
AB To date, most facial expression analysis has been based on visible and posed expression databases. Visible images, however, are easily affected by illumination variations, while posed expressions differ in appearance and timing from natural ones. In this paper, we propose and establish a natural visible and infrared facial expression database, which contains both spontaneous and posed expressions of more than 100 subjects, recorded simultaneously by a visible and an infrared thermal camera, with illumination provided from three different directions. The posed database includes the apex expressional images with and without glasses. As an elementary assessment of the usability of our spontaneous database for expression recognition and emotion inference, we conduct visible facial expression recognition using four typical methods, including the eigenface approach [principle component analysis (PCA)], the fisherface approach [PCA + linear discriminant analysis (LDA)], the Active Appearance Model (AAM), and the AAM-based + LDA. We also use PCA and PCA+LDA to recognize expressions from infrared thermal images. In addition, we analyze the relationship between facial temperature and emotion through statistical analysis. Our database is available for research purposes.
C1 [Wang, Shangfei; Liu, Zhilei; Lv, Siliang; Lv, Yanpeng; Wu, Guobing; Peng, Peng; Chen, Fei; Wang, Xufa] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Wang, SF (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei 230027, Peoples R China.
EM sfwang@ustc.edu.cn; leivo@mail.ustc.edu.cn; lsliang@mail.ustc.edu.cn;
   lvyp@mail.ustc.edu.cn; guobing@mail.ustc.edu.cn;
   dbpeng@mail.ustc.edu.cn; feichen@mail.ustc.edu.cn; xfwang@ustc.edu.cn
RI Liu, Zhilei/B-3733-2015; Peng, Peng/Y-6644-2019
OI Liu, Zhilei/0000-0003-1447-6256; Peng, Peng/0000-0002-6506-1184
FU National 863 Program [2008AA01Z122]; Anhui Provincial Natural Science
   Foundation [070412056]; SRF for ROCS, SEM
FX Manuscript received December 13, 2009; revised March 24, 2010 and June
   23, 2010; accepted June 24, 2010. Date of publication July 26, 2010;
   date of current version October 15, 2010. This paper is supported in
   part by National 863 Program (2008AA01Z122), in part by Anhui Provincial
   Natural Science Foundation (No. 070412056), and in part bySRF for ROCS,
   SEM. The associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Caifeng Shan.
CR [Anonymous], MACHINE ANAL FACIAL
   [Anonymous], 1991, P 1991 IEEE COMP SOC, DOI DOI 10.1109/CVPR.1991.139758
   [Anonymous], 2007, HDB EMOTION ELICITAT
   Bartlett MS, 2005, PROC CVPR IEEE, P568
   Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228
   Chung S. J., 2000, L'expression et la perception de l'emotion extraite de la parole spontanee: evidences du coreen et de l'anglais
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   *D5I, 2008, D5I FIN REP WP5
   Delac K, 2005, INT J IMAG SYST TECH, V15, P252, DOI 10.1002/ima.20059
   Douglas-Cowie E., 2003, P 15 INT C PHONETIC, P2877
   Fasel B, 2003, PATTERN RECOGN, V36, P259, DOI 10.1016/S0031-3203(02)00052-3
   Gajsek R, 2009, INFORM-J COMPUT INFO, V33, P101
   Jenkins S, 2009, INT J DES, V3, P53
   Khan MM, 2006, ACM T AUTON ADAP SYS, V1, P91, DOI 10.1145/1152934.1152939
   Khan MM, 2009, ACM T APPL PERCEPT, V6, DOI 10.1145/1462055.1462061
   KRZYWICKI AT, 2009, P SPIE INT SOC OPTIC, V7343, P1
   Merla A, 2007, P ANN INT IEEE EMBS, P247, DOI 10.1109/IEMBS.2007.4352270
   Nhan BR, 2009, PHYSIOL MEAS, V30, pN23, DOI 10.1088/0967-3334/30/4/N01
   O'Toole AJ, 2005, IEEE T PATTERN ANAL, V27, P812, DOI 10.1109/TPAMI.2005.90
   Roisman GI, 2004, DEV PSYCHOL, V40, P776, DOI 10.1037/0012-1649.40.5.776
   Scherer KR, 1997, MOTIV EMOTION, V21, P211, DOI 10.1023/A:1024498629430
   Sebe N., 2004, P 6 IEEE INT C AUT F
   Shan SG, 2003, IEEE INTERNATIONAL WORKSHOP ON ANALYSIS AND MODELING OF FACE AND GESTURES, P157
   SUGIMOTO Y, 2000, ROBOT AUTONOMOUS SYS, V31
   TANAKA H, 2000, P 2000 IEEE INT C SY, V1, P1265
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
NR 26
TC 238
Z9 268
U1 3
U2 53
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2010
VL 12
IS 7
BP 682
EP 691
DI 10.1109/TMM.2010.2060716
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 670SV
UT WOS:000283448500006
DA 2024-07-18
ER

PT J
AU Lin, GS
   Chang, YT
   Lie, WN
AF Lin, Guo-Shiang
   Chang, Yi-Ting
   Lie, Wen-Nung
TI A Framework of Enhancing Image Steganography With Picture Quality
   Optimization and Anti-Steganalysis Based on Simulated Annealing
   Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Data embedding; human visual system; image watermarking; steganalysis;
   steganography
ID VIDEO
AB Picture quality and statistical undetectability are two key issues related to steganography techniques. In this paper, we propose a closed-loop computing framework that iteratively searches proper modifications of pixels/coefficients to enhance a base steganographic scheme with optimized picture quality and higher anti-steganalysis capability. To achieve this goal, an anti-steganalysis tester and an embedding controller-based on the simulated annealing (SA) algorithm with a proper cost function-are incorporated into the processing loop to conduct the convergence of searches. The cost function integrates several performance indices, namely, the mean square error, the human visual system (HVS) deviation, and the differences in statistical features, and guides a proper direction of searches during SA optimization. Our proposed framework is suitable for the kind of steganographic schemes that spreads each message information into multiple pixels/coefficients. We have selected two base steganographic schemes for implementation to show the applicability of the proposed framework. Experiment results show that the base schemes can be enhanced with better performances in image PSNR (by more than 5.0 dB), file-size variation, and anti-steganalysis pass-rate (by about 10% similar to 86%, at middle to high embedding capacities).
C1 [Lin, Guo-Shiang] Da Yeh Univ, Dept Comp Sci & Informat Engn, Changhua 51591, Taiwan.
   [Chang, Yi-Ting; Lie, Wen-Nung] Natl Chung Cheng Univ, Dept Elect Engn, Chiayi 621, Taiwan.
C3 Da Yeh University; National Chung Cheng University
RP Lin, GS (corresponding author), Da Yeh Univ, Dept Comp Sci & Informat Engn, Changhua 51591, Taiwan.
EM khlin@mail.dyu.edu.tw; spear4703@yahoo.com.tw; wnlie@ee.ccu.edu.tw
RI Lie, Wen-Nung/AFP-1266-2022
CR Avcibas I, 2003, IEEE T IMAGE PROCESS, V12, P221, DOI 10.1109/TIP.2002.807363
   Barni M, 2004, IEEE SIGNAL PROC MAG, V21, P28, DOI 10.1109/MSP.2004.1276109
   Budhia U, 2006, IEEE T INF FOREN SEC, V1, P502, DOI 10.1109/TIFS.2006.885020
   Chandramouli R, 2002, PROC SPIE, V4675, P14, DOI 10.1117/12.465273
   Cheng Q, 2001, IEEE T MULTIMEDIA, V3, P273, DOI 10.1109/6046.944472
   Comesaña P, 2007, EURASIP J INF SECUR, DOI 10.1155/2007/25308
   Damera-Venkata N, 2000, IEEE T IMAGE PROCESS, V9, P636, DOI 10.1109/83.841940
   Duda R. O., 2000, PATTERN CLASSIFICATI
   Fridrich J, 2002, PROC SPIE, V4675, P1, DOI 10.1117/12.465263
   HARMSEN JJ, 2003, P SOC PHOTO-OPT INS, V5020, P21
   Huang CH, 2008, IEEE T MULTIMEDIA, V10, P557, DOI 10.1109/TMM.2008.921733
   Ker AD, 2007, IEEE T INF FOREN SEC, V2, P46, DOI 10.1109/TIFS.2006.890519
   Kharrazi M, 2006, IEEE IMAGE PROC, P117, DOI 10.1109/ICIP.2006.312386
   Kong XW, 2002, LECT NOTES COMPUT SC, V2532, P434
   Levicky D, 2004, RADIOENGINEERING, V13, P38
   Lie WN, 2006, IEEE T INF FOREN SEC, V1, P330, DOI 10.1109/TIFS.2006.879297
   Lie WN, 2005, IEEE T MULTIMEDIA, V7, P1007, DOI 10.1109/TMM.2005.858377
   Lin B, 2008, IEEE SIGNAL PROC LET, V15, P793, DOI 10.1109/LSP.2008.2005815
   Lin CY, 2001, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P90, DOI 10.1109/ITCC.2001.918771
   Luo WB, 2002, FIFTH IEEE SOUTHWEST SYMPOSIUM ON IMAGE ANALYSIS AND INTERPRETATION, PROCEEDINGS, P158, DOI 10.1109/IAI.2002.999910
   Lyu SW, 2006, IEEE T INF FOREN SEC, V1, P111, DOI 10.1109/TIFS.2005.863485
   Miller ML, 2004, IEEE T IMAGE PROCESS, V13, P792, DOI 10.1109/TIP.2003.821551
   Ogihara T., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P675, DOI 10.1109/ICPR.1996.546908
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   Petrowski K, 2005, INT CONF ACOUST SPEE, P537
   PEVNY T, 2006, P INF SEC, V153, P77
   Ram DJ, 1996, J PARALLEL DISTR COM, V37, P207, DOI 10.1006/jpdc.1996.0121
   Seki Y, 2005, IEEE INT SYMP CIRC S, P4987, DOI 10.1109/ISCAS.2005.1465753
   Sonka M., 2008, IMAGE PROCESSING ANA
   Upham D., 1997, JSTEG
   Westfeld A., 2001, P 4 INT WORKSHOP INF, V2137, P289
   Wu M, 2003, IEEE T IMAGE PROCESS, V12, P685, DOI 10.1109/TIP.2003.810588
   Wu YT, 2006, IEEE T SYST MAN CY B, V36, P24, DOI 10.1109/TSMCB.2005.852474
   Zhang T., 2003, SAC 03, P307, DOI DOI 10.1145/952532.952595
NR 34
TC 40
Z9 41
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2010
VL 12
IS 5
BP 345
EP 357
DI 10.1109/TMM.2010.2051243
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 656DN
UT WOS:000282306500001
DA 2024-07-18
ER

PT J
AU Wang, HG
   Hempel, M
   Peng, DM
   Wang, W
   Sharif, H
   Chen, HH
AF Wang, Honggang
   Hempel, Michael
   Peng, Dongming
   Wang, Wei
   Sharif, Hamid
   Chen, Hsiao-Hwa
TI Index-Based Selective Audio Encryption for Wireless Multimedia Sensor
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio streaming; modified discrete cosine transform; security; wireless
   sensor network
ID TRANSMISSION
AB Wireless multimedia sensor networks (WMSNs) support many acoustic applications for audio surveillance, animal tracking/vocalization, human health monitoring, etc. However, resource constraints in sensor networks (such as limited battery power, bandwidth/computation capability, etc.) pose challenges for the quality and security of audio data transmission and processing. The security is a critical issue since audio information can be accessed or even manipulated in WMSNs. In order to ensure security, audio quality and energy efficiency, we propose an index-based selective audio encryption scheme for WMSNs. The scheme protects data transmissions by incorporating both resource allocation and selective encryption based on modified discrete cosine transform (MDCT). In this proposed scheme, the audio data importance is leveraged using the MDCT audio index, and wireless audio data transmission proceeds with energy efficient selective encryption. The simulation results show that the proposed approach offers a significant gain in terms of energy efficiency, encryption performance and audio transmission quality.
C1 [Wang, Honggang] Univ Massachusetts, Dept Elect & Comp Engn, Dartmouth, MA 02747 USA.
C3 University of Massachusetts System; University Massachusetts Dartmouth
RP Wang, HG (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, Dartmouth, MA 02747 USA.
EM hwang1@umassd.edu; mhempel@mail.unomaha.edu; dpeng@unl.edu;
   wei.wang@sdstate.edu; hsharif@unl.edu; hshwchen@ieee.org
RI Sharif, Haidar/AAR-6783-2021; Wang, Honggang/D-6079-2013
OI Sharif, Haidar/0000-0001-7235-6004; Wang, Honggang/0000-0001-9475-2630;
   Sharif-Kashani, Hamid/0000-0001-6229-2043; Hempel,
   Michael/0000-0002-7091-8349
CR Alattar A. M., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P256, DOI 10.1109/ICIP.1999.819590
   Alattar AM, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, P340, DOI 10.1109/ISCAS.1999.780011
   Britanak V, 2002, SIGNAL PROCESS, V82, P433, DOI 10.1016/S0165-1684(01)00195-5
   Cheng H, 2000, IEEE T SIGNAL PROCES, V48, P2439, DOI 10.1109/78.852023
   CHENG H, 1998, THESIS U ALBERTA EDM
   DAM TV, 2003, P ACM SENS 03 LOS AN
   Ganesan P., 2003, CM International Workshop on Wireless Sensor networks and Applications, P151, DOI DOI 10.1145/941350.941372
   HELLERUD E, 2006, P IEEE ICDT, P30
   Hoos H., 2005, Stochastic Local Search
   Khalifeh A, 2008, IEEE WCNC, P3191
   Perkins C, 1998, IEEE NETWORK, V12, P40, DOI 10.1109/65.730750
   PROCYK TJ, 1979, AUTOMATICA, V15, P15, DOI 10.1016/0005-1098(79)90084-0
   Servetti A, 2002, IEEE T SPEECH AUDI P, V10, P637, DOI 10.1109/TSA.2002.804300
   SINHA D, 1999, P ICASSP 1999 MAR, V5, P2423
   SPANOS GA, 1996, P C COMP COMM MAR, P72
   Wah BW, 2000, INTERNATIONAL SYMPOSIUM ON MULTIMEDIA SOFTWARE ENGINEERING, PROCEEDINGS, P17, DOI 10.1109/MMSE.2000.897185
   Wang HG, 2007, GLOB TELECOMM CONF, P971
   Wang HG, 2009, IEEE T WIREL COMMUN, V8, P757, DOI 10.1109/TWC.2009.070769
   WANG W, 2007, SPECIAL ISSUE DISTRI
   Wang W, 2008, IEEE T MULTIMEDIA, V10, P1169, DOI 10.1109/TMM.2008.2001354
   Wang W, 2007, GLOB TELECOMM CONF, P976
   Wang W, 2007, IEEE WCNC, P4094
   Wang W, 2009, WIREL COMMUN MOB COM, V9, P383, DOI 10.1002/wcm.550
   WANG Y, P 11 ACM INT C MULT
   WOOD A, P INT C EMB NETW SEN, V1, P395
   Wu ZY, 2005, IEEE T COMMUN, V53, P1648, DOI 10.1109/TCOMM.2005.857142
   YUNG CW, 1999, P IEEE ISCAS JUL, P342
NR 27
TC 38
Z9 41
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2010
VL 12
IS 3
BP 215
EP 223
DI 10.1109/TMM.2010.2041102
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 570IN
UT WOS:000275666900006
DA 2024-07-18
ER

PT J
AU Hu, HS
   Zhou, MC
   Li, ZW
AF Hu, HeSuan
   Zhou, MengChu
   Li, ZhiWu
TI Liveness Enforcing Supervision of Video Streaming Systems Using
   Nonsequential Petri Nets
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deadlock prevention; discrete event system; Petri nets; resource
   allocation system; video streaming systems
ID TERM SCHEDULABILITY ANALYSIS; RESOURCE-ALLOCATION SYSTEMS; DEADLOCK
   PREVENTION POLICY; CRUDE-OIL OPERATIONS; ELEMENTARY SIPHONS; INTERNET;
   SYNCHRONIZATION; AVOIDANCE; ALGORITHM; REFINERY
AB Internet-motivated video streaming systems face such complicated issues as a high degree of network-resource sharing amongst many flows, which potentially leads to deadlocks. Using siphons and their corresponding dangerous markings, this work investigates a method to enforce control iteratively. At each iteration, a generalized mutual exclusion constraint is produced to keep only those markings under which liveness is enforced. Furthermore, a generalized elementary siphon control method is proposed such that the final supervisor is structurally simple. Examples are used to illustrate the proposed approach.
C1 [Hu, HeSuan; Zhou, MengChu] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
   [Hu, HeSuan; Zhou, MengChu; Li, ZhiWu] Xidian Univ, Sch Electromech Engn, Xian 710071, Shaanxi, Peoples R China.
C3 New Jersey Institute of Technology; Xidian University
RP Hu, HS (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM huhesuan@gmail.com; zhou@njit.edu; zhwli@xidian.edu.cn
RI Zhou, MengChu/H-9897-2014; Li, Zhiwu/A-7884-2010
OI Li, Zhiwu/0000-0003-1547-5503
FU Natural Science Foundation of China [60474018, 60773001]; ChangJiang
   Scholars Program, PRC Ministry of Education
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 60474018 and 60773001 and in part by the ChangJiang
   Scholars Program, PRC Ministry of Education. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Jiangchuan (JC) Liu.
CR Albanese M, 2008, IEEE T MULTIMEDIA, V10, P982, DOI 10.1109/TMM.2008.2001369
   Barkaoui Kamel, 1996, LECT NOTES COMPUTER, V1091, P57
   Chen JC, 2004, IEEE J SEL AREA COMM, V22, P1920, DOI 10.1109/JSAC.2004.836000
   Chen M, 2008, IEEE T MULTIMEDIA, V10, P910, DOI 10.1109/TMM.2008.922846
   DALLY WJ, 1993, IEEE T PARALL DISTR, V4, P466, DOI 10.1109/71.219761
   DIAZ M, 1993, P 1 INT C MULT MOD, P257
   Ding ZJ, 2008, IEEE T SYST MAN CY A, V38, P791, DOI 10.1109/TSMCA.2008.923064
   Ding ZJ, 2008, IEEE T SYST MAN CY B, V38, P881, DOI 10.1109/TSMCB.2008.917177
   Du YY, 2008, IEEE T SYST MAN CY C, V38, P93, DOI 10.1109/TSMCC.2007.896995
   EZPELETA J, 1995, IEEE T ROBOTIC AUTOM, V11, P173, DOI 10.1109/70.370500
   Guan SU, 1998, IEEE T COMPUT, V47, P477, DOI 10.1109/12.675716
   Hruz B., 2007, MODELING CONTROL DIS
   Huang YS, 2001, INT J PROD RES, V39, P283, DOI 10.1080/00207540010002405
   KUROSE JF, 1989, IEEE T COMPUT, V38, P705, DOI 10.1109/12.24272
   Lautenbach K., 1996, Technical Report
   Lee JS, 2008, IEEE T SYST MAN CY A, V38, P493, DOI 10.1109/TSMCA.2007.914747
   Li Z.W., 2009, Deadlock Resolution in Automated Manufacturing Systems: A Novel Petri Net Approach
   Li ZW, 2008, IEEE T SYST MAN CY A, V38, P667, DOI 10.1109/TSMCA.2008.918605
   Li ZW, 2008, IEEE T SYST MAN CY C, V38, P173, DOI 10.1109/TSMCC.2007.913920
   Li ZW, 2008, IEEE T AUTOM SCI ENG, V5, P182, DOI 10.1109/TASE.2006.884674
   Li ZW, 2008, IEEE T SYST MAN CY A, V38, P133, DOI 10.1109/TSMCA.2007.909548
   Li ZW, 2006, IEEE T IND INFORM, V2, P313, DOI 10.1109/TII.2006.885185
   Li ZW, 2004, IEEE T SYST MAN CY A, V34, P38, DOI 10.1109/TSMCA.2003.820576
   LITTLE TDC, 1990, IEEE J SEL AREA COMM, V8, P413, DOI 10.1109/49.53017
   López-Grao JP, 2006, LECT NOTES COMPUT SC, V4229, P323
   López-Grao JP, 2006, IEEE SYS MAN CYBERN, P3052, DOI 10.1109/ICSMC.2006.384584
   Moody JO, 2000, IEEE T AUTOMAT CONTR, V45, P462, DOI 10.1109/9.847725
   Nguyen T, 2008, IEEE T MULTIMEDIA, V10, P523, DOI 10.1109/TMM.2008.917351
   Park J, 2001, IEEE T AUTOMAT CONTR, V46, P1572, DOI 10.1109/9.956052
   Reveliotis SA, 2003, LECT NOTES COMPUT SC, V2679, P241
   Reveliotis SA, 2007, IEEE T SYST MAN CY A, V37, P319, DOI 10.1109/TSMCA.2007.893461
   SENAC P, 1994, ANN TELECOMMUN, V49, P297
   Su SC, 2008, IEEE T MULTIMEDIA, V10, P1197, DOI 10.1109/TMM.2008.2001366
   Tan R, 2005, IEEE T MULTIMEDIA, V7, P869, DOI 10.1109/TMM.2005.854377
   Tricas F, 2005, IEEE INT CONF ROBOT, P271
   van der Aalst WMP, 2003, LECT NOTES COMPUT SC, V2678, P1
   Wang AR, 2009, IEEE T SYST MAN CY A, V39, P912, DOI 10.1109/TSMCA.2009.2019880
   Woo P.T. K., 1994, PARASITIC PROTOZOA, VVIII., P1, DOI DOI 10.1016/B978-0-08-092414-4.50006-5
   Wu DP, 2000, P IEEE, V88, P1855, DOI 10.1109/5.899055
   Wu DP, 2001, IEEE T CIRC SYST VID, V11, P282, DOI 10.1109/76.911156
   Wu NQ, 2008, IEEE T SYST MAN CY C, V38, P765, DOI 10.1109/TSMCC.2008.2001688
   Wu NQ, 2008, IEEE T AUTOM SCI ENG, V5, P661, DOI 10.1109/TASE.2008.916737
   Wu NQ, 2008, IEEE T SYST MAN CY A, V38, P56, DOI 10.1109/TSMCA.2007.909542
   Wu N, 2009, IEEE T SYST MAN CY C, V39, P1, DOI 10.1109/TSMCC.2008.2001709
   Xing KY, 2009, IEEE T SYST MAN CY A, V39, P188, DOI 10.1109/TSMCA.2008.2007947
   Xiong PC, 2008, IEEE T SYST MAN CY A, V38, P888, DOI 10.1109/TSMCA.2008.923062
   Yamalidou K, 1996, AUTOMATICA, V32, P15, DOI 10.1016/0005-1098(95)00103-4
   Yoon K, 1998, INTERNATIONAL WORKSHOP ON MULTI-MEDIA DATABASE MANAGEMENT SYSTEMS- PROCEEDINGS, P136, DOI 10.1109/MMDBMS.1998.709776
   ZHOU MC, 1992, IEEE T ROBOTIC AUTOM, V8, P350, DOI 10.1109/70.143353
   Zhou MC., 1998, MODELING SIMULATION
   Zhou Meng., 1993, Petri Net Synthesis for Discrete Event Control of Manufacturing Systems
   Zhovtobryukh D, 2007, SIMUL-T SOC MOD SIM, V83, P33, DOI 10.1177/0037549707079226
NR 52
TC 69
Z9 71
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1457
EP 1465
DI 10.1109/TMM.2009.2032678
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000006
DA 2024-07-18
ER

PT J
AU Ren, DN
   Li, YTH
   Chan, SHG
AF Ren, Dongni
   Li, Yui-Tung Hillman
   Chan, S. -H. Gary
TI Fast-Mesh: A Low-Delay High-Bandwidth Mesh for Peer-to-Peer Live
   Streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Minimize mesh delay; multimedia communication; P2P live streaming
AB Peer-to-peer (P2P) technology has emerged as a promising scalable solution for live streaming to a large group. In this paper, we address the design of an overlay mesh which achieves low source-to-peer delay, accommodates asymmetric and diverse uplink bandwidth, and continuously improves delay based on an existing pool of peers. By considering a streaming mesh as an aggregation of data flows along multiple spanning trees, the peer delay in the mesh is then its longest delay (including both propagation and scheduling delay) among all the trees. Clearly, such delay can be very high if the mesh is not designed well. In this paper, we propose and study a mesh protocol called Fast-Mesh, which optimizes such delay while meeting a certain streaming bandwidth requirement. Fast-Mesh is particularly suitable for a mildly dynamic network consisting of proxies, supernodes, or content distribution servers.
   We first formulate the minimum delay multiple trees (MDMT) problem and show that it is NP-hard. Then we propose a centralized heuristic based on complete knowledge, which may be used when the network is small or managed, and serves as an optimal benchmark for all the other schemes under comparison. We then propose a simple distributed algorithm, Fast-Mesh, where peers select their parents based on the concept of power in networks given by the ratio of throughput and delay. By maximizing the network power, our algorithm achieves low delay. The algorithm makes continuous improvement on delay until some minimum delay is reached. Simulation and PlanetLab experiments show that our distributed algorithm performs very well in terms of delay and source workload, and substantially outperforms traditional and state-of-the art approaches.
C1 [Ren, Dongni; Li, Yui-Tung Hillman; Chan, S. -H. Gary] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Ren, DN (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM tonyren@cse.ust.hk; hillmanl@cse.ust.hk; gchan@cse.ust.hk
OI Chan, Gary Shueng Han/0000-0003-4207-764X
CR Banerjee S, 2002, ACM SIGCOMM COMP COM, V32, P205, DOI 10.1145/964725.633045
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   CHAN CY, 2004, P 4 IEEE ACM INT S C
   Chi HC, 2007, IEEE J SEL AREA COMM, V25, P119, DOI 10.1109/JSAC.2007.070112
   Chu YH, 2002, IEEE J SEL AREA COMM, V20, P1456, DOI 10.1109/JSAC.2002.803066
   Hei X, 2006, P IPTV WORKSH INT WO
   Hei XJ, 2007, IEEE T MULTIMEDIA, V9, P1672, DOI 10.1109/TMM.2007.907451
   Kostic D., 2003, Operating Systems Review, V37, P282, DOI 10.1145/1165389.945473
   Magharei N, 2007, IEEE INFOCOM SER, P1424
   Medina A., 2001, P MASCOTS 01 JAN
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Pai V., 2005, P 4 INT WORKSH PEER
   Pendarakis D, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P49
   REN D, 2008, P IEEE INFOCOM PHOEN
   Setton E, 2006, IEEE IMAGE PROC, P721, DOI 10.1109/ICIP.2006.312442
   Setton E, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P569, DOI 10.1109/ICME.2006.262472
   Silverston T., 2007, P 17 INT WORKSH NETW
   Small T, 2007, IEEE J SEL AREA COMM, V25, P35, DOI 10.1109/JSAC.2007.070105
   SRIPANIDKULCHAI K, 2004, P ACM SIGCOMM, P107, DOI DOI 10.1145/1030194.1015480
   Sripanidkulchai K., 2004, Proc. ACM Internet Measurement Conference, P41
   Tran DA, 2004, IEEE J SEL AREA COMM, V22, P121, DOI 10.1109/JSAC.2003.818803
   Veloso E, 2006, IEEE ACM T NETWORK, V14, P133, DOI 10.1109/TNET.2005.863709
   WANG F., 2008, P IEEE INFOCOM PHOEN
   WU C, 2008, P IEEE INFOCOM PHOEN
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
NR 25
TC 27
Z9 33
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2009
VL 11
IS 8
BP 1446
EP 1456
DI 10.1109/TMM.2009.2032677
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 520KL
UT WOS:000271843000005
DA 2024-07-18
ER

PT J
AU Huang, YS
   Mao, SW
   Midkiff, SF
AF Huang, Yingsong
   Mao, Shiwen
   Midkiff, Scott F.
TI A Control-Theoretic Approach to Rate Control for Streaming Videos
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Congestion control; feedback control; rate control; stability; video
   streaming
ID RATE-CONTROL SCHEME; LAGRANGE MULTIPLIER; TRANSMISSION; ALLOCATION;
   DISTORTION; TRANSPORT; STABILITY; SYSTEMS; DESIGN
AB As streaming videos are becoming increasingly popular, it is important to understand the end-to-end streaming system and to develop effective algorithms for quality control. In this paper, we address the problem of rate control for streaming videos with a control-theoretic approach. Among the various control knobs, video bit rate is one of the most effective in the sense that it has a direct impact on the interaction between the video coder and network system. While increasing rate reduces the coder-induced distortion, it may also cause congestion at a bottleneck link. The packet loss due to congestion will, then, increase the distortion of the decoded video. We model end-to-end video steaming as a feedback control system, taking into account video codec and sequence characteristics, rate control, active queue management, and receiver feedback. We then develop effective proportional (P) controllers to stabilize the received video quality as well as the bottleneck link queue, for both homogeneous and heterogeneous video systems. Simulation results are presented to demonstrate the efficacy of the P controllers and the viability of the proposed control-theoretic approach.
C1 [Huang, Yingsong; Mao, Shiwen] Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
   [Midkiff, Scott F.] Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
C3 Auburn University System; Auburn University; Virginia Polytechnic
   Institute & State University
RP Huang, YS (corresponding author), Auburn Univ, Dept Elect & Comp Engn, Auburn, AL 36849 USA.
EM yzh0002@auburn.edu; smao@ieee.org; midkiff@vt.edu
RI Mao, Shiwen/AAY-4471-2020; Midkiff, Scott/U-9596-2019
OI Midkiff, Scott/0000-0003-4933-7360
FU National Science Foundation (NSF) [ECCS-0802113]; Wireless Internet
   Center for Advanced Technology at Auburn University; Div Of Electrical,
   Commun & Cyber Sys; Directorate For Engineering [0802113] Funding
   Source: National Science Foundation
FX This work was supported in part by the National Science Foundation (NSF)
   under Grant ECCS-0802113 and in part through the Wireless Internet
   Center for Advanced Technology at Auburn University. This material was
   also based on work supported in part by the NSF, while S. F. Midkiff was
   working at the Foundation.
CR Ahmad I, 2006, IEEE T CIRC SYST VID, V16, P209, DOI 10.1109/TCSVT.2005.856899
   Allman M, 2000, ACM SIGCOMM COMP COM, V30, P10, DOI 10.1145/505672.505674
   Aramvith S, 2001, IEEE T CIRC SYST VID, V11, P569, DOI 10.1109/76.920187
   Balan HV, 2007, IEEE INFOCOM SER, P2009, DOI 10.1109/INFCOM.2007.233
   Black D.C., 2004, SystemC: From the Ground Up
   BRADEN B, 1998, 2309 IETF RFC
   Carter WPL, 1996, INT J CHEM KINET, V28, P497, DOI 10.1002/(SICI)1097-4601(1996)28:7<497::AID-KIN4>3.3.CO;2-7
   CHOI JH, 1994, IEEE T IMAGE PROCESS, V3, P546, DOI 10.1109/83.334986
   CI S, 2008, ADV MULTIMEDIA, P1
   Ding W, 1997, IEEE T CIRC SYST VID, V7, P266, DOI 10.1109/76.564106
   He ZH, 2002, IEEE T CIRC SYST VID, V12, P970, DOI 10.1109/TCSVT.2002.805511
   Hollot CV, 2002, IEEE T AUTOMAT CONTR, V47, P945, DOI 10.1109/TAC.2002.1008360
   Hollot CV, 2001, IEEE INFOCOM SER, P1510, DOI 10.1109/INFCOM.2001.916647
   Jiang MQ, 2006, IEEE T CIRC SYST VID, V16, P663, DOI 10.1109/TCSVT.2006.873159
   Johan Astrom K., 2021, FEEDBACK SYSTEMS INT
   Katabi D, 2002, ACM SIGCOMM COMP COM, V32, P89, DOI 10.1145/964725.633035
   Kompella S, 2007, IEEE J SEL AREA COMM, V25, P831, DOI 10.1109/JSAC.2007.070518
   Kompella S, 2009, IEEE ACM T NETWORK, V17, P212, DOI 10.1109/TNET.2008.925942
   LINDLEY DV, 1952, P CAMB PHILOS SOC, V48, P277, DOI 10.1017/S0305004100027638
   Liu Y, 2007, IEEE T CIRC SYST VID, V17, P68, DOI 10.1109/TCSVT.2006.887081
   Ma SW, 2005, IEEE T CIRC SYST VID, V15, P1533, DOI 10.1109/TCSVT.2005.857300
   Mao SW, 2006, IEEE T VEH TECHNOL, V55, P317, DOI 10.1109/TVT.2005.861208
   Neely MJ, 2005, IEEE J SEL AREA COMM, V23, P89, DOI 10.1109/JSAC.2004.837349
   Oottamakorn C, 2006, IEEE T MULTIMEDIA, V8, P1209, DOI 10.1109/TMM.2006.884613
   Ramakrishnan K., 2001, 3168 RFC
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Tai CH, 2008, IEEE INFOCOM SER, P201
   TASSIULAS L, 1992, IEEE T AUTOMAT CONTR, V37, P1936, DOI 10.1109/9.182479
   Wang LJ, 2007, COMPUT NETW, V51, P4475, DOI 10.1016/j.comnet.2007.06.022
   Wang Y, 2006, IEEE T CIRC SYST VID, V16, P716, DOI 10.1109/TCSVT.2006.875203
   Wu DL, 2007, IEEE J SEL AREA COMM, V25, P841, DOI 10.1109/JSAC.2007.070519
   Zhang Q, 2008, P IEEE, V96, P64, DOI 10.1109/JPROC.2007.909930
NR 32
TC 35
Z9 37
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2009
VL 11
IS 6
BP 1072
EP 1081
DI 10.1109/TMM.2009.2026085
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 493YW
UT WOS:000269776700004
DA 2024-07-18
ER

PT J
AU Argyriou, A
AF Argyriou, Antonios
TI Cross-Layer Error Control for Multimedia Streaming in Wireless/Wireline
   Packet Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 802.11 wireless LAN; ARQ; cross-layer design; FEC; hybrid wireless/wired
   network; wireless multimedia streaming
ID VIDEO TRANSMISSION; CHANNEL
AB In this paper, we propose a cross-layer error control framework for robust and low delay multimedia streaming in tandem-connected IEEE 802.11 wireless LANs and the Internet. For this network configuration, we model the end-to-end delay and packet loss rate as a function of the automatic repeat request (ARQ) and forward error correction (FEC) error control mechanisms that are employed at the application and wireless link layers. The analytical model is used as the basis of a delay-constrained error control algorithm that adapts the protection level at the application and link layers so that the end-to-end packet loss rate is minimized. With extensive simulations, we validate the efficiency of the proposed cross-layer error control methodology for delay-sensitive pre-compressed video streaming.
C1 Philips Res, NL-5656 AE Eindhoven, Netherlands.
C3 Philips; Philips Research
RP Argyriou, A (corresponding author), Philips Res, NL-5656 AE Eindhoven, Netherlands.
EM anargyr@ieee.org
RI Argyriou, Antonios/AAF-9586-2021; Argyriou, Antonios/J-5170-2012
OI Argyriou, Antonios/0000-0002-2510-3124; Argyriou,
   Antonios/0000-0002-2510-3124
CR [Anonymous], 1999, 80211 IEEE WG 11
   [Anonymous], MSRTR200135
   [Anonymous], 2005, 80211ED130 IEEE
   [Anonymous], 1999, IEEE Std. 802.11a
   [Anonymous], JVT REF SOFTW
   BOLOT JC, 1999, INFOCOM
   Borgonovo F, 2005, IEEE T VEH TECHNOL, V54, P246, DOI 10.1109/TVT.2004.838823
   Frossard P, 2001, IEEE T IMAGE PROCESS, V10, P1815, DOI 10.1109/83.974566
   HARTANTO F, 1999, IEEE 10 WORKSH LOC M
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   LIANG YJ, 2002, 36 AS C SIGN SYST CO
   MUKHERJEE A, 1992, MSCIS9283 U PENNS PH
   ROSENBERG J, 2000, INFOCOM
   SEFEROGLU H, 2005, INT C IM PROC
   Sun M.-T., 2001, COMPRESSED VIDEO NET
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   van der Schaar M., 2007, MULTIMEDIA IP WIRELE
   WANG CH, 2003, GLOB TEL C GLOBECOM, V6, P3361
   Wang HS, 1996, IEEE T VEH TECHNOL, V45, P353, DOI 10.1109/25.492909
   WEI HC, 2004, ISCAS
   Zhai F, 2006, IEEE T IMAGE PROCESS, V15, P40, DOI 10.1109/TIP.2005.860353
NR 21
TC 29
Z9 37
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2008
VL 10
IS 6
BP 1121
EP 1127
DI 10.1109/TMM.2008.2001371
PG 7
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 371WE
UT WOS:000260862600015
DA 2024-07-18
ER

PT J
AU Boato, G
   De Natale, FGB
   Fontanari, C
AF Boato, G.
   De Natale, F. G. B.
   Fontanari, C.
TI A multilevel asymmetric scheme for digital fingerprinting
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE asymmetric watermarking; digital fingerprinting; polynomial
   interpolation
AB The present paper proposes an asymmetric watermarking scheme suitable for fingerprinting and precision-critical applications. The method is based on linear algebra and is proved to be secure under projection attack. The problem of anonymous fingerprinting is also addressed, by allowing a client to get a watermarked image from a server without revealing her own identity. In particular, we consider the specific scenario where the client is a structured organization being trusted as a whole but involving possibly untrusted members. In such a context, where the watermarked copy can be made available to all members, but only authorized subgroups should be able to remove the watermark and recover a distortion-free image, a multilevel access to the embedding key is provided by applying Birkhoff polynomial interpolation. Extensive simulations demonstrate the robustness of the proposed method against standard image degradation operators.
C1 [Boato, G.; De Natale, F. G. B.] Politecn Torino, Dept Informat Engn & Comp Sci, I-10129 Turin, Italy.
   [Fontanari, C.] Politecn Torino, Dept Math, Sch Informat Technol, I-10129 Turin, Italy.
C3 Polytechnic University of Turin; Polytechnic University of Turin
RP Boato, G (corresponding author), Politecn Torino, Multimedia Signal Proc & Understanding Lab, I-10129 Turin, Italy.
EM boato@disi.unitn.it; denatale@disi.unitn.it; claudio.fontanari@polito.it
CR Barni M, 2003, SIGNAL PROCESS, V83, P2069, DOI 10.1016/S0165-1684(03)00168-3
   Barni M., 2004, SIGNAL PROCESSING CO
   BOATA G, 2005, P ICIP 2005 GEN IT S
   Boato G, 2006, IEEE T SIGNAL PROCES, V54, P2833, DOI 10.1109/TSP.2006.874411
   Celik MU, 2005, IEEE T IMAGE PROCESS, V14, P253, DOI 10.1109/TIP.2004.840686
   Choi H., 2004, IEEE SIGNAL PROCESS, V11
   COX I, 2002, DIGITAL WATERMAKING
   DeVore Ronald A, 1993, Constructive approximation, V303
   Domingo-Ferrer J, 2002, INTERNATIONAL CONFERENCE ON INFORMATION TECHNOLOGY: CODING AND COMPUTING, PROCEEDINGS, P152, DOI 10.1109/ITCC.2002.1000379
   *ECRYPT NETW EXC C, IST507932 ECRYPT NET
   EGGERS JJ, 2000, P SICH MED
   EGGERS JJ, 2000, P EUR SIGN PROC C
   Fridrich J, 2002, EURASIP J APPL SIG P, V2002, P185, DOI 10.1155/S1110865702000537
   Furon T, 2003, IEEE T SIGNAL PROCES, V51, P981, DOI 10.1109/TSP.2003.809376
   Kalker T, 2002, DSP 2002: 14TH INTERNATIONAL CONFERENCE ON DIGITAL SIGNAL PROCESSING PROCEEDINGS, VOLS 1 AND 2, P71, DOI 10.1109/ICDSP.2002.1027818
   SHAMIR A, 1979, COMMUN ACM, V22, P612, DOI 10.1145/359168.359176
   Tassa T, 2004, LECT NOTES COMPUT SC, V2951, P473
   Tian J, 2003, IEEE T CIRC SYST VID, V13, P890, DOI 10.1109/TCSVT.2003.815962
   Tzeng J, 2005, IEEE T SIGNAL PROCES, V53, P784, DOI 10.1109/TSP.2004.839921
NR 19
TC 4
Z9 4
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2008
VL 10
IS 5
BP 758
EP 766
DI 10.1109/TMM.2008.922857
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 334LR
UT WOS:000258223800009
OA Green Accepted, Green Submitted
DA 2024-07-18
ER

PT J
AU Nishikawa, K
   Munadi, K
   Kiya, H
AF Nishikawa, Kiyoshi
   Munadi, Khairul
   Kiya, Hitoshi
TI No-reference PSNR estimation for quality monitoring of motion JPEG2000
   video over lossy packet networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE image communication; JPEG2000; JP2; no-reference PSNR estimation; QoS;
   video quality monitoring
ID IMAGE
AB In this paper, we propose a no-reference (NR) method for estimating the degradation of Motion JPEG2000 (MJP2) video due to packet loss. The proposed method estimates the peak signal-to-noise ratio (PSNR) between received MJP2 frames affected by packet loss and the originally encoded frames without having information about the latter. The PSNR of a frame could be estimated in the wavelet domain, namely before the JPEG2000 decoding process, by utilizing adjacent frames that possess similar amounts of wavelet energy. Simulation results showed that the estimated PSNR is very close to the true value obtained by the full-reference calculation. Our method can be used to monitor the video quality in an IP-based transmission of MJP2, as well as for automatic judgment of error control in streaming applications, or in video broadcasting. For example, if the PSNR of a frame does not reach a predefined level, the frame will be rejected before the decoding process or it will be processed by a concealment techniques before being displayed.
C1 [Nishikawa, Kiyoshi; Kiya, Hitoshi] Tokyo Metropolitan Univ, Dept Informat & Commun Syst Engn, Fac Syst Design, Tokyo 1910065, Japan.
   [Munadi, Khairul] Syiah Kuala Univ, Fac Engn, Dept Elect Engn, Darussalam 23111, Banda Aceh, Indonesia.
C3 Tokyo Metropolitan University; Universitas Syiah Kuala
RP Nishikawa, K (corresponding author), Tokyo Metropolitan Univ, Dept Informat & Commun Syst Engn, Fac Syst Design, Tokyo 1910065, Japan.
EM knishikawa@m.ieice.org; munadi@unsyiah.net; kiya@eei.metro-u.ac.jp
RI Munadi, Khairul/Q-4028-2017; Munadi, Khairul/AAS-1349-2020
OI Munadi, Khairul/0000-0002-7507-9476; Munadi, Khairul/0000-0002-7507-9476
CR [Anonymous], 2000, 154441 ISOIEC
   Bilgin A, 2000, IEEE T IMAGE PROCESS, V9, P1972, DOI 10.1109/83.877218
   *DIG CIN IN LLC ME, 2007, DIG CIN SYST SPEC V
   Fössel S, 2003, IEEE T CONSUM ELECTR, V49, P787, DOI 10.1109/TCE.2003.1261156
   Ichigaya A, 2006, IEEE T CIRC SYST VID, V16, P251, DOI 10.1109/TCSVT.2005.858745
   *ISO IEC, 2005, 154443 ISOIEC
   KNEE M, 2000, PICTURE APPRAISAL RA
   LEE MH, 2006, P AUSTR TEL NETW APP
   Marziliano P, 2004, SIGNAL PROCESS-IMAGE, V19, P163, DOI 10.1016/j.image.2003.08.003
   Ngo C. W., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P36, DOI 10.1109/CVPR.1999.786914
   Nishikawa K, 2006, IEICE T FUND ELECTR, VE89A, P2119, DOI 10.1093/ietfec/e89-a.8.2119
   ONG E, 2003, P IEEE ICME, V1
   Reibman AR, 2004, IEEE T MULTIMEDIA, V6, P327, DOI 10.1109/TMM.2003.822785
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Shirai D, 2006, 2006 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, P1855
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Sugimoto O, 2006, P SOC PHOTO-OPT INS, V6059, pT590, DOI 10.1117/12.644051
   TABESH A, 2005, P IEEE DCC, V1, P329
   Turaga DS, 2004, SIGNAL PROCESS-IMAGE, V19, P173, DOI 10.1016/j.image.2003.09.001
   YANG F, 2003, P IEEE INT S PERS IN, V3, P2707
NR 20
TC 7
Z9 7
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2008
VL 10
IS 4
BP 637
EP 645
DI 10.1109/TMM.2008.921849
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 342EK
UT WOS:000258767200009
DA 2024-07-18
ER

PT J
AU Djama, I
   Ahmed, T
   Nafaa, A
   Boutaba, R
AF Djama, Ismail
   Ahmed, Toufik
   Nafaa, Abdelhamid
   Boutaba, Raouf
TI Meet in the middle cross-layer adaptation for audiovisual content
   delivery
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE cross-layer adaptation; forward error correction; link-layer quality;
   MPEG-21 multimedia framework; QoS metrics; real-time streaming
AB This paper describes a new architecture and implementation of an adaptive streaming system (e.g., Television over IP, Video on Demand) based on cross-layer interactions. At the center of the proposed architecture is the Meet In the Middle concept involving both bottom-up and top-down cross layer interactions. Each streaming session is entirely controlled at the RTP layer where we maintain a rich context that centralizes the collection of i) instantaneous network conditions measured at the underlying layers (i.e.: link, network, and transport layers) and ii) user- and terminal-triggered events that impose new real-time QoS adaptation strategies. Thus, each active multimedia session is tied to a broad range of parameters, which enable it to coordinate the QoS adaptation throughout the protocol layers and thus eliminating the overhead and preventing counter-productiveness among separate mechanisms implemented at different layers. The MPEG-21 framework is used to provide a common support for implementing and managing the end-to-end QoS of audio/video streams. Performance evaluations using Peak Signal to Noise Ratio (PSNR) and Structural Similarity Index (SSIM) objective video quality metrics show the benefits of using the proposed Meet In the Middle cross-layer design compared to traditional media delivery approaches.
C1 [Djama, Ismail; Ahmed, Toufik] Univ Bordeaux 1, CNRS, LaBRI Lab, F-33405 Talence, France.
   [Nafaa, Abdelhamid] Univ Coll Dublin, Sch Informat & Comp Sci, Dublin 4, Ireland.
   [Boutaba, Raouf] Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.
C3 Universite de Bordeaux; Centre National de la Recherche Scientifique
   (CNRS); University College Dublin; University of Waterloo
RP Djama, I (corresponding author), Univ Bordeaux 1, CNRS, LaBRI Lab, F-33405 Talence, France.
EM djama@labri.fir; nafaa@ieee.org; rboutaba@bbcr.uwaterloo.ca
RI Boutaba, Raouf/AAT-2801-2020; Boutaba, Raouf/G-8483-2017
OI Boutaba, Raouf/0000-0001-7936-6862; AHMED, Toufik/0000-0002-9245-0759
CR Ahmed T, 2005, IEEE J SEL AREA COMM, V23, P385, DOI 10.1109/JSAC.2004.839425
   [Anonymous], 2006, JOINT SCALABLE VIDEO
   BRASKICH T, 2005, P IEEE WCNC 05 NEW O, V3, P1602
   BUTALA LT, 2005, EURASIP J APPL SIG P, P129
   CHEN J, 2004, P INT S CIRC SYST IS, P176
   CLARK DD, 1990, P ACM S COMM ARCH PR, P200
   DE S, 2004, IEEE J SEL AREA COMM, P1271
   Djama I, 2007, IEEE T BROADCAST, V53, P382, DOI 10.1109/TBC.2006.889111
   ERNST H, 2004, P IEEE VTC 04 MIL IT, V5, P2916
   GUENKOVALUY T, 2005, IN PRESS IETF DR FEB
   HANDLEY M, 2003, IETF RFC, V3448
   Handley Mark, 1998, RFC, V2327
   HARATCHEREV J, 2005, WIRELESS COMMUNICATI, V5, P412
   *ISO IEC, 2004, 210007 ISO IEC
   KOHLER E, 2006, IETF             MAR
   KRISHNAMACHARI S, 2003, P 13 INT PACK WORKSH
   LARZON LA, 2004, IETF             JUL
   LI M, 2005, P 15 ACM NOSSDAV 05
   Nafaa A, 2005, COMPUT NETW, V49, P766, DOI 10.1016/j.comnet.2005.02.006
   Qiao D., 2002, IEEE T MOBILE COMPUT, V1, P278, DOI DOI 10.1109/TMC.2002.1175541
   SCHULZRINNE H, 2003, IETF RFC, V3550
   Shakkottai S, 2003, IEEE COMMUN MAG, V41, P74, DOI 10.1109/MCOM.2003.1235598
   Shan Y, 2002, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL I AND II, PROCEEDINGS, P277
   STEWART R, 2000, IETF             OCT
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Verikoukis C., 2005, IEEE COMMUNICATIONS, V43, P1, DOI 10.1109/MCOM.2005.1470797
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JLC, 2001, 15TH INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING, PROCEEDINGS, P411, DOI 10.1109/ICOIN.2001.905459
   Zhang Q, 2005, P IEEE, V93, P123, DOI 10.1109/JPROC.2004.839603
NR 29
TC 12
Z9 14
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2008
VL 10
IS 1
BP 105
EP 120
DI 10.1109/TMM.2007.911243
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 245QZ
UT WOS:000251952200011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Sung, YWE
   Bishop, MA
   Rao, SG
AF Sung, Yu-Wei Eric
   Bishop, Michael A.
   Rao, Sanjay G.
TI Enabling contribution awareness in an overlay broadcasting system
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE bandwidth detection; incentive; multitree; overlay multicast; NAT;
   saturation detection
AB We consider the design of bandwidth-demanding broadcasting applications using overlays in environments characterized by hosts with limited and asymmetric bandwidth, and significant heterogeneity in upload bandwidth. Such environments are critical to consider to extend the applicability of overlay multicast to mainstream Internet environments where insufficient bandwidth exists to support all hosts, but have not received adequate attention from the research community. We leverage the multitree framework and design heuristics to enable it to consider host contribution and operate in bandwidth-scarce environments. Our extensions seek to simultaneously achieve good utilization of system resources, performance to hosts commensurate to their contributions, and consistent performance. We have implemented the system and conducted an Internet evaluation on PlanetLab using real traces from previous operational deployments of an overlay broadcasting system. Our results indicate for these traces, our heuristics can improve the performance of high contributors by 10-240% and facilitate equitable bandwidth distribution among hosts with similar contributions.
C1 Purdue Univ, Dept Elect & Comp Engn, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Sung, YWE (corresponding author), Purdue Univ, Dept Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM sungy@ecn.purdue; michael.bishop@microsoft.com; sanjay@ecn.purdue.edu
CR ADLER M, 2004, P 2 WORKSH EC PEER P
   [Anonymous], P ACM SIGCOMM
   BUCHEGGER S, 2003, 2 WORKSH EC PEER PEE
   Castro M., 2003, Operating Systems Review, V37, P298, DOI 10.1145/1165389.945474
   Castro M, 2002, IEEE J SEL AREA COMM, V20, P1489, DOI 10.1109/JSAC.2002.803069
   CHOU P, 2003, PACK VID WORKSH
   Chu Yang-hua., 2004, Proceedings of the ACM SIGCOMM Workshop on Practice and Theory of Incentives in Networked Systems (PINS), P205
   Chu YH, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE GENERAL TRACK 2004 USENIX ANNUAL TECHNICAL CONFERENCE, P155
   Chu YH, 2000, PERF E R SI, V28, P1, DOI 10.1145/345063.339337
   COHEN B, 2003, 1 WORKSH EC PEER PEE
   DUTTA D, 2003, 1 WORKSH EC PEER PEE
   FRANCIS P, VOID EXTENDING INTER
   Ganjam A., 2004, NOSSDAV 04, P54
   Goyal VK, 2001, IEEE SIGNAL PROC MAG, V18, P74, DOI 10.1109/79.952806
   Habib A, 2004, INT WORKSH QUAL SERV, P171
   Hei X., 2006, IPTV WORKSH
   Hu NN, 2003, IEEE J SEL AREA COMM, V21, P879, DOI 10.1109/JSAC.2003.814505
   Jannotti J, 2000, USENIX ASSOCIATION PROCEEDINGS OF THE FOURTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P197
   KAMVAR S, 2003, 1 WORKSH EC PEER PEE
   Kostic D., 2003, Operating Systems Review, V37, P282, DOI 10.1145/1165389.945473
   KULBAK Y, 2005, TR2005 HEBR U JER
   KUNG HT, 2003, 1 WORKSH EC PEER PEE
   Liebeherr J, 2001, GLOB TELECOMM CONF, P1651, DOI 10.1109/GLOCOM.2001.965860
   Liu B, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY, PROCEEDINGS, P49
   LNG T, 2004, 2 WORKSH EC PEER PEE
   MA RTB, 2004, P JOINT INT C MEAS M, P189
   Magharei N, 2007, IEEE INFOCOM SER, P1424
   Padmanabhan V.N., 2002, P 12 INT WORKSHOP NE, P177
   Pai V, 2005, LECT NOTES COMPUT SC, V3640, P127
   Pendarakis D, 2001, USENIX ASSOCIATION PROCEEDINGS OF THE 3RD USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS, P49
   RATNASAMY S, 2001, P 3 INT WORKSH NETW, P14
   Shrivastava V., 2005, Proceedings of the 15th International Workshop on Network and Operating Systems Support for Digital Audio and Video. NOSSDAV 2005, P93, DOI 10.1145/1065983.1066006
   SRIPANIDKULCHAI K, 2004, P ACM SIGCOMM, P107, DOI DOI 10.1145/1030194.1015480
   Strauss Jacob., 2003, IMC 03, P39, DOI [DOI 10.1145/948205.948211, 10.1145/948205.948211]
   VENKATARAMAN V, 2006, P ICNP
   Wang W., 2002, Proc. Networked Group Communication, P154
   Yuen S, 2005, IEEE INFOCOM SER, P2135
   Zhang XY, 2005, IEEE INFOCOM SER, P2102
   ZHUANG SQ, 2001, P 11 INT WORKSH NETW, P11
   TMESH BROADCAST SYST
   ESM BROADCAST SYSTEM
NR 41
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2007
VL 9
IS 8
BP 1605
EP 1620
DI 10.1109/TMM.2007.907454
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 233SF
UT WOS:000251109900007
OA Green Published
DA 2024-07-18
ER

PT J
AU Fan, J
   Luo, H
   Gao, Y
   Jain, R
AF Fan, Jianping
   Luo, Hangzai
   Gao, Yuli
   Jain, Ramesh
TI Incorporating concept ontology for hierarchical video classification,
   annotation, and visualization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE concept ontology; hierarchical boosting; hyperbolic visualization;
   multimodal boosting; multitask learning; semantic gap; video
   classification; annotation
ID FRAMEWORK; MULTIMEDIA; RETRIEVAL; SYSTEM
AB Most existing content-based video retrieval (CBVR) systems are now amenable to support automatic low-level feature extraction, but they still have limited effectiveness from a user's perspective because of the semantic gap. Automatic video concept detection via semantic classification is one promising solution to bridge the semantic gap. To speed up SVM video classifier training in high-dimensional heterogeneous feature space, a novel multimodal boosting algorithm is proposed by incorporating feature hierarchy and boosting to reduce both the training cost and the size of training samples significantly. To avoid the inter-level error transmission problem, a novel hierarchical boosting scheme is proposed by incorporating concept ontology and multitask learning to boost hierarchical video classifier training through exploiting the strong correlations between the video concepts. To bridge the semantic gap between the available video concepts and the users' real needs, a novel hyperbolic visualization framework is seamlessly incorporated to enable intuitive query specification and evaluation by acquainting the users with a good global view of large-scale video collections. Our experiments in one specific domain of surgery education videos have also provided very convincing results.
C1 Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
   Univ Calif Irvine, Sch Informat & Comp Sci, Dept Comp Sci, Irvine, CA 92623 USA.
C3 University of North Carolina; University of North Carolina Charlotte;
   University of California System; University of California Irvine
RP Fan, J (corresponding author), Univ N Carolina, Dept Comp Sci, Charlotte, NC 28223 USA.
EM jfan@uncc.edu; hluo@uncc.edu; ygao@uncc.edu; jain@ics.uci.edu
CR ADAMS WH, 2003, EURASIP JASP, V2, P170
   [Anonymous], 2000, ICML
   [Anonymous], 1999, Advances in kernel methods: Support vector learning
   [Anonymous], IEEE MULTIMEDIA
   [Anonymous], ART COMPUTER PROGRAM
   [Anonymous], 1998, Proc. International Conference on Computer Vision
   Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731
   Ben-David S, 2003, LECT NOTES ARTIF INT, V2777, P567, DOI 10.1007/978-3-540-45167-9_41
   BENITEZ AB, 2001, ACM MULTIMEDIA
   BENITEZ AB, 2000, P SPIE, V4210
   Buitelaar Paul, 2005, Ontology Learning From Text: Methods, Evaluation and Applications
   CHAKRABARTI S, 1997, P VLDB
   CHANG SF, 1998, P IEEE INT C IM PROC, P531
   CHRISTEL M, 2005, P CIVR
   CIARAMITA M, 2003, P IJCAI
   Crammer K., 2001, J. Mach. Learn. Res., V2, P265
   DEMENTHON D, 2005, MULTIMEDIA TOOLS APP
   Deng YN, 1998, IEEE T CIRC SYST VID, V8, P616, DOI 10.1109/76.718508
   Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263
   Dumais S., 2000, Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, Greece, 24-28 July 2000, P256
   Evgeniou T, 2005, J MACH LEARN RES, V6, P615
   Evgeniou T., 2004, P ACM SIGKDD
   Fan JP, 2004, IEEE T IMAGE PROCESS, V13, P974, DOI 10.1109/TIP.2004.827232
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   FIKES R, 1997, TOOLS ASSEMBLING MOD, P436
   Freund Y., 1996, INT C MACHINE LEARNI, P148
   Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223
   Gruen RL, 2004, BMC HEALTH SERV RES, V4, DOI 10.1186/1472-6963-4-8
   HARE JS, 2006, P SPIE, V6073
   HAUPTMANN AG, 2004, P INT C IM VID RETR, P674
   Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601
   HOLLINK L, 2005, S ACM MULTIMEDIA
   Hunter J, 2003, IEEE T CIRC SYST VID, V13, P49, DOI 10.1109/TCSVT.2002.808088
   JAIMES A, 2003, P IEEE ICME
   JAIMES A, 2005, P MIR, P3
   Jaimes Alejandro, 2003, P CIVR
   Jörgensen C, 2001, J AM SOC INF SCI TEC, V52, P938, DOI 10.1002/asi.1161
   Ke Y., 2005, P IEEE ICCV
   Kohavi R, 1997, ARTIF INTELL, V97, P273, DOI 10.1016/S0004-3702(97)00043-X
   Kojima A, 2002, INT J COMPUT VISION, V50, P171, DOI 10.1023/A:1020346032608
   Koller Daphne., 1997, P ICML
   KOSKELA M, IN PRESS IEEE T MULT
   Lamping J, 1996, J VISUAL LANG COMPUT, V7, P33, DOI 10.1006/jvlc.1996.0003
   LINDLEY CA, 1997, ER 97 WORKSH CONC MO
   LIU T, 2004, P IEEE ISMSE
   LUO H, 2004, P ACM CIVR, P374
   LUO H, 2006, P IEEE VAST
   MAEDCHE A., 2002, Ontology learning for the semantic web
   McCallum A. K., 1998, P 15 INT C MACH LEAR, P359
   Moghaddam B, 2004, INT J COMPUT VISION, V56, P109, DOI 10.1023/B:VISI.0000004834.62090.74
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Naphade MR, 2002, IEEE T CIRC SYST VID, V12, P40
   NAPHADE MR, 2004, P ACM MULT
   NEVATIA R, 2003, P IEEE CVPR WORKSH E
   NGUYEN GP, 2005, P AVIVDILIB
   PUNERA K, 2005, AUTOMATICALLY LEARNI, P1010
   Sanderson M, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P206, DOI 10.1145/312624.312679
   Satoh S., 1997, P CVPR
   Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923
   SCHREIBER G, 2001, IEEE INTELL SYST
   Snoek CGM, 2006, IEEE T PATTERN ANAL, V28, P1678, DOI 10.1109/TPAMI.2006.212
   SNOEK CGM, 2006, ACM T MULTIMEDIA COM, V2
   SNOEK CGM, IN PRESS IEEE T MULT
   Stan D, 2003, INFORM PROCESS MANAG, V39, P335, DOI 10.1016/S0306-4573(02)00131-0
   Thrun S., 1997, LEARNING LEARN
   TIEU K, 2000, P IEEE CVPR
   Torralba A., 2004, P CVPR
   Vapnik V., 1999, NATURE STAT LEARNING
   VASCONCELOS N, 2001, P IEEE CVPR
   Wactlar HD, 1999, COMPUTER, V32, P66, DOI 10.1109/2.745722
   WALTER JA, 2002, P ACM SIGKDD
   WU Y, 2004, P IEEE ICME
   YU K, 2003, P INT C UNC ART INT
   Zhao R, 2002, IEEE T MULTIMEDIA, V4, P189, DOI 10.1109/TMM.2002.1017733
   ONTOLOGY ALIGNMENT
NR 75
TC 30
Z9 40
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2007
VL 9
IS 5
BP 939
EP 957
DI 10.1109/TMM.2007.900143
PG 19
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 193ZX
UT WOS:000248314800004
DA 2024-07-18
ER

PT J
AU Boato, G
   De Natale, FGB
   Fontanari, C
AF Boato, Giulia
   De Natale, Francesco G. B.
   Fontanari, Claudio
TI Digital image tracing by sequential multiple watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article; Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP 2006)
CY OCT 08-11, 2006
CL Atlanta, GA
SP IEEE
DE asymmetric watermarking; image tracing; linear algebra; multiple
   watermarking
AB The possibility of adding several watermarks to the same image would enable many interesting applications such as multimedia document tracing, data usage monitoring, multiple property management. In this paper, we present a novel watermarking scheme which allows to insert and reliably detect multiple watermarks sequentially embedded into a digital image. The proposed method, based on elementary linear algebra, is asymmetric, secure under projection attack and robust against distortion due to basic operations such as storage, transmission, format conversion, etc.
C1 Univ Trent, Dept Informat & Commun Technol, I-38050 Trento, Italy.
   Politecn Torino, Sch Engn Informat Technol 3, Dept Math, I-10129 Turin, Italy.
C3 Fondazione Bruno Kessler; FBK-ICT - Center for Information &
   Communication Technology; University of Trento; Polytechnic University
   of Turin
RP Boato, G (corresponding author), Univ Trent, Dept Informat & Commun Technol, I-38050 Trento, Italy.
EM boato@dit.unitn.it; denatale@ing.unitn.it; claudio.fontanari@polito.it
CR Boato G, 2006, IEEE T SIGNAL PROCES, V54, P2833, DOI 10.1109/TSP.2006.874411
   Busch C, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P207, DOI 10.1109/MMSP.2001.962735
   Coatrieux G, 2000, ENG MED BIOL SOC ANN, P250, DOI 10.1109/ITAB.2000.892396
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   Giakoumaki A, 2004, P ANN INT IEEE EMBS, V26, P3241
   Giakoumaki A, 2003, P ANN INT IEEE EMBS, V25, P856, DOI 10.1109/IEMBS.2003.1279900
   Giakoumaki A, 2006, IEEE T INF TECHNOL B, V10, P722, DOI 10.1109/TITB.2006.875655
   Hsu CT, 1999, IEEE T IMAGE PROCESS, V8, P58, DOI 10.1109/83.736686
   Li MY, 2004, P ANN INT IEEE EMBS, V26, P3233
   Lu CS, 2001, IEEE T IMAGE PROCESS, V10, P1579, DOI 10.1109/83.951542
   Lu ZM, 2005, IEEE T IMAGE PROCESS, V14, P822, DOI 10.1109/TIP.2005.847324
   Mintzer F, 1999, INT CONF ACOUST SPEE, P2067, DOI 10.1109/ICASSP.1999.758338
   Piva A, 2005, IEE P-VIS IMAGE SIGN, V152, P604, DOI 10.1049/ip-vis:20041240
   Stankovic S, 2001, IEEE T IMAGE PROCESS, V10, P650, DOI 10.1109/83.913599
   Wong PHW, 2003, IEEE T CIRC SYST VID, V13, P813, DOI 10.1109/TCSVT.2003.815948
   WONG PHW, 2004, P IEEE INT C AC SPEE, V3, P393
   WOO CS, 2005, P WORKSH DIG IM COMP
NR 17
TC 19
Z9 21
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2007
VL 9
IS 4
BP 677
EP 686
DI 10.1109/TMM.2007.893335
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science; Telecommunications
GA 170NW
UT WOS:000246671200001
DA 2024-07-18
ER

PT J
AU Cooper, M
   Liu, T
   Rieffel, E
AF Cooper, Matthew
   Liu, Ting
   Rieffel, Eleanor
TI Video segmentation via temporal pattern classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE video analysis; video shot boundary detection
ID SHOT-CHANGE DETECTION
AB We present a general approach to temporal media segmentation using supervised classification. Given standard low-level features representing each time sample, we build intermediate features via pairwise similarity. The intermediate features comprehensively characterize local temporal structure, and are input to an efficient supervised classifier to identify shot boundaries. We integrate discriminative feature selection based on mutual information to enhance performance and reduce processing requirements. Experimental results using large-scale test sets provided by the TRECVID evaluations for abrupt and gradual shot boundary detection are presented, demonstrating excellent performance.
C1 FX Palo Alto Lab, Palo Alto, CA 94304 USA.
   Google Inc, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Cooper, M (corresponding author), FX Palo Alto Lab, Palo Alto, CA 94304 USA.
EM cooper@fxpal.com
OI Rieffel, Eleanor/0009-0005-2957-9372
CR Adcock J., 2004, P TREC VID RETR EV T, P70
   [Anonymous], 2004, THEDISTRIBUTIONOFBIO
   [Anonymous], P TRECVID 2003 C DEC
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   ARMAN F, 1993, MULTIMEDIA 93, P267
   Bescós J, 2004, IEEE T CIRC SYST VID, V14, P475, DOI 10.1109/TCSVT.2004.825546
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   Cooper M, 2001, 2001 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P378, DOI 10.1109/ICIP.2001.958130
   COOPER M, 2004, MULTIMEDIA 04, P252
   Foote J, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P77, DOI 10.1145/319463.319472
   FRADKIN D, 2000, KDD 03 P 9 ACM SIGKD, P517
   Friedman J., 2001, ELEMENTS STAT LEARNI, V1, DOI DOI 10.1007/978-0-387-84858-7
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Günsel B, 1998, J ELECTRON IMAGING, V7, P592, DOI 10.1117/1.482613
   Hanjalic A, 2002, IEEE T CIRC SYST VID, V12, P90, DOI 10.1109/76.988656
   HEESCH D, 2004, P TREC VID RETR EV T, P92
   HOASHI K, 2004, P TREC VID RETR EV T, P109
   Liu T., 2004, Advances in Neural Information Processing Systems, V16
   Martínez JM, 2002, IEEE MULTIMEDIA, V9, P78, DOI 10.1109/93.998074
   PETERSOHN C, 2004, P TREC VID RETR EV T, P64
   Puzicha J, 1997, PROC CVPR IEEE, P267, DOI 10.1109/CVPR.1997.609331
   PYE D, 1998, P INT C SPOK LANG PR
   Qi Y, 2003, 2003 INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOL II, PROCEEDINGS, P689
   SLANEY M, 2001, MULTIMEDIA 01, P29
   Vasconcelos N, 2004, PROC CVPR IEEE, P770
   Vasconcelos N, 2003, PROC CVPR IEEE, P762
   Vasconcelos N, 2000, IEEE T IMAGE PROCESS, V9, P3, DOI 10.1109/83.817595
   VOLKMER T, 2004, P TREC VID RETR EV T, P171
   WITKIN A, 1981, P IEEE INT C AC SPEE
   YEO BL, 1995, P INT C MULT COMP SY, P81
   YUAN J, 2004, P TREC VID RETR EV T, P184
NR 31
TC 42
Z9 57
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2007
VL 9
IS 3
BP 610
EP 618
DI 10.1109/TMM.2006.888015
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 148PW
UT WOS:000245088100015
DA 2024-07-18
ER

PT J
AU Boutell, MR
   Luo, JB
   Brown, CM
AF Boutell, Matthew R.
   Luo, Jiebo
   Brown, Christopher M.
TI Scene parsing using region-based generative models
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE factor graph; generative models; scene classification; semantic features
ID DETECTING SKY
AB Semantic scene classification is a challenging problem in computer vision. In contrast to the common approach of using low-level features computed from the whole scene, we propose "scene parsing" utilizing semantic object detectors (e.g., sky, foliage, and pavement) and region-based scene-configuration models. Because semantic detectors are faulty in practice, it is critical to develop a region-based generative model of outdoor scenes based on characteristic objects in the scene and spatial relationships between them. Since a fully connected scene configuration model is intractable, we chose to model pairwise relationships between regions and estimate scene probabilities using loopy belief propagation on a factor graph. We demonstrate the promise of this approach on a set of over 2000 outdoor photographs, comparing it with existing discriminative approaches and those using low-level features.
C1 Rose Hulman Inst Technol, Dept Comp Sci & Software Engn, Terre Haute, IN 47803 USA.
   Eastman Kodak Co, Res & Dev Labs, Rochester, NY 14650 USA.
   Univ Rochester, Dept Comp Sci, Rochester, NY 14620 USA.
C3 Rose Hulman Institute Technology; Eastman Kodak; University of Rochester
RP Boutell, MR (corresponding author), Rose Hulman Inst Technol, Dept Comp Sci & Software Engn, Terre Haute, IN 47803 USA.
EM boutell@rose-hulman.edu; jiebo.luo@kodak.com; brown@cs.rochester.edu
RI Luo, Jiebo/AAI-7549-2020
OI Luo, Jiebo/0000-0002-4516-9729
CR [Anonymous], 1995, NATURE STAT LEARNING
   BOUTELL M, 2005, P INT C MULT EXP AMS
   BOUTELL M, 2004, P WORKSH STAT REL LE
   BOUTELL M, 2006, P IEEE INT C MULT EX
   BRADSHAW B, 2001, MSRTR200199
   CHOU P, 1988, THESIS U ROCHESTER R
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   FAN J, 2005, P IEEE C COMP VIS PA
   Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075
   GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596
   Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572
   Luo JB, 2002, IEEE T IMAGE PROCESS, V11, P201, DOI 10.1109/83.988954
   MULHEM P, 2001, P INT JOINT C ART IN, P1397
   Naphade MR, 2001, IEEE T MULTIMEDIA, V3, P141, DOI 10.1109/6046.909601
   Pearl J., 1988, PROBABILISTIC REASON
   Singhal A., 2003, P IEEE C COMP VIS PA
   Smith JR, 1999, COMPUT VIS IMAGE UND, V75, P165, DOI 10.1006/cviu.1999.0771
   SMITH JR, 2003, P IEEE INT C MULT EX
   Torralba A, 2004, PROC CVPR IEEE, P762
   Vailaya A, 2000, PROC SPIE, V3972, P411
   VAILAYA A, 1999, P IEEE MULT SYST 99
   Wang YM, 2004, COMPUT VIS IMAGE UND, V93, P328, DOI 10.1016/j.cviu.2003.10.006
NR 22
TC 25
Z9 36
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2007
VL 9
IS 1
BP 136
EP 146
DI 10.1109/TMM.2006.886372
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 119VE
UT WOS:000243041500013
OA Green Submitted
DA 2024-07-18
ER

PT J
AU van der Schaar, M
   Turaga, DS
   Wong, R
AF van der Schaar, Mihaela
   Turaga, Deepak S.
   Wong, Raymond
TI Classification-based system for cross-layer optimized wireless video
   transmission
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE classification for delay-constrained video transmission; cross-layer
   optimization
AB Joint optimization strategies across various layers of the protocol stack have recently been proposed for improving the performance of real-time video transmission over wireless networks. In this paper, we propose a new, low complexity system for determining the optimal cross-layer strategies for wireless multimedia transmission based on classification and machine learning techniques. We first determine offline the optimal cross-layer strategy for various video sequences and channel conditions (training data). Subsequently, we extract relevant and easy to compute content features, encoder-specific parameters, and channel resources from the training data, and train a statistical classifier based on these optimal results. At run-time, we predict using the classifier the optimal cross-layer compression and transmission strategy using these simple, on-the-fly computed features. Hence, we consider the complex problem of finding the optimal cross-layer strategy during the training phase only, and rely at transmission-time on low-complexity classification techniques. We illustrate the proposed classification-based system by performing MAC-application layer optimizations for video transmission over 802.11a wireless LANs. Specifically, we predict the optimal MAC retry limits for the various video packets and compare our results against both optimal and conventionally used ad-hoc cross-layer solutions. Our results indicate that considerable improvements can be obtained through the proposed cross-layer techniques relying on classification as opposed to optimized ad-hoc solutions. The improvements are especially important at high packet-loss rates (5% and higher), where deploying a judicious mixture of strategies at the various layers becomes essential. Furthermore, our proposed classification-based system can be easily modified to include other layers from the OSI stack during the cross-layer optimization.
C1 Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
   IBM Corp, TJ Watson Res Ctr, Hawthorne, NY 10532 USA.
   Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
C3 University of California System; University of California Los Angeles;
   International Business Machines (IBM); University of California System;
   University of California Davis
RP van der Schaar, M (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM mihaela@ee.ucla.edu; turaga@us.ibm.com; rswong@gmail.com
CR AIZERMAN MA, 1965, AUTOMAT REM CONTR+, V25, P821
   Andreopoulos Y, 2004, SIGNAL PROCESS-IMAGE, V19, P653, DOI 10.1016/j.image.2004.05.007
   [Anonymous], 2002, JPEG2000: Image Compression Fundamentals, Standards, and Practice
   [Anonymous], 880211 ISOIEC
   Bianchi G, 2000, IEEE J SEL AREA COMM, V18, P535, DOI 10.1109/49.840210
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   CARUANA R, 2005, 20051973 CORN U
   Choi SJ, 1999, IEEE T IMAGE PROCESS, V8, P155, DOI 10.1109/83.743851
   Cristiani N., 2000, An introduction to support vector machines
   Dimitrova N, 2002, IEEE MULTIMEDIA, V9, P42, DOI 10.1109/MMUL.2002.1022858
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   EBERT J, 1999, TKN99002 TU BERL
   Girgensohn A, 1999, INT CONF ACOUST SPEE, P3045, DOI 10.1109/ICASSP.1999.757483
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Karczewicz M, 2003, IEEE T CIRC SYST VID, V13, P637, DOI 10.1109/TCSVT.2003.814969
   Kumwilaisak W, 2003, IEEE J SEL AREA COMM, V21, P1685, DOI 10.1109/JSAC.2003.816445
   Li Q, 2004, IEEE T MULTIMEDIA, V6, P278, DOI 10.1109/TMM.2003.822792
   Majumdar A, 2002, IEEE T CIRC SYST VID, V12, P524, DOI 10.1109/TCSVT.2002.800315
   Ohm JR, 2005, P IEEE, V93, P42, DOI 10.1109/JPROC.2004.839611
   Ohm JR, 2004, SIGNAL PROCESS-IMAGE, V19, P877, DOI 10.1016/j.image.2004.06.004
   PEI Y, 2001, P IEEE ICIP 2001 THE
   Qian LM, 1999, IEEE DATA COMPR CONF, P414, DOI 10.1109/DCC.1999.755691
   Qiao D., 2002, IEEE T MOBILE COMPUT, V1, P278, DOI DOI 10.1109/TMC.2002.1175541
   REIBMAN, 2000, COMPRESSED VIDEO NET
   SHAKKOTTAI S, 2003, IEEE COMMUN MAG  OCT
   SHAN Y, 2002, P IEEE ICME 2002 LAU
   Shinozaki K, 2000, CURR OPIN PLANT BIOL, V3, P217, DOI 10.1016/S1369-5266(00)00067-4
   TURAGA D, 2005, P IEEE ICIP 2005 GEN
   Turaga DS, 2001, IEEE T MULTIMEDIA, V3, P41, DOI 10.1109/6046.909593
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   van der Schaar M, 2005, IEEE WIREL COMMUN, V12, P50, DOI 10.1109/MWC.2005.1497858
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   WONG R, 2005, P ICC 2005 OCT, V2, P1271
   ZHU H, 2003, ICCCN 03 OCT
NR 34
TC 24
Z9 29
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2006
VL 8
IS 5
BP 1082
EP 1095
DI 10.1109/TMM.2006.879827
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 091TY
UT WOS:000241052400019
DA 2024-07-18
ER

PT J
AU Sun, LF
   Ifeachor, EC
AF Sun, Lingfen
   Ifeachor, Emmanuel C.
TI Voice quality prediction models and their application in VoIP networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE conversational speech quality; E-model; jitter buffer optimization;
   nonintrusive; perceptual evaluation of speech quality (PESQ); regression
   model; voice over IP; voice quality prediction
ID PERCEPTUAL EVALUATION; ITU STANDARD; PESQ
AB The primary aim of this paper is to present new models for objective, nonintrusive, prediction of voice quality for IP networks and to illustrate their application to voice quality monitoring and playout buffer control in VoIP networks. The contributions of the paper are threefold. First, we present a new methodology for developing perceptually accurate models for nonintrusive prediction of voice quality which avoids time-consuming subjective tests. The methodology is generic and as such it has wide applicability in multimedia applications. Second, based on the new methodology, we present efficient regression models for predicting conversational voice quality nonintrusively for four modern codees (G.729, G.723.1, AMR and iLBC). Third, we illustrate the usefulness of the models in two main applications-voice quality prediction for real Internet VoIP traces and perceived quality-driven playout buffer optimization. For voice quality prediction, the results show that the models have accuracy close to the combined ITU PESQ/E-model method using real Internet traces (correlation coefficient over 0.98). For playout buffer optimization, the proposed buffer algorithm provides an optimum voice quality when compared to five other buffer algorithms for all the traces considered.
C1 Univ Plymouth, Sch Comp Commun & Elect, Plymouth PL4 8AA, Devon, England.
C3 University of Plymouth
RP Sun, LF (corresponding author), Univ Plymouth, Sch Comp Commun & Elect, Plymouth PL4 8AA, Devon, England.
EM L.Sun@plymouth.ac.uk; E.Ifeachor@plymouth.ac.uk
OI Sun, Lingfen/0000-0002-9921-2817
CR [Anonymous], 1889 RFC IETF
   [Anonymous], P INT TEL WORKSH NEW
   [Anonymous], 1998, Characteristics of TCP Connection Arrivals
   *ANSI, 2003, T180103 ANSI
   Beerends JG, 2002, J AUDIO ENG SOC, V50, P765
   Boutremans C, 2003, IEEE INFOCOM SER, P652
   Cole RG, 2001, ACM SIGCOMM COMP COM, V31, P9, DOI 10.1145/505666.505669
   Fujimoto K, 2002, GLOB TELECOMM CONF, P2451
   HOENE C, P INT S PERF EV COMP
   Johannesson NO, 1997, IEEE COMMUN MAG, V35, P70, DOI 10.1109/35.568213
   Markopoulou AP, 2002, IEEE INFOCOM SER, P150, DOI 10.1109/INFCOM.2002.1019256
   Mohamed S, 2002, IEEE T CIRC SYST VID, V12, P1071, DOI 10.1109/TCSVT.2002.806808
   MOHAMED S, 2001, P IEEE INFOCOM 01 AN, V2, P641
   MOHAMED S, 2000, NETWORK INFORM SYST, P595
   Möller S, 2002, J AUDIO ENG SOC, V50, P667
   Moon SB, 1998, MULTIMEDIA SYST, V6, P17, DOI 10.1007/s005300050073
   Qiao ZH, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1473, DOI 10.1109/ICC.2004.1312756
   RAMACHANDRAN R, 1994, P IEEE INFOCOM, V2, P680
   Ramos VMR, 2003, LECT NOTES COMPUT SC, V2707, P155
   Rix AW, 2002, J AUDIO ENG SOC, V50, P755
   Rix AW, 2003, P ONL WORKSH MEAS SP, P17
   Rosenberg J., 2000, Proceedings IEEE INFOCOM 2000. Conference on Computer Communications. Nineteenth Annual Joint Conference of the IEEE Computer and Communications Societies (Cat. No.00CH37064), P1705, DOI 10.1109/INFCOM.2000.832570
   SUN L, 2004, THESIS U PLYMOUTH PL
   Sun LF, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-7, P1478
   Sun LF, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, P1
   Sun LF, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS, VOLS 1-5, CONFERENCE PROCEEDINGS, P2573, DOI 10.1109/ICC.2002.997307
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   YAMAMOTO L, 1997, P EXP ATM TRAFF S MY
NR 28
TC 93
Z9 114
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2006
VL 8
IS 4
BP 809
EP 820
DI 10.1109/TMM.2006.876279
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 069BS
UT WOS:000239420300014
DA 2024-07-18
ER

PT J
AU Pi, MH
   Li, CH
   Li, H
AF Pi, Ming Hong
   Li, Chun Hung
   Li, Hua
TI A novel Fractal image watermarking
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE detector response; fractal block coding; orthogonalization fractal
   transform; watermarking
ID COMPRESSION
AB A novel watermarking method is proposed to hide a binary watermark into image files compressed by fractal block coding. This watermarking method utilizes a special type of orthogonalization fractal coding method where the fractal affine transform is determined by the range block mean and contrast scaling. Such orthogonalization fractal decoding is a mean-invariant iteration. In contrast, the fractal parameters of classical fractal compression are very sensitive to any change of domain block pool and to common signal and geometric distortion. Hence, it is impossible to directly place a watermark in fractal parameters. The proposed watermark embedding procedure inserts a permutated pseudo-random binary sequence into the quantized range block means. The watermark is detected by computing the correlation coefficient between the original and the extracted watermark. Experimental results show that the proposed fractal watermarking scheme is robust against common signal and geometric distortion such as JPEG compression, low-pass filtering, rescaling, and clipping.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2V4, Canada.
   Hong Kong Baptist Univ, Hong Kong, Hong Kong, Peoples R China.
   Univ Lethbridge, Dept Math & Comp Sci, Lethbridge, AB T1K 3M4, Canada.
C3 University of Alberta; Hong Kong Baptist University; University of
   Lethbridge
RP Pi, MH (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2V4, Canada.
EM minghong@cs.ualberta.ca; chli@comp.hkbu.edu.hk; huali@cs.uleth.ca
CR Bas P, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P469, DOI 10.1109/ICIP.1998.723532
   Cox IJ, 1997, IEEE T IMAGE PROCESS, V6, P1673, DOI 10.1109/83.650120
   DAVERN P, 1996, SPRINGER LECT NOTES, V1174, P279
   JACOBS EW, 1992, SIGNAL PROCESS, V29, P251, DOI 10.1016/0165-1684(92)90085-B
   JACQUIN AE, 1993, P IEEE, V81, P1451, DOI 10.1109/5.241507
   LI CH, IEICE T FUND E A, V83, P1286
   Lin CY, 2001, IEEE T IMAGE PROCESS, V10, P767, DOI 10.1109/83.918569
   MATSUI K, 1994, P IMA INTELLECTUAL P, V1, P187
   OIEN GE, 1995, FRACTAL IMAGE COMPRE, pCH8
   Peterson R.L., 1995, INTRO SPREAD SPECTRU
   Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065
   PI M, 2003, P ICIP 2003 BARC SPA, V2, P241
   Pi MH, 2004, IEEE IMAGE PROC, P505
   PI MH, 2004, P IEEE INT C AC SPEE, V5, P369
   Proakis J. G., DIGITAL COMMUNICATIO
   Puate J., 1996, P SPIE PHOT E S BOST
   TIRKEL AZ, 1995, DICTA 95, P378
   Tong CS, 2001, IEEE T IMAGE PROCESS, V10, P1269, DOI 10.1109/83.941851
   VANSCHYNDELL RG, 1994, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP.1994.413536
   Wu HC, 2003, FUND INFORM, V58, P189
   Zhu WW, 1999, IEEE T CIRC SYST VID, V9, P545, DOI 10.1109/76.767121
NR 21
TC 24
Z9 34
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2006
VL 8
IS 3
BP 488
EP 499
DI 10.1109/TMM.2006.870738
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 046OX
UT WOS:000237822000006
DA 2024-07-18
ER

PT J
AU Herley, C
AF Herley, C
TI ARGOS: Automatically extracting repeating objects from multimedia
   streams
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio fingerprint; low-dimension representation; multimedia; repeats;
   segmentation
ID SEARCH; AUDIO
AB Many media streams consist of distinct objects that repeat. For example, broadcast television and radio signals contain advertisements, call sign jingles, songs, and even whole programs that repeat. The problem we address is to explicitly identify the underlying structure in repetitive streams and de-construct them into their component objects. Our algorithm exploits dimension reduction techniques on the audio portion of a multimedia stream to make search and buffering feasible. Our architecture assumes no a priori knowledge of the streams, and does not require that the repeating objects (ROs) be known. Everything the system needs, including the position and duration of the ROs, is learned on the fly. We demonstrate that it is perfectly feasible to identify in real-time ROs that occur days or even weeks apart in audio or video streams. Both the compute and buffering requirements are comfortably within reach for a basic desktop computer. We outline the algorithms, enumerate several applications and present results from real broadcast streams.
C1 Microsoft Soft, Redmond, WA 98052 USA.
C3 Microsoft
RP Herley, C (corresponding author), Microsoft Soft, Redmond, WA 98052 USA.
EM c.herley@ieee.org
CR [Anonymous], 1973, The art of computer programming
   Bell T. C., 1990, TEXT COMPRESSION
   BIMBO AD, 2000, P ICME, P479
   Burges CJC, 2003, IEEE T SPEECH AUDI P, V11, P165, DOI 10.1109/TSA.2003.811538
   DELANEY K, 2001, INSIDE SQL SERVER 20
   Duda R., 1973, Pattern Classification and Scene Analysis
   Faloutsos C., 1996, SEARCHING MULTIMEDIA
   Garcia-Molina Hector., 2000, DATABASE SYSTEM IMPL
   Haitsma J, 2002, ISMIR 2002 3 INT C M
   HAMPAPUR A, 2000, P ICME
   HERLEY C, 2004, P ICASSP         MAY
   Hsu JL, 2001, IEEE T MULTIMEDIA, V3, P311, DOI 10.1109/6046.944475
   JAIMES A, 2003, P IEEE ICME SIB ROM
   Jayant N.C., 1984, Digital Coding of Waveforms: Principles and Applications to Speech and Video
   JIANG H, 2000, P ICME
   KANG J, 2002, P 28 INT VLDB C HONG
   Kashino K, 2003, IEEE T MULTIMEDIA, V5, P348, DOI 10.1109/TMM.2003.813281
   Lienhart R, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P509, DOI 10.1109/MMCS.1997.609763
   Marple SL., 1987, Digital spectral analysis with applications
   MURAMOTO T, 2000, P ICME, P1547
   Pass G., 1996, Proceedings ACM Multimedia 96, P65, DOI 10.1145/244130.244148
   Pfeiffer S., 1996, Proceedings ACM Multimedia 96, P21, DOI 10.1145/244130.244139
   RAGNO R, ACM MIR 2005 SING, P73
   SUDNARAM H, 2000, P ICME
   Wold E, 1996, IEEE MULTIMEDIA, V3, P27, DOI 10.1109/93.556537
   Yeung M, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P296, DOI 10.1109/MMCS.1996.534991
NR 26
TC 36
Z9 41
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2006
VL 8
IS 1
BP 115
EP 129
DI 10.1109/TMM.2005.861286
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 005UR
UT WOS:000234850000011
DA 2024-07-18
ER

PT J
AU Logan, B
   Van Thong, JM
   Moreno, PJ
AF Logan, B
   Van Thong, JM
   Moreno, PJ
TI Approaches to reduce the effects of OOV queries on indexed spoken audio
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE audio indexing; speech indexing; spoken document retrieval;
   out-of-vocabulary words
AB We present several novel approaches to the Out of Vocabulary (OOV) query problem for spoken audio: indexing based on syllable-like units called particles and query expansion according to acoustic confusability for a word index. We also examine linear and OOV-based combination of indexing schemes. We experiment on 75 h of broadcast news, comparing our techniques to a word index, a phoneme index and a phoneme index queried with phoneme sequences. Our results show that our approaches are superior to both a word index and a phoneme index for OOV words, and have comparable performance to the sequence of phonemes scheme. The particle system has worse performance than the acoustic query expansion scheme. The best system uses word queries for in-vocabulary words and a linear combination of the phoneme sequence scheme and acoustic query expansion for OOV words. Using the best possible weights for linear combination, this system improves the average precision from 0.35 for a word index to 0.40, a result only obtainable if the weights could be learnt on a development query set. The next best system used a word index for in-vocabulary words and the phoneme sequence system otherwise and had average precision of 0.39.
C1 Hewlett Packard Labs, Cambridge, MA 02142 USA.
C3 Hewlett-Packard
RP Hewlett Packard Labs, Cambridge, MA 02142 USA.
EM Beth.Logan@hp.com; JM.Vanthong@hp.com; pedro@google.com
CR ABBERLEY D, 1999, P 8 TEXT RETR C TREC, P128
   [Anonymous], P SPEC INT GROUP INF
   [Anonymous], P 23 ANN INT ACM SIG
   Buckley Chris, 2000, Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR'00, page, P33, DOI [DOI 10.1145/345508.345543, 10.1145/345508.345543]
   BURROWS M, 1998, Patent No. 5745899
   CHANG SF, 2000, SIGN PROC COMMUN SER, V2, P559
   CLEMENTS M, 2001, P 20 ANN AVIOS C
   Dharanipragada S, 2002, IEEE T SPEECH AUDI P, V10, P542, DOI 10.1109/TSA.2002.804543
   James DA, 1994, P IEEE INT C AC SPEE, P279
   Kemp Thomas, 1998, P INT C SPOK LANG PR, P1839
   LOGAN B, 2000, P INT C SPOK LANG PR
   Ng K, 2000, INT CONF ACOUST SPEE, P2405, DOI 10.1109/ICASSP.2000.859326
   Ng K., 1998, Proceedings of International Conference on Spoken Language Processing, Volume, V3, P939
   PAGEL V, 1998, P ICSLP, P2015
   SCHAEUBLE P, 1995, P IJCAI 95 WORKSH IN, P95
   VANTHONG JM, 2000, P INT C COMP ASS INF
   Vogt C. C., 1999, Information Retrieval, V1, P151, DOI 10.1023/A:1009980820262
   WACTLAR HD, 1996, ARPA SPEECH REC WORK
   WHITTAKER EWD, 2001, P 2001 AUT SPEECH RE
   Witbrock MJ, 1997, ACM DIGITAL LIBRARIES '97, P30
   Woodland P. C., 2000, SIGIR Forum, V34, P372
NR 21
TC 34
Z9 40
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2005
VL 7
IS 5
BP 899
EP 906
DI 10.1109/TMM.2005.854429
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 967IM
UT WOS:000232084900011
DA 2024-07-18
ER

PT J
AU Nam, J
   Tewfik, AH
AF Nam, J
   Tewfik, AH
TI Detection of gradual transitions in video sequences using b-spline
   interpolation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE dissolve; fade; gradual scene changes; Radon transform; shot boundary
   detection; splines; temporal video segmentation; wavelet representation;
   wipe
AB We present a novel technique for detecting the presence of a gradual transition in video sequences and automatically identifying its type. Our scheme focuses on analyzing the characteristics of the underlying special edit effects and estimates actual transitions by polynomial data interpolation. In particular, a B-spline interpolation curve fitting technique is used. We make use of "goodness" of fitting to determine the presence of gradual transitions. Our approach is able to recover the original transition behavior of an edit effect even if it is distorted by various post-processing stages. Our gradual transition detectors have been extensively tested on various genres of real video sequences to evaluate the performance of the proposed algorithms.
C1 ETRI, Broadcasting Media Res Grp, Taejon 305350, South Korea.
   Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
C3 Electronics & Telecommunications Research Institute - Korea (ETRI);
   University of Minnesota System; University of Minnesota Twin Cities
RP ETRI, Broadcasting Media Res Grp, Taejon 305350, South Korea.
EM namjeho@etri.re.kr; tewfik@ece.umn.edu
CR ALATTAR AM, 1997, P IEEE INT C AC SPEE, V4, P3025
   [Anonymous], 1995, P ACM MULT, DOI DOI 10.1145/217279.215266
   [Anonymous], 2001, International Journal of Image and Graphics (IJIG), Vol, DOI DOI 10.1142/S021946780100027X
   [Anonymous], 1993, Multimedia Systems, DOI DOI 10.1007/BF01210504
   Boreczky JS, 1996, P SOC PHOTO-OPT INS, V2670, P170, DOI 10.1117/12.234794
   BOVE VM, 1992, SMPTE J          JAN, P2
   De Boor C, 1978, A Pratical Guide to Splines, V27
   Fernando W. A. C., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P299, DOI 10.1109/ICIP.1999.817121
   FERNANDO WAC, 1999, P IEEE INT C IM PROC, V3, P294
   Gargi U, 2000, IEEE T CIRC SYST VID, V10, P1, DOI 10.1109/76.825852
   Hampapur A., 1994, Proceedings ACM Multimedia '94, P357, DOI 10.1145/192593.192699
   Hansen KV, 1996, IEEE T IMAGE PROCESS, V5, P1651, DOI 10.1109/83.544572
   Jain A., 1988, Fundamentals of Digital Image Processing
   KIM H, 1999, SPIE, V3656, P280
   KOBLA V, 1999, SPIE, V3656, P290
   LIENHART R, 1999, SPIE, V3656, P302
   Lu H. B., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P304, DOI 10.1109/ICIP.1999.817122
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   MENG J, 1995, SPIE, V2419, P26
   NAM J, 2000, P IEEE INT C MULT EX, V3, P1349
   Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804
   Unser M, 1999, IEEE SIGNAL PROC MAG, V16, P22, DOI 10.1109/79.799930
   *VID GROUP, 2002, N4928 ISOIEC JTC1SC2
   Wu M, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P893, DOI 10.1109/ICIP.1998.723664
   Yeo BL, 1995, IEEE T CIRC SYST VID, V5, P533, DOI 10.1109/76.475896
   YU H, 1997, P IEEE INT C IM PROC, V2, P498
   YU H, 1998, P IEEE INT C AC SPEE, V5, P2965
NR 27
TC 37
Z9 43
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 667
EP 679
DI 10.1109/TMM.2005.843362
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000008
DA 2024-07-18
ER

PT J
AU Pi, MH
   Mandal, MK
   Basu, A
AF Pi, MH
   Mandal, MK
   Basu, A
TI Image retrieval based on histogram of fractal parameters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE fractal compression; fractal index; image indexing and retrieval
AB Image indexing and retrieval techniques are important for efficient management of visual databases. These techniques are generally developed based on the associated compression techniques. In the fractal domain, luminance offset and contrast scaling parameter are typically used as the fractal indices. However, luminance offset and contrast scaling parameter are strongly correlated. In this paper, we prove that range block mean and contrast scaling parameters are independent. Based on this independence, we propose four statistical indices for efficient image retrieval. In addition, we propose an efficient hierarchical indexing strategy based on the dc and ac component analysis. Experimental results on a database of 416 texture images, created by decomposing 26 images, indicate that the proposed indices significantly improve the retrieval rate, compared to other retrieval methods.
C1 Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
   Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2E8, Canada.
C3 University of Alberta; University of Alberta
RP Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada.
EM minghong@cs.ualberta.ca; mandal@ualberta.ca; anup@cs.ualberta.ca
CR [Anonymous], P SPIE C DIG IM STOR
   BARNSLEY MF, 1988, BYTE, V13, P215
   Do MN, 2002, IEEE T IMAGE PROCESS, V11, P146, DOI 10.1109/83.982822
   JACOBS EW, 1992, SIGNAL PROCESS, V29, P251, DOI 10.1016/0165-1684(92)90085-B
   JACQUIN AE, 1993, P IEEE, V81, P1451, DOI 10.1109/5.241507
   JULIE JM, 1997, P IEEE INT FOR RES T, P2
   Lasfar A, 2000, INT C PATT RECOG, P1031, DOI 10.1109/ICPR.2000.905647
   MEHTRE BM, 1995, PATTERN RECOGN LETT, V16, P325, DOI 10.1016/0167-8655(94)00096-L
   Moon YH, 2000, IEEE T IMAGE PROCESS, V9, P941, DOI 10.1109/83.841539
   OIEN GE, 1994, SIGNAL PROCESS, V40, P105, DOI 10.1016/0165-1684(94)90024-8
   Schouten BAM, 2000, 2000 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL II, PROCEEDINGS, P534, DOI 10.1109/ICIP.2000.899474
   Tong CS, 2001, IEEE T IMAGE PROCESS, V10, P1269, DOI 10.1109/83.941851
   Wohlberg B, 1999, IEEE T IMAGE PROCESS, V8, P1716, DOI 10.1109/83.806618
NR 13
TC 27
Z9 28
U1 1
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2005
VL 7
IS 4
BP 597
EP 605
DI 10.1109/TMM.2005.846796
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 946XQ
UT WOS:000230607000001
DA 2024-07-18
ER

PT J
AU Lee, JYB
AF Lee, JYB
TI Channel folding-an algorithm to improve efficiency of multicast
   video-on-demand systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE caching; channel folding; merging; multicast; performance analysis;
   unified video-on-demand (UVoD); video-on-demand (VoD)
AB Recently a number of researchers have proposed and investigated new video-on-demand architectures that make use of network multicast to achieve vastly improved efficiency. Techniques such as hatching, patching, periodic broadcasting, chaining, and piggybacking have been explored both in isolation and in combinations. This study investigates a new tool in the arsenal-channel folding, where aggressive client-side caching is used to merge clients from one multicast channel into the other. In particular, this channel folding algorithm is applied to a previously proposed unified video-on-demand (UVoD) architecture to demonstrate and to quantify its impact on the performance and the tradeoff in a multicast video distribution architecture. This paper presents this channel folding algorithm in the context of UVoD and derives a performance model to obtain the system latency, the near-optimal channel partition policy, and the client buffer requirement. Numerical results show that channel folding can double the capacity of UVoD with a remarkably small overhead in the client buffer requirement.
C1 Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Hong Kong, Peoples R China.
EM jacklee@computer.org
RI Lee, Jack/P-7331-2019; Lee, Yiu Bun/G-3743-2011
OI Lee, Jack/0000-0002-4584-929X; Lee, Yiu Bun/0000-0002-3583-6428
CR Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P118, DOI 10.1109/MMCS.1996.534963
   Aggarwal CC, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, P253, DOI 10.1109/MMCS.1996.534983
   Almeroth KC, 1996, IEEE J SEL AREA COMM, V14, P1110, DOI 10.1109/49.508282
   Birk Y, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 1, P226, DOI 10.1109/MMCS.1999.779198
   CAI Y, 1999, P SPIE ACM C MULT CO, P204
   Carter SW, 1997, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATIONS AND NETWORKS, PROCEEDINGS, P200, DOI 10.1109/ICCCN.1997.623313
   Chiueh TC, 1996, P SOC PHOTO-OPT INS, V2615, P162, DOI 10.1117/12.229201
   Dan A., 1994, Proceedings ACM Multimedia '94, P15, DOI 10.1145/192593.192614
   Dan A, 1996, MULTIMEDIA SYST, V4, P112, DOI 10.1007/s005300050016
   DAN A, 1994, RC19588 IBM RES
   DEBEY HC, 1995, Patent No. 5421031
   Eager D, 2000, PROC SPIE, V3969, P206
   EAGER DL, 1998, P 4 INT WORKSH MULT, P18
   Ebram-Profeta EL, 1997, ACM MULTIMEDIA 97, PROCEEDINGS, P359, DOI 10.1145/266180.266387
   GAO L, 1998, P NOSSDAV CAMBR UK J
   Gao LX, 1999, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS, PROCEEDINGS VOL 2, P117, DOI 10.1109/MMCS.1999.778179
   Gao LX, 1999, ACM MULTIMEDIA 99, PROCEEDINGS, P203
   Golubchik L, 1996, MULTIMEDIA SYST, V4, P140, DOI 10.1007/s005300050019
   Golubchik L., 1995, P ACM SIGMETRICS JOI, P25
   Hua K., 1997, PROC SIGCOMM, P89
   Hua K. A., 1998, Proceedings ACM Multimedia 98, P191, DOI 10.1145/290747.290771
   Hua KA, 1998, IEEE IC COMP COM NET, P227, DOI 10.1109/ICCCN.1998.998781
   Juhn LS, 1997, IEEE T CONSUM ELECTR, V43, P1110, DOI 10.1109/30.642378
   Juhn LS, 1997, IEEE T BROADCAST, V43, P268, DOI 10.1109/11.632927
   Lau SW, 1998, MULTIMEDIA SYST, V6, P29, DOI 10.1007/s005300050074
   Lee JS, 2002, MAR BIOTECHNOL, V4, P1, DOI 10.1007/s10126-001-0077-3
   Liao W, 1997, IEEE MULTIMEDIA, V4, P51, DOI 10.1109/93.641879
   Pâris JF, 1998, IEEE IC COMP COM NET, P690, DOI 10.1109/ICCCN.1998.998831
   Paris JF, 1998, SIXTH INTERNATIONAL SYMPOSIUM ON MODELING, ANALYSIS AND SIMULATION OF COMPUTER AND TELECOMMUNICATION SYSTEMS, PROCEEDINGS, P127, DOI 10.1109/MASCOT.1998.693685
   Ramesh S, 2001, IEEE T CIRC SYST VID, V11, P440, DOI 10.1109/76.911167
   Shachnai H, 1997, PROCEEDINGS OF THE EIGHTH ISRAELI CONFERENCE ON COMPUTER SYSTEMS AND SOFTWARE ENGINEERING, P67, DOI 10.1109/ICCSSE.1997.599877
   Sheu S, 1997, IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA COMPUTING AND SYSTEMS '97, PROCEEDINGS, P110, DOI 10.1109/MMCS.1997.609583
   SHEU S, 1997, P 5 INT C DAT SYST A, P481
   Viswanathan S, 1996, MULTIMEDIA SYST, V4, P197, DOI 10.1007/s005300050023
NR 34
TC 6
Z9 6
U1 0
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2005
VL 7
IS 2
BP 366
EP 378
DI 10.1109/TMM.2005.843356
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 909OO
UT WOS:000227869400017
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nguyen, T
   Zakhor, A
AF Nguyen, T
   Zakhor, A
TI Multiple sender distributed video streaming
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE distributed streaming; multimedia streaming; network protocols; video
ID ERROR; TRANSMISSION; MULTICAST
AB With the explosive growth of video applications over the Internet, many approaches have been proposed to stream video effectively over packet switched, best-effort networks. In this paper, we propose a receiver-driven protocol for simultaneous video streaming from multiple senders to a single receiver in order to achieve higher throughput, and to increase tolerance to packet loss and delay due to network congestion. Our receiver-driven protocol employs a novel rate allocation algorithm (RAA) and a packet partition algorithm (PPA). The RAA, run at the receiver, determines the sending rate for each sender by taking into account available network bandwidth, channel characteristics, and a prespecified, fixed level of forward error correction, in such a way as to minimize the probability of packet loss. The PPA, run at the senders based on a set of parameters estimated by the receiver, ensures that every packet is sent by one and only one sender, and at the same time, minimizes the startup delay. Using both simulations and Internet experiments, we demonstrate the effectiveness of our protocol in reducing packet loss.
C1 Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
EM thinhq@eecs.berkeley.edu; avz@eecs.berkeley.edu
RI Zakhor, Avideh/GYA-1602-2022
OI Zakhor, Avideh/0000-0003-4770-6353
CR AGARWAL S, 2003, IEEE OPENARCH
   ANDERSEN DG, 2001, THESIS MIT CAMBRIDGE
   [Anonymous], 2001, P 9 ACM INT C MULTIM, DOI DOI 10.1145/500141.500205
   [Anonymous], IEEE J SEL AREAS COM
   APOSTOLOPOULOS J, 2000, P INF JUN
   Apostolopoulos JG, 2001, PROC SPIE, V4310, P392
   Bolot JC, 1999, IEEE INFOCOM SER, P1453, DOI 10.1109/INFCOM.1999.752166
   Borella MS, 1998, PROCEEDINGS OF THE 1998 ICPP WORKSHOPS ON ARCHITECTURAL AND OS SUPPORT FOR MULTIMEDIA APPLICATIONS - FLEXIBLE COMMUNICATION SYSTEMS - WIRELESS NETWORKS AND MOBILE COMPUTING, P3, DOI 10.1109/ICPPW.1998.721868
   Byers JW, 1999, IEEE INFOCOM SER, P275, DOI 10.1109/INFCOM.1999.749293
   CHAKARESKI J, 2003, DAT COMPR C SNOWB UT
   Chou PA, 2001, IEEE T MULTIMEDIA, V3, P108, DOI 10.1109/6046.909598
   Deering S, 1996, IEEE ACM T NETWORK, V4, P153, DOI 10.1109/90.490743
   Floyd S, 2000, ACM SIGCOMM COMP COM, V30, P43, DOI 10.1145/347057.347397
   Goyal VK, 2001, IEEE T INFORM THEORY, V47, P2199, DOI 10.1109/18.945243
   Horn U, 1999, SIGNAL PROCESS-IMAGE, V15, P77, DOI 10.1016/S0923-5965(99)00025-9
   Luby M, 1997, P 29 ANN ACM S THEOR
   Ma H., 1998, P INT SOC OPT ENG NO, V3528, P69
   MAXEMCHUK N, 1975, THESIS U PENNSYLVANI
   Mohr AE, 2000, IEEE J SEL AREA COMM, V18, P819, DOI 10.1109/49.848236
   NGUYEN T, 2003, THESIS U CALIFORNIA
   NGUYEN T, 2002, PACK VID WORKSH PITT
   Nguyen TP, 2002, PROC SPIE, V4673, P186
   Padmanabhan V. N., 2002, ACM NOSSDAV
   POOR HV, 1998, WIRLESS COMMUNICATIO
   Puri R, 2001, SIGNAL PROCESS-IMAGE, V16, P745, DOI 10.1016/S0923-5965(01)00005-4
   RABIN MO, 1989, J ACM, V36, P335, DOI 10.1145/62044.62050
   REIBMAN AR, 1999, P IEEE INT C IM PROC, V3, P837
   Rejaie R., 2003, NOSSDAV
   Robinson JA, 2000, IEEE J SEL AREA COMM, V18, P1099, DOI 10.1109/49.848259
   STEINBACH EG, 2002, TYRRH INT WORKSH DIG, P67
   TAN W, 1999, P 6 INT C IM PROC OC, V1, P401
   Tan WT, 1999, IEEE T MULTIMEDIA, V1, P172, DOI 10.1109/6046.766738
   Wang Y, 2001, IEEE T IMAGE PROCESS, V10, P351, DOI 10.1109/83.908500
   Yajnik M, 1999, IEEE INFOCOM SER, P345, DOI 10.1109/INFCOM.1999.749301
   [No title captured]
NR 35
TC 77
Z9 105
U1 0
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 315
EP 326
DI 10.1109/TMM.2003.822790
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400010
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shen, B
   Lee, SJ
   Basu, S
AF Shen, B
   Lee, SJ
   Basu, S
TI Caching strategies in transcoding-enabled proxy systems for streaming
   media distribution networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE network measurements; proxy caching; streaming in wireless networks;
   streaming media; streaming media distribution; video transcoding
ID VIDEO DELIVERY
AB With the wide availability of high-speed network access, we are experiencing high quality streaming media delivery over the Internet. The emergence of ubiquitous computing enables mobile users to access the Internet with their laptops, PDAs, or even cell phones. When nomadic users connect to the network via wireless links or phone lines, high quality video transfer can be problematic due to long delay or size mismatch between the application display and the screen. Our proposed solution to this problem is to enable network proxies with the transcoding capability, and hence provide different, appropriate video quality to different network environment. The proxies in our transcoding-enabled caching (TeC) system perform transcoding as well as caching for efficient rich media delivery to heterogeneous network users. This design choice allows us to perform content adaptation at the network edges. We propose three different TeC caching strategies. We describe each algorithm and discuss its merits and shortcomings. We also study how the user access pattern affects the performance of TeC caching algorithms and compare them with other approaches. We evaluate TeC performance by conducting two types of simulation. Our first experiment uses synthesized traces while the other uses real traces derived from an enterprise media server logs. The results indicate that compared with the traditional network caches, with marginal transcoding load, TeC improves the cache effectiveness, decreases the user-perceived latency, and reduces the traffic between the proxy and the content origin server.
C1 Hewlett Packard Labs, Palo Alto, CA 94304 USA.
C3 Hewlett-Packard
RP Hewlett Packard Labs, Palo Alto, CA 94304 USA.
EM sjlee@hpl.hp.com
RI Lee, Sung-Ju/D-8084-2015
CR Acharya S, 1999, PROC INT CONF DATA, P40, DOI 10.1109/ICDE.1999.754896
   ACHARYA S, 2000, P NOSSDAV 2000 CHAP
   AMIR E, 1995, P ACM MULT 95 SAN FR, P255
   Assunçao PAA, 1998, IEEE T CIRC SYST VID, V8, P953, DOI 10.1109/76.736724
   Bharadvaj H, 1998, SYM REL DIST SYST, P118, DOI 10.1109/RELDIS.1998.740482
   Bommaiah E, 2000, SIXTH IEEE REAL-TIME TECHNOLOGY AND APPLICATIONS SYMPOSIUM, PROCEEDINGS, P111, DOI 10.1109/RTTAS.2000.852456
   Cardellini V., 2000, Proceedings of the Ninth International Conference on Information and Knowledge Management. CIKM 2000, P520, DOI 10.1145/354756.354861
   Chandra S, 2000, IEEE J SEL AREA COMM, V18, P2544, DOI 10.1109/49.898736
   Chang CY, 2002, PROC INT CONF DATA, P383, DOI 10.1109/ICDE.2002.994752
   Chen S., 2003, P 13 INT WORKSH NETW, P22
   CHESIRE M, 2001, P USITS 2001 SAN FRA
   Fox A, 1998, IEEE PERS COMMUN, V5, P10, DOI 10.1109/98.709365
   HARTANTO F, 2002, P IEEE ICME 2002 LAU
   Jia Wang, 1999, Computer Communication Review, V29, P36, DOI 10.1145/505696.505701
   Jin S, 2001, COMPUT COMMUN, V24, P174, DOI 10.1016/S0140-3664(00)00312-1
   Jin SD, 2002, INT CON DISTR COMP S, P153, DOI 10.1109/ICDCS.2002.1022252
   Kangasharju J, 1998, COMPUT NETWORKS ISDN, V30, P2113, DOI 10.1016/S0169-7552(98)00254-2
   Lee SJ, 2002, COMPUT COMMUN, V25, P424, DOI 10.1016/S0140-3664(01)00414-5
   MAHESHWARI A, 2002, P IEEE RIDE 2002 SAN
   Miao ZR, 2002, IEEE J SEL AREA COMM, V20, P1315, DOI 10.1109/JSAC.2002.802061
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   ONEIL EJ, 1993, P ACM SIGMOD INT C M, P297
   REJAIE R, 2001, P ACM NOSSDAV 2001 P
   Sen S, 1999, IEEE INFOCOM SER, P1310, DOI 10.1109/INFCOM.1999.752149
   SHEN B, 2001, HPL2001258R1
   SHEN B, 2002, HPL2002210R1
   TANG X, 2002, P ICPP 2002 VANC BC
   Verscheure O, 2002, COMPUT COMMUN, V25, P413, DOI 10.1016/S0140-3664(01)00413-3
   Vetro A, 2003, IEEE SIGNAL PROC MAG, V20, P18, DOI 10.1109/MSP.2003.1184336
   Zhang ZL, 2000, IEEE ACM T NETWORK, V8, P429, DOI 10.1109/90.865072
NR 30
TC 86
Z9 98
U1 0
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2004
VL 6
IS 2
BP 375
EP 386
DI 10.1109/TMM.2003.822791
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 807NO
UT WOS:000220508400016
DA 2024-07-18
ER

PT J
AU Fung, KT
   Chan, YL
   Siu, WC
AF Fung, KT
   Chan, YL
   Siu, WC
TI Low-complexity and high-quality frame-skipping transcoder for continuous
   presence multipoint video conferencing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE compressed-domain processing; frame skipping; video compression; video
   conferencing; video transcoding
ID MOTION ESTIMATION; SEARCH ALGORITHM; DECIMATION
AB This paper presents a new frame-skipping transcoding approach for video combiners in multipoint video conferencing. Transcoding is regarded as a process of converting a previously compressed video bitstream into a lower bitrate bitstream. A high transcoding ratio may result in an unacceptable picture quality when the incoming video bitstream is transcoded with the full frame rate. Frame skipping is often used as an efficient scheme to allocate more bits to representative frames, so that an acceptable quality for each frame can be maintained. However, the skipped frame must be decompressed completely, and should act as the reference frame to the nonskipped frame for reconstruction. The newly quantized DCT coefficients of prediction error need to be recomputed for the nonskipped frame with reference to the previous nonskipped frame; this can create an undesirable complexity in the real time application as well as introduce re-encoding error. A new frame-skipping transcoding architecture for improved picture quality and reduced complexity is proposed. The proposed architecture is mainly performed on the discrete cosine transform (DCT) domain to achieve a low complexity transcoder. It is observed that the re-encoding error is avoided at the frame-skipping transcoder when the strategy of direct summation of DCT coefficients is employed. By using the proposed frame-skipping transcoder and dynamically allocating more frames to the active participants in video combining, we are able to make more uniform peak signal-to-noise ratio (PSNR) performance of the subsequences and the video qualities of the active subsequences can be improved significantly.
C1 Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Hong Kong Polytech Univ, Dept Elect & Informat Engn, Ctr Multimedia Signal Proc, Kowloon, Hong Kong, Peoples R China.
EM fung@eie.polyu.edu.hk; enwcsiu@polyu.edu.hk; enylchan@polyu.edu.hk
RI Chan, Yui-Lam/C-3799-2014
OI Chan, Yui-Lam/0000-0002-1473-094X
CR ASSUNCAO P, 1998, IEEE T CIRCUITS SYST, V8
   Assuncao PAA, 1996, INT CONF ACOUST SPEE, P1998, DOI 10.1109/ICASSP.1996.544846
   Chan YL, 1996, IEEE T CIRC SYST VID, V6, P113, DOI 10.1109/76.486426
   Chan YL, 1998, J VIS COMMUN IMAGE R, V9, P139, DOI 10.1006/jvci.1998.0388
   CHAN YL, P I ELECT ENG VIS IM, V144, P136
   Cheng FH, 1999, IEEE T CIRC SYST VID, V9, P977, DOI 10.1109/76.795049
   CHIARIGLIONE L, 1995, P IEEE, V83, P151, DOI 10.1109/5.364469
   Hwang JN, 1998, 1998 IEEE SECOND WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P616, DOI 10.1109/MMSP.1998.739049
   *ISO IEC, 1996, 138182 ISO IEC
   *ISO IEC, 1993, 111722 ISO IEC
   *ITU T STUD GROUP, 1992, VID COD AUD SERV
   *ITU T STUD GROUP, 1992, MULT UN AUD SYST US
   *ITU T STUD GROUP, 1997, VID COD LOW BITR COM
   *ITU T STUD GROUP, 1992, PROC EST COMM BETW 3
   Keeman G., 1996, SIGNAL PROCESS-IMAGE, V8, P481
   Kwok SH, 1997, GRAPH MODEL IM PROC, V59, P128, DOI 10.1006/gmip.1997.0423
   Kwok SH, 1998, IEEE T CIRC SYST VID, V8, P104, DOI 10.1109/76.660833
   LEI SM, 1994, IEEE T CIRC SYST VID, V4, P425, DOI 10.1109/76.313137
   LIN CW, 2000, P IEEE INT S CIRC SY, V2, P17
   MORRISON DG, P 6 INTY WORKSH PACK
   Nakajima Y., 1995, Proceedings. International Conference on Image Processing (Cat. No.95CB35819), P408, DOI 10.1109/ICIP.1995.537658
   STUTTGEN HJ, 1995, IEEE MULTIMEDIA, V2, P42, DOI 10.1109/93.410513
   Sun HF, 1996, IEEE T CIRC SYST VID, V6, P191, DOI 10.1109/76.488826
   Sun MT, 1997, IEEE T CIRC SYST VID, V7, P855, DOI 10.1109/76.644065
   Sun MT, 1998, IEEE T CIRCUITS-II, V45, P644, DOI 10.1109/82.673649
   Tham JY, 1998, IEEE T CIRC SYST VID, V8, P369, DOI 10.1109/76.709403
   YIP P, 1988, CIRCUITS SYSTEMS SIG, P4
   YONG M, 1994, P 6 INT WORKSH PACK
   Youn J, 1998, IEEE T CONSUM ELECTR, V44, P649, DOI 10.1109/30.713176
   Youn J, 1999, IEEE T MULTIMEDIA, V1, P30, DOI 10.1109/6046.748169
NR 30
TC 26
Z9 30
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2004
VL 6
IS 1
BP 31
EP 46
DI 10.1109/TMM.2003.819761
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 765QP
UT WOS:000188295200003
OA Green Published
DA 2024-07-18
ER

PT J
AU Fang, H
   Jia, ZY
   Qiu, YP
   Zhang, JY
   Zhang, WM
   Chang, EC
AF Fang, Han
   Jia, Zhaoyang
   Qiu, Yupeng
   Zhang, Jiyi
   Zhang, Weiming
   Chang, Ee-Chien
TI De-END: Decoder-Driven Watermarking Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Decoding; Watermarking; Feature extraction; Robustness; Transform
   coding; Distortion; Visualization; Deep-learning Watermarking;
   decoder-driven
ID IMAGE WATERMARKING
AB Deep-learning-based watermarking technique is being extensively studied. Most existing approaches adopt a similar encoder-driven scheme which we name END (Encoder-NoiseLayer-Decoder) architecture. In this paper, we revamp the architecture and creatively design a decoder-driven watermarking network dubbed De-END which greatly outperforms the existing END-based methods. The motivation for designing De-END originated from the potential drawback we discovered in END architecture: The encoder may embed redundant features that are not necessary for decoding, limiting the performance of the whole network. We conducted a detailed analysis and found that such limitations are caused by unsatisfactory coupling between the encoder and decoder in END. De-END addresses such drawbacks by adopting a Decoder -Encoder-Noiselayer-Decoder architecture. In De-END, the host image is firstly processed by the decoder to generate a latent feature map instead of being directly fed into the encoder. This latent feature map is concatenated to the original watermark message and then processed by the encoder. This change in design is crucial as it makes the feature of encoder and decoder directly shared thus the encoder and decoder are better coupled. We conducted extensive experiments and the results show that this framework outperforms the existing state-of-the-art (SOTA) END-based deep learning watermarking both in visual quality and robustness. On the premise of the same decoder structure, the visual quality (measured by PSNR) of De-END improves by 1.6dB (45.16dB to 46.84dB), and extraction accuracy after JPEG compression (QF=50) distortion outperforms more than 4% (94.9% to 99.1%).
C1 [Fang, Han; Qiu, Yupeng; Zhang, Jiyi; Chang, Ee-Chien] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Jia, Zhaoyang; Zhang, Weiming] Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
C3 National University of Singapore; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Chang, EC (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.; Zhang, WM (corresponding author), Univ Sci & Technol China, CAS Key Lab Electromagnet Space Informat, Hefei 230026, Peoples R China.
EM fanghan@nus.edu.sg; jzy_ustc@mail.ustc.edu.cn; qiu_yupeng@u.nus.edu;
   jiyizhang@u.nus.edu; zhangwm@ustc.edu.cn; changec@comp.nus.edu.sg
RI Tang, Zhicheng/H-2817-2018
OI Tang, Zhicheng/0000-0002-4595-193X; Qiu, Yupeng/0000-0002-2494-4992;
   Zhang, Weiming/0000-0001-5576-6108
FU National Research Foundation, Singapore
FX This work was supported by the National Research Foundation, Singapore
   under its Strategic Capability Research Centres Funding Initiative.
CR Ahmadi M, 2020, EXPERT SYST APPL, V146, DOI 10.1016/j.eswa.2019.113157
   Chen ZG, 2018, IEEE T MULTIMEDIA, V20, P1973, DOI 10.1109/TMM.2018.2794985
   Daren H., 2001, P IEEE INT C MULT EX, P313
   Fang H, 2021, IEEE T CIRC SYST VID, V31, P1436, DOI 10.1109/TCSVT.2020.3009349
   Fang H, 2019, IEEE T INF FOREN SEC, V14, P1403, DOI 10.1109/TIFS.2018.2878541
   Gao YC, 2019, J REAL-TIME IMAGE PR, V16, P565, DOI 10.1007/s11554-018-0812-x
   Hamidi M, 2015, I C COMP SYST APPLIC
   Hu HT, 2019, DIGIT SIGNAL PROCESS, V87, P75, DOI 10.1016/j.dsp.2019.01.006
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang Y, 2019, IEEE T MULTIMEDIA, V21, P2447, DOI 10.1109/TMM.2019.2907475
   Jia ZY, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P41, DOI 10.1145/3474085.3475324
   Kandi H, 2017, COMPUT SECUR, V65, P247, DOI 10.1016/j.cose.2016.11.016
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Kang XG, 2010, IEEE T INF FOREN SEC, V5, P1, DOI 10.1109/TIFS.2009.2039604
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Ko HJ, 2020, INFORM SCIENCES, V517, P128, DOI 10.1016/j.ins.2019.11.005
   Langelaar GC, 2001, IEEE T IMAGE PROCESS, V10, P148, DOI 10.1109/83.892451
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu Y, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1509, DOI 10.1145/3343031.3351025
   Mellimi S, 2021, PATTERN RECOGN LETT, V151, P222, DOI 10.1016/j.patrec.2021.08.015
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Mun SM, 2017, Arxiv, DOI arXiv:1704.03248
   Mun SM, 2019, NEUROCOMPUTING, V337, P191, DOI 10.1016/j.neucom.2019.01.067
   Paszke A, 2019, ADV NEUR IN, V32
   sipi.usc, A. "The USC-SIPI image database
   Tancik M, 2020, PROC CVPR IEEE, P2114, DOI 10.1109/CVPR42600.2020.00219
   Tsui TK, 2008, IEEE T INF FOREN SEC, V3, P16, DOI 10.1109/TIFS.2007.916275
   Wen BY, 2019, Arxiv, DOI arXiv:1910.01221
   Wengrowski E, 2019, PROC CVPR IEEE, P1515, DOI 10.1109/CVPR.2019.00161
   Xiyang Luo, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13545, DOI 10.1109/CVPR42600.2020.01356
   Zhang C., 2020, Adv. Neural Inf. Process. Syst., V33, P10223
   Zhang PY, 2021, IEEE T VIS COMPUT GR, V27, P326, DOI 10.1109/TVCG.2020.3030343
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhu JR, 2018, LECT NOTES COMPUT SC, V11219, P682, DOI 10.1007/978-3-030-01267-0_40
NR 34
TC 1
Z9 1
U1 14
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7571
EP 7581
DI 10.1109/TMM.2022.3223559
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000060
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Fu, X
   Deng, HY
   Yuan, X
   Hu, JL
AF Fu, Xiao
   Deng, Hangyu
   Yuan, Xin
   Hu, Jinglu
TI Generating High Coherence Monophonic Music Using Monte-Carlo Tree Search
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Machine learning; music generation; sequence prediction; Monte-Carlo
   tree search; transformer
ID GO
AB Music generation task is commonly considered as a note-by-note prediction problem. Moreover, prediction models generating one musical note at a time may ignore the overall coherence because the music phrase is incomplete and unable to demonstrate musicality. To address these issues, in this study, we propose a feasible monophonic music generation framework that can simulate subsequent trends for each predicted musical note. The framework generates a musical note mainly in three steps: 1) a sequence prediction model is used to predict the most potential candidates, 2) the subsequent trends for each candidate are modeled and evaluated, and 3) the best candidate is selected as the final result. We use the Monte-Carlo tree search algorithm because of its great capability of discovering near-optimal results. We establish a method of training a value network that can assess musical coherence to evaluate the simulated sequences. Further, we used a smoothed polynomial upper confidence trees algorithm to improve the accuracy and efficiency of the search process. An accurate dataset labeled by us, which contains 36 transcribed samples from real-world pop songs, was used to validate our framework. Compared with the note-by-note sequence prediction model, our framework exhibits a better sense of musicality. Our framework can be applied to generate symbolic monophonic music, particularly the main melody track in pop music.
C1 [Fu, Xiao; Deng, Hangyu; Yuan, Xin; Hu, Jinglu] Waseda Univ, Grad Sch Informat Prod & Syst, Informat Architecture, Kitakyushu 8080135, Japan.
C3 Waseda University
RP Hu, JL (corresponding author), Waseda Univ, Grad Sch Informat Prod & Syst, Informat Architecture, Kitakyushu 8080135, Japan.
EM hirabarahyt@gmail.com; deng.hangyu@fuji.waseda.jp;
   sherryyuan@ruri.waseda.jp; jinglu@waseda.jp
CR Andersson H. H., 2016, Programming a hearthstone agentusing Monte Carlo tree search
   [Anonymous], 2016, P INT C LEARN REPR
   [Anonymous], 2016, International Joint Conference on Artificial Intelligence
   Choe JSB, 2019, IEEE CONF COMPU INTE, P1
   Chu H, 2017, PROC INT C LEARN REP, P1
   Coulom R, 2007, LECT NOTES COMPUT SC, V4630, P72
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   Gao C, 2018, IEEE T GAMES, V10, P336, DOI 10.1109/TG.2017.2785042
   Guo X., 2014, Advances in Neural Information Processing Systems, P3338
   Hooktheory, US
   Hsueh CH, 2015, LECT NOTES COMPUT SC, V9525, P29, DOI 10.1007/978-3-319-27992-3_4
   Huanget C.-Z. A, 2019, P INT C LEARN REPR, P1
   Jhamtani T., 2019, P MACH LEARN MUS DIS, P1
   Jouandeau N, 2015, 2015 CONFERENCE ON TECHNOLOGIES AND APPLICATIONS OF ARTIFICIAL INTELLIGENCE (TAAI), P467, DOI 10.1109/TAAI.2015.7407121
   Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29
   Mogren O., 2016, CORR
   Mozer M. C., 1994, Connection Science, V6, P247, DOI 10.1080/09540099408915726
   Rosin C, 2011, ANN MATH ARTIF INTEL, V61, P203, DOI 10.1007/s10472-011-9258-6
   Santos Andre, 2017, 2017 IEEE Conference on Computational Intelligence and Games (CIG), P272, DOI 10.1109/CIG.2017.8080446
   Schaefers L, 2015, IEEE T COMP INTEL AI, V7, P361, DOI 10.1109/TCIAIG.2014.2346997
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Van den Broeck G, 2009, LECT NOTES ARTIF INT, V5828, P367, DOI 10.1007/978-3-642-05224-8_28
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang SQ, 2018, CHIN CONT DECIS CONF, P6639, DOI 10.1109/CCDC.2018.8408299
   Yang L.-C., 2017, ARXIV PREPRINT ARXIV, P324
   Ye W., 2021, ADV NEURAL INFORM PR, P25476, DOI DOI 10.48550/ARXIV.2111.00210
   Yu LT, 2017, AAAI CONF ARTIF INTE, P2852
   Zhang N, 2023, IEEE T NEUR NET LEAR, V34, P1754, DOI 10.1109/TNNLS.2020.2990746
NR 29
TC 1
Z9 1
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3763
EP 3772
DI 10.1109/TMM.2022.3165718
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AD7R0
UT WOS:001116593700005
DA 2024-07-18
ER

PT J
AU Gao, YT
   Liang, LQ
   Lang, CY
   Feng, SH
   Li, YD
   Wei, YC
AF Gao, Yutong
   Liang, Liqian
   Lang, Congyan
   Feng, Songhe
   Li, Yidong
   Wei, Yunchao
TI Clicking Matters: Towards Interactive Human Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human parsing; interactive segmentation; semantic segmentation
ID IMAGE SEGMENTATION
AB In this work, we focus on Interactive Human Parsing (IHP), which aims to segment a human image into multiple human body parts with guidance from users' interactions. This new task inherits the class-aware property of human parsing, which cannot be well solved by traditional interactive image segmentation approaches that are generally class-agnostic. To tackle this new task, we first exploit user clicks to identify different human parts in the given image. These clicks are subsequently transformed into semantic-aware localization maps, which are concatenated with the RGB image to form the input of the segmentation network and generate the initial parsing result. To enable the network to better perceive user's purpose during the correction process, we investigate several principal ways for the refinement, and reveal that random-sampling-based click augmentation is the best way for promoting the correction effectiveness. Furthermore, we also propose a semantic-perceiving loss (SP-loss) to augment the training, which can effectively exploit the semantic relationships of clicks for better optimization. To the best knowledge, this work is the first attempt to tackle the human parsing task under the interactive setting. Our IHP solution achieves 85% mIoU on the benchmark LIP, 80% mIoU on PASCAL-Person-Part and CIHP, 75% mIoU on Helen with only 1.95, 3.02, 2.84 and 1.09 clicks per class respectively. These results demonstrate that we can simply acquire high-quality human parsing masks with only a few human effort. We hope this work can motivate more researchers to develop data-efficient solutions to IHP in the future.
C1 [Gao, Yutong; Liang, Liqian; Lang, Congyan; Feng, Songhe; Li, Yidong] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Wei, Yunchao] Beijing Jiaotong Univ, Inst Informat Sci, Beijing 10044, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University
RP Lang, CY (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
EM ytgao92@bjtu.edu.cn; lqliang@bjtu.edu.cn; cylang@bjtu.edu.cn;
   shfeng@bjtu.edu.cn; ydli@bjtu.edu.cn; wychao1987@gmail.com
OI Liang, Liqian/0000-0001-8701-4074
FU National Natural Science Foundation of China [62072027, 61872032,
   62076021]; Beijing Natural Science Foundation [4202057, 4202058,
   4202060]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072027, 61872032, and 62076021 and in
   part by the Beijing Natural Science Foundation under Grants 4202057,
   4202058, and 4202060.
CR Acuna D, 2018, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2018.00096
   Agustsson E, 2019, PROC CVPR IEEE, P11614, DOI 10.1109/CVPR.2019.01189
   Andriluka M, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1957, DOI 10.1145/3240508.3241916
   Bai X, 2009, INT J COMPUT VISION, V82, P113, DOI 10.1007/s11263-008-0191-z
   Benard A, 2017, Arxiv, DOI arXiv:1801.00269
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chen DJ, 2018, AAAI CONF ARTIF INTE, P2119
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen LC, 2016, PROC CVPR IEEE, P3640, DOI 10.1109/CVPR.2016.396
   Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254
   Criminisi A, 2008, LECT NOTES COMPUT SC, V5302, P99, DOI 10.1007/978-3-540-88682-2_9
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang H.-S., 2020, P IEEECVF C COMPUTER, P11444
   Gong K, 2018, LECT NOTES COMPUT SC, V11208, P805, DOI 10.1007/978-3-030-01225-0_47
   Gong K, 2019, PROC CVPR IEEE, P7442, DOI [10.1109/cvpr.2019.00763, 10.1109/CVPR.2019.00763]
   Grady L, 2006, IEEE T PATTERN ANAL, V28, P1768, DOI 10.1109/TPAMI.2006.233
   Gulshan V, 2010, PROC CVPR IEEE, P3129, DOI 10.1109/CVPR.2010.5540073
   Huang T., 2019, SCI INF CHINA SCI, V62, P1
   Jang WD, 2019, PROC CVPR IEEE, P5292, DOI 10.1109/CVPR.2019.00544
   KASS M, 1987, INT J COMPUT VISION, V1, P321, DOI 10.1007/BF00133570
   Le H, 2018, LECT NOTES COMPUT SC, V11218, P20, DOI 10.1007/978-3-030-01264-9_2
   Le V, 2012, LECT NOTES COMPUT SC, V7574, P679, DOI 10.1007/978-3-642-33712-3_49
   Li Y, 2004, ACM T GRAPHIC, V23, P303, DOI 10.1145/1015706.1015719
   Li ZW, 2018, PROC CVPR IEEE, P577, DOI 10.1109/CVPR.2018.00067
   Liang XD, 2019, IEEE T PATTERN ANAL, V41, P871, DOI 10.1109/TPAMI.2018.2820063
   Liang XD, 2017, PROC CVPR IEEE, P2175, DOI 10.1109/CVPR.2017.234
   Liang XD, 2016, IEEE T MULTIMEDIA, V18, P1175, DOI 10.1109/TMM.2016.2542983
   Liang XD, 2015, IEEE I CONF COMP VIS, P1386, DOI 10.1109/ICCV.2015.163
   Liew JH, 2017, IEEE I CONF COMP VIS, P2746, DOI 10.1109/ICCV.2017.297
   Lin J., 2016, SIGGRAPH ASIA 2016 V, P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2015, IEEE T MULTIMEDIA, V17, P1347, DOI 10.1109/TMM.2015.2443559
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Luo YW, 2018, LECT NOTES COMPUT SC, V11213, P424, DOI 10.1007/978-3-030-01240-3_26
   Mahadevan S., 2018, Plant disease forecasting in the era of climate change: Trends and applications, P1
   Maninis KK, 2018, PROC CVPR IEEE, P616, DOI 10.1109/CVPR.2018.00071
   Maninis KK, 2018, IEEE T PATTERN ANAL, V40, P819, DOI 10.1109/TPAMI.2017.2700300
   Mortensen E. N., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P191, DOI 10.1145/218380.218442
   Nie XC, 2018, LECT NOTES COMPUT SC, V11209, P519, DOI 10.1007/978-3-030-01228-1_31
   Papadopoulos DP, 2017, IEEE I CONF COMP VIS, pCP38, DOI 10.1109/ICCV.2017.528
   Paszke A, 2019, ADV NEUR IN, V32
   Price BL, 2010, PROC CVPR IEEE, P3161, DOI 10.1109/CVPR.2010.5540079
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Ruan T, 2019, AAAI CONF ARTIF INTE, P4814
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shiyin Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12231, DOI 10.1109/CVPR42600.2020.01225
   Smith BM, 2013, PROC CVPR IEEE, P3484, DOI 10.1109/CVPR.2013.447
   Sofiiuk Konstantin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8620, DOI 10.1109/CVPR42600.2020.00865
   Tao Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9260, DOI 10.1109/CVPR42600.2020.00928
   Vezhnevets V., 2005, Proc. Graphicon, V1, P150
   Wang Z, 2019, PROC CVPR IEEE, P7492, DOI 10.1109/CVPR.2019.00768
   Wenguan Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8926, DOI 10.1109/CVPR42600.2020.00895
   Xiaomei Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8968, DOI 10.1109/CVPR42600.2020.00899
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu N, 2016, PROC CVPR IEEE, P373, DOI 10.1109/CVPR.2016.47
   Yang L, 2019, PROC CVPR IEEE, P364, DOI 10.1109/CVPR.2019.00045
   Yuan YH, 2021, Arxiv, DOI arXiv:1809.00916
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zheng Lin, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13336, DOI 10.1109/CVPR42600.2020.01335
NR 61
TC 3
Z9 3
U1 3
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3190
EP 3203
DI 10.1109/TMM.2022.3156812
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Han, L
   Yin, ZZ
AF Han, Liang
   Yin, Zhaozheng
TI Global Memory and Local Continuity for Video Object Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Object detection; Detectors; Proposals; Target
   tracking; Signal processing algorithms; Costs; Video object detection;
   global memory bank; feature aggregation; local continuity; object
   tracker
AB To deal with the challenges in video object detection (VOD), such as occlusion and motion blur, many state-of-the-art video object detectors adopt a feature aggregation module to encode the long-range contextual information to support the current frame. The main drawbacks of these detectors are three-folds: first, the frame-wise detection slows down the detection speed; second, the frame-wise detection usually ignores the local continuity of the objects in a video, resulting in temporal inconsistent detection; third, the feature aggregation module usually encodes temporal features either from a local video clip or a single video, without exploiting the features in other videos. In this work, we develop an online VOD algorithm, aiming at a balanced high-speed and high-accuracy, by exploiting the global memory and local continuity. In the algorithm, an effective and efficient global memory bank (GMB) is designed to deposit and update object class features, which enables us to exploit the support features in other videos to enhance object features in the current video frames. Besides, to further speed up the detection, we design an object tracker to perform object detection for non-key frames based on the detection results of the key frame by leveraging the local continuity property of the video. Considering the trade-off between detection accuracy and speed, the proposed framework achieves superior performance on the ImageNet VID dataset. Source codes will be released to the public via our GitHub website.
C1 [Han, Liang; Yin, Zhaozheng] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Yin, Zhaozheng] SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Stony Brook; State University of New York (SUNY) System; State
   University of New York (SUNY) Stony Brook
RP Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.; Yin, ZZ (corresponding author), SUNY Stony Brook, Dept Biomed Informat, Stony Brook, NY 11794 USA.
EM liahan@cs.stonybrook.edu; zyin@cs.stonybrook.edu
OI Yin, Zhaozheng/0000-0002-9602-6488
FU National Science Foundation through National Robotics Initiative
   [CMMI-1954548]; Human Technology Frontier [ECCS-2025929]
FX The work of Liang Han and Zhaozheng Yin was supported in part by the
   National Science Foundation through National Robotics Initiative under
   Grant CMMI-1954548, and in part by Human Technology Frontier under Grant
   ECCS-2025929.
CR Angelini F, 2020, IEEE T MULTIMEDIA, V22, P1433, DOI 10.1109/TMM.2019.2944745
   Bertasius G, 2018, LECT NOTES COMPUT SC, V11216, P342, DOI 10.1007/978-3-030-01258-8_21
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cai Q, 2018, PROC CVPR IEEE, P4080, DOI 10.1109/CVPR.2018.00429
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen K, 2018, PROC CVPR IEEE, P7814, DOI 10.1109/CVPR.2018.00815
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Chen Y., 2020, CVPR, P10337
   Chun-Han Yao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P160, DOI 10.1007/978-3-030-58568-6_10
   Dai Jifeng, 2016, Advances in Neural Information Processing Systems, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075
   Deng HM, 2019, IEEE I CONF COMP VIS, P6677, DOI 10.1109/ICCV.2019.00678
   Deng JJ, 2021, IEEE T MULTIMEDIA, V23, P846, DOI 10.1109/TMM.2020.2990070
   Deng JJ, 2019, IEEE I CONF COMP VIS, P7022, DOI 10.1109/ICCV.2019.00712
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Feichtenhofer C, 2017, IEEE I CONF COMP VIS, P3057, DOI 10.1109/ICCV.2017.330
   Fu C.-Y., 2017, DSSD: Deconvolutional Single Shot Detector
   Ge Wenbin, 2021, CVPR, P16836
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Guo CX, 2019, IEEE I CONF COMP VIS, P3908, DOI 10.1109/ICCV.2019.00401
   Han L, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1469, DOI 10.1145/3394171.3413927
   Han Liang, 2021, IEEE T CIRCUITS SYST
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hetang Congrui, 2017, IMPRESSION NETWORK V
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378
   Hu L, 2021, PROC CVPR IEEE, P4142, DOI 10.1109/CVPR46437.2021.00413
   Jiang ZK, 2019, AAAI CONF ARTIF INTE, P8529
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kang K, 2018, IEEE T CIRC SYST VID, V28, P2896, DOI 10.1109/TCSVT.2017.2736553
   Kang K, 2017, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2017.101
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   KIM PK, 2019, P 9TH INT C ADV COMP, P81
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Lin LJ, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1855, DOI 10.1145/3394171.3413583
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   LIU M, 2019, LOOKING FAST AND SLO
   Liu MS, 2018, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2018.00596
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu Z, 2017, IEEE T CIRC SYST VID, V27, P2527, DOI 10.1109/TCSVT.2016.2595324
   Liu Z, 2014, IEEE T CIRC SYST VID, V24, P1522, DOI 10.1109/TCSVT.2014.2308642
   Luo H, 2019, AAAI CONF ARTIF INTE, P8803
   LYU Y, 2020, PLUG PLAY CONVOLUTIO
   Mingfei Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P431, DOI 10.1007/978-3-030-58589-1_26
   Moniruzzaman M, 2022, IEEE T MULTIMEDIA, V24, P689, DOI 10.1109/TMM.2021.3058050
   Oh SW, 2019, IEEE I CONF COMP VIS, P9225, DOI 10.1109/iccv.2019.00932
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Redmon J., 2018, P IEEE C COMP VIS PA
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Shvets M, 2019, IEEE I CONF COMP VIS, P9755, DOI 10.1109/ICCV.2019.00985
   Sun GX, 2021, AAAI CONF ARTIF INTE, V35, P2620
   Tang P, 2020, IEEE T PATTERN ANAL, V42, P1272, DOI 10.1109/TPAMI.2019.2910529
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Vaswani Ashish, 2017, Advances in Neural Information Processing Systems (NeurIPS), V17, P6000, DOI DOI 10.48550/ARXIV.1706.03762
   Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274
   Wang SY, 2019, IEEE I CONF COMP VIS, P7103, DOI 10.1109/ICCV.2019.00720
   Wang SY, 2018, LECT NOTES COMPUT SC, V11217, P557, DOI 10.1007/978-3-030-01261-8_33
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P38, DOI 10.1109/TIP.2017.2754941
   Wang XG, 2021, COMPUT VIS IMAGE UND, V206, DOI 10.1016/j.cviu.2021.103188
   Wang Z., 2020, COMPUTER VISION ECCV, P107, DOI [10.1007/978-3-030-58621-8_7, DOI 10.1007/978-3-030-58621-8_7]
   Wu HP, 2019, IEEE I CONF COMP VIS, P9216, DOI 10.1109/ICCV.2019.00931
   Xiao FY, 2018, LECT NOTES COMPUT SC, V11212, P494, DOI 10.1007/978-3-030-01237-3_30
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Zhengkai Jiang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12361), P18, DOI 10.1007/978-3-030-58517-4_2
   Zhou XF, 2018, IEEE T MULTIMEDIA, V20, P2993, DOI 10.1109/TMM.2018.2829605
   Zhu XZ, 2018, PROC CVPR IEEE, P7210, DOI 10.1109/CVPR.2018.00753
   Zhu XZ, 2017, IEEE I CONF COMP VIS, P408, DOI 10.1109/ICCV.2017.52
   Zhu XZ, 2017, PROC CVPR IEEE, P4141, DOI 10.1109/CVPR.2017.441
   Zhujun Xu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P220, DOI 10.1007/978-3-030-58595-2_14
NR 77
TC 5
Z9 5
U1 16
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3681
EP 3693
DI 10.1109/TMM.2022.3164253
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500012
DA 2024-07-18
ER

PT J
AU Li, P
   Gao, J
   Zhang, JN
   Jin, S
   Chen, ZK
AF Li, Peng
   Gao, Jing
   Zhang, Jianing
   Jin, Shan
   Chen, Zhikui
TI Deep Reinforcement Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Prototypes; Data mining; Markov processes; Deep learning; Data
   structures; Neural networks; Clustering methods; Cluster analysis; deep
   clustering; Markov decision processes; neural networks; reinforcement
   clustering
AB Deep clustering has attracted plentiful attention in various domains owning to the superior performance. However, the previous deep clustering methods are guided by pre-specified clustering strategies that lack sustained explorations of data structures, degrading recognition of intrinsic patterns hidden in data. To address this challenge, deep reinforcement clustering (DRC) is proposed to learn an adaptive partition policy for pattern mining, which can fully explore structure knowledge of data in an adaptive manner. DRC is defined as a Markov decision process of data partitions, which chooses the optimal cluster prototype for data via maximizing the cumulative reward in state transition of environment. To implement the definition, a Bernoulli action prototype is devised to capture decision distributions in the transition of states, where the heavy-tailed Cauchy distribution precisely measures the structure divergences of data. Furthermore, a reward maximizing policy is designed to guide sustained explorations of data structures, which ensures intra-cluster compactness and inter-cluster separation of data partitions. Finally, extensive experiments are conducted on eight benchmark datasets, and the results demonstrate that DRC outperforms the state-of-the-art baseline methods.
C1 [Li, Peng; Gao, Jing; Zhang, Jianing; Jin, Shan; Chen, Zhikui] Dalian Univ Technol, Sch Software Technol, Dalian 116620, Peoples R China.
   [Gao, Jing; Chen, Zhikui] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116620, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology
RP Gao, J (corresponding author), Dalian Univ Technol, Sch Software Technol, Dalian 116620, Peoples R China.
EM lipeng2015@mail.dlut.edu.cn; gaojing@dlut.edu.cn;
   zhang1234567893@mail.dlut.edu.cn; jinshan0924@mail.dlut.edu.cn;
   zkchen@dlut.edu.cn
OI Zhang, Jianing/0000-0002-0695-2788
FU National Natural Science Foundation of China
FX No Statement Available
CR Bo DY, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P1400, DOI 10.1145/3366423.3380214
   Chang JL, 2020, IEEE T PATTERN ANAL, V42, P809, DOI 10.1109/TPAMI.2018.2889949
   de Mello DPM, 2022, AAAI CONF ARTIF INTE, P7770
   Dizaji KG, 2019, PROC CVPR IEEE, P4386, DOI 10.1109/CVPR.2019.00452
   Engstrom L., 2020, INT C LEARNING REPRE
   Fanti C, 2004, ADV NEUR IN, V16, P1603
   Gong L., 2022, PROC IJCAI, P3015
   Gowda SN, 2022, LECT NOTES COMPUT SC, V13680, P187, DOI 10.1007/978-3-031-20044-1_11
   Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753
   Hassani K, 2020, PR MACH LEARN RES, V119
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Huang J., 2020, P IEEE CVF C COMP VI, P8849, DOI DOI 10.1109/CVPR42600.2020.00887
   Ji P, 2017, ADV NEUR IN, V30
   Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965
   Jung YM, 2020, APPL MATH COMPUT, V382, DOI 10.1016/j.amc.2020.125328
   Kipf T.N., 2016, BAYESIAN DEEP LEARNI
   Li X., 2019, P INT C LEARN REPR N
   Li YF, 2021, AAAI CONF ARTIF INTE, V35, P8547
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Likas A, 1999, NEURAL COMPUT, V11, P1915, DOI 10.1162/089976699300016025
   Lin TE, 2020, AAAI CONF ARTIF INTE, V34, P8360
   Liu BW, 2021, INT J MACH LEARN CYB, V12, P1597, DOI 10.1007/s13042-020-01257-6
   Liu JL, 2010, AAAI CONF ARTIF INTE, P512
   Mahadevan S., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P328
   Mukherjee S, 2019, AAAI CONF ARTIF INTE, P4610
   Niu C, 2022, IEEE T IMAGE PROCESS, V31, P7264, DOI 10.1109/TIP.2022.3221290
   Pan SR, 2020, IEEE T CYBERNETICS, V50, P2475, DOI 10.1109/TCYB.2019.2932096
   Pang YS, 2020, INT CONF DAT MIN WOR, P464, DOI 10.1109/ICDMW51313.2020.00137
   Peng X, 2020, IEEE T NEUR NET LEAR, V31, P4857, DOI 10.1109/TNNLS.2019.2958324
   Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870
   Shaham U., 2018, PINT C LEARN REPR
   Sun MJ, 2022, IEEE T MULTIMEDIA, V24, P2567, DOI 10.1109/TMM.2021.3086727
   Tang C, 2022, IEEE T KNOWL DATA EN, V34, P4705, DOI 10.1109/TKDE.2020.3048678
   Tu WX, 2021, AAAI CONF ARTIF INTE, V35, P9978
   Van Gansbeke Wouter, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P268, DOI 10.1007/978-3-030-58607-2_16
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang C, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3670
   Wang JF, 2015, IEEE T KNOWL DATA EN, V27, P180, DOI 10.1109/TKDE.2014.2324592
   Xie JY, 2016, PR MACH LEARN RES, V48
   Yang B, 2017, PR MACH LEARN RES, V70
   Yang L, 2022, IEEE T NEUR NET LEAR, V33, P340, DOI 10.1109/TNNLS.2020.3027761
   Yang X, 2019, PROC CVPR IEEE, P4061, DOI 10.1109/CVPR.2019.00419
   Yu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3047
   Zhan XH, 2020, PROC CVPR IEEE, P6687, DOI 10.1109/CVPR42600.2020.00672
   Zhao WT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4404
   Zhu XF, 2019, IEEE T KNOWL DATA EN, V31, P2022, DOI 10.1109/TKDE.2018.2873378
NR 48
TC 0
Z9 0
U1 9
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8183
EP 8193
DI 10.1109/TMM.2022.3233249
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000073
DA 2024-07-18
ER

PT J
AU Li, Q
   Zhang, ZY
   Zhang, F
   Xiao, F
AF Li, Qun
   Zhang, Ziyi
   Zhang, Feng
   Xiao, Fu
TI HRNeXt: High-Resolution Context Network for Crowd Pose Estimation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Pose estimation; Convolution; Task analysis; Kernel; Feature extraction;
   Context modeling; Transformers; Crowd pose estimation; context modeling;
   feed-forward network; attention mechanism; high-resolution
   representation
AB Occlusion handling in crowded scenes is an intractable challenge for human pose estimation. To address this problem, we propose two novel feed-forward network structures named Global Feed-Forward Network (GFFN) and Dynamic Feed-Forward Network (DFFN), which are specifically designed for image-based tasks to capture both local and global contextual information within intermediate features and update feature representations with high adaptability for occlusions. By exploiting the context modeling ability of the proposed GFFN and DFFN, we present a novel backbone network, namely High-Resolution Context Network (HRNeXt), which learns high-resolution representations with abundant contextual information to better estimate poses of occluded human bodies. Compared to state-of-the-art pose estimation networks, our HRNeXt absorbs advantages of convolution operation and attention mechanism, and it is more efficient in terms of training data sizes, network parameters and computational costs. Experimental results show that our HRNeXt significantly outperforms state-of-the-art backbone networks on challenging pose estimation datasets with high occurrence of crowds and occlusions.
C1 [Li, Qun; Zhang, Ziyi; Zhang, Feng; Xiao, Fu] Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing 210023, Peoples R China.
C3 Nanjing University of Posts & Telecommunications
RP Xiao, F (corresponding author), Nanjing Univ Posts & Telecommun, Sch Comp Sci, Nanjing 210023, Peoples R China.
EM liqun@njupt.edu.cn; zhangziyi_njupt@hotmail.com;
   zhangfeng01@njupt.edu.cn; xiaof@njupt.edu.cn
RI Zhang, Ziyi/GZM-9540-2022; Li, Qun/JEP-3834-2023
OI Zhang, Ziyi/0000-0003-1728-2588; Li, Qun/0000-0002-8034-6030
FU National Science Fund for Distinguished Young Scholars of China
   [62125203]; Key Program of the National Natural Science Foundation of
   China [61932013]; Natural Science Foundation of China [62276143]
FX Manuscript received 15 June 2022; revised 1 November 2022 and 29 January
   2023; accepted 17 February 2023. Date of publication 23 February 2023;
   date of current version 8 May 2023. This work was supported in part by
   the National Science Fund for Distinguished Young Scholars of China
   under Grant 62125203, in part by the Key Program of the National Natural
   Science Foundation of China under Grant 61932013, and in part by the
   National Natural Science Foundation of China under Grant 62276143. The
   guest editor coordinating the review of this manuscript and approving it
   for publication was Prof. Jun Wu. (Corresponding author: Fu Xiao.)
CR Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143
   Chen YL, 2018, PROC CVPR IEEE, P7103, DOI 10.1109/CVPR.2018.00742
   Cheng BW, 2020, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR42600.2020.00543
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Guo JY, 2022, PROC CVPR IEEE, P12165, DOI 10.1109/CVPR52688.2022.01186
   Guo MH, 2022, Arxiv, DOI arXiv:2202.09741
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   Hendrycks D, 2020, Arxiv, DOI arXiv:1606.08415
   Huang JJ, 2020, PROC CVPR IEEE, P5699, DOI 10.1109/CVPR42600.2020.00574
   Khirodkar R., 2021, P IEEECVF INT C COMP, P3122
   Li JF, 2019, PROC CVPR IEEE, P10855, DOI 10.1109/CVPR.2019.01112
   Li Q., 2022, P INT JOINT C ARTIFI, P1095
   Li YJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11293, DOI 10.1109/ICCV48922.2021.01112
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lingteng Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12364), P488, DOI 10.1007/978-3-030-58529-7_29
   Liu Z, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9992, DOI 10.1109/ICCV48922.2021.00986
   MMPose-Contributors, 2020, OpenMMLab pose estimation toolbox and benchmark
   Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29
   Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395
   Park J., 2018, P BRIT MACH VIS C
   Redmon J., 2018, arXiv, DOI DOI 10.48550/ARXIV.1804.02767
   Sun K, 2019, PROC CVPR IEEE, P5686, DOI 10.1109/CVPR.2019.00584
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29
   Xie EZ, 2021, ADV NEUR IN, V34
   Yang B, 2019, ADV NEUR IN, V32
   Yang S, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P11782, DOI 10.1109/ICCV48922.2021.01159
   Yinpeng Chen, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11027, DOI 10.1109/CVPR42600.2020.01104
   Yu CQ, 2021, PROC CVPR IEEE, P10435, DOI 10.1109/CVPR46437.2021.01030
   Yuan K, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P559, DOI 10.1109/ICCV48922.2021.00062
   Yuan Y., 2021, Adv. Neural Inform. Process. Syst, V34, P7281
   Zhang F, 2020, PROC CVPR IEEE, P7091, DOI 10.1109/CVPR42600.2020.00712
   Zhang SH, 2019, PROC CVPR IEEE, P889, DOI 10.1109/CVPR.2019.00098
NR 40
TC 3
Z9 3
U1 5
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1521
EP 1528
DI 10.1109/TMM.2023.3248144
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA G2GW2
UT WOS:000987415000011
DA 2024-07-18
ER

PT J
AU Lin, HB
   Zhang, CY
   Wang, SP
   Guo, WZ
AF Lin, Huibin
   Zhang, Chun-Yang
   Wang, Shiping
   Guo, Wenzhong
TI A Probabilistic Contrastive Framework for Semi-Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Predictive models; Semantics;
   Probabilistic logic; Image classification; Annotations; Semi-supervised
   learning; contrastive learning; image classification; consistency
   regularization; data augmen- tation; Pseudo-labeling
AB Semi-supervised learning is a common way that investigates how to improve performance of a visual learning model, while data annotation is far from sufficient. Recent works in semi-supervised deep learning have successfully applied consistency regularization, which encourages a model to maintain consistent predictions for different perturbed versions of an image. However, most of such methods ignore the category correlation of image features, especially when exploiting strong augmentation methods for unlabeled images. To address this problem, we propose PConMatch, a model that leverages a probabilistic contrastive learning framework to separate the features of strongly-augmented versions from different classes. A semi-supervised probabilistic contrastive loss is designed, which takes both labeled and unlabeled samples into account and develops an auxiliary module to generate a probability score to measure the model prediction confidence for each sample. Specifically, PConMatch first generates a pair of weakly-augmented versions for each labeled sample, and produces a weakly-augmented version and a corresponding pair of strongly-augmented versions for each unlabeled sample. Second, a probability score module is proposed to assign pseudo-labeling confidence scores to strongly-augmented unlabeled images. Finally, the probability score of each sample is further passed to the contrastive loss, combining with consistency regularization to enable the model to learn better feature representations. Extensive experiments on four publicly available image classification benchmarks demonstrate that the proposed approach achieves state-of-the-art performance in image classification. Several rigorous ablation studies are conducted to validate the effectiveness of the method.
C1 [Lin, Huibin; Zhang, Chun-Yang; Wang, Shiping; Guo, Wenzhong] Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350025, Peoples R China.
C3 Fuzhou University
RP Guo, WZ (corresponding author), Fuzhou Univ, Coll Comp & Data Sci, Fuzhou 350025, Peoples R China.
EM huibinlin@outlook.com; zhangcy@fzu.edu.cn; shipingwangphd@163.com;
   guowenzhong@fzu.edu.cn
OI Lin, Huibin/0000-0002-7774-7722; Zhang, Chun-Yang/0000-0001-6151-7028
FU National Natural Science Foundation of China
FX No Statement Available
CR Arazo E, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207304
   Bachman P, 2014, ADV NEUR IN, V27
   Bachman P, 2019, ADV NEUR IN, V32
   Berthelot D., 2020, Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring
   Berthelot D, 2019, ADV NEUR IN, V32
   Caron M., 2020, Advances in neural information processing systems, V33, P9912
   Chen K., 2013, EFFICIENT ESTIMATION, P2
   Chen Ting, 2020, ADV NEURAL INFORM PR, V33, P22243
   Chen Ting, 2019, 25 AMERICAS C INFORM
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Cheng SY, 2021, PROC CVPR IEEE, P4419, DOI 10.1109/CVPR46437.2021.00440
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   DeVries T, 2017, Arxiv, DOI [arXiv:1708.04552, DOI 10.48550/ARXIV.1708.04552]
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Gutmann Michael, 2010, P MACHINE LEARNING R, P297, DOI DOI 10.1145/3292500.3330651
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2020, C COMP VIS PATT REC, P2, DOI [DOI 10.1109/CVPR42600.2020.00975, 10.1109/CVPR42600.2020.00975]
   Henaff OJ, 2020, PR MACH LEARN RES, V119
   Pham H, 2021, PROC CVPR IEEE, P11552, DOI 10.1109/CVPR46437.2021.01139
   Hu ZJ, 2021, PROC CVPR IEEE, P15094, DOI 10.1109/CVPR46437.2021.01485
   Khosla P., 2020, C NEUR INF PROC SYST
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3065386
   Li C.-L., 2020, Advances in neural information processing systems, V33, P596
   Li ZL, 2022, IEEE T MULTIMEDIA, V24, P2461, DOI 10.1109/TMM.2021.3081930
   Loshchilov I, 2017, P 5 INT C LEARN REPR
   Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927
   Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821
   Nassar I, 2021, PROC CVPR IEEE, P7237, DOI 10.1109/CVPR46437.2021.00716
   Netzer Y., 2011, READING DIGITS NATUR
   Oliver A, 2018, ADV NEUR IN, V31
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Radford A, 2021, PR MACH LEARN RES, V139
   Rasmus A, 2015, ADV NEUR IN, V28
   Ravi S., 2017, C TRACK P, P1
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Speer R, 2017, AAAI CONF ARTIF INTE, P4444
   Tarvainen A, 2017, ADV NEUR IN, V30
   Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635
   Wu ZR, 2018, PROC CVPR IEEE, P3733, DOI 10.1109/CVPR.2018.00393
   Xiaojin Z, 2002, CMUCALD02107
   Xie Q., 2020, ADV NEURAL INFORM PR, V33, P6256, DOI DOI 10.48550/ARXIV.1904.12848
   Xu HH, 2023, IEEE T PATTERN ANAL, V45, P3753, DOI 10.1109/TPAMI.2022.3176690
   Yonglong Tian, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P776, DOI 10.1007/978-3-030-58621-8_45
   Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612
   Zagoruyko S, 2015, PROC CVPR IEEE, P4353, DOI 10.1109/CVPR.2015.7299064
   Zhang B., 2021, Proc. Adv. Neural Inf. Process. Syst., P18408
   Zhang Hongyi, 2018, MIXUP EMPIRICAL RISK, DOI DOI 10.48550/ARXIV.1710.09412
   Zheng MK, 2022, PROC CVPR IEEE, P14451, DOI 10.1109/CVPR52688.2022.01407
NR 49
TC 2
Z9 2
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8767
EP 8779
DI 10.1109/TMM.2023.3241539
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000060
DA 2024-07-18
ER

PT J
AU Luo, Y
   Wong, YK
   Kankanhalli, MS
   Zhao, Q
AF Luo, Yan
   Wong, Yongkang
   Kankanhalli, Mohan S.
   Zhao, Qi
TI Learning to Minimize the Remainder in Supervised Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deep learning; gradient adjustment; remainder; supervised learning
ID OPTIMIZATION; GRADIENT
AB Thelearning process of deep learning methods usually updates the model's parameters in multiple iterations. Each iteration can be viewed as the first-order approximation of Taylor's series expansion. The remainder, which consists of higher-order terms, is usually ignored in the learning process for simplicity. This learning scheme empowers various multimedia-based applications, such as image retrieval, recommendation system, and video search. Generally, multimedia data (e.g. images) are semantics-rich and high-dimensional, hence the remainders of approximations are possibly non-zero. In this work, we consider that the remainder is informative and study how it affects the learning process. To this end, we propose a new learning approach, namely gradient adjustment learning (GAL), to leverage the knowledge learned from the past training iterations to adjust vanilla gradients, such that the remainders are minimized and the approximations are improved. The proposed GAL is model- and optimizer-agnostic, and is easy to adapt to the standard learning framework. It is evaluated on three tasks, i.e. image classification, object detection, and regression, with state-of-the-art models and optimizers. The experiments show that the proposed GAL consistently enhances the evaluated models, whereas the ablation studies validate various aspects of the proposed GAL. The code is available at https://github.com/luoyan407/gradient_adjustment.git.
C1 [Luo, Yan; Zhao, Qi] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
   [Wong, Yongkang; Kankanhalli, Mohan S.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   National University of Singapore
RP Zhao, Q (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM luoxx648@umn.edu; yongkang.wong@nus.edu.sg; mohan@comp.nus.edu.sg;
   qzhao@cs.umn.edu
RI Kankanhalli, Mohan/Q-9284-2019; zhao, qi/KGK-3760-2024
OI Kankanhalli, Mohan/0000-0002-4846-2015; Wong,
   Yongkang/0000-0002-1239-4428; Luo, Yan/0000-0001-5135-0316
FU NSF [1908711, 1849107]; National Research Foundation, Singapore under
   its Strategic Capability Research Centres Funding Initiative
FX This work was supported in part by the NSF under Grants 1908711 and
   1849107, and in part by the National Research Foundation, Singapore
   under its Strategic Capability Research Centres Funding Initiative.
CR Achieser N. I., 1956, THEORY APPROXIMATION
   Andrychowicz M, 2016, ADV NEUR IN, V29
   [Anonymous], 2013, Introductory lectures on convex optimization: A basic course
   Asuncion A., 2007, Uci machine learning repository
   Berz M., 1998, Reliable Computing, V4, P83, DOI 10.1023/A:1009958918582
   Black P.E., 2005, Dictionary of algorithms and data structures
   Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173
   Boyd S., 2004, CONVEX OPTIMIZATION
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Carion N., 2020, EUR C COMP VIS, P213, DOI DOI 10.1007/978-3-030-58452-8_13
   Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296
   Chen YT, 2017, PR MACH LEARN RES, V70
   Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Cong Y, 2012, IEEE T MULTIMEDIA, V14, P66, DOI 10.1109/TMM.2011.2166951
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Foret P., 2021, PROC INT C LEARN REP
   Goodfellow I.J., 2015, PROC 3 INT C LEARN R
   HARRISON D, 1978, J ENVIRON ECON MANAG, V5, P81, DOI 10.1016/0095-0696(78)90006-2
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, Cited on, V14, P2
   Ji JL, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2614
   Jin C, 2017, PR MACH LEARN RES, V70
   Kingma D. P, 2015, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li J., 2020, P IEEECVF WINTER C A, P3019
   Li JN, 2020, INT J COMPUT VISION, V128, P1750, DOI 10.1007/s11263-020-01295-1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu L., 2020, PROC INT C LEARN REP
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Loshchilov I., 2019, DECOUPLED WEIGHT DEC
   Luo L., 2019, PROC INT C LEARN REP
   Luo Y, 2021, IEEE T PATTERN ANAL, V43, P1928, DOI 10.1109/TPAMI.2019.2963387
   MILNE WE, 1949, J RES NAT BUR STAND, V43, P501, DOI 10.6028/jres.043.042
   Mohri M., 2012, Foundations of Machine Learning
   Pace RK, 1997, STAT PROBABIL LETT, V33, P291, DOI 10.1016/s0167-7152(96)00140-x
   Pedregosa F, 2016, PR MACH LEARN RES, V48
   Reddi S., 2018, AISTATS, V84, P1233
   Ren S., 2015, ADV NEURAL INFORM PR, V28, P91
   ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Stancu D. D., 1963, Mathematics of Computation, V17, P270
   Stancu D. D., 1964, Journal of the Society for Industrial and Applied Mathematics, Series B: Numerical Analysis, V1, P137
   Tan MX, 2021, PR MACH LEARN RES, V139, P7102
   Tan MX, 2019, PR MACH LEARN RES, V97
   Timan A. F., 1963, Theory of approximation of functions of a real variable, V34
   TOLSTIKHIN I., 2021, P 35 C NEUR INF PROC
   Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640
   Xia SY, 2012, IEEE T MULTIMEDIA, V14, P1046, DOI 10.1109/TMM.2012.2187436
   Xu BJ, 2020, IEEE T MULTIMEDIA, V22, P1423, DOI 10.1109/TMM.2019.2943753
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yan Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P502, DOI 10.1007/978-3-030-58598-3_30
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang Michael, 2019, Advances in Neural Information Processing Systems, P9593
   Zhu ZX, 2019, PR MACH LEARN RES, V97
   Zhuang JT, 2020, ADV NEUR IN, V33
NR 62
TC 0
Z9 0
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 1738
EP 1748
DI 10.1109/TMM.2022.3158066
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Ma, R
   Wu, QB
   Ngan, KN
   Li, HL
   Meng, FM
   Xu, LF
AF Ma, Rui
   Wu, Qingbo
   Ngan, King Ngi
   Li, Hongliang
   Meng, Fanman
   Xu, Linfeng
TI Forgetting to Remember: A Scalable Incremental Learning Framework for
   Cross-Task Blind Image Quality Assessment
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cross-Task BIQA; relevance-aware incremental learning; task relevance;
   parameter reuse; additional task learning
AB Recent years have witnessed the great success of blind image quality assessment (BIQA) in various task-specific scenarios, which present invariable distortion types and evaluation criteria. However, due to the rigid structure and learning framework, they cannot apply to the cross-task BIQA scenario, where the distortion types and evaluation criteria keep changing in practical applications. This paper proposes a scalable incremental learning framework (SILF) that could sequentially conduct BIQA across multiple evaluation tasks with limited memory capacity. More specifically, we develop a dynamic parameter isolation strategy to sequentially update the task-specific parameter subsets, which are non-overlapped with each other. Each parameter subset is temporarily settled to Remember one evaluation preference toward its corresponding task, and the previously settled parameter subsets can be adaptively reused in the following BIQA to achieve better performance based on the task relevance. To suppress the unrestrained expansion of memory capacity in sequential tasks learning, we develop a scalable memory unit by gradually and selectively pruning unimportant neurons from previously settled parameter subsets, which enable us to Forget part of previous experiences and free the limited memory capacity for adapting to the emerging new tasks. Extensive experiments on eleven IQA datasets demonstrate that our proposed method significantly outperforms the other state-of-the-art methods in cross-task BIQA.
C1 [Ma, Rui; Wu, Qingbo; Ngan, King Ngi; Li, Hongliang; Meng, Fanman; Xu, Linfeng] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Wu, QB; Meng, FM (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
EM ruima@std.uestc.edu.cn; qbwu@uestc.edu.cn; knngan@uestc.edu.cn;
   hlli@uestc.edu.cn; fmmeng@uestc.edu.cn; lfxu@uestc.edu.cn
RI Lu, Wang/JVO-0416-2024; Wu, Qingbo/M-5065-2015; Xu,
   Linfeng/HME-1913-2023
OI Wu, Qingbo/0000-0003-2936-6340; Xu, Linfeng/0000-0002-9934-0958
FU National Natural Science Foundation of China
FX No Statement Available
CR Rusu AA, 2016, Arxiv, DOI arXiv:1606.04671
   Banarse D., 2017, arXiv
   Bare B, 2017, IEEE INT CON MULTI, P1356, DOI 10.1109/ICME.2017.8019508
   Belouadah E, 2019, IEEE I CONF COMP VIS, P583, DOI 10.1109/ICCV.2019.00067
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ebrahimi S., 2020, P INT C MACH LEARN R, P1
   Fang YM, 2020, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR42600.2020.00373
   Finn C, 2017, PR MACH LEARN RES, V70
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Goodfellow I, 2014, ADV NEURAL INFORM PR, P2672
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P432, DOI 10.1109/TMM.2016.2518868
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Guan JW, 2017, IEEE T MULTIMEDIA, V19, P2505, DOI 10.1109/TMM.2017.2703148
   Hancheng Zhu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P14131, DOI 10.1109/CVPR42600.2020.01415
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hosu V, 2020, IEEE T IMAGE PROCESS, V29, P4041, DOI 10.1109/TIP.2020.2967829
   Huang YM, 2021, DISPLAYS, V70, DOI 10.1016/j.displa.2021.102115
   Hung C.-Y., 2019, P ADV NEUR INF PROC, P13677
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Jiang QP, 2022, IEEE T CIRC SYST VID, V32, P5959, DOI 10.1109/TCSVT.2022.3164918
   Jiang QP, 2022, IEEE T IMAGE PROCESS, V31, P2279, DOI 10.1109/TIP.2022.3154588
   Jiang QP, 2019, IEEE T CIRC SYST VID, V29, P323, DOI 10.1109/TCSVT.2017.2783938
   Jiang QP, 2018, IEEE T MULTIMEDIA, V20, P2035, DOI 10.1109/TMM.2017.2763321
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Kim J, 2017, IEEE J-STSP, V11, P206, DOI 10.1109/JSTSP.2016.2639328
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li QH, 2016, IEEE T MULTIMEDIA, V18, P2457, DOI 10.1109/TMM.2016.2601028
   Li XL, 2019, PR MACH LEARN RES, V97
   Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081
   Lin HH, 2019, INT WORK QUAL MULTIM
   Liu JZ, 2023, IEEE T MULTIMEDIA, V25, P5358, DOI 10.1109/TMM.2022.3190700
   Liu YT, 2020, ACM T MULTIM COMPUT, V16, DOI 10.1145/3414837
   Lopez-Paz D, 2017, ADV NEUR IN, V30
   Ma R, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P5248, DOI 10.1145/3474085.3475642
   Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5
   Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810
   McCloskey M., 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI [10.1016/S0079-7421(08)60536-8, DOI 10.1016/S0079-7421(08)60536-8]
   Min XK, 2019, IEEE T MULTIMEDIA, V21, P2319, DOI 10.1109/TMM.2019.2902097
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Ponomarenko Nikolay, 2013, 2013 4th European Workshop on Visual Information Processing (EUVIP), P106
   Rajasegaran J., 2019, Advances in Neural Information Processing Systems, P12669
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Schwarz J, 2018, PR MACH LEARN RES, V80
   Serra Joan, 2018, International Conference on Machine Learning, P4548
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shin H, 2017, ADV NEUR IN, V30
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Singh PK, 2020, APPL INTELL, V50, P4708, DOI 10.1007/s10489-020-01775-4
   Su SL, 2020, PROC CVPR IEEE, P3664, DOI 10.1109/CVPR42600.2020.00372
   Wu C., 2018, P ANN C NEUR INF PRO, P5966
   Wu QB, 2019, IEEE IMAGE PROC, P2364, DOI [10.1109/icip.2019.8803329, 10.1109/ICIP.2019.8803329]
   Wu QB, 2018, IEEE T IMAGE PROCESS, V27, P2499, DOI 10.1109/TIP.2018.2799331
   Wu QB, 2017, IEEE T MULTIMEDIA, V19, P2490, DOI 10.1109/TMM.2017.2700206
   Wu QB, 2015, IEEE IMAGE PROC, P339, DOI 10.1109/ICIP.2015.7350816
   Xiang Y, 2019, IEEE I CONF COMP VIS, P6618, DOI 10.1109/ICCV.2019.00672
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu JW, 2022, DISPLAYS, V74, DOI 10.1016/j.displa.2022.102188
   Yang D, 2019, IEEE INT CONF COMP V, P3913, DOI 10.1109/ICCVW.2019.00485
   Yang S, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1383, DOI 10.1145/3343031.3350990
   Yaoyao Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12242, DOI 10.1109/CVPR42600.2020.01226
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Yoon J., 2018, P INT C MACH LEARN R, P1
   Zhai GT, 2021, ACM T MULTIM COMPUT, V17, DOI 10.1145/3457905
   Zhai GT, 2021, IEEE T MULTIMEDIA, V23, P3700, DOI 10.1109/TMM.2020.3029891
   Zhan YB, 2017, IEEE T MULTIMEDIA, V19, P1837, DOI 10.1109/TMM.2017.2689923
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
   Zhang WX, 2023, IEEE T PATTERN ANAL, V45, P2864, DOI 10.1109/TPAMI.2022.3178874
   Zhang WX, 2023, Arxiv, DOI arXiv:2107.13429
   Zhang WX, 2020, IEEE IMAGE PROC, P111, DOI [10.1109/icip40778.2020.9191278, 10.1109/ICIP40778.2020.9191278]
   Zhang WX, 2020, IEEE T CIRC SYST VID, V30, P36, DOI 10.1109/TCSVT.2018.2886771
NR 77
TC 5
Z9 5
U1 1
U2 1
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8817
EP 8827
DI 10.1109/TMM.2023.3242143
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000043
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Nikoonezhad, F
   Ghanbari, M
AF Nikoonezhad, Fatemeh
   Ghanbari, Mohammed
TI PRAM: Penalized Resource Allocation Method for Video Services
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Streaming media; Resource management; Video recording; Quality
   assessment; Quality of service; Degradation; Bandwidth; Multiplexing;
   resource allocation; routing; video quality measurement
ID NETWORKS; STREAMS
AB The human visual system response to picture quality degradation due to packet loss is very different from the responses of objective quality measures. While video quality due to packet loss may be impaired by at most for one Group of Pictures (GOP), its subjective quality degradation may last for several GOPs. This has a great impact on resource allocation strategies, which normally make decisions on instantaneous conditions of multiplexing buffer. This is because, when the perceptual impact of degraded video quality is much longer than its objective degradation period, any assigned resources to the degraded flow is wasted. This paper, through both simulations and analysis shows that, during resource allocation, if the quality of a video stream is significantly degraded, it is better to penalize this degraded flow from getting its full bandwidth share and instead assign the remaining share to other flows preventing them from undergoing quality degradation.
C1 [Nikoonezhad, Fatemeh; Ghanbari, Mohammed] Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran 1439957131, Iran.
   [Ghanbari, Mohammed] Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
C3 University of Tehran; University of Essex
RP Ghanbari, M (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran 1439957131, Iran.; Ghanbari, M (corresponding author), Univ Essex, Sch Comp Sci & Elect Engn, Colchester CO4 3SQ, Essex, England.
EM fatemeh.nikoonezhad@gmail.com; ghan@essex.ac.uk
OI Ghanbari, Mohammad/0000-0002-5482-8378
CR Akbari B, 2008, COMPUT COMMUN, V31, P551, DOI 10.1016/j.comcom.2007.08.025
   ALDRIDGE R, 1995, IEE P-VIS IMAGE SIGN, V142, P149, DOI 10.1049/ip-vis:19951937
   ALDRIDGE R, 1995, P 5 IEE INT C IM PRO, P336
   ALPERT T, 1996, P INT WORKSHOP HDTV, P417
   Amirpour H, 2020, IEEE DATA COMPR CONF, P358, DOI 10.1109/DCC47342.2020.00080
   Assunçao PAA, 2000, IEEE T CIRC SYST VID, V10, P83, DOI 10.1109/76.825863
   Barakabitze AA, 2020, IEEE COMMUN SURV TUT, V22, P526, DOI 10.1109/COMST.2019.2958784
   Bideh MK, 2016, PEER PEER NETW APPL, V9, P436, DOI 10.1007/s12083-015-0355-x
   BOUDOUA S, 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596785.
   Braden R., 1994, INTEGRATED SERVICES
   Chakareski J, 2006, IEEE T MULTIMEDIA, V8, P207, DOI 10.1109/TMM.2005.864284
   DERIDDER H, 1997, J SMPTE, V160, P123
   FANG Y, 2010, IJCSIS INT J COMPUT, V8
   GHANBARI M, 1995, IEEE T CIRC SYST VID, V5, P171, DOI 10.1109/76.388066
   GHANBARI M, 2011, VIDEO QUALITY MEASUR
   Goudarzi P, 2011, COMPUT ELECTR ENG, V37, P75, DOI 10.1016/j.compeleceng.2010.09.003
   HUSZK A, 2010, P FINAL PROG ABSTR B, P3
   HUYNHTHU Q, 2006, P 2ND INT WORKSHOP V
   *IPHOMW HHI, JM 190 H 264 AVC REF
   KAHNEMAN D, 1979, ECONOMETRICA, V47, P263, DOI 10.2307/1914185
   LI Z, 2016, TOWARD A PRACTICAL P, V6, P2
   Martin A, 2018, IEEE T BROADCAST, V64, P561, DOI 10.1109/TBC.2018.2828608
   MARUTA K, 2017, SENSORS, V17, P1
   MOHARRAMI A, 2021, AN IN ROUTER IDENTIF
   *NS2PROJ, NETW NSS SIM
   PETRANGELI S, 2018, ACM T MULTIM COMPUT, V14, P2
   Prassanna J, 2019, MOBILE NETW APPL, V24, P1214, DOI 10.1007/s11036-019-01259-x
   Przylucki S, 2017, WIRELESS PERS COMMUN, V96, P5391, DOI 10.1007/s11277-016-3747-1
   SEFERIDIS V, 1992, ELECTRON LETT, V28, P2013, DOI 10.1049/el:19921290
   Shi HZ, 2014, IEEE COMMUN SURV TUT, V16, P5, DOI 10.1109/SURV.2013.050113.00015
   Shreedhar M, 1996, IEEE ACM T NETWORK, V4, P375, DOI 10.1109/90.502236
   Skorin-Kapov L, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3176648
   Tan KT, 1998, SIGNAL PROCESS, V70, P279, DOI 10.1016/S0165-1684(98)00129-7
   TAN KT, 2000, OBJECTIVE PICTURE QU
   Tianfu Zhang, 2017, 2017 IEEE International Ultrasonics Symposium (IUS), DOI 10.1109/ULTSYM.2017.8092279
   Tiwari M, 2011, IEEE T IMAGE PROCESS, V20, P3219, DOI 10.1109/TIP.2011.2146262
   Yang M, 2014, IEEE T MULTIMEDIA, V16, P1849, DOI 10.1109/TMM.2014.2343943
   Yousaf FZ, 2017, IEEE J SEL AREA COMM, V35, P2468, DOI 10.1109/JSAC.2017.2760418
   Zhang F, 2006, IEEE T MULTIMEDIA, V8, P1005, DOI 10.1109/TMM.2006.879865
   ZHU X, 2005, P IEEE INT C IMAGE P
   Zhu XQ, 2004, IEEE IMAGE PROC, P2547
   2012, METHODOLOGY FOR THE
   2016, CISCO VISUAL NETWORK, P2016
NR 43
TC 0
Z9 0
U1 0
U2 0
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3525
EP 3533
DI 10.1109/TMM.2022.3162102
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500003
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Pang, Y
   Wu, CD
   Wu, H
   Yu, XS
AF Pang, Yu
   Wu, Chengdong
   Wu, Hao
   Yu, Xiaosheng
TI Unsupervised Multi-Subclass Saliency Classification for Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Object detection; Task analysis; Predictive models;
   Automobiles; Saliency detection; Manuals; Label distribution learning;
   multi-subclass classification; refinement technology; salient object
   detection; spatial smoothness
ID FEATURES; RANKING
AB Numerous bottom-up salient object detection algorithms formulate the problem as a classification task. For an input image, these methods usually utilize prior cues to select some regions as training set, and learn a classifier to classify all regions into foreground/background. However, such binary classification based approaches suffer from accuracy problems in some complex scenes. To this end, we propose a novel framework, namely Multi-Subclass Classification with Label Distribution Learning (MSCLDL). Specifically, prior knowledge is firstly employed to build a training set from input image, in which each sample is associated with one of two class labels. Previous works usually learn directly a binary classification model from training set. Different with them, we further decompose two classes into a certain number of subclasses, each sample is thus described by one of multiple subclass labels. Based on the multi-subclass training set, we learn a label distribution model to predict the subclass label of each image region. Furthermore, the saliency value of each image region could be computed via exploring the relationship class and subclass labels. The MSCLDL could overcome the limitation of existing classification-based algorithms in some challenging scenes. Finally, a novel refinement technology is presented to further refine the saliency map obtained by MSCLDL. We compare the proposed method and other state-of-the-art methods on four benchmark datasets, the superiority of our model is adequately demonstrated via the experimental results analysis.
C1 [Pang, Yu; Wu, Chengdong; Yu, Xiaosheng] Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Peoples R China.
   [Wu, Hao] Univ Sydney, Australian Ctr Field Robot, Sydney, Australia.
C3 Northeastern University - China; University of Sydney
RP Pang, Y; Wu, CD (corresponding author), Northeastern Univ, Fac Robot Sci & Engn, Shenyang 110169, Peoples R China.
EM pangyu@stumail.neu.edu.cn; wuchengdongneu@163.com; neupangy@163.com;
   yuxiaoshengneu@163.com
FU National Natural Science Foundation of China [61973063, 61701101,
   U1713216, 61901098, 61971118]; Fundamental Research Fund for the Central
   Universities of China [N2026005, N181602014, N2026004, N2026006,
   N2026001, N2011001]; Project for the Science and Technology Major
   Special Plan of Liaoning [2019JH1/10100005]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61973063, 61701101, U1713216, 61901098,
   and 61971118, in part by the Fundamental Research Fund for the Central
   Universities of China under Grants N2026005, N181602014, N2026004,
   N2026006, N2026001, and N2011001, and in part by the Project for the
   Science and Technology Major Special Plan of Liaoning under Grant
   2019JH1/10100005.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   Cai CL, 2019, IEEE T CIRC SYST VID, V29, P3687, DOI 10.1109/TCSVT.2018.2880492
   Chen BH, 2019, IEEE T CIRC SYST VID, V29, P982, DOI 10.1109/TCSVT.2018.2828606
   Chen CLZ, 2020, Arxiv, DOI arXiv:2008.02966
   Chen CLZ, 2018, IEEE T MULTIMEDIA, V20, P3324, DOI 10.1109/TMM.2018.2839523
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021
   Elder, 2010, P IEEE C COMP VIS PA, P49, DOI [10.1109/CVPRW.2010.5543739, DOI 10.1109/CVPRW.2010.5543739]
   Fang S, 2017, IEEE T NEUR NET LEAR, V28, P1095, DOI 10.1109/TNNLS.2016.2522440
   Filali I, 2016, SIGNAL PROCESS-IMAGE, V47, P380, DOI 10.1016/j.image.2016.07.007
   Gao BB, 2017, IEEE T IMAGE PROCESS, V26, P2825, DOI 10.1109/TIP.2017.2689998
   Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658
   Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51
   Gong C, 2015, PROC CVPR IEEE, P2531, DOI 10.1109/CVPR.2015.7298868
   He SF, 2015, INT J COMPUT VISION, V115, P330, DOI 10.1007/s11263-015-0822-0
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Huo SW, 2017, IEEE IJCNN, P3130, DOI 10.1109/IJCNN.2017.7966246
   Jiang P, 2020, IEEE T IMAGE PROCESS, V29, P2903, DOI 10.1109/TIP.2019.2954209
   Kruthiventi SSS, 2016, PROC CVPR IEEE, P5781, DOI 10.1109/CVPR.2016.623
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin X, 2019, IEEE T MULTIMEDIA, V21, P1646, DOI 10.1109/TMM.2018.2884474
   Ling MG, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2922818
   Liu GH, 2019, IEEE T IMAGE PROCESS, V28, P6, DOI 10.1109/TIP.2018.2847422
   Lu HC, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2524198
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Pang Y., 2020, IEEE ACCESS, V8
   Pang Y, 2020, SIGNAL PROCESS-IMAGE, V87, DOI 10.1016/j.image.2020.115928
   Qian MY, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.021
   Qin Y, 2018, INT J COMPUT VISION, V126, P751, DOI 10.1007/s11263-017-1062-2
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tong N, 2015, PROC CVPR IEEE, P1884, DOI 10.1109/CVPR.2015.7298798
   Tong N, 2015, PATTERN RECOGN, V48, P3258, DOI 10.1016/j.patcog.2014.12.005
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Wang J, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3712
   Wang LJ, 2017, PROC CVPR IEEE, P3796, DOI 10.1109/CVPR.2017.404
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang TT, 2016, LECT NOTES COMPUT SC, V9912, P450, DOI 10.1007/978-3-319-46484-8_27
   Xu CD, 2019, AAAI CONF ARTIF INTE, P5533
   Xu N, 2021, IEEE T KNOWL DATA EN, V33, P1632, DOI 10.1109/TKDE.2019.2947040
   Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Zeng Y, 2018, IEEE T IMAGE PROCESS, V27, P4545, DOI 10.1109/TIP.2018.2838761
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang LH, 2020, IEEE T IMAGE PROCESS, V29, P2258, DOI 10.1109/TIP.2019.2945679
   Zhang LH, 2018, IEEE T IMAGE PROCESS, V27, P987, DOI 10.1109/TIP.2017.2766787
   Zhang LH, 2017, IEEE T PATTERN ANAL, V39, P1892, DOI 10.1109/TPAMI.2016.2609426
   Zhang M, 2019, ADV NEUR IN, V32
   Zhang M, 2018, J VIS COMMUN IMAGE R, V53, P215, DOI 10.1016/j.jvcir.2018.03.019
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao P, 2018, AAAI CONF ARTIF INTE, P4506
   Zhao T, 2019, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2019.00320
   Zhao X., 2020, P EUR C COMP VIS, P35, DOI 10.1007/ 978-3-030-58536-5_3
   Zhou Y, 2019, IEEE T MULTIMEDIA, V21, P74, DOI 10.1109/TMM.2018.2845667
NR 57
TC 3
Z9 3
U1 1
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2189
EP 2202
DI 10.1109/TMM.2022.3144070
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA J1QV6
UT WOS:001007432100046
DA 2024-07-18
ER

PT J
AU Pang, ZQ
   Zhao, LL
   Liu, QY
   Wang, CY
AF Pang, Zhiqi
   Zhao, Lingling
   Liu, Qiuyang
   Wang, Chunyu
TI Camera Invariant Feature Learning for Unsupervised Person
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cameras; Data models; Clustering algorithms; Representation learning;
   Feature extraction; Stochastic processes; Optimization methods;
   Clustering algorithm; person re-identification; representation learning;
   unsupervised learning
AB Fully unsupervised person re-identification (ReID) methods aim to learn discriminative features without using labeled ReID data. Because these methods are easily affected by camera discrepancies, similar studies have typically designed optimization methods to enable the model to learn camera-invariant features. However, they often ignore the impact of camera discrepancies on clustering results. Specifically, camera discrepancies will reduce the intra-class camera diversity and promote the generation of noise labels. To solve the above problems, we propose a unified unsupervised learning framework: camera invariant feature learning (CIFL) framework. First, we designed a novel DBSCAN-NN algorithm in the CIFL framework that improves the intra-class camera diversity by forcibly merging samples from different cameras. Then, we designed feature ensemble clustering that improves the accuracy of the pseudo-labels by clustering feature ensembles. In addition, we designed an optimization method for camera discrepancies: stochastic pulled loss. With the stochastic pulled loss, the ReID model is forced to learn camera-invariant features. We verified the effectiveness and generalization of CIFL on four ReID datasets (Market-1501, DukeMTMC-reID, MSMT17 and CUHK03-NP). The experimental results show that CIFL not only outperforms the existing fully unsupervised methods but also is superior to the unsupervised domain adaptation methods.
C1 [Pang, Zhiqi; Zhao, Lingling; Liu, Qiuyang; Wang, Chunyu] Harbin Inst Technol, Fac Comp, Harbin 150001, Heilongjiang, Peoples R China.
C3 Harbin Institute of Technology
RP Wang, CY (corresponding author), Harbin Inst Technol, Fac Comp, Harbin 150001, Heilongjiang, Peoples R China.
EM 1370890813@qq.com; zhaoll@hit.edu.cn; 443667536@qq.com;
   chunyu@hit.edu.cn
RI Wang, Chunyu/AAN-5766-2020; ZHAO, lingling/AAM-3755-2020
OI Wang, Chunyu/0000-0002-2965-9920; ZHAO, lingling/0000-0003-0315-4569;
   Pang, Zhiqi/0000-0003-0940-3351
FU National Natural Science Foundation of China [61872114, 62171164];
   Testworks, Inc., South Korea
FX This wok was supported in part by the National Natural Science
   Foundation of China under Grants 61872114 and 62171164 and in part by
   Testworks, Inc., South Korea.
CR Cao M, 2021, IEEE T MULTIMEDIA, V23, P1239, DOI 10.1109/TMM.2020.2994524
   Chen H, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P14940, DOI 10.1109/ICCV48922.2021.01469
   Chen H, 2021, PROC CVPR IEEE, P2004, DOI 10.1109/CVPR46437.2021.00204
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Chuanchen Luo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P224, DOI 10.1007/978-3-030-58555-6_14
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Dongkai Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10978, DOI 10.1109/CVPR42600.2020.01099
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Ge Y., 2020, P NIPS, V33, P11309
   Ge YX, 2020, Arxiv, DOI [arXiv:2001.01526, 10.48550/arXiv.2001.01526]
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guangyi Chen, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P643, DOI 10.1007/978-3-030-58598-3_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Huang Y, 2019, IEEE I CONF COMP VIS, P9526, DOI 10.1109/ICCV.2019.00962
   Jianing Li, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P483, DOI 10.1007/978-3-030-58586-0_29
   Jin X, 2020, PROC CVPR IEEE, P3140, DOI 10.1109/CVPR42600.2020.00321
   Kaiwei Zeng, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13654, DOI 10.1109/CVPR42600.2020.01367
   Kingma D. P., 2014, arXiv
   Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27
   Li YJ, 2019, IEEE I CONF COMP VIS, P7918, DOI 10.1109/ICCV.2019.00801
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Lin YT, 2020, IEEE T IMAGE PROCESS, V29, P5481, DOI 10.1109/TIP.2020.2982826
   Lin YT, 2019, AAAI CONF ARTIF INTE, P8738
   Liu JW, 2019, PROC CVPR IEEE, P7195, DOI 10.1109/CVPR.2019.00737
   Pang ZQ, 2022, IEEE T CIRC SYST VID, V32, P3164, DOI 10.1109/TCSVT.2021.3103753
   Pang ZQ, 2021, IEEE SIGNAL PROC LET, V28, P2142, DOI 10.1109/LSP.2021.3119208
   Pang ZQ, 2022, APPL INTELL, V52, P2987, DOI 10.1007/s10489-021-02551-8
   Song XL, 2021, IEEE T MULTIMEDIA, V24, P3229, DOI 10.1109/TMM.2021.3096014
   Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552
   Wang GQ, 2020, PROC CVPR IEEE, P6677, DOI 10.1109/CVPR42600.2020.00671
   Wang JY, 2018, PROC CVPR IEEE, P2275, DOI 10.1109/CVPR.2018.00242
   Wang ML, 2021, AAAI CONF ARTIF INTE, V35, P2764
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu YH, 2022, AAAI CONF ARTIF INTE, P2750
   Xiang W., 2020, PROC ASIAN C COMPUT, P1
   Xu J, 2018, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2018.00226
   Xuan SY, 2021, PROC CVPR IEEE, P11921, DOI 10.1109/CVPR46437.2021.01175
   Yang FX, 2021, PROC CVPR IEEE, P4853, DOI 10.1109/CVPR46437.2021.00482
   Yang Zou, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12347), P87, DOI 10.1007/978-3-030-58536-5_6
   Yu HX, 2020, PROC CVPR IEEE, P5527, DOI 10.1109/CVPR42600.2020.00557
   Yu HX, 2019, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR.2019.00225
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zhai Y, 2020, COMPUTER VISION ECCV, P594, DOI DOI 10.1007/978-3-030-58571-6_35
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zheng F, 2019, PROC CVPR IEEE, P8506, DOI 10.1109/CVPR.2019.00871
   Zheng KC, 2021, PROC CVPR IEEE, P5306, DOI 10.1109/CVPR46437.2021.00527
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhong Z, 2018, LECT NOTES COMPUT SC, V11217, P176, DOI 10.1007/978-3-030-01261-8_11
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2018, PROC CVPR IEEE, pCP99, DOI 10.1109/CVPR.2018.00541
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhongdao Wang, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P72, DOI 10.1007/978-3-030-58621-8_5
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Zijie Zhuang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12357), P140, DOI 10.1007/978-3-030-58610-2_9
NR 59
TC 7
Z9 8
U1 8
U2 8
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6171
EP 6182
DI 10.1109/TMM.2022.3206662
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA X5KG5
UT WOS:001098831500038
DA 2024-07-18
ER

PT J
AU Tao, YS
   Zhang, J
   Hong, JJ
   Zhu, YS
AF Tao, Yusheng
   Zhang, Jian
   Hong, Jiajing
   Zhu, Yuesheng
TI DREAMT: Diversity Enlarged Mutual Teaching for Unsupervised Domain
   Adaptive Person Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Domain adaption; mutual teaching; person re-identification
AB Pseudo-label-based methods of unsupervised domain adaption (UDA) can transfer the knowledge learned from a labeled source domain to an unlabeled target domain and have recently achieved significant progress in the application of person reidentification (re-ID). However, these methods suffer from serious label noise problems that downgrade the retrieval performance in UDA person re-ID. The mutual teaching framework (MTF) with dual networks attempts to tackle this problem by generating reliable soft pseudo labels but results in a mutual convergence problem. In this paper, a novel DiveRsity EnlArged Mutual Teaching framework (DREAMT) is proposed to solve the problem mentioned above. Based on the primary mutual-mean-teaching mechanism two strategies are developed in DREAMT, that is, GAN-based source domain augmentation (GSDA) and cross-branch mutual supervision (CBMS) for dual networks. Specifically, GSDA exploits two GANs to augment source domain datasets in different ways for pre-training to improve the pre-trained models' performance and enlarge the diversity at the beginning of target domain adaption. During target adaption, each network in MTF adopts two branches to extract different features. CBMS based on hard and soft pseudo labels is across branches and networks and can help to maintain the diversity between training peers in the whole training process. Extensive experiments have demonstrated that our proposed DREAMT framework achieves better mAP and CMC performance than the existing mutual teaching methods and outperforms various state-of-the-art methods in UDA person re-ID tasks.
C1 [Tao, Yusheng; Zhang, Jian; Hong, Jiajing; Zhu, Yuesheng] Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
C3 Peking University
RP Zhu, YS (corresponding author), Peking Univ, Sch Elect & Comp Engn, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
EM ystao@stu.pku.edu.cn; zhangjian.sz@pku.edu.cn; hjj1901213119@pku.edu.cn;
   zhuys@pku.edu.cn
FU National Innovation 2030 Major S&T Project of China [2020AAA0104203];
   Nature Science Foundation of China [62006007]
FX This work was supported in part by the National Innovation2030 Major S&T
   Project of China under Grant 2020AAA0104203, and in part by the Nature
   Science Foundation of China under Grant 62006007.
CR Azizpour Hossein, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), P36, DOI 10.1109/CVPRW.2015.7301270
   Behera NKS, 2020, PATTERN RECOGN LETT, V138, P282, DOI 10.1016/j.patrec.2020.07.030
   Chen GY, 2019, IEEE I CONF COMP VIS, P9636, DOI 10.1109/ICCV.2019.00973
   Chen H, 2021, IEEE WINT CONF APPL, P1, DOI 10.1109/WACV48630.2021.00005
   Chen YB, 2019, IEEE I CONF COMP VIS, P232, DOI 10.1109/ICCV.2019.00032
   Dai ZZ, 2019, IEEE I CONF COMP VIS, P3690, DOI 10.1109/ICCV.2019.00379
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng WJ, 2018, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.2018.00110
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fang Zhao, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P526, DOI 10.1007/978-3-030-58621-8_31
   Fu Y, 2019, IEEE I CONF COMP VIS, P6111, DOI 10.1109/ICCV.2019.00621
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ge Yixiao, 2020, ARXIV200101526
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans A, 2017, Arxiv, DOI [arXiv:1703.07737, DOI 10.48550/ARXIV.1703.07737]
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Jia MX, 2021, AAAI CONF ARTIF INTE, V35, P1673
   Jia MX, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1026
   Komodakis N, 2017, P ICLR
   Leng QM, 2020, IEEE T CIRC SYST VID, V30, P1092, DOI 10.1109/TCSVT.2019.2898940
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin YT, 2020, PROC CVPR IEEE, P3387, DOI 10.1109/CVPR42600.2020.00345
   Liu XB, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P547, DOI 10.1145/3394171.3413904
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Pan L, 2020, INT CONF ACOUST SPEE, P4302, DOI [10.1109/icassp40776.2020.9053719, 10.1109/ICASSP40776.2020.9053719]
   Quan RJ, 2019, IEEE I CONF COMP VIS, P3749, DOI 10.1109/ICCV.2019.00385
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Tarvainen Antti, 2017, ADV NEURAL INFORM PR, P2, DOI DOI 10.1137/0330046
   Tung F, 2019, IEEE I CONF COMP VIS, P1365, DOI 10.1109/ICCV.2019.00145
   Wang D., 2020, P IEEE CVF C COMP VI, P10981
   Wang WH, 2022, IEEE T IMAGE PROCESS, V31, P1532, DOI 10.1109/TIP.2022.3140614
   Wang X., 2007, P IEEE CVF INT C COM, P1
   Wang Z, 2019, IEEE T MULTIMEDIA, V21, P2376, DOI 10.1109/TMM.2019.2898753
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Xu Z., 2018, P INT C LEARN REPR, P1
   Yang FX, 2020, AAAI CONF ARTIF INTE, V34, P12597
   Yang FX, 2020, IEEE T MULTIMEDIA, V22, P2444, DOI 10.1109/TMM.2019.2957928
   Ye M, 2022, IEEE T PATTERN ANAL, V44, P2872, DOI 10.1109/TPAMI.2021.3054775
   Yunpeng Zhai, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9018, DOI 10.1109/CVPR42600.2020.00904
   Zeng ZL, 2020, IEEE T MULTIMEDIA, V22, P3064, DOI 10.1109/TMM.2020.2969782
   Zhang J, 2019, AAAI CONF ARTIF INTE, P9185
   Zhang XY, 2019, IEEE I CONF COMP VIS, P8221, DOI 10.1109/ICCV.2019.00831
   Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zhong Z., 2017, P IEEE CVF C COMP VI, P1318, DOI [10.1109/CVPR.2017.389, DOI 10.1109/CVPR.2017.389]
   Zhong Z, 2021, IEEE T PATTERN ANAL, V43, P2723, DOI 10.1109/TPAMI.2020.2976933
   Zhong Z, 2019, PROC CVPR IEEE, P598, DOI 10.1109/CVPR.2019.00069
   Zhong Z, 2019, IEEE T IMAGE PROCESS, V28, P1176, DOI 10.1109/TIP.2018.2874313
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhou KY, 2019, IEEE I CONF COMP VIS, P3701, DOI 10.1109/ICCV.2019.00380
NR 54
TC 7
Z9 7
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 4586
EP 4597
DI 10.1109/TMM.2022.3178599
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA W1OJ1
UT WOS:001089390200037
DA 2024-07-18
ER

PT J
AU Tian, HY
   Ma, X
   Li, X
   Li, YB
AF Tian, Haoyu
   Ma, Xin
   Li, Xiang
   Li, Yibin
TI Skeleton-Based Action Recognition With Select-Assemble-Normalize Graph
   Convolutional Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Skeleton-based action recognition; graph convolution network;
   select-assemble-normalize graph; non-local neighborhood; bi-level
   aggregation
ID NEURAL-NETWORKS
AB Skeleton-based action recognition has been substantially driven by the development of artificial intelligence technology and deep sensors. Recently, graph convolutional networks (GCNs) have achieved excellent performances in skeleton-based action recognition. However, the performances of GCN-based methods are impaired by inappropriate node partitioning strategy and obstructed long-range information flow. To solve these issues, a novel Select-Assemble-Normalize Graph Convolution Network (SAN-GCN) is proposed to model the spatio-temporal features of skeleton. First, all skeleton joints are selected as root nodes, and the neighborhoods of the root joints are assembled and normalized according to the body structure, which explicitly and interpretably expresses the spatial geometry relation of the skeleton joints. Second, we propose an attention-based assembly and normalization strategy to adaptively capture non-local joints. The adaptive assembly and normalization can avoid the dilution of key long-range features. Moreover, a bi-level aggregation strategy is introduced to learn spatio-temporal dependencies of joints, where the low-level aggregation aligns the normalized neighborhood graphs, and the high-level aggregation aggregates the features of neighbor nodes by a standard convolution kernel. In high-level aggregation, it is convenient to realize factorized spatio-temporal aggregation or unified spatio-temporal aggregation. Extensive experiments on four datasets with different numbers of action patterns demonstrate that our model achieves comparable performance with the state-of-the-art works.
C1 [Tian, Haoyu; Ma, Xin; Li, Xiang; Li, Yibin] Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250061, Peoples R China.
   [Tian, Haoyu; Ma, Xin; Li, Xiang; Li, Yibin] Minist Educ, Engn Res Ctr Intelligent Unmanned Syst, Beijing 100044, Peoples R China.
C3 Shandong University
RP Ma, X (corresponding author), Shandong Univ, Ctr Robot, Sch Control Sci & Engn, Jinan 250061, Peoples R China.; Ma, X (corresponding author), Minist Educ, Engn Res Ctr Intelligent Unmanned Syst, Beijing 100044, Peoples R China.
EM tianhaoyu@mail.sdu.edu.cn; maxin@sdu.edu.cn;
   lixiang0814@mail.sdu.edu.cn; liyb@sdu.edu.cn
OI Li, Xiang/0000-0003-1529-7057; Tian, Haoyu/0000-0003-3789-2084
FU Basic Research Key Development Program of Shandong Province
FX No Statement Available
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Bruna J., 2014, P INT C LEARN REPR
   Chen YX, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13339, DOI 10.1109/ICCV48922.2021.01311
   Cheng K, 2020, PROC CVPR IEEE, P180, DOI 10.1109/CVPR42600.2020.00026
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gao X, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P601, DOI 10.1145/3343031.3351170
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Henaff M, 2015, Arxiv, DOI arXiv:1506.05163
   Hu ZS, 2022, NEUROCOMPUTING, V492, P624, DOI 10.1016/j.neucom.2021.12.054
   Ke Cheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12369), P536, DOI 10.1007/978-3-030-58586-0_32
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Kipf T.N., 2017, INT C LEARNING REPRE, DOI DOI 10.48550/ARXIV.1609.02907
   Kong J, 2021, IEEE T CIRC SYST VID, V31, P4394, DOI 10.1109/TCSVT.2021.3050807
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Li B, 2019, AAAI CONF ARTIF INTE, P8561
   Li C, 2017, IEEE INT CONF MULTI
   Li MS, 2021, IEEE T IMAGE PROCESS, V30, P7760, DOI 10.1109/TIP.2021.3108708
   Li MS, 2019, PROC CVPR IEEE, P3590, DOI 10.1109/CVPR.2019.00371
   Li RY, 2018, AAAI CONF ARTIF INTE, P3546
   Li SJ, 2021, IEEE ROBOT AUTOM LET, V6, P1028, DOI 10.1109/LRA.2021.3056361
   Li TJ, 2021, PROC CVPR IEEE, P16261, DOI 10.1109/CVPR46437.2021.01600
   Liu J, 2020, IEEE T PATTERN ANAL, V42, P2684, DOI 10.1109/TPAMI.2019.2916873
   Liu X, 2021, NEUROCOMPUTING, V444, P288, DOI 10.1016/j.neucom.2020.03.126
   Liu YA, 2022, KNOWL-BASED SYST, V240, DOI 10.1016/j.knosys.2022.108146
   Liu ZY, 2020, PROC CVPR IEEE, P140, DOI 10.1109/CVPR42600.2020.00022
   Lu QJ, 2022, PROCEEDINGS OF THE 30TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2022, P2249, DOI 10.1145/3503161.3548287
   Miao SY, 2022, IEEE T CIRC SYST VID, V32, P4893, DOI 10.1109/TCSVT.2021.3124562
   Niepert M, 2016, PR MACH LEARN RES, V48
   Pei H., 2020, INT C LEARNING REPRE
   Peng W, 2021, IEEE SIGNAL PROC LET, V28, P244, DOI 10.1109/LSP.2021.3049691
   Plizzari C, 2021, COMPUT VIS IMAGE UND, V208, DOI 10.1016/j.cviu.2021.103219
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2020, IEEE T IMAGE PROCESS, V29, P9532, DOI 10.1109/TIP.2020.3028207
   Shi L, 2019, PROC CVPR IEEE, P7904, DOI 10.1109/CVPR.2019.00810
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Shu XB, 2022, IEEE T CIRC SYST VID, V32, P5281, DOI 10.1109/TCSVT.2022.3142771
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Song YF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1625, DOI 10.1145/3394171.3413802
   Song YF, 2021, IEEE T CIRC SYST VID, V31, P1915, DOI 10.1109/TCSVT.2020.3015051
   Tianjiao Li, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P420, DOI 10.1007/978-3-030-58621-8_25
   Velickovic Petar, 2018, INT C LEARN REPR
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang J, 2014, PROC CVPR IEEE, P2649, DOI 10.1109/CVPR.2014.339
   Wang J, 2012, PROC CVPR IEEE, P1290, DOI 10.1109/CVPR.2012.6247813
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Weinland D, 2011, COMPUT VIS IMAGE UND, V115, P224, DOI 10.1016/j.cviu.2010.10.002
   Wu C, 2022, IEEE T CIRC SYST VID, V32, P2120, DOI 10.1109/TCSVT.2021.3085959
   Wu HB, 2020, IEEE T MULTIMEDIA, V22, P2293, DOI 10.1109/TMM.2019.2953814
   Xia RJ, 2022, IEEE T MULTIMEDIA, V24, P2648, DOI 10.1109/TMM.2021.3086758
   Xie YL, 2022, IET COMPUT VIS, V16, P266, DOI 10.1049/cvi2.12086
   Xu BQ, 2022, IEEE T IMAGE PROCESS, V31, P3852, DOI 10.1109/TIP.2022.3175605
   Xu KL, 2022, AAAI CONF ARTIF INTE, P2866
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang H, 2022, IEEE T IMAGE PROCESS, V31, P164, DOI 10.1109/TIP.2021.3129117
   Ye FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P55, DOI 10.1145/3394171.3413941
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhao MY, 2022, NEUROCOMPUTING, V501, P640, DOI 10.1016/j.neucom.2022.06.070
NR 61
TC 1
Z9 1
U1 12
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 8527
EP 8538
DI 10.1109/TMM.2023.3318325
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000037
DA 2024-07-18
ER

PT J
AU Wu, SS
   Tang, H
   Jing, XY
   Zhao, HF
   Qian, JJ
   Sebe, N
   Yan, Y
AF Wu, Songsong
   Tang, Hao
   Jing, Xiao-Yuan
   Zhao, Haifeng
   Qian, Jianjun
   Sebe, Nicu
   Yan, Yan
TI Cross-View Panorama Image Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Generators; Image synthesis; Task analysis; Image segmentation; Feature
   extraction; Semantics; Generative adversarial networks; Cross-view
   panorama generation; feedback adversarial learning; multi-scale feature
   alignment; GANs
AB In this paper, we tackle the problem of synthesizing a ground-view panorama image conditioned on a top-view aerial image, which is a challenging problem due to the large gap between the two image domains with different view-points. Instead of learning cross-view mapping in a feedforward pass, we propose a novel adversarial feedback GAN framework named PanoGAN with two key components: an adversarial feedback module and a dual branch discrimination strategy. First, the aerial image is fed into the generator to produce a target panorama image and its associated segmentation map in favor of model training with layout semantics. Second, the feature responses of the discriminator encoded by our adversarial feedback module are fed back to the generator to refine the intermediate representations, so that the generation performance is continually improved through an iterative generation process. Third, to pursue high-fidelity and semantic consistency of the generated panorama image, we propose a pixel-segmentation alignment mechanism under the dual branch discrimiantion strategy to facilitate cooperation between the generator and the discriminator. Extensive experimental results on two challenging cross-view image datasets show that PanoGAN enables high-quality panorama image generation with more convincing details than state-of-the-art approaches. The source code and trained models are available at https://github.com/ sswuai/ PanoGAN.
C1 [Wu, Songsong; Jing, Xiao-Yuan] Guangdong Univ Petrochem Technol, Sch Comp Sci, Maoming 525000, Peoples R China.
   [Tang, Hao] Swiss Fed Inst Technol, Dept Informat Technol & Elect Engn, CH-8092 Zurich, Switzerland.
   [Jing, Xiao-Yuan] Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.
   [Jing, Xiao-Yuan] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
   [Zhao, Haifeng] Jinling Inst Technol, Sch Software Engn, Nanjing 211169, Peoples R China.
   [Zhao, Haifeng] Jiangsu Hoperun Software Co Ltd, Nanjing 210012, Peoples R China.
   [Qian, Jianjun] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci DISI, I-38123 Trento, Italy.
   [Yan, Yan] IIT, Dept Comp Sci, Chicago, IL 60616 USA.
C3 Guangdong University of Petrochemical Technology; Swiss Federal
   Institutes of Technology Domain; ETH Zurich; Wuhan University; Nanjing
   University; Jinling Institute of Technology; Nanjing University of
   Science & Technology; University of Trento; Illinois Institute of
   Technology
RP Jing, XY (corresponding author), Guangdong Univ Petrochem Technol, Sch Comp Sci, Maoming 525000, Peoples R China.; Jing, XY (corresponding author), Wuhan Univ, Sch Comp Sci, Wuhan 430072, Peoples R China.; Jing, XY (corresponding author), Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM sswuai@126.com; hao.tang@vision.ee.ethz.ch; jingxy_2020@126.com;
   zhf@jit.edu.cn; csjqian@njust.edu.cn; sebe@disi.unitn.it;
   tom_yan@txstate.edu
RI Zhang, Yunxuan/IXD-9283-2023; Zhao, Haifeng/AAE-6063-2021; Sebe,
   Niculae/KEC-2000-2024
OI Zhao, Haifeng/0000-0002-5196-4921; Sebe, Niculae/0000-0002-6597-7248;
   Qian, Jianjun/0000-0002-0968-8556; Tang, Hao/0000-0002-2077-1246
FU National Natural Science Foundation of China [61876083, 61933013,
   62176069]; Natural Science Foundation of Guangdong Province
   [2019A1515011076]; Innovation Group of Guangdong Education Department
   [2020KCXTD014]; International Science and Technology Cooperation Project
   of Jiangsu Province [BZ2020069]; Major Program of University Natural
   Science Research of Jiangsu Province [21KJA520001]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61876083, 61933013 and 62176069, in
   part by the Natural Science Foundation of Guangdong Province under Grant
   2019A1515011076, in part by the Innovation Group of Guangdong Education
   Department under Grant 2020KCXTD014, in part by the International
   Science and Technology Cooperation Project of Jiangsu Province under
   Grant BZ2020069, and in part by the Major Program of University Natural
   Science Research of Jiangsu Province under Grant 21KJA520001. The
   Associate Editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jiebo Luo.
CR Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512
   Dosovitskiy A, 2017, IEEE T PATTERN ANAL, V39, P692, DOI 10.1109/TPAMI.2016.2567384
   Gu XL, 2021, IEEE T MULTIMEDIA, V23, P2361, DOI 10.1109/TMM.2020.3009500
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Hao Tang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P717, DOI 10.1007/978-3-030-58595-2_43
   Hu SX, 2018, PROC CVPR IEEE, P7258, DOI 10.1109/CVPR.2018.00758
   Huh M, 2019, PROC CVPR IEEE, P1476, DOI 10.1109/CVPR.2019.00157
   Isola Phillip, 2016, ARXIV161107004, DOI [DOI 10.1109/CVPR.2017.632, 10.1109/CVPR.2017.632]
   Kim T., 2017, P 34 INT C MACH LEAR, P1857, DOI DOI 10.1109/WPT.2017.7953894
   Kingma D. P., 2014, arXiv
   Li S, 2018, IEEE T MULTIMEDIA, V20, P1948, DOI 10.1109/TMM.2018.2791810
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin T.Y., 2017, P IEEE C COMPUTER VI, P2117, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]
   Liu GW, 2020, INT CONF ACOUST SPEE, P1843, DOI [10.1109/icassp40776.2020.9053957, 10.1109/ICASSP40776.2020.9053957]
   Liu X., 2019, Advances in Neural Information Processing Systems, P570
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Mirza M., 2014, ARXIV PREPRINT ARXIV, P2672, DOI 10.48550/arXiv.1411.1784
   Narayan Sanath, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12367), P479, DOI 10.1007/978-3-030-58542-6_29
   Nguyen T.D., 2017, PREPRINT, DOI [10.48550/arXiv.1709.03831, DOI 10.48550/ARXIV.1709.03831]
   Regmi K, 2018, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2018.00369
   Regmi K, 2019, IEEE I CONF COMP VIS, P470, DOI 10.1109/ICCV.2019.00056
   Salimans T, 2016, ADV NEUR IN, V29
   Shama F, 2019, IEEE I CONF COMP VIS, P3204, DOI 10.1109/ICCV.2019.00330
   Tang H., 2020, P IEEE CVPR, P7870
   Tang H, 2019, PROC CVPR IEEE, P2412, DOI 10.1109/CVPR.2019.00252
   Tian YC, 2017, PROC CVPR IEEE, P1998, DOI 10.1109/CVPR.2017.216
   Ulyanov Dmitry, 2016, arXiv
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Wang W, 2020, IEEE T MULTIMEDIA, V22, P2808, DOI 10.1109/TMM.2019.2963621
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Workman S, 2015, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2015.451
   Wu PW, 2019, IEEE I CONF COMP VIS, P5913, DOI 10.1109/ICCV.2019.00601
   Yang J., 2015, Advances in Neural Information Processing Systems, P1099
   Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440
   Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhou TH, 2016, LECT NOTES COMPUT SC, V9908, P286, DOI 10.1007/978-3-319-46493-0_18
   Zhou Y., 2017, 1 AS AUSTR C PREC PA, P1, DOI DOI 10.5244/C.31.186
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
NR 39
TC 2
Z9 2
U1 5
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3546
EP 3559
DI 10.1109/TMM.2022.3162474
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FE2H4
UT WOS:001144015500005
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, C
   Chen, ML
   Yuan, Y
   Wang, Q
AF Yang, Chuang
   Chen, Mulin
   Yuan, Yuan
   Wang, Qi
TI Text Growing on Leaf
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Scene text detection; irregular-shaped text; leaf vein; text
   representation method
AB Irregular-shaped texts bring challenges to Scene Text Detection (STD). Although existing regression-based approaches achieve comparable performances, they fail to cover some highly curved ribbon-like text lines. Inspired by morphology, we found that the leaf vein can easily cover various geometries. Specifically, lateral and thin veins are emitted to margin along main vein gradually with the leaf growth. This process can decompose a concave object into consecutive convex regions, which are easier to fit. Hence, the leaf vein is suitable for representing highly curved texts. Considering the aforementioned advantage, we design a leaf vein-based text representation method (LVT), where text contour is treated as leaf margin and represented through main, lateral, and thin veins. We further construct a detection framework based on LVT, namely LeafText. In the text reconstruction stage, LeafText simulates the leaf growth process to rebuild text contours. It grows main veins in Cartesian coordinates to locate texts roughly at first. Then, lateral and thin veins are generated along the main vein growth direction in polar coordinates. They are responsible for generating the coarse contour and refining it, respectively. Meanwhile, Multi-Oriented Smoother (MOS) is designed to smooth the main vein for ensuring reliable growth directions of lateral and thin veins. Additionally, a global incentive loss is proposed to enhance the predictions of lateral and thin veins. Ablation experiments demonstrate LVT can fit irregular-shaped texts precisely and verify the effectiveness of MOS and global incentive loss. Comparisons show that LeafText is superior to existing state-of-the-art (SOTA) methods on MSRA-TD500, CTW1500, Total-Text, and ICDAR2015 datasets.
C1 [Yang, Chuang] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Yang, Chuang; Chen, Mulin; Yuan, Yuan; Wang, Qi] Northwestern Polytech Univ, Sch Artificial Intelligence OPt & Elect IOPEN, Xian 710072, Shaanxi, Peoples R China.
C3 Northwestern Polytechnical University; Northwestern Polytechnical
   University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Artificial Intelligence OPt & Elect IOPEN, Xian 710072, Shaanxi, Peoples R China.
EM cyang113@mail.nwpu.edu.cn; chenmulin@mail.nwpu.edu.cn;
   y.yuan.ieee@gmail.com; crabwq@gmail.com
RI Wang, Chen/JZE-6385-2024; JIANG, Peng/KGL-3427-2024; Yang,
   Chuang/KHW-4673-2024
OI Wang, Qi/0000-0002-7028-4956
FU National Natural Science Foundation of China
FX No Statement Available
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Cai Y, 2022, PATTERN RECOGN, V127, DOI 10.1016/j.patcog.2022.108608
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2022, IEEE T MULTIMEDIA, V24, P1883, DOI 10.1109/TMM.2021.3073575
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Enze Xie, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P12190, DOI 10.1109/CVPR42600.2020.01221
   Feng W, 2021, PROC CVPR IEEE, P1695, DOI 10.1109/CVPR46437.2021.00174
   Feng W, 2019, IEEE I CONF COMP VIS, P9075, DOI 10.1109/ICCV.2019.00917
   Fu Z, 2022, Trans. Multimedia Comput., Commun. Appl., V19, P1
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Huang LC, 2015, Arxiv, DOI arXiv:1509.04874
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Kingma D. P., 2015, P INT C LEARN REPR, P1, DOI DOI 10.1002/9781118900772.ETRDS0277
   Li M, 2023, IEEE T MULTIMEDIA, V25, P649, DOI 10.1109/TMM.2021.3129651
   Liao MH, 2023, IEEE T PATTERN ANAL, V45, P919, DOI 10.1109/TPAMI.2022.3155612
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZC, 2018, PROC CVPR IEEE, P6936, DOI 10.1109/CVPR.2018.00725
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma CX, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107684
   Milletari F, 2016, INT CONF 3D VISION, P565, DOI 10.1109/3DV.2016.79
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Peng DZ, 2023, IEEE T MULTIMEDIA, V25, P2368, DOI 10.1109/TMM.2022.3146771
   Qin Xugong, 2019, ICDAR, P559
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shi-Xue Zhang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9696, DOI 10.1109/CVPR42600.2020.00972
   Su YC, 2023, IEEE T MULTIMEDIA, V25, P5030, DOI 10.1109/TMM.2022.3186431
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   VATTI BR, 1992, COMMUN ACM, V35, P56, DOI 10.1145/129902.129906
   Wan Q, 2021, PROC CVPR IEEE, P5979, DOI 10.1109/CVPR46437.2021.00592
   Wang FF, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P111, DOI 10.1145/3394171.3413819
   Wang H, 2020, AAAI CONF ARTIF INTE, V34, P12160
   Wang WH, 2022, IEEE T PATTERN ANAL, V44, P5349, DOI 10.1109/TPAMI.2021.3077555
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wu LT, 2023, IEEE T MULTIMEDIA, V25, P2404, DOI 10.1109/TMM.2022.3146779
   Xu YC, 2021, IEEE T PATTERN ANAL, V43, P1452, DOI [10.1109/TGRS.2020.3026387, 10.1109/TPAMI.2020.2974745]
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Yang C, 2022, IEEE T IMAGE PROCESS, V31, P2864, DOI 10.1109/TIP.2022.3141844
   Yao C, 2014, IEEE T IMAGE PROCESS, V23, P4737, DOI 10.1109/TIP.2014.2353813
   Yao C, 2012, PROC CVPR IEEE, P1083, DOI 10.1109/CVPR.2012.6247787
   Yuliang L, 2017, Arxiv, DOI arXiv:1712.02170
   Yuliang Liu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9806, DOI 10.1109/CVPR42600.2020.00983
   Yuxin Wang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11750, DOI 10.1109/CVPR42600.2020.01177
   Zhang CQ, 2019, PROC CVPR IEEE, P10544, DOI 10.1109/CVPR.2019.01080
   Zhang S, 2021, IEEE T MULTIMEDIA, V23, P454, DOI 10.1109/TMM.2020.2978630
   Zhang Shi-Xue, 2020, IEEE C COMPUTER VISI, P9699
   Zhang Z, 2016, PROC CVPR IEEE, P4159, DOI 10.1109/CVPR.2016.451
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu YQ, 2021, PROC CVPR IEEE, P3122, DOI 10.1109/CVPR46437.2021.00314
NR 67
TC 0
Z9 0
U1 2
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9029
EP 9043
DI 10.1109/TMM.2023.3244322
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN3Z4
UT WOS:001125902000006
DA 2024-07-18
ER

PT J
AU Yang, K
   Zhang, HJ
   Gao, F
   Shi, JY
   Zhang, YF
   Wu, QMJ
AF Yang, Kai
   Zhang, Haijun
   Gao, Feng
   Shi, Jianyang
   Zhang, Yanfeng
   Wu, Q. M. Jonathan
TI DETA: A Point-Based Tracker With Deformable Transformer and Task-Aligned
   Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Target tracking; Training; Transformers; Task analysis; Correlation;
   Feature extraction; Detectors; Border feature; deformable transformer;
   point-based tracker; task-aligned learning; training sample assignment;
   visual tracking
AB Current point-based trackers are usually implemented by the following two branches: a classification branch for predicting the target candidate locations and a regression branch for regressing the tracking box, which may lead to a spatial misalignment between the two tasks. Meanwhile, they ignore a meaningful exploration on how to define positive and negative samples during training and explicit border information for accurate box prediction. In this research, we investigate the key issues of point-based trackers and unlock their key limitations. First, we design a novel task-aligned component and a new loss function, named task-aligned loss, to learn the alignment of the classification and regression tasks. Second, we introduce a border alignment (BorderAlign) component in both the classification and regression branches to effectively exploit the border features of a tracking target. Third, we develop an adaptive training sample assignment (ATSA) to adaptively divide the positive and negative samples based on the statistical characteristics of the tracking object. Finally, a deformable transformer is developed to enhance the representations of search features and explore rich temporal contexts among video frames. Extensive experimental results demonstrate that the proposed tracker achieves state-of-the-art performance on six tracking benchmark datasets.
C1 [Yang, Kai; Zhang, Haijun; Shi, Jianyang; Zhang, Yanfeng] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
   [Gao, Feng] Xi An Jiao Tong Univ, State Grid Shaanxi Elect Power Res Inst, SKLMS Lab, Xian 710049, Peoples R China.
   [Wu, Q. M. Jonathan] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
C3 Harbin Institute of Technology; Xi'an Jiaotong University; University of
   Windsor
RP Zhang, HJ (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM 18b951032@stu.hit.edu.cn; hjzhang@hit.edu.cn; fgao@sei.xjtu.edu.cn;
   19b951026@stu.hit.edu.cn; zhangyanfeng@hit.edu.cn; jwu@uwindsor.ca
RI Zhang, Haijun/N-8470-2015
FU National Natural Science Foundation of China [61972112, 61832004];
   Guangdong Basic and Applied Basic Research Foundation [2021B1515020088];
   Shenzhen Science and Technology Program [JCYJ20210324131203009]; project
   of State Grid Shanxi Electrical Power Company [5226SX21002Q]; HITSZ-J&A
   Joint Laboratory of Digital Design and Intelligent Fabrication
   [HITSZ-JA-2021A01]
FX This work was supported in part by the National Natural Science
   Foundation of China, under Grants 61972112 and 61832004, in part by
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2021B1515020088, in part by Shenzhen Science and Technology Program
   under Grant JCYJ20210324131203009, in part by the project of State Grid
   Shanxi Electrical Power Company under Grant 5226SX21002Q, and in part by
   the HITSZ-J & A Joint Laboratory of Digital Design and Intelligent
   Fabrication under Grant HITSZ-J & A-2021A01.
CR Bhat Goutam, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12368), P205, DOI 10.1007/978-3-030-58592-1_13
   Bhat G, 2019, IEEE I CONF COMP VIS, P6181, DOI 10.1109/ICCV.2019.00628
   Chen X, 2022, Arxiv, DOI arXiv:2203.13537
   Chen X, 2021, PROC CVPR IEEE, P8122, DOI 10.1109/CVPR46437.2021.00803
   Chenchen Zhu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12354), P91, DOI 10.1007/978-3-030-58545-7_6
   Cui YT, 2022, COMPUT VIS IMAGE UND, V224, DOI 10.1016/j.cviu.2022.103547
   Dai J., 2021, ICLR
   Danelljan M, 2020, PROC CVPR IEEE, P7181, DOI 10.1109/CVPR42600.2020.00721
   Danelljan M, 2019, PROC CVPR IEEE, P4655, DOI 10.1109/CVPR.2019.00479
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Feng CJ, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P3490, DOI 10.1109/ICCV48922.2021.00349
   Fu Z., 2022, P 31 INT JOINT C ART, P905
   Fu ZH, 2021, PROC CVPR IEEE, P13769, DOI 10.1109/CVPR46437.2021.01356
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1134, DOI 10.1109/ICCV.2017.128
   Gao SY, 2022, LECT NOTES COMPUT SC, V13682, P146, DOI 10.1007/978-3-031-20047-2_9
   Guo DY, 2021, PROC CVPR IEEE, P9538, DOI 10.1109/CVPR46437.2021.00942
   Guo DY, 2020, PROC CVPR IEEE, P6268, DOI 10.1109/CVPR42600.2020.00630
   Han Qiu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P549, DOI 10.1007/978-3-030-58452-8_32
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Kang Kim, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12370), P355, DOI 10.1007/978-3-030-58595-2_22
   Kingma D. P., 2014, arXiv
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linyu Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12360), P759, DOI 10.1007/978-3-030-58555-6_45
   Ma F, 2022, PROC CVPR IEEE, P8771, DOI 10.1109/CVPR52688.2022.00858
   Mayer C, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13424, DOI 10.1109/ICCV48922.2021.01319
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Rezatofighi H, 2019, PROC CVPR IEEE, P658, DOI 10.1109/CVPR.2019.00075
   Tian SJ, 2021, IEEE T MULTIMEDIA, V23, P120, DOI 10.1109/TMM.2020.2978636
   Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Voigtlaender P, 2020, PROC CVPR IEEE, P6577, DOI 10.1109/CVPR42600.2020.00661
   Wang GT, 2020, PROC CVPR IEEE, P6287, DOI 10.1109/CVPR42600.2020.00632
   Wang N, 2021, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR46437.2021.00162
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xi M, 2022, IEEE T MULTIMEDIA, V24, P2791, DOI 10.1109/TMM.2021.3087340
   Xu YD, 2020, AAAI CONF ARTIF INTE, V34, P12549
   Yan B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10428, DOI 10.1109/ICCV48922.2021.01028
   Yu B, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9836, DOI 10.1109/ICCV48922.2021.00971
   Yu YC, 2020, PROC CVPR IEEE, P6727, DOI 10.1109/CVPR42600.2020.00676
   Yuan D, 2022, ACM T MULTIM COMPUT, V18, DOI 10.1145/3486678
   Yuan D, 2022, J VIS COMMUN IMAGE R, V83, DOI 10.1016/j.jvcir.2021.103428
   Yuan D, 2021, IEEE T IMAGE PROCESS, V30, P976, DOI 10.1109/TIP.2020.3037518
   Zhang HY, 2021, PROC CVPR IEEE, P8510, DOI 10.1109/CVPR46437.2021.00841
   Zhang S., 2020, P IEEECVF C COMPUTER, P9759
   Zhang XS, 2019, ADV NEUR IN, V32
   Zhang ZP, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P13319, DOI 10.1109/ICCV48922.2021.01309
   Zhang ZP, 2019, PROC CVPR IEEE, P4586, DOI 10.1109/CVPR.2019.00472
   Zhao MJ, 2021, Arxiv, DOI arXiv:2105.03817
   Zhipeng Zhang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12366), P771, DOI 10.1007/978-3-030-58589-1_46
   Zhou ZK, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P9846, DOI 10.1109/ICCV48922.2021.00972
NR 54
TC 3
Z9 3
U1 4
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 7545
EP 7558
DI 10.1109/TMM.2022.3223213
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000058
DA 2024-07-18
ER

PT J
AU Yang, YX
   Tian, X
   Ng, WWY
   Gao, Y
AF Yang, Yuxiang
   Tian, Xing
   Ng, Wing W. Y.
   Gao, Ying
TI Knowledge Distillation Hashing for Occluded Face Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Face recognition; Faces; Codes; Feature extraction; Representation
   learning; Convolutional neural networks; Training; Face retrieval;
   hashing; knowledge distillation; occlusion
ID OCCLUSION
AB Deep hashing has proven to be efficient and effective for large-scale face retrieval. However, existing hashing methods are designed for normal face images only. They fail to consider the fact that face images may be occluded because of wearing masks, hats, glasses, etc. Retrieval performance of existing face retrieval methods is much worse when dealing with occluded face images. In this work, we propose the knowledge distillation hashing (KDH) to deal with occluded face images. The KDH is a two-stage learning approach with teacher-student model distillation. We first train a teacher hashing network using normal face images and then the knowledge from teacher model is used to guide the optimization of the student model using occluded face images as input only. With knowledge distillation, we build a connection between imperfect face information and the optimal hash codes. Experimental results show that the KDH yields significant improvements and better retrieval performance in comparison to existing state-of-the-art deep hashing retrieval methods under six different face occlusion situations.
C1 [Yang, Yuxiang; Ng, Wing W. Y.; Gao, Ying] South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cyb, Guangzhou 510006, Peoples R China.
   [Tian, Xing] South China Normal Univ, Sch Artificial Intelligence, Guangzhou 510631, Peoples R China.
C3 South China University of Technology; South China Normal University
RP Ng, WWY (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangdong Prov Key Lab Computat Intelligence & Cyb, Guangzhou 510006, Peoples R China.; Tian, X (corresponding author), South China Normal Univ, Sch Artificial Intelligence, Guangzhou 510631, Peoples R China.
EM fotonyoung@gmail.com; shawntian123@gmail.com; wingng@ieee.org;
   gaoying@scut.edu.cn
RI yang, yuxiang/AAS-5211-2021; TIAN, XING/L-8374-2018
OI yang, yuxiang/0000-0002-4750-7293; TIAN, XING/0000-0002-7546-1018
FU National Natural Science Foundation of China; EPSRC [EP/V002740/2]
   Funding Source: UKRI
FX No Statement Available
CR Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598
   Fang YC, 2021, AAAI CONF ARTIF INTE, V35, P107
   Gao M., 2018, P IEEE C COMP VIS PA, P4118
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LX, 2019, IEEE T IMAGE PROCESS, V28, P791, DOI 10.1109/TIP.2018.2870946
   Heo JP, 2015, IEEE T PATTERN ANAL, V37, P2304, DOI 10.1109/TPAMI.2015.2408363
   Hinton G., 2015, COMPUT SCI, V2
   Huber M, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9667081
   Jang YK, 2019, LECT NOTES COMPUT SC, V11366, P325, DOI 10.1007/978-3-030-20876-9_21
   Jiang Z, 2020, IEEE T MULTIMEDIA, V22, P540, DOI 10.1109/TMM.2019.2929957
   Li WJ, 2016, IJCAI, P1711
   Li YJ, 2017, PROC CVPR IEEE, P5892, DOI 10.1109/CVPR.2017.624
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425
   Lv JJ, 2017, NEUROCOMPUTING, V230, P184, DOI 10.1016/j.neucom.2016.12.025
   Lv YM, 2015, IEEE T MULTIMEDIA, V17, P1225, DOI 10.1109/TMM.2015.2437712
   McLaughlin N, 2017, IEEE T CYBERNETICS, V47, P796, DOI 10.1109/TCYB.2016.2529300
   Ng WWY, 2022, IEEE T CYBERNETICS, V52, P1269, DOI 10.1109/TCYB.2020.3000754
   Osherov E, 2017, IEEE I CONF COMP VIS, P550, DOI 10.1109/ICCV.2017.67
   Ou WH, 2018, PATTERN RECOGN LETT, V107, P41, DOI 10.1016/j.patrec.2017.07.006
   Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278
   Qiu HB, 2022, IEEE T PATTERN ANAL, V44, P6939, DOI 10.1109/TPAMI.2021.3098962
   Romero A., 2015, ICLR, P1, DOI DOI 10.48550/ARXIV.1412.6550
   Saito S, 2016, LECT NOTES COMPUT SC, V9912, P244, DOI 10.1007/978-3-319-46484-8_15
   Song LX, 2019, IEEE I CONF COMP VIS, P773, DOI 10.1109/ICCV.2019.00086
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Tang JH, 2018, PATTERN RECOGN, V75, P25, DOI 10.1016/j.patcog.2017.03.028
   Tian X, 2021, IEEE T MULTIMEDIA, V23, P1210, DOI 10.1109/TMM.2020.2994509
   Trigueros DS, 2018, IMAGE VISION COMPUT, V79, P99, DOI 10.1016/j.imavis.2018.09.011
   Wan WT, 2017, IEEE IMAGE PROC, P3795, DOI 10.1109/ICIP.2017.8296992
   Wang GA, 2020, PROC CVPR IEEE, P6448, DOI 10.1109/CVPR42600.2020.00648
   Wu L, 2019, IEEE ACCESS, V7, P36489, DOI 10.1109/ACCESS.2019.2900489
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2156
   Yang H, 2015, IEEE T IMAGE PROCESS, V24, P2393, DOI 10.1109/TIP.2015.2421438
   Yuan L, 2020, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR42600.2020.00315
   Zeng D, 2021, IET BIOMETRICS, V10, P581, DOI 10.1049/bme2.12029
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhai HJ, 2021, IEEE T CIRC SYST VID, V31, P742, DOI 10.1109/TCSVT.2020.2991171
   Zhang J, 2018, IEEE T MULTIMEDIA, V20, P2400, DOI 10.1109/TMM.2018.2804763
   Zhao F, 2018, IEEE T IMAGE PROCESS, V27, P778, DOI 10.1109/TIP.2017.2771408
NR 41
TC 1
Z9 1
U1 3
U2 3
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 9096
EP 9107
DI 10.1109/TMM.2023.3246238
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DP7K5
UT WOS:001133324200009
DA 2024-07-18
ER

PT J
AU Yu, C
   Wu, Z
   Zhang, DH
   Lu, Z
   Hu, Y
   Chen, Y
AF Yu, Cong
   Wu, Zhi
   Zhang, Dongheng
   Lu, Zhi
   Hu, Yang
   Chen, Yan
TI RFGAN: RF-Based Human Synthesis
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human synthesis; GAN; RF sensing
ID RECOGNITION
AB This paper demonstrates human synthesis based on the Radio Frequency (RF) signals, which leverages the fact that RF signals can record human movements with the signal reflections off the human body. Different from existing RF sensing works that can only perceive humans roughly, this paper aims to generate fine-grained optical human images by introducing a novel cross-modal RFGAN model. Specifically, we first build a radio system equipped with horizontal and vertical antenna arrays to transceive RF signals. Since the reflected RF signals are processed as obscure signal projection heatmaps on the horizontal and vertical planes, we design a RF-Extractor with RNN in RFGAN for RF heatmap encoding and combining to obtain the human activity information. Then we inject the information extracted by the RF-Extractor and RNN as the condition into GAN using the proposed RF-based adaptive normalizations. Finally, we train the whole model in an end-to-end manner. To evaluate our proposed model, we create two cross-modal datasets (RF-Walk & RF-Activity) that contain thousands of optical human activity frames and corresponding RF signals. Experimental results show that the RFGAN can generate target human activity frames using RF signals. To the best of our knowledge, this is the first work to generate optical images based on RF signals.
C1 [Yu, Cong; Wu, Zhi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Peoples R China.
   [Hu, Yang] Univ Sci & Technol China, Sch Informat Sci & Technol, Hefei 230026, Peoples R China.
   [Wu, Zhi; Zhang, Dongheng; Chen, Yan] Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
C3 University of Electronic Science & Technology of China; Chinese Academy
   of Sciences; University of Science & Technology of China, CAS; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Chen, Y (corresponding author), Univ Sci & Technol China, Sch Cyber Sci & Technol, Hefei 230026, Peoples R China.
EM congyu@std.uestc.edu.cn; wzwyyx@mail.ustc.edu.cn; dongheng@ustc.edu.cn;
   zhilu@std.uestc.edu.cn; eeyhu@ustc.edu.cn; eecyan@ustc.edu.cn
RI Yu, Zhou/KBP-8384-2024
OI Zhang, Dongheng/0000-0001-6309-6626; Lu, Zhi/0000-0001-6941-981X; wu,
   zhi/0000-0001-7097-4837
FU National Natural Science Foundation of China [62172381]
FX & nbsp;This work was supported by the National Natural Science
   Foundation of China under Grant 62172381.& nbsp;
CR Adib F, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818072
   Adib F, 2013, ACM SIGCOMM COMP COM, V43, P75, DOI 10.1145/2534169.2486039
   Agethen S, 2020, IEEE T MULTIMEDIA, V22, P819, DOI 10.1109/TMM.2019.2932564
   Bowen Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P7877, DOI 10.1109/CVPR42600.2020.00790
   Chen JB, 2021, Arxiv, DOI arXiv:2112.06639
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Chen Y., 2019, IEEE T MOBILE COMPUT, V19, P2891
   Chen Y, 2021, IEEE INTERNET THINGS, V8, P2762, DOI 10.1109/JIOT.2020.3022071
   Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986
   Chen-Yu Hsu, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130924
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fan L., 2020, P IEEE C COMP VIS PA, P10696, DOI 10.1109/CVPR42600.2020.01071
   Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256
   Ghazalian R, 2021, IEEE T MULTIMEDIA, V23, P823, DOI 10.1109/TMM.2020.2990077
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   He Y, 2020, IEEE INTERNET THINGS, V7, P8296, DOI 10.1109/JIOT.2020.2989426
   Hensel M, 2017, ADV NEUR IN, V30
   Hsu C.-Y., 2019, PROC CHI C HUM FACTO, P1
   Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   Jiang W, 2020, PROCEEDINGS OF THE 2020 INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING (ICONE2020), VOL 3
   Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453
   Kotaru M, 2015, ACM SIGCOMM COMP COM, V45, P269, DOI 10.1145/2829988.2787487
   Li YD, 2022, Arxiv, DOI arXiv:2111.06195
   Liu YH, 2022, IEEE T MULTIMEDIA, V24, P3060, DOI 10.1109/TMM.2021.3092579
   Lu CC, 2017, PROC CVPR IEEE, P2137, DOI 10.1109/CVPR.2017.230
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Niu K, 2022, IEEE T MOBILE COMPUT, V21, P4156, DOI 10.1109/TMC.2021.3063135
   Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244
   Qian K, 2018, ACM T EMBED COMPUT S, V17, DOI 10.1145/3157677
   Qian K, 2017, MOBIHOC'17: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, DOI 10.1145/3084041.3084067
   Rahman T, 2015, PROCEEDINGS OF THE 2015 ACM INTERNATIONAL JOINT CONFERENCE ON PERVASIVE AND UBIQUITOUS COMPUTING (UBICOMP 2015), P39, DOI 10.1145/2750858.2804280
   Reed S, 2016, PR MACH LEARN RES, V48
   Richards M.A., 2005, Fundamentals of Radar Signal Processing, VVolume 1
   Richardson E, 2021, PROC CVPR IEEE, P2287, DOI 10.1109/CVPR46437.2021.00232
   Salimans T, 2016, ADV NEUR IN, V29
   Sengupta A, 2020, IEEE SENS J, V20, P10032, DOI 10.1109/JSEN.2020.2991741
   Shichao Yue, 2018, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V2, DOI 10.1145/3214289
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Si CY, 2018, LECT NOTES COMPUT SC, V11205, P106, DOI 10.1007/978-3-030-01246-5_7
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Tang H, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2052, DOI 10.1145/3343031.3350980
   Ventura C, 2019, PROC CVPR IEEE, P5272, DOI 10.1109/CVPR.2019.00542
   Wang F, 2019, IEEE I CONF COMP VIS, P5451, DOI 10.1109/ICCV.2019.00555
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu CS, 2015, IEEE J SEL AREA COMM, V33, P2329, DOI 10.1109/JSAC.2015.2430294
   Xiang Li, 2017, Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, V1, DOI 10.1145/3130940
   Yu C, 2019, IEEE I CONF COMP VIS, P9045, DOI 10.1109/ICCV.2019.00914
   Zeng YZ, 2016, 2016 15TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN)
   Zhang B.-B., 2021, arXiv
   Zhang D., 2019, IEEE Syst. J., V14, P661
   Zhang DH, 2021, IEEE INTERNET THINGS, V8, P3904, DOI 10.1109/JIOT.2020.3025820
   Zhang DH, 2019, IEEE INTERNET THINGS, V6, P3899, DOI 10.1109/JIOT.2019.2893330
   Zhang DH, 2018, IEEE T VEH TECHNOL, V67, P7101, DOI 10.1109/TVT.2018.2827408
   Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629
   Zhang ML, 2021, IEEE T MULTIMEDIA, V23, P1938, DOI 10.1109/TMM.2020.3006414
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhao M., 2017, ICML, P4100
   Zhao MM, 2019, IEEE I CONF COMP VIS, P10112, DOI 10.1109/ICCV.2019.01021
   Zhao MM, 2018, PROC CVPR IEEE, P7356, DOI 10.1109/CVPR.2018.00768
   Zhao MM, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P267, DOI 10.1145/3230543.3230579
NR 65
TC 4
Z9 4
U1 5
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2926
EP 2938
DI 10.1109/TMM.2022.3153136
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600037
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Zhang, HA
   Liu, LJ
   Kang, BY
   Zheng, NN
AF Zhang, Haonan
   Liu, Longjun
   Kang, Bingyao
   Zheng, Nanning
TI Hierarchical Model Compression via Shape-Edge Representation of Feature
   Maps-an Enlightenment From the Primate Visual System
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Frequency modulation; Shape; Image edge detection; Visualization; Visual
   systems; Streaming media; Feature extraction; Model compression; feature
   maps; shape and edge; primate visual system; deep neural network
ID TOP-DOWN FACILITATION; MECHANISMS; CORTEX; FILTER; BRAIN
AB The cumbersome computation of deep neural networks (DNNs) limits their practical deployment on resource-constrained mobile multimedia devices. To deploy DNNs on devices with limited computing resources, model compression techniques are leveraged to accelerate the networks, where network pruning can improve the inference efficiency of DNNs by removing redundant weights and structures. As one of the important components of DNNs, the feature maps (FMs) can be leveraged to evaluate the importance of network structures for DNN pruning. However, previous methods neglect to fully explore the characteristics of FMs in network pruning. In this paper, we investigate the high capacity and resource efficient analogy-ventral dual-pathway primates visual system (PVS) to propose a hierarchical pruning framework (dubbed as HPSE). In an efficient PVS, the analog pathway analyzes low-frequency information to facilitate the high-frequency information inference in ventral stream. In HPSE, we extract the low-frequency shape information and high-frequency edge information from FMs to present a novel pruning pipeline that resembles the analysis mechanism of PVS. In particular, we first imitate the analogy pathway to group different FMs in each layer by calculating the shape-feature overlap. Secondly, we leverage the edge information modulated by the grouping results of the first step to prune the network. The effectiveness of HPSE is verified by pruning various DNNs on different benchmarks. For example, for ResNet-56 on CIFAR-10, HPSE reduces 52.9% of FLOPs with a slight accuracy improvement; for ResNet-50 on ImageNet, we achieve 54.3%-FLOPs drop with only 0.49% Top-1 accuracy loss.
C1 [Zhang, Haonan; Liu, Longjun; Zheng, Nanning] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Coll Artificial Intelligence, Xian 710049, Peoples R China.
   [Kang, Bingyao] Sichuan Univ, West China Univ Hosp 2, Chengdu, Peoples R China.
C3 Xi'an Jiaotong University; Sichuan University
RP Liu, LJ (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Coll Artificial Intelligence, Xian 710049, Peoples R China.
EM haonanzhang@stu.xjtu.edu.cn; liulongjun@xjtu.edu.cn; kby_168@sina.com;
   nnzheng@xjtu.edu.cn
OI Zhang, Haonan/0000-0002-4239-6141
FU National Basic Strengthen Research Program of ReRAM [2022-00];
   Fundamental Research Funds for the Central Universities [xzy022022064];
   Natural Science Foundation of Shaanxi Province [2022JM-366]; China
   Postdoctoral Science Foundation [227556, 2020T130514]
FX This work was supported in part by the National Basic Strengthen
   Research Program of ReRAM under Grant 2022-00, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   xzy022022064, in part by the Natural Science Foundation of Shaanxi
   Province under Grant 2022JM-366, and in part by the China Postdoctoral
   Science Foundation under Grants 227556 and 2020T130514.
CR Ballester P, 2016, AAAI CONF ARTIF INTE, P1124
   Bar M, 2003, J COGNITIVE NEUROSCI, V15, P600, DOI 10.1162/089892903321662976
   Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103
   Bar M, 2001, NEURON, V29, P529, DOI 10.1016/S0896-6273(01)00224-0
   Bar M, 2007, TRENDS COGN SCI, V11, P280, DOI 10.1016/j.tics.2007.05.005
   Brendel Wieland, 2019, ARXIV190400760
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Denton E, 2014, ADV NEUR IN, V27
   Dosovitskiy Alexey, 2021, 2021 INT C LEARN REP
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Fenske MJ, 2006, PROG BRAIN RES, V155, P3, DOI 10.1016/S0079-6123(06)55001-0
   Gatys L., 2015, NIPS
   Gatys LA, 2017, PROC CVPR IEEE, P3730, DOI 10.1109/CVPR.2017.397
   Gatys LA, 2017, CURR OPIN NEUROBIOL, V46, P178, DOI 10.1016/j.conb.2017.08.019
   Geirhos R, 2019, 7 INT C LEARN REPR I
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234
   He Y, 2020, PROC CVPR IEEE, P2006, DOI 10.1109/CVPR42600.2020.00208
   He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hopfinger JB, 2000, NAT NEUROSCI, V3, P284, DOI 10.1038/72999
   Hosseini H, 2018, IEEE COMPUT SOC CONF, P2004, DOI 10.1109/CVPRW.2018.00258
   Hu HY, 2016, Arxiv, DOI arXiv:1607.03250
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   HUANG TS, 1979, IEEE T ACOUST SPEECH, V27, P13, DOI 10.1109/TASSP.1979.1163188
   Huang ZH, 2018, LECT NOTES COMPUT SC, V11220, P317, DOI 10.1007/978-3-030-01270-0_19
   Kang M, 2020, PR MACH LEARN RES, V119
   KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996
   Komodakis N, 2017, P ICLR
   Kong H, 2013, IEEE T CYBERNETICS, V43, P1719, DOI 10.1109/TSMCB.2012.2228639
   Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896
   Lai YK, 2000, J VIS COMMUN IMAGE R, V11, P17, DOI 10.1006/jvci.1999.0433
   Laws K. I, 1980, Tech. Rep. 940
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li H., 2017, P INT C LEARN REPR I
   Li SJ, 2023, IEEE T MULTIMEDIA, V25, P3180, DOI 10.1109/TMM.2022.3156699
   Li YC, 2019, PROC CVPR IEEE, P2795, DOI 10.1109/CVPR.2019.00291
   Li ZF, 2023, IEEE T MULTIMEDIA, V25, P214, DOI 10.1109/TMM.2021.3124095
   Lin MB, 2020, PROC CVPR IEEE, P1526, DOI 10.1109/CVPR42600.2020.00160
   Lin SH, 2019, PROC CVPR IEEE, P2785, DOI 10.1109/CVPR.2019.00290
   Liu Z., 2019, P INT C LEARN REPR I
   Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Luo JH, 2017, IEEE I CONF COMP VIS, P5068, DOI 10.1109/ICCV.2017.541
   Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8
   MacKay D. J. C., 2003, INFORM THEORY INFERE
   MacQueen J., 1967, P 5 BERK S MATH STAT, P281
   Ople JJM, 2023, IEEE T MULTIMEDIA, V25, P1125, DOI 10.1109/TMM.2021.3139215
   Pascual-Leone A, 2001, SCIENCE, V292, P510, DOI 10.1126/science.1057099
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Ritter S, 2017, PR MACH LEARN RES, V70
   Ruch D.K., 2011, Wavelet theory: an elementary approach with applications
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   SALIN PA, 1995, PHYSIOL REV, V75, P107, DOI 10.1152/physrev.1995.75.1.107
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sharma PK, 2023, IEEE T MULTIMEDIA, V25, P953, DOI 10.1109/TMM.2021.3134158
   Shu YC, 2023, IEEE T MULTIMEDIA, V25, P1700, DOI 10.1109/TMM.2022.3154159
   Suau X, 2020, IEEE WINT CONF APPL, P3129, DOI [10.1109/WACV45572.2020.9093546, 10.1109/wacv45572.2020.9093546]
   Sugase Y, 1999, NATURE, V400, P869, DOI 10.1038/23703
   Tamura H, 2001, CEREB CORTEX, V11, P384, DOI 10.1093/cercor/11.5.384
   Tanaka K, 1996, ANNU REV NEUROSCI, V19, P109, DOI 10.1146/annurev.ne.19.030196.000545
   Tang YH, 2022, PROC CVPR IEEE, P12155, DOI 10.1109/CVPR52688.2022.01185
   Wang D, 2018, Arxiv, DOI arXiv:1803.05729
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang H., 2021, P INT C LEARN REPR I
   Wen W, 2016, ADV NEUR IN, V29
   Wimmer P, 2022, PROC CVPR IEEE, P12517, DOI 10.1109/CVPR52688.2022.01220
   Ye F, 2022, IEEE T MULTIMEDIA, V24, P116, DOI 10.1109/TMM.2020.3046884
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang HM, 2023, IEEE T MULTIMEDIA, V25, P2203, DOI 10.1109/TMM.2022.3144804
   Zhang HN, 2021, PROCEEDINGS OF THE 29TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, MM 2021, P648, DOI 10.1145/3474085.3475228
   Zhang HN, 2022, MACH LEARN, V111, P831, DOI 10.1007/s10994-021-06077-5
   Zhao CL, 2019, PROC CVPR IEEE, P2775, DOI 10.1109/CVPR.2019.00289
   Zhao XM, 2023, IEEE T MULTIMEDIA, V25, P3101, DOI 10.1109/TMM.2022.3155927
   Zhu MJ, 2021, Arxiv, DOI arXiv:2104.08500
NR 79
TC 1
Z9 1
U1 8
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 6958
EP 6970
DI 10.1109/TMM.2022.3216477
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA Y1AC7
UT WOS:001102654000018
DA 2024-07-18
ER

PT J
AU Zhao, XM
   Wu, XM
   Miao, JY
   Chen, WH
   Chen, PCY
   Li, ZG
AF Zhao, Xiaoming
   Wu, Xingming
   Miao, Jinyu
   Chen, Weihai
   Chen, Peter C. Y.
   Li, Zhengguo
TI ALIKE: Accurate and Lightweight Keypoint Detection and Descriptor
   Extraction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Keypoint detection; keypoint descriptor; deep learning; local feature;
   image feature extraction; image matching
ID IMAGES
AB Existing methods detect the keypoints in a non-differentiable way, therefore they can not directly optimize the position of keypoints through back-propagation. To address this issue, we present a partially differentiable keypoint detection module, which outputs accurate sub-pixel keypoints. The reprojection loss is then proposed to directly optimize these sub-pixel keypoints, and the dispersity peak loss is presented for accurate keypoints regularization. We also extract the descriptors in a sub-pixel way, and they are trained with the stable neural reprojection error loss. Moreover, a lightweight network is designed for keypoint detection and descriptor extraction, which can run at 95 frames per second for 640 x 480 images on a commercial GPU. On homography estimation, camera pose estimation, and visual (re-)localization tasks, the proposed method achieves equivalent performance with the state-of-the-art approaches, while greatly reduces the inference time.
C1 [Zhao, Xiaoming; Wu, Xingming; Miao, Jinyu] Beihang Univ, Sch Automat Sci & Elect Engn, Beijing 100191, Peoples R China.
   [Chen, Weihai] Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.
   [Chen, Peter C. Y.] Natl Univ Singapore, Dept Mech Engn, Singapore 119077, Singapore.
   [Li, Zhengguo] Inst Infocomm Res, SRO Dept, Singapore 138632, Singapore.
C3 Beihang University; Anhui University; National University of Singapore;
   Agency for Science Technology & Research (A*STAR); A*STAR - Institute
   for Infocomm Research (I2R)
RP Chen, WH (corresponding author), Anhui Univ, Sch Elect Engn & Automat, Hefei 230601, Peoples R China.; Li, ZG (corresponding author), Inst Infocomm Res, SRO Dept, Singapore 138632, Singapore.
EM xmzhao@buaa.edu.cn; wxmbuaa@163.com; jinyu.miao97@gmail.com;
   whchen@buaa.edu.cn; mpechenp@nus.edu.sg; ezgli@i2r.a-star.edu.sg
RI Chen, Peter/JHU-8943-2023; Chen, Wei-Hai/AAU-3487-2020
OI Li, Zhengguo/0000-0002-4525-1204; Chen, Weihai/0000-0001-7912-4505;
   Miao, Jinyu/0000-0001-8558-9173
FU Key Research and Development Program of Zhejiang Province [2020C01109];
   Key Research and Development Program of Shandong Province
   [2019JZZY011101]; Macao Science and Technology Development Fund
   [0022/2019/AKP]; National Nature Science Foundation of China
   [61620106012, 61573048]; China Scholarship Council [201906020023]
FX This work was supported in part by the Key Research and Development
   Program of Zhejiang Province under Grant 2020C01109, in part by the Key
   Research and Development Program of Shandong Province under Grant
   2019JZZY011101, in part by the Macao Science and Technology Development
   Fund under Grant 0022/2019/AKP, in part by the National Nature Science
   Foundation of China under Grants 61620106012 and 61573048, and in part
   by the China Scholarship Council under Grant 201906020023. The Associate
   Editor coordinating the review of this manuscript and approving it for
   publication was Prof. Engin Erzin.
CR Balntas V., 2016, Bmvc, DOI DOI 10.5244/C.30.119
   Balntas V, 2017, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2017.410
   Barroso-Laguna A, 2019, IEEE I CONF COMP VIS, P5835, DOI 10.1109/ICCV.2019.00593
   Barroso-Laguna Axel, 2020, P AS C COMP VIS
   Bhowmik A, 2020, PROC CVPR IEEE, P4947, DOI 10.1109/CVPR42600.2020.00500
   Chapelle O, 2010, INFORM RETRIEVAL, V13, P216, DOI 10.1007/s10791-009-9110-3
   DeTone D, 2018, IEEE COMPUT SOC CONF, P337, DOI 10.1109/CVPRW.2018.00060
   Droge Greg, 2021, Journal of Control and Decision, V8, P77, DOI 10.1080/23307706.2019.1709989
   Dusmanu M, 2019, PROC CVPR IEEE, P8084, DOI 10.1109/CVPR.2019.00828
   Germain Hugo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P626, DOI 10.1007/978-3-030-58580-8_37
   Germain H, 2021, PROC CVPR IEEE, P414, DOI 10.1109/CVPR46437.2021.00048
   Germain H, 2019, INT CONF 3D VISION, P513, DOI 10.1109/3DV.2019.00063
   Gu K., 2021, P IEEECVF INT C COMP, P11067
   Han XF, 2015, PROC CVPR IEEE, P3279, DOI 10.1109/CVPR.2015.7298948
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He K, 2018, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2018.00069
   Huang CR, 2008, IEEE T MULTIMEDIA, V10, P1097, DOI 10.1109/TMM.2008.2001374
   Jin YH, 2021, INT J COMPUT VISION, V129, P517, DOI 10.1007/s11263-020-01385-0
   Karpushin M, 2016, IEEE T MULTIMEDIA, V18, P1762, DOI 10.1109/TMM.2016.2590305
   Kingma D. P., 2014, arXiv
   Kou F, 2018, IEEE T MULTIMEDIA, V20, P484, DOI 10.1109/TMM.2017.2743988
   Li ZQ, 2018, PROC CVPR IEEE, P2041, DOI 10.1109/CVPR.2018.00218
   Liu X., 2020, PROC ASIAN C COMPUT, P1
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Luo ZX, 2020, PROC CVPR IEEE, P6588, DOI 10.1109/CVPR42600.2020.00662
   Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188
   Mishchuk A., 2018, ADV NEURAL INF PROCE
   Mur-Artal R, 2017, IEEE T ROBOT, V33, P1255, DOI 10.1109/TRO.2017.2705103
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Ono Y, 2018, ADV NEUR IN, V31
   Qianqian Wang, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P757, DOI 10.1007/978-3-030-58452-8_44
   Rana A, 2019, IEEE T MULTIMEDIA, V21, P256, DOI 10.1109/TMM.2018.2839885
   Revaud J., 2019, Advances in Neural Information Processing Systems, P1, DOI DOI 10.48550/ARXIV.1906.06195
   Rublee E, 2011, IEEE I CONF COMP VIS, P2564, DOI 10.1109/ICCV.2011.6126544
   Sattler T, 2018, PROC CVPR IEEE, P8601, DOI 10.1109/CVPR.2018.00897
   Savinov N, 2017, PROC CVPR IEEE, P3929, DOI 10.1109/CVPR.2017.418
   Schönberger JL, 2016, PROC CVPR IEEE, P4104, DOI 10.1109/CVPR.2016.445
   Song YF, 2020, Arxiv, DOI arXiv:2006.05077
   Sun X, 2018, LECT NOTES COMPUT SC, V11210, P536, DOI 10.1007/978-3-030-01231-1_33
   Tian Y., 2020, PROC ASIAN C COMPUT, P1
   Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127
   Tian YR, 2017, PROC CVPR IEEE, P6128, DOI 10.1109/CVPR.2017.649
   Tyszkiewicz M., 2020, Adv. Neural Inf. Process. Syst. (NerulIPS), V33, P14254
   Verdie Y, 2015, PROC CVPR IEEE, P5279, DOI 10.1109/CVPR.2015.7299165
   Yang TY, 2020, Arxiv, DOI arXiv:2001.07252
   Yang X, 2021, IEEE T MULTIMEDIA, V23, P4208, DOI 10.1109/TMM.2020.3038323
   Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28
   Zaffar M, 2021, INT J COMPUT VISION, V129, P2136, DOI 10.1007/s11263-021-01469-5
   Zheng JH, 2013, IEEE T IMAGE PROCESS, V22, P5190, DOI 10.1109/TIP.2013.2283401
NR 49
TC 34
Z9 35
U1 20
U2 45
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 3101
EP 3112
DI 10.1109/TMM.2022.3155927
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA O7RP6
UT WOS:001045742200012
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zheng, ZQ
   Bin, Y
   Lv, XO
   Wu, Y
   Yang, Y
   Shen, HT
AF Zheng, Ziqiang
   Bin, Yi
   Lv, Xiaoou
   Wu, Yang
   Yang, Yang
   Shen, Heng Tao
TI Asynchronous Generative Adversarial Network for Asymmetric Unpaired
   Image-to-Image Translation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Asymmetric image translation; generative adversarial networks; unpaired
   image-to-image translation
AB The unpaired image-to-image translation aims to translate input images from one source domain to some desired outputs in a target domain by learning from unpaired training data. Cycle-consistency constraint provides a general principle to estimate and measure forward and backward mapping functions between two domains. In many cases, the information entropy of images from the two domains is not equal, resulting in an information-rich domain and an information-poor domain. However, existing solutions based on cycle-consistency either completely discard the information asymmetry between the two domains (a common choice), which leads to inferior translation performance for the asymmetric unpaired image-to-image translation, or have to rely on special task-specific designs and introduce extra loss components. These elaborative designs especially for the relatively harder translation direction from the information-poor domain to the information-rich domain ("poor-to-rich" translation) require extra labor and are limited to some specific tasks. In this paper, we propose a novel asynchronous generative adversarial network named Async-GAN, which provides a model-agnostic framework for easily turning symmetrical models into powerful asymmetric counterparts that can handle asymmetric unpaired image-to-image translation much better. The key innovation is to iteratively build gradually-improving intermediate domains for generating pseudo paired training samples, which provide stronger full supervision for assisting the poor-to-rich translation. Extensive experiments on various asymmetric unpaired translation tasks demonstrate the superiority of the proposal. Furthermore, the proposed training framework could be extended to various Cycle-GAN solutions and achieve a performance gain.
C1 [Zheng, Ziqiang; Bin, Yi; Yang, Yang; Shen, Heng Tao] Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 610056, Sichuan, Peoples R China.
   [Lv, Xiaoou] UCL, Dept Stat Sci, London WC1E 6BT, England.
   [Wu, Yang] Tencent PCG, Appl Res Ctr ARC, Shenzhen 518057, Peoples R China.
   [Shen, Heng Tao] Peng Cheng Lab, Shenzhen 518066, Guangdong, Peoples R China.
C3 University of Electronic Science & Technology of China; University of
   London; University College London; Peng Cheng Laboratory
RP Yang, Y (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Chengdu 610056, Sichuan, Peoples R China.; Wu, Y (corresponding author), Tencent PCG, Appl Res Ctr ARC, Shenzhen 518057, Peoples R China.
EM zhengziqiang1@gmail.com; yi.bin@hotmail.com; xiaoou_lu@outlook.com;
   dylanywu@tencent.com; dlyyang@gmail.com; shenhengtao@hotmail.com
RI Shen, Heng Tao/ABD-5331-2021
FU National Natural Science Foundation of China [U20B2063, 62102070];
   Dongguan Songshan Lake Introduction Program of Leading Innovative and
   Entrepreneurial Talents
FX & nbsp;This work was supported in part by the National Natural Science
   Foundation of China under Grants U20B2063 and 62102070, and in part by
   the Dongguan Songshan Lake Introduction Program of Leading Innovative
   and Entrepreneurial Talents. The Associate Editor coordinating the
   review of this manuscript and approving it for publication was Dr. Chang
   Xu. (Ziqiang Zheng and Yi Bin are contributed equally to this work).&
   nbsp;
CR Anirudh R, 2018, Arxiv, DOI arXiv:1805.07281
   [Anonymous], 2017, Advances in neural information processing systems
   Chang HW, 2018, PROC CVPR IEEE, P40, DOI 10.1109/CVPR.2018.00012
   Chen L, 2019, IEEE T MULTIMEDIA, V21, P2664, DOI 10.1109/TMM.2019.2907052
   Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   de Bezenac E., 2019, ICLR
   Dou H, 2019, INT CONF ACOUST SPEE, P1757, DOI [10.1109/icassp.2019.8682600, 10.1109/ICASSP.2019.8682600]
   Emami H, 2021, IEEE T MULTIMEDIA, V23, P391, DOI 10.1109/TMM.2020.2975961
   Fu H, 2019, PROC CVPR IEEE, P2422, DOI [10.1109/CVPR.2019.00253, 10.1109/cvpr.2019.00253]
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Hensel M, 2017, ADV NEUR IN, V30
   Hore Alain, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2366, DOI 10.1109/ICPR.2010.579
   Huang JL, 2021, IEEE T MULTIMEDIA, V23, P1654, DOI 10.1109/TMM.2020.3001536
   Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11
   Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632
   JiweiWei Yang Yang, 2021, IEEE Transactions on Pattern Analysis and Machine Intelligence
   Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Karras T, 2021, IEEE T PATTERN ANAL, V43, P4217, DOI 10.1109/TPAMI.2020.2970919
   Kim J., 2020, ICLR, P1
   Kupyn O, 2018, PROC CVPR IEEE, P8183, DOI 10.1109/CVPR.2018.00854
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3
   Li N, 2018, IEEE ACCESS, V6, P54241, DOI 10.1109/ACCESS.2018.2870854
   Li Y, 2019, IEEE T IMAGE PROCESS, V28, P5881, DOI 10.1109/TIP.2019.2922854
   Liu M, 2019, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2019.00379
   Mirza M, 2014, Arxiv, DOI [arXiv:1411.1784, DOI 10.48550/ARXIV.1411.1784]
   Murez Z, 2018, PROC CVPR IEEE, P4500, DOI 10.1109/CVPR.2018.00473
   Ran Yi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8214, DOI 10.1109/CVPR42600.2020.00824
   Runtao Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P36, DOI 10.1007/978-3-030-58580-8_3
   Sakaridis C, 2018, LECT NOTES COMPUT SC, V11217, P707, DOI 10.1007/978-3-030-01261-8_42
   Shannon C. E., 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Shen C., 2021, CVPR, P3350
   Shen HT, 2021, IEEE T KNOWL DATA EN, V33, P3351, DOI [10.1109/TKDE.2020.2970050, 10.1109/TNNLS.2020.2995708]
   Tang H., 2018, P AS C COMP VIS ACCV, P3
   Tsai YH, 2019, IEEE I CONF COMP VIS, P1456, DOI 10.1109/ICCV.2019.00154
   Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang C, 2018, P EUR C COMP VIS ECC, P770
   Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917
   Xu X, 2020, IEEE T CYBERNETICS, V50, P2400, DOI 10.1109/TCYB.2019.2928180
   Yang Dingdong, 2019, INT C LEARN REPR
   Yang Y, 2018, IEEE T IMAGE PROCESS, V27, P5600, DOI 10.1109/TIP.2018.2855422
   Yi R, 2019, PROC CVPR IEEE, P10735, DOI 10.1109/CVPR.2019.01100
   Yu F, 2020, PROC CVPR IEEE, P2633, DOI 10.1109/CVPR42600.2020.00271
   Yunjey Choi, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P8185, DOI 10.1109/CVPR42600.2020.00821
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40
   Zheng ZQ, 2022, IEEE T MULTIMEDIA, V24, P480, DOI 10.1109/TMM.2021.3053775
   Zheng ZQ, 2019, NEUROCOMPUTING, V355, P71, DOI 10.1016/j.neucom.2019.04.032
   Zhou YF, 2019, IEEE T MULTIMEDIA, V21, P3136, DOI 10.1109/TMM.2019.2920613
   Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244
   Ziqiang Zheng, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12348), P155, DOI 10.1007/978-3-030-58580-8_10
   Zou Y., 2021, PROC INT C LEARN REP
NR 55
TC 17
Z9 17
U1 6
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2023
VL 25
BP 2474
EP 2487
DI 10.1109/TMM.2022.3147425
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA M5LW1
UT WOS:001030640600002
DA 2024-07-18
ER

PT J
AU Cheng, YP
   Guo, Q
   Juefei, F
   Lin, SW
   Feng, W
   Lin, WS
   Liu, Y
AF Cheng, Yupeng
   Guo, Qing
   Juefei-Xu, Felix
   Lin, Shang-Wei
   Feng, Wei
   Lin, Weisi
   Liu, Yang
TI <i>Pasadena:</i> Perceptually Aware and Stealthy Adversarial Denoise
   Attack
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Noise reduction; Kernel; Task analysis; Image denoising; Image quality;
   Noise measurement; Deep learning; Adversarial denoise attack; image
   denoising; image classification; adversarial attack
ID IMAGE QUALITY ASSESSMENT; EDGE; CNN
AB Image denoising can remove natural noise that widely exists in images captured by multimedia devices due to low-quality imaging sensors, unstable image transmission processes, or low light conditions. Recent works also find that image denoising benefits the high-level vision tasks, e.g., image classification. In this work, we try to challenge this common sense and explore a totally new problem, i.e., whether the image denoising can be given the capability of fooling the state-of-the-art deep neural networks (DNNs) while enhancing the image quality. To this end, we initiate the very first attempt to study this problem from the perspective of adversarial attack and propose the adversarial denoise attack. More specifically, our main contributions are three-fold: First, we identify a new task that stealthily embeds attacks inside the image denoising module widely deployed in multimedia devices as an image post-processing operation to simultaneously enhance the visual image quality and fool DNNs. Second, we formulate this new task as a kernel prediction problem for image filtering and propose the adversarial-denoising kernel prediction that can produce adversarial-noiseless kernels for effective denoising and adversarial attacking simultaneously. Third, we implement an adaptive perceptual region localization to identify semantic-related vulnerability regions with which the attack can be more effective while not doing too much harm to the denoising. We name the proposed method as Pasadena (Perceptually Aware and Stealthy Adversarial DENoise Attack) and validate our method on the NeurIPS'17 adversarial competition dataset, CVPR2021-AIC-VI: unrestricted adversarial attacks on ImageNet, and Tiny-ImageNet-C dataset. The comprehensive evaluation and analysis demonstrate that our method not only realizes denoising but also achieves a significantly higher success rate and transferability over state-of-the-art attacks.
C1 [Cheng, Yupeng; Guo, Qing; Lin, Shang-Wei; Lin, Weisi; Liu, Yang] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Guo, Qing; Feng, Wei] Tianjin Univ, Sch Comp Sci & Technol, Coll Intelligence & Comp, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300305, Peoples R China.
   [Guo, Qing; Feng, Wei] State Adm Cultural Heritage, Key Res Ctr Surface Monitoring & Anal Cultural Re, Beijing 100009, Peoples R China.
   [Juefei-Xu, Felix] Alibaba Grp, Sunnyvale, CA 94085 USA.
C3 Nanyang Technological University; Tianjin University; Alibaba Group
RP Guo, Q (corresponding author), Nanyang Technol Univ, Singapore 639798, Singapore.; Guo, Q (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Coll Intelligence & Comp, Tianjin Key Lab Cognit Comp & Applicat, Tianjin 300305, Peoples R China.
EM ycheng024@e.ntu.edu.sg; tsingqguo@ieee.org; juefei.xu@gmail.com;
   shang-wei.lin@ntu.edu.sg; wfeng@ieee.org; wslin@ntu.edu.sg;
   yangliu@ntu.edu.sg
RI Feng, Wei/E-3985-2016; Guo, Qing/KGM-8687-2024; Guo, Qing/J-9661-2016;
   Liu, Yang/D-2306-2013; Lin, Wei/D-3353-2012; Lin, Weisi/A-3696-2011
OI Guo, Qing/0000-0003-0974-9299; Liu, Yang/0000-0001-7300-9215; Lin,
   Shang-Wei/0000-0002-9726-3434; Lin, Weisi/0000-0001-9866-1947
FU Natural Science Foundation of Tianjin [20JCQNJC00720]; National Research
   Foundation, Singapore under its the AI Singapore Programme
   [AISG2-RP-2020-019]; National Research Foundation, Prime Ministers
   Office, Singapore under its National Cybersecurity RD Program
   [NRF2018NCR-NCR005-0001]; NRF [NRFI06-2020-0001]; National Research
   Foundation through its National Satellite of Excellence in Trustworthy
   Software Systems (NSOE-TSS) Project under the National Cybersecurity RD
   (NCR) [NRF2018NCR-NSOE003-0001]; Ministry of Education, Singapore, under
   its Academic Tier-2 Research Fund [MOE2018-T2-1-068]
FX This work was supported in part by the Natural Science Foundation of
   Tianjin underGrant 20JCQNJC00720, in part by theNational Research
   Foundation, Singapore under its the AI Singapore Programme
   (AISG2-RP-2020-019), in part by the National Research Foundation,
   PrimeMinisters Office, Singapore under its National Cybersecurity R&D
   Program (No. NRF2018NCR-NCR0050001), NRF Investigatorship
   NRFI06-2020-0001, in part by the National Research Foundation through
   its National Satellite of Excellence in Trustworthy Software Systems
   (NSOE-TSS) Project under the National Cybersecurity R&D (NCR) under
   Grant NRF2018NCR-NSOE003-0001, in part by the Ministry of Education,
   Singapore, under its Academic Tier-2 Research Fund under Grant
   MOE2018-T2-1-068.
CR Ackerman E, 2017, IEEE SPECTR MAG, V1
   [Anonymous], 2021, ADVERSARIAL ATTACKS
   Bako S, 2017, ACM T GRAPHIC, V36, DOI [10.1145/3072959.3073703, 10.1145/3072959.3073708]
   Bengio S, 2016, ARXIV
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chen MJ, 2011, IEEE T MULTIMEDIA, V13, P1195, DOI 10.1109/TMM.2011.2166538
   Chen XY, 2016, PROC SPIE, V9971, DOI 10.1117/12.2239260
   Cho SI, 2019, IEEE T MULTIMEDIA, V21, P484, DOI 10.1109/TMM.2018.2859791
   Cho SI, 2018, IEEE T MULTIMEDIA, V20, P1738, DOI 10.1109/TMM.2017.2781371
   Cong RM, 2019, IEEE T MULTIMEDIA, V21, P1660, DOI 10.1109/TMM.2018.2884481
   Croce F, 2019, IEEE I CONF COMP VIS, P4723, DOI 10.1109/ICCV.2019.00482
   Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238
   Ding CX, 2015, IEEE T MULTIMEDIA, V17, P2049, DOI 10.1109/TMM.2015.2477042
   Dolhansky B, 2020, Arxiv, DOI arXiv:2006.07397
   Dong L, 2018, IEEE T MULTIMEDIA, V20, P2012, DOI 10.1109/TMM.2017.2788205
   Dong YP, 2020, PROC CVPR IEEE, P318, DOI 10.1109/CVPR42600.2020.00040
   Dong YP, 2019, PROC CVPR IEEE, P4307, DOI 10.1109/CVPR.2019.00444
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Fu L, 2021, PROC CVPR IEEE, P10566, DOI 10.1109/CVPR46437.2021.01043
   Gao RJ, 2021, Arxiv, DOI arXiv:2104.13673
   Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399
   Ghosh S, 2019, IEEE IMAGE PROC, P205, DOI [10.1109/ICIP.2019.8802986, 10.1109/icip.2019.8802986]
   Ghosh S, 2017, J ELECTRON IMAGING, V26, DOI 10.1117/1.JEI.26.6.063012
   Ghosh S, 2017, IET IMAGE PROCESS, V11, P317, DOI 10.1049/iet-ipr.2016.0331
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Guo Q., 2020, P ADV NEUR INF PROC, V33, P975
   Guo Q., 2021, PROC IEEE INT C COMP
   Guo Q., 2020, P ECCV, V12370, P202
   Guo Q, 2016, IEEE T CIRC SYST VID, V26, P868, DOI 10.1109/TCSVT.2015.2416631
   Guo Q, 2021, AAAI CONF ARTIF INTE, V35, P1487
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   He JZ, 2019, PROC CVPR IEEE, P3823, DOI 10.1109/CVPR.2019.00395
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Heide F, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925875
   Heide F, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2661229.2661260
   Hendrycks Dan, 2019, ARXIV190312261
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jiang JJ, 2017, IEEE T MULTIMEDIA, V19, P27, DOI 10.1109/TMM.2016.2601020
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification
   Kurakin A, 2018, SPRING SER CHALLENGE, P195, DOI 10.1007/978-3-319-94042-7_11
   Lian CF, 2022, IEEE T CYBERNETICS, V52, P1992, DOI 10.1109/TCYB.2020.3005859
   Liu D, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P842
   Luisier F, 2010, IEEE T CIRC SYST VID, V20, P913, DOI 10.1109/TCSVT.2010.2045819
   Maggioni M, 2011, PROC SPIE, V7870, DOI 10.1117/12.872569
   Malladi SRSP, 2021, IEEE T MULTIMEDIA, V23, P2297, DOI 10.1109/TMM.2020.3009502
   Mildenhall B, 2018, PROC CVPR IEEE, P2502, DOI 10.1109/CVPR.2018.00265
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Najafabadi MM, 2015, Journal of big data, V2, P1, DOI [10.1186/s40537-014-0007-7, DOI 10.1186/S40537-014-0007-7]
   Papernot N, 2018, 2018 3RD IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY (EUROS&P 2018), P399, DOI 10.1109/EuroSP.2018.00035
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205
   Rahman SMM, 2007, IEEE T CIRC SYST VID, V17, P187, DOI 10.1109/TCSVT.2006.887079
   Rao Q, 2018, PROCEEDINGS 2018 IEEE/ACM 1ST INTERNATIONAL WORKSHOP ON SOFTWARE ENGINEERING FOR AI IN AUTONOMOUS SYSTEMS (SEFAIAS), P35, DOI 10.1145/3194085.3194087
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Su SC, 2017, PROC CVPR IEEE, P237, DOI 10.1109/CVPR.2017.33
   Szegedy C, 2014, 2 INT C LEARN REPRES
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tao X, 2017, IEEE I CONF COMP VIS, P4482, DOI 10.1109/ICCV.2017.479
   Tian B, 2021, P 30 INT JOINT C ART, P1046, DOI [10.24963/ijcai.2021/145, DOI 10.24963/IJCAI.2021/145]
   Tian CW, 2020, NEURAL NETWORKS, V131, P251, DOI 10.1016/j.neunet.2020.07.025
   Wang R., 2020, PROC ACM INT C MULTI, P1376
   Wang R, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3444
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wong E., 2020, INT C LEARN REPR
   Xie CH, 2019, PROC CVPR IEEE, P2725, DOI 10.1109/CVPR.2019.00284
   Zhai L., 2020, arXiv
   Zhang CJ, 2019, IEEE T MULTIMEDIA, V21, P2482, DOI 10.1109/TMM.2019.2903628
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
NR 76
TC 8
Z9 8
U1 4
U2 26
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3807
EP 3822
DI 10.1109/TMM.2021.3108009
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400011
DA 2024-07-18
ER

PT J
AU Dai, PW
   Li, Y
   Zhang, H
   Li, JZ
   Cao, XC
AF Dai, Pengwen
   Li, Yang
   Zhang, Hua
   Li, Jingzhi
   Cao, Xiaochun
TI Accurate Scene Text Detection Via Scale-Aware Data Augmentation and
   Shape Similarity Constraint
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Shape; Image segmentation; Agriculture; Proposals; Location
   awareness; Training data; Scene text detection; arbitrary shape; text
   part; global context; data augmentation; accurate localization
AB Scene text detection has attracted increasing concerns with the rapid development of deep neural networks in recent years. However, existing scene text detectors may overfit on the public datasets due to the limited training data, or generate inaccurate localization for arbitrary-shape scene texts. This paper presents an arbitrary-shape scene text detection method that can achieve better generalization ability and more accurate localization. We first propose a Scale-Aware Data Augmentation (SADA) technique to increase the diversity of training samples. SADA considers the scale variations and local visual variations of scene texts, which can effectively relieve the dilemma of limited training data. At the same time, SADA can enrich the training minibatch, which contributes to accelerating the training process. Furthermore, a Shape Similarity Constraint (SSC) technique is exploited to model the global shape structure of arbitrary-shape scene texts and backgrounds from the perspective of the loss function. SSC encourages the segmentation of text or non-text in the candidate boxes to be similar to the corresponding ground truth, which is helpful to localize more accurate boundaries for arbitrary-shape scene texts. Extensive experiments have demonstrated the effectiveness of the proposed techniques, and state-of-the-art performances are achieved over public arbitrary-shape scene text benchmarks (e.g., CTW1500, Total-Text and ArT).
C1 [Dai, Pengwen; Zhang, Hua; Li, Jingzhi; Cao, Xiaochun] Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100195, Peoples R China.
   [Dai, Pengwen; Zhang, Hua; Li, Jingzhi; Cao, Xiaochun] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Li, Yang] Delft Univ Technol, NL-2628 Delft, Netherlands.
   [Cao, Xiaochun] Cyberspace Secur Res Ctr, Peng Cheng Lab, Shenzhen 518055, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Delft University of Technology; Peng Cheng Laboratory
RP Cao, XC (corresponding author), Chinese Acad Sci, Inst Informat Engn, State Key Lab Informat Secur, Beijing 100195, Peoples R China.; Cao, XC (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM daipengwen@iie.ac.cn; Y.Li-31@tudelft.nl; zhanghua@iie.ac.cn;
   lijingzhi@iie.ac.cn; caoxiaochun@iie.ac.cn
RI Dai, Pengwen/HQZ-2709-2023
OI Dai, Pengwen/0000-0002-6262-982X
FU National Key R and D Program of China [2020YFB1406704]; National Natural
   Science Foundation of China [62025604, 61733007, 62072454, U1936208,
   U1736219]; Key Program of the Chinese Academy of Sciences
   [QYZDB-SSW-JSC003]; Beijing Natural Science Foundation [4202084]; Peng
   ChengLaboratory Project of Guangdong Province [PCL2018KP004]
FX This work was supported in part by the National Key R and D Program of
   China under Grant 2020YFB1406704, in part by the National Natural
   Science Foundation of China under Grants 62025604, 61733007, 62072454,
   U1936208, and U1736219, in part by the Key Program of the Chinese
   Academy of Sciences under Grant QYZDB-SSW-JSC003, in part by Beijing
   Natural Science Foundation underGrant 4202084, and in part by the Peng
   ChengLaboratory Project of Guangdong Province under Grant PCL2018KP004.
CR Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Biten AF, 2019, IEEE I CONF COMP VIS, P4290, DOI 10.1109/ICCV.2019.00439
   Ch'ng CK, 2017, PROC INT CONF DOC, P935, DOI 10.1109/ICDAR.2017.157
   Chee Kheng Chng, 2019, 2019 International Conference on Document Analysis and Recognition (ICDAR). Proceedings, P1571, DOI 10.1109/ICDAR.2019.00252
   Cheng PR, 2020, IEEE T CIRC SYST VID, V30, P4171, DOI 10.1109/TCSVT.2019.2947475
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89
   Dai PW, 2020, IEEE T MULTIMEDIA, V22, P1969, DOI 10.1109/TMM.2019.2952978
   Dai YC, 2018, INT C PATT RECOG, P3604, DOI 10.1109/ICPR.2018.8546066
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fang HS, 2019, IEEE I CONF COMP VIS, P682, DOI 10.1109/ICCV.2019.00077
   Gomez R, 2017, PROC INT CONF DOC, P1435, DOI 10.1109/ICDAR.2017.234
   Gupta A, 2016, PROC CVPR IEEE, P2315, DOI 10.1109/CVPR.2016.254
   He Kaiming, 2017, P IEEE INT C COMPUTE
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He WH, 2017, IEEE I CONF COMP VIS, P745, DOI 10.1109/ICCV.2017.87
   Hong ZY, 2019, IEEE I CONF COMP VIS, P2861, DOI 10.1109/ICCV.2019.00295
   Hu H, 2017, IEEE I CONF COMP VIS, P4950, DOI 10.1109/ICCV.2017.529
   Jaderberg Max, 2014, WORKSH DEEP LEARN NI
   Karaoglu S, 2017, IEEE T MULTIMEDIA, V19, P1063, DOI 10.1109/TMM.2016.2638622
   Karatzas D, 2015, PROC INT CONF DOC, P1156, DOI 10.1109/ICDAR.2015.7333942
   Krasin I., 2017, Openimages: A public dataset for large-scale multi-label and multi-class image classification
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu YL, 2020, IEEE T IMAGE PROCESS, V29, P2918, DOI 10.1109/TIP.2019.2954218
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu ZD, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3356728
   Long SB, 2021, INT J COMPUT VISION, V129, DOI 10.1007/s11263-020-01369-0
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lorensen William E., 1987, COMPUT GRAPH, P163, DOI DOI 10.1145/37402.37422
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Qin XB, 2019, PROC CVPR IEEE, P7471, DOI 10.1109/CVPR.2019.00766
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren XH, 2017, IEEE T MULTIMEDIA, V19, P506, DOI 10.1109/TMM.2016.2625259
   Shi BG, 2017, PROC INT CONF DOC, P1429, DOI 10.1109/ICDAR.2017.233
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Singh B., 2018, Advances in Neural Information Processing Systems, V31, P9310
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tang YB, 2018, IEEE T MULTIMEDIA, V20, P2276, DOI 10.1109/TMM.2018.2802644
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Tian ZT, 2019, PROC CVPR IEEE, P4229, DOI 10.1109/CVPR.2019.00436
   Wang FF, 2018, PROC CVPR IEEE, P1381, DOI 10.1109/CVPR.2018.00150
   Wang PF, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1277, DOI 10.1145/3343031.3350988
   Wang WH, 2019, IEEE I CONF COMP VIS, P8439, DOI 10.1109/ICCV.2019.00853
   Wang WH, 2019, PROC CVPR IEEE, P9328, DOI 10.1109/CVPR.2019.00956
   Wang XB, 2019, PROC CVPR IEEE, P6442, DOI 10.1109/CVPR.2019.00661
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Xie EZ, 2019, AAAI CONF ARTIF INTE, P9038
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Zhan FN, 2018, LECT NOTES COMPUT SC, V11212, P257, DOI 10.1007/978-3-030-01237-3_16
   Zhan FN, 2019, IEEE I CONF COMP VIS, P9104, DOI 10.1109/ICCV.2019.00920
   Zhan FN, 2019, PROC CVPR IEEE, P3648, DOI 10.1109/CVPR.2019.00377
   Zhang C, 2019, IEEE I CONF COMP VIS, P9586, DOI 10.1109/ICCV.2019.00968
   Zhang S, 2018, AAAI CONF ARTIF INTE, P2612
   Zhong Z, 2020, AAAI CONF ARTIF INTE, V34, P13001
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
NR 63
TC 24
Z9 24
U1 1
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1883
EP 1895
DI 10.1109/TMM.2021.3073575
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0H8DA
UT WOS:000778959200009
DA 2024-07-18
ER

PT J
AU Deng, Y
   Xiao, JM
   Zhou, SZ
AF Deng, Yong
   Xiao, Jimin
   Zhou, Steven Zhiying
TI ToF and Stereo Data Fusion Using Dynamic Search Range Stereo Matching
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Estimation; Reliability; Cameras; Data integration; Feature extraction;
   Task analysis; Probabilistic logic; Time-of-flight; stereo matching;
   fusion; neural network
ID TIME-OF-FLIGHT; DEPTH
AB Time-of-Flight (ToF) sensors and stereo vision systems are both widely used for capturing depth data. They have some complementary strengths and limitations, which have been exploited in prior research to produce more accurate depth maps by fusing data from the two sources. However, among these diverse data fusion approaches, none of them provides an end-to-end neural network solution. In this work, we propose the first end-to-end ToF and stereo data fusion network using the coarse-to-fine matching framework, where the prior of ToF depth is integrated into the stereo matching process by constraining the search range of stereo matching within an interval around the ToF camera depth measurement. We adopt a dynamic search range for each pixel according to an estimated ToF error map, which is more efficient and effective than a constant one when handling various errors. The ToF error map is estimated by the ToF error estimator branching out from the stereo matching network. Both ToF error estimation and stereo matching are performed in a joint framework, with the two tasks assisting each other mutually. We also propose an upsampling module to replace the naive bilinear upsampling in the coarse-to-fine stereo matching network, which reduces the error caused by the upsampling. The proposed deep network is trained end-to-end on synthetic datasets and generalizable to real-world datasets without further fine-tuning. Experimental results show that our fusion method achieves higher accuracy than either ToF or stereo alone, and outperforms state-of-the-art fusion methods on both synthetic and real data.
C1 [Deng, Yong; Zhou, Steven Zhiying] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Xiao, Jimin] Xian Jiaotong Liverpool Univ, Dept Elect & Elect Engn, Suzhou 215123, Jiangsu, Peoples R China.
   [Zhou, Steven Zhiying] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
   [Zhou, Steven Zhiying] Natl Univ Singapore, Suzhou Res Inst, Suzhou 215123, Jiangsu, Peoples R China.
C3 National University of Singapore; Xi'an Jiaotong-Liverpool University;
   National University of Singapore; National University of Singapore
RP Zhou, SZ (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.; Zhou, SZ (corresponding author), Natl Univ Singapore, Suzhou Res Inst, Suzhou 215123, Jiangsu, Peoples R China.
EM dengyong@u.nus.edu; jimin.xiao@xjtlu.edu.cn; elezzy@nus.edu.sg
OI Deng, Yong/0000-0003-0987-2182
FU National Key Research and Development Program of China [2018YFB1004904];
   Science and Technology Program of Suzhou City [SYG201920]; National
   Natural Science Foundation of China [61972323]; Key Program Special Fund
   in XJTLU [KSF-T-02, KSF-P-02]
FX This work was supported in part by National Key Research and Development
   Program of China under Grant 2018YFB1004904, in part by Science and
   Technology Program of Suzhou City under Grant SYG201920, in part by
   National Natural Science Foundation of China under Grant 61972323, and
   in part by the Key Program Special Fund in XJTLU under Grant KSF-T-02,
   KSF-P-02.
CR Agresti G., 2018, P EUR C COMP VIS ECC
   Agresti G, 2019, INFORM FUSION, V49, P161, DOI 10.1016/j.inffus.2018.11.006
   Agresti G, 2017, IEEE INT CONF COMP V, P697, DOI 10.1109/ICCVW.2017.88
   Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   [Anonymous], 2012, SPRINGER SCI BUSINES
   [Anonymous], 2016, TOF range-imaging cameras
   [Anonymous], 2010, PROC 3DPVT
   Beder C, 2007, LECT NOTES COMPUT SC, V4713, P11
   Chen BL, 2018, IEEE T MULTIMEDIA, V20, P2882, DOI 10.1109/TMM.2018.2825883
   Chen BL, 2017, IEEE IMAGE PROC, P1437, DOI 10.1109/ICIP.2017.8296519
   Dal Mutto C, 2015, IEEE T PATTERN ANAL, V37, P2260, DOI 10.1109/TPAMI.2015.2408361
   Dal Mutto C, 2012, LECT NOTES COMPUT SC, V7583, P598, DOI 10.1007/978-3-642-33863-2_62
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Evangelidis GD, 2015, IEEE T PATTERN ANAL, V37, P2178, DOI 10.1109/TPAMI.2015.2400465
   Freedman D, 2014, LECT NOTES COMPUT SC, V8689, P234, DOI 10.1007/978-3-319-10590-1_16
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guomundsson Sigurjon Arni, 2008, International Journal of Intelligent Systems Technologies and Applications, V5, P425, DOI 10.1504/IJISTA.2008.021305
   Hahne U, 2009, LECT NOTES COMPUT SC, V5742, P70, DOI 10.1007/978-3-642-03778-8_6
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hirschmüller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI 10.1109/TPAMl.2007.1166
   Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46
   Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17
   Kingma D. P., 2014, arXiv
   Kuhnert KD, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P4780, DOI 10.1109/IROS.2006.282349
   Lee Z, 2015, IEEE T MULTIMEDIA, V17, P792, DOI 10.1109/TMM.2015.2425141
   Loop C., 1999, Comput. Vision Pattern Recognit, V1, P1125, DOI [10.1109/CVPR.1999.786928, DOI 10.1109/CVPR.1999.786928]
   Luvizon DC, 2018, PROC CVPR IEEE, P5137, DOI 10.1109/CVPR.2018.00539
   Marin G, 2016, LECT NOTES COMPUT SC, V9911, P386, DOI 10.1007/978-3-319-46478-7_24
   Mattoccia Stefano, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1763, DOI 10.1109/ICCVW.2009.5457496
   Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438
   Meister S., 2013, VMV, P33
   Nair Rahul, 2013, Time-Of-Flight and Depth Imaging. Sensors, Algorithms and Applications. Dagstuhl 2012 Seminar on Time-of-Flight Imaging and GCPR 2013 Workshop on Imaging New Modalities: LNCS 8200, P105, DOI 10.1007/978-3-642-44964-2_6
   Nair R, 2012, LECT NOTES COMPUT SC, V7584, P1, DOI 10.1007/978-3-642-33868-7_1
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Poggi M, 2017, IEEE I CONF COMP VIS, P5238, DOI 10.1109/ICCV.2017.559
   Pu C, 2019, IEEE IMAGE PROC, P1765, DOI [10.1109/ICIP.2019.8803180, 10.1109/icip.2019.8803180]
   Qingxiong Yang, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P69, DOI 10.1109/MMSP.2010.5661996
   Quam L.H., 1987, Readings in computer vision: issues, problems, principles, and paradigms, P80
   Song Y, 2011, LECT NOTES COMPUT SC, V6688, P467, DOI 10.1007/978-3-642-21227-7_44
   Tonioni A, 2019, PROC CVPR IEEE, P195, DOI 10.1109/CVPR.2019.00028
   Van Meerbergen G, 2002, INT J COMPUT VISION, V47, P275, DOI 10.1023/A:1014562312225
   Wang CQ, 2017, IEEE INT C INT ROBOT, P109, DOI 10.1109/IROS.2017.8202145
   Wu HY, 2007, IEEE I CONF COMP VIS, P628, DOI 10.1109/cvpr.2007.383211
   Yin ZC, 2019, PROC CVPR IEEE, P6037, DOI 10.1109/CVPR.2019.00620
   Yu F, 2018, PROC CVPR IEEE, P2403, DOI 10.1109/CVPR.2018.00255
   Zanuttigh P, 2016, TECHNOL APPL, P100
   Zhang L, 2003, PROC CVPR IEEE, P367
   Zhu JJ, 2008, PROC CVPR IEEE, P3262
   Zhu JJ, 2011, IEEE T PATTERN ANAL, V33, P1400, DOI 10.1109/TPAMI.2010.172
   Zhu JJ, 2010, IEEE T PATTERN ANAL, V32, P899, DOI 10.1109/TPAMI.2009.68
NR 50
TC 5
Z9 5
U1 2
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2739
EP 2751
DI 10.1109/TMM.2021.3087017
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 2A3LX
UT WOS:000809408000004
DA 2024-07-18
ER

PT J
AU Gao, L
   Guan, L
AF Gao, Lei
   Guan, Ling
TI A Discriminative Vectorial Framework for Multi-Modal Feature
   Representation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Correlation; Task analysis; Emotion recognition;
   Visualization; Transforms; Image recognition; Audio emotion recognition;
   cross-modal analysis; discriminative correlation maximization; image
   analysis and recognition; knowledge discovery; multi-modal feature
   representation; multi-modal hashing
ID CANONICAL CORRELATION-ANALYSIS; SPARSE REPRESENTATION; KNOWLEDGE
   DISCOVERY; FACE RECOGNITION; FEATURE-EXTRACTION; FEATURE FUSION; DEEP;
   DESCRIPTOR; PROJECTION; WORDS
AB Due to the rapid advancements of sensory and computing technology, multi-modal data sources that represent the same pattern or phenomenon have attracted growing attention. As a result, finding means to explore useful information from these multi-modal data sources has quickly become a necessity. In this paper, a discriminative vectorial framework is proposed for multi-modal feature representation in knowledge discovery by employing multi-modal hashing (MH) and discriminative correlation maximization (DCM) analysis. Specifically, the proposed framework is capable of minimizing the semantic similarity among different modalities by MH and exacting intrinsic discriminative representations across multiple data sources by DCM analysis jointly, enabling a novel vectorial framework of multi-modal feature representation. Moreover, the proposed feature representation strategy is analyzed and further optimized based on canonical and non-canonical cases, respectively. Consequently, the generated feature representation leads to effective utilization of the input data sources of high quality, producing improved, sometimes quite impressive, results in various applications. The effectiveness and generality of the proposed framework are demonstrated by utilizing classical features and deep neural network (DNN) based features with applications to image and multimedia analysis and recognition tasks, including data visualization, face recognition, object recognition; cross-modal (text-image) recognition and audio emotion recognition. Experimental results show that the proposed solutions are superior to state-of-the-art statistical machine learning (SML) and DNN algorithms.
C1 [Gao, Lei; Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Toronto Metropolitan University
RP Gao, L (corresponding author), Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
EM iegaolei@gmail.com; lguan@ee.ryerson.ca
OI Gao, Lei/0000-0001-5583-713X
CR Angelov P, 2020, NEURAL NETWORKS, V130, P185, DOI 10.1016/j.neunet.2020.07.010
   [Anonymous], 2012, NIPS
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Azad HK, 2019, INFORM SCIENCES, V492, P147, DOI 10.1016/j.ins.2019.04.019
   Banitalebi-Dehkordi M, 2018, MULTIMED TOOLS APPL, V77, P14007, DOI 10.1007/s11042-017-5007-0
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bhat MR, 2020, J INFORM OPTIM SCI, V41, P823, DOI 10.1080/02522667.2019.1616911
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Chen ZL, 2018, IEEE T CIRC SYST VID, V28, P2667, DOI 10.1109/TCSVT.2017.2710478
   Cho S, 2018, IEEE T NUCL SCI, V65, P856, DOI 10.1109/TNS.2018.2803658
   Correa NM, 2010, IEEE SIGNAL PROC MAG, V27, P39, DOI 10.1109/MSP.2010.936725
   Duan YQ, 2018, IEEE T PATTERN ANAL, V40, P1139, DOI 10.1109/TPAMI.2017.2710183
   Duan YQ, 2017, PROC CVPR IEEE, P4857, DOI 10.1109/CVPR.2017.516
   Duda P, 2018, INFORM SCIENCES, V460, P497, DOI 10.1016/j.ins.2017.07.013
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   FRAWLEY WJ, 1992, AI MAG, V13, P57
   Gao L, 2019, LECT NOTES COMPUT SC, V11663, P81, DOI 10.1007/978-3-030-27272-2_7
   Gao L, 2019, IEEE IMAGE PROC, P2224, DOI [10.1109/ICIP.2019.8803250, 10.1109/icip.2019.8803250]
   Gao L, 2019, IEEE T MULTIMEDIA, V21, P375, DOI 10.1109/TMM.2018.2859590
   Gao L, 2018, IEEE T IMAGE PROCESS, V27, P1951, DOI 10.1109/TIP.2017.2765820
   Ghalyan IFJ, 2020, PATTERN RECOGN, V99, DOI 10.1016/j.patcog.2019.107094
   Gui J, 2018, IEEE T PATTERN ANAL, V40, P490, DOI 10.1109/TPAMI.2017.2678475
   Hajizadeh R, 2020, EXPERT SYST APPL, V139, DOI 10.1016/j.eswa.2019.112860
   Han JW, 2018, IEEE SIGNAL PROC MAG, V35, P84, DOI 10.1109/MSP.2017.2749125
   Han N, 2020, IEEE T NEUR NET LEAR, V31, P5630, DOI 10.1109/TNNLS.2020.2966746
   Hao M, 2020, NEUROCOMPUTING, V391, P42, DOI 10.1016/j.neucom.2020.01.048
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He R, 2015, IEEE T IMAGE PROCESS, V24, P5543, DOI 10.1109/TIP.2015.2466106
   Hu MQ, 2019, IEEE T IMAGE PROCESS, V28, P2770, DOI 10.1109/TIP.2018.2890144
   Huang D, 2014, IEEE T IMAGE PROCESS, V23, P4680, DOI 10.1109/TIP.2014.2353814
   Jiang QY, 2017, PROC CVPR IEEE, P3270, DOI 10.1109/CVPR.2017.348
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kumar D, 2020, NEUROCOMPUTING, V408, P273, DOI 10.1016/j.neucom.2019.10.117
   Lai Zhi-Hui., 2018, NEURAL PROCESS LETT, P1
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Li K, 2017, IEEE T PATTERN ANAL, V39, P1825, DOI 10.1109/TPAMI.2016.2610969
   Li T, 2014, IEEE T MULTIMEDIA, V16, P1185, DOI 10.1109/TMM.2014.2325693
   Li T, 2011, IEEE T CIRC SYST VID, V21, P381, DOI 10.1109/TCSVT.2010.2041828
   Li YH, 2003, IEEE T KNOWL DATA EN, V15, P871, DOI 10.1109/TKDE.2003.1209005
   Lin GF, 2017, PATTERN RECOGN, V68, P14, DOI 10.1016/j.patcog.2017.03.014
   Lin ZJ, 2017, IEEE T CYBERNETICS, V47, P4342, DOI 10.1109/TCYB.2016.2608906
   Ling Guan, 2010, International Journal of Multimedia Intelligence and Security, V1, P5, DOI 10.1504/IJMIS.2010.035969
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1209, DOI 10.1109/TMM.2016.2645404
   Liong VE, 2017, IEEE T MULTIMEDIA, V19, P1234, DOI 10.1109/TMM.2016.2646180
   Liu QF, 2017, IEEE T NEUR NET LEAR, V28, P2010, DOI 10.1109/TNNLS.2016.2572204
   Lu XQ, 2017, IEEE T IMAGE PROCESS, V26, P355, DOI 10.1109/TIP.2016.2627801
   Luo W, 2018, IEEE T NEUR NET LEAR, V29, P3289, DOI 10.1109/TNNLS.2017.2712793
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Mahmood A, 2017, IEEE IMAGE PROC, P1597, DOI 10.1109/ICIP.2017.8296551
   Meng M, 2020, IEEE T IMAGE PROCESS, V29, P186, DOI 10.1109/TIP.2019.2926774
   Nie L., 2016, Synthesis Lectures on Information Concepts Retrieval & Services, V8, P1, DOI 10.2200/S00714ED1V01Y201603ICR048
   Nie L., 2019, SYNTHESIS LECT IMAGE, V9, P1
   Ning X, 2018, IEEE T IMAGE PROCESS, V27, P2575, DOI 10.1109/TIP.2018.2806229
   Noroozi F, 2019, IEEE T AFFECT COMPUT, V10, P60, DOI 10.1109/TAFFC.2017.2713783
   Norouzi M, 2012, PROC CVPR IEEE, P3108, DOI 10.1109/CVPR.2012.6248043
   Ooi CS, 2014, EXPERT SYST APPL, V41, P5858, DOI 10.1016/j.eswa.2014.03.026
   Puthenputhussery A, 2017, IEEE T MULTIMEDIA, V19, P1757, DOI 10.1109/TMM.2017.2685179
   Roddick JF, 2002, IEEE T KNOWL DATA EN, V14, P750, DOI 10.1109/TKDE.2002.1019212
   Sato Y, 2019, EXPERT SYST APPL, V119, P247, DOI 10.1016/j.eswa.2018.10.047
   Satpathy A, 2014, IEEE T IMAGE PROCESS, V24, P1953, DOI 10.1109/TIP.2014.2310123
   Sebe N, 2006, INT C PATT RECOG, P1136
   Shen YM, 2017, IEEE I CONF COMP VIS, P4117, DOI 10.1109/ICCV.2017.441
   Simonyan K., 2014, 14091556 ARXIV
   Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950
   Song J., 2011, P 19 ACM INT C MULT, P423, DOI DOI 10.1145/2072298.2072354
   Su SP, 2019, IEEE I CONF COMP VIS, P3027, DOI 10.1109/ICCV.2019.00312
   Sun QS, 2005, PATTERN RECOGN, V38, P2437, DOI 10.1016/j.patcog.2004.12.013
   Wang C, 2016, MULTIMED TOOLS APPL, V75, P9255, DOI 10.1007/s11042-016-3380-8
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P2393, DOI 10.1109/TPAMI.2012.48
   Wang KZ, 2017, IEEE T CIRC SYST VID, V27, P2591, DOI 10.1109/TCSVT.2016.2589879
   Wang X, 2020, NAT COMMUN, V11, DOI [10.1038/s41467-020-16015-z, 10.1038/s41467-020-15476-6, 10.1038/s41467-020-18785-y]
   Wang XS, 2020, SIGNAL PROCESS-IMAGE, V84, DOI 10.1016/j.image.2020.115831
   Wang Y, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P999, DOI 10.1145/2766462.2767825
   Wang YJ, 2008, IEEE T MULTIMEDIA, V10, P936, DOI 10.1109/TMM.2008.927665
   Wang YH, 2020, J VIS COMMUN IMAGE R, V67, DOI 10.1016/j.jvcir.2020.102764
   Wei Y, 2016, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2744204
   Wu GS, 2019, IEEE T IND ELECTRON, V66, P9868, DOI 10.1109/TIE.2018.2873547
   Wu M, 2022, IEEE T AFFECT COMPUT, V13, P805, DOI 10.1109/TAFFC.2020.2966440
   Xu BR, 2018, NEUROCOMPUTING, V284, P99, DOI 10.1016/j.neucom.2018.01.014
   Xu MX, 2020, IEEE T CYBERNETICS, V50, P4772, DOI 10.1109/TCYB.2019.2904753
   Xu Y, 2017, IEEE T NEUR NET LEAR, V28, P2233, DOI 10.1109/TNNLS.2016.2580572
   Yan XQ, 2019, IEEE ACCESS, V7, P36045, DOI 10.1109/ACCESS.2019.2904554
   Yang AY, 2013, IEEE T IMAGE PROCESS, V22, P3234, DOI 10.1109/TIP.2013.2262292
   Yang J, 2003, PATTERN RECOGN, V36, P1369, DOI 10.1016/S0031-3203(02)00262-5
   Yang M, 2010, LECT NOTES COMPUT SC, V6316, P448, DOI 10.1007/978-3-642-15567-3_33
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yang XJ, 2018, MULTIMED TOOLS APPL, V77, P3071, DOI 10.1007/s11042-017-5022-1
   Yao T, 2017, IEEE I CONF COMP VIS, P4904, DOI 10.1109/ICCV.2017.524
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P4479, DOI 10.1109/TNNLS.2017.2748952
   Zhang CJ, 2018, IEEE T MULTIMEDIA, V20, P903, DOI 10.1109/TMM.2017.2759500
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P428, DOI 10.1109/TCSVT.2016.2613125
   Zhang CJ, 2018, IEEE T CIRC SYST VID, V28, P1719, DOI 10.1109/TCSVT.2017.2694060
   Zhang CJ, 2018, IEEE T NEUR NET LEAR, V29, P3442, DOI 10.1109/TNNLS.2017.2728060
   Zhang D, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P225
   Zhang DQ, 2014, AAAI CONF ARTIF INTE, P2177
   Zhang H, 2017, IEEE T PATTERN ANAL, V39, P1690, DOI 10.1109/TPAMI.2016.2613924
   Zhang L, 2011, IEEE I CONF COMP VIS, P471, DOI 10.1109/ICCV.2011.6126277
   Zhang SQ, 2018, IEEE T CIRC SYST VID, V28, P3030, DOI 10.1109/TCSVT.2017.2719043
   Zhang SQ, 2018, IEEE T MULTIMEDIA, V20, P1576, DOI 10.1109/TMM.2017.2766843
   Zhang Z, 2019, IEEE T IMAGE PROCESS, V28, P4803, DOI 10.1109/TIP.2019.2912290
   Zheng CQ, 2020, IEEE T KNOWL DATA EN, V32, P2171, DOI 10.1109/TKDE.2019.2913388
   Zheng Y, 2017, PATTERN RECOGN, V67, P97, DOI 10.1016/j.patcog.2017.01.029
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
   Zhou LJ, 2020, PROC CVPR IEEE, P4623, DOI 10.1109/CVPR42600.2020.00468
   Zhou P, 2017, IEEE T IMAGE PROCESS, V26, P1173, DOI 10.1109/TIP.2016.2623487
   Zhou YC, 2018, IEEE T CIRC SYST VID, V28, P2742, DOI 10.1109/TCSVT.2017.2766199
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
   Zhu L, 2017, IEEE T KNOWL DATA EN, V29, P472, DOI 10.1109/TKDE.2016.2562624
NR 110
TC 8
Z9 8
U1 1
U2 31
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1503
EP 1514
DI 10.1109/TMM.2021.3066118
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200020
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lu, JJ
   Zhang, WD
   Yin, HB
AF Lu, Jianjie
   Zhang, Weidong
   Yin, Haibing
TI Generate and Purify: Efficient Person Data Generation for
   Re-Identification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Convolutional codes; Data models; Image synthesis; Heating
   systems; Generative adversarial networks; Training data; Graph
   clustering; person generation; re-identification
AB Generating person images has been a promising approach to enhance the input richness for re-identification (reID) tasks in recent works. A key challenge is that the generated data often contains noise, which is caused by identity inconsistency between the generated person and the original input and failure cases in generative adversarial networks (GAN). Directly training using generated images may greatly affect learning good feature embeddings, resulting in unsatisfactory reID performance. This work presents a two-stage framework that can generate high-quality person images and purify failure cases for reID training. Experimental results demonstrate that our proposed generative model can produce person images with superior appearance consistency comparing with other state-of-the-art methods. Furthermore, we show that our method yields a significant improvement in re-identification (reID) task on public datasets with insufficient training data.
C1 [Lu, Jianjie; Zhang, Weidong; Yin, Haibing] Peking Univ, Adv Inst Informat Technol, Beijing 100871, Peoples R China.
   [Yin, Haibing] Hangzhou Dianzi Univ, Sch Informat Engn, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Peking University; Hangzhou Dianzi University
RP Lu, JJ (corresponding author), Peking Univ, Adv Inst Informat Technol, Beijing 100871, Peoples R China.
EM jjlu@aiit.org.cn; wdzhang@aiit.org.cn; haibingyin@aiit.org.cn
FU NSFC [61972123, 61931008, 61901150, 2018YFC0830106]
FX This work was supported in part by NSFC under Grants 61972123, 61931008,
   and 61901150, and in part by Key R&D Projects 2018YFC0830106.
CR Bak S, 2017, PROC CVPR IEEE, P1571, DOI 10.1109/CVPR.2017.171
   Cao Z, 2021, IEEE T PATTERN ANAL, V43, P172, DOI 10.1109/TPAMI.2019.2929257
   Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145
   Ding GD, 2019, IEEE T MULTIMEDIA, V21, P2891, DOI 10.1109/TMM.2019.2916456
   Eom C., 2019, ADV NEURAL INFORM PR, P5297
   Esser P, 2018, PROC CVPR IEEE, P8857, DOI 10.1109/CVPR.2018.00923
   Ester M., 1996, KDD 96, P226, DOI DOI 10.5555/3001460.3001507
   Fan HH, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3243316
   Fu Y, 2019, AAAI CONF ARTIF INTE, P8295
   Ge YX, 2018, ADV NEUR IN, V31
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hermans Alexander, 2017, ARXIV170307737
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Kipf TN, 2017, INT C LEARN REPR
   LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489
   Luo H, 2020, IEEE T MULTIMEDIA, V22, P2597, DOI 10.1109/TMM.2019.2958756
   Luo H, 2019, IEEE COMPUT SOC CONF, P1487, DOI 10.1109/CVPRW.2019.00190
   Ma LQ, 2017, ADV NEUR IN, V30
   Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018
   Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899
   Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40
   Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2
   Siarohin A, 2018, PROC CVPR IEEE, P3408, DOI 10.1109/CVPR.2018.00359
   SIBSON R, 1973, COMPUT J, V16, P30, DOI 10.1093/comjnl/16.1.30
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30
   Wan CQ, 2020, IEEE T MULTIMEDIA, V22, P1605, DOI 10.1109/TMM.2019.2946486
   Wang YC, 2018, PROC CVPR IEEE, P1470, DOI 10.1109/CVPR.2018.00159
   Wang Z, 2016, IEEE T MULTIMEDIA, V18, P260, DOI 10.1109/TMM.2015.2505083
   Wang ZD, 2019, PROC CVPR IEEE, P1117, DOI 10.1109/CVPR.2019.00121
   Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016
   Wu Y, 2019, IEEE T IMAGE PROCESS, V28, P2872, DOI 10.1109/TIP.2019.2891895
   Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386
   Xiao T, 2016, PROC CVPR IEEE, P1249, DOI 10.1109/CVPR.2016.140
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang FX, 2020, IEEE T MULTIMEDIA, V22, P2444, DOI 10.1109/TMM.2019.2957928
   Yang L, 2019, PROC CVPR IEEE, P2293, DOI 10.1109/CVPR.2019.00240
   Zhao CR, 2020, IEEE T MULTIMEDIA, V22, P3180, DOI 10.1109/TMM.2020.2972125
   Zheng L, 2018, IEEE T PATTERN ANAL, V40, P1224, DOI 10.1109/TPAMI.2017.2709749
   Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133
   Zheng ZD, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3159171
   Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI 10.1109/CVPR.2019.00224
   Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405
   Zhu Z, 2019, PROC CVPR IEEE, P2342, DOI 10.1109/CVPR.2019.00245
NR 44
TC 4
Z9 4
U1 3
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 558
EP 566
DI 10.1109/TMM.2021.3054973
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100004
DA 2024-07-18
ER

PT J
AU Shu, YY
   Li, Q
   Xu, C
   Liu, SW
   Xu, GD
AF Shu, Yangyang
   Li, Qian
   Xu, Chang
   Liu, Shaowu
   Xu, Guandong
TI V-SVR plus : Support Vector Regression With Variational Privileged
   Information
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Support vector machines; Task analysis; Testing; Optimization;
   Object recognition; Kernel; Support vector regression; variational
   privileged information
ID PREDICTION; NETWORKS
AB Many regression tasks encounter an asymmetric distribution of information between training and testing phases where the additional information available in training, the so-called privileged information (PI), is often inaccessible in testing. In practice, the privileged information in training data might be expressed in different formats, such as continuous, ordinal, or binary values. However, most the existing learning using privileged information (LUPI) paradigms primarily deal with the continuous form of PI, preventing them from managing variational PI, which motivates this research. Therefore, in this paper, we propose a unified framework to systematically address the aforementioned three forms of privileged information. The proposed V-SVR+ method integrates continuous, ordinal, and binary PI into the learning process of support vector regression (SVR) via three losses. For continuous privileged information, we define a linear correcting (slack) function in the privileged information space to estimate slack variables in the standard SVR method using privileged information. For the ordinal relations of privileged information, we first rank the privileged information and then, regard this ordinal privileged information as auxiliary information used in the learning process of the SVR model. For the binary or Boolean privileged information, we infer a probabilistic dependency between the privileged information and labels from the summarized privileged information knowledge. Then, we transfer the privileged information knowledge to constraints and form a constrained optimization problem. We evaluate the proposed method in three applications: music emotion recognition from songs with the help of implicit information about music elements judged by composers; multiple object recognition from images with the help of implicit information about the object's importance conveyed by the list of manually annotated image tags; and photo aesthetic assessment enhanced by high-level aesthetic attributes hidden in photos. Experiment results demonstrate that the proposed methods are superior to the classic learning paradigm when solving practical problems.
C1 [Shu, Yangyang; Li, Qian; Liu, Shaowu; Xu, Guandong] Univ Technol Sydney, Data Sci & Machine Intelligence Lab, Sydney, NSW 2007, Australia.
   [Shu, Yangyang; Li, Qian; Liu, Shaowu; Xu, Guandong] Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
   [Xu, Chang] Univ Sydney, Fac Engn & Informat Technol, UBTECH Sydney Artificial Intelligence Ctr, Darlington, NSW 2008, Australia.
   [Xu, Chang] Univ Sydney, Fac Engn & Informat Technol, Sch Informat Technol, Darlington, NSW 2008, Australia.
C3 University of Technology Sydney; University of Technology Sydney;
   University of Sydney; University of Sydney
RP Xu, GD (corresponding author), Univ Technol Sydney, Data Sci & Machine Intelligence Lab, Sydney, NSW 2007, Australia.; Xu, GD (corresponding author), Univ Technol Sydney, Fac Engn & Informat Technol, Sydney, NSW 2007, Australia.
EM Yangyang.Shu@student.uts.edu.au; Qian.Li@uts.edu.au; c.xu@sydney.edu.au;
   Shaowu.Liu@uts.edu.au; Guandong.Xu@uts.edu.au
RI Xu, Chang/AAG-9337-2019; Xu, Guandong/HSH-3463-2023
OI Xu, Chang/0000-0002-4756-0609; Xu, Guandong/0000-0003-4493-6663; li,
   qian/0000-0002-8308-9551
FU Australian Research Council [DP200101374, LP170100891]; Australian
   Research Council [LP170100891, DP200101374] Funding Source: Australian
   Research Council
FX This work was supported in part by the Australian Research Council under
   Grants DP200101374 and LP170100891.
CR Al-Jawad A, 2015, 2015 1ST IEEE CONFERENCE ON NETWORK SOFTWARIZATION (NETSOFT)
   Barriere V., 2015, P MEDIAEVAL MULT BEN, P1
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chapaneri S, 2017, LECT NOTES COMPUT SC, V10597, P647, DOI 10.1007/978-3-319-69900-4_82
   Chen YA, 2015, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2015.7178058
   Coutinho E., 2015, P CEUR WORKSHOP P, V1436, P549
   Dvornik N, 2021, IEEE T PATTERN ANAL, V43, P2014, DOI 10.1109/TPAMI.2019.2961896
   EVERETT H, 1963, OPER RES, V11, P399, DOI 10.1287/opre.11.3.399
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   FELDMAN LA, 1995, J PERS SOC PSYCHOL, V69, P153, DOI 10.1037/0022-3514.69.1.153
   Felzenszwalb P, 2008, PROC CVPR IEEE, P1984
   Feng C., 2011, ADV LEARNING APPROAC
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu X, 2017, IEEE T AFFECT COMPUT, V8, P228, DOI 10.1109/TAFFC.2016.2523503
   Hwang SJ, 2012, INT J COMPUT VISION, V100, P134, DOI 10.1007/s11263-011-0494-3
   Hwang SJ, 2012, IEEE T PATTERN ANAL, V34, P1145, DOI 10.1109/TPAMI.2011.190
   Hwang SY, 2011, PURE APPL CHEM, V83, P233, DOI 10.1351/PAC-CON-10-09-35
   Ji Y, 2012, INT C PATT RECOG, P2323
   Kamienny P.-A., 2020, 8 INT C LEARN REPR W
   Kao YY, 2017, IEEE T IMAGE PROCESS, V26, P1482, DOI 10.1109/TIP.2017.2651399
   Kong S, 2016, LECT NOTES COMPUT SC, V9905, P662, DOI 10.1007/978-3-319-46448-0_40
   Lambert J, 2018, PROC CVPR IEEE, P8886, DOI 10.1109/CVPR.2018.00926
   Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002
   Li W, 2016, PROC CVPR IEEE, P2258, DOI 10.1109/CVPR.2016.248
   Li W, 2014, LECT NOTES COMPUT SC, V8693, P437, DOI 10.1007/978-3-319-10602-1_29
   Li XX, 2016, INT CONF ACOUST SPEE, P544, DOI 10.1109/ICASSP.2016.7471734
   Li X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2411
   Liu J., 2013, J INF COMPUT SCI, V2
   Ma S, 2017, PROC CVPR IEEE, P722, DOI 10.1109/CVPR.2017.84
   Michal C., 2015, MEDIAEVAL
   Murray N, 2012, PROC CVPR IEEE, P2408, DOI 10.1109/CVPR.2012.6247954
   Niu LF, 2012, INT CONF DAT MIN WOR, P495, DOI 10.1109/ICDMW.2012.79
   Oliva A, 2006, PROG BRAIN RES, V155, P23, DOI 10.1016/S0079-6123(06)55002-2
   Olivier L., 2011, MIRTOOLBOX 1 3 4 USE
   Orjesek R, 2019, 2019 29TH INTERNATIONAL CONFERENCE RADIOELEKTRONIKA (RADIOELEKTRONIKA), P213, DOI 10.1109/radioelek.2019.8733572
   Pan BW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P566, DOI 10.1145/3343031.3351049
   Pan BW, 2019, AAAI CONF ARTIF INTE, P679
   Platt J., 1998, SEQUENTIAL MINIMAL O
   Qi G.-J., 2013, Proceedings of the sixth ACM international conference on Web search and data mining, P617
   Qi GJ, 2013, PROC INT CONF DATA, P793, DOI 10.1109/ICDE.2013.6544875
   Saeid M., 2019, DOMAIN ADAPTATION PR
   Sarafianos N, 2017, IEEE INT CONF COMP V, P2637, DOI 10.1109/ICCVW.2017.313
   Sarafianos N, 2016, INT C PATT RECOG, P3115, DOI 10.1109/ICPR.2016.7900113
   Shu YY, 2020, NEUROCOMPUTING, V404, P304, DOI 10.1016/j.neucom.2020.04.142
   Shu YY, 2019, LECT NOTES ARTIF INT, V11670, P121, DOI 10.1007/978-3-030-29908-8_10
   Sloboda J.A. Juslin., 2011, Handbook of music and emotion: theory, research, applications
   Tang J, 2007, ELECTRON LETT, V43, P448, DOI 10.1049/el:20073674
   Tang J., 2007, P 15 ACM INT C MULT, P297
   Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042
   Wang JD, 2012, INFORM RETRIEVAL, V15, P278, DOI 10.1007/s10791-012-9193-0
   Wang JC, 2016, HUM-COMPUT INT-SPRIN, P227, DOI 10.1007/978-3-319-31413-6_12
   Wang SF, 2021, IEEE T AFFECT COMPUT, V12, P1002, DOI 10.1109/TAFFC.2019.2912377
   Wang SF, 2018, PATTERN RECOGN, V81, P60, DOI 10.1016/j.patcog.2018.03.033
   Wang ZY, 2017, IEEE IJCNN, P941, DOI 10.1109/IJCNN.2017.7965953
   Xianyu HS, 2016, INT CONF ACOUST SPEE, P549, DOI 10.1109/ICASSP.2016.7471735
   Yan Y, 2016, IEEE T MULTIMEDIA, V18, P2494, DOI 10.1109/TMM.2016.2602938
   Yang H, 2017, PROC CVPR IEEE, P5996, DOI 10.1109/CVPR.2017.635
   You S, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3336
NR 58
TC 6
Z9 6
U1 3
U2 11
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 876
EP 889
DI 10.1109/TMM.2021.3060955
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100028
OA Green Published
DA 2024-07-18
ER

PT J
AU Sim, K
   Yang, JC
   Lu, W
   Gao, XB
AF Sim, Kyohoon
   Yang, Jiachen
   Lu, Wen
   Gao, Xinbo
TI Blind Stereoscopic Image Quality Evaluator Based on Binocular Semantic
   and Quality Channels
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Feature extraction; Stereo image processing; Image quality;
   Image recognition; Visualization; Three-dimensional displays; Semantic
   channel; quality channel; stereoscopic image; image quality assessment;
   image recognition; convolutional neural networks
AB Human beings always evaluate the perceptual quality of an image coupled with identifying the semantic content of images. This paper addresses the correlation issue between stereoscopic image quality assessment (SIQA) and semantic recognition. In contrast to the previous SIQA methods that relied on binocular quality-aware features of a stereoscopic image, our approach also extracts binocular semantic features using a pre-trained deep convolutional neural network (DCNN) on a large dataset like ImageNet dataset, as well as the manually designed binocular quality-aware features. It can solve the problem of limited SIQA dataset size and facilitate better prediction on the quality. Experimental results demonstrate that the binocular semantic features are a good predictor for the stereoscopic image quality. The proposed method outperforms the state-of-the-art SIQA methods on four benchmark SIQA datasets. Significantly, all Spearman rank-order correlation coefficients (SROCCs) between the predicted scores and the subjective scores on the four datasets exceed 0.95. The MATLAB source code of the proposed method is available at https://github.com/kyohoonsim/Blind-Stereoscopic-Image-Quality-Evaluator-based-on-Binocular-Semantic-and-Quality-Channels https://github.com/kyohoonsim.
C1 [Sim, Kyohoon; Yang, Jiachen] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Lu, Wen] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.
   [Gao, Xinbo] Xidian Univ, Sch Elect Engn, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
C3 Tianjin University; Xidian University; Xidian University
RP Yang, JC (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM tlarygns0211@tju.edu.cn; yangjiachen@tju.edu.cn; luwen@xidian.edu.cn;
   xbgao@mail.xidian.edu.cn
RI Yang, Jiachen/ABH-5032-2020
OI Yang, Jiachen/0000-0003-2558-552X; SIM, Kyohoon/0000-0002-5214-7675
FU National Natural Science Foundation of China [61871283]; Foundation of
   Pre-Research on Equipment of China [61403120103]; Major Civil-Military
   Integration Project in Tianjin, China [18ZXJMTG00170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61871283, in part by the Foundation of
   Pre-Research on Equipment of China under Grant 61403120103, and in part
   by the Major Civil-Military Integration Project in Tianjin, China
   underGrant 18ZXJMTG00170. The associate editor coordinating the review
   of this manuscript and approving it for publication was Dr. Hantao Liu.
CR Adhikarla VK, 2017, PROC CVPR IEEE, P3720, DOI 10.1109/CVPR.2017.396
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   Appina B, 2016, SIGNAL PROCESS-IMAGE, V43, P1, DOI 10.1016/j.image.2016.02.001
   Appina Balasubramanyam, 2020, 2020 INT C SIGN PROC, P1
   Caitlin M., 2015, J VISUAL-JAPAN, V15
   Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76
   Chen MJ, 2013, SIGNAL PROCESS-IMAGE, V28, P1143, DOI 10.1016/j.image.2013.05.006
   Chen MJ, 2013, IEEE T IMAGE PROCESS, V22, P3379, DOI 10.1109/TIP.2013.2267393
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Greene MR, 2014, J VISION, V14, DOI 10.1167/14.1.14
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Javaheri Alireza, 2017, 2017 IEEE International Conference on Multimedia and Expo: Workshops (ICMEW), P1, DOI 10.1109/ICMEW.2017.8026263
   Ko H, 2017, J VIS COMMUN IMAGE R, V45, P156, DOI 10.1016/j.jvcir.2017.02.014
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee HJ, 2017, IEEE T MULTIMEDIA, V19, P1921, DOI 10.1109/TMM.2017.2687759
   Mahmud S, 2020, FRONT ARTIF INTEL AP, V325, P1332, DOI 10.3233/FAIA200236
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2013, SIGNAL PROCESS-IMAGE, V28, P870, DOI 10.1016/j.image.2012.08.004
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Schelkens P., 2020, P DIG HOL 3 DIM IM O
   Shao F, 2016, IEEE T IMAGE PROCESS, V25, P2059, DOI 10.1109/TIP.2016.2538462
   Shao F, 2016, IEEE T CYBERNETICS, V46, P730, DOI 10.1109/TCYB.2015.2414479
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Shen LQ, 2019, IEEE TETCI, V3, P59, DOI 10.1109/TETCI.2018.2804885
   Su CC, 2015, IEEE T IMAGE PROCESS, V24, P1685, DOI 10.1109/TIP.2015.2409558
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang JH, 2015, IEEE T IMAGE PROCESS, V24, P3400, DOI 10.1109/TIP.2015.2446942
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Yan J., 2020, 2020 IEEE INT C MULT, P1
   Yang JC, 2019, IEEE T MULTIMEDIA, V21, P1750, DOI 10.1109/TMM.2018.2889562
   Yue GH, 2018, SIGNAL PROCESS, V150, P204, DOI 10.1016/j.sigpro.2018.04.019
   Zhang W, 2016, PATTERN RECOGN, V59, P176, DOI 10.1016/j.patcog.2016.01.034
   Zhou WJ, 2017, IEEE T BROADCAST, V63, P404, DOI 10.1109/TBC.2016.2638620
   Zhou WJ, 2016, IEEE T MULTIMEDIA, V18, P1077, DOI 10.1109/TMM.2016.2542580
NR 37
TC 24
Z9 26
U1 4
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 1389
EP 1398
DI 10.1109/TMM.2021.3064240
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 0D8GN
UT WOS:000776227200012
DA 2024-07-18
ER

PT J
AU Tan, YH
   Rahman, MM
   Yan, YF
   Xue, J
   Shao, L
   Lu, K
AF Tan, Yanhao
   Rahman, Mohammad Muntasir
   Yan, Yanfu
   Xue, Jian
   Shao, Ling
   Lu, Ke
TI Fine-Grained Categorization From RGB-D Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dogs; Sensors; Automobiles; Birds; Benchmark testing; Task analysis;
   Image color analysis; Deep convolutional neural network; fine-grained
   categorization; modality-specific features; multimodal; RGB-D dataset
ID TRACKING
AB In the field of computer vision, fine-grained visual categorization has attracted a lot of attention and made great progress due to convolutional neural networks and a large number of publicly available datasets. With next-generation sensing technology, RGB-D cameras can provide high-quality synchronized RGB and depth images for solving many computer vision problems. Although RGB-D cameras have been used in the context of multi-view object category detection and scene understanding, they have not been widely used in fine-grained classification. In this paper, we introduce a multiview RGB-D dataset RGBD-FG for fine-grained categorization. Currently, the dataset contains 93 051 RGB-D images covering 19 super-categories and 50 sub-categories of common vegetables and fruit, and is organized in a hierarchical manner. We provide extensive experimental results to establish state-of-the-art benchmarks for our dataset, illustrating its diversity and scope for improvement through future work. We also propose a novel modality-specific multimodal network called FS-Multimodal network, which can solve two limitations of multimodal networks trained based on fine-tuning techniques: over-fitting and lack of effective depth-specific features. We hope that our study lays the foundations for fine-grained categorization of RGB-D data.
C1 [Tan, Yanhao; Rahman, Mohammad Muntasir; Yan, Yanfu; Xue, Jian; Lu, Ke] Univ Chinese Acad Sci, Sch Engn Sci, Beijing 100049, Peoples R China.
   [Rahman, Mohammad Muntasir] Islamic Univ, Dept Comp Sci & Engn, Kushtia 7003, Bangladesh.
   [Rahman, Mohammad Muntasir] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Lu, Ke] Peng Cheng Lab, Shenzhen 518055, Guangdong, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Islamic University; Mohamed Bin Zayed University of Artificial
   Intelligence; Peng Cheng Laboratory
RP Lu, K (corresponding author), Univ Chinese Acad Sci, Sch Engn Sci, Beijing 100049, Peoples R China.
EM tanyanhao15@mails.ucas.ac.cn; yanyanfu16@mails.ucas.ac.cn;
   xuejian@ucas.ac.cn; ling.shao@ieee.org; luk@ucas.ac.cn
RI Xue, Jian/W-1980-2019; Shao, Ling/D-3535-2011; Rahman, Mohammad
   Muntasir/U-2440-2018
OI Xue, Jian/0000-0002-9460-802X; Rahman, Mohammad
   Muntasir/0000-0003-2233-9251
FU National Key R&D Program of China [2017YFB1002203]; National Natural
   Science Foundation of China [62032022, 61972375, 61671426, 61871258,
   61929104]; Beijing Natural Science Foundation [4182071]; Fundamental
   Research Funds for the Central Universities [Y95401YXX2]; Scientific
   Research Program of Beijing Municipal Education Commission
   [KZ201911417048]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2017YFB1002203, in part by the National Natural Science
   Foundation of China (62032022, 61972375, 61671426, 61871258, 61929104),
   in part by the Beijing Natural Science Foundation (4182071), in part by
   the Fundamental Research Funds for the Central Universities
   (Y95401YXX2), in part by the Scientific Research Program of Beijing
   Municipal Education Commission (KZ201911417048).
CR [Anonymous], 2018, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2018.00749
   Asif U, 2018, IEEE T PATTERN ANAL, V40, P2051, DOI 10.1109/TPAMI.2017.2747134
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen TS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P627
   Cui Q., 2019, DEEP LEARNING FINE G
   Cui Y, 2016, PROC CVPR IEEE, P1153, DOI 10.1109/CVPR.2016.130
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Fothergill S., 2012, P SIGCHI C HUM FACT, P1737, DOI DOI 10.1145/2207676.2208303
   Fu JL, 2017, PROC CVPR IEEE, P4476, DOI 10.1109/CVPR.2017.476
   Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41
   Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He XT, 2017, AAAI CONF ARTIF INTE, P4075
   He XT, 2017, PROC CVPR IEEE, P7332, DOI 10.1109/CVPR.2017.775
   Hou J, 2019, PROC CVPR IEEE, P4416, DOI 10.1109/CVPR.2019.00455
   Hou L., 2019, P IEEE VIS COMM IM P, P1
   Hou SH, 2017, IEEE I CONF COMP VIS, P541, DOI 10.1109/ICCV.2017.66
   Huang G, 2022, IEEE T PATTERN ANAL, V44, P8704, DOI 10.1109/TPAMI.2019.2918284
   Janoch A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS)
   Jia CC, 2016, IEEE T IMAGE PROCESS, V25, P4641, DOI 10.1109/TIP.2016.2589320
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Khosla A., 2011, P CVPR WORKSH FIN GR
   Kong Y, 2016, IEEE T IMAGE PROCESS, V25, P2856, DOI 10.1109/TIP.2016.2556940
   Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwolek B, 2014, COMPUT METH PROG BIO, V117, P489, DOI 10.1016/j.cmpb.2014.09.005
   Lai K, 2011, IEEE INT CONF ROBOT, P1817
   Lin D, 2015, PROC CVPR IEEE, P1666, DOI 10.1109/CVPR.2015.7298775
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170
   Maji S., 2013, COMPUT RES REPOSITOR
   Nilsback M.E., 2006, IEEE C COMP VIS PATT, P1447, DOI DOI 10.1109/CVPR.2006.42
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Rahman MM, 2020, IEEE T IMAGE PROCESS, V29, P2947, DOI 10.1109/TIP.2019.2955239
   Rahman MM, 2019, INFORM SCIENCES, V476, P147, DOI 10.1016/j.ins.2018.09.040
   Rahman MM, 2017, IEEE INT CON MULTI, P991, DOI 10.1109/ICME.2017.8019538
   Ren LL, 2019, IEEE T IMAGE PROCESS, V28, P4970, DOI 10.1109/TIP.2019.2915655
   Rogez G, 2015, IEEE I CONF COMP VIS, P3889, DOI 10.1109/ICCV.2015.443
   Schöps T, 2019, PROC CVPR IEEE, P134, DOI 10.1109/CVPR.2019.00022
   Shao L, 2017, INFORM SCIENCES, V385, P266, DOI 10.1016/j.ins.2017.01.013
   Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54
   Simonyan K., 2015, P ICLR
   Singh A, 2014, IEEE INT CONF ROBOT, P509, DOI 10.1109/ICRA.2014.6906903
   Song SR, 2013, IEEE I CONF COMP VIS, P233, DOI 10.1109/ICCV.2013.36
   Song XH, 2019, IEEE T IMAGE PROCESS, V28, P980, DOI 10.1109/TIP.2018.2872629
   Sturm J, 2012, IEEE INT C INT ROBOT, P573, DOI 10.1109/IROS.2012.6385773
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan Y., 2020, 2020 IEEE INT C MULT, P1
   Wah C., 2011, CALTECH UCSD BIRDS 2
   Wang AR, 2015, IEEE T IMAGE PROCESS, V24, P4459, DOI 10.1109/TIP.2015.2465133
   Wang KK, 2017, IEEE T IMAGE PROCESS, V26, P5966, DOI 10.1109/TIP.2017.2740624
   Wang L, 2020, IEEE T IMAGE PROCESS, V29, P15, DOI 10.1109/TIP.2019.2925285
   Wang YM, 2018, PROC CVPR IEEE, P4148, DOI 10.1109/CVPR.2018.00436
   Wang YC, 2018, IEEE T IMAGE PROCESS, V27, P3571, DOI 10.1109/TIP.2018.2820809
   Welinder P., 2010, Technical Report CNS-TR-2010-001
   Xiao XL, 2019, IEEE T IMAGE PROCESS, V28, P2126, DOI 10.1109/TIP.2018.2882156
   Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143
   Yang JY, 2019, IEEE T IMAGE PROCESS, V28, P4746, DOI 10.1109/TIP.2019.2909197
   Yang Z, 2018, LECT NOTES COMPUT SC, V11218, P438, DOI 10.1007/978-3-030-01264-9_26
   Zhang N, 2014, LECT NOTES COMPUT SC, V8689, P834, DOI 10.1007/978-3-319-10590-1_54
NR 61
TC 2
Z9 2
U1 5
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 917
EP 928
DI 10.1109/TMM.2021.3061284
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YW5XZ
UT WOS:000753488100031
DA 2024-07-18
ER

PT J
AU Smith, JW
   Furxhi, O
   Torlak, M
AF W. Smith, Josiah
   Furxhi, Orges
   Torlak, Murat
TI An FCNN-Based Super-Resolution Mmwave Radar Framework for Contactless
   Musical Instrument Interface
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Radar tracking; Radar; Tracking; Optical sensors; Music; Radar imaging;
   Human computer interaction; deep learning; human-computer interaction
   (HCI); fully-convolutional neural network (FCNN); millimeter-wave
   (mmWave); multiple-input multiple-output (MIMO); radar perception;
   super-resolution
ID GESTURE; RECOGNITION
AB In this article, we propose a framework for contactless human-computer interaction (HCI) using novel tracking techniques based on deep learning-based super-resolution and tracking algorithms. Our system offers unprecedented high-resolution tracking of hand position and motion characteristics by leveraging spatial and temporal features embedded in the reflected radar waveform. Rather than classifying samples from a predefined set of hand gestures, as common in existing work on deep learning with mmWave radar, our proposed imager employs a regressive full convolutional neural network (FCNN) approach to improve localization accuracy by spatial super-resolution. While the proposed techniques are suitable for a host of tracking applications, this article focuses on their application as a musical interface to demonstrate the robustness of the gesture sensing pipeline and deep learning signal processing chain. The user can control the instrument by varying the position and velocity of their hand above the vertically-facing sensor. By employing a commercially available multiple-input-multiple-output (MIMO) radar rather than a traditional optical sensor, our framework demonstrates the efficacy of the mmWave sensing modality for fine motion tracking and offers an elegant solution to a host of HCI tasks. Additionally, we provide a freely available software package and user interface for controlling the device, streaming the data to MATLAB in real-time, and increasing accessibility to the signal processing and device interface functionality utilized in this article.
C1 [W. Smith, Josiah; Torlak, Murat] Univ Texas Dallas, Dept Elect & Comp Engn, Richardson, TX 75080 USA.
   [Furxhi, Orges] IMEC, Camera Syst & Computat Imaging, Kissimmee 34744, FL USA.
C3 University of Texas System; University of Texas Dallas; IMEC
RP Smith, JW (corresponding author), Univ Texas Dallas, Dept Elect & Comp Engn, Richardson, TX 75080 USA.
EM jws160130@utdallas.edu; orges.furxhi@imec.int.com; torlak@utdallas.edu
OI Smith, Josiah/0000-0002-3388-4805
FU imec USA summer internship program; NSF
FX Josiah Smith's work was supported by the imec USA summer internship
   program. The work of Murat Torlak (while serving at NSF) was supported
   by the NSF. The associate editor coordinating the review of this
   manuscript and approving it for publication was Professor Preeti Rao.
CR Akbari M, 2015, IEEE T MULTIMEDIA, V17, P2113, DOI 10.1109/TMM.2015.2473702
   [Anonymous], 2016, [No title captured]
   Ballan L, 2012, LECT NOTES COMPUT SC, V7577, P640, DOI 10.1007/978-3-642-33783-3_46
   Bernardo Francisco, 2017, NIME 2017 Proceedings of the International Conference on New Interfaces for Musical Expression, P283
   Brown D., 2016, P NIME BRISB AUSTR J, P300
   Dai YP, 2019, J ENG-JOE, V2019, P6840, DOI 10.1049/joe.2019.0543
   Gao JK, 2019, IEEE GEOSCI REMOTE S, V16, P35, DOI 10.1109/LGRS.2018.2866567
   García J, 2013, IEEE T SYST MAN CY-S, V43, P606, DOI 10.1109/TSMCA.2012.2220540
   Gurbuz SZ, 2021, IEEE SENS J, V21, P3763, DOI 10.1109/JSEN.2020.3022376
   Han J., 2014, P NIME JUN 30 JUL 4, P371
   Hantrakul L., 2014, P INT COMP MUS C PRO, P648
   Jensenius A. R., 2013, P NIME DAEJ KOR MAY, P196
   Joshi K., 2015, 12 USENIX S NETW SYS, P189
   Kim H, 2020, I C INF COMM TECH CO, P1384, DOI 10.1109/ICTC49870.2020.9289364
   Kim J.H., 2018, ARXIV180501994
   Kim Y, 2016, IEEE ACCESS, V4, P7125, DOI 10.1109/ACCESS.2016.2617282
   Le Gland F, 2004, ANN APPL PROBAB, V14, P144
   Li ZH, 2020, IEEE ICCE, P322
   Maragliulo S, 2019, IEEE SENS J, V19, P10187, DOI 10.1109/JSEN.2019.2931715
   Neto P, 2010, IND ROBOT, V37, P137, DOI 10.1108/01439911011018911
   Nieto O., 2013, P 10 INT S COMP MUS, V10, P15
   Oikonomidis I., 2011, P BMVC, V1
   Pardue L., 2013, P NIME DAEJ KOR MAY, P90
   Polfreman R, 2011, P INT COMP MUS C PRO, P147
   Rao S., 2017, INTRO MMWAVE SENSING
   Sang Y., 2018, IEEE ACCESS, V6, p49 339
   Schramm R, 2015, IEEE T MULTIMEDIA, V17, P243, DOI 10.1109/TMM.2014.2377553
   Senturk S., 2012, P NIME ANN ARB MI US, P449
   Skeldon KD, 1998, AM J PHYS, V66, P945, DOI 10.1119/1.19004
   Smith JW, 2021, IEEE ACCESS, V9, P10893, DOI 10.1109/ACCESS.2021.3051454
   Smith JW, 2020, IEEE RAD CONF, DOI 10.1109/radarconf2043947.2020.9266412
   Sridhar S, 2015, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2015.7298941
   Sridhar S, 2013, IEEE I CONF COMP VIS, P2456, DOI 10.1109/ICCV.2013.305
   Sun Yongdian, 2019, 2019 UK CHINA EMERGI, P1, DOI 10.1109/UCET.2019.8881866
   Tang DH, 2015, IEEE I CONF COMP VIS, P3325, DOI 10.1109/ICCV.2015.380
   Taylor J, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925965
   Tindale A, 2011, IEEE T MULTIMEDIA, V13, P50, DOI 10.1109/TMM.2010.2089786
   Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500
   Trail Shawn., 12 INT C NEW INTERFA, DOI DOI 10.5281/ZENODO.1178435
   Winkler T, 1995, P 1995 INT COMPUTER, P261
   Winkler V, 2007, EUROP RADAR CONF, P165
   Yanik M. E., 2019, IEEE ACCESS, V7, p31 801
   Yanik M. E., 2020, IEEE ACCESS, V8
   Ye Q, 2016, LECT NOTES COMPUT SC, V9912, P346, DOI 10.1007/978-3-319-46484-8_21
NR 44
TC 6
Z9 6
U1 3
U2 22
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2315
EP 2328
DI 10.1109/TMM.2021.3079695
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600008
OA Bronze, Green Submitted
DA 2024-07-18
ER

PT J
AU Websdale, D
   Taylor, S
   Milner, B
AF Websdale, Danny
   Taylor, Sarah
   Milner, Ben
TI Speaker-Independent Speech Animation Using Perceptual Loss Functions and
   Synthetic Data
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Speech recognition; Real-time systems; Facial animation;
   Hidden Markov models; Face recognition; Mouth; Audio-visual systems;
   recurrent neural networks
ID FEATURES
AB We propose a real-time speaker-independent speech-to-facial animation system that predicts lip and jaw movements on a reference face for audio speech taken from any speaker. Our approach is motivated by two key observations; 1) Speakerindependent facial animation can be generated from phoneme labels, but to perform this automatically a speech recogniser is needed which, due to contextual look-ahead, introduces too much time lag. 2) Audio-driven speech animation can be performed in real-time but requires large, multi-speaker audio-visual speech datasets of which there are few. We adopt a novel threestage training procedure that leverages the advantages of each approach. First we train a phoneme-to-visual speech model from a large single-speaker audio-visual dataset. Next, we use this model to generate the synthetic visual component of a large multi-speaker audio dataset of which the video is not available. Finally, we learn an audio-to-visual speech mapping using the synthetic visual features as the target. Furthermore, we increase the realism of the predicted facial animation by introducing two perceptually-based loss functions that aim to improve mouth closures and openings. The proposed method and loss functions are evaluated objectively using mean square error, global variance and a new metric that measures the extent of mouth opening. Subjective tests show that our approach produces facial animation comparable to those produced from phoneme sequences and that improved mouth closures, particularly for bilabial closures, are achieved.
C1 [Websdale, Danny; Taylor, Sarah; Milner, Ben] Univ East Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
C3 University of East Anglia
RP Milner, B (corresponding author), Univ East Anglia, Sch Comp Sci, Norwich NR4 7TJ, Norfolk, England.
EM d.websdale@uea.ac.uk; S.L.Taylor@uea.ac.uk; b.milner@uea.ac.uk
FU Engineering and Physical Research Council [EP/M014053/1, EP/S001816/1];
   Research and Specialist Computing Support service at the University of
   East Anglia; EPSRC [EP/M014053/1, EP/S001816/1] Funding Source: UKRI
FX This work was supported by the Engineering and Physical Research Council
   (Grant numbers EP/M014053/1 and EP/S001816/1). The research presented in
   this paperwas carried out on the High Performance Computing Cluster
   supported by the Research and Specialist Computing Support service at
   the University of East Anglia.
CR Anderson R, 2013, PROC CVPR IEEE, P3382, DOI 10.1109/CVPR.2013.434
   [Anonymous], 2009, NIPS WORKSH DEEP LEA
   [Anonymous], 1994, MODELS TECHNIQUES CO
   [Anonymous], 2009, INT C AUDITORY VISUA
   [Anonymous], 2003, GITUT114
   Bregler C., 1997, Computer Graphics Proceedings, SIGGRAPH 97, P353, DOI 10.1145/258734.258880
   Charalambous C, 2019, COMPUT ANIMAT VIRT W, V30, DOI 10.1002/cav.1892
   Chen LL, 2019, PROC CVPR IEEE, P7824, DOI 10.1109/CVPR.2019.00802
   Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774, DOI 10.1109/ICASSP.2018.8462105
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Deena S. P., 2012, THESIS U MANCHESTER
   Deena S, 2013, IEEE T MULTIMEDIA, V15, P1755, DOI 10.1109/TMM.2013.2279659
   Ding C, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P3345
   Ding C, 2015, MULTIMED TOOLS APPL, V74, P9871, DOI 10.1007/s11042-014-2156-2
   Edwards P, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925984
   Eskimez SE, 2020, IEEE-ACM T AUDIO SPE, V28, P27, DOI 10.1109/TASLP.2019.2947741
   ETSI, 2003, 202212 ES ETSI
   Ezzat T, 2002, ACM T GRAPHIC, V21, P388, DOI 10.1145/566570.566594
   Fan B, 2015, INT CONF ACOUST SPEE, P4884, DOI 10.1109/ICASSP.2015.7178899
   Filntisis PP, 2017, SPEECH COMMUN, V95, P137, DOI 10.1016/j.specom.2017.08.011
   GOWER JC, 1975, PSYCHOMETRIKA, V40, P33, DOI 10.1007/BF02291478
   Graves A., 2013, GENERATING SEQUENCES
   Haag K, 2016, LECT NOTES ARTIF INT, V10011, P198, DOI 10.1007/978-3-319-47665-0_18
   Harte N, 2015, IEEE T MULTIMEDIA, V17, P603, DOI 10.1109/TMM.2015.2407694
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Karras T, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073658
   Kim T, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P577, DOI 10.1145/2783258.2783356
   Luo C., 2014, 2014 IEEE S SWARM IN, P1, DOI [DOI 10.1109/ICMEW.2014.6890554, 10.1109/SIS.2014.7011782]
   Maas AL., 2013, P ICML WORKSHOP DEEP, V28, P1
   Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3
   Mattheyses W, 2011, 12TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2011 (INTERSPEECH 2011), VOLS 1-5, P2184
   Parker J, 2017, INT CONF ACOUST SPEE, P4920, DOI 10.1109/ICASSP.2017.7953092
   Pham HX, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P361, DOI 10.1145/3242969.3243017
   Richard A., PROC IEEECVFWINTER C, V2021, P41
   Sadoughi N, 2021, IEEE T AFFECT COMPUT, V12, P1031, DOI 10.1109/TAFFC.2019.2916031
   Sadoughi N, 2017, LECT NOTES ARTIF INT, V10498, P389, DOI 10.1007/978-3-319-67401-8_49
   Schabus D, 2014, IEEE J-STSP, V8, P336, DOI 10.1109/JSTSP.2013.2281036
   Schreer O, 2008, IEEE T MULTIMEDIA, V10, P352, DOI 10.1109/TMM.2008.917336
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Taylor S., 2017, ACM T GRAPHIC, V36, P270
   Taylor S, 2016, INTERSPEECH, P1482, DOI 10.21437/Interspeech.2016-483
   Taylor Sarah L, 2012, roceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation, P275, DOI DOI 10.2312/SCA/SCA12/275-284
   Thangthai A., 2015, P C FAC AN AN AUD VI, P88
   Thangthai A, 2016, INTERSPEECH, P2458, DOI 10.21437/Interspeech.2016-1084
   Tian GZ, 2019, IEEE INT CONF MULTI, P366, DOI 10.1109/ICMEW.2019.00069
   Vougioukas K, 2020, INT J COMPUT VISION, V128, P1398, DOI 10.1007/s11263-019-01251-8
   Vougioukas Konstantinos, 2019, CVPR WORKSHOPS, P37
   Wang LJ, 2011, INT CONF ACOUST SPEE, P4580
   Watts O, 2016, INT CONF ACOUST SPEE, P5505, DOI 10.1109/ICASSP.2016.7472730
   Websdale D, 2018, INTERSPEECH, P2479, DOI 10.21437/Interspeech.2018-2066
   Zhou Y, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201292
NR 51
TC 3
Z9 3
U1 1
U2 13
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2539
EP 2552
DI 10.1109/TMM.2021.3087020
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600024
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Wu, JL
   Song, LC
   Zhang, Q
   Yang, M
   Yuan, JS
AF Wu, Jialian
   Song, Liangchen
   Zhang, Qian
   Yang, Ming
   Yuan, Junsong
TI ForestDet: Large-Vocabulary Long-Tailed Object Detection and Instance
   Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Vegetation; Forestry; Noise measurement; Toy manufacturing industry;
   Proposals; Classification tree analysis; Calibration; Object detection;
   instance segmentation; large vocabulary; long-tailed data distribution
AB Object detection and instance segmentation with a large number of object categories and long-tailed data distribution are challenging for most existing deep learning models. As the number of classes increases, the outputs of a classifier become sensitive to likely noisy logits, which can easily result in an incorrect recognition. To alleviate the large-vocabulary problem, we cluster fine-grained classes into coarser parent classes and then build a classification tree to classify an object into a fine-grained class via its parent class. Because the number of parent class is much fewer, their logits are more stable to suppress the wrong/noisy logits existed in the fine-grained class nodes. Due to a variety of ways for clustering fine-grained classes into parent classes, we can further construct multiple trees to build a classification forest where each single tree contributes its vote to the fine-grained classification. Moreover, a simple yet effective resampling method, termed as NMS Resampling, is proposed aiming at solving the long tail (data imbalance) problem. Our method, coined as ForestDet, serves as a plug-and-play module, which can be readily employed in both one-stage and two-stage object recognition models for recognizing more than 1000 categories. Extensive experiments are conducted on the large vocabulary dataset LVIS. Compared to the Mask R-CNN baseline, our two-stage counterpart Forest R-CNN significantly boosts the performance by 11.5% and 3.9% AP improvements on the rare categories and overall categories, respectively. Compared to the RetinaNet baseline, our one-stage counterpart Forest RetinaNet improves 2.1% AP on overall categories. Moreover, we achieve state-of-the-art results on the LVIS dataset. Code and models are available at https://github.com/JialianW/Forest_RCNN.
C1 [Wu, Jialian; Song, Liangchen; Yuan, Junsong] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
   [Zhang, Qian; Yang, Ming] Horizon Robot Inc, Beijing 100080, Peoples R China.
C3 State University of New York (SUNY) System; State University of New York
   (SUNY) Buffalo
RP Wu, JL (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.
EM jialianw@buffalo.edu; lsong8@buffalo.edu; qian01.zhang@horizon.ai;
   mingyang2008@u.northwestern.edu; jsyuan@buffalo.edu
RI Yuan, Junsong/A-5171-2011; Song, Liangchen/AAZ-9431-2021; Yang,
   Ming-Hsuan/T-9533-2019
OI Song, Liangchen/0000-0002-8366-5088; Yang,
   Ming-Hsuan/0000-0003-4848-2304; Yang, Ming/0000-0003-1691-6817; Yuan,
   Junsong/0000-0002-7901-8793
FU National Science Foundation [CNS1951952]; Horizon Robotics
FX This work was supported in part by a gift grant from Horizon Robotics
   and National Science Foundation under Grant CNS1951952.
CR Bolya D, 2019, IEEE I CONF COMP VIS, P9156, DOI 10.1109/ICCV.2019.00925
   Boyan Zhou, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P9716, DOI 10.1109/CVPR42600.2020.00974
   Byrd J, 2019, PR MACH LEARN RES, V97
   Cao JL, 2019, IEEE I CONF COMP VIS, P9704, DOI 10.1109/ICCV.2019.00980
   Cao JL, 2019, PROC CVPR IEEE, P7384, DOI 10.1109/CVPR.2019.00757
   Cao Jiale, 2020, EUR C COMP VIS, P2
   Cao KD, 2019, ADV NEUR IN, V32
   Chen CL, 2019, IEEE T MULTIMEDIA, V21, P3205, DOI 10.1109/TMM.2019.2916104
   Chen L, 2021, IEEE T MULTIMEDIA, V23, P4297, DOI 10.1109/TMM.2020.3040539
   Cui Y, 2019, PROC CVPR IEEE, P9260, DOI 10.1109/CVPR.2019.00949
   Dai JF, 2016, LECT NOTES COMPUT SC, V9910, P534, DOI 10.1007/978-3-319-46466-4_32
   Dai P, 2019, IEEE T MULTIMEDIA, V21, P1709, DOI 10.1109/TMM.2018.2885922
   Das D, 2020, IEEE T IMAGE PROCESS, V29, P3336, DOI 10.1109/TIP.2019.2959254
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Fang F, 2020, IEEE T IMAGE PROCESS, V29, P2052, DOI 10.1109/TIP.2019.2947792
   Girshick R., 2014, P 2014 IEEE C COMPUT, P580, DOI [DOI 10.1109/CVPR.2014.81, 10.1109/CVPR.2014.81]
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Guo YD, 2019, IEEE T MULTIMEDIA, V21, P2903, DOI 10.1109/TMM.2019.2912703
   Gupta A, 2019, PROC CVPR IEEE, P5351, DOI 10.1109/CVPR.2019.00550
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu X., 2020, P IEEE CVF C COMP VI
   Huang C, 2016, PROC CVPR IEEE, P5375, DOI 10.1109/CVPR.2016.580
   Huang ZJ, 2019, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2019.00657
   Ji Z, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1349, DOI 10.1145/3343031.3351064
   Jiale Cao, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11482, DOI 10.1109/CVPR42600.2020.01150
   Jialian Wu, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13427, DOI 10.1109/CVPR42600.2020.01344
   Li JA, 2018, IEEE T MULTIMEDIA, V20, P1645, DOI 10.1109/TMM.2017.2772796
   Li JN, 2017, IEEE T MULTIMEDIA, V19, P944, DOI 10.1109/TMM.2016.2642789
   Li Y, 2017, PROC CVPR IEEE, P4438, DOI 10.1109/CVPR.2017.472
   Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Liu ZW, 2019, PROC CVPR IEEE, P2532, DOI 10.1109/CVPR.2019.00264
   Miller George, 1998, WORDNET ELECT LEXICA
   Ouyang WL, 2016, PROC CVPR IEEE, P864, DOI 10.1109/CVPR.2016.100
   Pang YW, 2021, IEEE T IMAGE PROCESS, V30, P207, DOI 10.1109/TIP.2020.3034487
   Qiu HQ, 2020, IEEE T MULTIMEDIA, V22, P3039, DOI 10.1109/TMM.2020.2971175
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Sabirin H, 2012, IEEE T MULTIMEDIA, V14, P657, DOI 10.1109/TMM.2012.2187777
   Shen L, 2016, LECT NOTES COMPUT SC, V9911, P467, DOI 10.1007/978-3-319-46478-7_29
   Singh B, 2018, PROC CVPR IEEE, P1081, DOI 10.1109/CVPR.2018.00119
   Singh B, 2018, PROC CVPR IEEE, P3578, DOI 10.1109/CVPR.2018.00377
   Tan J., 2020, PROC IEEE C COMPUT V, p11 662
   Tang Kaihua, 2020, NEURIPS
   Tang YX, 2017, IEEE T MULTIMEDIA, V19, P393, DOI 10.1109/TMM.2016.2614862
   Wang P, 2021, PROC CVPR IEEE, P943, DOI 10.1109/CVPR46437.2021.00100
   Wang X., 2020, Advances in Neural information processing systems
   Wang XD, 2022, Arxiv, DOI arXiv:2010.01809
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P1570, DOI 10.1145/3394171.3413970
   Wu JL, 2020, MM '20: PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA, P2012, DOI 10.1145/3394171.3413634
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xu WQ, 2019, IEEE I CONF COMP VIS, P5167, DOI 10.1109/ICCV.2019.00527
   Yan CX, 2020, IEEE T IMAGE PROCESS, V29, P8163, DOI 10.1109/TIP.2020.3011807
   Yu JG, 2020, IEEE T IMAGE PROCESS, V29, P389, DOI 10.1109/TIP.2019.2923571
   Yu Li, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P10988, DOI 10.1109/CVPR42600.2020.01100
   Zhang H, 2020, IEEE T IMAGE PROCESS, V29, P2078, DOI 10.1109/TIP.2019.2947806
   Zhang X, 2017, IEEE I CONF COMP VIS, P5419, DOI 10.1109/ICCV.2017.578
   Zhang Y., P AAAI C ART, V35, P3447
   Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644
   Zhong YY, 2019, PROC CVPR IEEE, P7804, DOI 10.1109/CVPR.2019.00800
NR 63
TC 6
Z9 6
U1 2
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 3693
EP 3705
DI 10.1109/TMM.2021.3106096
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 3R1UG
UT WOS:000838704400003
OA Bronze
DA 2024-07-18
ER

PT J
AU Zhou, WJ
   Lin, XY
   Lei, JS
   Yu, L
   Hwang, JN
AF Zhou, Wujie
   Lin, Xinyang
   Lei, Jingsheng
   Yu, Lu
   Hwang, Jenq-Neng
TI MFFENet: Multiscale Feature Fusion and Enhancement Network For
   RGB-Thermal Urban Road Scene Parsing
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Hafnium; Autonomous driving; urban road scene; semantic segmentation;
   computer vision; thermal image; multi-task supervision
AB Compared with traditional handcrafted features, deep learning has greatly improved the performance of scene parsing. However, it remains challenging under various environmental conditions caused by imaging limitations. Thermal imaging cameras have several advantages over cameras for the visible spectrum, such as operation in total darkness, robustness to shadow effects, insensitivity to illumination variations, and strong ability to penetrate smog and haze. These advantages of thermal imaging cameras make them ideal for the scene parsing of semantic objects in daytime and nighttime. In this paper, we propose a novel multiscale feature fusion and enhancement network (MFFENet) for accurate parsing of RGB-thermal urban road scenes even when the quality of the available RGB data is compromised. The proposed MFFENet consists of two encoders, a feature fusion layer, and a multi-label supervision layer. We concatenate the multi-scale features with the features that contain global semantic information. Furthermore, we explore the cross-modal fusion of RGB and thermal features at multiple stages, rather than fusing them once at the low or high stage. Then, we propose a spatial attention mechanism module that provides a higher weight to (focuses more on) the foreground area, allowing MFFENet to emphasize foreground objects. Finally, multi-label supervision is introduced to optimize parameters of the proposed MFFENet. Experimental results confirm that the proposed MFFENet outperforms similar high-performing methods.
C1 [Zhou, Wujie; Lin, Xinyang; Lei, Jingsheng] Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
   [Yu, Lu] Zhejiang Univ, Inst Informat & Commun Engn, Hangzhou 310027, Peoples R China.
   [Hwang, Jenq-Neng] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA.
C3 Zhejiang University of Science & Technology; Zhejiang University;
   University of Washington; University of Washington Seattle
RP Zhou, WJ (corresponding author), Zhejiang Univ Sci & Technol, Sch Informat & Elect Engn, Hangzhou 310023, Peoples R China.
EM wujiezhou@163.com; linxinyang@zust.edu.cn; leijingsheng@zust.edu.cn;
   yul@zju.edu.cn; hwang@uw.edu
OI Hwang, Jenq-Neng/0000-0002-8877-2421; zhou, wujie/0000-0002-3055-2493
FU National Natural Science Foundation of China [61502429, 61972357];
   Zhejiang Provincial Natural Science Foundation of China [LY18F020012]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61502429 and 61972357, and in part by
   the Zhejiang Provincial Natural Science Foundation of China under Grant
   LY18F020012.
CR [Anonymous], 2019, IEEE T MED IMAGING, DOI DOI 10.1109/TMI.2018.2867261
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Berman M, 2018, PROC CVPR IEEE, P4413, DOI 10.1109/CVPR.2018.00464
   Brostow G.J., 2008, EUR C COMP VIS ECCV, DOI DOI 10.1007/978-3-540-88682-2_5
   Cai PD, 2020, IEEE ROBOT AUTOM LET, V5, P1247, DOI 10.1109/LRA.2020.2967299
   Chen CF, 2021, IEEE T MULTIMEDIA, V23, P2335, DOI 10.1109/TMM.2020.3009499
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng YH, 2017, PROC CVPR IEEE, P1475, DOI 10.1109/CVPR.2017.161
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Dai JT, 2018, IET COMPUT VIS, V12, P1171, DOI 10.1049/iet-cvi.2018.5218
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Darms Michael, 2008, 2008 IEEE Intelligent Vehicles Symposium (IV), P1197, DOI 10.1109/IVS.2008.4621259
   Deng Liuyuan, 2019, RFBNET DEEP MULTIMOD
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Dutta A., 2020, 2020 IEEE INT C POWE, P1, DOI DOI 10.1109/PEDES49360.2020.9379823
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297
   Ha Q, 2017, IEEE INT C INT ROBOT, P5108, DOI 10.1109/IROS.2017.8206396
   Hazirbas C, 2017, LECT NOTES COMPUT SC, V10111, P213, DOI 10.1007/978-3-319-54181-5_14
   He JJ, 2019, PROC CVPR IEEE, P7511, DOI 10.1109/CVPR.2019.00770
   He Kaiming, 2016, P INT C COMP VIS PAT, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang SF, 2020, IEEE T IMAGE PROCESS, V29, P8251, DOI 10.1109/TIP.2020.3013142
   Huang ZL, 2019, IEEE I CONF COMP VIS, P603, DOI 10.1109/ICCV.2019.00069
   Jianbo Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12371), P1, DOI 10.1007/978-3-030-58574-7_1
   Jiang, 2018, ARXIV PREPRINT ARXIV, DOI DOI 10.5194/ACP-2018-920
   Jiao JB, 2019, PROC CVPR IEEE, P2864, DOI 10.1109/CVPR.2019.00298
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kang B, 2018, IEEE T MULTIMEDIA, V20, P2478, DOI 10.1109/TMM.2018.2798282
   Kingma D. P., 2014, arXiv
   Kocic J, 2018, 2018 26TH TELECOMMUNICATIONS FORUM (TELFOR), P575
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Längkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008
   Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060
   Li YB, 2017, IEEE IMAGE PROC, P1262, DOI 10.1109/ICIP.2017.8296484
   Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34
   Lin D, 2020, IEEE T CYBERNETICS, V50, P1120, DOI 10.1109/TCYB.2018.2885062
   Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu H, 2018, MULTIMED TOOLS APPL, V77, P22475, DOI 10.1007/s11042-018-6056-8
   Long J., 2015, P IEEE C COMP VIS PA, P3431
   Long Jonathan., 2014, NIPS
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mottaghi R, 2014, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2014.119
   Nguyen T, 2019, IEEE ROBOT AUTOM LET, V4, P3908, DOI 10.1109/LRA.2019.2928734
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pal A., 2019, P 11 IND C COMP VIS
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Pohlen T, 2017, PROC CVPR IEEE, P3309, DOI 10.1109/CVPR.2017.353
   Poudel R.P.K., 2019, ARXIV190204502, P289
   Ranzato MA., 2007, CVPR, DOI [10.1109/cvpr.2007.383157, 10.1109/CVPR.2007.383157]
   Romera E, 2018, IEEE T INTELL TRANSP, V19, P263, DOI 10.1109/TITS.2017.2750080
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Shivakumar SS, 2020, IEEE INT CONF ROBOT, P9441, DOI [10.1109/icra40945.2020.9196831, 10.1109/ICRA40945.2020.9196831]
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sun K., 2019, arXiv:1904.04514
   Sun YX, 2021, IEEE T AUTOM SCI ENG, V18, P1000, DOI 10.1109/TASE.2020.2993143
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Tian Z, 2019, PROC CVPR IEEE, P3121, DOI 10.1109/CVPR.2019.00324
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Valada A, 2020, INT J COMPUT VISION, V128, P1239, DOI 10.1007/s11263-019-01188-y
   Wang JH, 2016, LECT NOTES COMPUT SC, V9909, P664, DOI 10.1007/978-3-319-46454-1_40
   Wang WY, 2018, LECT NOTES COMPUT SC, V11215, P144, DOI 10.1007/978-3-030-01252-6_9
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Xiaokang Chen, 2020, Computer Vision - ECCV 2020 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12356), P561, DOI 10.1007/978-3-030-58621-8_33
   Xiong ZT, 2020, PROC CVPR IEEE, P3991, DOI 10.1109/CVPR42600.2020.00405
   Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20
   Yu CQ, 2018, PROC CVPR IEEE, P1857, DOI 10.1109/CVPR.2018.00199
   Zhang TY, 2019, IEEE T MULTIMEDIA, V21, P2930, DOI 10.1109/TMM.2019.2914870
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
   Zhou WJ, 2023, IEEE T COGN DEV SYST, V15, P476, DOI 10.1109/TCDS.2021.3051010
   Zhou WJ, 2022, IEEE T CIRC SYST VID, V32, P1224, DOI 10.1109/TCSVT.2021.3077058
   Zhou WJ, 2022, IEEE T MULTIMEDIA, V24, P2192, DOI 10.1109/TMM.2021.3077767
   Zhou WJ, 2021, IEEE T MULTIMEDIA, V23, P3388, DOI 10.1109/TMM.2020.3025166
   Zhou WJ, 2021, IEEE INTELL SYST, V36, P73, DOI 10.1109/MIS.2020.2999462
   Zhou WJ, 2020, IEEE T COMPUT IMAG, V6, P883, DOI 10.1109/TCI.2020.2993640
   Zhou WJ, 2021, IEEE T SYST MAN CY-S, V51, P3641, DOI 10.1109/TSMC.2019.2957386
NR 79
TC 62
Z9 63
U1 9
U2 78
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 2526
EP 2538
DI 10.1109/TMM.2021.3086618
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA 1D5KQ
UT WOS:000793839600023
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Zhu, XF
   Wu, XJ
   Xu, TY
   Feng, ZH
   Kittler, J
AF Zhu, Xue-Feng
   Wu, Xiao-Jun
   Xu, Tianyang
   Feng, Zhen-Hua
   Kittler, Josef
TI Robust Visual Object Tracking Via Adaptive Attribute-Aware
   Discriminative Correlation Filters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Discriminative correlation filter; spatial attention; visual attribute;
   visual object tracking
AB In recent years, attention mechanisms have been widely studied in Discriminative Correlation Filter (DCF) based visual object tracking. To realise spatial attention and discriminative feature mining, existing approaches usually apply regularisation terms to the spatial dimension of multi-channel features. However, these spatial regularisation approaches construct a shared spatial attention pattern for all multi-channel features, without considering the diversity across channels. As each feature map (channel) focuses on a specific visual attribute, a shared spatial attention pattern limits the capability for mining important information from different channels. To address this issue, we advocate channel-specific spatial attention for DCF-based trackers. The key ingredient of the proposed method is an Adaptive Attribute-Aware spatial attention mechanism for constructing a novel DCF-based tracker (A$<^>3$ DCF). To highlight the discriminative elements in each feature map, spatial sparsity is imposed in the filter learning stage, moderated by the prior knowledge regarding the expected concentration of signal energy. In addition, we perform a post processing of the identified spatial patterns to alleviate the impact of less significant channels. The net effect is that the irrelevant and inconsistent channels are removed by the proposed method. The results obtained on a number of well-known benchmarking datasets, including OTB2015, DTB70, UAV123, VOT2018, LaSOT, GOT-10 K and TrackingNet, demonstrate the merits of the proposed A(3) DCF tracker, with improved performance compared to the state-of-the-art methods.
C1 [Zhu, Xue-Feng; Wu, Xiao-Jun; Xu, Tianyang] Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
   [Xu, Tianyang] Jiangnan Univ, Sch IoT, Wuxi, Jiangsu, Peoples R China.
   [Xu, Tianyang; Feng, Zhen-Hua; Kittler, Josef] Univ Surrey, Ctr Vis Speech & Signal Proc, Guildford GU2 7XH, Surrey, England.
   [Feng, Zhen-Hua] Univ Surrey, Dept Comp Sci, Guildford GU2 7XH, Surrey, England.
C3 Jiangnan University; Jiangnan University; University of Surrey;
   University of Surrey
RP Wu, XJ (corresponding author), Jiangnan Univ, Sch Artificial Intelligence & Comp Sci, Wuxi 214122, Jiangsu, Peoples R China.
EM xuefeng_zhu95@163.com; xiaojun_wujnu@163.com; tianyang.xu@surrey.ac.uk;
   zieng@surrey.ac.uk; j.kittler@surrey.ac.uk
RI Feng, Zhenhua/T-3139-2019; Zhu, Xuefeng/HPC-6314-2023; Xu,
   Tianyang/AAE-1982-2019; Zhu, Xuefeng/KEJ-0328-2024
OI Feng, Zhenhua/0000-0002-4485-4249; Zhu, Xuefeng/0000-0003-0262-5891; Xu,
   Tianyang/0000-0002-9015-3128; Wu, Xiao-Jun/0000-0002-0310-5778; Kittler,
   Josef/0000-0002-8110-9205
FU National Natural Science Foundation of China [62020106012, U1836218,
   61672265]; 111 Project of Ministry of Education of China [B12018];
   Engineering and Physical Sciences Research Council (EPSRC)
   [EP/N007743/1, EP/R018456/1]; EPSRC [EP/N007743/1, EP/R018456/1] Funding
   Source: UKRI
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62020106012, U1836218, and 61672265, the 111 Project
   of Ministry of Education of China under Grant B12018, and the
   Engineering and Physical Sciences Research Council (EPSRC) under Grants
   EP/N007743/1, MURI/EPSRC/DSTL, EP/R018456/1.
CR Bertinetto L, 2016, PROC CVPR IEEE, P1401, DOI 10.1109/CVPR.2016.156
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bhat G, 2018, LECT NOTES COMPUT SC, V11206, P493, DOI 10.1007/978-3-030-01216-8_30
   Bolme DS, 2010, PROC CVPR IEEE, P2544, DOI 10.1109/CVPR.2010.5539960
   Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016
   Chen K, 2019, IEEE T MULTIMEDIA, V21, P86, DOI 10.1109/TMM.2018.2846405
   Dai KN, 2019, PROC CVPR IEEE, P4665, DOI 10.1109/CVPR.2019.00480
   Dalal N, 2005, P CVPR, P01
   Danelljan M, 2017, PROC CVPR IEEE, P6931, DOI 10.1109/CVPR.2017.733
   Danelljan M, 2017, IEEE T PATTERN ANAL, V39, P1561, DOI 10.1109/TPAMI.2016.2609928
   Danelljan M, 2016, LECT NOTES COMPUT SC, V9909, P472, DOI 10.1007/978-3-319-46454-1_29
   Danelljan M, 2015, IEEE I CONF COMP VIS, P4310, DOI 10.1109/ICCV.2015.490
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   Du DW, 2019, IEEE INT CONF COMP V, P199, DOI 10.1109/ICCVW.2019.00029
   Fan H, 2019, PROC CVPR IEEE, P7944, DOI 10.1109/CVPR.2019.00814
   Fan H, 2019, PROC CVPR IEEE, P5369, DOI 10.1109/CVPR.2019.00552
   Fan H, 2017, IEEE COMPUT SOC CONF, P2217, DOI 10.1109/CVPRW.2017.275
   Galoogahi HK, 2017, IEEE I CONF COMP VIS, P1144, DOI [10.1109/ICCV.2017.128, 10.1109/ICCV.2017.129]
   Gao JY, 2019, PROC CVPR IEEE, P4644, DOI 10.1109/CVPR.2019.00478
   Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006
   Guo Q, 2017, IEEE I CONF COMP VIS, P1781, DOI 10.1109/ICCV.2017.196
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Held D, 2016, LECT NOTES COMPUT SC, V9905, P749, DOI 10.1007/978-3-319-46448-0_45
   Henriques JF, 2015, IEEE T PATTERN ANAL, V37, P583, DOI 10.1109/TPAMI.2014.2345390
   Henriques JF, 2012, LECT NOTES COMPUT SC, V7575, P702, DOI 10.1007/978-3-642-33765-9_50
   Huang B, 2020, IEEE T MULTIMEDIA, V22, P2820, DOI 10.1109/TMM.2020.2965482
   Huang LH, 2021, IEEE T PATTERN ANAL, V43, P1562, DOI 10.1109/TPAMI.2019.2957464
   Jiang BY, 2019, IEEE I CONF COMP VIS, P2000, DOI 10.1109/ICCV.2019.00209
   Kristan M, 2018, IEEMA Engineer Infinite Conference (eTechNxT)
   Kristan M, 2017, IEEE INT CONF COMP V, P1949, DOI 10.1109/ICCVW.2017.230
   Kristanl M, 2019, IEEE INT CONF COMP V, P2206, DOI 10.1109/ICCVW.2019.00276
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2019, PROC CVPR IEEE, P4277, DOI 10.1109/CVPR.2019.00441
   Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935
   Li F, 2018, PROC CVPR IEEE, P4904, DOI 10.1109/CVPR.2018.00515
   Li PX, 2019, IEEE I CONF COMP VIS, P6161, DOI 10.1109/ICCV.2019.00626
   Li SY, 2017, AAAI CONF ARTIF INTE, P4140
   Li X, 2019, PROC CVPR IEEE, P1369, DOI 10.1109/CVPR.2019.00146
   Lukezic A, 2017, PROC CVPR IEEE, P4847, DOI 10.1109/CVPR.2017.515
   Ma C, 2015, IEEE I CONF COMP VIS, P3074, DOI 10.1109/ICCV.2015.352
   Müller M, 2018, LECT NOTES COMPUT SC, V11205, P310, DOI 10.1007/978-3-030-01246-5_19
   Mueller M, 2017, PROC CVPR IEEE, P1387, DOI 10.1109/CVPR.2017.152
   Mueller M, 2016, LECT NOTES COMPUT SC, V9905, P445, DOI 10.1007/978-3-319-46448-0_27
   Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465
   Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466
   Ruan WJ, 2019, IEEE T MULTIMEDIA, V21, P1122, DOI 10.1109/TMM.2018.2872897
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Sun C, 2018, PROC CVPR IEEE, P489, DOI 10.1109/CVPR.2018.00058
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Valmadre J, 2017, PROC CVPR IEEE, P5000, DOI 10.1109/CVPR.2017.531
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   Wang NY, 2015, IEEE I CONF COMP VIS, P3101, DOI 10.1109/ICCV.2015.355
   Wang Q, 2018, PROC CVPR IEEE, P4854, DOI 10.1109/CVPR.2018.00510
   Wu Y, 2015, IEEE T PATTERN ANAL, V37, P1834, DOI 10.1109/TPAMI.2014.2388226
   Xu TY, 2020, IEEE T CIRC SYST VID, V30, P3727, DOI 10.1109/TCSVT.2019.2945068
   Xu TY, 2019, IEEE I CONF COMP VIS, P7949, DOI 10.1109/ICCV.2019.00804
   Xu TY, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2919201
   Xu TY, 2018, INT C PATT RECOG, P1888, DOI 10.1109/ICPR.2018.8546146
   Young I. T., 1983, SIGNALS SYSTEMS SOLU
   Zhang TZ, 2019, IEEE T PATTERN ANAL, V41, P365, DOI 10.1109/TPAMI.2018.2797062
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zheng LY, 2019, IEEE I CONF COMP VIS, P4019, DOI 10.1109/ICCV.2019.00412
   Zhu Z, 2018, LECT NOTES COMPUT SC, V11213, P103, DOI 10.1007/978-3-030-01240-3_7
NR 64
TC 25
Z9 26
U1 7
U2 33
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2022
VL 24
BP 301
EP 312
DI 10.1109/TMM.2021.3050073
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA YK9LD
UT WOS:000745524300023
OA Green Published
DA 2024-07-18
ER

PT J
AU Bentaleb, A
   Begen, AC
   Harous, S
   Zimmermann, R
AF Bentaleb, Abdelhak
   Begen, Ali C.
   Harous, Saad
   Zimmermann, Roger
TI Data-Driven Bandwidth Prediction Models and Automated Model Selection
   for Low Latency
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bandwidth; Prediction algorithms; Smoothing methods; Predictive models;
   Bit rate; Adaptation models; Quality of experience; HTTP adaptive
   streaming; DASH; ABR; CMAF; low latency; bandwidth prediction; chunked
   transfer encoding
ID RATE ADAPTATION; VIDEO
AB Today's HTTP adaptive streaming solutions use a variety of algorithms to measure the available network bandwidth and predict its future values. Bandwidth prediction, which is already a difficult task, must be more accurate when lower latency is desired due to the shorter time available to react to bandwidth changes, and when mobile networks are involved due to their inherently more frequent and potentially larger bandwidth fluctuations. Any inaccuracy in bandwidth prediction results in flawed adaptation decisions, which will in turn translate into a diminished viewer experience. We propose an Automated Model for Prediction (AMP) that encompasses techniques for bandwidth prediction and model auto-selection specifically designed for low-latency live steaming with chunked transfer encoding. We first study statistical and computational intelligence techniques to implement a suite of bandwidth prediction models that can work accurately under a broad range of network conditions, and second, we introduce an automated prediction model selection method. We confirm the effectiveness of our solution through trace-driven live streaming experiments.
C1 [Bentaleb, Abdelhak; Zimmermann, Roger] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
   [Begen, Ali C.] Ozyegin Univ, TR-34794 Istanbul, Turkey.
   [Begen, Ali C.] Networked Media, TR-34794 Istanbul, Turkey.
   [Harous, Saad] United Arab Emirates Univ, Coll Informat Technol, Al Ain, U Arab Emirates.
C3 National University of Singapore; Ozyegin University; United Arab
   Emirates University
RP Bentaleb, A (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM bentaleb@comp.nus.edu.sg; acbegen@ieee.org; harous@uaeu.ac.ae;
   rogerz@comp.nus.edu.sg
RI Begen, Ali C./R-5897-2016; Zimmermann, Roger/D-7944-2015
OI Begen, Ali C./0000-0002-0835-3017; Zimmermann,
   Roger/0000-0002-7410-2590; Bentaleb, Abdelhak/0000-0002-5382-6530;
   Harous, Saad/0000-0001-6524-7352
FU Singapore Ministry of Education Academic Research Fund
   [MOE2018-T2-1-103]; UAE University [31T102-UPAR-1-2017]
FX This work was supported by the Singapore Ministry of Education Academic
   Research Fund Tier 2 under MOE's official under Grant MOE2018-T2-1-103
   and in part by UAE University, under Grant 31T102-UPAR-1-2017. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. MeaWang.
CR Adhikari R., 2013, INTRO STUDY TIME SER, DOI [10.13140/2.1.2771.8084, DOI 10.13140/2.1.2771.8084]
   Akhtar Z, 2018, PROCEEDINGS OF THE 2018 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '18), P44, DOI 10.1145/3230543.3230558
   [Anonymous], 2016, P 8 INT WORKSH MOB V
   [Anonymous], 2018, C1173842901 CISCO
   [Anonymous], 2019, DASH REFERENCE PLAYE
   Balakrishnan H., 1997, Performance Evaluation Review, V25, P2, DOI 10.1145/258623.258631
   Bentaleb A, 2019, PROCEEDINGS OF THE 29TH ACM WORKSHOP ON NETWORK AND OPERATING SYSTEMS SUPPORT FOR DIGITAL AUDIO AND VIDEO (NOSSDAV'19), P7, DOI 10.1145/3304112.3325611
   Bentaleb A, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P13, DOI 10.1145/3204949.3204961
   Bentaleb A, 2019, IEEE COMMUN SURV TUT, V21, P562, DOI 10.1109/COMST.2018.2862938
   Bentaleb A, 2018, ACM T MULTIM COMPUT, V14, DOI 10.1145/3219752
   Bentaleb A, 2017, IEEE T MULTIMEDIA, V19, P2136, DOI 10.1109/TMM.2017.2733344
   Bentaleb A, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1296, DOI 10.1145/2964284.2964332
   Camacho E.F., 2013, Model predictive control
   CHEUNG YW, 1995, J BUS ECON STAT, V13, P277, DOI 10.2307/1392187
   De Cicco L, 2013, 2013 20TH INTERNATIONAL PACKET VIDEO WORKSHOP (PV)
   Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1
   Du HP, 2018, IEEE ACCESS, V6, P9554, DOI 10.1109/ACCESS.2017.2788057
   Haddad RJ, 2013, IEEE COMMUN SURV TUT, V15, P1803, DOI 10.1109/SURV.2013.032213.00091
   Huang TY, 2014, SIGCOMM'14: PROCEEDINGS OF THE 2014 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P187, DOI 10.1145/2619239.2626296
   Hughes K., 2017, ISO/IEC, V19, P23000
   Jiang J., 2012, P 8 INT C EM NETW EX, P97
   Kreutz D, 2015, P IEEE, V103, P14, DOI 10.1109/JPROC.2014.2371999
   Law W, ULTRA LOW ATENCY STR
   Li Z, 2014, IEEE J SEL AREA COMM, V32, P719, DOI 10.1109/JSAC.2014.140405
   Mao HZ, 2017, SIGCOMM '17: PROCEEDINGS OF THE 2017 CONFERENCE OF THE ACM SPECIAL INTEREST GROUP ON DATA COMMUNICATION, P197, DOI 10.1145/3098822.3098843
   Mei LF, 2019, LECT NOTES COMPUT SC, V11419, P34, DOI 10.1007/978-3-030-15986-3_3
   Pantos R., 2019, HTTP LIVE STREAMING
   Raca D, 2019, PROCEEDINGS OF THE 10TH ACM MULTIMEDIA SYSTEMS CONFERENCE (ACM MMSYS'19), P201, DOI 10.1145/3304109.3306233
   Riiser Haakon, 2013, P 4 ACM MULT SYST C, P114, DOI DOI 10.1145/2483977.2483991
   Robitza W, 2018, PROCEEDINGS OF THE 9TH ACM MULTIMEDIA SYSTEMS CONFERENCE (MMSYS'18), P466, DOI 10.1145/3204949.3208124
   Smilkov D., 2019, ARXIV190105350
   Sodagar I, 2011, IEEE MULTIMEDIA, V18, P62, DOI 10.1109/MMUL.2011.71
   Spiteri K, 2016, IEEE INFOCOM SER, DOI 10.1109/infocom.2016.7524428
   Sun Y, 2016, PROCEEDINGS OF THE 2016 ACM CONFERENCE ON SPECIAL INTEREST GROUP ON DATA COMMUNICATION (SIGCOMM '16), P272, DOI 10.1145/2934872.2934898
   van der Hooft J, 2016, IEEE COMMUN LETT, V20, P2177, DOI 10.1109/LCOMM.2016.2601087
   Xie XF, 2016, GETMOBILE-MOB COMPU, V20, P31
   Yin XQ, 2015, ACM SIGCOMM COMP COM, V45, P325, DOI 10.1145/2785956.2787486
   Zahran AH, 2018, IEEE T MOBILE COMPUT, V17, P2716, DOI 10.1109/TMC.2018.2825384
   Zou XK, 2015, 16TH INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE' 15), P57, DOI 10.1145/2699343.2699359
NR 39
TC 18
Z9 18
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2588
EP 2601
DI 10.1109/TMM.2020.3013387
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600004
DA 2024-07-18
ER

PT J
AU Chen, WL
   Gu, K
   Zhao, TS
   Jiang, GY
   Le Callet, P
AF Chen, Weiling
   Gu, Ke
   Zhao, Tiesong
   Jiang, Gangyi
   Le Callet, Patrick
TI Semi-Reference Sonar Image Quality Assessment Based on Task and Visual
   Perception
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Sonar measurements; Image quality; Feature extraction;
   Sonar detection; Sonar image; semi-reference; image quality asse-ssment
   (IQA); task-aware quality assessment
AB In submarine and underwater detection tasks, conventional optical imaging and analysis methods are not universally applicable due to the limited penetration depth of visible light. Instead, sonar imaging has become a preferred alternative. However, the capture and transmission conditions in complicated and dynamic underwater environments inevitably lead to visual quality degradation of sonar images, which might also impede further recognition, analysis and understanding. To measure this quality decrease and provide a solid quality indicator for sonar image enhancement, we propose a task- and perception-oriented sonar image quality assessment (TPSIQA) method, in which a semi-reference (SR) approach is applied to adapt to the limited bandwidth of underwater communication channels. In particular, we exploit reduced visual features that are critical for both human perception of and object recognition in sonar images. The final quality indicator is obtained through ensemble learning, which aggregates an optimal subset of multiple base learners to achieve both high accuracy and a high generalization ability. In this way, we are able to develop a compact but generalized quality metric using a small database of sonar images. Experimental results demonstrate competitive performance, high efficiency, and strong robustness of our method compared to the latest available image quality metrics.
C1 [Chen, Weiling; Zhao, Tiesong] Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350108, Peoples R China.
   [Gu, Ke] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Computat Intelligence & Intellige, Fac Informat Technol,Minist Educ,Engn Res Ctr Int, Beijing 100124, Peoples R China.
   [Jiang, Gangyi] Ningbo Univ, Fac Informat Sci & Engn, Ningbo 315211, Peoples R China.
   [Le Callet, Patrick] Univ Nantes, Lab Sci Numer Nantes, Equipe Image Percept & Interact, F-44035 Nantes, France.
C3 Fuzhou University; Beijing University of Technology; Ningbo University;
   Nantes Universite
RP Zhao, TS (corresponding author), Fuzhou Univ, Fujian Key Lab Intelligent Proc & Wireless Transm, Fuzhou 350108, Peoples R China.
EM weiling.chen@fzu.edu.cn; guke.doctor@gmail.com; t.zhao@fzu.edu.cn;
   jianggangyi@nbu.edu.cn; patrick.lecallet@univ-nantes.fr
RI jiang, gang/KII-8233-2024; Weiling, Chen/JAA-9972-2023; Le Callet,
   Patrick/F-5772-2010
FU National Natural Science Foundation of China [61901119, 61671152,
   61931022]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61901119, Grant 61671152, and Grant
   61931022.
CR [Anonymous], 2009, P OCEANS 2009 EUROPE
   [Anonymous], 2016, P IEEE INT C QUAL MU
   Bosse S, 2018, IEEE T IMAGE PROCESS, V27, P206, DOI 10.1109/TIP.2017.2760518
   Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655
   Chen W., 2018, P OCEANS MTS IEEE KO, P1
   Chen W., 2016, IEEE INFOCOM SER, P1
   Chen WL, 2020, IEEE T CIRC SYST VID, V30, P334, DOI 10.1109/TCSVT.2019.2890878
   Chen WL, 2019, IEEE T IMAGE PROCESS, V28, P5336, DOI 10.1109/TIP.2019.2910666
   Chen WL, 2018, IEEE T AERO ELEC SYS, V54, P2776, DOI 10.1109/TAES.2018.2829378
   Chen WL, 2017, IEEE IMAGE PROC, P176, DOI 10.1109/ICIP.2017.8296266
   Demirors E, 2018, IEEE ACCESS, V6, P18602, DOI 10.1109/ACCESS.2018.2815026
   Do MN, 2005, IEEE T IMAGE PROCESS, V14, P2091, DOI 10.1109/TIP.2005.859376
   Dolly SR, 2019, PHYS MED BIOL, V64, DOI 10.1088/1361-6560/ab2dc5
   Dong W, 2019, IEEE ACCESS, V7, P78715, DOI 10.1109/ACCESS.2019.2922011
   Freitas PG, 2018, IEEE T MULTIMEDIA, V20, P3353, DOI 10.1109/TMM.2018.2839529
   Gu K, 2020, IEEE T INSTRUM MEAS, V69, P660, DOI 10.1109/TIM.2019.2905904
   Gu K, 2018, IEEE T IMAGE PROCESS, V27, P394, DOI 10.1109/TIP.2017.2733164
   Gu K, 2017, IEEE T IMAGE PROCESS, V26, P4005, DOI 10.1109/TIP.2017.2711279
   Gu K, 2018, IEEE T NEUR NET LEAR, V29, P1301, DOI 10.1109/TNNLS.2017.2649101
   Gu K, 2017, IEEE T IND ELECTRON, V64, P3903, DOI 10.1109/TIE.2017.2652339
   Gu K, 2016, IEEE T BROADCAST, V62, P446, DOI 10.1109/TBC.2015.2511624
   Gu K, 2016, IEEE T MULTIMEDIA, V18, P1098, DOI 10.1109/TMM.2016.2547343
   Gu K, 2014, IEEE IMAGE PROC, P506, DOI 10.1109/ICIP.2014.7025101
   Gu K, 2016, IEEE T CYBERNETICS, V46, P284, DOI 10.1109/TCYB.2015.2401732
   Gu K, 2015, IEEE T CIRC SYST VID, V25, P1480, DOI 10.1109/TCSVT.2014.2372392
   Kalwa J, 2004, TSI PRESS S, V15, P33
   Kaplan LM, 2009, IEEE J-STSP, V3, P222, DOI 10.1109/JSTSP.2009.2014500
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li QH, 2017, NEUROCOMPUTING, V236, P93, DOI 10.1016/j.neucom.2016.09.105
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Ma KD, 2018, IEEE T IMAGE PROCESS, V27, P1202, DOI 10.1109/TIP.2017.2774045
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Murphy TM, 2007, NEURAL NETWORKS, V20, P851, DOI 10.1016/j.neunet.2007.06.004
   Pan D, 2018, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2018.00667
   Prashnani E, 2018, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2018.00194
   Rahul K, 2019, IET IMAGE PROCESS, V13, P1170, DOI 10.1049/iet-ipr.2018.5496
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Rouse D. M., 2009, P HUM VIS EL IM 14 S, P1
   Rouse DM, 2011, J OPT SOC AM A, V28, P157, DOI 10.1364/JOSAA.28.000157
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Shi ZF, 2018, SIGNAL PROCESS, V145, P99, DOI 10.1016/j.sigpro.2017.11.015
   Soundararajan R, 2012, IEEE T IMAGE PROCESS, V21, P517, DOI 10.1109/TIP.2011.2166082
   Sun W, 2018, IEEE T IMAGE PROCESS, V27, P4232, DOI 10.1109/TIP.2018.2837341
   Tang ZS, 2019, IEEE T BROADCAST, V65, P138, DOI 10.1109/TBC.2018.2871376
   Tarroni G, 2019, IEEE T MED IMAGING, V38, P1127, DOI 10.1109/TMI.2018.2878509
   Wang SQ, 2016, IEEE J EM SEL TOP C, V6, P532, DOI 10.1109/JETCAS.2016.2598756
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Williams DP, 2010, INT CONF ACOUST SPEE, P2114, DOI 10.1109/ICASSP.2010.5495165
   Winter RM, 2018, RADIOTHER ONCOL, V128, P485, DOI 10.1016/j.radonc.2018.04.018
   Wu JJ, 2016, INFORM SCIENCES, V351, P18, DOI 10.1016/j.ins.2016.02.043
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Yan B, 2019, IEEE T MULTIMEDIA, V21, P2603, DOI 10.1109/TMM.2019.2904879
   Zhan YB, 2018, IEEE T MULTIMEDIA, V20, P1796, DOI 10.1109/TMM.2017.2780770
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   [朱敏 Zhu Min], 2014, [科学通报, Chinese Science Bulletin], V59, P3462
   Zhu WH, 2019, IEEE T MULTIMEDIA, V21, P2334, DOI 10.1109/TMM.2019.2902484
NR 58
TC 31
Z9 32
U1 3
U2 60
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1008
EP 1020
DI 10.1109/TMM.2020.2991546
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QO8XK
UT WOS:000623420300014
DA 2024-07-18
ER

PT J
AU Chen, Y
   Zhao, JY
   Shi, CW
   Yuan, DD
AF Chen, Yu
   Zhao, Jieyu
   Shi, Congwei
   Yuan, Dongdong
TI Mesh Convolution: A Novel Feature Extraction Method for 3D Nonrigid
   Object Classification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Shape; Convolution;
   Computational modeling; Feature extraction; Analytical models; 3 d
   nonrigid model; 3 d shape feature; markov chain; mesh convolution;
   spatial co-occurrence information
ID RECOGNITION
AB Applying convolution methods to domains that lack regular underlying structures is a challenging task for 3D vision. Existing methods require the manual design of feature representations suitable for the task or full-voxel-level analysis, which is memory intensive. In this paper, we propose a novel feature extraction method to facilitate 3D nonrigid shape analysis. Our approach, called 3D-MConv, extends convolution operations from regular grids to irregular mesh sets by parametrizing a series of convolutional templates and adopts a novel local perspective to ensure that the algorithm is more invariant against global isometric deformation and articulation. We carefully design the convolutional template as a polynomial function that flexibly represents the local shape. An unsupervised learning method is adopted to learn the convolutional template function. By using the convolution operation and the movement of the template on the model surface, we can obtain the distribution of the typical template shapes. We combine this distribution feature with the spatial co-occurrence information of typical template shapes modelled by Markov chains to form a high-level descriptor of a 3D model. The support vector machine method is used to classify the nonrigid 3D objects. Experiments on SHREC10 and SHREC15 demonstrate that 3D-MConv achieves state-of-the-art accuracy on standard benchmarks.
C1 [Chen, Yu; Zhao, Jieyu; Shi, Congwei; Yuan, Dongdong] Ningbo Univ, Ningbo 315211, Peoples R China.
C3 Ningbo University
RP Zhao, JY (corresponding author), Ningbo Univ, Ningbo 315211, Peoples R China.
EM chenyu_cycy@126.com; zhao_jieyu@nbu.edu.cn; scw_vv@126.com;
   dongdong.ydd@dtwave-inc.com
OI Chen, Yu/0000-0001-6765-9174
FU National Natural Science Foundation of China [61571247]; National
   Natural Science Foundation of Zhejiang Province [LZ16F030001,
   LY17F030002]; International Cooperation Projects of Zhejiang Province
   [2013C24027]; K. C. Wong Magna Fund, Ningbo University
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61571247, in part by the National
   Natural Science Foundation of Zhejiang Province under Grants LZ16F030001
   and LY17F030002, in part by the International Cooperation Projects of
   Zhejiang Province under Grant 2013C24027, and in part by K. C. Wong
   Magna Fund, Ningbo University.
CR Agudo A, 2017, IEEE WINT CONF APPL, P264, DOI 10.1109/WACV.2017.36
   Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396
   Boddapati V, 2017, PROCEDIA COMPUT SCI, V112, P2048, DOI 10.1016/j.procs.2017.08.250
   Bohg J, 2010, ROBOT AUTON SYST, V58, P362, DOI 10.1016/j.robot.2009.10.003
   Bronstein MM, 2010, PROC CVPR IEEE, P1704, DOI 10.1109/CVPR.2010.5539838
   Bruna J., 2013, INT C LEARNING REPRE
   Bu SH, 2014, IEEE T MULTIMEDIA, V16, P2154, DOI 10.1109/TMM.2014.2351788
   Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Defferrard M., 2016, P 30 INT C NEURAL IN, V29, P3844
   Gao ZH, 2014, COMPUT AIDED DESIGN, V53, P62, DOI 10.1016/j.cad.2014.03.008
   Garcia A, 2017, APPR DIGIT GAME STUD, V5, P1
   Grabner A, 2018, PROC CVPR IEEE, P3022, DOI 10.1109/CVPR.2018.00319
   Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005
   Han ZZ, 2017, IEEE T IMAGE PROCESS, V26, P3707, DOI 10.1109/TIP.2017.2704426
   Henaff M., 2015, ARXIV150605163
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Kalogerakis E, 2017, PROC CVPR IEEE, P6630, DOI 10.1109/CVPR.2017.702
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Leng B, 2015, NEUROCOMPUTING, V151, P593, DOI 10.1016/j.neucom.2014.06.084
   Li HB, 2017, IEEE T MULTIMEDIA, V19, P2816, DOI 10.1109/TMM.2017.2713408
   Li JW, 2008, PROCEEDINGS OF THE 2008 CHINESE CONFERENCE ON PATTERN RECOGNITION (CCPR 2008), P1
   Li YY, 2018, ADV NEUR IN, V31
   Lindeberg T., 2012, SCHOLARPEDIA, V7, P10491, DOI [10.4249/scholarpedia.10491, DOI 10.4249/SCHOLARPEDIA.10491]
   Ma C, 2019, IEEE T MULTIMEDIA, V21, P1169, DOI 10.1109/TMM.2018.2875512
   Mahmoudi M, 2009, GRAPH MODELS, V71, P22, DOI 10.1016/j.gmod.2008.10.002
   Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481
   Ni B., 2009, IEEE COMP SOC C COMP
   Owen M, 2011, IEEE ACM T COMPUT BI, V8, P2, DOI 10.1109/TCBB.2010.3
   Qi C.R., 2017, ABS170602413 CORR, P5099
   Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609
   Reuter M, 2006, COMPUT AIDED DESIGN, V38, P342, DOI 10.1016/j.cad.2005.10.011
   Rustamov Raif M, 2007, P S GEOM PROC, V257, P225
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Sipiran I, 2011, VISUAL COMPUT, V27, P963, DOI 10.1007/s00371-011-0610-y
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Sun JA, 2009, COMPUT GRAPH FORUM, V28, P1383, DOI 10.1111/j.1467-8659.2009.01515.x
   Wei LY, 2016, PROC CVPR IEEE, P1544, DOI 10.1109/CVPR.2016.171
   Wu ZR, 2015, PROC CVPR IEEE, P1912, DOI 10.1109/CVPR.2015.7298801
   Xu Y, 2018, ADV SOC SCI EDUC HUM, V284, P87
   Yi L., 2016, ABS161200606 CORR
   Zhao Y, 2015, IEEE INTEL TRANSP SY, V7, P29, DOI 10.1109/MITS.2015.2427366
   Zhao YH, 2019, PROC CVPR IEEE, P1009, DOI 10.1109/CVPR.2019.00110
NR 43
TC 14
Z9 16
U1 2
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3098
EP 3111
DI 10.1109/TMM.2020.3020693
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UU6IS
UT WOS:000698902000012
DA 2024-07-18
ER

PT J
AU Huang, NAC
   Liu, Y
   Zhang, Q
   Han, JG
AF Huang, Nianchang
   Liu, Yi
   Zhang, Qiang
   Han, Jungong
TI Joint Cross-Modal and Unimodal Features for RGB-D Salient Object
   Detection
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Saliency detection; Object detection; Computational
   modeling; Task analysis; Computer vision; Visualization; RGB-D; saliency
   detection; multi-branch feature fusion and feature selection
ID NETWORK
AB RGB-D salient object detection is one of the basic tasks in computer vision. Most existing models focus on investigating efficient ways of fusing the complementary information from RGB and depth images for better saliency detection. However, for many real-life cases, where one of the input images has poor visual quality or contains affluent saliency cues, fusing cross-modal features does not help to improve the detection accuracy, when compared to using unimodal features only. In view of this, a novel RGB-D salient object detection model is proposed by simultaneously exploiting the cross-modal features from the RGB-D images and the unimodal features from the input RGB and depth images for saliency detection. To this end, a Multi-branch Feature Fusion Module is presented to effectively capture the cross-level and cross-modal complementary information between RGB-D images, as well as the cross-level unimodal features from the RGB images and the depth images separately. On top of that, a Feature Selection Module is designed to adaptively select those highly discriminative features for the final saliency prediction from the fused cross-modal features and the unimodal features. Extensive evaluations on four benchmark datasets demonstrate that the proposed model outperforms the state-of-the-art approaches by a large margin.
C1 [Huang, Nianchang; Liu, Yi; Zhang, Qiang] Xidian Univ, Key Lab Elect Equipment Struct Design, Minist Educ, Xian 710071, Shaanxi, Peoples R China.
   [Huang, Nianchang; Liu, Yi; Zhang, Qiang] Xidian Univ, Sch Mechano Elect Engn, Ctr Complex Syst, Xian 710071, Shaanxi, Peoples R China.
   [Han, Jungong] Aberystwyth Univ, Comp Sci Dept, Aberystwyth SY23 3FL, Dyfed, Wales.
C3 Xidian University; Xidian University; Aberystwyth University
RP Zhang, Q (corresponding author), Xidian Univ, Key Lab Elect Equipment Struct Design, Minist Educ, Xian 710071, Shaanxi, Peoples R China.; Zhang, Q (corresponding author), Xidian Univ, Sch Mechano Elect Engn, Ctr Complex Syst, Xian 710071, Shaanxi, Peoples R China.; Han, JG (corresponding author), Aberystwyth Univ, Comp Sci Dept, Aberystwyth SY23 3FL, Dyfed, Wales.
EM nchuang@stu.xidian.edu.cn; liuyixd@xidian.edu.cn; qzhang@xidian.edu.cn;
   jungonghan77@gmail.com
RI Han, Jungong/ABE-6812-2020
OI Nianchang, Huang/0000-0001-9530-3490
FU National Natural Science Foundation of China [61773301, 61876140]; China
   Postdoctoral Support Scheme for Innovative Talents [BX20180236]
FX This work is supported in part by the National Natural Science
   Foundation of China under Grants 61773301 and 61876140 and in part by
   the China Postdoctoral Support Scheme for Innovative Talents under Grant
   BX20180236.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   Borji A, 2019, COMPUT VIS MEDIA, V5, P117, DOI 10.1007/s41095-019-0149-9
   Cao Y, 2019, IEEE INT CONF COMP V, P1971, DOI 10.1109/ICCVW.2019.00246
   Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104
   Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322
   Chen L, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278067
   Chen Liang-Chieh, 2014, Comput Sci, DOI DOI 10.48550/ARXIV.1412.7062
   Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   FAN DP, ARXIV190706781
   Fu H, 2018, PROC CVPR IEEE, P2002, DOI 10.1109/CVPR.2018.00214
   Fu KR, 2019, IEEE T MULTIMEDIA, V21, P457, DOI 10.1109/TMM.2018.2859746
   Glorot X., 2010, INT C ARTIFICIAL INT, P249
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hong S, 2015, PR MACH LEARN RES, V37, P597
   Hu J., IEEE Trans. Pattern Anal. Mach. Intell., V1, P1
   I. Realsense, 2020, INTR INT REALS LID C
   Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222
   Kroner A, 2020, NEURAL NETWORKS, V129, P261, DOI 10.1016/j.neunet.2020.05.004
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Li GB, 2017, PROC CVPR IEEE, P247, DOI 10.1109/CVPR.2017.34
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Liu Y, 2020, IEEE T IMAGE PROCESS, V29, P360, DOI 10.1109/TIP.2019.2930906
   Liu Y, 2019, IEEE T CIRC SYST VID, V29, P1023, DOI 10.1109/TCSVT.2018.2823769
   Liu ZY, 2019, NEUROCOMPUTING, V363, P46, DOI 10.1016/j.neucom.2019.07.012
   Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698
   Nair V., 2010, P 27 INT C MACHINE L, P807
   Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708
   Oh SJ, 2017, PROC CVPR IEEE, P5038, DOI 10.1109/CVPR.2017.535
   Paszke A, 2019, ADV NEUR IN, V32
   Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7
   Pingping Zhang, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P202, DOI 10.1109/ICCV.2017.31
   Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981
   Ren ZX, 2014, IEEE T CIRC SYST VID, V24, P769, DOI 10.1109/TCSVT.2013.2280096
   Shigematsu R, 2017, IEEE INT CONF COMP V, P2749, DOI 10.1109/ICCVW.2017.323
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sutskever I., 2013, INT C MACHINE LEARNI, P1139
   Tang J., 2018, ARXIV181110763
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Wang AZ, 2017, IEEE SIGNAL PROC LET, V24, P663, DOI 10.1109/LSP.2017.2688136
   Wang L, 2018, IEEE IPCCC
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang NN, 2019, IEEE ACCESS, V7, P55277, DOI 10.1109/ACCESS.2019.2913107
   Wang TT, 2018, PROC CVPR IEEE, P3127, DOI 10.1109/CVPR.2018.00330
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie Q, 2018, IEEE T MULTIMEDIA, V20, P580, DOI 10.1109/TMM.2017.2751965
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie SN, 2017, INT J COMPUT VISION, V125, P3, DOI 10.1007/s11263-017-1004-z
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Zhang L, 2018, PROC CVPR IEEE, P1741, DOI 10.1109/CVPR.2018.00187
   Zhao J, 2020, IEEE T FUZZY SYST, V28, P2287, DOI 10.1109/TFUZZ.2019.2930492
   Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731
   Zhu CB, 2019, IEEE INT CON MULTI, P199, DOI 10.1109/ICME.2019.00042
NR 57
TC 25
Z9 25
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2428
EP 2441
DI 10.1109/TMM.2020.3011327
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800021
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Lai, QX
   Khan, S
   Nie, YW
   Sun, HQ
   Shen, JB
   Shao, L
AF Lai, Qiuxia
   Khan, Salman
   Nie, Yongwei
   Sun, Hanqiu
   Shen, Jianbing
   Shao, Ling
TI Understanding More About Human and Machine Attention in Deep Neural
   Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Task analysis; Visualization; Computer vision; Neural networks; Object
   segmentation; Image recognition; Reliability; Attention mechanism; human
   attention; artificial attention; deep learning
ID SALIENT OBJECT DETECTION; VISUAL-ATTENTION; BOTTOM-UP; MODEL
AB Human visual system can selectively attend to parts of a scene for quick perception, a biological mechanism known as Human attention. Inspired by this, recent deep learning models encode attention mechanisms to focus on the most task-relevant parts of the input signal for further processing, which is called Machine/Neural/Artificial attention. Understanding the relation between human and machine attention is important for interpreting and designing neural networks. Many works claim that the attention mechanism offers an extra dimension of interpretability by explaining where the neural networks look. However, recent studies demonstrate that artificial attention maps do not always coincide with common intuition. In view of these conflicting evidence, here we make a systematic study on using artificial attention and human attention in neural network design. With three example computer vision tasks (i.e., salient object segmentation, video action recognition, and fine-grained image classification), diverse representative backbones (i.e., AlexNet, VGGNet, ResNet) and famous architectures (i.e., Two-stream, FCN), corresponding real human gaze data, and systematically conducted large-scale quantitative studies, we quantify the consistency between artificial attention and human visual attention and offer novel insights into existing artificial attention mechanisms by giving preliminary answers to several key questions related to human and artificial attention mechanisms. Overall results demonstrate that human attention can benchmark the meaningful 'ground-truth' in attention-driven tasks, where the more the artificial attention is close to human attention, the better the performance; for higher-level vision tasks, it is case-by-case. It would be advisable for attention-driven tasks to explicitly force a better alignment between artificial and human attention to boost the performance; such alignment would also improve the network explainability for higher-level computer vision tasks.
C1 [Lai, Qiuxia] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Khan, Salman] Mohamed Bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Nie, Yongwei] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Sun, Hanqiu] Univ Elect Sci & Technol China, Chengdu 610051, Peoples R China.
   [Shen, Jianbing; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.
C3 Chinese University of Hong Kong; Mohamed Bin Zayed University of
   Artificial Intelligence; South China University of Technology;
   University of Electronic Science & Technology of China
RP Nie, YW (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
EM qxlai@cse.cuhk.edu.hk; salman.khan@anu.edu.au; nieyongwei@scut.edu.cn;
   sunnyqiu24@gmail.com; shenjianbingcg@gmail.com; ling.shao@ieee.org
RI Shao, Ling/D-3535-2011; Lai, Qiuxia/HNI-8353-2023; Khan, Salman
   Hameed/M-4834-2016
OI Lai, Qiuxia/0000-0001-6872-5540; Khan, Salman
   Hameed/0000-0002-9502-1749; Shen, Jianbing/0000-0002-4109-8353
FU Natural Science Foundation of Guangdong Province, China
   [2019A1515010860]; Fundamental Research Funds for the Central
   Universities [D2190670]; Science and Technology Project of Guangzhou
   City [201707010140]
FX This work was supported in part by the Natural Science Foundation of
   Guangdong Province, China, under Grant 2019A1515010860, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   D2190670, and in part by the Science and Technology Project of Guangzhou
   City under Grant 201707010140.
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2017, ADV NEURAL INFORM PR
   [Anonymous], 2010, CNSTR2010001 CALTECH
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   [Anonymous], 2015, P INT C LEARN REPR
   [Anonymous], 2018, ECCV
   Bahdanau Dzmitry, 2015, P 3 INT C LEARN REPR
   Biparva M, 2017, IEEE INT CONF COMP V, P2715, DOI 10.1109/ICCVW.2017.319
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Borji A, 2013, IEEE I CONF COMP VIS, P921, DOI 10.1109/ICCV.2013.118
   Bruce N., 2006, P ADV NEUR INF PROC, P155
   Bylinskii Z, 2019, IEEE T PATTERN ANAL, V41, P740, DOI 10.1109/TPAMI.2018.2815601
   Bylinskii Z, 2016, LECT NOTES COMPUT SC, V9909, P809, DOI 10.1007/978-3-319-46454-1_49
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Carrasco M, 2011, VISION RES, V51, P1484, DOI 10.1016/j.visres.2011.04.012
   Chen SH, 2018, LECT NOTES COMPUT SC, V11213, P236, DOI 10.1007/978-3-030-01240-3_15
   Connor CE, 2004, CURR BIOL, V14, pR850, DOI 10.1016/j.cub.2004.09.041
   Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   ERIKSEN CW, 1972, PERCEPT PSYCHOPHYS, V12, P201, DOI 10.3758/BF03212870
   Farazi M. M., 2018, P BRIT MACH VIS C
   Gao D., 2005, P ADV NEUR INF PROC, P481
   Goodfellow Ian., 2015, STAT-US
   Hadizadeh H, 2012, IEEE T IMAGE PROCESS, V21, P898, DOI 10.1109/TIP.2011.2165292
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hwang AD, 2009, J VISION, V9, DOI 10.1167/9.5.25
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Jetley S., 2018, P INT C LEARN REPR
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710
   Judd T., 2012, MIT CSAIL TR
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Karessli N, 2017, PROC CVPR IEEE, P6412, DOI 10.1109/CVPR.2017.679
   Katsuki F, 2014, NEUROSCIENTIST, V20, P509, DOI 10.1177/1073858413514136
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koch K, 2006, CURR BIOL, V16, P1428, DOI 10.1016/j.cub.2006.05.056
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kümmerer M, 2015, P NATL ACAD SCI USA, V112, P16054, DOI 10.1073/pnas.1510393112
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li J, 2018, IEEE T IMAGE PROCESS, V27, P349, DOI 10.1109/TIP.2017.2762594
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lipton ZC, 2018, COMMUN ACM, V61, P36, DOI 10.1145/3233231
   Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326
   Liu XL, 2017, IEEE T IMAGE PROCESS, V26, P5324, DOI 10.1109/TIP.2017.2729896
   Lu JS, 2016, ADV NEUR IN, V29
   Marszalek M, 2009, PROC CVPR IEEE, P2921, DOI 10.1109/CVPRW.2009.5206557
   Mathe S, 2015, IEEE T PATTERN ANAL, V37, P1408, DOI 10.1109/TPAMI.2014.2366154
   Mital PK, 2011, COGN COMPUT, V3, P5, DOI 10.1007/s12559-010-9074-z
   Mnih V., 2014, Neural Information Processing Systems, P2204
   Navalpakkam Vidhya, 2006, Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit, V2, P2049, DOI DOI 10.1109/CVPR.2006.54
   Pinto Y, 2013, J VISION, V13, DOI 10.1167/13.3.16
   Rauber J., 2017, P REL MACH LEARN WIL, DOI DOI 10.21105/JOSS.02607
   Rodriguez MD, 2008, PROC CVPR IEEE, P3001, DOI 10.1109/cvpr.2008.4587727
   Rush AlexanderM., 2015, P 2015 C EMP METH NA
   Sharma G, 2012, PROC CVPR IEEE, P3506, DOI 10.1109/CVPR.2012.6248093
   Sharma S, 2016, IEEE INT WORK SIGN P
   Simonyan K., 2014, CORR
   Simonyan K, 2014, ADV NEUR IN, V27
   Simonyan Karen, 2014, WORKSH P INT C LEARN
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Wang W, ARXIV190409146, V2019
   Wang WG, 2019, IEEE I CONF COMP VIS, P9235, DOI 10.1109/ICCV.2019.00933
   Wang WG, 2019, PROC CVPR IEEE, P1448, DOI 10.1109/CVPR.2019.00154
   Wang WG, 2019, PROC CVPR IEEE, P3059, DOI 10.1109/CVPR.2019.00318
   Wang WG, 2018, PROC CVPR IEEE, P4271, DOI 10.1109/CVPR.2018.00449
   Wang WG, 2018, PROC CVPR IEEE, P4894, DOI 10.1109/CVPR.2018.00514
   Wang WG, 2018, PROC CVPR IEEE, P1711, DOI 10.1109/CVPR.2018.00184
   Wang WG, 2018, IEEE T IMAGE PROCESS, V27, P2368, DOI 10.1109/TIP.2017.2787612
   Wang WG, 2018, IEEE T PATTERN ANAL, V40, P20, DOI 10.1109/TPAMI.2017.2662005
   Wang Y., 2016, P 2016 C EMPIRICAL M, P606, DOI 10.18653/v1/D16-1058
   WOLFE JM, 1989, J EXP PSYCHOL HUMAN, V15, P419, DOI 10.1037/0096-1523.15.3.419
   Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1
   Xie CH, 2019, IEEE I CONF COMP VIS, P8857, DOI 10.1109/ICCV.2019.00895
   Xu D, 2018, PROC CVPR IEEE, P3917, DOI 10.1109/CVPR.2018.00412
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zagoruyko S., 2017, P INT C LEARN REPR
   Zhai ML, 2020, IEEE T CIRC SYST VID, V30, P3663, DOI 10.1109/TCSVT.2019.2943140
   Zhang JM, 2018, INT J COMPUT VISION, V126, P1084, DOI 10.1007/s11263-017-1059-x
   Zhang P., 2018, CORR, P135
   Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
   Zhu C., 2018, P EUR C COMP VIS, P136
NR 91
TC 34
Z9 34
U1 3
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2086
EP 2099
DI 10.1109/TMM.2020.3007321
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TC8FT
UT WOS:000668875100019
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, J
   Huo, HT
   Li, C
   Wang, RH
   Feng, Q
AF Li, Jing
   Huo, Hongtao
   Li, Chang
   Wang, Renhua
   Feng, Qi
TI AttentionFGAN: Infrared and Visible Image Fusion Using Attention-Based
   Generative Adversarial Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Attention mechanism; generative adversarial networks; infrared and
   visible image fusion
ID MULTI-FOCUS; TRANSFORM
AB Infrared and visible image fusion aims to describe the same scene from different aspects by combining complementary information of multi-modality images. The existing Generative adversarial networks (GAN) based infrared and visible image fusion methods cannot perceive the most discriminative regions, and hence fail to highlight the typical parts existing in infrared and visible images. To this end, we integrate multi-scale attention mechanism into both generator and discriminator of GAN to fuse infrared and visible images (AttentionFGAN). The multi-scale attention mechanism aims to not only capture comprehensive spatial information to help generator focus on the foreground target information of infrared image and background detail information of visible image, but also constrain the discriminators focus more on the attention regions rather than the whole input image. The generator of AttentionFGAN consists of two multi-scale attention networks and an image fusion network. Two multi-scale attention networks capture the attention maps of infrared and visible images respectively, so that the fusion network can reconstruct the fused image by paying more attention to the typical regions of source images. Besides, two discriminators are adopted to force the fused result keep more intensity and texture information from infrared and visible image respectively. Moreover, to keep more information of attention region from source images, an attention loss function is designed. Finally, the ablation experiments illustrate the effectiveness of the key parts of our method, and extensive qualitative and quantitative experiments on three public datasets demonstrate the advantages and effectiveness of AttentionFGAN compared with the other state-of-the-art methods.
C1 [Li, Jing; Huo, Hongtao; Wang, Renhua] Peoples Publ Secur Univ China, Dept Informat Technol & Cyber Secur, Beijing 100038, Peoples R China.
   [Li, Chang] Hefei Univ Technol, Dept Biomed Engn, Hefei 230009, Peoples R China.
   [Feng, Qi] Peoples Publ Secur Univ China, Remote Sensing Ctr Publ Secur, Beijing 100038, Peoples R China.
C3 People's Public Security University of China; Hefei University of
   Technology; People's Public Security University of China
RP Huo, HT (corresponding author), Peoples Publ Secur Univ China, Dept Informat Technol & Cyber Secur, Beijing 100038, Peoples R China.
EM lijing2017@126.com; huohongtao@ppsuc.edu.cn; changli@hfut.edu.cn;
   renhuawang@163.com; fengqi@ppsuc.edu.cn
RI Huo, Hongtao/AAN-8968-2020; Wang, Renhua/ABE-4789-2020
OI Li, Jing/0000-0003-0716-5329; , Huo/0000-0002-1552-4400
FU Key Program of High-Resolution Earth Observation System
   [GFZX0404130307]; NationalNatural Science Foundation ofChina [41901350];
   Fundamental Research Funds of People's Public Security University of
   China [2019JKF330]; Fundamental Research Funds for the Central
   Universities [JZ2019HGBZ0151]
FX This work was supported in part by the Key Program of High-Resolution
   Earth Observation System under Grant GFZX0404130307, in part by the
   NationalNatural Science Foundation ofChina underGrant 41901350, in part
   by the Fundamental Research Funds of People's Public Security University
   of China under Grant 2019JKF330, and in part by the Fundamental Research
   Funds for the Central Universities under Grant JZ2019HGBZ0151. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Jingdong Wang.
CR Arjovsky M, 2017, PR MACH LEARN RES, V70
   Bahdanau D, 2016, Arxiv, DOI [arXiv:1409.0473, 10.48550/arXiv.1409.0473]
   BURT PJ, 1983, IEEE T COMMUN, V31, P532, DOI 10.1109/TCOM.1983.1095851
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chaudhari S, 2021, ACM T INTEL SYST TEC, V12, DOI 10.1145/3465055
   Chipman LJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC248
   Cui GM, 2015, OPT COMMUN, V341, P199, DOI 10.1016/j.optcom.2014.12.032
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   Hu HM, 2017, IEEE T MULTIMEDIA, V19, P2706, DOI 10.1109/TMM.2017.2711422
   Gulrajani I, 2017, ADV NEUR IN, V30
   Lewis JJ, 2007, INFORM FUSION, V8, P119, DOI 10.1016/j.inffus.2005.09.006
   Li H, 2018, INT C PATT RECOG, P2705, DOI 10.1109/ICPR.2018.8546006
   Li H, 2019, IEEE T IMAGE PROCESS, V28, P2614, DOI 10.1109/TIP.2018.2887342
   Li J, 2020, IEEE T MULTIMEDIA, V22, P2990, DOI 10.1109/TMM.2020.2965434
   Li LJ, 2019, IEEE I CONF COMP VIS, P10312, DOI 10.1109/ICCV.2019.01041
   Li QL, 2021, IEEE SENS J, V21, P7458, DOI 10.1109/JSEN.2019.2921803
   Li ST, 2017, INFORM FUSION, V33, P100, DOI 10.1016/j.inffus.2016.05.004
   Liu CH, 2017, INFRARED PHYS TECHN, V83, P94, DOI 10.1016/j.infrared.2017.04.018
   Liu Y, 2018, INT J WAVELETS MULTI, V16, DOI 10.1142/S0219691318500182
   Liu Y, 2018, INFORM FUSION, V42, P158, DOI 10.1016/j.inffus.2017.10.007
   Liu Y, 2015, INFORM FUSION, V24, P147, DOI 10.1016/j.inffus.2014.09.004
   Lu JS, 2016, ADV NEUR IN, V29
   Ma JY, 2019, INFORM FUSION, V48, P11, DOI 10.1016/j.inffus.2018.09.004
   Ma JY, 2019, INFORM FUSION, V45, P153, DOI 10.1016/j.inffus.2018.02.004
   Ma JY, 2016, INFORM FUSION, V31, P100, DOI 10.1016/j.inffus.2016.02.001
   Ma YK, 2018, AAAI CONF ARTIF INTE, P5876
   Mitianoudis N, 2007, INFORM FUSION, V8, P131, DOI 10.1016/j.inffus.2005.09.001
   Mou J, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P1046, DOI 10.1109/CISP.2013.6745210
   Naidu VPS, 2011, DEFENCE SCI J, V61, P479, DOI 10.14429/dsj.61.705
   Nencini F, 2007, INFORM FUSION, V8, P143, DOI 10.1016/j.inffus.2006.02.001
   Pumarola A, 2018, LECT NOTES COMPUT SC, V11214, P835, DOI 10.1007/978-3-030-01249-6_50
   Qu GH, 2002, ELECTRON LETT, V38, P313, DOI 10.1049/el:20020212
   Roberts JW, 2008, J APPL REMOTE SENS, V2, DOI 10.1117/1.2945910
   Song Y, 2019, ARXIV190209314
   TOET A, 1989, PATTERN RECOGN LETT, V9, P245, DOI 10.1016/0167-8655(89)90003-2
   Wang, 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1601.06823
   Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683
   Xu H, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3954
   Zagoruyko S., 2016, ARXIV160507146, DOI DOI 10.5244/C.30.87
   Zhang Q, 2018, INFORM FUSION, V40, P57, DOI 10.1016/j.inffus.2017.05.006
   Zhang XY, 2017, J OPT SOC AM A, V34, P1400, DOI 10.1364/JOSAA.34.001400
   Zhao WD, 2018, IEEE T MULTIMEDIA, V20, P866, DOI 10.1109/TMM.2017.2760100
   Zheng Y, 2004, P SOC PHOTO-OPT INS, V5298, P177, DOI 10.1117/12.523966
   Zhu ZQ, 2018, INFORM SCIENCES, V432, P516, DOI 10.1016/j.ins.2017.09.010
NR 45
TC 162
Z9 173
U1 41
U2 182
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 1383
EP 1396
DI 10.1109/TMM.2020.2997127
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA RU3SE
UT WOS:000645068200016
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Liu, K
   Gao, L
   Khan, NM
   Qi, L
   Guan, L
AF Liu, Kai
   Gao, Lei
   Khan, Naimul Mefraz
   Qi, Lin
   Guan, Ling
TI A Multi-Stream Graph Convolutional Networks-Hidden Conditional Random
   Field Model for Skeleton-Based Action Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Feature extraction; Convolution; Adaptation models; Neural networks;
   Bones; Message passing; GCN; CRF; skeleton; hidden part state; action
   recognition
ID FEATURES
AB Recently, Graph Convolutional Network(GCN) methods for skeleton-based action recognition have achieved great success due to their ability to preserve structural information of the skeleton. However, these methods abandon the structural information in the classification stage by employing traditional fully-connected layers and softmax classifier, leading to sub-optimal performance. In this work, a novel Graph Convolutional Networks-Hidden conditional Random Field (GCN-HCRF) model is proposed to solve this problem. The proposed method combines GCN with HCRF to retain the human skeleton structure information even during the classification stage. Our model is trained end-to-end by utilizing the message passing from the belief propagation algorithm on the human structure graph. To further capture spatial and temporal information, we propose a multi-stream framework which takes the relative coordinate of the joints and bone direction as two static feature streams, and the temporal displacements between two consecutive frames as the dynamic feature stream. Experimental results on three challenging benchmarks (NTU RGB+D, N-UCLA, SYSU) show the superior performance of the proposed model over state-of-the-art models.
C1 [Liu, Kai; Qi, Lin] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
   [Gao, Lei; Khan, Naimul Mefraz; Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON M5B 2K3, Canada.
C3 Zhengzhou University; Toronto Metropolitan University
RP Qi, L (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450001, Peoples R China.
EM 09liukai08@gmail.com; iegaolei@gmail.com; n77khan@ee.ryerson.ca;
   ielqi@zzu.edu.cn; lguan@ee.ryerson.ca
OI Liu, Kai/0000-0001-6301-7756; Gao, Lei/0000-0001-5583-713X
FU NSFC-Henan Joint Fund [U1804152]
FX This work was supported by NSFC-Henan Joint Fund under Grant U1804152.
CR [Anonymous], EFFICIENT INFERENCE
   [Anonymous], 2019, PERSONAL UBIQUITOUS
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2018, P BRIT MACH VIS C BM
   Chang JY, 2016, IEEE T PATTERN ANAL, V38, P1612, DOI 10.1109/TPAMI.2016.2519021
   Du Y, 2016, IEEE T IMAGE PROCESS, V25, P3010, DOI 10.1109/TIP.2016.2552404
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Fan ZX, 2019, IEEE T MULTIMEDIA, V21, P363, DOI 10.1109/TMM.2018.2859620
   Gao XY, 2019, CONF REC ASILOMAR C, P930, DOI [10.1109/IEEECONF44664.2019.9048939, 10.1109/ieeeconf44664.2019.9048939]
   Hu G., 2019, P IEEE INT EL DEV M, P1, DOI [10.1109/IEDM19573.2019.8993604, DOI 10.1109/IEDM19573.2019.8993604]
   Hu JF, 2017, IEEE T PATTERN ANAL, V39, P2186, DOI 10.1109/TPAMI.2016.2640292
   Hu JF, 2016, LECT NOTES COMPUT SC, V9905, P280, DOI 10.1007/978-3-319-46448-0_17
   Kim TS, 2017, IEEE COMPUT SOC CONF, P1623, DOI 10.1109/CVPRW.2017.207
   Lafferty John, 2001, INT C MACH LEARN ICM
   Lee I, 2017, IEEE I CONF COMP VIS, P1012, DOI 10.1109/ICCV.2017.115
   Lei J, 2016, 2016 INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2016), P63, DOI 10.1109/ICIVC.2016.7571275
   Li C., 2018, PROC INT JOINT C ART, P1
   Li M, 2016, IEEE T MULTIMEDIA, V18, P2293, DOI 10.1109/TMM.2016.2614228
   Liu CH, 2016, PATTERN RECOGN, V59, P213, DOI 10.1016/j.patcog.2016.03.019
   Liu J, 2016, LECT NOTES COMPUT SC, V9907, P816, DOI 10.1007/978-3-319-46487-9_50
   Liu MY, 2017, PATTERN RECOGN, V68, P346, DOI 10.1016/j.patcog.2017.02.030
   Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124
   Raptis M, 2012, PROC CVPR IEEE, P1242, DOI 10.1109/CVPR.2012.6247807
   Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115
   Shi L, 2019, PROC CVPR IEEE, P12018, DOI 10.1109/CVPR.2019.01230
   Si CY, 2019, PROC CVPR IEEE, P1227, DOI 10.1109/CVPR.2019.00132
   Sigurdsson JV, 2017, VIKING FRIENDSHIP: THE SOCIAL BOND IN ICELAND AND NORWAY, C. 900-1300, P1
   Simonyan K, 2014, ADV NEUR IN, V27
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Tang YS, 2018, PROC CVPR IEEE, P5323, DOI 10.1109/CVPR.2018.00558
   Vemulapalli R, 2014, PROC CVPR IEEE, P588, DOI 10.1109/CVPR.2014.82
   Wang HS, 2018, IEEE T IMAGE PROCESS, V27, P4382, DOI 10.1109/TIP.2018.2837386
   Wang J, 2014, IEEE T PATTERN ANAL, V36, P914, DOI 10.1109/TPAMI.2013.198
   Wang KZ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P97, DOI 10.1145/2647868.2654912
   Wang S.B., 2006, P IEEE COMP SOC C CO
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang Y, 2011, IEEE T PATTERN ANAL, V33, P1310, DOI 10.1109/TPAMI.2010.214
   Xu LC, 2013, IEEE ASME INT C ADV, P26, DOI 10.1109/AIM.2013.6584063
   Xu M, 2017, IEEE INT CON MULTI, P517, DOI 10.1109/ICME.2017.8019351
   Yan SJ, 2018, AAAI CONF ARTIF INTE, P7444
   Yang YH, 2017, IEEE T MULTIMEDIA, V19, P519, DOI 10.1109/TMM.2016.2626959
   Yedidia J.S., 2003, EXPLORING ARTIFICIAL, P239, DOI DOI 10.5555/779343.779352
   Zhang P., 2018, CORR, P135
   Zhang P.-F., 2020, P IEEE WIR COMM NETW, P1
   Zhang PF, 2019, IEEE T PATTERN ANAL, V41, P1963, DOI 10.1109/TPAMI.2019.2896631
   Zhang PF, 2017, IEEE I CONF COMP VIS, P2136, DOI [10.1109/ICCV.2017.233, 10.1109/ICCV.2017.231]
   Zhang SY, 2018, IEEE T MULTIMEDIA, V20, P2330, DOI 10.1109/TMM.2018.2802648
   Zhang YP, 2012, IEEE VTS VEH TECHNOL
   Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179
NR 49
TC 43
Z9 43
U1 4
U2 35
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 64
EP 76
DI 10.1109/TMM.2020.2974323
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600006
DA 2024-07-18
ER

PT J
AU Liu, L
   Feng, G
   Beautemps, D
   Zhang, XP
AF Liu, Li
   Feng, Gang
   Beautemps, Denis
   Zhang, Xiao-Ping
TI Re-Synchronization Using the Hand Preceding Model for Multi-Modal Fusion
   in Automatic Continuous Cued Speech Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Lips; Shape; Feature extraction; Hidden Markov models; Speech
   recognition; Organizations; Encoding; Cued speech; multi-modal fusion;
   re-synchronization procedure; automatic CS recognition; CNN; MSHMM
ID RGB-D SLAM; MOTION REMOVAL
AB Cued Speech (CS) is an augmented lip reading system complemented by hand coding, and it is very helpful to the deaf people. Automatic CS recognition can help communications between the deaf people and others. Due to the asynchronous nature of lips and hand movements, fusion of them in automatic CS recognition is a challenging problem. In this work, we propose a novel re-synchronization procedure for multi-modal fusion, which aligns the hand features with lips feature. It is realized by delaying hand position and hand shape with their optimal hand preceding time which is derived by investigating the temporal organizations of hand position and hand shape movements in CS. This re-synchronization procedure is incorporated into a practical continuous CS recognition system that combines convolutional neural network (CNN) with multi-stream hidden markov model (MSHMM). A significant improvement of about 4.6% has been achieved retaining 76.6% CS phoneme recognition correctness compared with the state-of-the-art architecture (72.04%), which did not take into account the asynchrony issue of multi-modal fusion in CS. To our knowledge, this is the first work to tackle the asynchronous multi-modal fusion in the automatic continuous CS recognition.
C1 [Liu, Li] Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.
   [Feng, Gang; Beautemps, Denis] Univ Grenoble Alpes, Grenoble INP, CNRS, GIPSA Lap, F-38000 Grenoble, France.
   [Zhang, Xiao-Ping] Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
C3 Shenzhen Research Institute of Big Data; Communaute Universite Grenoble
   Alpes; Institut National Polytechnique de Grenoble; Centre National de
   la Recherche Scientifique (CNRS); Universite Grenoble Alpes (UGA);
   Toronto Metropolitan University
RP Liu, L (corresponding author), Shenzhen Res Inst Big Data, Shenzhen 518172, Peoples R China.; Zhang, XP (corresponding author), Ryerson Univ, Dept Elect Comp & Biomed Engn, Toronto, ON M5B 2K3, Canada.
EM liliu.math@gmail.com; gang.feng@gipsa-lab.grenoble-inp.fr;
   Denis.Beautemps@gipsa-lab.grenoble-inp.fr; xzhang@ee.ryerson.ca
RI Zhang, Xiao-Ping (Steven)/B-1436-2016
OI Zhang, Xiao-Ping (Steven)/0000-0001-5241-0069; Beautemps,
   Denis/0000-0001-9625-3018
FU Universite Grenoble Alpes in France; Natural Sciences and Engineering
   Research Council of Canada [RGPIN239031]
FX This work was supported in part by the Ph.D thesis grant of Universite
   Grenoble Alpes in France and in part by the Natural Sciences and
   Engineering Research Council of Canada under Grant RGPIN239031. Part of
   this work has been presented in conference Eusipco 2019. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. M. Shamim Hossain.
CR Aboutabit N., 2006, P IEEE INT C AC SPEE, V1
   Aboutabit N., 2007, THESIS
   Abreu DV, 2008, ENT-EAR NOSE THROAT, V87, P208, DOI 10.1177/014556130808700411
   Attina M.-A., 2005, INT GESTURE WORKSHOP, P13
   Attina V, 2004, SPEECH COMMUN, V44, P197, DOI 10.1016/j.specom.2004.10.013
   BEAUTEMPS D, 2007, P ASSISTH 2007, P201
   Bechet F., 2001, TRAITEMENT AUTOMATIQ, V42, P47
   Burger Thomas, 2005, 2005 13th European Signal Processing Conference, P1
   Chollet F, 2015, KERAS
   CORNETT RO, 1967, AM ANN DEAF, V112, P3
   Dodd B., 1987, Hearing by Eye: The psychology of lip-reading
   Gibert G, 2005, J ACOUST SOC AM, V118, P1144, DOI 10.1121/1.1944587
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Heracleous P, 2012, EUR SIGNAL PR CONF, P2090
   Heracleous P, 2010, SPEECH COMMUN, V52, P504, DOI 10.1016/j.specom.2010.03.001
   LaSasso Carol J, 2010, CUED SPEECH CUED LAN
   Liddell Scott K., 1989, SIGN LANGUAGE STUDIE, V1, P195, DOI [10.1353/sls.1989.0027, DOI 10.1353/SLS.1989.0027]
   Liu L, 2018, THESIS
   Liu L., 2017, 2017 32 GEN ASSEMBLY, P1
   Liu L, 2019, AM ANN DEAF, V164, P496, DOI 10.1353/aad.2019.0031
   Liu L, 2018, INTERSPEECH, P2643, DOI 10.21437/Interspeech.2018-2434
   Liu L, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P3061, DOI 10.1109/ICASSP.2018.8462090
   Liu L, 2017, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-017-0233-y
   Liu L, 2017, INT CONF ACOUST SPEE, P5130, DOI 10.1109/ICASSP.2017.7953134
   Naylor J., 2014, MAGIXMOVIE EDIT PRO
   NICHOLLS GH, 1982, J SPEECH HEAR RES, V25, P262, DOI 10.1044/jshr.2502.262
   Potamianos G, 2003, P IEEE, V91, P1306, DOI 10.1109/JPROC.2003.817150
   Reynolds S. E., 2007, 315 WASH U SCH MED P
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schwartz Jean-Luc, 2009, LANGUAGE SPEECH PROC, P377
   SHI JB, 1994, 1994 IEEE COMPUTER SOCIETY CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION, PROCEEDINGS, P593, DOI 10.1109/CVPR.1994.323794
   Stauffer C., 1999, Proceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), P246, DOI 10.1109/CVPR.1999.784637
   Stillittano S, 2013, MACH VISION APPL, V24, P1, DOI 10.1007/s00138-012-0445-1
   Stokoe WC, 2005, J DEAF STUD DEAF EDU, V10, P3, DOI 10.1093/deafed/eni001
   Sun YX, 2019, IEEE T AUTOM SCI ENG, V16, P1596, DOI 10.1109/TASE.2019.2893414
   Sun YX, 2019, IEEE ROBOT AUTOM LET, V4, P2576, DOI 10.1109/LRA.2019.2904733
   Sun YX, 2018, ROBOT AUTON SYST, V108, P115, DOI 10.1016/j.robot.2018.07.002
   Sun YX, 2017, ROBOT AUTON SYST, V89, P110, DOI 10.1016/j.robot.2016.11.012
   Svozil D, 1997, CHEMOMETR INTELL LAB, V39, P43, DOI 10.1016/S0169-7439(97)00061-0
   Tang MF, 2017, IEEE T MULTIMEDIA, V19, P408, DOI 10.1109/TMM.2016.2613639
   Tinwell Angela, 2015, Int J Mech Robot Syst, V2, P97, DOI [DOI 10.1504/IJMRS.2015.068991, 10.1504/IJMRS.2015.068991]
   Valli C., 2000, Linguistics of American Sign Language
   World Health Organization, 2019, DEAFN HEAR LOSS
   Young S., 1994, P ARPA WORKSHOP HUMA, P307, DOI 10.3115/1075812.1075885
   Young S.J., 1993, HTK HIDDEN MARKOV MO
   Zou FH, 2019, J POLYM RES, V27, DOI 10.1007/s10965-019-1979-y
NR 46
TC 19
Z9 19
U1 5
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 292
EP 305
DI 10.1109/TMM.2020.2976493
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600023
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Shi, Y
   Nie, XS
   Chen, M
   Lian, L
   Yin, YL
AF Shi, Yang
   Nie, Xiushan
   Chen, Meng
   Lian, Li
   Yin, Yilong
TI Deep Hashing With Weighted Spatial Importance
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Semantics; Hash functions; Feature extraction; Binary codes; Distributed
   databases; Deep learning; Data models; Hash; spatial importance; weight
AB Hashing method has been widely used in big data retrieval because of its low computational complexity. Most of existing hashing methods learn the final hash code from the semantic information of the whole image. However, different spatial regions of an image have different influences during the hash learning. To tackle this issue, we propose a new deep hashing with weighted spatial importance (DWSH) in this paper. Specifically, the proposed DWSH first utilizes a spatial attention model to learn the importance of different spatial regions in the original image, and then assigns different weights to these spatial regions according to their importance. The final hash codes are learned based on the weighted spatial information. In addition, two strategies are designed to utilize the spatial importance, including discrete weight strategy and continuous weight strategy, which weight the spatial information with discrete and continuous values, respectively. The results of extensive experiments conducted on three benchmark datasets show that the proposed DWSH method is superior to the state-of-the-art hashing method based on different evaluation protocols.
C1 [Shi, Yang; Chen, Meng; Lian, Li; Yin, Yilong] Shandong Univ, Sch Software, Jinan 251600, Peoples R China.
   [Nie, Xiushan] Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
C3 Shandong University; Shandong Jianzhu University
RP Yin, YL (corresponding author), Shandong Univ, Sch Software, Jinan 251600, Peoples R China.; Nie, XS (corresponding author), Shandong Jianzhu Univ, Sch Comp Sci & Technol, Jinan 250101, Peoples R China.
EM shiyang@mail.sdu.edu.cn; niexsh@hotmail.com; mchen@sdu.edu.cn;
   lianli@sdu.edu.cn; ylyin@sdu.edu.cn
OI Shi, Yang/0000-0003-2515-1588
FU National Natural Science Foundation of China [61876098, 61671274,
   61573219, 61906107]; Natural Science Foundation of Shandong Province of
   China [ZR2019BF010]; National Key R&D Program of China [2018YFC0830100,
   2018YFC0830102]; special funds for distinguished professors of Shandong
   Jianzhu University
FX Manuscript received July 7, 2020; revised September 9, 2020 and
   September 25, 2020; accepted October 7, 2020. Date of publication
   November 18, 2020; date of current version October 19, 2021. This work
   was supported in part by the National Natural Science Foundation of
   China under Grants 61876098, 61671274, 61573219, and 61906107, in part
   the Natural Science Foundation of Shandong Province of China under Grant
   ZR2019BF010, in part by the National Key R&D Program of China under
   Grants 2018YFC0830100 and 2018YFC0830102, and in part by special funds
   for distinguished professors of Shandong Jianzhu University.
   (Corresponding authors: Xiushan Nie; Yilong Yin.)
CR Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494
   [Anonymous], 2012, P INT C NEUR INF PRO
   [Anonymous], 2006, Book Annosearch: Image auto-annotation by search, DOI DOI 10.1109/CVPR.2006.58
   [Anonymous], 2015, P 2015 C EMP METH NA, DOI [10.18653/v1/D15-1166, DOI 10.48550/ARXIV.1508.04025]
   Bai JL, 2019, IEEE T MULTIMEDIA, V21, P3178, DOI 10.1109/TMM.2019.2920601
   Cao Y, 2018, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2018.00134
   Chatfield K., 2014, PROC BRIT MACH VIS C, P1557
   Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P1996, DOI 10.1109/TMM.2017.2705918
   Chen ZX, 2017, IEEE T MULTIMEDIA, V19, P123, DOI 10.1109/TMM.2016.2620604
   Chua T.-S., 2009, P ACM INT C IM VID R, P1
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fu J, 2019, PROC CVPR IEEE, P3141, DOI 10.1109/CVPR.2019.00326
   Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jaderberg M, 2015, ADV NEUR IN, V28
   Jiang QY, 2018, AAAI CONF ARTIF INTE, P3342
   Jiang QY, 2018, IEEE T IMAGE PROCESS, V27, P5996, DOI 10.1109/TIP.2018.2864894
   Jin L, 2019, IEEE T IMAGE PROCESS, V28, P2173, DOI 10.1109/TIP.2018.2883522
   Kan MN, 2014, IEEE T CIRC SYST VID, V24, P704, DOI 10.1109/TCSVT.2013.2276713
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li CX, 2019, IEEE T MULTIMEDIA, V21, P2863, DOI 10.1109/TMM.2019.2912714
   Li Q, 2017, ADV NEUR IN, V30
   Li SY, 2020, IEEE T MULTIMEDIA, V22, P1542, DOI 10.1109/TMM.2019.2946096
   Li WJ, 2016, IJCAI, P1711
   Li ZC, 2020, INT J COMPUT VISION, V128, P2265, DOI 10.1007/s11263-020-01331-0
   Li ZC, 2015, IEEE T MULTIMEDIA, V17, P1989, DOI 10.1109/TMM.2015.2477035
   Lin RS, 2010, PROC CVPR IEEE, P848, DOI 10.1109/CVPR.2010.5540129
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liong VE, 2015, PROC CVPR IEEE, P2475, DOI 10.1109/CVPR.2015.7298862
   Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227
   Liu W, 2011, SER INF MANAGE SCI, V10, P1
   Liu XB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3031
   Liu XB, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1561, DOI 10.1145/3343031.3351091
   Liu XB, 2020, IEEE T IMAGE PROCESS, V29, P4254, DOI 10.1109/TIP.2020.2970577
   Liu XB, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1678, DOI 10.1109/ICASSP.2018.8462454
   Liu YF, 2014, PROC VLDB ENDOW, V7, P745, DOI 10.14778/2732939.2732947
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu JW, 2017, IEEE T IMAGE PROCESS, V26, P2352, DOI 10.1109/TIP.2017.2678163
   Luo X, 2018, ACM/SIGIR PROCEEDINGS 2018, P735, DOI 10.1145/3209978.3210035
   Mao XJ, 2017, IEEE T MULTIMEDIA, V19, P382, DOI 10.1109/TMM.2016.2614858
   Nie XS, 2020, IEEE T KNOWL DATA EN, V32, P1951, DOI 10.1109/TKDE.2019.2913383
   Nie XS, 2017, IEEE T MULTIMEDIA, V19, P785, DOI 10.1109/TMM.2016.2629758
   Norouzi M.E., 2011, ICML
   Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222
   Pan YW, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P53, DOI 10.1145/2766462.2767725
   Peng ZM, 2019, IEEE I CONF COMP VIS, P441, DOI 10.1109/ICCV.2019.00053
   Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598
   Simonyan K., 2014, 14091556 ARXIV
   Tang JH, 2018, IEEE T NEUR NET LEAR, V29, P6154, DOI 10.1109/TNNLS.2018.2816743
   Tang JH, 2018, IEEE T CIRC SYST VID, V28, P2730, DOI 10.1109/TCSVT.2017.2715227
   Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960
   Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976
   Wang J, 2010, PROC CVPR IEEE, P3424, DOI 10.1109/CVPR.2010.5539994
   Wang Jun., 2010, ICML, P1127
   Wang M, 2020, IEEE T MULTIMEDIA, V22, P1507, DOI 10.1109/TMM.2019.2943778
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang YB, 2020, IEEE T MULTIMEDIA, V22, P1458, DOI 10.1109/TMM.2019.2947197
   Weiss P, 2008, INT CONF ACOUST SPEE, P1173, DOI 10.1109/ICASSP.2008.4517824
   Wu ZX, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P461, DOI 10.1145/2733373.2806222
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Yang HF, 2018, IEEE T PATTERN ANAL, V40, P437, DOI 10.1109/TPAMI.2017.2666812
   You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503
   Zhai DM, 2018, IEEE T MULTIMEDIA, V20, P675, DOI 10.1109/TMM.2017.2749160
   Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319
NR 66
TC 3
Z9 4
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 3778
EP 3792
DI 10.1109/TMM.2020.3031092
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA WJ5OB
UT WOS:000709093100027
DA 2024-07-18
ER

PT J
AU Tang, C
   Liu, XW
   An, S
   Wang, PC
AF Tang, Chang
   Liu, Xinwang
   An, Shan
   Wang, Pichao
TI BR<SUP>2</SUP>Net: Defocus Blur Detection Via a Bidirectional Channel
   Attention Residual Refining Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Defocus blur detection; deep convolutional neural network; residual
   learning; dense connection
ID MAP ESTIMATION; SINGLE-IMAGE
AB Due to the remarkable potential applications, defocus blur detection, which aims to separate blurry regions from an image, has attracted much attention. Although significant progress has been made by many methods, there are still various challenges that hinder the results, e.g., confusing background areas, sensitivity to the scale and missing the boundary details of the defocus blur regions. To solve these issues, in this paper, we propose a deep convolutional neural network (CNN) for defocus blur detection via a Bi-directional Residual Refining network (BR(2)Net). Specifically, a residual learning and refining module (RLRM) is designed to correct the prediction errors in the intermediate defocus blur map. Then, we develop a bidirectional residual feature refining network with two branches by embedding multiple RLRMs into it for recurrently combining and refining the residual features. One branch of the network refines the residual features from the shallow layers to the deep layers, and the other branch refines the residual features from the deep layers to the shallow layers. In such a manner, both the low-level spatial details and high-level semantic information can be encoded step by step in two directions to suppress background clutter and enhance the detected region details. The outputs of the two branches are fused to generate the final results. In addition, with the observation that different feature channels have different extents of discrimination for detecting blurred regions, we add a channel attention module to each feature extraction layer to select more discriminative features for residual learning. To promote further research on defocus blur detection, we create a new dataset with various challenging images and manually annotate their corresponding pixelwise ground truths. The proposed network is validated on two commonly used defocus blur detection datasets and our newly collected dataset by comparing it with 10 other state-of-the-art methods. Extensive experiments with ablation studies demonstrate that BR2$Net consistently and significantly outperforms the competitors in terms of both the efficiency and accuracy.
C1 [Tang, Chang] China Univ Geosci, Sch Comp Sci, Wuhan 430074, Peoples R China.
   [Liu, Xinwang] Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China.
   [An, Shan] Beihang Univ, Sch Comp Sci & Engn, Beijing 100191, Peoples R China.
   [Wang, Pichao] Alibaba Grp US Inc, Bellevue, WA 98004 USA.
C3 China University of Geosciences; National University of Defense
   Technology - China; Beihang University
RP Liu, XW (corresponding author), Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Peoples R China.
EM tangchang@cug.edu.cn; xinwangliu@nudt.edu.cn; anshan.tju@gmail.com;
   pichaowang@gmail.com
RI Tang, Chang/AAU-8995-2020; LIU, Xinwang/L-8089-2019
OI Tang, Chang/0000-0002-6515-7696; LIU, Xinwang/0000-0001-9066-1475; Wang,
   Pichao/0000-0002-1430-0237; An, Shan/0000-0001-7796-6952
FU National Science Foundation of China [61701451, 61773392, 61901205,
   U1711266, 41925007]; Fundamental Research Funds for the Central
   Universities, China University of Geosciences (Wuhan) [CUG170654]
FX This work was supported in part by the National Science Foundation of
   China under Grants 61701451, 61773392, 61901205, U1711266, and 41925007
   and in part by the Fundamental Research Funds for the Central
   Universities, China University of Geosciences (Wuhan) under Grant
   CUG170654. The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Lamberto Ballan.
CR BAE S, 2007, COMPUT GRAPH FORUM, V26, P571, DOI DOI 10.1111/J.1467-8659.2007.01080.X
   Chakrabarti A, 2010, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2010.5539954
   Couzinié-Devy F, 2013, PROC CVPR IEEE, P1075, DOI 10.1109/CVPR.2013.143
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301
   Golestaneh SA, 2017, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2017.71
   Guo XP, 2019, IEEE T MULTIMEDIA, V21, P1982, DOI 10.1109/TMM.2019.2895292
   He K., 2016, PROC CVPR IEEE, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]
   Jiang P, 2013, IEEE I CONF COMP VIS, P1976, DOI 10.1109/ICCV.2013.248
   Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099
   Kang K, 2016, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2016.95
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li DQ, 2019, IEEE T MULTIMEDIA, V21, P1221, DOI 10.1109/TMM.2018.2875354
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Liu RT, 2008, PROC CVPR IEEE, P954
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Masia B, 2012, COMPUT GRAPH FORUM, V31, P1867, DOI 10.1111/j.1467-8659.2012.03067.x
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Pang YW, 2016, IEEE T CYBERNETICS, V46, P2220, DOI 10.1109/TCYB.2015.2472478
   Park J, 2017, PROC CVPR IEEE, P2760, DOI 10.1109/CVPR.2017.295
   Park SJ, 2017, IEEE I CONF COMP VIS, P4990, DOI 10.1109/ICCV.2017.533
   Purohit K, 2018, IEEE IMAGE PROC, P2202, DOI 10.1109/ICIP.2018.8451765
   QI Y, 2019, PROC CVPR IEEE, V41, P1116
   Ren C, 2019, IEEE T MULTIMEDIA, V21, P731, DOI 10.1109/TMM.2018.2866362
   Saad E, 2016, IEEE T IMAGE PROCESS, V25, P3141, DOI 10.1109/TIP.2016.2555702
   Shi JP, 2015, PROC CVPR IEEE, P657, DOI 10.1109/CVPR.2015.7298665
   Shi JP, 2014, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2014.379
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song YB, 2017, IEEE I CONF COMP VIS, P2574, DOI 10.1109/ICCV.2017.279
   Su B, 2011, Proceedings of the 19th ACM International Conference on Multimedia. MM'11, DOI [DOI 10.1145/2072298.2072024, DOI 10.5555/1785794.1785825]
   Sun C, 2018, PROC CVPR IEEE, P8962, DOI 10.1109/CVPR.2018.00934
   Tai YW, 2009, IEEE IMAGE PROC, P1797, DOI 10.1109/ICIP.2009.5414620
   Tang C., 2019, CVPR
   Tang C, 2020, P AAAI C ART INT
   TANG C, 2017, PROC CVPR IEEE, V63, P10
   Tang C, 2016, IEEE SIGNAL PROC LET, V23, P1652, DOI 10.1109/LSP.2016.2611608
   Tang C, 2013, OPT LETT, V38, P1706, DOI 10.1364/OL.38.001706
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Wang X, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P467, DOI 10.1109/CISP.2008.371
   Wei Zhang, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P1947, DOI 10.1109/ICCVW.2009.5457520
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI [10.1109/ICCV.2015.164, 10.1007/s11263-017-1004-z]
   Xu GD, 2017, IEEE I CONF COMP VIS, P5381, DOI 10.1109/ICCV.2017.574
   YAN R, 2016, PROC CVPR IEEE, V25, P1910
   YI X, 2016, PROC CVPR IEEE, V25, P1626
   Yu X, 2014, IEEE T MULTIMEDIA, V16, P1510, DOI 10.1109/TMM.2014.2321734
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang SH, 2018, PROC CVPR IEEE, P6586, DOI 10.1109/CVPR.2018.00689
   Zhang W, 2012, IEEE T IMAGE PROCESS, V21, P873, DOI 10.1109/TIP.2011.2162739
   Zhang XX, 2016, J VIS COMMUN IMAGE R, V35, P257, DOI 10.1016/j.jvcir.2016.01.002
   Zhang Y, 2013, PROC CVPR IEEE, P1091, DOI 10.1109/CVPR.2013.145
   Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI 10.1007/978-3-030-01234-2_18
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao WD, 2019, PROC CVPR IEEE, P8897, DOI 10.1109/CVPR.2019.00911
   Zhao WD, 2020, IEEE T PATTERN ANAL, V42, P1884, DOI 10.1109/TPAMI.2019.2906588
   Zhao WD, 2018, PROC CVPR IEEE, P3080, DOI 10.1109/CVPR.2018.00325
   Zhu X, 2013, IEEE T IMAGE PROCESS, V22, P4879, DOI 10.1109/TIP.2013.2279316
   Zhuo SJ, 2011, PATTERN RECOGN, V44, P1852, DOI 10.1016/j.patcog.2011.03.009
NR 62
TC 39
Z9 40
U1 3
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 624
EP 635
DI 10.1109/TMM.2020.2985541
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA QA6NL
UT WOS:000613560200008
DA 2024-07-18
ER

PT J
AU Wang, BY
   Hu, YL
   Gao, JB
   Sun, YF
   Ju, FJ
   Yin, BC
AF Wang, Boyue
   Hu, Yongli
   Gao, Junbin
   Sun, Yanfeng
   Ju, Fujiao
   Yin, Baocai
TI Learning Adaptive Neighborhood Graph on Grassmann Manifolds for
   Video/Image-Set Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Manifolds; Laplace equations; Learning systems; Videos; Clustering
   methods; Streaming media; Electronic mail; Subspace clustering;
   Grassmann manifolds; adaptive neighborhood regularization
ID ALGORITHM
AB The objective of self-expression based spectral clustering is to learn an affinity matrix which accurately reflects the similarity among data, and the Laplacian constraint is usually exploited to make the affinity matrix preserve the global structure of raw data. However, there exist two drawbacks: firstly, these methods are mostly designed for vectorial data in Euclidean spaces, which are not suitable for multidimensional data with nonlinear manifold structure, e.g., videos and image-sets. Secondly, the clustering performance heavily relies on the quality of a pre-learned Laplacian matrix in which the global structure may be mis-interpreted without considering manifold structures. In this paper, we firstly provide a unified framework about self-expression learning on Grassmann manifolds, which implements the clustering tasks for multidimensional data under subspace views. Then, to assign optimal neighbors to each data depending on the local distance, we adaptively learn the neighborhood relationship from the obtained self-expression coefficient matrix, referred to Learning Adaptive Neighborhood Graph on Grassmann manifolds (GMAN). In the optimization process, the neighborhood relationship can be adaptively learned and updated with the coefficient matrix. The experimental results on five public datasets show that the proposed method is obviously better than many related clustering methods based on Grassmann manifolds, proving the effectiveness of GMAN in multidimensional data clustering.
C1 [Wang, Boyue; Hu, Yongli; Sun, Yanfeng; Ju, Fujiao] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
   [Gao, Junbin] Univ Sydney, Business Sch, Discipline Business Analyt, Camperdown, NSW 2006, Australia.
   [Yin, Baocai] Dalian Univ Technol, Fac Elect Informat & Elect Engn, Coll Comp Sci & Technol, Dalian 116620, Peoples R China.
   [Yin, Baocai] Beijing Univ Technol, Beijing Municipal Key Lab Multimedia & Intelligen, Beijing 100124, Peoples R China.
C3 Beijing University of Technology; University of Sydney; Dalian
   University of Technology; Beijing University of Technology
RP Hu, YL (corresponding author), Beijing Univ Technol, Beijing Artificial Intelligence Inst, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
EM wby@bjut.edu.cn; huyongli@bjut.edu.cn; junbin.gao@sydney.edu.au;
   yfsun@bjut.edu.cn; ybc@bjut.edu.cn
RI Gao, Junbin/C-6566-2008; Gao, Junbin/A-1766-2009
OI Gao, Junbin/0000-0001-9803-0256; Hu, Yongli/0000-0003-0440-438X
FU National Natural Science Foundation of China [U19B2039, 61906011,
   61632006, 61772048, 61672071, U1811463]; Beijing Natural Science
   Foundation [4204086]; Beijing Municipal Science and Technology
   [KM202010005014, KM201910005028]; Beijing Talents Project [2017A24]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants U19B2039, 61906011, 61632006, 61772048,
   61672071, and U1811463, in part by Beijing Natural Science Foundation
   4204086, in part by Beijing Municipal Science and Technology Project
   KM202010005014 and KM201910005028, and in part by Beijing Talents
   Project (2017A24).
CR Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1
   [Anonymous], 1999, P 5 ACM SIGKDD INT C, DOI [10.1145/312129.312186, DOI 10.1145/312129.312186]
   [Anonymous], 2017, ADV NEURAL INF PROCE
   [Anonymous], 2014, P AS C COMP VIS
   [Anonymous], 2013, P IEEE C COMP VIS PA
   [Anonymous], 2004, Advances in neural information processing systems (NIPS)
   [Anonymous], 2011, Proc. NIPS, DOI DOI 10.1109/TPAMI.2013.57
   Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996
   Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015
   Brookes M., 2011, The Matrix Reference Manual
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Elhamifar E, 2009, PROC CVPR IEEE, P2782
   Guo XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3547
   Hamm Jihun, 2008, P 25 INT C MACH LEAR, P376, DOI DOI 10.1145/1390156.1390204
   Harandi M, 2013, IEEE I CONF COMP VIS, P3120, DOI 10.1109/ICCV.2013.387
   Harandi MT, 2014, LECT NOTES COMPUT SC, V8695, P408, DOI 10.1007/978-3-319-10584-0_27
   Jian M, 2014, IEEE T MULTIMEDIA, V16, P413, DOI 10.1109/TMM.2013.2291657
   Jiang YBY, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P718, DOI 10.1145/3240508.3240582
   Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X
   KVALSETH TO, 1987, IEEE T SYST MAN CYB, V17, P517, DOI 10.1109/TSMC.1987.4309069
   Lang CY, 2012, IEEE T IMAGE PROCESS, V21, P1327, DOI 10.1109/TIP.2011.2169274
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Lu CY, 2012, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON COMPUTER VISION IN REMOTE SENSING, P34
   Lu YW, 2017, IEEE T MULTIMEDIA, V19, P2391, DOI 10.1109/TMM.2017.2703130
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Oh TH, 2016, IEEE T PATTERN ANAL, V38, P744, DOI 10.1109/TPAMI.2015.2465956
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323
   RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F
   Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688
   Shirazi S, 2012, IEEE IMAGE PROC, P781, DOI 10.1109/ICIP.2012.6466976
   Tabia H, 2015, IEEE T MULTIMEDIA, V17, P1591, DOI 10.1109/TMM.2015.2457676
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Turaga P, 2011, IEEE T PATTERN ANAL, V33, P2273, DOI 10.1109/TPAMI.2011.52
   Wang BY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2755
   Wang BY, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3092690
   Wang BY, 2018, PATTERN RECOGN, V76, P623, DOI 10.1016/j.patcog.2017.07.009
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Yin M, 2016, PROC CVPR IEEE, P5157, DOI 10.1109/CVPR.2016.557
   Zhang J., 2019, CVPR, P5473
   Zhuang LS, 2012, PROC CVPR IEEE, P2328, DOI 10.1109/CVPR.2012.6247944
NR 46
TC 26
Z9 27
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 216
EP 227
DI 10.1109/TMM.2020.2975394
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA PJ6LW
UT WOS:000601877600017
DA 2024-07-18
ER

PT J
AU Wang, YH
   Gelli, F
   von der Weth, C
   Kankanhalli, M
AF Wang, Yuhui
   Gelli, Francesco
   von der Weth, Christian
   Kankanhalli, Mohan
TI A Matrix Factorization Based Framework for Fusion of Physical and Social
   Sensors
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Sensor fusion; Task analysis; Cameras; Semantics; Event detection; Image
   sensors; Physical social fusion; multimodal and multisource analysis;
   situation awareness; spatial temporal filtering
AB Our world is witnessing the on-going substantial increase in the number of multimodal physical and social sensors that are ubiquitously distributed and observing or reporting what is happening in their surroundings. These sensors provide massive amounts of spatio-temporal digital footprints which can be analyzed for various tasks such as event detection or situation awareness. However, inherent noise due to the nature of these sensors result in imprecise data and hence imprecise analysis. Also, the heterogeneous data from different modalities, formats and sources make interpreting different levels of information a big challenge. To overcome these limitations, we propose a novel unified matrix factorization-based model to fuse physical and social sensor signals for spatio-temporal analysis. Readings of physical sensor signals are represented by a spatio-temporal situation matrix, which then incorporates social content that can provide explanations for the signal strengths. We test our framework on large-scale real-world data including PSI stations data, traffic CCTV camera images, and tweets for situation prediction as well as for filtering noise to detect events of diverse situations. The experimental results suggest that the proposed matrix factorization approach can utilize the sources correlation, resulting in better performances in various situational understanding tasks.
C1 [Wang, Yuhui; Gelli, Francesco; von der Weth, Christian; Kankanhalli, Mohan] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
C3 National University of Singapore
RP Wang, YH (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM fredwang0216@gmail.com; francesco.gelli@u.nus.edu;
   vonderweth@nus.edu.sg; mohan@comp.nus.edu.sg
RI Kankanhalli, Mohan/Q-9284-2019; 王, 宇慧/GXZ-7548-2022
OI Kankanhalli, Mohan/0000-0002-4846-2015; 
FU National Research Foundation, Prime Ministers Office, Singapore under
   its International Research Centre in Singapore Funding Initiative
FX Manuscript received August 11, 2016; revised May 22, 2017 and December
   24, 2017; accepted September 14, 2018. Date of publication August 12,
   2020; date of current version August 24, 2021. This work was supported
   by National Research Foundation, Prime Ministers Office, Singapore under
   its International Research Centre in Singapore Funding Initiative. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Chengcui Zhang. (Corresponding
   author: Yuhui Wang.)
CR [Anonymous], 2014, P INT C MULT RETR
   [Anonymous], 2013, OAIR
   Atrey PK, 2010, MULTIMEDIA SYST, V16, P345, DOI 10.1007/s00530-010-0182-0
   Atrey PK, 2007, ACM T MULTIM COMPUT, V3, DOI 10.1145/1198302.1198304
   Atrey PK, 2006, MULTIMEDIA SYST, V12, P239, DOI 10.1007/s00530-006-0063-8
   Babari R, 2012, TRANSPORT RES C-EMER, V22, P17, DOI 10.1016/j.trc.2011.11.012
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Bredin H, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/70186
   Cai HY, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P89, DOI 10.1145/2733373.2806236
   Chen ZS, 2011, IEEE T MULTIMEDIA, V13, P1371, DOI 10.1109/TMM.2011.2166380
   Ding GG, 2014, PROC CVPR IEEE, P2083, DOI 10.1109/CVPR.2014.267
   Essid S, 2013, IEEE T MULTIMEDIA, V15, P415, DOI 10.1109/TMM.2012.2228474
   Ewerth R, 2012, IEEE T MULTIMEDIA, V14, P1008, DOI 10.1109/TMM.2012.2186956
   Gao Y, 2013, IEEE T IMAGE PROCESS, V22, P363, DOI 10.1109/TIP.2012.2202676
   Geng J, 2015, IEEE T MULTIMEDIA, V17, P498, DOI 10.1109/TMM.2015.2398195
   Giridhar P, 2014, INT CONF PERVAS COMP, P395, DOI 10.1109/PerComW.2014.6815239
   González MC, 2008, NATURE, V453, P779, DOI 10.1038/nature06958
   Hsieh HP, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P181, DOI 10.1145/2733373.2809931
   Hu GN, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1756
   Jacobs N., 2009, P 17 ACM SIGSPATIAL, P111, DOI [DOI 10.1145/1653771.1653789, 10.1145/1653771.1653789]
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263
   Lanagan J., 2011, P 5 INT AAAI C WEBL, P542
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Liu J, 2014, IEEE T MULTIMEDIA, V16, P588, DOI 10.1109/TMM.2014.2302732
   Ma Hao, 2008, P CIKM08 C INFORM KN, P931
   McAuley Julian, 2013, RECSYS
   Mnih A., 2007, ADV NEURAL INFORM PR, V20
   Pan Bei, 2013, P 21 ACM SIGSPATIAL, P344, DOI DOI 10.1145/2525314.2525343
   Phan Thomas, 2014, 2014 IEEE 11th Consumer Communications and Networking Conference (CCNC), P98, DOI 10.1109/CCNC.2014.6866555
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127
   Sakaki T., 2010, P 19 INT C WORLD WID, P851
   Singh V.K., 2010, P INT C MULTIMEDIA, P481
   Tang J., 2013, P 23 INT JOINT C ART, P2712
   Tang S, 2012, IEEE T MULTIMEDIA, V14, P43, DOI 10.1109/TMM.2011.2168198
   Walther Maximilian, 2013, Advances in Information Retrieval. 35th European Conference on IR Research, ECIR 2013. Proceedings, P356, DOI 10.1007/978-3-642-36973-5_30
   Wang YH, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1231, DOI 10.1145/2736277.2741634
   Xie K., 2013, P 13 INT WORKSH MULT
   Yue Gao, 2015, MultiMedia Modeling. 21st International Conference, MMM 2015, January 5-7, 2015, Proceedings: LNCS 8935, P269, DOI 10.1007/978-3-319-14445-0_24
   Zahálka J, 2015, IEEE T MULTIMEDIA, V17, P2235, DOI 10.1109/TMM.2015.2480007
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao SC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P1025, DOI 10.1145/2647868.2655035
   Zhao S, 2015, SIGNAL PROCESS, V112, P110, DOI 10.1016/j.sigpro.2014.09.038
   Zhao Sicheng, 2016, P 24 ACM INT C MULT, P1385, DOI DOI 10.1145/2964284.2964289
NR 46
TC 3
Z9 3
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2782
EP 2793
DI 10.1109/TMM.2020.3016222
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600019
DA 2024-07-18
ER

PT J
AU Xiao, XL
   Gong, YJ
   Hua, ZY
   Chen, WN
AF Xiao, Xiaolin
   Gong, Yue-Jiao
   Hua, Zhongyun
   Chen, Wei-Neng
TI On Reliable Multi-View Affinity Learning for Subspace Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Affinity learning; connectivity and sparsity; low-rank tensor;
   multi-view subspace clustering; self-representation
ID FACTORIZATION
AB In multi-view subspace clustering, the low-rankness of the stacked self-representation tensor is widely accepted to capture the high-order cross-view correlation. However, using the nuclear norm as a convex surrogate of the rank function, the self-representation tensor exhibits strong connectivity with dense coefficients. When noise exists in the data, the generated affinity matrix may be unreliable for subspace clustering as it retains the connections across inter-cluster samples due to the lack of sparsity. Since both the connectivity and sparsity of the self-representation coefficients are curial for subspace clustering, we propose a Reliable Multi-View Affinity Learning (RMVAL) method so as to optimize both properties in a single model. Specifically, RMVAL employs the low-rank tensor constraint to yield a well-connected yet dense solution, and purifies the densely connected self-representation tensor by preserving only the connections in local neighborhoods using the l(1)-norm regularization. This way, the strong connections on the self-representation tensor are retained and the trivial coefficients corresponding to the inter-cluster connections are suppressed, leading to a "clean" self-representation tensor and also a reliable affinity matrix. We propose an efficient algorithm to solve RMVAL using the alternating direction method of multipliers. Extensive experiments on benchmark databases have demonstrated the superiority of RMVAL.
C1 [Xiao, Xiaolin; Gong, Yue-Jiao; Chen, Wei-Neng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Hua, Zhongyun] Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
C3 South China University of Technology; Harbin Institute of Technology
RP Gong, YJ (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.; Hua, ZY (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Shenzhen 518055, Peoples R China.
EM shellyxiaolin@gmail.com; gongyuejiao@gmail.com; huazyum@gmail.com;
   cwnraul634@aliyun.com
RI Hua, Zhongyun/F-1887-2016
OI Hua, Zhongyun/0000-0002-3529-0541
FU Key Project of Science and Technology Innovation 2030 by Ministry of
   Science and Technology of China [2018AAA0101300]; NationalNatural
   Science Foundation ofChina [62 006 080, 61 873 095, 62 071 142]; China
   Postdoctoral Science Foundation [2019M662913]; Fundamental Research
   Funds for the Central Universities
FX Thisworkwas supported in part by the Key Project of Science and
   Technology Innovation 2030 supported by the Ministry of Science and
   Technology of China under Grant 2018AAA0101300, in part by the
   NationalNatural Science Foundation ofChina underGrants 62 006 080, 61
   873 095, and 62 071 142, in part by China Postdoctoral Science
   Foundation under Grant 2019M662913, and in part by the Fundamental
   Research Funds for the Central Universities.
CR [Anonymous], 2014, ARXIV14127056
   Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970
   Chen CH, 2016, MATH PROGRAM, V155, P57, DOI 10.1007/s10107-014-0826-5
   Chen Y, 2020, PROC CVPR IEEE, P4154, DOI 10.1109/CVPR42600.2020.00421
   Chen YY, 2020, IEEE T MULTIMEDIA, V22, P1985, DOI 10.1109/TMM.2019.2952984
   Cheng B, 2011, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2011.6126528
   Gao HC, 2015, IEEE I CONF COMP VIS, P4238, DOI 10.1109/ICCV.2015.482
   Huang J, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2601434
   Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li XH, 2014, IEEE T GEOSCI REMOTE, V52, P7086, DOI 10.1109/TGRS.2014.2307354
   Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063
   Li ZH, 2018, IEEE T NEUR NET LEAR, V29, P6323, DOI 10.1109/TNNLS.2018.2829867
   Lin Z., 2011, ADV NEURAL INFORM PR, V24, P612, DOI DOI 10.1007/S11263-013-0611-6
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu HR, 2012, INT J COMPUT VISION, V98, P65, DOI 10.1007/s11263-011-0496-1
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Lu CY, 2013, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2013.170
   Nasihatkon B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2137, DOI 10.1109/CVPR.2011.5995679
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Peng X, 2019, PR MACH LEARN RES, V97
   Peng X, 2017, IEEE T CYBERNETICS, V47, P1053, DOI 10.1109/TCYB.2016.2536752
   Peng X, 2015, AAAI CONF ARTIF INTE, P3827
   Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537
   Rubinstein R, 2012, INT CONF ACOUST SPEE, P5405, DOI 10.1109/ICASSP.2012.6289143
   Sarfraz MS, 2019, PROC CVPR IEEE, P8926, DOI 10.1109/CVPR.2019.00914
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang BY, 2021, IEEE T MULTIMEDIA, V23, P216, DOI 10.1109/TMM.2020.2975394
   Wang YX, 2019, IEEE T INFORM THEORY, V65, P5406, DOI 10.1109/TIT.2019.2915593
   Wang Z, 2021, IEEE T MULTIMEDIA, V23, P1855, DOI 10.1109/TMM.2020.3003747
   Wu J., 2020, AAAI, P6388
   Wu JL, 2019, IEEE T IMAGE PROCESS, V28, P5910, DOI 10.1109/TIP.2019.2916740
   Xiao XL, 2021, IEEE T NEUR NET LEAR, V32, P1325, DOI 10.1109/TNNLS.2020.2984625
   Xiaobo Wang, 2017, 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P1, DOI 10.1109/CVPR.2017.8
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yang J, 2017, IEEE T PATTERN ANAL, V39, P156, DOI 10.1109/TPAMI.2016.2535218
   Yang JF, 2020, IEEE T PATTERN ANAL, V42, P1537, DOI 10.1109/TPAMI.2019.2913863
   Yin M., 2018, IEEE T INSTRUM MEAS, P1
   You C, 2016, PROC CVPR IEEE, P3928, DOI 10.1109/CVPR.2016.426
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang YF, 2009, IEEE T MULTIMEDIA, V11, P1276, DOI 10.1109/TMM.2009.2030629
   Zhang ZM, 2014, PROC CVPR IEEE, P3842, DOI 10.1109/CVPR.2014.485
   Zhou P, 2018, IEEE T IMAGE PROCESS, V27, P1152, DOI 10.1109/TIP.2017.2762595
   Zhou T, 2020, IEEE T CYBERNETICS, V50, P1655, DOI 10.1109/TCYB.2018.2883673
NR 50
TC 18
Z9 18
U1 3
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 4555
EP 4566
DI 10.1109/TMM.2020.3045259
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA XM6HD
UT WOS:000728924800013
DA 2024-07-18
ER

PT J
AU Xue, ML
   Shivakumara, P
   Zhang, C
   Xiao, Y
   Lu, T
   Pal, U
   Lopresti, D
   Yang, ZB
AF Xue, Minglong
   Shivakumara, Palaiahnakote
   Zhang, Chao
   Xiao, Yao
   Lu, Tong
   Pal, Umapada
   Lopresti, Daniel
   Yang, Zhibo
TI Arbitrarily-Oriented Text Detection in Low Light Natural Scene Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Licenses; Feature extraction; Image segmentation; Proposals; Machine
   learning; Image enhancement; Convolutional neural networks; Image
   enhancement; gaussian pyramid filter; Homomorphic filter; COLD features;
   convolutional neural network; arbitrarily-oriented text detection
ID RECOGNITION
AB Text detection in low light natural scene images is challenging due to poor image quality and low contrast. Unlike most existing methods that focus on well-lit (normally daylight) images, the proposed method considers much darker natural scene images. For this task, our method first integrates spatial and frequency domain features through fusion to enhance fine details in the image. Next, we use Maximally Stable Extremal Regions (MSER) for detecting text candidates from the enhanced images. We then introduce Cloud of Line Distribution (COLD) features, which capture the distribution of pixels of text candidates in the polar domain. The extracted features are sent to a Convolution Neural Network (CNN) to correct the bounding boxes for arbitrarily oriented text lines by removing false positives. Experiments are conducted on a dataset of low light images to evaluate the proposed enhancement step. The results show our approach is more effective compared to existing methods in terms of standard quality measures, namely, BRISQE, NIQE and PIQE. In addition, experimental results on a variety of standard benchmark datasets, namely, ICDAR 2013, ICDAR 2015, SVT, Total-Text, ICDAR 2017-MLT and CTW1500, show that the proposed approach not only produces better results for low light images, at the same time it is also competitive for daylight images.
C1 [Xue, Minglong; Zhang, Chao; Xiao, Yao; Lu, Tong] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
   [Shivakumara, Palaiahnakote] Univ Malaya, Dept Comp Syst & Informat Technol, Kuala Lumpur 43200, Malaysia.
   [Pal, Umapada] Indian Stat Inst, Kolkata 700001, India.
   [Lopresti, Daniel] Lehigh Univ, Comp Sci & Engn, Bethlehem, PA 18015 USA.
   [Yang, Zhibo] Alibaba Grroup, Hangzhou 310000, Peoples R China.
C3 Nanjing University; Universiti Malaya; Indian Statistical Institute;
   Indian Statistical Institute Kolkata; Lehigh University
RP Lu, T (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
EM xueml@smail.nju.edu.cn; shiva@um.edu.my; zhangchao_nju@126.com;
   iam_xiaoyao@126.com; lutong@nju.edu.cn; umapada@isical.ac.in;
   lopresti@cse.lehigh.edu; hibo.yzb@alibaba-inc.com
RI Yang, Zhibo/J-1831-2017; Pal, Umapada/AAC-4930-2022; Palaiahnakote,
   Shivakumara/B-6261-2013; Palaiahnakote, Shivakumara/ITU-6488-2023
FU Natural Science Foundation of China [61672273, 61832008]; Alibaba Group;
   University of Malaya, Malaysia [GPF014D-2019]
FX The work of Tong Lu was supported by the Natural Science Foundation of
   China under Grants 61672273 and 61832008, in part by the Alibaba Group
   through Alibaba Innovative Research Program. The work of Palaiahnakote
   Shivakumara was supported by the Faculty Grant: GPF014D-2019, University
   of Malaya, Malaysia.
CR Al-Shemarry MS, 2018, EXPERT SYST APPL, V92, P216, DOI 10.1016/j.eswa.2017.09.036
   Baek Y, 2019, PROC CVPR IEEE, P9357, DOI 10.1109/CVPR.2019.00959
   Bazazian D, 2019, PATTERN RECOGN LETT, V119, P112, DOI 10.1016/j.patrec.2017.08.030
   Boonsim N, 2017, PATTERN ANAL APPL, V20, P1195, DOI 10.1007/s10044-016-0559-6
   Deng D, 2018, AAAI CONF ARTIF INTE, P6773
   Deng LJ, 2019, NEUROCOMPUTING, V334, P134, DOI 10.1016/j.neucom.2019.01.013
   NguyenVan D, 2019, PATTERN RECOGN, V87, P118, DOI 10.1016/j.patcog.2018.10.012
   Dong XC, 2011, INT C PAR DISTRIB SY, P9, DOI 10.1109/ICPADS.2011.115
   Donoser M., 2006, COMPUTER VISION PATT, V1, P553, DOI DOI 10.1109/CVPR.2006.107
   Epshtein B, 2010, PROC CVPR IEEE, P2963, DOI 10.1109/CVPR.2010.5540041
   Gao YZ, 2019, NEUROCOMPUTING, V339, P161, DOI 10.1016/j.neucom.2019.01.094
   He P, 2017, IEEE I CONF COMP VIS, P3066, DOI 10.1109/ICCV.2017.331
   He S, 2017, PATTERN RECOGN, V63, P321, DOI 10.1016/j.patcog.2016.09.017
   He WH, 2018, IEEE T IMAGE PROCESS, V27, P5406, DOI 10.1109/TIP.2018.2855399
   Huang SZ, 2018, INT SYM COMPUT INTEL, P195, DOI 10.1109/ISCID.2018.00051
   Huang WL, 2014, LECT NOTES COMPUT SC, V8692, P497, DOI 10.1007/978-3-319-10593-2_33
   Jiang XS, 2013, IEEE IMAGE PROC, P553, DOI 10.1109/ICIP.2013.6738114
   Li X., 2018, ARXIV180602559
   Liao MH, 2020, AAAI CONF ARTIF INTE, V34, P11474
   Liao MH, 2021, IEEE T PATTERN ANAL, V43, P532, DOI 10.1109/TPAMI.2019.2937086
   Liao MH, 2018, PROC CVPR IEEE, P5909, DOI 10.1109/CVPR.2018.00619
   Liao MH, 2018, IEEE T IMAGE PROCESS, V27, P3676, DOI 10.1109/TIP.2018.2825107
   Liao MH, 2017, AAAI CONF ARTIF INTE, P4161
   Lin CH, 2018, PROCEEDINGS OF 4TH IEEE INTERNATIONAL CONFERENCE ON APPLIED SYSTEM INNOVATION 2018 ( IEEE ICASI 2018 ), P224, DOI 10.1109/ICASI.2018.8394573
   Liu XB, 2018, PROC CVPR IEEE, P5676, DOI 10.1109/CVPR.2018.00595
   Liu Y., 2020, P COMP VIS PATT REC, P9809
   Liu YL, 2019, PATTERN RECOGN, V90, P337, DOI 10.1016/j.patcog.2019.02.002
   Liu Yuliang, 2017, Detecting Curve Text in the Wild: New Dataset and New Solution
   Long SB, 2018, LECT NOTES COMPUT SC, V11206, P19, DOI 10.1007/978-3-030-01216-8_2
   Lu SJ, 2015, INT J DOC ANAL RECOG, V18, P125, DOI 10.1007/s10032-015-0237-z
   Lyu PY, 2018, PROC CVPR IEEE, P7553, DOI 10.1109/CVPR.2018.00788
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Mohanty S, 2018, INT C PATT RECOG, P2588, DOI 10.1109/ICPR.2018.8545198
   Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097
   Panahi R, 2017, IEEE T INTELL TRANSP, V18, P767, DOI 10.1109/TITS.2016.2586520
   Raghunandan KS, 2019, IEEE T CIRC SYST VID, V29, P1145, DOI 10.1109/TCSVT.2018.2817642
   Raghunandan KS, 2018, IEEE T CIRC SYST VID, V28, P2276, DOI 10.1109/TCSVT.2017.2713806
   Ravisankar P, 2018, MULTIMED TOOLS APPL, V77, P5547, DOI 10.1007/s11042-017-4466-7
   Sharma S, 2016, PROCEEDINGS OF THE 2016 2ND INTERNATIONAL CONFERENCE ON CONTEMPORARY COMPUTING AND INFORMATICS (IC3I), P285, DOI 10.1109/IC3I.2016.7917976
   Shi BG, 2017, PROC CVPR IEEE, P3482, DOI 10.1109/CVPR.2017.371
   Shivakumara P, 2019, EXPERT SYST APPL, V118, P1, DOI 10.1016/j.eswa.2018.08.015
   Tang J, 2019, PATTERN RECOGN, V96, DOI 10.1016/j.patcog.2019.06.020
   Tian SX, 2017, IEEE I CONF COMP VIS, P1501, DOI 10.1109/ICCV.2017.166
   Tian SX, 2015, IEEE I CONF COMP VIS, P4651, DOI 10.1109/ICCV.2015.528
   Tian Z, 2016, LECT NOTES COMPUT SC, V9912, P56, DOI 10.1007/978-3-319-46484-8_4
   Venkatanath N, 2015, NATL CONF COMMUN
   Wahyono, 2015, NEUROCOMPUTING, V151, P1033, DOI 10.1016/j.neucom.2014.07.079
   Wang QQ, 2015, PROC INT CONF DOC, P106, DOI 10.1109/ICDAR.2015.7333735
   Rui W, 2017, 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC 2017), P315, DOI 10.1109/ICIVC.2017.7984568
   Wu YR, 2017, PROC INT CONF DOC, P1249, DOI 10.1109/ICDAR.2017.206
   Xu YC, 2019, IEEE T IMAGE PROCESS, V28, DOI 10.1109/TIP.2019.2900589
   Xue CH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P989
   Xue CH, 2018, LECT NOTES COMPUT SC, V11220, P370, DOI 10.1007/978-3-030-01270-0_22
   Xue M., 2019, MULTIMEDIA TOOLS APP, V78, P1
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P220, DOI 10.1109/TITS.2017.2749977
   Zhan FN, 2019, IEEE I CONF COMP VIS, P9104, DOI 10.1109/ICCV.2019.00920
   Zhang C, 2018, LECT NOTES COMPUT SC, V11166, P46, DOI 10.1007/978-3-030-00764-5_5
   Zhou XY, 2017, PROC CVPR IEEE, P2642, DOI 10.1109/CVPR.2017.283
   Zhu AN, 2016, PATTERN RECOGN, V58, P204, DOI 10.1016/j.patcog.2016.04.011
NR 61
TC 25
Z9 25
U1 5
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2706
EP 2720
DI 10.1109/TMM.2020.3015037
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600013
DA 2024-07-18
ER

PT J
AU Zhang, JM
   Sang, JT
   Xu, KY
   Wu, SX
   Zhao, X
   Sun, YF
   Hu, YL
   Yu, J
AF Zhang, Jiaming
   Sang, Jitao
   Xu, Kaiyuan
   Wu, Shangxi
   Zhao, Xian
   Sun, Yanfeng
   Hu, Yongli
   Yu, Jian
TI Robust CAPTCHAs Towards Malicious OCR
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE CAPTCHAs; Optical character recognition software; Robustness;
   Distortion; Character recognition; Machine learning; Complexity theory;
   Adversarial example; CAPTCHA; OCR
ID RECOGNITION; SECURITY
AB Turing test was originally proposed to examine whether machine's behavior is indistinguishable from a human. The most popular and practical Turing test is CAPTCHA, which is to discriminate algorithm from human by offering recognition-alike questions. The recent development of deep learning has significantly advanced the capability of algorithm in solving CAPTCHA questions, forcing CAPTCHA designers to increase question complexity. Instead of designing questions difficult for both algorithm and human, this study attempts to employ the limitations of algorithm to design robust CAPTCHA questions easily solvable to human. Specifically, our data analysis observes that human and algorithm demonstrates different vulnerability to visual distortions: adversarial perturbation is significantly annoying to algorithm yet friendly to human. We are motivated to employ adversarially perturbed images for robust CAPTCHA design in the context of character-based questions. Four modules of multi-target attack, ensemble adversarial training, image preprocessing differentiable approximation, and expectation are proposed to address the characteristics of character-based CAPTCHA cracking. Qualitative and quantitative experimental results demonstrate the effectiveness of the proposed solution. We hope this study can lead to the discussions around adversarial attack/defense in CAPTCHA design and also inspire the future attempts in employing algorithm limitation for practical usage.
C1 [Zhang, Jiaming; Sang, Jitao; Xu, Kaiyuan; Wu, Shangxi; Zhao, Xian; Yu, Jian] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Zhang, Jiaming; Sang, Jitao; Xu, Kaiyuan; Wu, Shangxi; Zhao, Xian; Yu, Jian] Beijing Jiaotong Univ, Beijing Key Lab Traff Data Anal & Min, Beijing 100044, Peoples R China.
   [Zhang, Jiaming; Sang, Jitao] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Sun, Yanfeng; Hu, Yongli] Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Fac Informat Technol, Beijing 100124, Peoples R China.
   [Sun, Yanfeng; Hu, Yongli] Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing 100124, Peoples R China.
C3 Beijing Jiaotong University; Beijing Jiaotong University; Peng Cheng
   Laboratory; Beijing University of Technology; Beijing University of
   Technology
RP Hu, YL (corresponding author), Beijing Univ Technol, Beijing Key Lab Multimedia & Intelligent Software, Fac Informat Technol, Beijing 100124, Peoples R China.; Hu, YL (corresponding author), Beijing Univ Technol, Beijing Artificial Intelligence Inst, Fac Informat Technol, Beijing 100124, Peoples R China.
EM lanzhang1107@gmail.com; jtsang@bjtu.edu.cn; 15281106@bjtu.edu.cn;
   kirinng0709@gmail.com; lavender.zxshane@gmail.com; yfsun@bjut.edu.cn;
   huyongli@bjut.edu.cn; jianyu@bjtu.edu.cn
RI Wu, Shangxi/HPE-9791-2023; Yu, Jian/HJY-2670-2023
OI Hu, Yongli/0000-0003-0440-438X; Zhang, Jiaming/0000-0003-0991-7109
FU National Key R, and D Program of China [2018AAA0100604]; National
   Natural Science Foundation of China [61632004, 61832002, 61672518,
   U19B2039, 61632006, 61772048, 61672071, U1811463]; Beijing Talents
   Project [2017A24]; Beijing Outstanding Young Scientists Projects
   [BJJWZYJH01201910005018]
FX This work was supported in part by the National Key R, and D Program of
   China under Grant 2018AAA0100604, and in part by the National Natural
   Science Foundation of China under Grants 61632004, 61832002, 61672518,
   U19B2039, 61632006, 61772048, 61672071, and U1811463, and in part by the
   Beijing Talents Project (2017A24), and in part by the Beijing
   Outstanding Young Scientists Projects (BJJWZYJH01201910005018). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris. (Corresponding
   author: Yongli Hu.) Jiaming Zhang and Jitao Sang are with the School of
   Computer and Information Technology & Beijing Key Laboratory of Traffic
   Data Analysis and Mining, Beijing Jiaotong University, Beijing 100044,
   China, and also with the Peng Cheng Laboratory, Shenzhen 518055, China
CR [Anonymous], 1950, MIND, DOI 10.1093/mind/LIX.236.433
   Athalye A, 2018, PR MACH LEARN RES, V80
   Athalye A, 2018, PR MACH LEARN RES, V80
   Breuel TM, 2017, PROC INT CONF DOC, P11, DOI 10.1109/ICDAR.2017.12
   Breuel TM, 2013, PROC INT CONF DOC, P683, DOI 10.1109/ICDAR.2013.140
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009
   Chandavale A. A., 2009, 2009 2nd International Conference on Emerging Trends in Engineering and Technology (ICETET 2009), P258, DOI 10.1109/ICETET.2009.24
   Chen J, 2019, IEEE ACCESS, V7, P22246, DOI 10.1109/ACCESS.2019.2899044
   Chow Y. W., 2019, Advances in Cyber Security: Principles, Techniques, and Applications, P69
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957
   Geman D, 2015, P NATL ACAD SCI USA, V112, P3618, DOI 10.1073/pnas.1422953112
   Gemmeke JF, 2017, INT CONF ACOUST SPEE, P776, DOI 10.1109/ICASSP.2017.7952261
   George D, 2017, SCIENCE, V358, DOI 10.1126/science.aag2612
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hoque ME, 2006, PROCEEDINGS OF THE IEEE SOUTHEASTCON 2006, P165, DOI 10.1109/second.2006.1629343
   Imsamai M., 2010, 2010 INT C INFORM SC, P1, DOI [DOI 10.1109/ICISA.2010.5480258, 10.1109/ICISA.2010.5480258]
   Jenckel M, 2018, INT CONF FRONT HAND, P122, DOI 10.1109/ICFHR-2018.2018.00030
   Kurakin Alexey, 2017, INT C LEARN REPR
   Kwon H, 2018, IEICE T INF SYST, VE101D, P543, DOI 10.1587/transinf.2017EDL8175
   Li JF, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23138
   Li XL, 2020, LECT NOTES ELECTR EN, V586, P88, DOI 10.1007/978-981-32-9050-1_10
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Ling X, 2019, P IEEE S SECUR PRIV, P673, DOI 10.1109/SP.2019.00023
   Liu Q, 2015, PROC INT CONF DOC, P461, DOI 10.1109/ICDAR.2015.7333804
   Liu Y., 2017, PROC INT C LEARN REP
   Lv YP, 2016, IEEE C EVOL COMPUTAT, P4854, DOI 10.1109/CEC.2016.7744412
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Mori G, 2003, PROC CVPR IEEE, P134
   Ogiela M. R., 2018, CONCURRENCY COMPUT P, V30, P1
   Osadchy M, 2017, IEEE T INF FOREN SEC, V12, P2640, DOI 10.1109/TIFS.2017.2718479
   Park J.-S., 2019, PROC IEEE STUDENT C, P1
   Rajpurkar P., 2016, ARXIV
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392
   Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371
   Shi C., 2020, P ACM C COMP COMM SE, P1
   Sivakorn S, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P388, DOI 10.1109/EuroSP.2016.37
   Szegedy C, 2014, INT C LEARN REPR
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Vladu Adrian, 2018, PROC 6 INT C LEARN R
   von Ahn L, 2004, COMMUN ACM, V47, P57, DOI 10.1145/966389.966390
   von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379
   Nguyen VD, 2014, COMPUT SECUR, V45, P84, DOI 10.1016/j.cose.2014.05.004
   Vu Duc Nguyen, 2012, Applied Cryptography and Network Security. Proceedings 10th International Conference, ACNS 2012, P12, DOI 10.1007/978-3-642-31284-7_2
   Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153
   Ye GX, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P332, DOI 10.1145/3243734.3243754
   Ye Q, 2014, 2014 11TH IAPR INTERNATIONAL WORKSHOP ON DOCUMENT ANALYSIS SYSTEMS (DAS 2014), P319, DOI 10.1109/DAS.2014.31
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang Y, 2018, INT CONF CYBER DIST, P1, DOI 10.1109/CyberC.2018.00013
   Zhou W., 2018, PROC EURCONF COMPUT, P452
   Zi Y, 2020, IEEE T INF FOREN SEC, V15, P753, DOI 10.1109/TIFS.2019.2928622
NR 55
TC 11
Z9 12
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2575
EP 2587
DI 10.1109/TMM.2020.3013376
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA UE9PT
UT WOS:000688215600003
DA 2024-07-18
ER

PT J
AU Zheng, CM
   Wu, ZW
   Wang, T
   Cai, Y
   Li, Q
AF Zheng, Changmeng
   Wu, Zhiwei
   Wang, Tao
   Cai, Yi
   Li, Qing
TI Object-Aware Multimodal Named Entity Recognition in Social Media Posts
   With Adversarial Learning
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Social network services; Feature extraction; Task
   analysis; Logic gates; Data mining; Training; Named entity recognition;
   social media posts; adversarial training; bilinear attention network
ID ATTENTION
AB Named Entity Recognition (NER) in social media posts is challenging since texts are usually short and contexts are lacking. Most recent works show that visual information can boost the NER performance since images can provide complementary contextual information for texts. However, the image-level features ignore the mapping relations between fine-grained visual objects and textual entities, which results in error detection in entities with different types. To better exploit visual and textual information in NER, we propose an adversarial gated bilinear attention neural network (AGBAN). The model jointly extracts entity-related features from both visual objects and texts, and leverages an adversarial training to map two different representations into a shared representation. As a result, domain information contained in an image can be transferred and applied for extracting named entities in the text associated with the image. Experimental results on Tweets dataset demonstrate that our model outperforms the state-of-the-art methods. Moreover, we systematically evaluate the effectiveness of the proposed gated bilinear attention network in capturing the interactions of mutimodal features visual objects and textual words. Our results indicate that the adversarial training can effectively exploit commonalities across heterogeneous data sources, which leads to improved performance in NER when compared to models purely exploiting text data or combining the image-level visual features.
C1 [Zheng, Changmeng; Wu, Zhiwei; Cai, Yi] South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
   [Wang, Tao] Kings Coll London, Dept Biostat & Hlth Informat, London WC2R 2LS, England.
   [Li, Qing] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 South China University of Technology; University of London; King's
   College London; Hong Kong Polytechnic University
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510640, Peoples R China.
EM sethecharm@mail.scut.edu.cn; zhiwei.w@qq.com; wtgmme@gmail.com;
   ycai@scut.edu.cn; itqli@cityu.edu.hk
RI zheng, yi/JOZ-7204-2023; Li, Qing/JMH-1365-2023
OI Li, Qing/0000-0003-3370-471X; zheng, changmeng/0000-0002-2945-8248;
   Wang, Tao/0000-0002-0437-0557
FU Fundamental Research Funds for the Central Universities, SCUT
   [2017ZD048, D2182480]; Science and Technology Planning Project of
   Guangdong Province [2017B050506004]; Science and Technology Programs of
   Guangzhou [201704030076, 201802010027, 201902010046]; CUHK Research
   Committee Funding [EE16963]; Hong Kong Research Grants Council [C1031-18
   G]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities, SCUT under Grants 2017ZD048 and D2182480, in
   part by the Science and Technology Planning Project of Guangdong
   Province under Grant 2017B050506004, in part by the Science and
   Technology Programs of Guangzhou under Grants 201704030076,
   201802010027, and 201902010046 and the collaborative research grants
   from a CUHK Research Committee Funding (Direct Grants) (Project Code:
   EE16963) and the Hong Kong Research Grants Council (Project no. C1031-18
   G).
CR Akbik Alan, 2018, P 27 INT C COMP LING, P1638
   Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636
   [Anonymous], 2017, P 3 WORKSH NOIS US G, DOI [DOI 10.18653/V1/W17-4419, 10.18653/v1/W17-4419]
   [Anonymous], 2014, P 2014 C EMP METH NA, DOI DOI 10.3115/V1/D14-1005
   Baldwin T., 2015, P WORKSH NOIS US GEN, P126, DOI [10.18653/v1/W15-4319, DOI 10.18653/V1/W15-4319]
   Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607
   Bruni E, 2014, J ARTIF INTELL RES, V49, P1, DOI 10.1613/jair.4135
   Chen FH, 2018, IEEE T MULTIMEDIA, V20, P997, DOI 10.1109/TMM.2017.2757769
   Chen X., 2018, Transactions of the Association for Computational Linguistics, V6, P557, DOI 10.1162/tacl_a_00039
   Chen X, 2016, ADV NEUR IN, V29
   Collell G, 2017, AAAI CONF ARTIF INTE, P4378
   Devlin J., 2018, BERT PRE TRAINING DE
   EGLY R, 1994, J EXP PSYCHOL GEN, V123, P161, DOI 10.1037/0096-3445.123.2.161
   Finkel Jenny Rose, 2005, ACL, P363
   Ganin Y, 2015, PR MACH LEARN RES, V37, P1180
   Gimpel Kevin, 2011, P ACL, V2, P42
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Gupta N., 2017, P 2017 C EMP METH NA, P2681, DOI DOI 10.18653/V1/D17-1284
   He Kaiming, 2017, P IEEE INT C COMPUTE
   He L, 2017, IEEE INT CON MULTI, P1153, DOI 10.1109/ICME.2017.8019549
   Ji RR, 2019, IEEE T MULTIMEDIA, V21, P1062, DOI 10.1109/TMM.2018.2867718
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Khandpur RP, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1049, DOI 10.1145/3132847.3132866
   Kim J.-H., 2018, ADV NEURAL INFORM PR, P1564
   Lample M., 2016, P NAACL HLT, P260, DOI DOI 10.18653/V1/N16-1030
   Li CL, 2015, IEEE T KNOWL DATA EN, V27, P558, DOI 10.1109/TKDE.2014.2327042
   Li Jiwei, 2017, P 2017 C EMP METH NA, P2157, DOI DOI 10.18653/V1/D17-1230
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Y, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P799
   Lu D, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1990
   Lu JS, 2016, ADV NEUR IN, V29
   Ma XZ, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1064
   Moon L., 2018, P 2018 C N AM CHAPTE, P852, DOI DOI 10.18653/V1/N18-1078
   Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232
   Paul M.J., 2011, ICWSM
   Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI [DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]
   Phuvipadawat S., 2010, Proceedings of the 2010 IEEE/ACM International Conference on Web Intelligence-Intelligent Agent Technology - Workshops (WI-IAT 2010), P120, DOI 10.1109/WI-IAT.2010.205
   Ritter A., 2011, P EMNLP, P1524
   Ritter A., 2012, P 18 ACM SIGKDD INT
   Ritter A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P896, DOI 10.1145/2736277.2741083
   Sakaki T, 2013, IEEE T KNOWL DATA EN, V25, P919, DOI 10.1109/TKDE.2012.29
   Sang EFTK, 1999, NINTH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS, P173
   Silberer C, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P721
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sukhbaatar Sainbayar, 2015, ADV NEURAL INFORM PR, P2440
   Tang YB, 2019, IEEE T MULTIMEDIA, V21, P2237, DOI 10.1109/TMM.2019.2900908
   Varga Istvan, 2013, P 51 ANN M ASS COMP, V1, P1619
   Wang BK, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P154, DOI 10.1145/3123266.3123326
   Wang DZ, 2019, IEEE T MULTIMEDIA, V21, P2985, DOI 10.1109/TMM.2019.2920620
   Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786
   Yang J, 2018, 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P74
   Zhang Q, 2018, AAAI CONF ARTIF INTE, P5674
   Zhang Yuan., 2017, Transactions of the Association for Computational Linguistics, V5, P515, DOI DOI 10.1162/TACL_A_00077
   Zheng SC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1227, DOI 10.18653/v1/P17-1113
NR 54
TC 333
Z9 342
U1 16
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PY 2021
VL 23
BP 2520
EP 2532
DI 10.1109/TMM.2020.3013398
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA TS3DH
UT WOS:000679533800028
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Ye, LW
   Liu, Z
   Wang, Y
AF Ye, Linwei
   Liu, Zhi
   Wang, Yang
TI Dual Convolutional LSTM Network for Referring Image Segmentation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image segmentation; Visualization; Decoding; Linguistics; Task analysis;
   Logic gates; Computer vision; Referring image segmentation;
   encoder-decoder; vision and language; deep learning
AB We consider referring image segmentation. It is a problem at the intersection of computer vision and natural language understanding. Given an input image and a referring expression in the form of a natural language sentence, the goal is to segment the object of interest in the image referred by the linguistic query. To this end, we propose a dual convolutional LSTM (ConvLSTM) network to tackle this problem. Our model consists of an encoder network and a decoder network, where ConvLSTM is used in both encoder and decoder networks to capture spatial and sequential information. The encoder network extracts visual and linguistic features for each word in the expression sentence, and adopts an attention mechanism to focus on words that are more informative in the multimodal interaction. The decoder network integrates the features generated by the encoder network at multiple levels as its input and produces the final precise segmentation mask. Experimental results on four challenging datasets demonstrate that the proposed network achieves superior segmentation performance compared with other state-of-the-art methods.
C1 [Ye, Linwei; Wang, Yang] Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.
   [Liu, Zhi] Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.
   [Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
C3 University of Manitoba; Shanghai University; Shanghai University
RP Wang, Y (corresponding author), Univ Manitoba, Dept Comp Sci, Winnipeg, MB R3T 2N2, Canada.; Liu, Z (corresponding author), Shanghai Univ, Shanghai Inst Adv Commun & Data Sci, Shanghai 200444, Peoples R China.; Liu, Z (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Shanghai 200444, Peoples R China.
EM yel3@cs.umanitoba.ca; liuzhisjtu@163.com; ywang@cs.umanitoba.ca
RI LIU, Zhi/D-4518-2012; Zhang, Han/JMR-0670-2023
OI LIU, Zhi/0000-0002-8428-1131; Ye, Linwei/0000-0002-7375-452X
FU NSERC; National Natural Science Foundation of China [61771301]; GETS
   Program at the University of Manitoba
FX This work was supported in part by the NSERC, in part by the National
   Natural Science Foundation of China under Grant 61771301, and in part by
   the GETS Program at the University of Manitoba.
CR [Anonymous], 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1003
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2019, CVPR
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2010, INT J COMPUT VISION, DOI [DOI 10.1007/s11263-009-0275-4, 10.1007/s11263-009-0275-4]
   Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344
   Gensler A, 2016, IEEE SYS MAN CYBERN, P2858, DOI 10.1109/SMC.2016.7844673
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   He Kaiming, 2017, P IEEE INT C COMPUTE
   Hu RH, 2016, LECT NOTES COMPUT SC, V9905, P108, DOI 10.1007/978-3-319-46448-0_7
   Escalante HJ, 2010, COMPUT VIS IMAGE UND, V114, P419, DOI 10.1016/j.cviu.2009.03.008
   Kazemzadeh S., 2014, EMNLP, P787, DOI DOI 10.3115/V1/D14-1086
   Kingma D. P., 2015, PROC INT C LEARN REP, P1
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li RY, 2018, PROC CVPR IEEE, P5745, DOI 10.1109/CVPR.2018.00602
   Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu CX, 2017, IEEE I CONF COMP VIS, P1280, DOI 10.1109/ICCV.2017.143
   Liu Z, 2012, IEEE T MULTIMEDIA, V14, P1275, DOI 10.1109/TMM.2012.2190385
   Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345
   Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9
   Marchi Erik, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P2164, DOI 10.1109/ICASSP.2014.6853982
   Margffoy-Tuay E, 2018, LECT NOTES COMPUT SC, V11215, P656, DOI 10.1007/978-3-030-01252-6_39
   Peng C, 2017, PROC CVPR IEEE, P1743, DOI 10.1109/CVPR.2017.189
   Rohrbach A, 2016, LECT NOTES COMPUT SC, V9905, P817, DOI 10.1007/978-3-319-46448-0_49
   Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Shi HC, 2018, LECT NOTES COMPUT SC, V11210, P38, DOI 10.1007/978-3-030-01231-1_3
   Shi XJ, 2015, ADV NEUR IN, V28
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Sutskever I, 2014, ADV NEUR IN, V27
   Underwood G, 2004, Q J EXP PSYCHOL-A, V57, P165, DOI 10.1080/02724980343000189
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Ye LW, 2017, IEEE T MULTIMEDIA, V19, P1742, DOI 10.1109/TMM.2017.2693022
   Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142
   Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5
   Zhang SH, 2017, IEEE I CONF COMP VIS, P3687, DOI 10.1109/ICCV.2017.396
   Zhang SY, 2017, IEEE WINT CONF APPL, P148, DOI 10.1109/WACV.2017.24
NR 40
TC 30
Z9 32
U1 1
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2020
VL 22
IS 12
BP 3224
EP 3235
DI 10.1109/TMM.2020.2971171
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA OU9BG
UT WOS:000591817700016
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yi, Y
   Wang, HL
   Li, QY
AF Yi, Yun
   Wang, Hanli
   Li, Qinyu
TI Affective Video Content Analysis With Adaptive Fusion Recurrent Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive systems; Correlation; Computer science; Motion pictures; Hidden
   Markov models; Support vector machines; Task analysis; Affective video
   content analysis; adaptive fusion; recurrent neural network;
   statistical-data layer; multiple modalities
ID DATABASE
AB Affective video content analysis is an important research topic in video content analysis and has extensive applications. Intuitively, multimodal features can depict elicited emotions, and the accumulation of temporal inputs influences the viewer's emotion. Although a number of research works have been proposed for this task, the adaptive weights of modalities and the correlation of temporal inputs are still not well studied. To address these issues, a novel framework is designed to learn the weights of modalities and temporal inputs from video data. Specifically, three network layers are designed, including statistical-data layer to improve the robustness of data, temporal-adaptive-fusion layer to fuse temporal inputs, and multimodal-adaptive-fusion layer to combine multiple modalities. In particular, the feature vectors of three input modalities are respectively extracted from three pre-trained convolutional neural networks and then fed to three statistical-data layers. Then, the output vectors of these three statistical-data layers are separately connected to three recurrent layers, and the corresponding outputs are fed to a fully-connected layer which shares parameters across modalities and temporal inputs. Finally, the outputs of the fully-connected layer are fused by the temporal-adaptive-fusion layer and then combined by the multimodal-adaptive-fusion layer. To discover the correlation of both multiple modalities and temporal inputs, adaptive weights of modalities and temporal inputs are introduced into loss functions for model training, and these weights are learned by an optimization algorithm. Extensive experiments are conducted on two challenging datasets, which demonstrate that the proposed method achieves better performances than baseline and other state-of-the-art methods.
C1 [Yi, Yun] Gannan Normal Univ, Dept Math & Comp Sci, Ganzhou 341000, Peoples R China.
   [Yi, Yun; Li, Qinyu] Tongji Univ, Dept Comp Sci & Technol, Shanghai 201804, Peoples R China.
   [Wang, Hanli] Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.
   [Wang, Hanli] Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
   [Li, Qinyu] Lanzhou City Univ, Dept Comp Sci, Lanzhou 730070, Peoples R China.
C3 Gannan Normal University; Tongji University; Tongji University; Tongji
   University; Lanzhou City University
RP Wang, HL (corresponding author), Tongji Univ, Dept Comp Sci & Technol, Key Lab Embedded Syst & Serv Comp, Minist Educ, Shanghai 200092, Peoples R China.; Wang, HL (corresponding author), Tongji Univ, Shanghai Inst Intelligent Sci & Technol, Shanghai 200092, Peoples R China.
EM 13yiyun@tongji.edu.cn; hanliwang@tongji.edu.cn; qinyu.li@tongji.edu.cn
RI Wang, Hanli/G-5111-2014; Yi, Yun/O-8432-2018; yuan, lin/JDW-7387-2023
OI Wang, Hanli/0000-0002-9999-4871; Yi, Yun/0000-0002-5644-8002; 
FU National Natural Science Foundation of China [61976159, 61622115,
   61962003]; Shanghai Engineering Research Center of Industrial Vision
   Perception & Intelligent Computing [17DZ2251600]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61976159, 61622115, and 61962003, and
   the Shanghai Engineering Research Center of Industrial Vision Perception
   & Intelligent Computing under Grant 17DZ2251600. Y. Yi and H. Wang are
   joint first authors contributed equally to this work. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Dr. Marco Bertini.
CR Anastasia T., 2016, P MEDIAEVAL WORKSH O
   [Anonymous], 2017, MULTIMEDIA TOOLS APP, DOI DOI 10.1007/S11042-017-4416-4
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 1997, NEURAL COMPUT
   [Anonymous], 2007, LECT NOTES COMPUT SC
   [Anonymous], 2017, P IEEE VIRT REAL ANN
   [Anonymous], 2019, MULTIMEDIA TOOLS APP, DOI DOI 10.1007/S11042-018-5662-9
   Baecchi C, 2017, PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL (ICMR'17), P72, DOI 10.1145/3078971.3079027
   Baveye Y, 2018, IEEE T AFFECT COMPUT, V9, P396, DOI 10.1109/TAFFC.2017.2661284
   Baveye Y, 2015, IEEE T AFFECT COMPUT, V6, P43, DOI 10.1109/TAFFC.2015.2396531
   Chakraborty R., 2015, P MEDIAEVAL WORKSH S
   Chen C., 2016, P ACM INT C MULT OCT, P127
   Chen S., 2016, P MEDIAEVAL WORKSH O
   Chen TF, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P769, DOI 10.1145/3123266.3123352
   Cho K., 2014, PROCS C EMPIRICAL ME, P1724, DOI DOI 10.3115/V1/D14-1179
   Dai Q., 2015, P MEDIAEVAL WORKSH S
   Dellandrea E., 2016, P MEDIAEVAL WORKSH O
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Gan Q, 2017, IEEE I CONF COMP VIS, P5123, DOI 10.1109/ICCV.2017.547
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Jan A., 2016, P MEDIAEVAL WORKSH O
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jiang YG, 2014, AAAI CONF ARTIF INTE, P73
   Khosla A, 2015, IEEE I CONF COMP VIS, P2390, DOI 10.1109/ICCV.2015.275
   King DB, 2015, ACS SYM SER, V1214, P1
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Kowalska M., 2017, HDB COGNITION EMOTIO, P1, DOI [https://doi.org/10.1007/978-3-319-28099-8495-1, DOI 10.1002/0470013494.CH3, 10.1002/0470013494.ch3]
   Lam V., 2015, P MEDIAEVAL WORKSH S
   Liu Y., 2016, HIGHER ED MERITOCRAC
   Ma Y., 2016, CLIN OBSERVATION REI
   Mironica I., 2015, P MEDIAEVAL WORKSH S
   Pang L, 2015, IEEE T MULTIMEDIA, V17, P2008, DOI 10.1109/TMM.2015.2482228
   Schaefer A, 2010, COGNITION EMOTION, V24, P1153, DOI 10.1080/02699930903274322
   Seddati O., 2015, P MEDIAEVAL WORKSH S
   Simonyan K, 2014, ADV NEUR IN, V27
   Sjoberg M., 2015, P MEDIAEVAL WORKSH S
   Snoek C. G. M., 2005, 13th Annual ACM International Conference on Multimedia, P399, DOI 10.1145/1101149.1101236
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Trigeorgis G., 2015, P MEDIAEVAL WORKSH S
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Vlastelica P. Marin, 2015, P MEDIAEVAL WORKSH S
   Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2
   Wang SF, 2015, IEEE T AFFECT COMPUT, V6, P410, DOI 10.1109/TAFFC.2015.2432791
   Wundt W. M., 1874, GRUNDZUGE PHYSL PSYC, P112
   Xu M., 2008, P ACM INT C MULT OCT, P677
   Yadati K, 2014, IEEE T MULTIMEDIA, V16, P15, DOI 10.1109/TMM.2013.2282128
   Yi Y., 2015, P MEDIAEVAL WORKSH S
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   Zhang SL, 2010, IEEE T MULTIMEDIA, V12, P510, DOI 10.1109/TMM.2010.2059634
NR 53
TC 12
Z9 12
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD SEPT
PY 2020
VL 22
IS 9
BP 2454
EP 2466
DI 10.1109/TMM.2019.2955300
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA NE0TL
UT WOS:000562310200019
DA 2024-07-18
ER

PT J
AU Chen, YY
   Xiao, XL
   Zhou, YC
AF Chen, Yongyong
   Xiao, Xiaolin
   Zhou, Yicong
TI Jointly Learning Kernel Representation Tensor and Affinity Matrix for
   Multi-View Clustering
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Tensors; Kernel; Symmetric matrices; Sparse matrices; Matrix
   decomposition; Correlation; Clustering algorithms; Multi-view
   clustering; low-rank tensor represen-tation; kernel trick; affinity
   matrix; adaptive weight
ID LOW-RANK; GRAPH
AB Multi-view clustering refers to the task of partitioning numerous unlabeled multimedia data into several distinct clusters using multiple features. In this paper, we propose a novel nonlinear method called joint learning multi-view clustering (JLMVC) to jointly learn kernel representation tensor and affinity matrix. The proposed JLMVC has three advantages: (1) unlike existing low-rank representation-based multi-view clustering methods that learn the representation tensor and affinity matrix in two separate steps, JLMVC jointly learns them both; (2) using the "kernel trick," JLMVC can handle nonlinear data structures for various real applications; and (3) different from most existing methods that treat representations of all views equally, JLMVC automatically learns a reasonable weight for each view. Based on the alternating direction method of multipliers, an effective algorithm is designed to solve the proposed model. Extensive experiments on eight multimedia datasets demonstrate the superiority of the proposed JLMVC over state-of-the-art methods.
C1 [Chen, Yongyong; Xiao, Xiaolin; Zhou, Yicong] Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
   [Xiao, Xiaolin] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 University of Macau; South China University of Technology
RP Zhou, YC (corresponding author), Univ Macau, Dept Comp & Informat Sci, Macau 999078, Peoples R China.
EM YongyongChen.cn@gmail.com; shellyxiaolin@gmail.com; yicongzhou@um.edu.mo
RI Chen, yongyong/U-8997-2019; Zhou, Yicong/A-8017-2009
OI Chen, yongyong/0000-0003-1970-1993; Zhou, Yicong/0000-0002-4487-6384
FU Science and Technology Development Fund, Macau SAR [189/2017/A3];
   Research Committee at University of Macau [MYRG201600123-FST,
   MYRG2018-00136-FST]
FX This work was supported in part by the Science and Technology
   Development Fund, Macau SAR (File no. 189/2017/A3), and in part by the
   Research Committee at University of Macau under Grants MYRG201600123-FST
   and MYRG2018-00136-FST. The associate editor coordinating the review of
   this manuscript and approving it for publication was Dr. Marco Carli.
CR Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317
   Cao XC, 2015, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2015.7298657
   Chao G., 2017, A survey on multi-view clustering
   Chao GQ, 2019, INFORM SCIENCES, V494, P278, DOI 10.1016/j.ins.2019.04.039
   Chen YY, 2018, IEEE J-STSP, V12, P1364, DOI 10.1109/JSTSP.2018.2873148
   Chen YY, 2017, IEEE T GEOSCI REMOTE, V55, P5366, DOI 10.1109/TGRS.2017.2706326
   Chen YY, 2017, SIGNAL IMAGE VIDEO P, V11, P1271, DOI 10.1007/s11760-017-1084-9
   Cheng B, 2011, IEEE I CONF COMP VIS, P2439, DOI 10.1109/ICCV.2011.6126528
   Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Guo XJ, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3547
   Hu WR, 2017, IEEE T NEUR NET LEAR, V28, P2961, DOI 10.1109/TNNLS.2016.2611525
   Ji P, 2017, ADV NEUR IN, V30
   Kang Z, 2020, IEEE T CYBERNETICS, V50, P1833, DOI 10.1109/TCYB.2018.2887094
   Kilmer ME, 2011, LINEAR ALGEBRA APPL, V435, P641, DOI 10.1016/j.laa.2010.09.020
   Kuanar SK, 2015, IEEE T MULTIMEDIA, V17, P1166, DOI 10.1109/TMM.2015.2443558
   Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88
   Liu J, 2013, INT SYMP ASYNCHRON C, P1, DOI 10.1109/ASYNC.2013.29
   Lu CY, 2019, IEEE T PATTERN ANAL, V41, P487, DOI 10.1109/TPAMI.2018.2794348
   Luo SR, 2018, AAAI CONF ARTIF INTE, P3730
   Ng AY, 2002, ADV NEUR IN, V14, P849
   Nie FP, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2022, DOI 10.1145/3219819.3220049
   Nie FP, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2564
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1962
   Nie FP, 2018, IEEE T IMAGE PROCESS, V27, P1501, DOI 10.1109/TIP.2017.2754939
   Nie FP, 2017, AAAI CONF ARTIF INTE, P2408
   Nie FP, 2016, AAAI CONF ARTIF INTE, P1969
   Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726
   Patel VM, 2014, IEEE IMAGE PROC, P2849, DOI 10.1109/ICIP.2014.7025576
   Peng X, 2017, IEEE T CYBERNETICS, V47, P1053, DOI 10.1109/TCYB.2016.2536752
   Qu Y., 2017, ARXIV170905083
   Tang C, 2019, IEEE T MULTIMEDIA, V21, P1724, DOI 10.1109/TMM.2018.2889560
   Wang H, 2020, IEEE T KNOWL DATA EN, V32, P1116, DOI 10.1109/TKDE.2019.2903810
   Wang H, 2016, IEEE DATA MINING, P1245, DOI [10.1109/ICDM.2016.34, 10.1109/ICDM.2016.0167]
   Wang X, 2017, ACSR ADV COMPUT, V82, P923
   Wang Y, 2018, IEEE T NEUR NET LEAR, V29, P4833, DOI 10.1109/TNNLS.2017.2777489
   Wu X, 2008, IEEE T MULTIMEDIA, V10, P188, DOI 10.1109/TMM.2007.911778
   Xia RK, 2014, AAAI CONF ARTIF INTE, P2149
   Xiao SJ, 2016, IEEE T NEUR NET LEAR, V27, P2268, DOI 10.1109/TNNLS.2015.2472284
   Xie Y, 2020, IEEE T CYBERNETICS, V50, P572, DOI 10.1109/TCYB.2018.2869789
   Xie Y, 2018, INT J COMPUT VISION, V126, P1157, DOI 10.1007/s11263-018-1086-2
   Yang SJ, 2019, IEEE T MULTIMEDIA, V21, P2916, DOI 10.1109/TMM.2019.2912735
   Yi SY, 2019, IEEE T MULTIMEDIA, V21, P1399, DOI 10.1109/TMM.2018.2877888
   Yin M, 2019, IEEE T NEUR NET LEAR, V30, P851, DOI 10.1109/TNNLS.2018.2851444
   Yin M, 2018, IEEE T IMAGE PROCESS, V27, P3716, DOI 10.1109/TIP.2018.2825647
   Zhan K, 2018, IEEE T CYBERNETICS, V48, P2887, DOI 10.1109/TCYB.2017.2751646
   Zhang CQ, 2020, IEEE T PATTERN ANAL, V42, P86, DOI 10.1109/TPAMI.2018.2877660
   Zhang CQ, 2017, PROC CVPR IEEE, P4333, DOI 10.1109/CVPR.2017.461
   Zhang CQ, 2015, IEEE I CONF COMP VIS, P1582, DOI 10.1109/ICCV.2015.185
   Zhang Z, 2018, LECT NOTES COMPUT SC, V11216, P731, DOI 10.1007/978-3-030-01258-8_44
   Zhang ZZ, 2019, IEEE T MULTIMEDIA, V21, P2878, DOI 10.1109/TMM.2019.2915036
   Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921
   ZHOU T, 2018, IEEE T CYBERN
   Zhuang LS, 2015, IEEE T IMAGE PROCESS, V24, P3717, DOI 10.1109/TIP.2015.2441632
NR 54
TC 80
Z9 82
U1 3
U2 25
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2020
VL 22
IS 8
BP 1985
EP 1997
DI 10.1109/TMM.2019.2952984
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MR2MS
UT WOS:000553424500006
DA 2024-07-18
ER

PT J
AU Wang, YL
   Su, H
   Zhang, B
   Hu, XL
AF Wang, Yulong
   Su, Hang
   Zhang, Bo
   Hu, Xiaolin
TI Learning Reliable Visual Saliency For Model Explanations
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Visualization; Reliability; Predictive models; Task analysis;
   Perturbation methods; Backpropagation; Real-time systems; Model
   interpretability; adversarial example defense; visual salience; deep
   learning
AB By highlighting important features that contribute to model prediction, visual saliency is used as a natural form to interpret the working mechanism of deep neural networks. Numerous methods have been proposed to achieve better saliency results. However, we find that previous visual saliency methods are not reliable enough to provide meaningful interpretation through a simple sanity check: saliency methods are required to explain the output of non-maximum prediction classes, which are usually not ground-truth classes. For example, let the methods interpret an image of "dog" given a wrong class label "fish" as the query. This procedure can test whether these methods reliably interpret model's predictions based on existing features that appear in the data. Our experiments show that previous methods failed to pass the test by generating similar saliency maps or scattered patterns. This false saliency response can be dangerous in certain scenarios, such as medical diagnosis. We find that these failure cases are mainly due to the attribution vanishing and adversarial noise within these methods. In order to learn reliable visual saliency, we propose a simple method that requires the output of the model to be close to the original output while learning an explanatory saliency mask. To enhance the smoothness of the optimized saliency masks, we then propose a simple Hierarchical Attribution Fusion (HAF) technique. In order to fully evaluate the reliability of visual saliency methods, we propose a new task Disturbed Weakly Supervised Object Localization (D-WSOL) to measure whether these methods can correctly attribute the model's output to existing features. Experiments show that previous methods fail to meet this standard, and our approach helps to improve the reliability by suppressing false saliency responses. After observing a significant layout difference in saliency masks between real and adversarial samples. we propose to train a simple CNN on these learned hierarchical attribution masks to distinguish adversarial samples. Experiments show that our method can improve detection performance over other approaches significantly.
C1 [Wang, Yulong; Su, Hang; Zhang, Bo; Hu, Xiaolin] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Comp Sci & Technol, Inst Artificial Intelligence,State Key Lab Intell, Beijing 100084, Peoples R China.
   [Zhang, Bo; Hu, Xiaolin] Tsinghua Univ, Ctr Brain Inspired Comp Res CBICR, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Hu, XL (corresponding author), Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Dept Comp Sci & Technol, Inst Artificial Intelligence,State Key Lab Intell, Beijing 100084, Peoples R China.
EM wang-yl15@mails.tsinghua.edu.cn; suhangss@mail.tsinghua.edu.cn;
   dcszb@tsinghua.edu.cn; xlhu@tsinghua.edu.cn
OI Hu, Xiaolin/0000-0002-4907-7354
FU National Key Research and Development Program of China [2017YFA0700904];
   National Natural Science Foundation of China [61836014, 61621136008,
   61620106010]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFA0700904, and in part by
   the National Natural Science Foundation of China under Grant 61836014,
   Grant 61621136008, and Grant 61620106010.
CR Adebayo J., 2018, ADV NEURAL INFORM PR, P9525, DOI DOI 10.48550/ARXIV.1810.03292
   Ancona Marco, 2018, INT C LEARNING REPRE
   Ang J.X., 2017, ARXIV171108998
   [Anonymous], 2014, WORKSHOP INT C LEARN
   Bargal S. A., 2019, P C COMP VIS PATT RE, P67
   Bargal S. A., 2018, ARXIV181202626
   Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354
   Cao CS, 2015, IEEE I CONF COMP VIS, P2956, DOI 10.1109/ICCV.2015.338
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Dabkowski Piotr., 2017, P 31 INT C NEUR INF, DOI DOI 10.48550/ARXIV.1705.07857
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Erhan D, 2009, Univ Montr, V1341, P1
   Feinman R., 2017, Detecting adversarial samples from artifacts
   Fong R, 2018, PROC CVPR IEEE, P8730, DOI 10.1109/CVPR.2018.00910
   Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371
   Ghorbani Amirata., 2017, AAAI
   Goodfellow I. J., 2015, 3 INT C LEARNING REP
   Guidotti R, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3236009
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He Kaiming, 2016, EUR C COMP VIS ECCV, DOI [DOI 10.1109/CVPR.2016.90, DOI 10.1007/978-3-319-46493-0_38]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kim J., 2017, P COMP VIS PATT REC
   Kindermans P.-J., 2018, P INT C LEARN REPR
   Kingma D. P., 2014, arXiv
   Koh PW, 2017, PR MACH LEARN RES, V70
   Kolen JF, 2009, A Field Guide to Dynamical Recurrent Networks, DOI [DOI 10.1109/9780470544037.CH14, 10.1109/9780470544037]
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A., 2016, WORKSHOP TRACK P
   Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Ma X., 2018, INT C LEARN REPR
   Madry A., 2018, ARXIV
   Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155
   Metzen J. H., 2012, P INT C LEARN REPR
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Nie WL, 2018, PR MACH LEARN RES, V80
   Pang T, 2018, ADV NEURAL INFORM PR, P4579, DOI DOI 10.5555/3327345.3327369
   Pedersen T, 2004, HLT NAACL 2004, VHLTNAACL, P38, DOI DOI 10.3115/1614025.1614037
   Petsiuk V., 2018, RISE RANDOMIZED INPU
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778
   Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI [10.1007/s11263-019-01228-7, 10.1109/ICCV.2017.74]
   Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.5555/3305890.3306006
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Springenberg J. T., 2015, ARXIV PREPRINT ARXIV
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tramer F., 2017, P INT C LEARN REPR
   Wang YL, 2018, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2018.00928
   Wu Y., 2016, GOOGLES NEURAL MACHI
   Zhang JM, 2016, LECT NOTES COMPUT SC, V9908, P543, DOI 10.1007/978-3-319-46493-0_33
   Zhang QS, 2018, FRONT INFORM TECH EL, V19, P27, DOI 10.1631/FITEE.1700808
   Zhang QS, 2018, PROC CVPR IEEE, P8827, DOI 10.1109/CVPR.2018.00920
   Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378
   Zhou B., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1109/CVPR.2016.319
NR 56
TC 18
Z9 19
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2020
VL 22
IS 7
BP 1796
EP 1807
DI 10.1109/TMM.2019.2949872
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA MG4GJ
UT WOS:000545990500012
DA 2024-07-18
ER

PT J
AU Tsai, CC
   Hsu, KJ
   Lin, YY
   Qian, XN
   Chuang, YY
AF Tsai, Chung-Chi
   Hsu, Kuang-Jui
   Lin, Yen-Yu
   Qian, Xiaoning
   Chuang, Yung-Yu
TI Deep Co-Saliency Detection via Stacked Autoencoder-Enabled Fusion and
   Self-Trained CNNs
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Proposals; Saliency detection; Image segmentation; Image reconstruction;
   Reliability; Task analysis; Fuses; Co-saliency detection; stacked
   autoencoder; reconstruction residual; adaptive fusion; optimization;
   self-paced learning; CNNs
ID OBJECT DETECTION; SEGMENTATION
AB Image co-saliency detection via fusion-based or learning-based methods faces cross-cutting issues. Fusion-based methods often combine saliency proposals using a majority voting rule. Their performance hence highly depends on the quality and coherence of individual proposals. Learning-based methods typically require ground-truth annotations for training, which are not available for co-saliency detection. In this work, we present a two-stage approach to address these issues jointly. At the first stage, an unsupervised deep learning model with stacked autoencoder (SAE) is proposed to evaluate the quality of saliency proposals. It employs latent representations for image foregrounds, and auto-encodes foreground consistency and foreground-background distinctiveness in a discriminative way. The resultant model, SAE-enabled fusion (SAEF), can combine multiple saliency proposals to yield a more reliable saliency map. At the second stage, motivated by the fact that fusion often leads to over-smoothed saliency maps, we develop self-trained convolutional neural networks (STCNN) to alleviate this negative effect. STCNN takes the saliency maps produced by SAEF as inputs. It propagates information from regions of high confidence to those of low confidence. During propagation, feature representations are distilled, resulting in sharper and better co-saliency maps. Our approach is comprehensively evaluated on three benchmarks, including MSRC, iCoseg, and Cosal2015, and performs favorably against the state-of-the-arts. In addition, we demonstrate that our method can be applied to object co-segmentation and object co-localization, achieving the state-of-the-art performance in both applications.
C1 [Tsai, Chung-Chi; Qian, Xiaoning] Texas A&M Univ, Dept Elect & Comp Engn, Uvalde, TX 77843 USA.
   [Tsai, Chung-Chi; Hsu, Kuang-Jui; Chuang, Yung-Yu] Acad Sinica, Res Ctr Informat Technol Innovat, Taipei 115, Taiwan.
   [Hsu, Kuang-Jui; Chuang, Yung-Yu] Natl Taiwan Univ, Dept Comp Sci & Informat Engn, Taipei 106, Taiwan.
   [Lin, Yen-Yu] Natl Chiaorung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
C3 Texas A&M University System; Academia Sinica - Taiwan; National Taiwan
   University
RP Lin, YY (corresponding author), Natl Chiaorung Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
EM chungchi@tamu.edu; kjhsu@citi.sinica.edu.tw; lin@cs.nctu.edu.tw;
   xqian@ece.tamu.edu; cyy@csie.ntu.edu.tw
RI ; Tsai, Chung-Chi/W-9145-2018
OI Lin, Yen-Yu/0000-0002-7183-6070; Qian, Xiaoning/0000-0002-4347-2476;
   Hsu, Kuang-Jui/0000-0003-4055-3585; Tsai, Chung-Chi/0000-0003-1792-9978
FU Ministry of Science and Technology (MOST) [107-2628-E-001-005-MY3,
   108-2634-F-007-009]; MOST Joint Research Center for AI Technology and
   All Vista Healthcare [108-2634-F-002-004]; National Science Foundation
   [1547557, 1553281]; Direct For Computer & Info Scie & Enginr; Division
   of Computing and Communication Foundations [1553281] Funding Source:
   National Science Foundation; Division Of Integrative Organismal Systems;
   Direct For Biological Sciences [1547557] Funding Source: National
   Science Foundation
FX This work was supported in part by the Ministry of Science and
   Technology (MOST) under Grants 107-2628-E-001-005-MY3 and
   108-2634-F-007-009, in part by MOST Joint Research Center for AI
   Technology and All Vista Healthcare under Grant 108-2634-F-002-004, and
   in part by the National Science Foundation Awards under Grants 1547557
   and 1553281.
CR [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2018, IEEE T CIRC SYST VID, DOI DOI 10.1109/TCSVT.2017.2706264
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2014, AUTOENCODING VARIATI
   Aytekin C, 2018, IEEE T MULTIMEDIA, V20, P82, DOI 10.1109/TMM.2017.2713982
   Batra D, 2010, PROC CVPR IEEE, P3169, DOI 10.1109/CVPR.2010.5540080
   Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833
   Cao XC, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P997, DOI 10.1145/2647868.2655007
   Cao XC, 2014, IEEE T IMAGE PROCESS, V23, P4175, DOI 10.1109/TIP.2014.2332399
   Chang HS, 2015, COMPUT VIS IMAGE UND, V141, P18, DOI 10.1016/j.cviu.2015.06.004
   Cho M, 2015, PROC CVPR IEEE, P1201, DOI 10.1109/CVPR.2015.7298724
   Collins E, 2018, LECT NOTES COMPUT SC, V11218, P352, DOI 10.1007/978-3-030-01264-9_21
   Faktor A, 2013, IEEE I CONF COMP VIS, P1297, DOI 10.1109/ICCV.2013.164
   Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487
   Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77
   Fu HZ, 2015, PROC CVPR IEEE, P4428, DOI 10.1109/CVPR.2015.7299072
   Fu HZ, 2013, IEEE T IMAGE PROCESS, V22, P3766, DOI 10.1109/TIP.2013.2260166
   Fu KR, 2017, IEEE T MULTIMEDIA, V19, P1531, DOI 10.1109/TMM.2017.2679898
   Gong D, 2017, IEEE I CONF COMP VIS, P1670, DOI 10.1109/ICCV.2017.184
   Grant M., 2014, CVX MATLAB SOFTWARE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Hsu KJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P748
   Hsu KJ, 2018, LECT NOTES COMPUT SC, V11209, P502, DOI 10.1007/978-3-030-01228-1_30
   Huang F, 2017, IEEE T IMAGE PROCESS, V26, P1911, DOI 10.1109/TIP.2017.2669878
   Huo SW, 2018, IEEE T MULTIMEDIA, V20, P1350, DOI 10.1109/TMM.2017.2769801
   Jerripothula KR, 2018, IEEE T MULTIMEDIA, V20, P2466, DOI 10.1109/TMM.2018.2798294
   Jerripothula KR, 2017, PROC CVPR IEEE, P3881, DOI 10.1109/CVPR.2017.413
   Jerripothula KR, 2016, IEEE T MULTIMEDIA, V18, P1896, DOI 10.1109/TMM.2016.2576283
   Jerripothula KR, 2015, IEEE IMAGE PROC, P4639, DOI 10.1109/ICIP.2015.7351686
   Jerripothula KR, 2014, IEEE IMAGE PROC, P3277, DOI 10.1109/ICIP.2014.7025663
   Jiang L, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P547, DOI 10.1145/2647868.2654918
   Jiang P, 2015, IEEE I CONF COMP VIS, P217, DOI 10.1109/ICCV.2015.33
   Kumar A, 2010, ASIA PACIF MICROWAVE, P1189
   Lee C, 2015, PROC CVPR IEEE, P3837, DOI 10.1109/CVPR.2015.7299008
   Li CY, 2015, PROC CVPR IEEE, P2710, DOI 10.1109/CVPR.2015.7298887
   Li GB, 2016, PROC CVPR IEEE, P478, DOI 10.1109/CVPR.2016.58
   Li HL, 2011, IEEE T IMAGE PROCESS, V20, P3365, DOI 10.1109/TIP.2011.2156803
   Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370
   Li X, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P439, DOI 10.1145/3123266.3123290
   Li YJ, 2015, IEEE SIGNAL PROC LET, V22, P588, DOI 10.1109/LSP.2014.2364896
   Lillo I, 2016, PROC CVPR IEEE, P1981, DOI 10.1109/CVPR.2016.218
   Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80
   Liu Z, 2014, IEEE SIGNAL PROC LET, V21, P88, DOI 10.1109/LSP.2013.2292873
   Margolin R, 2014, PROC CVPR IEEE, P248, DOI 10.1109/CVPR.2014.39
   Pont-Tuset J, 2017, IEEE T PATTERN ANAL, V39, P128, DOI 10.1109/TPAMI.2016.2537320
   Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308
   Tang K, 2014, PROC CVPR IEEE, P1464, DOI 10.1109/CVPR.2014.190
   Tang YB, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1083, DOI 10.1145/2733373.2806287
   Tao ZQ, 2017, AAAI CONF ARTIF INTE, P4285
   Tasi CC, 2019, IEEE T IMAGE PROCESS, V28, P56, DOI 10.1109/TIP.2018.2861217
   Tsai CC, 2017, IEEE INT CON MULTI, P523, DOI 10.1109/ICME.2017.8019413
   Tsai CC, 2017, INT CONF ACOUST SPEE, P1897, DOI 10.1109/ICASSP.2017.7952486
   Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938
   Wang ZL, 2017, IEEE T MULTIMEDIA, V19, P750, DOI 10.1109/TMM.2016.2636739
   Wei LN, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3041
   Wei XS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3048
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Xia C, 2016, IEEE T NEUR NET LEAR, V27, P1227, DOI 10.1109/TNNLS.2015.2512898
   Xue JR, 2011, IEEE T IMAGE PROCESS, V20, P1177, DOI 10.1109/TIP.2010.2077643
   Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407
   Yao XW, 2017, IEEE T IMAGE PROCESS, V26, P3196, DOI 10.1109/TIP.2017.2694222
   Ye LW, 2015, IEEE SIGNAL PROC LET, V22, P2073, DOI 10.1109/LSP.2015.2458434
   Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436
   Zhang DW, 2017, IEEE T PATTERN ANAL, V39, P865, DOI 10.1109/TPAMI.2016.2567393
   Zhang DW, 2016, INT J COMPUT VISION, V120, P215, DOI 10.1007/s11263-016-0907-4
   Zhang DW, 2016, IEEE T NEUR NET LEAR, V27, P1163, DOI 10.1109/TNNLS.2015.2495161
   Zhang JM, 2015, IEEE I CONF COMP VIS, P1404, DOI 10.1109/ICCV.2015.165
   Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31
   Zhang PP, 2017, IEEE I CONF COMP VIS, P212, DOI 10.1109/ICCV.2017.32
   Zhao Q, 2015, AAAI CONF ARTIF INTE, P3196
   Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360
NR 75
TC 12
Z9 15
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2020
VL 22
IS 4
BP 1016
EP 1031
DI 10.1109/TMM.2019.2936803
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KY2ZI
UT WOS:000522440400016
OA hybrid
DA 2024-07-18
ER

PT J
AU Zhan, C
   Hu, H
   Wang, Z
   Fan, RF
   Niyato, D
AF Zhan, Cheng
   Hu, Han
   Wang, Zhi
   Fan, Rongfei
   Niyato, Dusit
TI Unmanned Aircraft System Aided Adaptive Video Streaming: A Joint
   Optimization Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive video streaming; unmanned aerial vehicle; transmission rate
   allocation; UAV trajectory; fading channel
ID NETWORK SLICE DESIGN; UAV COMMUNICATION; TRANSMISSION; CLOUD; HTTP
AB Due to the coverage constraint of a wireless base station, mobile users suffer from the unstable network connection and poor service quality, especially for the prevalent video services. As an alternative solution, an unmanned aerial vehicle (UAV) is able to reach the cell edge and serve ground users (GUs). In this paper, we extend the UAV applications to the more challenging adaptive streaming service over fading channel. First, we decompose the system into different modules, and present mathematical models for each of them, including a trajectory model of the UAV, fading channels between the UAV and GUs, and video streaming utility. Second, we formulate the problem as a non-convex optimization problem by optimizing the UAV trajectory and transmit power allocation, jointly with transmission schedule and rate allocation for multiple users. The objective is to maximize the overall utility while guaranteeing the fairness among multiple users under the UAV energy budget and rate-outage probability constraints. Third, to tackle this problem, we first analyze the relationship between transmission rate and rate-outage probability over the fading channel, and then divide the original problem into three subproblems, which can be solved by leveraging the successive convex approximation technique. Furthermore, an overall iterative algorithm over the three subproblems is proposed to obtain a locally optimal solution by applying the block coordinate descent technique. Finally, through extensive experiments, we demonstrate that the proposed design can achieve almost performance gain in terms of max-min streaming utility for all users, compared with other benchmark schemes.
C1 [Zhan, Cheng] Southwest Univ, Sch Comp & Informat Sci, Chongqing 400715, Peoples R China.
   [Hu, Han; Fan, Rongfei] Beijing Inst Technol, Sch Informat & Elect, Beijing 100811, Peoples R China.
   [Wang, Zhi] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen 518055, Peoples R China.
   [Niyato, Dusit] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Southwest University - China; Beijing Institute of Technology; Tsinghua
   University; Tsinghua Shenzhen International Graduate School; Nanyang
   Technological University
RP Hu, H (corresponding author), Beijing Inst Technol, Sch Informat & Elect, Beijing 100811, Peoples R China.
EM zhanc@swu.edu.cn; hhu@bit.edu.cn; wangzhi@sz.tsinghua.edu.cn;
   fanrongfei@bit.edu.cn; dniyato@ntu.edu.sg
RI Niyato, Dusit/Y-2769-2019
OI Niyato, Dusit/0000-0002-7442-7416; Hu, Han/0000-0001-7532-0496; Fan,
   Rongfei/0000-0001-8782-0615; Wang, Zhi/0000-0002-5462-6178
FU National Natural Science Foundation of China [61702426, 61872215];
   Fundamental Research Funds for the Central Universities [XDJK2019C084,
   3052019041]; SZSTI [JCYJ20180306174057899]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61702426 and 61872215, in part by
   Fundamental Research Funds for the Central Universities under Grants
   XDJK2019C084 and 3052019041, and in part by SZSTI under Grant
   JCYJ20180306174057899.
CR [Anonymous], [No title captured]
   [Anonymous], 2017, CISC VIS NETW IND GL
   Argyriou A, 2015, IEEE T MULTIMEDIA, V17, P736, DOI 10.1109/TMM.2015.2408254
   Azari MM, 2018, IEEE T COMMUN, V66, P330, DOI 10.1109/TCOMM.2017.2746105
   Bethanabhotla D, 2015, IEEE T COMMUN, V63, P268, DOI 10.1109/TCOMM.2014.2378774
   Boyd S., 2004, CONVEX OPTIMIZATION
   Cai SJ, 2016, IEEE J SEL AREA COMM, V34, P1103, DOI 10.1109/JSAC.2016.2520217
   Dai J, 2012, IEEE J SEL AREA COMM, V30, P458, DOI 10.1109/JSAC.2012.120226
   Galanopoulos Apostolos, 2015, 2015 IEEE 16th International Symposium on a World of Wireless, Mobile and Multimedia Networks (WoWMoM), P1, DOI 10.1109/WoWMoM.2015.7158121
   Grant M., 2014, CVX MATLAB SOFTWARE
   Gupta L., 2016, IEEE COMMUN SURV TUT, V18, P1123, DOI [10.1109/COMST.2015.2495297, DOI 10.1109/COMST.2015.2495297]
   Hong MY, 2016, IEEE SIGNAL PROC MAG, V33, P57, DOI 10.1109/MSP.2015.2481563
   Hu H, 2019, ACM T MULTIM COMPUT, V15, DOI 10.1145/3289184
   Hu H, 2018, IEEE T MULTIMEDIA, V20, P1864, DOI 10.1109/TMM.2017.2779041
   Hu H, 2018, IEEE T CIRC SYST VID, V28, P759, DOI 10.1109/TCSVT.2016.2620152
   Hu H, 2017, IEEE J SEL AREA COMM, V35, P935, DOI 10.1109/JSAC.2017.2676598
   Huang XL, 2016, IEEE T WIREL COMMUN, V15, P4839, DOI 10.1109/TWC.2016.2547861
   Liu FM, 2015, IEEE T COMPUT, V64, P3051, DOI 10.1109/TC.2015.2401032
   Mozaffari M, 2017, IEEE T WIREL COMMUN, V16, P7574, DOI 10.1109/TWC.2017.2751045
   Ono F, 2016, IEEE T WIREL COMMUN, V15, P7699, DOI 10.1109/TWC.2016.2606388
   Oo TZ, 2017, IEEE T MOBILE COMPUT, V16, P2276, DOI 10.1109/TMC.2016.2613864
   Reichl P, 2013, TELECOMMUN SYST, V52, P587, DOI 10.1007/s11235-011-9503-7
   Shu P, 2013, IEEE INFOCOM SER, P195
   Tan B, 2018, IEEE WIREL COMMUN, V25, P88, DOI 10.1109/MWC.2018.1800021
   Tan B, 2017, IEEE T MULTIMEDIA, V19, P2293, DOI 10.1109/TMM.2017.2733303
   Wu QQ, 2018, IEEE T WIREL COMMUN, V17, P2109, DOI 10.1109/TWC.2017.2789293
   Yan CG, 2020, IEEE T MULTIMEDIA, V22, P229, DOI 10.1109/TMM.2019.2924576
   Yan CG, 2018, IEEE T MULTIMEDIA, V20, P3389, DOI 10.1109/TMM.2018.2838320
   Yan CL, 2019, IEEE VTS VEH TECHNOL, DOI 10.1109/vtcspring.2019.8746411
   Zeng Y, 2017, IEEE T WIREL COMMUN, V16, P3747, DOI 10.1109/TWC.2017.2688328
   Zeng Y, 2016, IEEE T COMMUN, V64, P4983, DOI 10.1109/TCOMM.2016.2611512
   Zhan C, 2018, IEEE WIREL COMMUN LE, V7, P328, DOI 10.1109/LWC.2017.2776922
   Zhang HX, 2018, PROCEEDINGS OF ICRCA 2018: 2018 THE 3RD INTERNATIONAL CONFERENCE ON ROBOTICS, CONTROL AND AUTOMATION / ICRMV 2018: 2018 THE 3RD INTERNATIONAL CONFERENCE ON ROBOTICS AND MACHINE VISION, P1, DOI 10.1145/3265639.3265642
   Zhang SH, 2018, IEEE COMMUN LETT, V22, P161, DOI 10.1109/LCOMM.2017.2763135
   Zhang WW, 2013, IEEE T MULTIMEDIA, V15, P1431, DOI 10.1109/TMM.2013.2247583
   Zhao M, 2018, IEEE T MOBILE COMPUT, V17, P2853, DOI 10.1109/TMC.2018.2817220
   Zhu LY, 2018, INT WIREL COMMUN, P30, DOI 10.1109/IWCMC.2018.8450454
NR 37
TC 29
Z9 29
U1 5
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2020
VL 22
IS 3
BP 795
EP 807
DI 10.1109/TMM.2019.2931441
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KU2ZL
UT WOS:000519576700018
DA 2024-07-18
ER

PT J
AU Wu, ST
   Zhong, SH
   Liu, Y
AF Wu, Songtao
   Zhong, Sheng-hua
   Liu, Yan
TI A Novel Convolutional Neural Network for Image Steganalysis With Shared
   Normalization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Training; Feature extraction; Standards; Convolutional neural networks;
   Task analysis; Data models; Steganalysis; steganography; convolutional
   neural network; batch normalization; shared normalization
ID STEGANOGRAPHY
AB Image steganalysis is to discriminate innocent images (cover images) and those suspected images (stego images) with hidden messages. The task is challenging since modifications to cover images due to message hiding are extremely small. To handle this difficulty, modern approaches proposed using convolutional neural network (CNN) models to detect steganography with paired learning, i.e., cover images and their stegos are both in training set. In this paper, we explore an important technique in CNN models, the batch normalization (BN), for the task of image steganalysis in the paired learning framework. Our theoretical analysis shows that a CNN model with multiple batch normalization layers is difficult to be generalized to new data in the test set when it is well trained with paired learning. To address this problem, we propose a novel normalization technique called shared normalization (SN) in this paper. Unlike the BN layer utilizing the mini-batch mean and standard deviation to normalize each input batch, SN shares consistent statistics for training samples. Based on the proposed SN layer, we further propose a novel neural network model for image steganalysis. Extensive experiments demonstrate that the proposed network with SN layers is stable and can detect the state-of-the-art steganography with better performances than previous methods.
C1 [Wu, Songtao; Zhong, Sheng-hua] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Liu, Yan] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
C3 Shenzhen University; Hong Kong Polytechnic University
RP Zhong, SH (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
EM csstwu@szu.edu.cn; csshzhong@szu.edu.cn; csyliu@comp.polyu.edu.hk
RI liu, yan/HGV-1365-2022
OI LIU, Yan/0000-0003-4242-4840
FU Natural Science Foundation of Guangdong Province [2016A030310053]; Hong
   Kong Polytechnic University; communication platform at the Third
   Affiliated Hospital of Sun Yat-sen University; Shenzhen high-level
   overseas talents program; National Engineering Laboratory for Big Data
   System Computing Technology
FX This work was supported in part by the Natural Science Foundation of
   Guangdong Province under Grant 2016A030310053, in part by the Hong Kong
   Polytechnic University under Grant G-UAEU, in part by the communication
   platform at the Third Affiliated Hospital of Sun Yat-sen University, in
   part by the Shenzhen high-level overseas talents program, and in part by
   the National Engineering Laboratory for Big Data System Computing
   Technology.
CR [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], P 3 IAPR AS C PATT R
   [Anonymous], 2014, Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific, DOI [10.1109/APSIPA.2014, 10.1109/APSIPA.2014.7041565]
   Bas Patrick, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P59, DOI 10.1007/978-3-642-24178-9_5
   Box GE, 1994, TIME SERIES ANAL FOR
   Cheddad A, 2010, SIGNAL PROCESS, V90, P727, DOI 10.1016/j.sigpro.2009.08.010
   Chen M, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P75, DOI 10.1145/3082031.3083248
   Denemark T, 2014, IEEE INT WORKS INFOR, P48, DOI 10.1109/WIFS.2014.7084302
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Filler T, 2010, IEEE T INF FOREN SEC, V5, P705, DOI 10.1109/TIFS.2010.2077629
   Fridrich J, 2002, PROC SPIE, V4675, P1, DOI 10.1117/12.465263
   Fridrich J, 2012, IEEE T INF FOREN SEC, V7, P868, DOI 10.1109/TIFS.2012.2190402
   Gatys LA, 2016, PROC CVPR IEEE, P2414, DOI 10.1109/CVPR.2016.265
   Girshick R, 2016, IEEE T PATTERN ANAL, V38, P142, DOI 10.1109/TPAMI.2015.2437384
   Gonçalves GR, 2018, SIBGRAPI, P110, DOI 10.1109/SIBGRAPI.2018.00021
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Holub V, 2014, EURASIP J INF SECUR, DOI 10.1186/1687-417X-2014-1
   Holub V, 2012, IEEE INT WORKS INFOR, P234, DOI 10.1109/WIFS.2012.6412655
   Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Kodovsky J, 2012, IEEE T INF FOREN SEC, V7, P432, DOI 10.1109/TIFS.2011.2175919
   Li B, 2014, IEEE IMAGE PROC, P4206, DOI 10.1109/ICIP.2014.7025854
   Lin GS, 2010, IEEE T MULTIMEDIA, V12, P345, DOI 10.1109/TMM.2010.2051243
   Lin YT, 2017, IEEE T MULTIMEDIA, V19, P196, DOI 10.1109/TMM.2016.2605499
   Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8
   Provos N., 2002, proc. of Network and Distributed System Security Symposium (NDSS), P1
   Qian YL, 2016, IEEE IMAGE PROC, P2752, DOI 10.1109/ICIP.2016.7532860
   Qian YL, 2015, PROC SPIE, V9409, DOI 10.1117/12.2083479
   Schwamberger V, 2010, MM&SEC 2010: 2010 ACM SIGMM MULTIMEDIA AND SECURITY WORKSHOP, PROCEEDINGS, P225
   Szegedy Christian, 2015, IEEE C COMP VIS PATT, DOI [10.1109/cvpr.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Tang WX, 2017, IEEE SIGNAL PROC LET, V24, P1547, DOI 10.1109/LSP.2017.2745572
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Wang HQ, 2004, COMMUN ACM, V47, P76, DOI 10.1145/1022594.1022597
   Wu ST, 2018, MULTIMED TOOLS APPL, V77, P10437, DOI 10.1007/s11042-017-4440-4
   Wu ST, 2017, IEEE INT CON MULTI, P241, DOI 10.1109/ICME.2017.8019304
   Xu GS, 2017, IH&MMSEC'17: PROCEEDINGS OF THE 2017 ACM WORKSHOP ON INFORMATION HIDING AND MULTIMEDIA SECURITY, P67, DOI 10.1145/3082031.3083236
   Xu GS, 2016, IEEE SIGNAL PROC LET, V23, P708, DOI 10.1109/LSP.2016.2548421
   Ye J, 2017, IEEE T INF FOREN SEC, V12, P2545, DOI 10.1109/TIFS.2017.2710946
   Yin H, 2012, IEEE T MULTIMEDIA, V14, P178, DOI 10.1109/TMM.2011.2170556
   Yosinski J, 2014, ADV NEUR IN, V27
   Zeng JS, 2018, IEEE T INF FOREN SEC, V13, P1200, DOI 10.1109/TIFS.2017.2779446
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhang WM, 2017, IEEE T CIRC SYST VID, V27, P2274, DOI 10.1109/TCSVT.2016.2587388
   Zhang WM, 2016, IEEE T MULTIMEDIA, V18, P1469, DOI 10.1109/TMM.2016.2569497
   Zoran D, 2009, IEEE I CONF COMP VIS, P2209, DOI 10.1109/ICCV.2009.5459476
NR 46
TC 36
Z9 36
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2020
VL 22
IS 1
BP 256
EP 270
DI 10.1109/TMM.2019.2920605
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KB6AZ
UT WOS:000506577000022
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Yang, WM
   Zhang, XC
   Tian, YP
   Wang, W
   Xue, JH
   Liao, QM
AF Yang, Wenming
   Zhang, Xuechen
   Tian, Yapeng
   Wang, Wei
   Xue, Jing-Hao
   Liao, Qingmin
TI Deep Learning for Single Image Super-Resolution: A Brief Review
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Review
DE Single image super-resolution; deep learning; neural networks; objective
   function
ID NEURAL-NETWORKS
AB Single image super-resolution (SISR) is a notoriously challenging ill-posed problem that aims to obtain a high-resolution output from one of its low-resolution versions. Recently, powerful deep learning algorithms have been applied to SISR and have achieved state-of-the-art performance. In this survey, we review representative deep learning-based SISR methods and group them into two categories according to their contributions to two essential aspects of SISR: The exploration of efficient neural network architectures for SISR and the development of effective optimization objectives for deep SISR learning. For each category, a baseline is first established, and several critical limitations of the baseline are summarized. Then, representative works on overcoming these limitations are presented based on their original content, as well as our critical exposition and analyses, and relevant comparisons are conducted from a variety of perspectives. Finally, we conclude this review with some current challenges and future trends in SISR that leverage deep learning algorithms.
C1 [Yang, Wenming; Zhang, Xuechen; Wang, Wei; Liao, Qingmin] Tsinghua Univ, Dept Elect Engn, Grad Sch Shenzhen, Beijing 100091, Peoples R China.
   [Tian, Yapeng] Univ Rochester, Rochester, NY 14627 USA.
   [Xue, Jing-Hao] UCL, Dept Stat Sci, London WC1E 6BT, England.
C3 Tsinghua Shenzhen International Graduate School; Tsinghua University;
   University of Rochester; University of London; University College London
RP Yang, WM (corresponding author), Tsinghua Univ, Dept Elect Engn, Grad Sch Shenzhen, Beijing 100091, Peoples R China.
EM yang.wenming@sz.tsinghua.edu.cn; xc-zhang16@mails.tsinghua.edu.cn;
   ytian21@ur.rochester.edu; wangwei17@mails.tsinghua.edu.cn;
   jinghao.xue@ucl.ac.uk; liaoqm@.tsinghua.edu.cn
RI Tian, Yapeng/GSD-1491-2022
OI Tian, Yapeng/0000-0003-4271-5293; wang, wei/0000-0002-7715-9365; Yang,
   Wenming/0000-0002-2506-1286; Xue, Jing-Hao/0000-0003-1174-610X
FU National Natural Science Foundation of China [61471216, 61771276];
   National Key Research and Development Program of China [2016YFB0101001];
   Special Foundation for the Development of Strategic Emerging Industries
   of Shenzhen [JCYJ20170307153940960, JCYJ20170817161845824]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61471216 and 61771276 and in part by
   the National Key Research and Development Program of China under Grant
   2016YFB0101001, and in part by the Special Foundation for the
   Development of Strategic Emerging Industries of Shenzhen under Grants
   JCYJ20170307153940960 and JCYJ20170817161845824. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Raouf Hamzaoui.
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199
   [Anonymous], 2018, P EUR C COMP VIS ECC
   [Anonymous], 2017, PROC IEEE C COMPUT V
   [Anonymous], ARXIV181112866
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2015, arXiv
   [Anonymous], 2017, P IEEE C COMPUTER VI
   [Anonymous], 2008, PROC INT C MACHINE L
   [Anonymous], 2015, NATURE, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], 2015, PROC CVPR IEEE
   [Anonymous], INT C NEUR INT PROC
   [Anonymous], 2016, P ADV NEUR INF PROC
   [Anonymous], 2015, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2015.123
   [Anonymous], 2001, FIELD GUIDE DYNAMICA
   [Anonymous], 2011, 22 INT JT C ART INT, DOI 10.5555/2283516.2283603
   [Anonymous], 2016, Residual Networks Behave Like Ensembles of Relatively Shallow Networks
   [Anonymous], 2018, ARXIV181001406
   [Anonymous], 2014, ARXIV 1401 4082
   [Anonymous], 2017, P IEEE C COMP VIS PA
   Arjovsky M, 2017, arXiv preprint arXiv:1701.07875
   Balduzzi D, 2017, 34 INT C MACHINE LEA, V70
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blau Y, 2018, PROC CVPR IEEE, P6228, DOI 10.1109/CVPR.2018.00652
   Cao FL, 2016, IEEE T NEUR NET LEAR, V27, P1550, DOI 10.1109/TNNLS.2015.2512563
   Chang H, 2004, PROC CVPR IEEE, P275, DOI 10.1109/cvpr.2004.1315043
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Dahl R, 2017, IEEE I CONF COMP VIS, P5449, DOI 10.1109/ICCV.2017.581
   Dai SY, 2009, IEEE T IMAGE PROCESS, V18, P969, DOI 10.1109/TIP.2009.2012908
   Deng C, 2016, IEEE T NEUR NET LEAR, V27, P2472, DOI 10.1109/TNNLS.2015.2468069
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dong C, 2016, LECT NOTES COMPUT SC, V9906, P391, DOI 10.1007/978-3-319-46475-6_25
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13
   Du S, 2017, CHIN CONTR CONF, P4470, DOI 10.23919/ChiCC.2017.8028062
   DUCHON CE, 1979, J APPL METEOROL, V18, P1016, DOI 10.1175/1520-0450(1979)018<1016:LFIOAT>2.0.CO;2
   Dumoulin V, 2018, Arxiv, DOI [arXiv:1603.07285, DOI 10.48550/ARXIV.1603.07285]
   Efrat N, 2013, IEEE I CONF COMP VIS, P2832, DOI 10.1109/ICCV.2013.352
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Fan XX, 2018, SIGNAL PROCESS, V146, P50, DOI 10.1016/j.sigpro.2017.12.017
   Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gregor K., 2010, P 27 INT C INT C MAC, P399
   Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hui Z, 2018, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2018.00082
   Husz~ar F., 2015, ARXIV151105101
   Hyun Ah Song, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8226, P466, DOI 10.1007/978-3-642-42054-2_58
   IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43
   Ju JS, 2008, IEEE ICCE, P530
   KEYS RG, 1981, IEEE T ACOUST SPEECH, V29, P1153, DOI 10.1109/TASSP.1981.1163711
   Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.182, 10.1109/CVPR.2016.181]
   Kingma D. P., 2014, arXiv
   Kingma DP, 2013, ARXIV
   Lai WS, 2017, PROC CVPR IEEE, P5835, DOI 10.1109/CVPR.2017.618
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Ledig C., 2017, P IEEE C COMP VIS PA, P4681
   Li K., 2018, arXiv preprint arXiv:1809.09087
   Liu D, 2016, IEEE T IMAGE PROCESS, V25, P3194, DOI 10.1109/TIP.2016.2564643
   Liu JY, 2017, IEEE T MULTIMEDIA, V19, P302, DOI 10.1109/TMM.2016.2614427
   Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Mechrez R., 2018, P EUR C COMP VIS ECC, P768
   Mechrez Roey, 2018, ARXIV PREPRINT ARXIV
   Meinhardt T, 2017, IEEE I CONF COMP VIS, P1799, DOI 10.1109/ICCV.2017.198
   Michaeli T, 2013, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2013.121
   Montúfar G, 2014, ADV NEUR IN, V27
   Nowozin S., 2016, Advances in Neural Information Processing Systems, P271
   Park SC, 2003, IEEE SIGNAL PROC MAG, V20, P21, DOI 10.1109/MSP.2003.1203207
   Radford A., 2015, ARXIV151106434
   ROCHESTER N, 1956, IRE T INFORM THEOR, V2, P80, DOI 10.1109/TIT.1956.1056810
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sajjadi MSM, 2017, IEEE I CONF COMP VIS, P4501, DOI 10.1109/ICCV.2017.481
   Salakhutdinov Ruslan, 2010, INT C ART INT STAT, P693
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schulter S, 2015, PROC CVPR IEEE, P3791, DOI 10.1109/CVPR.2015.7299003
   Shi W., 2016, ARXIV PREPRINT ARXIV
   Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207
   Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329
   Simonyan K., 2014, 14091556 ARXIV
   Singh A, 2014, 2014 5TH INTERNATIONAL CONFERENCE CONFLUENCE THE NEXT GENERATION INFORMATION TECHNOLOGY SUMMIT (CONFLUENCE), P555, DOI 10.1109/CONFLUENCE.2014.6949049
   Sutherland D. J., 2016, ARXIV161104488
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486
   Timofte R, 2016, PROC CVPR IEEE, P1865, DOI 10.1109/CVPR.2016.206
   Timofte R, 2016, COMPUT VIS IMAGE UND, V142, P1, DOI 10.1016/j.cviu.2015.09.008
   Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8
   Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241
   Tirer T, 2019, IEEE T IMAGE PROCESS, V28, P1220, DOI 10.1109/TIP.2018.2875569
   Tong T, 2017, IEEE I CONF COMP VIS, P4809, DOI 10.1109/ICCV.2017.514
   Trudel E, 2016, INT C COMP SUPP COOP, P145, DOI 10.1109/CSCWD.2016.7565979
   Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984
   van den Oord A, 2016, PR MACH LEARN RES, V48
   Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048
   Wang XT, 2018, PROC CVPR IEEE, P606, DOI 10.1109/CVPR.2018.00070
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50
   Yan Q, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2414877
   Yang CY, 2014, LECT NOTES COMPUT SC, V8692, P372, DOI 10.1007/978-3-319-10593-2_25
   Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625
   Yang WH, 2017, IEEE T IMAGE PROCESS, V26, P5895, DOI 10.1109/TIP.2017.2750403
   Yang WM, 2016, IEEE T MULTIMEDIA, V18, P313, DOI 10.1109/TMM.2016.2515997
   Yang Z, 2017, LECT NOTES COMPUT SC, V10132, P353, DOI 10.1007/978-3-319-51811-4_29
   Yu JF, 2014, IEEE T NEUR NET LEAR, V25, P780, DOI 10.1109/TNNLS.2013.2281313
   Yuan Y, 2018, IEEE COMPUT SOC CONF, P814, DOI 10.1109/CVPRW.2018.00113
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474
   Zeyde R., 2012, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47
   Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344
   Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300
   Zhang KB, 2017, IEEE T NEUR NET LEAR, V28, P1109, DOI 10.1109/TNNLS.2015.2511069
   Zhang KB, 2013, IEEE T NEUR NET LEAR, V24, P1648, DOI 10.1109/TNNLS.2013.2262001
   Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262
   Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865
   Zhu J.-Y., 2017, IEEE I CONF COMP VIS, P2223, DOI DOI 10.1109/ICCV.2017.244
   Zontak M, 2011, PROC CVPR IEEE, P977, DOI 10.1109/CVPR.2011.5995401
NR 125
TC 554
Z9 608
U1 18
U2 333
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2019
VL 21
IS 12
BP 3106
EP 3121
DI 10.1109/TMM.2019.2919431
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA KJ8ZS
UT WOS:000512345200011
OA Green Submitted
HC Y
HP N
DA 2024-07-18
ER

PT J
AU Chen, YD
   Hao, CY
   Liu, AX
   Wu, EH
AF Chen, Yadang
   Hao, Chuanyan
   Liu, Alex X.
   Wu, Enhua
TI Multilevel Model for Video Object Segmentation Based on Supervision
   Optimization
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video segmentation; supervised; supervision optimization; select frames;
   multi-level model; semantic classification
ID TRACKING
AB In this work, we present a supervised object segmentation algorithm for unconstrained video. Instead of arbitrarily picking a few frames for manual labeling, as in many existing supervised methods, the proposed method selects frames in a more reasonable manner, called supervision optimization. For this, we formulate a principled objective function by inferring the propagation error from appearance and motion clues. After this, we construct a multilevel segmentation model, which consists of low-level and high-level features. On the low level, image pixels are used for a more accurate estimation of motion and segmentation. On the high level, image segments are considered for a more semantic classification of the foreground and background. By integrating these in one segmentation graph, the result can be further improved by leveraging the knowledge from both levels. In experiments, the proposed approach is evaluated by different measures, and the results on a benchmark demonstrate the effectiveness in comparison with other state-of-the-art algorithms.
C1 [Chen, Yadang] Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
   [Hao, Chuanyan] Nanjing Univ Posts & Telecommun, Sch Educ Sci & Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Liu, Alex X.] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA.
   [Liu, Alex X.] Nanjing Univ, State Key Lab Novel Software Technol, Nanjing 210044, Jiangsu, Peoples R China.
   [Wu, Enhua] Univ Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
C3 Nanjing University of Information Science & Technology; Nanjing
   University of Posts & Telecommunications; Michigan State University;
   Nanjing University; Chinese Academy of Sciences; University of Chinese
   Academy of Sciences, CAS
RP Chen, YD (corresponding author), Nanjing Univ Informat Sci & Technol, Sch Comp & Software, Nanjing 210044, Jiangsu, Peoples R China.
EM cyd4511632@gmail.com; hcy@njupt.edu.cn; alexliu360@gmail.com;
   ehwu@umac.mo
RI Liu, xuefeng/IUP-1483-2023
OI wu, en hua/0000-0002-2174-1428
FU National Natural Science Foundation of China [61802197, 61702278,
   61872082, 61472184, 61602252]; National Science Foundation
   [CNS-1837146]; Natural Science Foundation of Jiangsu Province of China
   [BK20160964, BK20160902, BK20160967]; Priority Academic Program
   Development of Jiangsu Higher Education Institutions; Jiangsu Innovation
   and Entrepreneurship (Shuangchuang) Program
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61802197, 61702278, 61872082, 61472184,
   61602252), by the National Science Foundation (No. CNS-1837146), by the
   Natural Science Foundation of Jiangsu Province of China (No. BK20160964,
   BK20160902, BK20160967), by the Project through the Priority Academic
   Program Development of Jiangsu Higher Education Institutions, and by the
   Jiangsu Innovation and Entrepreneurship (Shuangchuang) Program. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Elisa Ricci. (Corresponding author:
   Yadang Chen.)
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Badrinarayanan V, 2013, IEEE T PATTERN ANAL, V35, P2751, DOI 10.1109/TPAMI.2013.54
   Boykov YY, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P105, DOI 10.1109/ICCV.2001.937505
   Chen L, 2015, IEEE T MULTIMEDIA, V17, P2225, DOI 10.1109/TMM.2015.2481711
   Chen YD, 2018, MULTIMED TOOLS APPL, V77, P6117, DOI 10.1007/s11042-017-4520-5
   Chen YD, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-0957-4
   Chockalingam P, 2009, IEEE I CONF COMP VIS, P1530, DOI 10.1109/ICCV.2009.5459276
   Cricri F, 2014, IEEE T MULTIMEDIA, V16, P917, DOI 10.1109/TMM.2014.2307552
   Dong XP, 2017, IEEE T MULTIMEDIA, V19, P763, DOI 10.1109/TMM.2016.2631884
   Faktor A., 2014, P BRIT MACH VIS C
   Fan QN, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818105
   Giordano D, 2015, PROC CVPR IEEE, P4814, DOI 10.1109/CVPR.2015.7299114
   Grundmann M, 2010, PROC CVPR IEEE, P2141, DOI 10.1109/CVPR.2010.5539893
   Hariharan B, 2015, PROC CVPR IEEE, P447, DOI 10.1109/CVPR.2015.7298642
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jain SD, 2014, LECT NOTES COMPUT SC, V8692, P656, DOI 10.1007/978-3-319-10593-2_43
   Jang WD, 2016, PROC CVPR IEEE, P696, DOI 10.1109/CVPR.2016.82
   Jiang HQ, 2015, IEEE T MULTIMEDIA, V17, P3, DOI 10.1109/TMM.2014.2368273
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee YJ, 2011, IEEE I CONF COMP VIS, P1995, DOI 10.1109/ICCV.2011.6126471
   Li FX, 2013, IEEE I CONF COMP VIS, P2192, DOI 10.1109/ICCV.2013.273
   Luo B, 2017, IEEE T MULTIMEDIA, V19, P1482, DOI 10.1109/TMM.2017.2671447
   Märki N, 2016, PROC CVPR IEEE, P743, DOI 10.1109/CVPR.2016.87
   Mazloom M, 2014, IEEE T MULTIMEDIA, V16, P2214, DOI 10.1109/TMM.2014.2359771
   Nagaraja NS, 2015, IEEE I CONF COMP VIS, P3235, DOI 10.1109/ICCV.2015.370
   Panda R, 2017, IEEE T MULTIMEDIA, V19, P2010, DOI 10.1109/TMM.2017.2708981
   Papazoglou A, 2013, IEEE I CONF COMP VIS, P1777, DOI 10.1109/ICCV.2013.223
   Perazzi F, 2016, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2016.85
   Perazzi F, 2017, PROC CVPR IEEE, P3491, DOI 10.1109/CVPR.2017.372
   Perazzi F, 2015, IEEE I CONF COMP VIS, P3227, DOI 10.1109/ICCV.2015.369
   Qiu ZF, 2018, IEEE T MULTIMEDIA, V20, P939, DOI 10.1109/TMM.2017.2759504
   Ramakanth SA, 2014, PROC CVPR IEEE, P376, DOI 10.1109/CVPR.2014.55
   Simonyan K., 2015, P 3 INT C LEARN REPR
   Spampinato C, 2017, IEEE T PATTERN ANAL, V39, P1942, DOI 10.1109/TPAMI.2016.2610973
   Sundaram N, 2010, LECT NOTES COMPUT SC, V6311, P438, DOI 10.1007/978-3-642-15549-9_32
   Tsai D, 2012, INT J COMPUT VISION, V100, P190, DOI 10.1007/s11263-011-0512-5
   Tsai YH, 2016, PROC CVPR IEEE, P3899, DOI 10.1109/CVPR.2016.423
   Varas D, 2014, PROC CVPR IEEE, P3470, DOI 10.1109/CVPR.2014.444
   Vijayanarasimhan S, 2012, LECT NOTES COMPUT SC, V7576, P496, DOI 10.1007/978-3-642-33715-4_36
   Wang BT, 2017, IEEE T CIRC SYST VID, V27, P992, DOI 10.1109/TCSVT.2016.2527378
   Wen LY, 2015, PROC CVPR IEEE, P2226, DOI 10.1109/CVPR.2015.7298835
   Wenguan Wang, 2015, 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3395, DOI 10.1109/CVPR.2015.7298961
   Xiao FY, 2016, PROC CVPR IEEE, P933, DOI 10.1109/CVPR.2016.107
   Xu CL, 2016, INT J COMPUT VISION, V119, P272, DOI 10.1007/s11263-016-0906-5
   Xu Z., 2007, Advances in Neural Information Processing Systems, P1641
   Yang J, 2016, IEEE T IMAGE PROCESS, V25, P503, DOI 10.1109/TIP.2015.2500820
   Zhang D, 2013, PROC CVPR IEEE, P628, DOI 10.1109/CVPR.2013.87
   Zhang Y, 2016, IEEE T MULTIMEDIA, V18, P418, DOI 10.1109/TMM.2016.2520827
   Zhang Z, 2016, IEEE T MULTIMEDIA, V18, P2079, DOI 10.1109/TMM.2016.2594138
NR 49
TC 21
Z9 25
U1 0
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1934
EP 1945
DI 10.1109/TMM.2018.2890361
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700004
OA Bronze
DA 2024-07-18
ER

PT J
AU Pan, X
   Zhou, YF
   Chen, ZG
   Zhang, CM
AF Pan, Xiao
   Zhou, Yuanfeng
   Chen, Zhonggui
   Zhang, Caiming
TI Texture Relative Superpixel Generation With Adaptive Parameters
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Superpixels; over-segmentation; compactness; flooding; neural network
ID OBJECT CO-SEGMENTATION; COMPACTNESS; ALGORITHM; COLOR
AB Superpixel generation, which is an essential step in many image processing applications, has attracted increasing attention from researchers. In this paper, we present an efficient flooding-based superpixel generation algorithm that generates compact and highly boundary adherent superpixels. In particular, by considering various superpixel properties, we measure the similarities between image pixels by proposing a new distance metric that combines various image features (e.g., colors, spatial locations, neighbor information, and texture features). To control the relative significance of these image features, we acquire the weights of image features (e.g., colors, texture features, and neighbor information of pixels) through a neural network. Then, the final superpixels are obtained through a greedy optimization that considers both the current superpixel and its neighboring superpixels. We perform extensive experiments on two datasets to verify the efficacy of our algorithm. The results show that our algorithm has considerable advantages over existing state-of-the- art methods, particularly regarding the compactness of the resulting superpixels.
C1 [Pan, Xiao] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250100, Shandong, Peoples R China.
   [Pan, Xiao] Shandong Univ Finance & Econ, Shandong Key Lab Digital Media Technol, Jinan 250100, Shandong, Peoples R China.
   [Pan, Xiao] Shandong Univ Finance & Econ, Shandong Res Ctr China US Digital Media Int Coope, Jinan 250100, Shandong, Peoples R China.
   [Zhou, Yuanfeng; Zhang, Caiming] Shandong Univ, Sch Software, Jinan 250100, Shandong, Peoples R China.
   [Chen, Zhonggui] Xiamen Univ, Dept Comp Sci, Xiamen 361005, Fujian, Peoples R China.
   [Zhang, Caiming] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai 264005, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University of
   Finance & Economics; Shandong University of Finance & Economics;
   Shandong University; Xiamen University
RP Pan, X (corresponding author), Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan 250100, Shandong, Peoples R China.; Pan, X (corresponding author), Shandong Univ Finance & Econ, Shandong Key Lab Digital Media Technol, Jinan 250100, Shandong, Peoples R China.; Pan, X (corresponding author), Shandong Univ Finance & Econ, Shandong Res Ctr China US Digital Media Int Coope, Jinan 250100, Shandong, Peoples R China.
EM xppanxiao@gmail.com; yfzhou@sdu.edu.cn; zgchenxmu@yeah.net;
   czhang@sdu.edu.cn
RI li, sixuan/KGR-3943-2024; liu, xinyi/KFB-4466-2024
OI zhang, caiming/0000-0003-0217-1543; Chen, Zhonggui/0000-0002-9960-4896
FU NSFC-Zhejiang Joint Fund of the Integration of Informatization and
   Industrialization [U1609218]; National Natural Science Foundation of
   China [61332015, 61772312, 61472332]; Fundamental Research Funds of
   Shandong University [2018JC030]; Key Research and Development Project of
   Shandong Province [2017GGX10110]; Excellent Youth Science Foundation
   Project of Shandong Province [ZR2018JL022]
FX This work was supported in part by the NSFC-Zhejiang Joint Fund of the
   Integration of Informatization and Industrialization (U1609218); in part
   by the National Natural Science Foundation of China under Grants
   61332015, 61772312, and 61472332; in part by the Fundamental Research
   Funds of Shandong University (2018JC030); in part by the Key Research
   and Development Project of Shandong Province (2017GGX10110); and in part
   by the Excellent Youth Science Foundation Project of Shandong Province
   (ZR2018JL022). The associate editor coordinating the review of this
   manuscript and approving it for publication was Dr. Joern Ostermann.
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236
   Fu HZ, 2014, IEEE T MULTIMEDIA, V16, P1165, DOI 10.1109/TMM.2014.2305571
   Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211
   JAIN AK, 1991, PATTERN RECOGN, V24, P1167, DOI 10.1016/0031-3203(91)90143-S
   Jie Wang, 2012, Computational Visual Media. Proceedings First International Conference, CVM 2012, P130, DOI 10.1007/978-3-642-34263-9_17
   Kaufhold J, 2006, INT C PATT RECOG, P755
   Kim JS, 2009, PATTERN RECOGN, V42, P735, DOI 10.1016/j.patcog.2008.09.031
   Lei JJ, 2016, IEEE T MULTIMEDIA, V18, P1783, DOI 10.1109/TMM.2016.2592325
   Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96
   Li ZQ, 2015, PROC CVPR IEEE, P1356, DOI 10.1109/CVPR.2015.7298741
   Liu Q., 2015, COMPUT VIS MEDIA, V1, P197
   Liu S, 2012, IEEE T MULTIMEDIA, V14, P361, DOI 10.1109/TMM.2011.2174780
   Liu YJ, 2018, IEEE T PATTERN ANAL, V40, P653, DOI 10.1109/TPAMI.2017.2686857
   Liu YJ, 2016, PROC CVPR IEEE, P651, DOI 10.1109/CVPR.2016.77
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   Meng FM, 2012, IEEE T MULTIMEDIA, V14, P1429, DOI 10.1109/TMM.2012.2197741
   Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Mori G, 2005, IEEE I CONF COMP VIS, P1417
   Nowozin S, 2010, LECT NOTES COMPUT SC, V6316, P98, DOI 10.1007/978-3-642-15567-3_8
   Pan X, 2014, IEEE IMAGE PROC, P4432, DOI 10.1109/ICIP.2014.7025899
   Schick A, 2014, PATTERN RECOGN LETT, V43, P71, DOI 10.1016/j.patrec.2013.09.013
   Shen JB, 2014, IEEE T IMAGE PROCESS, V23, P1451, DOI 10.1109/TIP.2014.2302892
   Van den Bergh M, 2012, LECT NOTES COMPUT SC, V7578, P13, DOI 10.1007/978-3-642-33786-4_2
   Veksler O, 2010, LECT NOTES COMPUT SC, V6315, P211, DOI 10.1007/978-3-642-15555-0_16
   VINCENT L, 1991, IEEE T PATTERN ANAL, V13, P583, DOI 10.1109/34.87344
   Wang C, 2014, IEEE T MULTIMEDIA, V16, P903, DOI 10.1109/TMM.2014.2306393
   Wang J, 2012, IEEE T PATTERN ANAL, V34, P1241, DOI 10.1109/TPAMI.2012.47
   Wang P, 2013, INT J COMPUT VISION, V103, P1, DOI 10.1007/s11263-012-0588-6
   Zhang LM, 2014, IEEE T MULTIMEDIA, V16, P470, DOI 10.1109/TMM.2013.2293424
   Zhang YX, 2017, IEEE T CIRC SYST VID, V27, P1502, DOI 10.1109/TCSVT.2016.2539839
   Zhou YJ, 2015, IEEE WINT CONF APPL, P1076, DOI 10.1109/WACV.2015.148
   Zhou YF, 2017, IEEE T CIRC SYST VID, V27, P2281, DOI 10.1109/TCSVT.2016.2589781
NR 35
TC 9
Z9 9
U1 3
U2 18
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 1997
EP 2011
DI 10.1109/TMM.2019.2895498
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700009
DA 2024-07-18
ER

PT J
AU Wang, D
   Yao, HX
   Tombari, F
   Zhao, SC
   Wang, B
   Liu, H
AF Wang, Dong
   Yao, Hongxun
   Tombari, Federico
   Zhao, Sicheng
   Wang, Bin
   Liu, Hong
TI Learning Descriptors With Cube Loss for View-Based 3-D Object Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cube loss; view-based 3-D object retrieval; triplet network; ModelNet
   dataset
ID CONVOLUTIONAL NEURAL-NETWORKS; 3D; RECOGNITION
AB 3-D object retrieval has been a hot research topic in recent years. Within such a field, view-based approaches are attracting increasing attention because of the flexibility of data representation as well as the reported state-of-the-art performance. One of the most important issues related to view-based 3-D object retrieval is how to learn embedding features that are discriminative across classes while being compactly distributed within each class. In this paper, we analyze the difference between the two tasks of classification and retrieval, and propose a novel way to learn a view-pooling feature via a triplet network. In addition, we propose a new loss, named cube loss, which is able to sample a number of triplets equal to the cube of the samples in a batch. With the new loss, both hard-negative and hard-positive pairs can be effectively investigated. The experimental results on the ModelNet benchmark demonstrate that the proposed method achieves superior performance compared to state-of-the-art approaches.
C1 [Wang, Dong; Wang, Bin; Liu, Hong] Harbin Inst Technol, Dept State Key Lab Robot & Syst, Harbin 150001, Heilongjiang, Peoples R China.
   [Yao, Hongxun] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
   [Tombari, Federico] Tech Univ Munich, Chair Comp Aided Med Procedures & Augmented Real, D-85748 Munich, Germany.
   [Zhao, Sicheng] Univ Calif Berkeley, Berkeley Artificial Intelligence Res Lab, Berkeley, CA 94720 USA.
C3 Harbin Institute of Technology; Harbin Institute of Technology;
   Technical University of Munich; University of California System;
   University of California Berkeley
RP Wang, B (corresponding author), Harbin Inst Technol, Dept State Key Lab Robot & Syst, Harbin 150001, Heilongjiang, Peoples R China.; Yao, HX (corresponding author), Harbin Inst Technol, Sch Comp Sci & Technol, Harbin 150001, Heilongjiang, Peoples R China.
EM dwang89@hit.edu.cn; h.yao@hit.edu.cn; tombari@in.tum.de;
   schzhao@gmail.com; wbhit@hit.edu.cn; hong.liu@hit.edu.cn
FU National Natural Science Foundation of China [61772158, 61702136,
   U1711265]; Self-Planned Task of State Key Laboratory of Robotics and
   System [SKLRS201805B]
FX This work was supported in part by the National Natural Science
   Foundation of China under Project No. 61772158, 61702136, and U1711265;
   and in part by Self-Planned Task of State Key Laboratory of Robotics and
   System under Grant NO. SKLRS201805B. The associate editor coordinating
   the review of this manuscript and approving it for publication was Dr.
   Lei Zhang. (Corresponding authors: Hongxun Yao and Bin Wang.)
CR [Anonymous], 2017, PLOSONE
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298801, 10.1109/CVPR.2015.7298801]
   [Anonymous], 2016, IEEE T AFFECT COMPUT
   [Anonymous], EUR WORKSH 3D OBJ RE
   [Anonymous], 2004, Guangdian Gongcheng
   [Anonymous], ARXIV171110108
   [Anonymous], 2017, PROC INT C LEARN REP
   [Anonymous], BRIT MACH VIS C
   [Anonymous], P IEEE C COMP VIS PA
   [Anonymous], P BMVC
   [Anonymous], P IEEE C COMP VIS PA
   Bai S, 2017, IEEE T MULTIMEDIA, V19, P1257, DOI 10.1109/TMM.2017.2652071
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Deng C, 2018, IEEE T IMAGE PROCESS, V27, P3893, DOI 10.1109/TIP.2018.2821921
   Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035
   Gao Y, 2014, IEEE MULTIMEDIA, V21, P52, DOI 10.1109/MMUL.2014.20
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7
   HUTTENLOCHER DP, 1993, IEEE T PATTERN ANAL, V15, P850, DOI 10.1109/34.232073
   KHOTANZAD A, 1990, IEEE T PATTERN ANAL, V12, P489, DOI 10.1109/34.55109
   Leibe B., 2017, ARXIV170307737CS
   Li C, 2018, PROC CVPR IEEE, P4242, DOI 10.1109/CVPR.2018.00446
   Liu AA, 2018, IEEE T CYBERNETICS, V48, P916, DOI 10.1109/TCYB.2017.2664503
   Liu AA, 2017, IEEE T PATTERN ANAL, V39, P102, DOI 10.1109/TPAMI.2016.2537337
   Liu AA, 2016, IEEE T IMAGE PROCESS, V25, P2103, DOI 10.1109/TIP.2016.2540802
   Mishchuk A., 2017, P ADV NEURAL INFORM, P4826
   Qi C. R., 2017, Advances in neural information processing systems, P5099
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sfikas K, 2018, COMPUT GRAPH-UK, V71, P208, DOI 10.1016/j.cag.2017.12.001
   Sfikas K, 2013, VISUAL COMPUT, V29, P1351, DOI 10.1007/s00371-013-0876-3
   Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114
   Tombari F, 2010, LECT NOTES COMPUT SC, V6313, P356, DOI 10.1007/978-3-642-15558-1_26
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang D, 2018, MULTIMED TOOLS APPL, V77, P19833, DOI 10.1007/s11042-017-5413-3
   Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180
   Wohlhart P, 2015, PROC CVPR IEEE, P3109, DOI 10.1109/CVPR.2015.7298930
   Yang HT, 2017, IEEE ENER CONV, P4595, DOI 10.1109/ECCE.2017.8096786
   Yap PT, 2003, IEEE T IMAGE PROCESS, V12, P1367, DOI 10.1109/TIP.2003.818019
   Zhang Y, 2016, NEUROCOMPUTING, V195, P40, DOI 10.1016/j.neucom.2015.09.118
   Zhang ZZ, 2018, IEEE T IMAGE PROCESS, V27, P5957, DOI 10.1109/TIP.2018.2862625
   Zhao SC, 2018, IEEE T CYBERNETICS, V48, P3218, DOI 10.1109/TCYB.2017.2762344
   Zhao SC, 2017, IEEE T MULTIMEDIA, V19, P632, DOI 10.1109/TMM.2016.2617741
NR 46
TC 8
Z9 9
U1 0
U2 37
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2019
VL 21
IS 8
BP 2071
EP 2082
DI 10.1109/TMM.2019.2892004
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IK7YM
UT WOS:000476809700014
DA 2024-07-18
ER

PT J
AU Jelodar, AB
   Paulius, D
   Sun, Y
AF Jelodar, Ahmad Babaeian
   Paulius, David
   Sun, Yu
TI Long Activity Video Understanding Using Functional Object-Oriented
   Network
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Video understanding; activity understanding; video knowledge
   representation
ID VISUAL KNOWLEDGE; RECOGNITION; CONTEXT; FRAMEWORK; LANGUAGE; IMAGE
AB Video understanding is one of the most challenging topics in computer vision. In this paper, a four-stage video understanding pipeline is presented to simultaneously recognize all atomic actions and the single ongoing activity in a video. This pipeline uses objects and motions from the video and a graph-based knowledge representation network as prior reference. Two deep networks are trained to identify objects and motions in each video sequence associated with an action and low level image features are used to identify objects of interest in the video sequence. Confidence scores are assigned to objects of interest to represent their involvement in the action and to motion classes based on results from a deep neural network that classifies an ongoing action in video into motion classes. Confidence scores are computed for each candidate functional unit to associate them with an action using a knowledge representation network, object confidences, and motion confidences. Each action, therefore, is associated with a functional unit, and the sequence of actions is evaluated to identify the sole activity occurring in the video. The knowledge representation used in the pipeline is called the functional object-oriented network, which is a graph-based network useful for encoding knowledge about manipulation tasks. Experiments are performed on a dataset of cooking videos to test the proposed algorithm with action inference and activity classification. Experiments show that using a functional object-oriented network improves video understanding significantly.
C1 [Jelodar, Ahmad Babaeian; Paulius, David; Sun, Yu] Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA.
C3 State University System of Florida; University of South Florida
RP Jelodar, AB (corresponding author), Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA.
EM ajelodar@mail.usf.edu; davidpaulius@mail.usf.edu; yusun@mail.usf.edu
OI Paulius, David/0000-0002-7891-8749
FU National Science Foundation [1421418]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [1421418] Funding
   Source: National Science Foundation
FX This work was supported by the National Science Foundation under Grant
   1421418.
CR [Anonymous], 2015, NEW DEV ROBOT VISION
   [Anonymous], 2008, Petri Net
   [Anonymous], PATTERN RECOGNITION
   [Anonymous], IEEE T PATTERN ANAL
   [Anonymous], ARXIV180506956
   [Anonymous], 2010, NETWORKS INTRO, DOI DOI 10.1093/ACPROF:OSO/9780199206650.001.0001
   Baker C.F., 1998, P 36 ANN M ASS COMP, P86, DOI [DOI 10.3115/980845.980860, DOI 10.3115/980451.980860]
   Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3
   Carlson A, 2010, AAAI CONF ARTIF INTE, P1306
   Chang XJ, 2017, IEEE T CYBERNETICS, V47, P1180, DOI 10.1109/TCYB.2016.2539546
   Chen JJ, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P32, DOI 10.1145/2964284.2964315
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Chéron G, 2015, IEEE I CONF COMP VIS, P3218, DOI 10.1109/ICCV.2015.368
   Crispim CF Jr, 2016, IEEE T PATTERN ANAL, V38, P1598, DOI 10.1109/TPAMI.2016.2537323
   de Souza FDM, 2017, INT J COMPUT VISION, V121, P5, DOI 10.1007/s11263-016-0913-6
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Fathi A, 2013, PROC CVPR IEEE, P2579, DOI 10.1109/CVPR.2013.333
   Fellbaum C., 1998, WORDNET ELECT LEXICA, DOI DOI 10.7551/MITPRESS/7287.001.0001
   Fermüller C, 2018, INT J COMPUT VISION, V126, P358, DOI 10.1007/s11263-017-0992-z
   Gibson JJ., 1977, The Theory of Affordances. Perceiving, Acting, and Knowing, P127
   Gupta A., IEEE COMPUT SOC C CO, DOI [10.1109/CVPR.2007.383331, DOI 10.1109/CVPR.2007.383331]
   Gupta A, 2009, IEEE T PATTERN ANAL, V31, P1775, DOI 10.1109/TPAMI.2009.83
   Han D, 2009, IEEE I CONF COMP VIS, P1933, DOI 10.1109/ICCV.2009.5459427
   Hasan M, 2015, IEEE T MULTIMEDIA, V17, P1909, DOI 10.1109/TMM.2015.2477242
   Herranz L, 2017, IEEE T MULTIMEDIA, V19, P430, DOI 10.1109/TMM.2016.2614861
   Jain Ashesh, 2015, ABS151105298 CORR, P5308
   Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7
   Kuehne H, 2016, IEEE WINT CONF APPL
   Kumar K, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P100, DOI 10.1145/3154979.3154998
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P26635, DOI 10.1007/s11042-018-5882-z
   Kumar K, 2018, MULTIMED TOOLS APPL, V77, P7383, DOI 10.1007/s11042-017-4642-9
   Kumar K, 2018, IEEE T MULTIMEDIA, V20, P323, DOI 10.1109/TMM.2017.2741423
   Kumar K, 2017, 2017 INTERNATIONAL CONFERENCE ON INNOVATIONS IN ELECTRONICS, SIGNAL PROCESSING AND COMMUNICATION (IESC), P106, DOI 10.1109/IESPC.2017.8071874
   Liu CW, 2016, INT J COMPUT VISION, V118, P240, DOI 10.1007/s11263-016-0897-2
   Maillot NE, 2008, IMAGE VISION COMPUT, V26, P102, DOI 10.1016/j.imavis.2005.07.027
   Matsuo Yusuke, 2015, 2015 10th Asia-Pacific Symposium on Information and Telecommunication Technologies (APSITT), P1, DOI 10.1109/APSITT.2015.7217125
   Mazloom M, 2016, IEEE T MULTIMEDIA, V18, P1378, DOI 10.1109/TMM.2016.2559947
   Merler M, 2012, IEEE T MULTIMEDIA, V14, P88, DOI 10.1109/TMM.2011.2168948
   Min WQ, 2018, IEEE T MULTIMEDIA, V20, P950, DOI 10.1109/TMM.2017.2759499
   Neumann B, 2008, IMAGE VISION COMPUT, V26, P82, DOI 10.1016/j.imavis.2007.08.013
   Ni BB, 2015, INT J COMPUT VISION, V111, P229, DOI 10.1007/s11263-014-0742-4
   Niu F, 2012, INT J SEMANT WEB INF, V8, P42, DOI 10.4018/jswis.2012070103
   Paul Debayan, 2018, 2018 4 INT C COMPUTE, P1
   Paulius D, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2655, DOI 10.1109/IROS.2016.7759413
   Paulssen D., 2018, Adv. Mater. Interfaces, V1800852, P1
   Chi PY, 2008, LECT NOTES COMPUT SC, V5033, P116, DOI 10.1007/978-3-540-68504-3_11
   Perez P., 2007, PROC IEEE 11 INT C C, P1
   Ren SG, 2013, 2013 IEEE WORKSHOP ON ROBOT VISION (WORV), P1, DOI 10.1109/WORV.2013.6521912
   Richardson M, 2006, MACH LEARN, V62, P107, DOI 10.1007/s10994-006-5833-1
   Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801
   Schuler KK., 2005, AAI3179808 U PENNS
   Souza F, 2015, PROC CVPR IEEE, P1229, DOI 10.1109/CVPR.2015.7298727
   Sun Y, 2014, ROBOT AUTON SYST, V62, P487, DOI 10.1016/j.robot.2013.12.005
   Town C, 2006, MACH VISION APPL, V17, P94, DOI 10.1007/s00138-006-0017-3
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang XY, 2017, IEEE T PATTERN ANAL, V39, P1770, DOI 10.1109/TPAMI.2016.2616308
   Wu J, 2007, PROCEEDINGS OF THE 2007 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS, P7, DOI 10.1109/SOLI.2007.4383891
   Wu Q, 2016, PROC CVPR IEEE, P4622, DOI 10.1109/CVPR.2016.500
   Xie KC, 2014, PROCEEDINGS OF THE ASME TURBO EXPO: TURBINE TECHNICAL CONFERENCE AND EXPOSITION, 2014, VOL 4B
   Yao BP, 2010, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2010.5540235
   Zhang XS, 2015, IEEE T MULTIMEDIA, V17, P1562, DOI 10.1109/TMM.2015.2449660
   Zhou Y, 2015, PROC CVPR IEEE, P3323, DOI 10.1109/CVPR.2015.7298953
   Zhu XT, 2016, INT J COMPUT VISION, V117, P247, DOI 10.1007/s11263-015-0864-3
NR 63
TC 14
Z9 17
U1 0
U2 7
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2019
VL 21
IS 7
BP 1813
EP 1824
DI 10.1109/TMM.2018.2885228
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA IF6IF
UT WOS:000473183700016
OA Green Submitted, hybrid
DA 2024-07-18
ER

PT J
AU Zhu, GM
   Zhang, L
   Shen, PY
   Song, J
   Shah, SAA
   Bennamoun, M
AF Zhu, Guangming
   Zhang, Liang
   Shen, Peiyi
   Song, Juan
   Shah, Syed Afaq Ali
   Bennamoun, Mohammed
TI Continuous Gesture Segmentation and Recognition Using 3DCNN and
   Convolutional LSTM
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Continuous gesture recognition; 3DCNN; convolutional LSTM; dilation
ID FUSION
AB Continuous gesture recognition aims at recognizing the ongoing gestures from continuous gesture sequences and is more meaningful for the scenarios, where the start and end frames of each gesture instance are generally unknown in practical applications. This paper presents an effective deep architecture for continuous gesture recognition. First, continuous gesture sequences are segmented into isolated gesture instances using the proposed temporal dilated Res3D network. A balanced squared hinge loss function is proposed to deal with the imbalance between boundaries and nonboundaries. Temporal dilation can preserve the temporal information for the dense detection of the boundaries at fine granularity, and the large temporal receptive field makes the segmentation resultsmore reasonable and effective. Then, the recognition network is constructed based on the 3-D convolutional neural network (3DCNN), the convolutional long-short-term-memory network (ConvLSTM), and the 2-D convolutional neural network (2DCNN) for isolated gesture recognition. The "3DCNN-ConvLSTM-2DCNN" architecture is more effective to learn long-term and deep spatiotemporal features. The proposed segmentation and recognition networks obtain the Jaccard index of 0.7163 on the Chalearn LAP ConGD dataset, which is 0.106 higher than the winner of 2017 ChaLearn LAP Large-Scale Continuous Gesture Recognition Challenge.
C1 [Zhu, Guangming; Zhang, Liang; Shen, Peiyi; Song, Juan] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Shaanxi, Peoples R China.
   [Shah, Syed Afaq Ali; Bennamoun, Mohammed] Univ Western Australia, Perth, WA 6000, Australia.
   [Shah, Syed Afaq Ali] Cent Queensland Univ, Rockhampton, Qld 4701, Australia.
C3 Xidian University; University of Western Australia; Central Queensland
   University
RP Zhang, L (corresponding author), Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Shaanxi, Peoples R China.
EM gmzhu@xidian.edu.cn; liangzhang@xidian.edu.cn; pyshen@xidian.edu.cn;
   songjuan@mail.xidian.edu.cn; afaq.shah@uwa.edu.au;
   mohammed.bennamoun@uwa.edu.au
RI Bennamoun, Mohammed/C-2789-2013
OI Bennamoun, Mohammed/0000-0002-6603-3257; Zhu,
   Guangming/0000-0003-3214-4095; Shah, Syed Afaq Ali/0000-0003-2181-8445;
   Zhang, Liang/0000-0003-4331-5830
FU National Natural Science Foundation of China [61702390]; Fundamental
   Research Funds for the Central Universities [JB181001]; Key Research and
   Development Program of Shaanxi Province [2018ZDXM-GY-036]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61702390, in part by the Fundamental
   Research Funds for the Central Universities under Grant JB181001, and in
   part by the Key Research and Development Program of Shaanxi Province
   under Grant 2018ZDXM-GY-036. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Chang-Su Kim. (Corresponding author: Liang Zhang.)
CR [Anonymous], 2014, WORKSH EUR C COMP VI
   Buch S, 2017, PROC CVPR IEEE, P6373, DOI 10.1109/CVPR.2017.675
   Camgoz NC, 2016, INT C PATT RECOG, P49, DOI 10.1109/ICPR.2016.7899606
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chai XJ, 2016, INT C PATT RECOG, P31, DOI 10.1109/ICPR.2016.7899603
   Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195
   Duan J., 2016, arXiv
   Duan J., 2018, ACM T MULTIM COMPUT, V14, P1, DOI DOI 10.1145/3131343
   Escalera S, 2016, J MACH LEARN RES, V17
   Escorcia V, 2016, LECT NOTES COMPUT SC, V9907, P768, DOI 10.1007/978-3-319-46487-9_47
   Gao LL, 2017, IEEE T MULTIMEDIA, V19, P2045, DOI 10.1109/TMM.2017.2729019
   Hara K., 2017, ARXIV171109577
   Howard A.G., 2017, MOBILENETS EFFICIENT, DOI DOI 10.48550/ARXIV.1704.04861
   Escalante HJ, 2016, INT C PATT RECOG, P67, DOI 10.1109/ICPR.2016.7899609
   Jiang F, 2015, J MACH LEARN RES, V16, P227
   Kaiser L., 2017, ARXIV170603059
   Lea C, 2017, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2017.113
   Li YA, 2018, IEEE T CIRC SYST VID, V28, P2956, DOI 10.1109/TCSVT.2017.2749509
   Li YN, 2016, INT C PATT RECOG, P25, DOI 10.1109/ICPR.2016.7899602
   Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343
   Liu Z, 2017, IEEE INT CONF COMP V, P3056, DOI 10.1109/ICCVW.2017.361
   Miao QG, 2017, IEEE INT CONF COMP V, P3047, DOI 10.1109/ICCVW.2017.360
   Molchanov P., 2016, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition, P4207, DOI DOI 10.1109/CVPR.2016.456
   Monnier C, 2015, LECT NOTES COMPUT SC, V8925, P491, DOI 10.1007/978-3-319-16178-5_34
   Narayana P, 2018, PROC CVPR IEEE, P5235, DOI 10.1109/CVPR.2018.00549
   Neverova N, 2016, IEEE T PATTERN ANAL, V38, P1692, DOI 10.1109/TPAMI.2015.2461544
   Neverova N, 2015, LECT NOTES COMPUT SC, V8925, P474, DOI 10.1007/978-3-319-16178-5_33
   Peng Xiaojiang., 2014, European Conference on Computer Vision, P518
   Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Ren Z, 2013, IEEE T MULTIMEDIA, V15, P1110, DOI 10.1109/TMM.2013.2246148
   Shi X., 2015, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1506.04214
   Shou Z, 2017, PROC CVPR IEEE, P1417, DOI 10.1109/CVPR.2017.155
   Shou Z, 2016, PROC CVPR IEEE, P1049, DOI 10.1109/CVPR.2016.119
   Song J, 2016, IEEE T IMAGE PROCESS, V25, P4999, DOI 10.1109/TIP.2016.2601260
   Tran Du, 2017, ARXIV170805038
   Triggs B, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P550, DOI 10.1109/ICCV.2001.937674
   Wan J, 2017, IEEE INT CONF COMP V, P3189, DOI 10.1109/ICCVW.2017.377
   Wan J, 2016, IEEE COMPUT SOC CONF, P761, DOI 10.1109/CVPRW.2016.100
   Wang C, 2015, IEEE T MULTIMEDIA, V17, P29, DOI 10.1109/TMM.2014.2374357
   Wang HG, 2017, IEEE INT CONF COMP V, P3138, DOI 10.1109/ICCVW.2017.371
   Wang HG, 2017, IEEE INT CONF COMP V, P3129, DOI 10.1109/ICCVW.2017.370
   Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678
   Wang PC, 2016, INT C PATT RECOG, P13, DOI 10.1109/ICPR.2016.7899600
   Wang PC, 2016, INT C PATT RECOG, P7, DOI 10.1109/ICPR.2016.7899599
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang XH, 2018, NEUROCOMPUTING, V275, P438, DOI 10.1016/j.neucom.2017.08.063
   Xie Saining, 2017, ARXIV171204851, V1, P5
   Xu HJ, 2017, IEEE I CONF COMP VIS, P5794, DOI 10.1109/ICCV.2017.617
   Yang K., 2017, ARXIV170803280
   Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75
   Zen G, 2016, IEEE T MULTIMEDIA, V18, P775, DOI 10.1109/TMM.2016.2523421
   Zhang L, 2017, IEEE INT CONF COMP V, P3120, DOI 10.1109/ICCVW.2017.369
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
   Zhou B., 2017, CoRR
   Zhu GM, 2016, INT C PATT RECOG, P19, DOI 10.1109/ICPR.2016.7899601
   Zhu GM, 2017, IEEE ACCESS, V5, P4517, DOI 10.1109/ACCESS.2017.2684186
NR 57
TC 59
Z9 66
U1 6
U2 54
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2019
VL 21
IS 4
BP 1011
EP 1021
DI 10.1109/TMM.2018.2869278
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HQ4XD
UT WOS:000462413700016
DA 2024-07-18
ER

PT J
AU Bakar, G
   Kirmizioglu, RA
   Tekalp, AM
AF Bakar, Gonca
   Kirmizioglu, Riza Arda
   Tekalp, A. Murat
TI Motion-Based Rate Adaptation in WebRTC Videoconferencing Using Scalable
   Video Coding
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE WebRTC; mesh-connection; selective forwarding unit; scalable VP9 coding;
   motion-adaptive layer selection
ID QUALITY
AB This paper proposes methods for rate adaptation by motion-based spatial and temporal resolution selection in both mesh-connected and selective-forwarding-unit (SFU) connected WebRTC videoconferencing using scalable video coding. In the mesh-connected case, the proposed motion-adaptive spatial/temporal layer selection allows each peer to send video to different peers with different terminal types and network rates at different rates using a single encoder. In the SFU-connected case, motion-adaptive rate control is used both at peers to adapt to the network rate between the sending peer and SFU by spatio-temporal resolution adaptation and at the SFU by layer selection to adapt to the network rate between the SFU and receiving peer. Experimental results show that our proposed motion-based rate adaptation achieves better perceptual video quality with sufficiently high frame rates and lower quantization parameter for video with high motion; and high spatial resolution and lower quantization parameter for video with low motion compared to simple rate-distortion model-based layer selection that does not use motion complexity, at the same rate.
C1 [Bakar, Gonca; Kirmizioglu, Riza Arda; Tekalp, A. Murat] Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
C3 Koc University
RP Bakar, G (corresponding author), Koc Univ, Dept Elect & Elect Engn, TR-34450 Istanbul, Turkey.
EM gbakar15@ku.edu.tr; rkirmizioglu@ku.edu.tr; mtekalp@ku.edu.tr
RI Tekalp, Murat/AAW-1060-2020
OI Kirmizioglu, Riza Arda/0000-0001-7195-5742; Bakar,
   Gonca/0000-0002-7006-7820
FU European CELTIC-Plus project VIRTUOSE; Turkish Academy of Sciences
FX This work was supported by the European CELTIC-Plus project VIRTUOSE.
   The work of A. M. Tekalp was supported by the Turkish Academy of
   Sciences. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Liang Zhou.
CR Alexandros E., 2006, Journal of Zhejiang University (Science), V7, P696, DOI 10.1631/jzus.2006.A0696
   Allkin B., 2017, Useful plants-medicines: at least 28,187 plant species are currently recorded as being of medicinal use
   Amirante A., 2014, P C PRINC SYST APPL, P7
   [Anonymous], 2007, METH SUBJ ASS VID QU
   [Anonymous], 2011, 6386 IETF RFC
   [Anonymous], P 7 INT C MULT SYST
   Baccichet P., 2007, P PACKET VIDEO, P173
   Bakar G., 2017, GLOBECOM 2017, P1
   Boyce JM, 2016, IEEE T CIRC SYST VID, V26, P20, DOI 10.1109/TCSVT.2015.2461951
   Civanlar R., 2009, U.S. Patent, Patent No. [7 593 032, 7593032]
   Grange Adrian, 2016, DRAFT VP9 BITSTREAM
   Hosking B, 2016, INT CONF ACOUST SPEE, P1486, DOI 10.1109/ICASSP.2016.7471924
   Hu H, 2013, IEEE T MULTIMEDIA, V15, P1638, DOI 10.1109/TMM.2013.2266092
   ITU-T, 2014, ADV VID COD GEN AUD
   Jing X, 2012, IEEE INT CONF NETWOR, P339, DOI 10.1109/ICON.2012.6506580
   Lee JS, 2011, IEEE T MULTIMEDIA, V13, P882, DOI 10.1109/TMM.2011.2157333
   Li CL, 2018, IEEE T MULTIMEDIA, V20, P361, DOI 10.1109/TMM.2017.2745709
   Liu JY, 2010, IEEE T CIRC SYST VID, V20, P967, DOI 10.1109/TCSVT.2010.2045924
   Molecualr Operating Environment, 2016, NON TRADITIONAL REF
   Murillo S. G., 2017, NON TRADITIONAL REF
   Ou YF, 2011, IEEE T CIRC SYST VID, V21, P286, DOI 10.1109/TCSVT.2010.2087833
   Overview of race and Hispanic origin, 2010, NON TRADITIONAL REF
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Singh V P., 2013, Progress in Clinical Neurosciences, P1, DOI DOI 10.1109/PV.2013.6691454
   Ukhanova A, 2012, IEEE IMAGE PROC, P1513, DOI 10.1109/ICIP.2012.6467159
   VANNES FL, 1967, J OPT SOC AM, V57, P1082, DOI 10.1364/JOSA.57.001082
   Westerlund M., 2015, 7667 RFC INT ENG TAS
   Xu XL, 2018, IEEE T MULTIMEDIA, V20, P271, DOI 10.1109/TMM.2017.2742699
   Xu Y, 2014, IEEE ACM T NETWORK, V22, P826, DOI 10.1109/TNET.2013.2260354
   Yamagishi K, 2017, IEEE T MULTIMEDIA, V19, P1545, DOI 10.1109/TMM.2017.2669859
   Yang Peng, 2011, 2011 18th IEEE International Conference on Image Processing (ICIP 2011), P2517, DOI 10.1109/ICIP.2011.6116174
NR 31
TC 14
Z9 14
U1 0
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2019
VL 21
IS 2
BP 429
EP 441
DI 10.1109/TMM.2018.2856629
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HJ7GT
UT WOS:000457364400013
DA 2024-07-18
ER

PT J
AU Zhao, TH
   Li, SN
   Ngan, KN
   Wu, FZ
AF Zhao, Tianhao
   Li, Songnan
   Ngan, King Ngi
   Wu, Fanzi
TI 3-D Reconstruction of Human Body Shape From a Single Commodity Depth
   Camera
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Human body shape; model fitting; PCA shape space; depth camera; human
   pose estimation
ID REAL-TIME; TRACKING; OBJECTS; MODEL; POSE
AB 3-D human body reconstruction is an important research topic in computer vision. A 3-D human body model can be used in sports science, movie industry and personalized entertainment, especially virtual reality games. Most of depth-based 3-D reconstruction algorithms need multiple cameras surrounding the user and require the user to keep a specific pose strictly while capturing depth images. In this paper, we propose an algorithm to reconstruct the 3-D shape of human bodies using a single commodity depth camera. Our algorithm only needs two depth images of the front-facing and back-facing bodies. It also has strong operability since the proposed method is insensitive to the pose variations between the two depth images. We reconstruct 3-D shapes of front-facing and back-facing bodies from the two depth images, respectively, and stitch them together. We also propose a novel registration method, namely, "iterative mid-distance points," which has fast convergence and robustness to the depth noise. The proposed method enables robust and easy-to-use human body reconstruction, and achieves higher accuracy than state-of-the-art methods.
C1 [Zhao, Tianhao; Li, Songnan; Ngan, King Ngi; Wu, Fanzi] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
   [Ngan, King Ngi] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu 611731, Sichuan, Peoples R China.
C3 Chinese University of Hong Kong; University of Electronic Science &
   Technology of China
RP Li, SN (corresponding author), Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China.
EM zhaotianhao168@gmail.com; snli@ee.cuhk.edu.hk; knngan@ee.cuhk.edu.hk;
   fzwu@ee.cuhk.edu.hk
RI Ngan, N/E-8240-2014
OI Ngan, N/0000-0003-1946-3235
CR Alexiadis DS, 2013, IEEE T MULTIMEDIA, V15, P339, DOI 10.1109/TMM.2012.2229264
   Anguelov D, 2005, ACM T GRAPHIC, V24, P408, DOI 10.1145/1073204.1073207
   [Anonymous], 2014, ACM T GRAPHIC, DOI DOI 10.1145/2601097.2601165
   Baran I, 2007, ACM T GRAPHIC, V26, DOI [10.1145/1239451.1239523, 10.1145/1276377.1276467]
   BESL PJ, 1992, P SOC PHOTO-OPT INS, V1611, P586, DOI 10.1117/12.57955
   Bogo F, 2015, IEEE I CONF COMP VIS, P2300, DOI 10.1109/ICCV.2015.265
   Chang W, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1966394.1966405
   Chen YP, 2013, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2013.21
   Cheng KL, 2016, IEEE T VIS COMPUT GR, V22, P2467, DOI 10.1109/TVCG.2015.2511751
   Cignoni P., 2008, 6 EUR IT CHAPT C, V73, P129
   Cui Y., 2012, ACCV Workshops, P133
   Dibra E, 2016, INT CONF 3D VISION, P108, DOI 10.1109/3DV.2016.19
   Gavrila DM, 1996, PROC CVPR IEEE, P73, DOI 10.1109/CVPR.1996.517056
   Guo YL, 2014, IEEE T MULTIMEDIA, V16, P1377, DOI 10.1109/TMM.2014.2316145
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Hasler N, 2009, COMPUT GRAPH FORUM, V28, P337, DOI 10.1111/j.1467-8659.2009.01373.x
   Helten T, 2013, 2013 INTERNATIONAL CONFERENCE ON 3D VISION (3DV 2013), P279, DOI 10.1109/3DV.2013.44
   Jain A, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1866158.1866174
   Li H, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618521
   Li H, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2508363.2508407
   Litomisky K., 2012, RAPPORT TECH U CALIF, V20
   Liu ZB, 2017, IEEE T CYBERNETICS, V47, P695, DOI 10.1109/TCYB.2016.2524406
   Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013
   Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378
   Park SI, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1360612.1360695
   Perbet F, 2014, PROC CVPR IEEE, P668, DOI 10.1109/CVPR.2014.91
   Pishchulin L, 2017, PATTERN RECOGN, V67, P276, DOI 10.1016/j.patcog.2017.02.018
   Plänkers R, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P394, DOI 10.1109/ICCV.2001.937545
   Pons-Moll G, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766993
   Probst T, 2016, LECT NOTES COMPUT SC, V9914, P285, DOI 10.1007/978-3-319-48881-3_20
   Shao Ling., 2014, COMPUTER VISION MACH
   Shapiro A, 2014, COMPUT ANIMAT VIRT W, V25, P201, DOI 10.1002/cav.1579
   Sigal L., 2004, P IEEE COMP SOC C CO, V1, pI
   Wei XL, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2366145.2366207
   Weiss A, 2011, IEEE I CONF COMP VIS, P1951, DOI 10.1109/ICCV.2011.6126465
   Wu PF, 2017, IEEE T MULTIMEDIA, V19, P266, DOI 10.1109/TMM.2016.2612761
   Yang YP, 2014, 2014 2ND INTERNATIONAL CONFERENCE ON 3D VISION, VOL. 2, P41, DOI 10.1109/3DV.2014.47
   Ye M, 2014, PROC CVPR IEEE, P2353, DOI 10.1109/CVPR.2014.301
   Zhang Q, 2014, PROC CVPR IEEE, P676, DOI 10.1109/CVPR.2014.92
   Zhang Z, 2013, IEEE T MULTIMEDIA, V15, P106, DOI 10.1109/TMM.2012.2225040
   Zuffi S, 2015, PROC CVPR IEEE, P3537, DOI 10.1109/CVPR.2015.7298976
NR 41
TC 25
Z9 27
U1 0
U2 27
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2019
VL 21
IS 1
BP 114
EP 123
DI 10.1109/TMM.2018.2844087
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HF5DV
UT WOS:000454253700010
DA 2024-07-18
ER

PT J
AU Kim, H
   No, A
   Lee, HJ
AF Kim, Hyun
   No, Albert
   Lee, Hyuk-Jae
TI SPIHT Algorithm With Adaptive Selection of Compression Ratio Depending
   on DWT Coefficients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive compression ratio; discrete wavelet transform; embedded
   compression; frame memory compression; set partitioning in hierarchical
   trees; optimization; video compression
ID FRAME-RECOMPRESSION ALGORITHM; IMAGE COMPRESSION; LOW-COMPLEXITY; CODING
   SYSTEM; TRUNCATION; EFFICIENT
AB In mobile multimedia devices, the frame memory compression (FMC) technique by embedded compression (EC) is becoming an increasingly important video-processing method for reducing the external data bandwidth requirement, which, in turn, results in power savings. Among various EC schemes, the combination of discrete wavelet transform (DWT) and set partitioning in hierarchical trees (SPIHT) is widely used for FMC because it achieves high compression efficiency with low computational complexity. However, there is room for improvement in the conventional DWT and SPIHT algorithm because it compresses all blocks with the same compression ratio without taking into account the correlation between DWT coefficients and the SPIHT algorithm. This study proposes a novel one-dimensional (1-D) DWT and SPIHT algorithm, which enhances the quality of the compressed video by internally applying an adaptive compression ratio for the SPIHT algorithm based on DWT coefficients while keeping the same bit-stream size. The block complexity is predicted from the distribution of DWT coefficients. Then, simple blocks are aggressively compressed with a low compression ratio, while the complex blocks are compressed with a high ratio. Furthermore, to achieve the hest video quality, each compression ratio is decided by an optimization technique based on mathematical formulation. Precisely, the logarithm of mean squared error by the SPIHT algorithm is assumed to be linearly correlated with the logarithm of processed DWT coefficients. Experimental results are provided that support the aforementioned model. Compared to the conventional 1-D DWT and SPIHT algorithm, the proposed scheme remarkably improves the video quality by an average of 2.23 dB in peak signal-to-noise ratio when the target compression ratio for the SPIHT algorithm is 5/16.
C1 [Kim, Hyun] Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
   [No, Albert] Hongik Univ, Dept Elect & Elect Engn, Seoul 04066, South Korea.
   [Lee, Hyuk-Jae] Seoul Natl Univ, Interuniv Semicond Res Ctr, Dept Elect & Comp Engn, Seoul 08226, South Korea.
C3 Seoul National University of Science & Technology; Hongik University;
   Seoul National University (SNU)
RP Lee, HJ (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Dept Elect & Comp Engn, Seoul 08226, South Korea.
EM hyunkim@seoultech.ac.kr; albertno@hongik.ac.kr;
   hyuk_jae_lee@capp.snu.ac.kr
FU "The Project of Industrial Technology Innovation" through the Ministry
   of Trade, Industry and Energy [10082585]; National Research Foundation
   of Korea grant - Korea Government (MSIT) [NRF-2017R1C1B5018298]
FX This work was supported in part by "The Project of Industrial Technology
   Innovation" through the Ministry of Trade, Industry and Energy under
   Grant 10082585,2017, and in part by the National Research Foundation of
   Korea grant funded by the Korea Government (MSIT) under Grant
   NRF-2017R1C1B5018298.
CR Bao XN, 2012, IEEE T MULTIMEDIA, V14, P237, DOI 10.1109/TMM.2011.2171677
   Bjotegaard G., 2001, VCEGM33
   Casares M, 2011, IEEE T CIRC SYST VID, V21, P1438, DOI 10.1109/TCSVT.2011.2162762
   Chen PY, 2004, IEEE T COMPUT, V53, P386, DOI 10.1109/TC.2004.1268396
   Christopoulos C, 2000, IEEE T CONSUM ELECTR, V46, P1103, DOI 10.1109/30.920468
   DELP EJ, 1979, IEEE T COMMUN, V27, P1335, DOI 10.1109/TCOM.1979.1094560
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   Gupte AD, 2011, IEEE T CIRC SYST VID, V21, P225, DOI 10.1109/TCSVT.2011.2105599
   Jin Y, 2012, IEEE T CIRC SYST VID, V22, P1064, DOI 10.1109/TCSVT.2012.2189793
   Jin Y, 2009, EURASIP J ADV SIG PR, DOI 10.1155/2009/629285
   Kim H, 2017, IEEE T CONSUM ELECTR, V63, P359, DOI 10.1109/TCE.2017.015073
   Kim H, 2016, IEEE T MULTIMEDIA, V18, P603, DOI 10.1109/TMM.2016.2525861
   Kim H, 2015, IEEE T VLSI SYST, V23, P2685, DOI 10.1109/TVLSI.2014.2369520
   Kim H, 2011, IEEE INT SYMP CIRC S, P571
   Kim J, 2010, IEEE T CIRC SYST VID, V20, P848, DOI 10.1109/TCSVT.2010.2045923
   Kim S, 2016, J DISP TECHNOL, V12, P376, DOI 10.1109/JDT.2015.2493163
   Kim S, 2016, IEEE T MULTIMEDIA, V18, P392, DOI 10.1109/TMM.2015.2514196
   Kuo HC, 2012, IEEE T MULTIMEDIA, V14, P500, DOI 10.1109/TMM.2012.2191945
   Lee TY, 2003, IEEE T CIRC SYST VID, V13, P529, DOI 10.1109/TCSVT.2003.813425
   Park S, 2016, IEEE J SOLID-ST CIRC, V51, P2380, DOI 10.1109/JSSC.2016.2582864
   Pearlman WA, 2004, IEEE T CIRC SYST VID, V14, P1219, DOI 10.1109/TCSVT.2004.835150
   Said A, 1996, IEEE T CIRC SYST VID, V6, P243, DOI 10.1109/76.499834
   Saywood Khalid, 2005, INTRO DATA COMPRESSI, p[4, 497]
   SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085
   SHENSA MJ, 1992, IEEE T SIGNAL PROCES, V40, P2464, DOI 10.1109/78.157290
   Taubman D, 2000, IEEE T IMAGE PROCESS, V9, P1158, DOI 10.1109/83.847830
   Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521
   Yng TLB, 2008, IEEE T CONSUM ELECTR, V54, P1453, DOI 10.1109/TCE.2008.4637640
NR 30
TC 11
Z9 12
U1 0
U2 2
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2018
VL 20
IS 12
BP 3200
EP 3211
DI 10.1109/TMM.2018.2832604
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA HA4EY
UT WOS:000450212600002
DA 2024-07-18
ER

PT J
AU Conti, C
   Soares, LD
   Nunes, P
AF Conti, Caroline
   Soares, Luis Ducla
   Nunes, Paulo
TI Light Field Coding With Field-of-View Scalability and Exemplar-Based
   Interlayer Prediction
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Light field; holoscopic; plenoptic; integral imaging; field-of-view
   scalability; image compression; HEVC
ID HEVC; SCHEME
AB Light field imaging based on microlens arrays-a.k.a. holoscopic, plenoptic, and integral imaging-has currently risen up as a feasible and prospective technology for future image and video applications. However, deploying actual light field applications will require identifying more powerful representations and coding solutions that support arising new manipulation and interaction functionalities. In this context, this paper proposes a novel scalable coding solution that supports a new type of scalability, referred to as field-of-view scalability. The proposed scalable coding solution comprises a base layer compliant with the High Efficiency Video Coding (HEVC) standard, complemented by one or more enhancement layers that progressively allow richer versions of the same light field content in terms of content manipulation and interaction possibilities. In addition, to achieve high-compression performance in the enhancement layers, novel exemplar-based interlayer coding tools are also proposed, namely: 1) a direct prediction based on exemplar texture samples from lower layers and 2) an interlayer compensated prediction using a reference picture that is built relying on an exemplar-based algorithm for texture synthesis. Experimental results demonstrate the advantages of the proposed scalable coding solution to cater to users with different preferences/requirements in terms of interaction functionalities, while providing better rate-distortion performance (independently of the optical setup used for acquisition) compared to HEVC and other scalable light field coding solutions in the literature.
C1 [Conti, Caroline; Soares, Luis Ducla; Nunes, Paulo] Inst Univ Lisboa, P-1649026 Lisbon, Portugal.
   [Conti, Caroline; Soares, Luis Ducla; Nunes, Paulo] Inst Telecomunicacoes, P-1049001 Lisbon, Portugal.
C3 Instituto Universitario de Lisboa; Instituto de Telecomunicacoes
RP Conti, C (corresponding author), Inst Univ Lisboa, P-1649026 Lisbon, Portugal.
EM caroline.conti@lx.it.pt; lds@lx.it.pt; paulo.nunes@lx.it.pt
RI Nunes, Paulo/AAG-7212-2020; Conti, Caroline/P-4534-2015; Soares,
   Luis/F-3166-2011
OI Nunes, Paulo/0000-0003-3982-5723; Conti, Caroline/0000-0002-9197-2627;
   Soares, Luis/0000-0001-9738-639X
FU FCT (Fundacao para a Ciencia e a Tecnologia, Portugal)
   [SFRH/BD/79480/2011, UID/EEA/50008/2013]; Fundação para a Ciência e a
   Tecnologia [SFRH/BD/79480/2011, UID/EEA/50008/2013] Funding Source: FCT
FX This work was supported by FCT (Fundacao para a Ciencia e a Tecnologia,
   Portugal) under Grant SFRH/BD/79480/2011 and UID/EEA/50008/2013 project.
CR [Anonymous], 2015, NOVEL 3D MEDIA TECHN
   [Anonymous], P ISO IEC JTC1 SC29
   [Anonymous], P ISO IEC JTC 1 SC29
   [Anonymous], 2015, LIGHT FIELD TOOLBOX
   [Anonymous], 2006, DIGITAL LIGHT FIELD
   [Anonymous], TODOR GEORGIEV GALLE
   [Anonymous], P ISO IEC JTC1 SC29
   [Anonymous], 2001, VCEG M
   Bishop TE, 2012, IEEE T PATTERN ANAL, V34, P972, DOI 10.1109/TPAMI.2011.168
   Bossen Bossen F. F., L1100 JCT VC, P1
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   Chou C, 2014, INT SYMP MICROARCH, P1, DOI 10.1109/MICRO.2014.63
   Conti C, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574667
   Conti C, 2016, SIGNAL PROCESS-IMAGE, V42, P59, DOI 10.1016/j.image.2016.01.008
   Conti C, 2013, IEEE SIGNAL PROC LET, V20, P819, DOI 10.1109/LSP.2013.2267234
   Conti C, 2012, IEEE IMAGE PROC, P1325, DOI 10.1109/ICIP.2012.6467112
   Criminisi A, 2004, IEEE T IMAGE PROCESS, V13, P1200, DOI 10.1109/TIP.2004.833105
   Dick J., 2011, 2011 IEEE EUROCON IN, P1
   Dricot A, 2015, EUR SIGNAL PR CONF, P101, DOI 10.1109/EUSIPCO.2015.7362353
   Ebrahimi-Moghadam A, 2005, IEEE T MULTIMEDIA, V7, P680, DOI 10.1109/TMM.2005.850967
   Georgiev T, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3442712
   Graziosi DB, 2015, PROC SPIE, V9391, DOI 10.1117/12.2083439
   Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199
   Li Y, 2016, IEEE T CIRC SYST VID, V26, P1308, DOI 10.1109/TCSVT.2015.2450333
   Li Y, 2016, IEEE T IMAGE PROCESS, V25, P80, DOI 10.1109/TIP.2015.2498406
   Liu D., 2016, DESIGN AUTOMATION C, P1, DOI DOI 10.1109/GLOCOM.2016.7842078
   Liu DY, 2016, SIGNAL PROCESS-IMAGE, V47, P438, DOI 10.1016/j.image.2016.08.004
   Lucas LER, 2014, EUR SIGNAL PR CONF, P11
   Lumsdaine A, 2012, PROC SPIE, V8299, DOI 10.1117/12.909691
   Lv H., 2012, P VIS COMM IM PROC N, P1
   Monteiro R, 2016, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2016.7574670
   Olsson R, 2006, IEEE IMAGE PROC, P513, DOI 10.1109/ICIP.2006.312389
   Park J, 2016, IEEE T BROADCAST, V62, P685, DOI 10.1109/TBC.2016.2515545
   Perra C, 2016, IEEE INT CONF MULTI
   Raghavendra R, 2015, IEEE T IMAGE PROCESS, V24, P1060, DOI 10.1109/TIP.2015.2395951
   Ramanathan P, 2007, IEEE T MULTIMEDIA, V9, P813, DOI 10.1109/TMM.2007.893350
   Rerabek Martin, 2016, P 8 INT C QUAL MULT
   Sánchez-Hernández JJ, 2015, IEEE T MULTIMEDIA, V17, P1829, DOI 10.1109/TMM.2015.2470595
   Schmeing M, 2015, IEEE T MULTIMEDIA, V17, P2160, DOI 10.1109/TMM.2015.2476372
   Shi S, 2011, IEEE IMAGE PROC, P137, DOI 10.1109/ICIP.2011.6115695
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Tan TK, 2006, IEEE IMAGE PROC, P1693, DOI 10.1109/ICIP.2006.312685
   Toni L, 2017, IEEE T MULTIMEDIA, V19, P2775, DOI 10.1109/TMM.2017.2713644
   Toni L, 2016, IEEE T MULTIMEDIA, V18, P852, DOI 10.1109/TMM.2016.2537207
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Vieira A, 2015, INT CONF IMAG PROC, P494, DOI 10.1109/IPTA.2015.7367195
   Viola I, 2017, IEEE J-STSP, V11, P1092, DOI 10.1109/JSTSP.2017.2740167
   Wang G, 2016, IEEE T IMAGE PROCESS, V25, P5104, DOI 10.1109/TIP.2016.2603602
   Xiao X, 2013, APPL OPTICS, V52, P546, DOI 10.1364/AO.52.000546
   Yan Piao, 2010, 2010 International Conference on Audio, Language and Image Processing (ICALIP), P1164, DOI 10.1109/ICALIP.2010.5685208
NR 51
TC 25
Z9 27
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 2905
EP 2920
DI 10.1109/TMM.2018.2825882
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800004
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Zhao, RM
   Wang, J
   Lu, KJ
   Chang, XM
   Jia, JC
   Zhang, SK
AF Zhao, Ruimin
   Wang, Jin
   Lu, Kejie
   Chang, Xiangmao
   Jia, Juncheng
   Zhang, Shukui
TI Optimal Transmission Topology Construction and Secure Linear Network
   Coding Design for Virtual-Source Multicast With Integral Link Rates
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Linear network coding; multi-source multicast; weakly secure;
   throughput; integral link rate
ID CODED PACKET NETWORKS; UNICAST; MULTIMEDIA; DELAY
AB The continuous demand for content-rich multimedia is pushing for high-speed and secure transmission approaches. In recent years, linear network coding (LNC) has been shown to be a promising technology to improve network throughput, transmission reliability, and information security. In this paper, we study the optimal transmission topology construction and LNC design for a secure multiple-source multicast to deliver the same content with integral link rates, which can be equivalent to the secure multicast problem with a virtual source, i.e., the integer secure virtual-source multicast (ISVM) problem. The objectives of the ISVM problem include the following: 1) satisfy the weakly secure requirements, 2) maximize the secure multicast rate (SMR), and 3) minimize the transmission cost when the SMR is maximized. First, we analyze the necessary and sufficient condition that there exist a transmission topology with integral link rates and a secure LNC that can achieve a given SMR R. Then, we model the ISVM problem as an integer linear programming based on the theoretical analysis and design an efficient transmission topology construction algorithm to solve the ISVM problem by utilizing the Lagrangian relaxation and subgradient method. We also analyze the size of finite field required to construct the deterministic LNC for a secure virtual-source multicast and the probability that the virtual-source multicast is weakly secure when using random LNC in the ISVM problem. Finally, we design upper and lower bounds for the ISVM problem and conduct extensive simulations to compare the performance of the proposed algorithms with these two bounds.
C1 [Zhao, Ruimin; Wang, Jin; Jia, Juncheng; Zhang, Shukui] Suzhou Univ, Dept Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Lu, Kejie] Univ Puerto Rico Mayaguez, Dept Comp Sci & Engn, Mayaguez, PR 00681 USA.
   [Chang, Xiangmao] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210007, Jiangsu, Peoples R China.
C3 University of Puerto Rico; University of Puerto Rico Mayaguez; Nanjing
   University of Aeronautics & Astronautics
RP Wang, J (corresponding author), Suzhou Univ, Dept Comp Sci & Technol, Suzhou 215006, Peoples R China.
EM RuiminZh@hotmail.com; wjin1985@suda.edu.cn; lukejie@ece.uprm.edu;
   xiangmaoch@nuaa.edu.cn; Jiajuncheng@suda.edu.cn; zhangsk@suda.edu.cn
RI Wang, Chengyin/F-1061-2014
OI Wang, Chengyin/0000-0002-9164-1385; Lu, Kejie/0000-0002-6315-2031
FU National Natural Science Foundation of China [61672370, 61572310,
   61672282]; Natural Science Foundation of the Higher Education
   Institutions of Jiangsu Province [16KJB520040]; Suzhou Key Laboratory of
   Converged Communication [SZS0805]; Prospective Application Foundation
   Research of Suzhou of China [SYG201730]; "Six Talent Peak" high-level
   personnel selection and training foundation of Jiangsu of China
   [2014-WLW-010]; Shanghai Key Laboratory of Intelligent Information
   Processing, Fudan University [IIPL-2016-008]
FX This work was supported in part by the National Natural Science
   Foundation of China (No. 61672370, 61572310, 61672282), in part by the
   Natural Science Foundation of the Higher Education Institutions of
   Jiangsu Province (No. 16KJB520040), in part by the Suzhou Key Laboratory
   of Converged Communication (No. SZS0805), in part by the Prospective
   Application Foundation Research of Suzhou of China (No. SYG201730), in
   part by the "Six Talent Peak" high-level personnel selection and
   training foundation of Jiangsu of China (No. 2014-WLW-010), and in part
   by the Shanghai Key Laboratory of Intelligent Information Processing,
   Fudan University under Grant IIPL-2016-008).
CR [Anonymous], CISCO VISUAL NETWORK
   AT&T, AT T N AM BACKB NETW
   Bhattad K., 2005, P NETW COD THEOR APP, P281
   Cui T, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P2736, DOI 10.1109/ISIT.2007.4557632
   Fiandrotti A, 2014, IEEE T MULTIMEDIA, V16, P521, DOI 10.1109/TMM.2013.2285518
   Heller I., 1956, Linear Inequalities Related Syst., V38, P247
   Ho T, 2006, IEEE T INFORM THEORY, V52, P4413, DOI 10.1109/TIT.2006.881746
   Huang SR, 2014, IEEE ACM T NETWORK, V22, P285, DOI 10.1109/TNET.2013.2270438
   Koch C., 2017, P IEEE INT S WORLD W, P1
   Leong D, 2009, 2009 43RD ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS, VOLS 1 AND 2, P414, DOI 10.1109/CISS.2009.5054756
   Li BC, 2011, P IEEE, V99, P513, DOI 10.1109/JPROC.2010.2091930
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Líma L, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P546, DOI 10.1109/ISIT.2007.4557282
   Liu Z., 2010, Proc. IEEE INFOCOM, P2070
   Llorca J, 2013, IEEE ICC, P3557, DOI 10.1109/ICC.2013.6655103
   Lun DS, 2006, IEEE T INFORM THEORY, V52, P2608, DOI 10.1109/TIT.2006.874523
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Raayatpanah MA, 2017, T EMERG TELECOMMUN T, V28, DOI 10.1002/ett.3079
   Raayatpanah MA, 2014, J NETW COMPUT APPL, V41, P217, DOI 10.1016/j.jnca.2013.12.004
   Raayatpanah MA, 2013, COMPUT NETW, V57, P1113, DOI 10.1016/j.comnet.2012.11.017
   Schrfiver Alexander, 1998, THEORY LINEAR INTEGE
   Sheikh AM, 2014, IEEE T MULTIMEDIA, V16, P2294, DOI 10.1109/TMM.2014.2357716
   Sherali HD, 1996, OPER RES LETT, V19, P105, DOI 10.1016/0167-6377(96)00019-3
   Stahlbuhk T, 2012, ANN ALLERTON CONF, P541, DOI 10.1109/Allerton.2012.6483265
   Wang J, 2016, COMPUT NETW, V110, P1, DOI 10.1016/j.comnet.2016.08.004
   Wang J, 2016, IEEE T MULTIMEDIA, V18, P1149, DOI 10.1109/TMM.2016.2545403
   Wang J, 2013, IEEE T PARALL DISTR, V24, P2025, DOI 10.1109/TPDS.2012.265
   Wang S, 2017, IEEE ACCESS, V5, P6757, DOI 10.1109/ACCESS.2017.2685434
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
   Yan XJ, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P54, DOI 10.1109/ISIT.2006.261597
   Yang J, 2018, IEEE T MULTIMEDIA, V20, P1260, DOI 10.1109/TMM.2017.2760630
   Youail R. S., 2009, P IND EL APPL, P768
   Zhang SQ, 2015, IEEE T NETW SERV MAN, V12, P580, DOI 10.1109/TNSM.2015.2465371
   Zhao RM, 2016, PROCEEDINGS OF 2016 IEEE 18TH INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS; IEEE 14TH INTERNATIONAL CONFERENCE ON SMART CITY; IEEE 2ND INTERNATIONAL CONFERENCE ON DATA SCIENCE AND SYSTEMS (HPCC/SMARTCITY/DSS), P967, DOI [10.1109/HPCC-SmartCity-DSS.2016.141, 10.1109/HPCC-SmartCity-DSS.2016.0138]
   Zhu WT, 2009, IEEE T MULTIMEDIA, V11, P556, DOI 10.1109/TMM.2009.2012920
NR 35
TC 6
Z9 7
U1 0
U2 21
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2018
VL 20
IS 11
BP 3069
EP 3083
DI 10.1109/TMM.2018.2827783
PG 15
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GX5WL
UT WOS:000447824800016
DA 2024-07-18
ER

PT J
AU Garroppo, RG
   Ahmed, M
   Niccolini, S
   Dusi, M
AF Garroppo, Rosario Giuseppe
   Ahmed, Mohamed
   Niccolini, Saverio
   Dusi, Maurizio
TI A Vocabulary for Growth: Topic Modeling of Content Popularity Evolution
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Gaussian mixture model; latent Dirichlet allocation model; topic model;
   popularity prediction
AB In this paper, we present a novel method to predict the long-term popularity of user-generated content (UGC). At first, the method clusters the dynamics of UGC popularity into a vocabulary of growth in popularity (sequence) by using a mixture model. Eventually, the method assigns to each sequence a topic model to describe the dynamics of the sequence in a compact way. We then use this topic model to identify similar patterns of growth in popularity of newly observed UGC. The proposed method has two key features: First, it considers the historical dynamics of the UGC popularity, and second it provides long-term popularity prediction. Results on the real dataset of UGC show that the proposed method is flexible, and able to accurately forecast the complete growth in popularity of a given UGC.
C1 [Garroppo, Rosario Giuseppe] Univ Pisa, Dipartimento Ingn Informaz, I-56126 Pisa, Italy.
   [Ahmed, Mohamed; Niccolini, Saverio] NEC Labs Europe, D-69115 Heidelberg, Germany.
   [Dusi, Maurizio] Copan Grp, I-25125 Brescia, Italy.
C3 University of Pisa; NEC Corporation
RP Garroppo, RG (corresponding author), Univ Pisa, Dipartimento Ingn Informaz, I-56126 Pisa, Italy.
EM r.garroppo@iet.unipi.it; mohamed.ahmed@neclab.eu;
   saverio.niccolini@neclab.eu; Maurizio.Dusi@copangroup.com
RI Garroppo, Rosario Giuseppe/AEK-5133-2022
OI Garroppo, Rosario Giuseppe/0000-0001-7465-6019
CR Ahmed Mohamed, 2013, P 6 ACM INT C WEB SE, P607, DOI [DOI 10.1145/2433396.2433473, 10.1145/2433396.2433473]
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI [DOI 10.1145/2433396.2433489, 10.1145/2433396.2433489]
   [Anonymous], 2014, Journal of Computational Information Systems
   [Anonymous], 2011, P 20 INT C COMPANION, DOI [10.1145/1963192.1963222, DOI 10.1145/1963192.1963222]
   [Anonymous], 2015, PROC 24 ACM INT C IN
   [Anonymous], 2011, P 20 ACM INT C INF K, DOI DOI 10.1145/2063576.2063915
   [Anonymous], 2013, P 6 ACM INT C WEB SE, DOI DOI 10.1145/2433396.2433443
   [Anonymous], 2016, INTERNET LIVE STAT
   [Anonymous], 2012, AISTATS
   [Anonymous], 2014, PREDICTIVE WEB ANAL
   Bishop Christopher M, 2006, PATTERN RECOGN, V128, P1
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Borghol Y., 2012, ACM SIGKDD International Knowledge Discovery and Data Mining, P1186, DOI [DOI 10.1145/2339530.2339717, 10.1145/2339530.2339717]
   Borghol Y, 2011, PERFORM EVALUATION, V68, P1037, DOI 10.1016/j.peva.2011.07.008
   Castillo C., 2014, P 17 ACM C COMP SUPP, P211, DOI [10.1145/2531602.2531623, DOI 10.1145/2531602.2531623]
   Chowdhury Shaiful Alam, 2013, WEBIST 2013. 9th International Conference on Web Information Systems and Technologies. Proceedings, P233
   Chowdhury SA, 2012, 2012 SEVENTH INTERNATIONAL CONFERENCE ON BROADBAND, WIRELESS COMPUTING, COMMUNICATION AND APPLICATIONS (BWCCA 2012), P244, DOI 10.1109/BWCCA.2012.47
   Figueiredo Flavio, 2011, P 4 ACM INT C WEB SE, P745, DOI DOI 10.1145/1935826.1935925
   Gao S, 2014, LECT NOTES COMPUT SC, V8709, P379, DOI 10.1007/978-3-319-11116-2_33
   Gill P, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P15
   Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101
   He XN, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P233, DOI 10.1145/2600428.2609558
   Hu CJ, 2014, LECT NOTES COMPUT SC, V8710, P58, DOI 10.1007/978-3-319-11119-3_6
   Jamali S, 2009, WISM: 2009 INTERNATIONAL CONFERENCE ON WEB INFORMATION SYSTEMS AND MINING, PROCEEDINGS, P32, DOI 10.1109/WISM.2009.15
   Lee JG, 2012, NEUROCOMPUTING, V76, P134, DOI 10.1016/j.neucom.2011.04.040
   Lerman K., 2010, P 19 INT C WORLD WID, P621, DOI DOI 10.1145/1772690.1772754
   Li CY, 2016, IEEE ACCESS, V4, P1630, DOI 10.1109/ACCESS.2016.2552218
   Lymperopoulos IN, 2016, INFORM SCIENCES, V369, P585, DOI 10.1016/j.ins.2016.07.043
   Matsubara Y., 2012, 18 ACM SIGKDD INT C, P6, DOI DOI 10.1145/2339530.2339537
   Mei T, 2011, ACM T INFORM SYST, V29, DOI 10.1145/1961209.1961213
   Radinsky K., 2012, P 21 INT C WORLD WID, P599, DOI DOI 10.1145/2187836.2187918
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Szabo G, 2010, COMMUN ACM, V53, P80, DOI 10.1145/1787234.1787254
   Tan ZY, 2016, IEEE T BROADCAST, V62, P436, DOI 10.1109/TBC.2016.2540522
   Tatar A, 2014, J INTERNET SERV APPL, V5, DOI 10.1186/s13174-014-0008-y
   Trzcinski T, 2017, IEEE T MULTIMEDIA, V19, P2561, DOI 10.1109/TMM.2017.2695439
   Wattenhofer Miriam., 2012, Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media, P354
   Wu B, 2016, AAAI CONF ARTIF INTE, P272
   Wu B, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P1336, DOI 10.1145/2964284.2964335
   Wu JQ, 2016, IEEE T MULTIMEDIA, V18, P1882, DOI 10.1109/TMM.2016.2579600
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yin Peifeng., 2012, P 5 ACM INT C WEB SE, P623, DOI DOI 10.1145/2124295.2124370
   Zink M, 2009, COMPUT NETW, V53, P501, DOI 10.1016/j.comnet.2008.09.022
NR 43
TC 10
Z9 10
U1 0
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2018
VL 20
IS 10
SI SI
BP 2683
EP 2692
DI 10.1109/TMM.2018.2811625
PG 10
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GT9YH
UT WOS:000444903000012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Guo, C
   Tian, XM
   Mei, T
AF Guo, Cong
   Tian, Xinmei
   Mei, Tao
TI Multigranular Event Recognition of Persona Photo Albums
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Photo album; event recognition; attention network; hierarchical
   structure
AB People are taking more photos than ever before in recent years. To effectively organize these personal photos, the photos are usually assigned to albums according to their events. An efficient way to manage our photos would be if we could recognize the events of the albums automatically. In this paper, we study the problem of recognizing events in personal photo albums. Recognizing events in photo albums is a new challenge since the contents of photos in albums are more complicated than in traditional single-photo tasks, since not all photos in an album are relevant to the event and a single photo in an album often fails to convey the meaningful event semantic behind the album. To solve this problem, we introduce an attention network to learn the representations of photo albums. Then, we adopt a hierarchical model to recognize events from coarse to fine using multigranular features. We evaluate our model on two real-world datasets consisting of personal albums; we find that our model achieves promising results.
C1 [Guo, Cong; Tian, Xinmei] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Anhui, Peoples R China.
   [Mei, Tao] Microsoft Res, Beijing 100080, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft
RP Tian, XM (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230027, Anhui, Peoples R China.
EM gcong18@mail.ustc.edu.cn; xinmei@ustc.edu.cn; tmei@microsoft.com
RI Mei, Tao/GQZ-0596-2022
OI Mei, Tao/0000-0002-5990-7307
FU National Key Research and Development Program of China [2017YFB1002203];
   NSFC [61572451, 61390514, 61632019]; Youth Innovation Promotion
   Association CAS [CX2100060016]; Fok Ying Tung Education Foundation
   [WF2100060004]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2017YFB1002203; in part by NSFC
   under Grant 61572451, Grant 61390514, and Grant 61632019; in part by the
   Youth Innovation Promotion Association CAS under Grant CX2100060016; and
   in part by the Fok Ying Tung Education Foundation under Grant
   WF2100060004.
CR [Anonymous], 2012, P 21 ACM INT C INFOR
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], TECHNOLOGY
   [Anonymous], P 2 ACM INT C MULT R
   [Anonymous], P COMP VIS PATT REC
   [Anonymous], 2014, P 31 INT C INT C MAC
   [Anonymous], JOINT ACM WORKSH MOD
   [Anonymous], 2011, ACM T INTEL SYST TEC, DOI DOI 10.1145/1961189.1961199
   [Anonymous], P INT WORKSH MULT IN
   Bossard L, 2013, IEEE I CONF COMP VIS, P1193, DOI 10.1109/ICCV.2013.151
   Cao L., 2008, Proc. ACM Multimedia, P121
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Dueck D, 2007, IEEE I CONF COMP VIS, P198
   Fu JL, 2015, IEEE I CONF COMP VIS, P1985, DOI 10.1109/ICCV.2015.230
   Guo Cheng, 2015, 2015 IEEE 34th International Performance Computing and Communications Conference (IPCCC), P1, DOI 10.1109/PCCC.2015.7410341
   Izadinia H, 2012, LECT NOTES COMPUT SC, V7575, P430, DOI 10.1007/978-3-642-33765-9_31
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172
   Li L.-J., 2007, PROC IEEE 11 INT C C, P1
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Papadopoulos S, 2011, IEEE MULTIMEDIA, V18, P52, DOI 10.1109/MMUL.2010.68
   Rehurek R., 2010, LREC, DOI DOI 10.13140/2.1.2393.1847
   Salvador A, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301334
   Schindler K., 2008, PROC IEEE C COMPUT V, P1
   Simonyan K., 2014, 14091556 ARXIV
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tang F, 2011, INT CONF ACOUST SPEE, P877
   Tsai SJ, 2011, PROG MOL BIOL TRANSL, V103, P1, DOI 10.1016/B978-0-12-415906-8.00008-X
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wu ZF, 2015, IEEE T MULTIMEDIA, V17, P1960, DOI 10.1109/TMM.2015.2477681
   Yan ZC, 2015, IEEE I CONF COMP VIS, P2740, DOI 10.1109/ICCV.2015.314
   Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10
   Zhou B., 2014, CORR, V1412, P6856
   Zhou BL, 2014, ADV NEUR IN, V27
   Zhu YY, 2016, NEUROCOMPUTING, V187, P83, DOI 10.1016/j.neucom.2015.09.114
NR 35
TC 7
Z9 7
U1 0
U2 9
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1837
EP 1847
DI 10.1109/TMM.2017.2777664
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100019
DA 2024-07-18
ER

PT J
AU Yang, YJ
   Xu, YB
   Wang, E
   Han, JY
   Yu, ZW
AF Yang, Yongjian
   Xu, Yuanbo
   Wang, En
   Han, Jiayu
   Yu, Zhiwen
TI Improving Existing Collaborative Filtering Recommendations via
   Serendipity-Based Algorithm
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Collaborative filtering; unrated items; serendipitous recommendation;
   matrix factorization
ID SYSTEMS
AB In this paper, we study how to address the sparsity, accuracy and serendipity issues of top-N recommendation with collaborative filtering (CF). Existing studies commonly use rated items (which form only a small section in a rating matrix) or import some additional information (e.g., details about the items and users) to improve the performance of CF. Unlike these methods, we propose a novel notion towards a huge amount of unrated items: serendipity item. By utilizing serendipity items, we propose concise satisfaction and interest injection (CSII), a method that can effectively find interesting, satisfying, and serendipitous items in unrated items. By preventing uninteresting and unsatisfying items to he recommended as top-N items, this concise-hut-novel method improves accuracy and recommendation quality (especially serendipity) substantially. Meanwhile, it can address the sparsity and cold-start issues by enriching the rating matrix in CF without additional information. As our method tackles rating matrix before recommendation procedure, it can be applied to most existing CF methods, such as item-based CF, user-based CF and matrix factorization-based CF. Through comprehensive experiments using abundant real-world datasets with LensKit implementation, we successfully demonstrate that our solution improves the performance of existing CF methods consistently and universally. Moreover, comparing with baseline methods, CSII can extract uninteresting items more carefully and cautiously, avoiding potential items inferred by mistake.
C1 [Yang, Yongjian; Xu, Yuanbo; Wang, En; Han, Jiayu] Jilin Univ, Dept Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.
   [Yu, Zhiwen] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
C3 Jilin University; Northwestern Polytechnical University
RP Wang, E (corresponding author), Jilin Univ, Dept Comp Sci & Technol, Changchun 130012, Jilin, Peoples R China.
EM yyj@jlu.edu.cn; yuanbox15@mails.jlu.edu.cn; wangen@jlu.edu.cn;
   jyhan15@mails.jlu.edu.cn; zhiwenyu@nwpu.edu.cn
OI Yu, Zhiwen/0000-0002-9905-3238
FU National Natural Science Foundation of China [61272412]; Jilin
   Scientific and Technological Development Program [20160204021GX];
   Graduate Innovation Fund of Jilin University [2016184]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61272412, in part by the Jilin
   Scientific and Technological Development Program under Grant
   20160204021GX, and in part by the Graduate Innovation Fund of Jilin
   University under Grant 2016184. The associate editor coordinating the
   review of this manuscript and approving it for publication was Dr. Elisa
   Ricci.
CR Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99
   Adomavicius G, 2015, IEEE T KNOWL DATA EN, V27, P1573, DOI 10.1109/TKDE.2014.2384502
   Bobadilla J, 2013, KNOWL-BASED SYST, V46, P109, DOI 10.1016/j.knosys.2013.03.012
   Ekstrand Michael D, 2011, P 5 ACM C REC SYST, P133, DOI DOI 10.1145/2043932.2043958
   GABRIEL KR, 1979, TECHNOMETRICS, V21, P489, DOI 10.2307/1268288
   Han T., IEEE T MULTIMEDIA, V19, P712
   He XN, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P355, DOI 10.1145/3077136.3080777
   He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569
   He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489
   Hu LK, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P345, DOI 10.1145/2600428.2609593
   Hwang WS, 2016, PROC INT CONF DATA, P349, DOI 10.1109/ICDE.2016.7498253
   Jiang M, 2015, IEEE T KNOWL DATA EN, V27, P3084, DOI 10.1109/TKDE.2015.2432811
   Kotkov D, 2016, KNOWL-BASED SYST, V111, P180, DOI 10.1016/j.knosys.2016.08.014
   Lei XJ, 2016, IEEE T MULTIMEDIA, V18, P1910, DOI 10.1109/TMM.2016.2575738
   Lou PL, 2016, 2016 IEEE SECOND INTERNATIONAL CONFERENCE ON MULTIMEDIA BIG DATA (BIGMM), P33, DOI 10.1109/BigMM.2016.38
   Lu J, 2015, DECIS SUPPORT SYST, V74, P12, DOI 10.1016/j.dss.2015.03.008
   Mouzhi Ge, 2010, P 4 ACM C REC SYST B, P257, DOI [10.1145/1864708.1864761, DOI 10.1145/1864708.1864761]
   Pan R, 2008, IEEE DATA MINING, P502, DOI 10.1109/ICDM.2008.16
   Qian XM, 2014, IEEE T KNOWL DATA EN, V26, P1763, DOI 10.1109/TKDE.2013.168
   Rendle S., 2012, C UNC ART INT, P452
   Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071
   Sarwat M, 2014, IEEE T KNOWL DATA EN, V26, P1384, DOI 10.1109/TKDE.2013.29
   Schafer J. B., 2004, ACM T INFORM SYST, V22, P5
   Shuhui Jiang, 2016, IEEE Transactions on Big Data, V2, P43, DOI 10.1109/TBDATA.2016.2541160
   Sun LF, 2017, IEEE T MULTIMEDIA, V19, P609, DOI 10.1109/TMM.2016.2635589
   Yu Z., IEEE T HUMAN MACH SY, V46, P151
   Yu ZW, 2016, ACM T KNOWL DISCOV D, V11, DOI 10.1145/2930671
   Zhang S, 2005, CEC 2005: Seventh IEEE International Conference on E-Commerce Technology, Proceedings, P257, DOI 10.1109/ICECT.2005.102
   Zhao GS, 2017, IEEE T BIG DATA, V3, P67, DOI 10.1109/TBDATA.2016.2552541
   Zhao GS, 2016, IEEE T KNOWL DATA EN, V28, P3382, DOI 10.1109/TKDE.2016.2607172
   Zhao GS, 2016, IEEE T MULTIMEDIA, V18, P496, DOI 10.1109/TMM.2016.2515362
   Zhao WNX, 2016, IEEE T KNOWL DATA EN, V28, P1147, DOI 10.1109/TKDE.2015.2508816
   Zhao Z, 2016, IEEE T KNOWL DATA EN, V28, P2522, DOI 10.1109/TKDE.2016.2569096
NR 33
TC 32
Z9 35
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1888
EP 1900
DI 10.1109/TMM.2017.2779043
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100023
DA 2024-07-18
ER

PT J
AU Zhan, YB
   Zhang, R
AF Zhan, Yibing
   Zhang, Rong
TI No-Reference Image Sharpness Assessment Based on Maximum Gradient and
   Variability of Gradients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Image sharpness assessment; no-reference; perceived sharpness; maximum
   gradient; variability of gradients
ID QUALITY ASSESSMENT; DATABASE
AB Gradients are commonly used in image sharpness assessment methods. However, research has not fully addressed the direct relationship between gradients and the perceived sharpness. In this paper, we discover and validate through experiments that the maximum gradient is an effective indicator of the perceived image sharpness on a global or local scale. Based on these observations, we propose a novel and efficient no-reference image quality assessment (NR-IQA) method for blurry images. Our method uses two elements to predict the quality of blurry images: the maximum gradient and the variability of gradients. The maximum gradient represents the sharpest spot in an image, and the variability of gradients shows variations within the content of the image. According to the characteristics of human visual systems, these factors are significant for humans when judging the quality of blurry images. The method was tested using blurry image datasets from five public IQA databases. Compared with nine other state-of-the-art NR-IQA methods for blurry images, the experimental results demonstrate that our method is more consistent with humans' subjective evaluations. The MATLAB source code of our method is available at https://github.com/Atmegal/Sharpness-evaluation.
C1 [Zhan, Yibing] Univ Sci & Technol China, Hefei 230027, Anhui, Peoples R China.
   [Zhang, Rong] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Zhang, R (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230027, Anhui, Peoples R China.
EM zybjy@mail.ustc.edu.cn; zrong@ustc.edu.cn
CR [Anonymous], IEEE T CIRCUITS SYST
   Bahrami K, 2016, IEEE T MULTIMEDIA, V18, P1568, DOI 10.1109/TMM.2016.2573139
   Bahrami K, 2014, IEEE SIGNAL PROC LET, V21, P751, DOI 10.1109/LSP.2014.2314487
   Bovik AC, 2006, MODERN IMAGE QUALITY
   Cai X, 2013, IEEE T IMAGE PROCESS, V22, P5395, DOI 10.1109/TIP.2013.2284073
   Chen MJ, 2011, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2011-3
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Ferzli R, 2009, IEEE T IMAGE PROCESS, V18, P717, DOI 10.1109/TIP.2008.2011760
   Graham N.V.S., 1989, Visual pattern analyzers
   Gu K, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2439035
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Hassen R, 2013, IEEE T IMAGE PROCESS, V22, P2798, DOI 10.1109/TIP.2013.2251643
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li LD, 2016, IEEE T CYBERNETICS, V46, P39, DOI 10.1109/TCYB.2015.2392129
   Li LD, 2017, IEEE T MULTIMEDIA, V19, P1030, DOI 10.1109/TMM.2016.2640762
   Li LD, 2016, IEEE T MULTIMEDIA, V18, P1085, DOI 10.1109/TMM.2016.2545398
   Li LD, 2016, NEUROCOMPUTING, V177, P572, DOI 10.1016/j.neucom.2015.11.063
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P918, DOI 10.1109/LSP.2014.2320743
   Li LD, 2014, IEEE SIGNAL PROC LET, V21, P122, DOI 10.1109/LSP.2013.2294333
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Luhong Liang, 2009, Proceedings of the 2009 16th IEEE International Conference on Image Processing (ICIP 2009), P4369, DOI 10.1109/ICIP.2009.5413545
   Marichal X., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P386, DOI 10.1109/ICIP.1999.822923
   Marziliano P, 2002, 2002 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL III, PROCEEDINGS, P57, DOI 10.1109/ICIP.2002.1038902
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Qian JS, 2014, DIGIT SIGNAL PROCESS, V33, P125, DOI 10.1016/j.dsp.2014.06.009
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sullivan GJ, 1998, IEEE SIGNAL PROC MAG, V15, P74, DOI 10.1109/79.733497
   Video Quality Experts Group, 2000, Final report from the video quality experts group on the validation of objective models of video quality assessment march 2000
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Vu CT, 2012, IEEE T IMAGE PROCESS, V21, P934, DOI 10.1109/TIP.2011.2169974
   Vu PV, 2012, IEEE SIGNAL PROC LET, V19, P423, DOI 10.1109/LSP.2012.2199980
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiegand T, 2003, IEEE T CIRC SYST VID, V13, P560, DOI 10.1109/TCSVT.2003.815165
   Zaric A, 2011, ELMAR PROC, P105
   Zhan YB, 2017, IEEE T MULTIMEDIA, V19, P1837, DOI 10.1109/TMM.2017.2689923
   Zhan YB, 2017, IEEE SIGNAL PROC LET, V24, P760, DOI 10.1109/LSP.2017.2688371
   Zhan YB, 2016, IEEE IMAGE PROC, P2072, DOI 10.1109/ICIP.2016.7532723
   Zhan YB, 2015, INT CONF ACOUST SPEE, P1662, DOI 10.1109/ICASSP.2015.7178253
   Zhan YB, 2016, INT CONF ACOUST SPEE, P1095, DOI 10.1109/ICASSP.2016.7471845
   Zhang F, 2011, IEEE T MULTIMEDIA, V13, P615, DOI 10.1109/TMM.2011.2134079
   Zhu JY, 2012, IEEE T IMAGE PROCESS, V21, P919, DOI 10.1109/TIP.2011.2169971
NR 48
TC 41
Z9 51
U1 3
U2 36
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2018
VL 20
IS 7
BP 1796
EP 1808
DI 10.1109/TMM.2017.2780770
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GJ7MC
UT WOS:000435570100016
DA 2024-07-18
ER

PT J
AU Gu, J
   Meng, GF
   Redi, JA
   Xiang, SM
   Pan, CH
AF Gu, Jie
   Meng, Gaofeng
   Redi, Judith A.
   Xiang, Shiming
   Pan, Chunhong
TI Blind Image Quality Assessment via Vector Regression and Object Oriented
   Pooling
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural network; image quality assessment; perceptual image
   quality; object oriented pooling; vector regression
ID PERCEPTUAL IMAGE; NEURAL-NETWORKS; FRAMEWORK; DATABASE; SCORES
AB This paper presents an effective method based on vector regression and object oriented pooling for blind image quality assessment. Unlike previous models that map the extracted features directly to a quality score, the proposed vector regression framework yields a vector of belief scores for the input image. We explore the uncertainty factors in quality assessment and design the belief scores to measure the confidences of an image to be assigned to the corresponding quality grades. Moreover, we propose an object oriented pooling strategy to further improve the performance by incorporating semantic information of image contents. According to this strategy, regions occupied by objects will be assigned more weights in the pooling phase, leading to a more accurate quality assessment. Extensive experiments on benchmark datasets demonstrate that our approach achieves state-of-the-art performance and shows a great generalization ability.
C1 [Gu, Jie; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Gu, Jie; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Redi, Judith A.] Delft Univ Technol, Dept Intelligent Syst, NL-2628 CD Delft, Netherlands.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Delft University of Technology
RP Gu, J (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM jie.gu@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn; J.A.Redi@tudelft.nl;
   smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
FU National Natural Science Foundation of China [61370039, 91646207,
   91338202]; Beijing Nature Science Foundation [4162064]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61370039, Grant 91646207, and Grant
   91338202, and in part by the Beijing Nature Science Foundation under
   Grant 4162064.
CR Alexe B, 2012, IEEE T PATTERN ANAL, V34, P2189, DOI 10.1109/TPAMI.2012.28
   [Anonymous], 2014, P INT C LEARN REPR
   [Anonymous], 2000, Psychometric scaling, a toolkit for imaging systems development
   [Anonymous], 2014, PROC IEEE C COMPUTER, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
   Arbeláez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49
   Bovik AC, 2013, P IEEE, V101, P2008, DOI 10.1109/JPROC.2013.2257632
   Cheng MM, 2014, PROC CVPR IEEE, P3286, DOI 10.1109/CVPR.2014.414
   Engelke U, 2011, IEEE SIGNAL PROC MAG, V28, P50, DOI 10.1109/MSP.2011.942473
   Gastaldo P, 2013, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2013-54
   Ghadiyaram D, 2017, J VISION, V17, DOI 10.1167/17.1.32
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Ghadiyaram D, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P946, DOI 10.1109/GlobalSIP.2014.7032260
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Golestaneh S, 2016, IEEE T IMAGE PROCESS, V25, P5293, DOI 10.1109/TIP.2016.2601821
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He LH, 2012, PROC CVPR IEEE, P1146, DOI 10.1109/CVPR.2012.6247795
   Hou WL, 2015, IEEE MULTIMEDIA, V22, P46, DOI 10.1109/MMUL.2014.55
   Hou WL, 2015, IEEE T NEUR NET LEAR, V26, P1275, DOI 10.1109/TNNLS.2014.2336852
   Judd T, 2009, IEEE I CONF COMP VIS, P2106, DOI 10.1109/ICCV.2009.5459462
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Krähenbühl P, 2014, LECT NOTES COMPUT SC, V8693, P725, DOI 10.1007/978-3-319-10602-1_47
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Larson EC, 2010, J ELECTRON IMAGING, V19, DOI 10.1117/1.3267105
   Li HX, 2016, IEEE T IMAGE PROCESS, V25, P1834, DOI 10.1109/TIP.2015.2510583
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Lu X, 2015, IEEE I CONF COMP VIS, P990, DOI 10.1109/ICCV.2015.119
   Ma KD, 2016, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2016.184
   Ma L, 2016, IEEE T MULTIMEDIA, V18, P2228, DOI 10.1109/TMM.2016.2614187
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Nair Vinod, 2010, INT C INT C MACHINE, P807
   Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178
   Ponomarenko N., 2009, ADV MODERN RADIOELEC, V10, P30, DOI 10.1109/TIP.2015.2439035
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   Redi J, 2011, PROC SPIE, V7865, DOI 10.1117/12.876712
   Rehman A, 2012, IEEE T IMAGE PROCESS, V21, P3378, DOI 10.1109/TIP.2012.2197011
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Saha A, 2015, IEEE T IMAGE PROCESS, V24, P1879, DOI 10.1109/TIP.2015.2411436
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P3440, DOI 10.1109/TIP.2006.881959
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang HX, 2014, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2014.368
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   Tourancheau S, 2008, IEEE IMAGE PROC, P365, DOI 10.1109/ICIP.2008.4711767
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang N., 2013, Advances in Neural Information Processing Systems, P809
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2011, IEEE T IMAGE PROCESS, V20, P1185, DOI 10.1109/TIP.2010.2092435
   Wu QB, 2016, IEEE T CIRC SYST VID, V26, P425, DOI 10.1109/TCSVT.2015.2412773
   Wu Q, 2017, AER ADV ENG RES, V107, P10
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P684, DOI 10.1109/TIP.2013.2293423
   Ye P, 2014, PROC CVPR IEEE, P4241, DOI 10.1109/CVPR.2014.540
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Ye P, 2012, IEEE T IMAGE PROCESS, V21, P3129, DOI 10.1109/TIP.2012.2190086
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P4959, DOI 10.1109/TIP.2016.2598679
   Zhang L, 2016, IEEE T IMAGE PROCESS, V25, P1177, DOI 10.1109/TIP.2016.2516952
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE T IMAGE PROCESS, V23, P4270, DOI 10.1109/TIP.2014.2346028
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang P, 2015, PROC CVPR IEEE, P2394, DOI 10.1109/CVPR.2015.7298853
NR 66
TC 34
Z9 36
U1 1
U2 19
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2018
VL 20
IS 5
BP 1140
EP 1153
DI 10.1109/TMM.2017.2761993
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA GD7YD
UT WOS:000430728400010
OA Green Published
DA 2024-07-18
ER

PT J
AU Beyan, C
   Capozzi, F
   Becchio, C
   Murino, V
AF Beyan, Cigdem
   Capozzi, Francesca
   Becchio, Cristina
   Murino, Vittorio
TI Prediction of the Leadership Style of an Emergent Leader Using Audio and
   Visual Nonverbal Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Emergent leader; in-the-wild; leadership style; multiple kernel learning
   (MKL); nonverbal features; small group interactions; social signal
   processing
ID DOMINANCE; SPEAKING; ALGORITHM; LOOKING
AB The coordination of a leader with group members is very important for an effective leadership given that this figure is the person who actually manages the team members to achieve a desired goal. Investigating the leadership and especially the leadership style is a prominent research topic in social and organizational psychology. However, this is a new problem in social signal processing that can actually make valuable contributions by analyzing multimodal data in a more effective and efficient way. In this work, we identify the leadership style of an emergent leader (i.e., the leader who naturally arises from a group, not designated) as autocratic or democratic. The proposed method is applied to a dataset in-the-wild; in other words, there is no role-playing, which is novel for this problem. Multiple kernel learning (MKL) using multimodal nonverbal features is utilized to predict leadership styles that proved to achieve better predictions as compared to traditional learning methods. Thanks to MKL and a simple heuristic proposed, the best performing features are also identified, showing that better predictions can be reached only by using those features. Additionally, correlation analysis between the extracted nonverbal features and the results of social psychology questionnaire is also performed. This shows that significantly high correlations exist for speaking activity based and prosodic nonverbal features.
C1 [Beyan, Cigdem; Murino, Vittorio] Ist Italiano Tecnol, Pattern Anal & Comp Vis Dept, I-16163 Genoa, Italy.
   [Capozzi, Francesca] McGill Univ, Dept Psychol, Montreal, PQ, Canada.
   [Becchio, Cristina] IIT, Cognit Mot & Neurosci Unit, I-16152 Genoa, Italy.
   [Becchio, Cristina] Univ Turin, Dept Psychol, I-10124 Turin, Italy.
   [Murino, Vittorio] Univ Verona, Dept Comp Sci, I-37129 Verona, Italy.
C3 Istituto Italiano di Tecnologia - IIT; McGill University; Istituto
   Italiano di Tecnologia - IIT; University of Turin; University of Verona
RP Beyan, C (corresponding author), Ist Italiano Tecnol, Pattern Anal & Comp Vis Dept, I-16163 Genoa, Italy.
EM cigdem.beyan@iit.it; francesca.capozzi@mcgill.ca;
   cristina.becchio@iit.it; vittorio.murino@iit.it
RI Castiello, Umberto/HKP-0663-2023; Murino, Vittorio/A-5570-2011; Beyan,
   Cigdem/AAA-4235-2019
OI Beyan, Cigdem/0000-0002-9583-0087; Murino, Vittorio/0000-0002-8645-2328;
   Capozzi, Francesca/0000-0002-0062-437X
CR Aldrich J, 1995, STAT SCI, V10, P364, DOI 10.1214/ss/1177009870
   Anderson C, 2009, J PERS SOC PSYCHOL, V96, P491, DOI 10.1037/a0014201
   [Anonymous], 2008, P 10 INT C MULT INT
   [Anonymous], 2008, P ICMI 2008, DOI DOI 10.1145/1452392.1452404
   [Anonymous], 1991, Joining together: Group theory and group skills
   [Anonymous], 2016, P ACM ICMI ASSP4MI
   Aran Oya, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3687, DOI 10.1109/ICPR.2010.898
   Aran O, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P11, DOI 10.1145/2522848.2522859
   Avci U, 2016, IEEE T MULTIMEDIA, V18, P643, DOI 10.1109/TMM.2016.2521348
   Avci Umut, 2014, WORKSH UND MOD MULT, P9
   Baird J., 1977, Southern Communication Journal, P352, DOI DOI 10.1080/10417947709372361
   Bales R.F., 1980, SYMLOG Case Study Kit
   Barron J. L., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P236, DOI 10.1109/CVPR.1992.223269
   Bass B., 2006, Transformational Leadership, V2nd, DOI DOI 10.4324/9781410617095
   Bass B.M., 2008, The Bass handbook of leadership: Theory, research, and managerialapplications, V4th
   Beyan C, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P317, DOI 10.1145/2993148.2993175
   Beyan C, 2015, PATTERN RECOGN, V48, P1653, DOI 10.1016/j.patcog.2014.10.032
   Bullee J., 2013, THESIS
   Cao Jie, 2010, 2010 IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS 2010), P312, DOI 10.1109/ICICISYS.2010.5658665
   Chittaranjan G., 2001, P IEEE INT C AUT FAC, P734
   Cristinacce D., 2006, BRIT MACH VIS C, V1, P3
   DOVIDIO JF, 1982, SOC PSYCHOL QUART, V45, P106, DOI 10.2307/3033933
   Escalera S, 2009, PROC CVPR IEEE, P856
   Escalera S, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/491819
   Feese S., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P1460, DOI 10.1109/PASSAT/SocialCom.2011.209
   Feese S, 2012, Proceedings of 2012 ASE/IEEE International Conference on Privacy, Security, Risk and Trust and 2012 ASE/IEEE International Conference on Social Computing (SocialCom/PASSAT 2012), P520, DOI 10.1109/SocialCom-PASSAT.2012.48
   Feese S, 2011, IEEE INT SYM WRBL CO, P119, DOI 10.1109/ISWC.2011.31
   Foster D.E., 2002, Communication Teacher, V16, P4
   Gonen M., 2008, ICML, P352, DOI DOI 10.1145/1390156.1390201
   Gönen M, 2011, J MACH LEARN RES, V12, P2211
   Gonzalez S, 2014, IEEE-ACM T AUDIO SPE, V22, P518, DOI 10.1109/TASLP.2013.2295918
   Hall JA, 2005, PSYCHOL BULL, V131, P898, DOI 10.1037/0033-2909.131.6.898
   Hare A., 1998, SYMLOG PRACTITIONER
   Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427
   Hung Hayley, 2008, 2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops), P1, DOI 10.1109/CVPRW.2008.4563178
   Hung H., 2008, P IEEE WORKSH COMP V, P2160
   Hung H, 2011, IEEE T AUDIO SPEECH, V19, P847, DOI 10.1109/TASL.2010.2066267
   Hung Hayley., 2008, P OF THE 10 INT C ON, P233, DOI DOI 10.1145/1452392.1452441
   JAYAGOPI D, 2008, P ACM INT C MULT, P809
   Jayagopi D. B., 2011, THESIS
   Jayagopi D. B., 2009, P ACM INT C MULT INT, P3
   Jayagopi DB, 2010, IEEE T MULTIMEDIA, V12, P790, DOI 10.1109/TMM.2010.2065218
   Jayagopi DB, 2009, IEEE INT CON MULTI, P370, DOI 10.1109/ICME.2009.5202511
   Jayagopi DB, 2009, IEEE T AUDIO SPEECH, V17, P501, DOI 10.1109/TASL.2008.2008238
   Jayagopi D, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P433
   Jayagopi D, 2012, MULTIMEDIA SYST, V18, P3, DOI 10.1007/s00530-011-0243-z
   Kalimeri Kyriaki, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P124, DOI 10.1007/978-3-642-25446-8_14
   Kalimeri K, 2012, ICMI '12: PROCEEDINGS OF THE ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P23
   Kalma A.K., 1993, LEADERSHIP QUART, V4, P45, DOI [DOI 10.1016/1048-9843, DOI 10.1016/1048-9843(93)90003-C, 10.1016/1048-9843(93)90003-C]
   Knapp M.L., 2013, Cengage Learning
   Koenigs R., 1999, SYMLOG RELIABILITY V
   Nguyen LS, 2014, IEEE T MULTIMEDIA, V16, P1018, DOI 10.1109/TMM.2014.2307169
   Mana N., 2007, Proceedings of the 2007 Workshop on tagging, mining and retrieval of human related activity information, P9
   MARKEL JD, 1972, IEEE T ACOUST SPEECH, VAU20, P367, DOI 10.1109/TAU.1972.1162410
   Mast MS, 2002, HUM COMMUN RES, V28, P420, DOI 10.1111/j.1468-2958.2002.tb00814.x
   McCowan I, 2005, IEEE T PATTERN ANAL, V27, P305, DOI 10.1109/TPAMI.2005.49
   Nanjundeswaraswamy T.S., 2014, ADV MANAGEMENT, V7, P57
   Okada S, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P15, DOI 10.1145/2818346.2820757
   Pearson K., 1895, P R SOC LOND, V58, P240, DOI [DOI 10.1098/RSPL.1895.0041, 10.1098/rspl.1895.0041]
   Pierro A, 2003, PERS SOC PSYCHOL B, V29, P405, DOI 10.1177/0146167203251191
   Rienks R, 2005, LECT NOTES COMPUT SC, V3869, P76
   Salah AA, 2011, COMPUTER ANALYSIS OF HUMAN BEHAVIOR, P1, DOI 10.1007/978-0-85729-994-9
   Sanchez-Cortes D., 2009, P ICMI MLMI WORKSH M
   Sanchez-Cortes D., 2013, THESIS
   Sanchez-Cortes D., 2011, TECH REP
   Sanchez-Cortes D, 2013, J MULTIMODAL USER IN, V7, P39, DOI 10.1007/s12193-012-0101-0
   Sanchez-Cortes D, 2012, IEEE T MULTIMEDIA, V14, P816, DOI 10.1109/TMM.2011.2181941
   Sanchez-Cortes Dairazalia, 2010, P ACM ICMI MLMI, P8
   STEIN RT, 1975, J PERS SOC PSYCHOL, V32, P125, DOI 10.1037/h0076842
   Subramanian R, 2013, ICMI'13: PROCEEDINGS OF THE 2013 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P3, DOI 10.1145/2522848.2522862
   Valente F, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P1182
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Varma M., 2009, Proceedings of the 26th Annual International Conference on Machine Learning, P1065, DOI DOI 10.1145/1553374.1553510
   Varma M, 2007, IEEE I CONF COMP VIS, P369
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Xu C, 2013, ARXIV PREPRINT ARXIV
   Zhang D, 2006, IEEE T MULTIMEDIA, V8, P509, DOI 10.1109/TMM.2006.870735
NR 77
TC 35
Z9 42
U1 0
U2 24
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 441
EP 456
DI 10.1109/TMM.2017.2740062
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200015
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Li, CL
   Xiong, HK
   Zou, JN
   Wu, DPO
AF Li, Chenglin
   Xiong, Hongkai
   Zou, Junni
   Wu, Dapeng Oliver
TI Joint Dynamic Rate Control and Transmission Scheduling for Scalable
   Video Multirate Multicast Over Wireless Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dynamic rate allocation; multirate multicast; network coding;
   opportunistic routing; scalable video coding
ID MODEL; OPTIMIZATION; ALLOCATION; DELIVERY; CAPACITY
AB In this paper, we consider the time-varying characteristics of practical wireless networks, and propose a joint dynamic rate allocation and transmission scheduling optimization scheme for scalable video multirate multicast based on opportunistic routing (OR) and network coding. With OR, the decision of optimal routes for scalable video coding layered streaming is integrated into the joint optimization formulation. The network throughput is also increased by taking advantage of the broadcast nature of the wireless shared medium and by network coding operations in intermediate nodes. To maximize the overall video reception quality among all destinations, the proposed scheme can jointly optimize the video reception rate, the associated routes to different destinations, and the time fraction scheduling of transmitter sets that are concurrently transmitting in the shared wireless medium. By using dual decomposition and primal-dual update approach, we develop a cross-layer algorithm in a fully distributed manner. Simulation results demonstrate significant network multicast throughput improvement and adaptation to dynamic network changes relative to existing optimization schemes.
C1 [Li, Chenglin] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, CH-1015 Lausanne, Switzerland.
   [Xiong, Hongkai] Shanghai Jiao Tong Univ, Dept Elect Engn, Shanghai 200240, Peoples R China.
   [Zou, Junni] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Wu, Dapeng Oliver] Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Shanghai Jiao Tong University; Shanghai Jiao Tong
   University; State University System of Florida; University of Florida
RP Wu, DPO (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
EM chenglin.li@epfl.ch; xionghongkai@sjtu.edu.cn; zou-jn@cs.sjtu.edu.cn;
   dpwu@ieee.org
RI LI, CHENGLIN/JUF-8254-2023
OI Xiong, Hongkai/0000-0003-4552-0029; Wu, Dapeng/0000-0003-1755-0183
FU NSFC [61501293, 61529101, 61425011, 61622112, 61472234]; Program of
   Shanghai Academic Research Leader [17XD1401900]; China Postdoctoral
   Science Foundation [2016T90372, 2015M570365]; China Scholarship Council;
   NSF [ECCS-1509212]
FX This work was supported in part by the NSFC under Grants 61501293,
   61529101, 61425011, 61622112, and 61472234, in part by the Program of
   Shanghai Academic Research Leader under Grant 17XD1401900, in part by
   the China Postdoctoral Science Foundation under Grants 2016T90372 and
   2015M570365, in part by the China Scholarship Council, and in part by
   the NSF under Grant ECCS-1509212. The associate editor coordinating the
   review of this manuscript and approving it for publication was Prof.
   Christian Timmerer. (Corresponding author: Dapeng Oliver Wu.)
CR Akyildiz IF, 2005, IEEE COMMUN MAG, V43, pS23, DOI 10.1109/MCOM.2005.1509968
   [Anonymous], P 27 C COMP COMM IEE
   [Anonymous], 2012, P ACM MOBIHOC
   [Anonymous], 2005, JCT1SC29WG11N6900 IS
   Biswas S, 2005, ACM SIGCOMM COMP COM, V35, P133, DOI 10.1145/1090191.1080108
   Chachulski S, 2007, ACM SIGCOMM COMP COM, V37, P169, DOI 10.1145/1282427.1282400
   Chakchouk N, 2015, IEEE COMMUN SURV TUT, V17, P2214, DOI 10.1109/COMST.2015.2411335
   Chen C, 2013, IEEE T CIRC SYST VID, V23, P1081, DOI 10.1109/TCSVT.2013.2254896
   Chen JT, 2011, IEEE T SIGNAL PROCES, V59, P2395, DOI 10.1109/TSP.2011.2106124
   Chen L, 2007, IEEE INFOCOM SER, P1163, DOI 10.1109/INFCOM.2007.139
   Cheng Y, 2012, IEEE T VEH TECHNOL, V61, P3151, DOI 10.1109/TVT.2012.2204411
   CISCO Visual Networking Index (VNI), 2015, CISCO VISUAL NETWORK
   Goldsmith A., 2005, WIRELESS COMMUNICATI
   Gupta P, 2000, IEEE T INFORM THEORY, V46, P388, DOI 10.1109/18.825799
   Hua S, 2011, IEEE T MULTIMEDIA, V13, P402, DOI 10.1109/TMM.2010.2103929
   JAIN KAMAL., 2003, Proceedings of the 9th annual international conference on Mobile computing and networking, MobiCom '03, P66, DOI DOI 10.1145/938985.938993
   Joseph V, 2014, IEEE INFOCOM SER, P82, DOI 10.1109/INFOCOM.2014.6847927
   Kar K, 2002, IEEE J SEL AREA COMM, V20, P1541, DOI 10.1109/JSAC.2002.803988
   Kar K, 2001, IEEE INFOCOM SER, P123, DOI 10.1109/INFCOM.2001.916694
   Koutsonikolas D, 2011, IEEE ACM T NETWORK, V19, P1368, DOI 10.1109/TNET.2011.2111382
   Li CL, 2012, IEEE T CIRC SYST VID, V22, P943, DOI 10.1109/TCSVT.2012.2186740
   Lin YF, 2008, I C NETWORK PROTOCOL, P13, DOI 10.1109/ICNP.2008.4697020
   Lun DS, 2006, IEEE T INFORM THEORY, V52, P2608, DOI 10.1109/TIT.2006.874523
   Mayne DQ, 2000, AUTOMATICA, V36, P789, DOI 10.1016/S0005-1098(99)00214-9
   Miu Allen, 2005, P 11 ANN INT C MOB C, P16
   Neely MJ, 2005, IEEE J SEL AREA COMM, V23, P89, DOI 10.1109/JSAC.2004.837349
   Palomar DP, 2006, IEEE J SEL AREA COMM, V24, P1439, DOI 10.1109/JSAC.2006.879350
   Radunovic B, 2010, IEEE ACM T NETWORK, V18, P420, DOI 10.1109/TNET.2009.2030682
   Reis C, 2006, ACM SIGCOMM COMP COM, V36, P51, DOI 10.1145/1151659.1159921
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Shakkottai S. G., 2008, Network Optimization and Control
   Sheu JP, 2013, IEEE T MOBILE COMPUT, V12, P90, DOI 10.1109/TMC.2011.250
   Shi Y, 2009, MOBIHOC'09 PROCEEDINGS OF THE TENTH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P239, DOI 10.1145/1530748.1530782
   Sklar B, 1997, IEEE COMMUN MAG, V35, P136, DOI 10.1109/35.620535
   Stuhlmüller K, 2000, IEEE J SEL AREA COMM, V18, P1012, DOI 10.1109/49.848253
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Trichakis N., 2008, P IFAC WORLD C, V41, P2907, DOI [DOI 10.3182/20080706-5-KR-1001.00489, 10.3182/20080706-5-KR-1001.00489]
   Vukobratovic D, 2009, IEEE T MULTIMEDIA, V11, P1094, DOI 10.1109/TMM.2009.2026087
   WANG HS, 1995, IEEE T VEH TECHNOL, V44, P163, DOI 10.1109/25.350282
   Xiong LX, 2012, IEEE J SEL AREA COMM, V30, P280, DOI 10.1109/JSAC.2012.120206
   Yan JY, 2006, IEEE T MULTIMEDIA, V8, P196, DOI 10.1109/TMM.2005.864265
   Yan Y, 2010, IEEE WIREL COMMUN, V17, P96, DOI 10.1109/MWC.2010.5490984
   YEUNG RW, 1995, IEEE T INFORM THEORY, V41, P412, DOI 10.1109/18.370142
   Zeng K, 2008, IEEE T WIREL COMMUN, V7, P5118, DOI 10.1109/T-WC.2008.071239
   Zhang QQ, 1999, IEEE T COMMUN, V47, P1688, DOI 10.1109/26.803503
   Zhao J, 2006, IEEE T MULTIMEDIA, V8, P1021, DOI 10.1109/TMM.2006.879847
   Zorzi M, 2003, IEEE T MOBILE COMPUT, V2, P337, DOI 10.1109/TMC.2003.1255648
NR 47
TC 22
Z9 23
U1 0
U2 23
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD FEB
PY 2018
VL 20
IS 2
BP 361
EP 378
DI 10.1109/TMM.2017.2745709
PG 18
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FT2ER
UT WOS:000422953200009
DA 2024-07-18
ER

PT J
AU Chen, Z
   Zhang, X
   Xu, YD
   Xiong, J
   Zhu, Y
   Wang, X
AF Chen, Zhe
   Zhang, Xu
   Xu, Yuedong
   Xiong, Jie
   Zhu, Yu
   Wang, Xin
TI MuVi: Multiview Video Aware Transmission Over MIMO Wireless Systems
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Antenna selection; distortion; MIMO; multiview video streaming; power
   allocation
ID SCALABLE VIDEO; VISIBILITY; SCHEME
AB Multiview video is essential for various mobile three-dimensional (3D) and immersive applications that can capture scenes from multiple angles for better user experience. However, robust transmission of multiview video is very challenging in wireless networks due to high bandwidth requirement and time-varying channel quality. Though the up-to-date 802.11 system enables spatial multiplexing MIMO to enhance transmission capacity, it is still agnostic to 3D source coding structure in the transmission. In this paper, we study the optimal resource allocation problem in MIMO systems that deliver 3D content with multiview video coding. The basic idea is to exploit the channel diversity of multiple antennas and the source coding characteristics so as to achieve unequal error protection against channel errors. To achieve this goal, we develop a nonlinear mixed integer programming framework to perform antenna selection and power allocation, and propose low-complexity algorithms to assign these resources. We implement a proof-of-concept system, namely MuVi, on the software-defined-radio platform, WARP, to evaluate the proposed algorithms. MuVi is the practical system to tackle 3D multiview streaming in the latest Wi-Fi networks such as IEEE 802.11ac under realistic channel conditions. Extensive experimental results demonstrate that the peak signal-to-noise-ratio of MuVi significantly outperforms that of the conventional power allocation scheme in a variety of indoor environments.
C1 [Chen, Zhe; Zhang, Xu; Wang, Xin] Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.
   [Xu, Yuedong; Zhu, Yu] Fudan Univ, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
   [Xiong, Jie] Singapore Management Univ, Sch Informat Syst, Singapore 188065, Singapore.
C3 Fudan University; Fudan University; Singapore Management University
RP Wang, X (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai 200433, Peoples R China.; Xu, YD (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai 200433, Peoples R China.
EM zhechen13@fudan.edu.cn; xuzhang09@fudan.edu.cn; ydxu@fudan.edu.cn;
   jxiong@smu.edu.sg; zhuyu@fudan.edu.cn; xinw@fudan.edu.cn
RI liu, huan/JEO-4705-2023; Chen, Zhe/CAH-1194-2022
OI Chen, Zhe/0000-0002-3215-2696
FU Natural Science Foundation of China [61402114, 61271223]; 863 Program of
   China [2015AA016106]; Natural Science Foundation of Jiangsu Province
   [BK20140404]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 61402114 and Grant 61271223, in part by the 863
   Program of China under Grant 2015AA016106, and in part by Natural
   Science Foundation of Jiangsu Province under Grant BK20140404. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Prof. Christian Timmerer.
   (Corresponding authors: Yuedong Xu; Xin Wang.)
CR Adeyemi-Ejeye AO, 2014, J ELECTR COMPUT ENG, V2014, DOI 10.1155/2014/183716
   [Anonymous], 2011, JVTB118R2
   [Anonymous], 2013, IEEE Std 802.11ac(TM)-2013 (Amendment to IEEE Std 802.11-2012, as amended by IEEE Std 802.11ae-2012, IEEE Std 802.11aa- 2012, and IEEE Std 802, P1
   Chang SH, 2016, IEEE T VEH TECHNOL, V65, P1244, DOI 10.1109/TVT.2015.2412655
   Chen ZB, 2016, INT J ANTENN PROPAG, V2016, DOI 10.1155/2016/8257930
   Fujihashi T., 2014, 7 IEEE 80 VEH TECHN, P1
   Goldsmith A., 2005, WIRELESS COMMUNICATI
   Halperin D, 2010, ACM SIGCOMM COMP COM, V40, P159, DOI 10.1145/1851275.1851203
   He DL, 2015, ACM S MODEL ANAL SIM, P327, DOI 10.1145/2811587.2811601
   Jakubczak S., 2011, PROC MOBICOM, P289
   Khalek AA, 2015, IEEE T MULTIMEDIA, V17, P1802, DOI 10.1109/TMM.2015.2468196
   Liu QA, 2010, IEEE INT CON MULTI, P968, DOI 10.1109/ICME.2010.5582986
   Liu XL, 2014, IEEE T MULTIMEDIA, V16, P2038, DOI 10.1109/TMM.2014.2331616
   Liu XL, 2012, MOBICOM 12: PROCEEDINGS OF THE 18TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P233
   Orlov Z., 2008, IEEE 19 INT S PERS I, P1
   ROSEN JB, 1960, J SOC IND APPL MATH, V8, P181, DOI 10.1137/0108011
   Salim OH, 2012, EURASIP J WIREL COMM, DOI 10.1186/1687-1499-2012-269
   Salim OH, 2013, 2013 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC), P4077
   Song D, 2007, IEEE T CIRC SYST VID, V17, P1218, DOI 10.1109/TCSVT.2007.905531
   Spencer QH, 2004, IEEE T SIGNAL PROCES, V52, P461, DOI 10.1109/TSP.2003.821107
   Toksari MD, 2008, INT J ADV MANUF TECH, V38, P801, DOI 10.1007/s00170-007-1128-3
   Vetro A, 2011, P IEEE, V99, P626, DOI 10.1109/JPROC.2010.2098830
   Vosoughi A, 2013, INT CONF ACOUST SPEE, P2050, DOI 10.1109/ICASSP.2013.6638014
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wiesel A, 2008, IEEE T SIGNAL PROCES, V56, P4409, DOI 10.1109/TSP.2008.924638
   Xie X., 2013, PROC 19 ANN INT C MO, P477
   Xiong J., 2012, PROC ACM HOTMOBILE, P1
   Xiong J, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P29, DOI 10.1145/2674005.2675014
   Xiong Jie., 2013, Nsdi
   Yan B, 2008, IEEE IMAGE PROC, P3064, DOI 10.1109/ICIP.2008.4712442
   Yang Z, 2013, IEEE T CIRC SYST VID, V23, P212, DOI 10.1109/TCSVT.2012.2203216
   Zhang HH, 2010, IEEE J SEL AREA COMM, V28, P344, DOI 10.1109/JSAC.2010.100406
   Zhou C, 2015, IEEE T CIRC SYST VID, V25, P1002, DOI 10.1109/TCSVT.2014.2364418
NR 33
TC 9
Z9 9
U1 1
U2 17
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2017
VL 19
IS 12
BP 2788
EP 2803
DI 10.1109/TMM.2017.2713414
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA FN8HG
UT WOS:000416263200012
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Jin, ZW
   Cao, J
   Zhang, YD
   Zhou, JS
   Tian, Q
AF Jin, Zhiwei
   Cao, Juan
   Zhang, Yongdong
   Zhou, Jianshe
   Tian, Qi
TI Novel Visual and Statistical Image Features for Microblogs News
   Verification
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Fake news detection; image features; microblogs news verification; rumor
   detection; social media
ID PREDICTION
AB Microblog has been a popular media platform for reporting and propagating news. However, fake news spreading on microblogs would severely jeopardize its public credibility. To identify the truthfulness of news on microblogs, images are very crucial content. In this paper, we explore the key role of image content in the task of automatic news verification on microblogs. Existing approaches to news verification depend on features extracted mainly from the text content of news tweets, while image features for news verification are often ignored. According to our study, however, images are very popular and have a great influence on microblogs news propagation. In addition, fake and real news events have different image distribution patterns. Therefore, we propose several visual and statistical features to characterize these patterns visually and statistically for detecting fake news. Experiments on a real-world multimedia dataset collected from Sina Weibo validate the effectiveness of our proposed image features. The news verification performance of our method outperforms baseline methods. To the best of our knowledge, this is the first attempt that systematically explores image features on news verification task.
C1 [Jin, Zhiwei; Cao, Juan; Zhang, Yongdong] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Zhang, Yongdong] Capital Normal Univ, Beijing Adv Innovat Ctr Imaging Technol, Beijing 100048, Peoples R China.
   [Zhou, Jianshe] Capital Normal Univ, Sch Literature, Beijing 100089, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Capital Normal University; Capital Normal University; University of
   Texas System; University of Texas at San Antonio (UTSA)
RP Jin, ZW (corresponding author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing 100190, Peoples R China.
EM jinzhiwei@ict.ac.cn; caojuan@ict.ac.cn; zhyd@ict.ac.cn;
   zhoujs@mail.cnu.edu.cn; qitian@cs.utsa.edu
FU National Nature Science Foundation of China [61525206, 61571424,
   61428207]; Beijing Advanced Innovation Center for Imaging Technology
   [BAICIT-2016009]
FX This work was supported in part by the National Nature Science
   Foundation of China under Grant 61525206, Grant 61571424, and Grant
   61428207, and in part by the Beijing Advanced Innovation Center for
   Imaging Technology under Grant BAICIT-2016009.
CR [Anonymous], 1993, C45 PROGRAMS MACHINE, DOI [DOI 10.1007/BF00993309, 10.1007/bf00993309]
   Bian JW, 2015, IEEE T MULTIMEDIA, V17, P216, DOI 10.1109/TMM.2014.2384912
   Castillo C., 2010, P 1 WORKSH SOC MED A, P71, DOI DOI 10.1145/1964858.1964869
   Castillo C., 2011, P 20 INT C WORLD WID, P675, DOI DOI 10.1145/1963405.1963500
   Cronen-Townsend S., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P299
   Deselaers Thomas., 2009, Proceedings of the ACM international conference on image and video retrieval, P1
   Fang Q, 2015, IEEE T MULTIMEDIA, V17, P2281, DOI 10.1109/TMM.2015.2491019
   Friggeri Adrien, 2014, 8 INT AAAI C WEBL SO
   Gupta A, 2013, PROCEEDINGS OF THE 22ND INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'13 COMPANION), P729
   Gupta MP, 2012, PROCEEDINGS OF THE ASME PACIFIC RIM TECHNICAL CONFERENCE AND EXHIBITION ON PACKAGING AND INTEGRATION OF ELECTRONIC AND PHOTONIC SYSTEMS, MEMS AND NEMS 2011, VOL 2, P153
   Hassan A., 2010, EMNLP 2010, P1245
   Hauff C., 2008, P 17 ACM C INF KNOWL, P439
   He JY, 2008, LECT NOTES COMPUT SC, V4956, P689
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Jensen E. C., 2005, SIGIR 2005. Proceedings of the Twenty-Eighth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P615, DOI 10.1145/1076034.1076155
   Jin Z., 2015, P MEDIAEVAL 2015 MUL, P240
   Jin ZW, 2016, AAAI CONF ARTIF INTE, P2972
   Jin ZW, 2014, IEEE DATA MINING, P230, DOI 10.1109/ICDM.2014.91
   KING B, 1967, J AM STAT ASSOC, V62, P86, DOI 10.2307/2282912
   Kompatsiaris Yiannis, 2015, MediaEval, V3, P7
   KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694
   Kwon S, 2013, IEEE DATA MINING, P1103, DOI 10.1109/ICDM.2013.61
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Morris Meredith Ringel, 2012, P ACM 2012 C COMP SU, P441, DOI DOI 10.1145/2145204.2145274
   Nister D., 2006, IEEE COMP SOC C COMP, P2161, DOI [10.1109/cvpr.2006.264, DOI 10.1109/CVPR.2006.264, 10.1109/CVPR]
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Qazvinian V., 2011, RUMOR HAS IT IDENTIF
   Qian SS, 2016, IEEE T MULTIMEDIA, V18, P233, DOI 10.1109/TMM.2015.2510329
   Shengyun Sun, 2013, Web Technologies and Applications. 15th Asia-Pacific Web Conference, APWeb 2013. Proceedings, P120, DOI 10.1007/978-3-642-37401-2_14
   Tian XM, 2012, IEEE T MULTIMEDIA, V14, P951, DOI 10.1109/TMM.2011.2177647
   Wang M, 2010, IEEE T MULTIMEDIA, V12, P829, DOI 10.1109/TMM.2010.2055045
   Wu B, 2016, AAAI CONF ARTIF INTE, P272
   Wu K, 2015, PROC INT CONF DATA, P651, DOI 10.1109/ICDE.2015.7113322
   Xia ZH, 2016, IEEE T PARALL DISTR, V27, P340, DOI 10.1109/TPDS.2015.2401003
   Yang F., 2012, P ACM SIGKDD WORKSHO, P1
   Yang XS, 2015, IEEE T MULTIMEDIA, V17, P346, DOI 10.1109/TMM.2015.2393635
   Yang Y., 1997, ICML, V97, P412
   Yang YM, 1999, IEEE INTELL SYST APP, V14, P32, DOI 10.1109/5254.784083
   You QZ, 2015, IEEE T MULTIMEDIA, V17, P2271, DOI 10.1109/TMM.2015.2487863
   Zhao Z, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1395, DOI 10.1145/2736277.2741637
   Zhou X, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P983, DOI 10.1145/2740908.2742571
NR 42
TC 222
Z9 244
U1 4
U2 92
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2017
VL 19
IS 3
BP 598
EP 608
DI 10.1109/TMM.2016.2617078
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA EN2VW
UT WOS:000395869400014
DA 2024-07-18
ER

PT J
AU Jin, X
   Dai, QH
AF Jin, Xin
   Dai, Qionghai
TI Clustering-Based Content Adaptive Tiles Under On-chip Memory Constraints
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Adaptive tiles; high efficiency video coding (HEVC); on-chip memory
   constrained video coding; rectangular clustering
ID EFFICIENCY
AB Tiles have been introduced to the next generation video coding standard, high-efficiency video coding (HEVC) standard, as a fundamental tool to reduce on-chip memory requirement during encoding and decoding high-definition video. In this paper, a content adaptive tile partitioning approach is proposed to improve the compression efficiency for HEVC under the on-chip memory constraint. Local competition optimization-based rectangular clustering is proposed to partition the frames into a required number of tiles adapting to content variations. Under the same memory constraint, the adaptive scheme improves compression efficiency by up to 1.8% bitrate saving relative to uniformly spaced tiles with negligible complexity increment. It especially benefits the videos with regional high spatial correlations and no penalty in compression efficiency is observed for other types of videos.
C1 [Jin, Xin] Tsinghua Univ, Grad Sch Shenzhen, Shenzhen Key Lab Broadband Network & Multimedia, Shenzhen 518055, Guangdong, Peoples R China.
   [Dai, Qionghai] Tsinghua Univ, Dept Automat, Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua Shenzhen International Graduate School;
   Tsinghua University
RP Jin, X (corresponding author), Tsinghua Univ, Grad Sch Shenzhen, Shenzhen Key Lab Broadband Network & Multimedia, Shenzhen 518055, Guangdong, Peoples R China.
EM jin.xin@sz.tsinghua.edu.cn; qhdai@tsinghua.edu.cn
RI jin, xin/GQZ-5811-2022; Dai, Qionghai/ABD-5298-2021
OI Dai, Qionghai/0000-0001-7043-3061
FU National Natural Science Foundation of China-Guangdong Joint Foundation
   Key Project [U1201255]; Natural Science Foundation of Guangdong
   Province, China [2014A030313733]
FX This work was supported in part by the National Natural Science
   Foundation of China-Guangdong Joint Foundation Key Project under Grant
   U1201255, and in part by the Natural Science Foundation of Guangdong
   Province, China, under Grant 2014A030313733. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Adrian Munteanu.
CR [Anonymous], JTC1SC29WG11 ISOIEC
   [Anonymous], 2012, METH SUBJ ASS QUAL T
   [Anonymous], 2012, JCTVCK1002
   [Anonymous], 2013, HIGH EFF VID COD
   [Anonymous], 2011, JCTVCF900
   [Anonymous], P DES AUT TEST EUR C
   [Anonymous], 2001, ITU T VCEG M AUST TE
   [Anonymous], MATH PROBL ENG, DOI DOI 10.7873/DATE.2014.232
   Blumenberg C, 2013, PICT COD SYMP, P185, DOI 10.1109/PCS.2013.6737714
   Chi CC, 2012, IEEE T CIRC SYST VID, V22, P1827, DOI 10.1109/TCSVT.2012.2223056
   Cho S, 2015, IEEE T MULTIMEDIA, V17, P778, DOI 10.1109/TMM.2015.2418995
   Fuldseth A., 2011, JCTVCE408
   Fuldseth A, 2012, 2012 PICTURE CODING SYMPOSIUM (PCS), P389, DOI 10.1109/PCS.2012.6213371
   Guo L, 2014, IEEE T MULTIMEDIA, V16, P2323, DOI 10.1109/TMM.2014.2350256
   Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830
   Joint Video Team (JVT) of ISO/IEC MPEG and ITU-T VCEG, 2003, DRAFT ITU T REC FIN
   Sampaio F, 2014, I SYMPOS LOW POWER E, P153, DOI 10.1145/2627369.2627615
   Sampaio F, 2014, ICCAD-IEEE ACM INT, P132, DOI 10.1109/ICCAD.2014.7001343
   Shafique M, 2014, IEEE IMAGE PROC, P1253, DOI 10.1109/ICIP.2014.7025250
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
NR 20
TC 5
Z9 5
U1 0
U2 6
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2331
EP 2344
DI 10.1109/TMM.2016.2600439
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200002
DA 2024-07-18
ER

PT J
AU Li, QH
   Lin, WS
   Xu, JT
   Fang, YM
AF Li, Qiaohong
   Lin, Weisi
   Xu, Jingtao
   Fang, Yuming
TI Blind Image Quality Assessment Using Statistical Structural and
   Luminance Features
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind image quality assessment (BIQA); human visual system (HVS);
   no-reference (NR); structural distortion
ID NATURAL SCENE STATISTICS; TEXTURE-DISCRIMINATION; DISTORTED IMAGES;
   JOINT STATISTICS; CONTRAST; GRADIENT; INFORMATION; DATABASE
AB Blind image quality assessment (BIQA) aims to develop quantitative measures to automatically and accurately estimate perceptual image quality without any prior information about the reference image. In this paper, we introduce a novel BIQA metric by structural and luminance information, based on the characteristics of human visual perception for distorted image. We extract the perceptual structural features of distorted image by the local binary pattern distribution. Besides, the distribution of normalized luminance magnitudes is extracted to represent the luminance changes in distorted image. After extracting the features for structures and luminance, support vector regression is adopted to model the complex nonlinear relationship from feature space to quality measure. The proposed BIQA model is called no-reference quality assessment using statistical structural and luminance features (NRSL). Extensive experiments conducted on four synthetically distorted image databases and three naturally distorted image databases have demonstrated that the proposed NRSL metric compares favorably with the relevant state-of-the-art BIQA models in terms of high correlation with human subjective ratings. The MATLAB source code and validation results of NRSL are publicly online at http://www.ntu.edu.sg/home/wslin/Publications.htm.
C1 [Li, Qiaohong; Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Xu, Jingtao] Beijing Univ Posts & Telecommun, Sch Informat & Commun Engn, Beijing 100876, Peoples R China.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
C3 Nanyang Technological University; Beijing University of Posts &
   Telecommunications; Jiangxi University of Finance & Economics
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM qli013@e.ntu.edu.sg; wslin@ntu.edu.sg; xjt@bupt.edu.cn;
   fa0001ng@e.ntu.edu.sg
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947
FU Institute for Media Innovation, Nanyang Technological University,
   Singapore
FX This work was supported by the Ph.D. Grant from the Institute for Media
   Innovation, Nanyang Technological University, Singapore. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Klara Nahrstedt. (Corresponding author: Yuming
   Fang.)
CR Alakarhu J., 2007, Proc. International Image Sensor Workshop, P1
   Anegekuh L, 2015, IEEE T MULTIMEDIA, V17, P1323, DOI 10.1109/TMM.2015.2444098
   [Anonymous], 2010, Categorical image quality (CSIQ) database
   [Anonymous], 2000, VQEG
   Ciancio A, 2011, IEEE T IMAGE PROCESS, V20, P64, DOI 10.1109/TIP.2010.2053549
   Fang YM, 2015, IEEE SIGNAL PROC LET, V22, P838, DOI 10.1109/LSP.2014.2372333
   Frazor RA, 2006, VISION RES, V46, P1585, DOI 10.1016/j.visres.2005.06.038
   Gaur A, 2014, INT C PATT RECOG, P3410, DOI 10.1109/ICPR.2014.587
   Ghadiyaram D, 2016, IEEE T IMAGE PROCESS, V25, P372, DOI 10.1109/TIP.2015.2500021
   Gu K, 2015, IEEE T MULTIMEDIA, V17, P50, DOI 10.1109/TMM.2014.2373812
   Jayaraman D, 2012, CONF REC ASILOMAR C, P1693, DOI 10.1109/ACSSC.2012.6489321
   Kang L, 2014, PROC CVPR IEEE, P1733, DOI 10.1109/CVPR.2014.224
   Li CF, 2011, IEEE T NEURAL NETWOR, V22, P793, DOI 10.1109/TNN.2011.2120620
   Li QH, 2016, IEEE SIGNAL PROC LET, V23, P541, DOI 10.1109/LSP.2016.2537321
   Li SN, 2011, IEEE T MULTIMEDIA, V13, P935, DOI 10.1109/TMM.2011.2152382
   Lin WS, 2011, J VIS COMMUN IMAGE R, V22, P297, DOI 10.1016/j.jvcir.2011.01.005
   Liu AM, 2012, IEEE T IMAGE PROCESS, V21, P1500, DOI 10.1109/TIP.2011.2175935
   Lyu SW, 2008, PROC CVPR IEEE, P3721
   Ma L, 2011, IEEE T MULTIMEDIA, V13, P824, DOI 10.1109/TMM.2011.2109701
   Mante V, 2005, NAT NEUROSCI, V8, P1690, DOI 10.1038/nn1556
   Mittal A, 2013, IEEE SIGNAL PROC LET, V20, P209, DOI 10.1109/LSP.2012.2227726
   Mittal A, 2012, IEEE T IMAGE PROCESS, V21, P4695, DOI 10.1109/TIP.2012.2214050
   Moorthy AK, 2011, IEEE T IMAGE PROCESS, V20, P3350, DOI 10.1109/TIP.2011.2147325
   Moorthy AK, 2010, IEEE SIGNAL PROC LET, V17, P513, DOI 10.1109/LSP.2010.2043888
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Narwaria M, 2010, IEEE T NEURAL NETWOR, V21, P515, DOI 10.1109/TNN.2010.2040192
   NOTHDURFT HC, 1985, VISION RES, V25, P1957, DOI 10.1016/0042-6989(85)90020-3
   Nuutinen M, 2014, J ELECTRON IMAGING, V23, DOI 10.1117/1.JEI.23.6.061111
   Ojala T, 2002, IEEE T PATTERN ANAL, V24, P971, DOI 10.1109/TPAMI.2002.1017623
   Ojala T, 2001, PATTERN RECOGN, V34, P727, DOI 10.1016/S0031-3203(00)00010-8
   Ong EP, 2003, SEVENTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOL 1, PROCEEDINGS, P469, DOI 10.1109/ISSPA.2003.1224741
   Ponomarenko N, 2015, SIGNAL PROCESS-IMAGE, V30, P57, DOI 10.1016/j.image.2014.10.009
   RUDERMAN DL, 1994, NETWORK-COMP NEURAL, V5, P517, DOI 10.1088/0954-898X/5/4/006
   Saad MA, 2015, IEEE SIGNAL PROC LET, V22, P1516, DOI 10.1109/LSP.2015.2406861
   Saad MA, 2012, IEEE T IMAGE PROCESS, V21, P3339, DOI 10.1109/TIP.2012.2191563
   Sazzad ZMP, 2008, SIGNAL PROCESS-IMAGE, V23, P257, DOI 10.1016/j.image.2008.03.005
   Scholkopf B., 2001, LEARNING KERNELS SUP
   Sheikh HR, 2006, IEEE T IMAGE PROCESS, V15, P430, DOI 10.1109/TIP.2005.859378
   Sheikh HR, 2005, IEEE T IMAGE PROCESS, V14, P1918, DOI 10.1109/TIP.2005.854492
   Sheikh HR, 2005, LIVE IMAGE QUALITY A, DOI DOI 10.1109/CVPR.2015.7298594
   Shen J, 2011, IEEE T IMAGE PROCESS, V20, P2089, DOI 10.1109/TIP.2011.2108661
   Tang HX, 2011, PROC CVPR IEEE, P305, DOI 10.1109/CVPR.2011.5995446
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Virtanen T, 2015, IEEE T IMAGE PROCESS, V24, P390, DOI 10.1109/TIP.2014.2378061
   Wang Z, 2003, CONF REC ASILOMAR C, P1398
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Wang Z, 2002, IEEE IMAGE PROC, P477
   Wang ZJ, 2011, IEEE SIGNAL PROC MAG, V28, P2, DOI 10.1109/MSP.2011.940297
   Watson AB, 1997, J OPT SOC AM A, V14, P2379, DOI 10.1364/JOSAA.14.002379
   Wu JJ, 2013, IEEE T MULTIMEDIA, V15, P1700, DOI 10.1109/TMM.2013.2266093
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P4892, DOI 10.1109/TIP.2013.2279934
   Wu JJ, 2013, IEEE T IMAGE PROCESS, V22, P43, DOI 10.1109/TIP.2012.2214048
   Xue WF, 2014, IEEE T IMAGE PROCESS, V23, P4850, DOI 10.1109/TIP.2014.2355716
   Xue WF, 2013, PROC CVPR IEEE, P995, DOI 10.1109/CVPR.2013.133
   Ye P, 2012, PROC CVPR IEEE, P1098, DOI 10.1109/CVPR.2012.6247789
   Zhai GT, 2008, IEEE T MULTIMEDIA, V10, P1316, DOI 10.1109/TMM.2008.2004910
   Zhang F, 2011, IEEE T MULTIMEDIA, V13, P615, DOI 10.1109/TMM.2011.2134079
   Zhang L, 2015, IEEE T IMAGE PROCESS, V24, DOI 10.1109/TIP.2015.2426416
   Zhang L, 2014, IEEE MULTIMEDIA, V21, P67, DOI 10.1109/MMUL.2014.50
   Zhang L, 2011, IEEE T IMAGE PROCESS, V20, P2378, DOI 10.1109/TIP.2011.2109730
   Zhang M, 2015, IEEE SIGNAL PROC LET, V22, P207, DOI 10.1109/LSP.2014.2326399
   Zhu T, 2014, EURASIP J IMAGE VIDE, DOI 10.1186/1687-5281-2014-5
NR 62
TC 135
Z9 138
U1 1
U2 41
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD DEC
PY 2016
VL 18
IS 12
BP 2457
EP 2469
DI 10.1109/TMM.2016.2601028
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA ED5VK
UT WOS:000388920200012
DA 2024-07-18
ER

PT J
AU Lin, XD
   Liu, JX
   Kang, XG
AF Lin, Xiaodan
   Liu, Jingxian
   Kang, Xiangui
TI Audio Recapture Detection With Convolutional Neural Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Audio recapture detection; convolutional neural network (CNN); electric
   network frequency (ENF)
ID FORENSICS; VIDEO
AB In this paper, we investigate how features can be effectively learned by deep neural networks for audio forensic problems. By providing a preliminary feature preprocessing based on electric network frequency (ENF) analysis, we propose a convolutional neural network (CNN) for training and classification of genuine and recaptured audio recordings. Hierarchical representations which contain levels of details of the ENF components are learned from the deep neural networks and can be used for further classification. The proposed method works for small audio clips of 2 second duration, whereas the state of the art may fail with such small audio clips. Experimental results demonstrate that the proposed network yields high detection accuracy with each ENF harmonic component represented as a single-channel input. The performance can be further improved by a combined input representation which incorporates both the fundamental ENF and its harmonics. The convergence property of the network and the effect of using an analysis window with various sizes are also studied. Performance comparison against the support tensor machine demonstrates the advantage of using CNN for the task of audio recapture detection. Moreover, visualization of the intermediate feature maps provides some insight into what the deep neural networks actually learn and how they make decisions.
C1 [Lin, Xiaodan] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangdong Key Lab Informat Secur, Guangzhou 510006, Guangdong, Peoples R China.
   [Lin, Xiaodan] Huaqiao Univ, Sch Informat Sci & Engn, Xiamen 361021, Peoples R China.
   [Liu, Jingxian; Kang, Xiangui] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Informat Secur, Guangzhou 510006, Guangdong, Peoples R China.
C3 Sun Yat Sen University; Huaqiao University; Sun Yat Sen University
RP Lin, XD (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangdong Key Lab Informat Secur, Guangzhou 510006, Guangdong, Peoples R China.
EM xd_lin@hqu.edu.cn; liujx3@mail2.sysu.edu.cn; isskxg@mail.sysu.edu.cn
RI Kang, Xiangui/AAO-5527-2020
FU NSFC [61379155, U1536204, 61502547, 61332012, 61272453]; NSF of
   Guangdong province [s2013020012788]
FX This work was supported by the NSFC under Grant 61379155, Grant
   U1536204, Grant 61502547, Grant 61332012, and Grant 61272453, and by the
   NSF of Guangdong province under Grant s2013020012788. The associate
   editor coordinating the review of this manuscript and approving it for
   publication was Prof. Jing-Ming Guo.
CR Bottou L., 1991, P NEUR, V91, P12
   Chen JS, 2015, IEEE SIGNAL PROC LET, V22, P1849, DOI 10.1109/LSP.2015.2438008
   Chuang W.-H., 2012, ACM C COMP COMM SEC
   Grigoras C, 2007, FORENSIC SCI INT, V167, P136, DOI 10.1016/j.forsciint.2006.06.033
   Humphrey Eric J, 2012, ICMLA
   Kakar P, 2011, IEEE T MULTIMEDIA, V13, P443, DOI 10.1109/TMM.2011.2121056
   Kang XG, 2011, IEEE T MULTIMEDIA, V13, P181, DOI 10.1109/TMM.2010.2098850
   Kotsia I, 2012, PATTERN RECOGN, V45, P4192, DOI 10.1016/j.patcog.2012.04.033
   Kotsia I, 2011, PROC CVPR IEEE, P633, DOI 10.1109/CVPR.2011.5995663
   LECUN Y, 1989, CONNECTIONISM IN PERSPECTIVE, P143
   Li J, 2015, IEEE T INF FOREN SEC, V10, P507, DOI 10.1109/TIFS.2014.2381872
   METZ CE, 1978, SEMIN NUCL MED, V8, P283, DOI 10.1016/S0001-2998(78)80014-2
   Muammar H, 2013, INT CONF ACOUST SPEE, P2242, DOI 10.1109/ICASSP.2013.6638053
   Nicolalde D. P., 2013, 21 EUR SIGN PROC C M
   Rodríguez DPN, 2010, IEEE T INF FOREN SEC, V5, P534, DOI 10.1109/TIFS.2010.2051270
   ROBINSON T, 1995, INT CONF ACOUST SPEE, P81, DOI 10.1109/ICASSP.1995.479278
   ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Shi Y. Q., 2007, ACM C 07 DALL TEX US
   Stamm MC, 2012, IEEE T INF FOREN SEC, V7, P1315, DOI 10.1109/TIFS.2012.2205568
   Su H, 2013, INT CONF ACOUST SPEE, P3018, DOI 10.1109/ICASSP.2013.6638212
   Swietojanski P, 2014, IEEE SIGNAL PROC LET, V21, P1120, DOI 10.1109/LSP.2014.2325781
   Tabatabaei SAH, 2015, IEEE T MULTIMEDIA, V17, P945, DOI 10.1109/TMM.2015.2432672
   Thongkamwitoon T, 2015, IEEE T INF FOREN SEC, V10, P953, DOI 10.1109/TIFS.2015.2392566
   Tian YH, 2013, IEEE MULTIMEDIA, V20, P72, DOI 10.1109/MMUL.2012.62
   Yu H, 2008, IEEE IMAGE PROC, P3140, DOI 10.1109/ICIP.2008.4712461
NR 26
TC 35
Z9 39
U1 3
U2 38
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2016
VL 18
IS 8
BP 1480
EP 1487
DI 10.1109/TMM.2016.2571999
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR5YJ
UT WOS:000379978000003
DA 2024-07-18
ER

PT J
AU Yan, JJ
   Zheng, WM
   Xu, QY
   Lu, GM
   Li, HB
   Wang, B
AF Yan, Jingjie
   Zheng, Wenming
   Xu, Qinyu
   Lu, Guanming
   Li, Haibo
   Wang, Bei
TI Sparse Kernel Reduced-Rank Regression for Bimodal Emotion Recognition
   From Facial Expression and Speech
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Bimodal emotion recognition; facial expression; feature fusion; sparse
   kernel reduced-rank regression (SKRRR); speech
ID PHENOTYPES; FRAMEWORK; FUSION; FACE
AB A novel bimodal emotion recognition approach from facial expression and speech based on the sparse kernel reduced-rank regression (SKRRR) fusion method is proposed in this paper. In this method, we use the openSMILE feature extractor and the scale invariant feature transform feature descriptor to respectively extract effective features from speech modality and facial expression modality, and then propose the SKRRR fusion approach to fuse the emotion features of two modalities. The proposed SKRRR method is a nonlinear extension of the traditional reduced-rank regression (RRR), where both predictor and response feature vectors in RRR are kernelized by being mapped onto two high-dimensional feature space via two nonlinear mappings, respectively. To solve the SKRRR problem, we propose a sparse representation (SR)-based approach to find the optimal solution of the coefficient matrices of SKRRR, where the introduction of the SR technique aims to fully consider the different contributions of training data samples to the derivation of optimal solution of SKRRR. Finally, we utilize the eNTERFACE '05 and AFEW4.0 bimodal emotion database to conduct the experiments of monomodal emotion recognition and bimodal emotion recognition, and the results indicate that our presented approach acquires the highest or comparable bimodal emotion recognition rate among some state-of-the-art approaches.
C1 [Yan, Jingjie; Xu, Qinyu; Lu, Guanming; Li, Haibo] Nanjing Univ Posts & Telecommun, Jiangsu Prov Key Lab Image Proc & Image Commun, Coll Telecomm & Informat Engn, Nanjing 210003, Peoples R China.
   [Li, Haibo] Royal Inst Technol, Sch Comp Sci & Commun, S-11428 Stockholm, Sweden.
   [Zheng, Wenming; Wang, Bei] Southeast Univ, Key Lab Child Dev & Learning Sci, Minist Educ, Res Ctr Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; Royal Institute of
   Technology; Southeast University - China
RP Zheng, WM (corresponding author), Southeast Univ, Key Lab Child Dev & Learning Sci, Minist Educ, Res Ctr Learning Sci, Nanjing 210096, Jiangsu, Peoples R China.
EM yanjingjie@njupt.edu.cn; wenming_zheng@seu.edu.cn; 1483973487@qq.com;
   lugm@njupt.edu.cn; lihb@njupt.edu.cn; wangbei198756@163.com
FU National Basic Research Program of China [2015CB351704]; National
   Natural Science Foundation of China [61231002, 61501249]; Natural
   Science Foundation of Jiangsu Province [BK20150855, BK20130020]; Ph.D.
   Program Foundation of the Ministry Education of China [20120092110054];
   Natural Science Foundation for Jiangsu Higher Education Institutions
   [15KJB510022]; NUPTSF [NY214143]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2015CB351704, in part by the National Natural
   Science Foundation of China under Grant 61231002 and Grant 61501249, in
   part by the Natural Science Foundation of Jiangsu Province under Grant
   BK20150855 and Grant BK20130020, in part by the Ph.D. Program Foundation
   of the Ministry Education of China under Grant 20120092110054, in part
   by the Natural Science Foundation for Jiangsu Higher Education
   Institutions under Grant 15KJB510022, and in part by the NUPTSF under
   Grant NY214143. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. K. Selcuk Candan.
   (Corresponding author: Wenming Zheng.)
CR ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580
   [Anonymous], 2007, P BMVC WARW UK 10 13
   [Anonymous], 2014, P 16 INT C MULT INT
   [Anonymous], HUM CENTR COMP HUMAN
   [Anonymous], 2015, P 5 INT WORKSH AUD V, DOI DOI 10.1145/2808196.2811642
   [Anonymous], 2006, 22 INT C DATA ENG WO
   [Anonymous], SPARSE CANONICAL COR
   [Anonymous], 2013, MULTIMEDIA EXPO ICME
   Cen L, 2008, SEVENTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P859, DOI 10.1109/ICMLA.2008.85
   Chen J., 2014, P 16 INT C MULT INT, P508, DOI DOI 10.1145/2663204.2666277
   Chen K, 2012, J ROY STAT SOC B, V74, P203, DOI 10.1111/j.1467-9868.2011.01002.x
   Chen LS, 2012, J AM STAT ASSOC, V107, P1533, DOI 10.1080/01621459.2012.734178
   Chen Y, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P1316
   Chu D., 2013, PROC INT MULTICONF E, V2202, P322
   Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467
   Cummins N., 2013, P 3 ACM INT WORKSH A, P11
   Datcu Dragos., 2011, P 12 INT C COMP SYST, P122
   De la Torre F, 2012, IEEE T PATTERN ANAL, V34, P1041, DOI 10.1109/TPAMI.2011.184
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Dhall A., 2014, PROC ICMI, P461, DOI DOI 10.1145/2663204.2666275
   Dobrisek S, 2013, INT J ADV ROBOT SYST, V10, DOI 10.5772/54002
   El Ayadi M, 2011, PATTERN RECOGN, V44, P572, DOI 10.1016/j.patcog.2010.09.020
   Eyben Florian, 2010, P 18 ACM INT C MULT, P1459
   Feng T., 2013, THESIS
   Gajsek Rok, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P4133, DOI 10.1109/ICPR.2010.1005
   Gunes Hatice, 2008, Affective Computing. Focus on Emotion Expression, Synthesis and Recognition, P185
   Gunes H, 2009, ADV INFORM KNOWL PRO, P161, DOI 10.1007/978-1-84800-346-0_10
   He L, 2015, INT CONF AFFECT, P260, DOI 10.1109/ACII.2015.7344581
   Huang D, 2012, LECT NOTES COMPUT SC, V7575, P616, DOI 10.1007/978-3-642-33765-9_44
   Huang D, 2010, LECT NOTES COMPUT SC, V6312, P364, DOI 10.1007/978-3-642-15552-9_27
   Kim Y, 2013, INT CONF ACOUST SPEE, P3687, DOI 10.1109/ICASSP.2013.6638346
   Kun Lu, 2013, 2013 Seventh International Conference on Image and Graphics (ICIG), P515, DOI 10.1109/ICIG.2013.109
   Lin JC, 2013, 1ST INTERNATIONAL CONFERENCE ON ORANGE TECHNOLOGIES (ICOT 2013), P278, DOI 10.1109/ICOT.2013.6521212
   Lin Z., 2009, UILUENG092215 DEP EL
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Mangai UG, 2010, IETE TECH REV, V27, P293, DOI 10.4103/0256-4602.64604
   Meghjani Malika., 2009, Applications of Computer Vision (WACV), 2009 Workshop on, P1
   Metallinou A, 2008, IEEE INT SYM MULTIM, P250, DOI 10.1109/ISM.2008.40
   Mukherjee Ashin, 2011, Stat Anal Data Min, V4, P612, DOI 10.1002/sam.10138
   Nicolaou MA, 2014, INT CONF ACOUST SPEE, DOI 10.1109/ICASSP.2014.6853852
   Paleari Marco, 2008, 2008 International Workshop on Content-based Multimedia Indexing - CBMI 2008, P425, DOI 10.1109/CBMI.2008.4564978
   Parkhomenko E, 2009, STAT APPL GENET MOL, V8, DOI 10.2202/1544-6115.1406
   Poria S, 2015, NEURAL NETWORKS, V63, P104, DOI 10.1016/j.neunet.2014.10.005
   Qiu W, 2011, THESIS
   Silver M, 2012, NEUROIMAGE, V63, P1681, DOI 10.1016/j.neuroimage.2012.08.002
   Sun Bo., 2014, Proceedings of the 16th International Conference on Multimodal Interaction. ACM, P481
   Vounou M, 2010, NEUROIMAGE, V53, P1147, DOI 10.1016/j.neuroimage.2010.07.002
   Wang B., 2014, INFORM RES, V40, P48
   Wang B. A., 2014, Laboratory investigation on influences of three polymers to foam stability at elevated temperature
   Wang YJ, 2012, IEEE T MULTIMEDIA, V14, P597, DOI 10.1109/TMM.2012.2189550
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xiaoqing Liu, 2011, 2011 11th IEEE International Conference on Advanced Learning Technologies (ICALT 2011), P63, DOI 10.1109/ICALT.2011.26
   Yan JJ, 2014, IEICE T FUND ELECTR, VE97A, P1650, DOI 10.1587/transfun.E97.A.1650
   Yan JJ, 2014, IEICE T INF SYST, VE97D, P610, DOI 10.1587/transinf.E97.D.610
   Yan JJ, 2013, ARCH ACOUST, V38, P465, DOI 10.2478/aoa-2013-0055
   [闫静杰 Yan Jingjie], 2013, [中国图象图形学报, Journal of Image and Graphics], V18, P1101
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
   Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52
   Zeng ZH, 2007, IEEE T MULTIMEDIA, V9, P424, DOI 10.1109/TMM.2006.886310
   Zeng ZH, 2009, AFFECTIVE INFORMATION PROCESSING, P241, DOI 10.1007/978-1-84800-306-4_14
   Zhalehpour S, 2014, 2014 IEEE INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND APPLICATIONS (INISTA 2014), P116, DOI 10.1109/INISTA.2014.6873606
   Zhang X, 2012, IEEE T MULTIMEDIA, V14, P995, DOI 10.1109/TMM.2012.2186121
   Zheng WM, 2014, IEEE T AFFECT COMPUT, V5, P71, DOI 10.1109/TAFFC.2014.2304712
   Zheng WM, 2014, IEEE SIGNAL PROC LET, V21, P569, DOI 10.1109/LSP.2014.2308954
   Zheng WM, 2012, INT C PATT RECOG, P1972
NR 65
TC 68
Z9 75
U1 4
U2 55
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2016
VL 18
IS 7
BP 1319
EP 1329
DI 10.1109/TMM.2016.2557721
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DR2RW
UT WOS:000379752600008
DA 2024-07-18
ER

PT J
AU Shen, WW
   Fan, YB
   Bai, YF
   Huang, LL
   Shang, Q
   Liu, C
   Zeng, XY
AF Shen, Weiwei
   Fan, Yibo
   Bai, Yufeng
   Huang, Leilei
   Shang, Qing
   Liu, Cong
   Zeng, Xiaoyang
TI A Combined Deblocking Filter and SAO Hardware Architecture for HEVC
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Deblocking filter (DF); hardware implementation; high-efficiency video
   coding (HEVC); sample adaptive offset (SAO); UHD
ID H.264/AVC; CYCLES/MB; STANDARD
AB The latest video coding standard high-efficiency video coding (HEVC) provides 50% improvement in coding efficiency compared to H.264/AVC to meet the rising demands for video streaming, better video quality, and higher resolution. The deblocking filter (DF) and sample adaptive offset (SAO) play an important role in the HEVC encoder, and the SAO is newly adopted in HEVC. Due to the high throughput requirement in the video encoder, design challenges such as data dependence, external memory traffic, and on-chip memory area become even more critical. To solve these problems, we first propose an interlacing memory organization on the basis of quarter-LCU to resolve the data dependence between vertical and horizontal filtering of DF. The on-chip SRAM area is also reduced to about 25% on the basis of quarter-LCU scheme without throughput loss. We also propose a simplified bitrate estimation method of rate-distortion cost calculation to reduce the computational complexity in the mode decision of SAO. Our proposed hardware architecture of combined DF and SAO is designed for the HEVC intraencoder, and the proposed simplified bitrate estimation method of SAO can be applied to both intra-and intercoding. As a result, our design can support ultrahigh definition 7680x4320 at 40 f/s applications at merely 182 MHz working frequency. Total logic gate count is 103.3 K in 65 nm CMOS process.
C1 [Shen, Weiwei; Fan, Yibo; Bai, Yufeng; Huang, Leilei; Shang, Qing; Liu, Cong; Zeng, Xiaoyang] Fudan Univ, State Key Lab ASIC & Syst, Shanghai 201203, Peoples R China.
C3 Fudan University
RP Shen, WW; Fan, YB; Bai, YF; Huang, LL; Shang, Q; Liu, C; Zeng, XY (corresponding author), Fudan Univ, State Key Lab ASIC & Syst, Shanghai 201203, Peoples R China.
EM 10110720024@fudan.edu.cn; fanyibo@fudan.edu.cn;
   12212020001@fudan.edu.cn; 10300720005@fudan.edu.cn;
   11212020039@fudan.edu.cn; 11212020033@fudan.edu.cn; xyzeng@fudan.edu.cn
RI Liu, Cong/GLT-5439-2022; HUANG, LING/HTR-1819-2023; fan,
   yi/GYU-1036-2022; Huang, Li/IUQ-0909-2023; huang, lei/GQP-8739-2022
FU National Natural Science Foundation of China [61306023]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 61306023. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Dr. Shahram Shirani.
CR [Anonymous], 2001, SC16Q6 ITUT
   [Anonymous], 2012, JCTVCH1101
   [Anonymous], 2012, JCTVCI1002
   [Anonymous], 2013, HM REFERENCE SOFTWAR
   [Anonymous], 2012, JCTVCK1003
   Bossen F, 2012, IEEE T CIRC SYST VID, V22, P1685, DOI 10.1109/TCSVT.2012.2221255
   CHIEN CA, 2008, P IEEE AS PAC C CIRC, P312
   Fu C.-M., 2011, PROC ITU T SG16 WP3, P1
   Fu CM, 2012, IEEE T CIRC SYST VID, V22, P1755, DOI 10.1109/TCSVT.2012.2221529
   Kay S. M., 1993, FUNDAMENTALS STAT SI, V1
   Lin YC, 2009, IEEE T VLSI SYST, V17, P838, DOI 10.1109/TVLSI.2008.2008456
   Liu TM, 2007, IEEE T CIRC SYST VID, V17, P937, DOI 10.1109/TCSVT.2007.897467
   Nadeem M, 2009, 2009 IEEE/ACM/IFIP 7TH WORKSHOP ON EMBEDDED SYSTEMS FOR REAL-TIME MULTIMEDIA, P18, DOI 10.1109/ESTMED.2009.5336814
   Norkin A, 2012, IEEE T CIRC SYST VID, V22, P1746, DOI 10.1109/TCSVT.2012.2223053
   Ozcan E, 2013, IEEE T CONSUM ELECTR, V59, P714, DOI 10.1109/TCE.2013.6626260
   Pourazad MT, 2012, IEEE CONSUM ELECTR M, V1, P36, DOI 10.1109/MCE.2012.2192754
   Rosa Vagner, 2010, Proceedings of the 2010 17th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2010), P383, DOI 10.1109/ICECS.2010.5724533
   Song R., 2012, P AS PAC SIGN ING PR, P1
   Ta N, 2009, ICECT: 2009 INTERNATIONAL CONFERENCE ON ELECTRONIC COMPUTER TECHNOLOGY, PROCEEDINGS, P627, DOI 10.1109/ICECT.2009.109
   Xu K, 2008, IEEE T CIRC SYST VID, V18, P363, DOI 10.1109/TCSVT.2008.918437
   Ye X., 2014, P IEEE VIS COMM IM P, P209
   Yuen M, 1998, SIGNAL PROCESS, V70, P247, DOI 10.1016/S0165-1684(98)00128-5
   Zhou JJ, 2009, IEEE INT CON MULTI, P1134, DOI 10.1109/ICME.2009.5202699
   Zhu JY, 2013, IEEE IMAGE PROC, P1967, DOI 10.1109/ICIP.2013.6738405
NR 24
TC 27
Z9 28
U1 0
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2016
VL 18
IS 6
BP 1022
EP 1033
DI 10.1109/TMM.2016.2532606
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA DM1LP
UT WOS:000376107100007
DA 2024-07-18
ER

PT J
AU Hu, WM
   Ding, XM
   Li, B
   Wang, JC
   Gao, Y
   Wang, FS
   Maybank, S
AF Hu, Weiming
   Ding, Xinmiao
   Li, Bing
   Wang, Jianchao
   Gao, Yan
   Wang, Fangshi
   Maybank, Stephen
TI Multi-Perspective Cost-Sensitive Context-Aware Multi-Instance Sparse
   Coding and Its Application to Sensitive Video Recognition
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cost-sensitive context-aware multi-instance sparse coding (MI-SC);
   horror video recognition; multi-perspective multi-instance joint sparse
   coding (MI-J-SC); video emotional feature extraction; violent video
   recognition
ID VIOLENCE DETECTION; COLOR PREFERENCE; CATEGORIZATION; AUDIO;
   REPRESENTATION; CLASSIFICATION; INFORMATION; CHILDHOOD; EMOTION
AB With the development of video-sharing websites, P2P, micro-blog, mobile WAP websites, and so on, sensitive videos can be more easily accessed. Effective sensitive video recognition is necessary for web content security. Among web sensitive videos, this paper focuses on violent and horror videos. Based on color emotion and color harmony theories, we extract visual emotional features from videos. A video is viewed as a bag and each shot in the video is represented by a key frame which is treated as an instance in the bag. Then, we combine multi-instance learning (MIL) with sparse coding to recognize violent and horror videos. The resulting MIL-based model can be updated online to adapt to changing web environments. We propose a cost-sensitive context-aware multi-instance sparse coding (MI-SC) method, in which the contextual structure of the key frames is modeled using a graph, and fusion between audio and visual features is carried out by extending the classic sparse coding into cost-sensitive sparse coding. We then propose a multi-perspective multi-instance joint sparse coding (MI-J-SC) method that handles each bag of instances from an independent perspective, a contextual perspective, and a holistic perspective. The experiments demonstrate that the features with an emotional meaning are effective for violent and horror video recognition, and our cost-sensitive context-aware MI-SC and multi-perspective MI-J-SC methods outperform the traditional MIL methods and the traditional SVM and KNN-based methods.
C1 [Hu, Weiming; Li, Bing; Wang, Jianchao; Gao, Yan] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Ding, Xinmiao] Shandong Inst Business & Technol, Yantai 264005, Peoples R China.
   [Wang, Fangshi] Beijing Jiaotong Uni, Sch Software Engn, Beijing 100044, Peoples R China.
   [Maybank, Stephen] Birkbeck Coll, Dept Comp Sci & Informat Syst, London WC1E 7HX, England.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Shandong
   Technology & Business University; University of London; Birkbeck
   University London
RP Hu, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
EM wmhu@nlpr.ia.ac.cn; dingxinmiao@126.com; bli@nlpr.ia.ac.cn;
   jianchao1030@163.com; 870327884@qq.com; fshwang@bjtu.edu.cn;
   sjmaybank@dcs.bbk.ac.uk
RI Li, Bing/AAX-5919-2021
FU 973 Basic Research Program of China [2014CB349303]; Natural Science
   Foundation of China [61472421, 61303086]; CAS Center for Excellence in
   Brain Science and Intelligence Technology; Guangdong Natural Science
   Foundation [S2012020011081]
FX This work was supported in part by the 973 Basic Research Program of
   China under Grant 2014CB349303, in part by the Natural Science
   Foundation of China under Grant 61472421 and Grant 61303086, in part by
   the CAS Center for Excellence in Brain Science and Intelligence
   Technology, and in part by the Guangdong Natural Science Foundation
   under Grant S2012020011081.
CR Acar E, 2013, INT WORK CONTENT MUL, P73, DOI 10.1109/CBMI.2013.6576556
   Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2015, PRODUKTION LOGISTIK
   [Anonymous], P INT C ADV MULT MOD
   [Anonymous], P MEDIAEVAL 2014 WOR
   [Anonymous], P ADV NEUR INF PROC
   [Anonymous], P MEDIAEVAL 2014 WOR
   [Anonymous], INT C AC SPEECH SIGN
   [Anonymous], P MEDIAEVAL 2014 WOR
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Cerneková Z, 2006, IEEE T CIRC SYST VID, V16, P82, DOI 10.1109/TCSVT.2005.856896
   Chen YX, 2006, IEEE T PATTERN ANAL, V28, P1931, DOI 10.1109/TPAMI.2006.248
   Chen YX, 2004, J MACH LEARN RES, V5, P913
   Cheng W.-H., 2003, P 5 ACM SIGMM INT WO, P109, DOI DOI 10.1145/973264.973282
   Datta A, 2002, INT C PATT RECOG, P433, DOI 10.1109/ICPR.2002.1044748
   Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3
   Ding X., 2012, P AS C COMP VIS, P599
   Endeshaw T, 2008, IEEE APP IMG PAT, P13
   Field AP, 2003, BEHAV RES THER, V41, P1277, DOI 10.1016/S0005-7967(03)00034-2
   Gartner T., 2002, P 9 INT C MACH LEARN, P179
   Geng J, 2015, IEEE T MULTIMEDIA, V17, P498, DOI 10.1109/TMM.2015.2398195
   Geusebroek JM, 2005, INT J COMPUT VISION, V62, P7, DOI 10.1007/s11263-005-4632-7
   Giannakopoulos T, 2007, 2007 IEEE NINTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P90, DOI 10.1109/MMSP.2007.4412825
   Giannakopoulos T, 2006, LECT NOTES COMPUT SC, V3955, P502
   Giannakopoulos T, 2010, LECT NOTES ARTIF INT, V6040, P91, DOI 10.1007/978-3-642-12842-4_13
   Hanjalic A, 2005, IEEE T MULTIMEDIA, V7, P143, DOI 10.1109/TMM.2004.840618
   Hyun-Don Kim, 2013, 2013 IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN), P412, DOI 10.1109/ROMAN.2013.6628514
   Jansohn C., 2009, PROC 17 ACM INT C MU, P601, DOI [DOI 10.1145/1631272.1631366, 10.1145/1631272.1631366]
   Kang CC, 2015, IEEE T MULTIMEDIA, V17, P370, DOI 10.1109/TMM.2015.2390499
   Kang H.-B., 2003, Proceedings of the 11th ACM International Conference on Multimedia, P259
   King NJ, 1998, BEHAV RES THER, V36, P297, DOI 10.1016/S0005-7967(98)00015-1
   Lee PY, 2005, IEEE T MULTIMEDIA, V7, P1183, DOI 10.1109/TMM.2005.858414
   Lee S, 2009, IEEE T CONSUM ELECTR, V55, P677, DOI 10.1109/TCE.2009.5174439
   Li M, 2010, PROC CVPR IEEE, P1395, DOI 10.1109/CVPR.2010.5539805
   Lin JA, 2009, LECT NOTES COMPUT SC, V5879, P930
   Liu J, 2009, SIMUL: 2009 FIRST INTERNATIONAL CONFERENCE ON ADVANCES IN SYSTEM SIMULATION, P1, DOI 10.1109/SIMUL.2009.24
   Liu YZ, 2011, IEEE INT CONF TRUST, P1488, DOI 10.1109/TrustCom.2011.205
   Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691
   Ma Z, 2013, IEEE T MULTIMEDIA, V15, P1628, DOI 10.1109/TMM.2013.2264928
   Maron O, 1998, ADV NEUR IN, V10, P570
   Nam J, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P353, DOI 10.1109/ICIP.1998.723496
   Nie F., 2010, ADV NEURAL INFORM PR, P1813
   Ou LC, 2006, COLOR RES APPL, V31, P191, DOI 10.1002/col.20208
   Ou LC, 2004, COLOR RES APPL, V29, P381, DOI 10.1002/col.20047
   Ou LC, 2004, COLOR RES APPL, V29, P232, DOI 10.1002/col.20010
   Rasheed Z, 2005, IEEE T CIRC SYST VID, V15, P52, DOI 10.1109/TCSVT.2004.839993
   Smeaton A. F., 2006, P 8 ACM INT WORKSHOP, P231
   Soleymani M, 2014, IEEE T MULTIMEDIA, V16, P1075, DOI 10.1109/TMM.2014.2305573
   Tekin C, 2015, IEEE T MULTIMEDIA, V17, P549, DOI 10.1109/TMM.2015.2403234
   Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wang H.Y., 2008, PROCEEDING 25 INT C, P1136
   Wang HL, 2006, IEEE T CIRC SYST VID, V16, P689, DOI 10.1109/TCSVT.2006.873781
   Wang J, 2000, P 17 INT C MACH LEAR, P1119
   Wang KY, 2012, COMM COM INF SC, V321, P137
   Winn J, 2005, IEEE I CONF COMP VIS, P1800
   Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79
   Xu M, 2005, 2005 IEEE International Conference on Multimedia and Expo (ICME), Vols 1 and 2, P622
   Yuan XT, 2010, PROC CVPR IEEE, P3493, DOI 10.1109/CVPR.2010.5539967
   Zhang Q, 2002, ADV NEUR IN, V14, P1073
   Zhou ZH., 2007, ICML '07, V227, P1167
   Zhou Zhi-Hua, 2009, PROC ANN INT C MACH, V382, P1249
   Zhu S, 2000, IEEE T IMAGE PROCESS, V9, P287, DOI 10.1109/83.821744
NR 63
TC 16
Z9 18
U1 0
U2 28
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JAN
PY 2016
VL 18
IS 1
BP 76
EP 89
DI 10.1109/TMM.2015.2496372
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CZ5JW
UT WOS:000367139700008
OA Green Accepted
DA 2024-07-18
ER

PT J
AU Hong, RC
   Hu, ZZ
   Liu, LQ
   Wang, M
   Yan, SC
   Tian, Q
AF Hong, Richang
   Hu, Zhenzhen
   Liu, Luoqi
   Wang, Meng
   Yan, Shuicheng
   Tian, Qi
TI Understanding Blooming Human Groups in Social Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Convolutional neural networks (CNNs); human group categorization
ID VISUAL KNOWLEDGE
AB Human group, which indicates the people who share similar characteristics, is used to categorize humans into distinct populations or groups. In recent years, with the explosive growth of image, new concepts of human group are blooming in social networks. People in the same human group can be categorized by their facial and clothes appearance characteristics. In this work, we propose an approach to understanding the new concepts of human group with few positive samples. To this end, we construct visual models crossing two modalities related to human images and surrounding texts. Two convolutional neural networks based on face and upper body are constructed separately. Two different convolutional neural networks (CNNs) architectures are explored for visual pre-traing. To assist the human group recognition, we also merge global convolutional feature of the image. The surrounding texts are represented by semantical vectors and utilized as image labels. We transform words in the text into fixed length vectors by the skip-gram model. Then the texts corresponding to each image are converted into one feature vector by sparse coding and max pooling. Given a few positive samples of new concepts of human group, the visual model can be improved to understand the semantical meaning of the new label. The experimental results demonstrate the effectiveness of the proposed visual model and show the excellent learning capacity with few samples.
C1 [Hong, Richang; Hu, Zhenzhen] Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
   [Liu, Luoqi; Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 11758, Singapore.
   [Wang, Meng] Hefei Univ Technol, Chinese Acad China, Chongqing Inst Green & Intelligent Technol, Hefei 230009, Peoples R China.
   [Tian, Qi] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Hefei University of Technology; National University of Singapore; Hefei
   University of Technology; Chinese Academy of Sciences; Chongqing
   Institute of Green & Intelligent Technology, CAS; University of Texas
   System; University of Texas at San Antonio (UTSA)
RP Hong, RC (corresponding author), Hefei Univ Technol, Sch Comp & Informat, Hefei 230009, Peoples R China.
EM hongrc@hfut.edu.cn; huzhen.ice@gmail.com; liuluoqi@nus.edu.sg;
   eric.mengwang@gmail.com; eleyans@nus.edu.sg; qi.tian@utsa.edu
RI Wang, Meng/ITR-8699-2023; Yan, Shuicheng/HCI-1431-2022
OI Hu, Zhenzhen/0000-0003-1042-8361
FU National High Technology Research and Development Project
   [2014AA015104]; National Basic Research Program [2013CB329603]; New
   Century Excellent Talents Grant [NCET-13-0764]; NSFC [61472116]; NSEC
   [61429201]; ARO [W911NF-15-1-0290, W911NF-12-1-0057]; NEC Laboratories
   of America Faculty Research Awards
FX This work was supported in part by the National High Technology Research
   and Development Project under Grant 2014AA015104, by the National Basic
   Research Program under Grant 2013CB329603, by the New Century Excellent
   Talents Grant NCET-13-0764, by the NSFC Grant 61472116, and by the NSEC
   Grant 61429201. The work of Q. Tian was supported in part by the ARO
   under Grant W911NF-15-1-0290 and Grant W911NF-12-1-0057, and by the NEC
   Laboratories of America Faculty Research Awards.
CR [Anonymous], 2013, P 31 INT C MACHINE L
   [Anonymous], 2008, P 16 ACM INT C MULT, DOI [DOI 10.1145/1459359.1459470, DOI 10.1145/1459359.1459470.11.P]
   [Anonymous], 2012, 2012 IEEE COMPUTER S
   [Anonymous], 2013, CORR
   [Anonymous], 2008, P 2008 23 INT S COMP
   [Anonymous], 2014, PROC IEEE C COMPUTER, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]
   [Anonymous], 2014, ARXIV14065726
   [Anonymous], 2013, CoRR
   [Anonymous], 2007, 0749 U MASS
   Bourdev L, 2011, IEEE I CONF COMP VIS, P1543, DOI 10.1109/ICCV.2011.6126413
   Chen HZ, 2012, LECT NOTES COMPUT SC, V7574, P609, DOI 10.1007/978-3-642-33712-3_44
   Chen XL, 2014, PROC CVPR IEEE, P2035, DOI 10.1109/CVPR.2014.261
   Chen XL, 2013, IEEE I CONF COMP VIS, P1409, DOI 10.1109/ICCV.2013.178
   Choon-Boon Ng, 2013, Advances in Neural Networks - ISNN 2013. 10th International Symposium on Neural Networks. Proceedings: LNCS 7951, P558, DOI 10.1007/978-3-642-39065-4_67
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Gao F, 2009, LECT NOTES COMPUT SC, V5558, P132
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Krizhevsky A., 2012, NIPS, V1, P4
   Kwak I. S., 2013, BRIT MACH VIS C, P951
   Liao S., 2014, ARXIV PREPRINT ARXIV
   Perona P, 2010, P IEEE, V98, P1526, DOI 10.1109/JPROC.2010.2049621
   Qi GJ, 2012, P IEEE, V100, P2688, DOI 10.1109/JPROC.2012.2201909
   Qi GJ, 2012, IEEE T PATTERN ANAL, V34, P850, DOI 10.1109/TPAMI.2011.191
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Rothwell N. V., 1988, UNDERSTANDING GENETI
   Shao M, 2013, IEEE I CONF COMP VIS, P3631, DOI 10.1109/ICCV.2013.451
   Tao D., INF SCI IN PRESS
   Tao D., IEEE T CYBE IN PRESS
   Xu C, 2015, IEEE T PATTERN ANAL, V37, P2531, DOI 10.1109/TPAMI.2015.2417578
   Xu C, 2014, IEEE T PATTERN ANAL, V36, P1559, DOI 10.1109/TPAMI.2013.2296528
   Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741
NR 31
TC 24
Z9 24
U1 1
U2 16
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2015
VL 17
IS 11
SI SI
BP 1980
EP 1988
DI 10.1109/TMM.2015.2476657
PG 9
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CV2QV
UT WOS:000364102400011
DA 2024-07-18
ER

PT J
AU Chen, J
   Jin, Q
   Bao, SH
   Su, Z
   Chen, SM
   Yu, Y
AF Chen, Jia
   Jin, Qin
   Bao, Shenghua
   Su, Zhong
   Chen, Shimin
   Yu, Yong
TI Exploitation and Exploration Balanced Hierarchical Summary for Landmark
   Images
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content-based retrieval
ID COLLECTIONS; SCENE
AB While we have made significant progress over image understanding and search, how to meet the ultimate goal of satisfying both exploration and exploitation in one single system is still an open challenge. In the context of landmark images, it means that a system should not only be able to help users to quickly locate the photo they are interested in (exploitation), but also to discover different parts of the landmark which have never been seen before (exploration), which is a common request as evidenced by many recent multimedia studies. To the best of our knowledge, existing systems mainly focus on either exploration (e.g., photo browsing) or exploitation (e.g., representative photo identification), while users' need of exploration and exploitation is dynamically mixed. In this paper, we tackle the challenge by organizing landmark images into a hierarchical summary which gives user the flexibility of conducting both exploration and exploitation. In the hierarchical summary construction, we introduce two principles: the coherence principle and the diversity principle. Behind these two principles, the intrinsic concept is "detail-level," which measures how much detail that an image reflects for a certain landmark. A new objective function is derived from the definition of both exploration and exploitation experience on detail-level. The problem of finding an optimal hierarchical summary is formulated as searching over a space of trees for the one that achieves the best objective score. Extensive quantitative experimental results and comprehensive user studies show that the optimized hierarchical summary is able to satisfy both experiences simultaneously.
C1 [Chen, Jia; Yu, Yong] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Jin, Qin] Renmin Univ China, Sch Informat, Beijing 100872, Peoples R China.
   [Bao, Shenghua] IBM Watson Grp, San Jose, CA 95120 USA.
   [Su, Zhong] IBM China Res Lab, Beijing 100193, Peoples R China.
   [Chen, Shimin] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
C3 Shanghai Jiao Tong University; Renmin University of China; International
   Business Machines (IBM); Chinese Academy of Sciences; Institute of
   Computing Technology, CAS
RP Chen, J (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM fchenjia@apex.sjtu.edu.cn; qjin@ruc.edu.cn; baoshhua@us.ibm.com;
   suzhong@cn.ibm.com; chensm@ict.ac.cn; yyug@apex.sjtu.edu.cn
FU Fundamental Research Funds for the Central Universities; Renmin
   University of China [14XNLQ01]; Beijing Natural Science Foundation
   [4142029]; NSFC [61303184]; Scientific Research Foundation for the
   Returned Overseas Chinese Scholars, State Education Ministry
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities and the Research Funds of Renmin University of
   China under Grant 14XNLQ01, in part by the Beijing Natural Science
   Foundation under Grant 4142029, in part by the NSFC under Grant
   61303184, and in part by the Scientific Research Foundation for the
   Returned Overseas Chinese Scholars, State Education Ministry. The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Sheng-Wei Chen.
CR Agarwal S, 2009, IEEE I CONF COMP VIS, P72, DOI 10.1109/ICCV.2009.5459148
   [Anonymous], 2007, OXF BUILD DAT
   [Anonymous], 2008, P 17 INT C WORLD WID
   [Anonymous], ACM MULTIMEDIA, DOI DOI 10.1145/1027527.1027729
   [Anonymous], P ACM INT C MULT NAR
   Avrithis Yannis, 2010, P 18 ACM INT C MULTI, P153, DOI 10.1145/1873951.1873973Place
   Boyd S., 2004, CONVEX OPTIMIZATION
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chum O, 2008, IEEE T PATTERN ANAL, V30, P1472, DOI 10.1109/TPAMI.2007.70787
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800
   Golub G.H., 1989, MATRIX COMPUTATIONS
   Hao Q, 2012, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2012.6248104
   Hu Y, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P527, DOI 10.1145/2647868.2654906
   Kennedy L.S., 2008, P ACM INT C WORLD WI, P297
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Klingner B, 2013, IEEE I CONF COMP VIS, P953, DOI 10.1109/ICCV.2013.122
   Kulis B, 2009, IEEE I CONF COMP VIS, P2130, DOI 10.1109/ICCV.2009.5459466
   [李丽 LI Li], 2009, [高分子通报, Polymer Bulletin], P71, DOI 10.1145/1526709.1526720
   Li XW, 2008, LECT NOTES COMPUT SC, V5302, P427
   Lior R, 2005, DATA MINING KNOWLEDG
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   MANNILA H, 1985, IEEE T COMPUT, V34, P318, DOI 10.1109/TC.1985.5009382
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724
   Philbin J., 2008, P CVPR, P1
   Rudinac S, 2013, IEEE T MULTIMEDIA, V15, P921, DOI 10.1109/TMM.2013.2237896
   Simon I, 2007, IEEE I CONF COMP VIS, P274
   Snavely N, 2008, INT J COMPUT VISION, V80, P189, DOI 10.1007/s11263-007-0107-3
   Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964
   Suditu N, 2012, P 21 ACM INT C INF K, P1323, DOI DOI 10.1145/2396761.2398435
   Van Leuken Reinier H, 2009, P 18 INT C WORLD WID, P341, DOI 10.1145/1526709.1526756
   van Zwol R., 2010, P 19 INT C WORLD WID, P961
   Wang JF, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P517, DOI 10.1145/2647868.2654898
   Wu CC, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P727, DOI 10.1145/2600428.2609569
   Zheng L, 2014, PROC CVPR IEEE, P1947, DOI 10.1109/CVPR.2014.250
   Zheng Yan-Tao., 2009, Proceedings of the 17th ACM international conference on Multimedia, P961, DOI DOI 10.1145/1631272.1631468
NR 37
TC 3
Z9 3
U1 0
U2 10
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD OCT
PY 2015
VL 17
IS 10
BP 1773
EP 1786
DI 10.1109/TMM.2015.2460111
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CR9OD
UT WOS:000361685400008
DA 2024-07-18
ER

PT J
AU Yuan, Y
   Yang, H
   Fang, YM
   Lin, WS
AF Yuan, Yuan
   Yang, Huan
   Fang, Yuming
   Lin, Weisi
TI Visual Object Tracking by Structure Complexity Coefficients
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Appearance stability; moving target; object tracking; structure
   complexity coefficients (SCC)
ID MODELS; BLUR
AB Appearance change of moving targets is a challenging problem in visual tracking. In this paper, we present a novel visual object tracking algorithm based on the observation dependent hidden Markov model (OD-HMM) framework. The observation dependency is computed by structure complexity coefficients (SCC) which is defined to predict the target appearance change. Unlike conventional methods addressing the appearance change problem by investigating different online appearance models, we handle this problem by addressing the fundamental reason of motion-related appearance change during visual tracking. Based on the analysis of motion-related appearance change, we investigate the relationship between the structure of the object surface and the appearance stability. The appearance of complex structural regions is easier to change compared with that of smooth structural regions with object moving. Based on this, we define SCC to predict the appearance stability of moving objects. Different from the standard HMM-based tracking algorithms where observations between different frames are assumed to be independent, we consider the observation dependency between consecutive frames with the information provided by SCC. Moreover, we present a novel outlier removing method in appearance model updating which helps to avoid error accumulation. Experimental results on challenging video sequences demonstrate that the proposed visual tracking algorithm with OD-HMM and SCC achieves better performance than existing related tracking algorithms.
C1 [Yuan, Yuan; Yang, Huan; Lin, Weisi] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Fang, Yuming] Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
C3 Nanyang Technological University; Jiangxi University of Finance &
   Economics
RP Fang, YM (corresponding author), Jiangxi Univ Finance & Econ, Sch Informat Technol, Nanchang 330032, Peoples R China.
EM yyuan004@ntu.edu.sg; hyang3@ntu.edu.sg; fa0001ng@e.ntu.edu.sg;
   wslin@ntu.edu.sg
RI Lin, Weisi/A-8011-2012; Lin, Weisi/A-3696-2011
OI Lin, Weisi/0000-0001-9866-1947
FU NSF of Jiangxi [20142BAB217011, 20151BDH80003]; SRF for ROCS, SEM, China
FX This work was supported in part by the NSF of Jiangxi under Grant
   20142BAB217011 and Grant 20151BDH80003, and by the SRF for ROCS, SEM,
   China. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Vasileios Mezaris.
   (Corresponding author: Yuming Fang.)
CR [Anonymous], 2006, IEEE C COMPUTER VISI, DOI DOI 10.1109/CVPR.2006.256
   [Anonymous], 2008, PROC CVPR IEEE
   Babenko B, 2009, PROC CVPR IEEE, P983, DOI 10.1109/CVPRW.2009.5206737
   Bex PJ, 2002, J OPT SOC AM A, V19, P1096, DOI 10.1364/JOSAA.19.001096
   Breitenstein MD, 2011, IEEE T PATTERN ANAL, V33, P1820, DOI 10.1109/TPAMI.2010.232
   Chou CH, 1995, IEEE T CIRC SYST VID, V5, P467, DOI 10.1109/76.475889
   Chu CT, 2013, IEEE T MULTIMEDIA, V15, P1602, DOI 10.1109/TMM.2013.2266634
   Cui P, 2009, IEEE T MULTIMEDIA, V11, P333, DOI 10.1109/TMM.2008.2009722
   Field DJ, 1997, VISION RES, V37, P3367, DOI 10.1016/S0042-6989(97)00181-8
   Ginsburg A. P., 1973, Proceedings of the National Aerospace Electronics Conference 1973, P309
   Grabner H., 2006, BMVC, P47
   Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19
   Hare S, 2011, IEEE I CONF COMP VIS, P263, DOI 10.1109/ICCV.2011.6126251
   He SF, 2013, PROC CVPR IEEE, P2427, DOI 10.1109/CVPR.2013.314
   Jepson AD, 2003, IEEE T PATTERN ANAL, V25, P1296, DOI 10.1109/TPAMI.2003.1233903
   Kalal Z, 2012, IEEE T PATTERN ANAL, V34, P1409, DOI 10.1109/TPAMI.2011.239
   Kim T.-K., 2007, U. S. Patent, Patent No. [7,171,023, 717023]
   Klein DA, 2010, IEEE INT C INT ROBOT, P772, DOI 10.1109/IROS.2010.5650583
   Kwon J, 2013, IEEE T PATTERN ANAL, V35, P2427, DOI 10.1109/TPAMI.2013.32
   Kwon J, 2010, PROC CVPR IEEE, P1269, DOI 10.1109/CVPR.2010.5539821
   Li XL, 2000, IEEE T PATTERN ANAL, V22, P371, DOI 10.1109/34.845379
   Matthews I, 2004, IEEE T PATTERN ANAL, V26, P810, DOI 10.1109/TPAMI.2004.16
   Mei X, 2011, IEEE T PATTERN ANAL, V33, P2259, DOI 10.1109/TPAMI.2011.66
   Narvekar ND, 2011, IEEE T IMAGE PROCESS, V20, P2678, DOI 10.1109/TIP.2011.2131660
   Oron S, 2012, PROC CVPR IEEE, P1940, DOI 10.1109/CVPR.2012.6247895
   Park DW, 2012, PROC CVPR IEEE, P1964, DOI 10.1109/CVPR.2012.6247898
   Potmesil M., 1983, Computer Graphics, V17, P389, DOI 10.1145/964967.801169
   Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7
   Santner J, 2010, PROC CVPR IEEE, P723, DOI 10.1109/CVPR.2010.5540145
   Sevilla-Lara L, 2012, PROC CVPR IEEE, P1910, DOI 10.1109/CVPR.2012.6247891
   Wang D, 2013, PROC CVPR IEEE, P2371, DOI 10.1109/CVPR.2013.307
   Wang D, 2013, SIGNAL PROCESS, V93, P1608, DOI 10.1016/j.sigpro.2012.07.015
   Wu Y, 2013, PROC CVPR IEEE, P2411, DOI 10.1109/CVPR.2013.312
   Wu Y, 2011, IEEE I CONF COMP VIS, P1100, DOI 10.1109/ICCV.2011.6126357
   Yilmaz A, 2006, ACM COMPUT SURV, V38, DOI 10.1145/1177352.1177355
   Yuan Y, 2014, IEEE T CIRC SYST VID, V24, P1898, DOI 10.1109/TCSVT.2014.2319632
   Zhang TZ, 2012, PROC CVPR IEEE, P2042, DOI 10.1109/CVPR.2012.6247908
   Zhong W, 2012, PROC CVPR IEEE, P1838, DOI 10.1109/CVPR.2012.6247882
NR 38
TC 33
Z9 36
U1 0
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2015
VL 17
IS 8
BP 1125
EP 1136
DI 10.1109/TMM.2015.2440996
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CN2EI
UT WOS:000358233000001
DA 2024-07-18
ER

PT J
AU Zhu, L
   Shen, JL
   Jin, H
   Xie, L
   Zheng, R
AF Zhu, Lei
   Shen, Jialie
   Jin, Hai
   Xie, Liang
   Zheng, Ran
TI Landmark Classification With Hierarchical Multi-Modal Exemplar Feature
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Dimension reduction; diverse visual contents; exemplar selection;
   hierarchical multi-modal exemplar feature (HMME); landmark
   classification; region-based locality-constrained linear coding (RLLC)
ID RECOGNITION
AB Landmark image classification attracts increasing research attention due to its great importance in real applications, ranging from travel guide recommendation to 3-D modelling and visualization of geolocation. While large amount of efforts have been invested, it still remains unsolved by academia and industry. One of the key reasons is the large intra-class variance rooted from the diverse visual appearance of landmark images. Distinguished from most existing methods based on scalable image search, we approach the problem from a new perspective and model landmark classification as multi-modal categorization, which enjoys advantages of low storage overhead and high classification efficiency. Toward this goal, a novel and effective feature representation, called hierarchical multi-modal exemplar (HMME) feature, is proposed to characterize landmark images. In order to compute HMME, training images are first partitioned into the regions with hierarchical grids to generate candidate images and regions. Then, at the stage of exemplar selection, hierarchical discriminative exemplars in multiple modalities are discovered automatically via iterative boosting and latent region label mining. Finally, HMME is generated via a region-based locality-constrained linear coding (RLLC), which effectively encodes semantics of the discovered exemplars into HMME. Meanwhile, dimension reduction is applied to reduce redundant information by projecting the raw HMME into lower-dimensional space. The final HMME enjoys advantages of discriminative and linearly separable. Experimental study has been carried out on real world landmark datasets, and the results demonstrate the superior performance of the proposed approach over several state-of-the-art techniques.
C1 [Zhu, Lei; Jin, Hai; Zheng, Ran] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Cluster & Grid Comp Lab, Serv Comp Technol & Syst Lab, Wuhan 430074, Peoples R China.
   [Shen, Jialie] Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
   [Xie, Liang] Wuhan Univ Technol, Sch Sci, Wuhan 430074, Peoples R China.
C3 Huazhong University of Science & Technology; Singapore Management
   University; Wuhan University of Technology
RP Shen, JL (corresponding author), Singapore Management Univ, Sch Informat Syst, Singapore 178902, Singapore.
EM jlshen@smu.edu.sg
RI SHEN, Jialie/E-8573-2012; Shen, Jialie/AAX-6851-2020; Zhu,
   Lei/AAC-6810-2019; Zhu, Lei/GQQ-1130-2022
OI Zhu, Lei/0000-0002-2993-7142; Zhu, Lei/0000-0002-5348-7532
FU National High Technology Research and Development Program of China
   [2012AA01A306]; National Natural Science Foundation of China [61133008];
   Singapore Ministry of Education Academic Research Fund Tier 2 (MOE)
   [MOE2013-T2-2-156]
FX This work was supported in part by the National High Technology Research
   and Development Program of China under Grant 2012AA01A306, and in part
   by the National Natural Science Foundation of China under Grant
   61133008. The work of J. Shen was supported by the Singapore Ministry of
   Education Academic Research Fund Tier 2 (MOE Ref: MOE2013-T2-2-156). The
   associate editor coordinating the review of this manuscript and
   approving it for publication was Dr. Vasileios Mezaris. (Corresponding
   author: Jialie Shen.)
CR Andrews Stuart, 2002, PROC 25 ANN C NEURAL, P561
   [Anonymous], 2007, Computer Vision
   [Anonymous], ADV MULTIMEDIA MODEL
   [Anonymous], 2014, CoRR
   [Anonymous], IEEE T CYBE IN PRESS
   [Anonymous], 2012, ACM T GRAPHIC, DOI DOI 10.1145/2185520.2185597
   [Anonymous], 2012, P 20 ACM INT C MULTI
   [Anonymous], P ACM INT C MULT NAR
   Bach Francis R, 2004, P 21 INT C MACH LEAR, P6, DOI 10.1145/1015330.1015424
   Bergamo A, 2013, PROC CVPR IEEE, P763, DOI 10.1109/CVPR.2013.104
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Chen DM, 2011, PROC CVPR IEEE, P737, DOI 10.1109/CVPR.2011.5995610
   Chen T, 2014, IEEE T CYBERNETICS, V44, P695, DOI 10.1109/TCYB.2013.2267015
   Chen T, 2014, IEEE T MULTIMEDIA, V16, P612, DOI 10.1109/TMM.2014.2301978
   Chen T, 2013, IEEE T CIRC SYST VID, V23, P1611, DOI 10.1109/TCSVT.2013.2254978
   Chum O, 2007, IEEE I CONF COMP VIS, P496, DOI 10.1109/cvpr.2007.383172
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Fan RE, 2008, J MACH LEARN RES, V9, P1871
   Fei-Fei L, 2005, PROC CVPR IEEE, P524
   Felzenszwalb PF, 2010, IEEE T PATTERN ANAL, V32, P1627, DOI 10.1109/TPAMI.2009.167
   Hao Q, 2012, PROC CVPR IEEE, P3594, DOI 10.1109/CVPR.2012.6248104
   Hays J, 2008, PROC CVPR IEEE, P3436
   James M., 1967, PROC BERKELEY S MATH, V1, P281, DOI DOI 10.1007/S11665-016-2173-6
   Ji RR, 2012, INT J COMPUT VISION, V96, P290, DOI 10.1007/s11263-011-0472-9
   Jiang SH, 2015, IEEE T MULTIMEDIA, V17, P907, DOI 10.1109/TMM.2015.2417506
   Junge Shen, 2014, MultiMedia Modeling. 20th Anniversary International Conference, MMM 2014. Proceedings: LNCS 8326, P227, DOI 10.1007/978-3-319-04117-9_21
   Li YP, 2009, IEEE I CONF COMP VIS, P1957, DOI 10.1109/ICCV.2009.5459432
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Min WQ, 2014, IEEE MULTIMEDIA, V21, P20, DOI 10.1109/MMUL.2014.1
   Min WQ, 2014, IEEE T MULTIMEDIA, V16, P623, DOI 10.1109/TMM.2014.2302744
   Mindru F, 2004, COMPUT VIS IMAGE UND, V94, P3, DOI 10.1016/j.cviu.2003.10.011
   Ojala T, 1996, PATTERN RECOGN, V29, P51, DOI 10.1016/0031-3203(95)00067-4
   Raguram R, 2011, INT J COMPUT VISION, V95, P213, DOI 10.1007/s11263-011-0445-z
   Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663
   Sun CS, 2013, IEEE T IMAGE PROCESS, V22, P3050, DOI 10.1109/TIP.2013.2255303
   Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153
   Witten I.H., 1996, Managing gigabytes: Compressing and indexing documents and images - errata
   Xiao X, 2012, IEEE T MULTIMEDIA, V14, P1246, DOI 10.1109/TMM.2012.2190384
   Zhang Shiliang., 2009, MM 09 P 17 ACM INT C, P75, DOI DOI 10.1145/1631272.1631285
   Zheng YT, 2009, PROC CVPR IEEE, P1085, DOI 10.1109/CVPRW.2009.5206749
NR 42
TC 43
Z9 44
U1 1
U2 12
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUL
PY 2015
VL 17
IS 7
BP 981
EP 993
DI 10.1109/TMM.2015.2431496
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CK8XC
UT WOS:000356522300006
OA Green Published
DA 2024-07-18
ER

PT J
AU Yuan, ZQ
   Xu, CS
   Sang, JT
   Yan, SC
   Hossain, MS
AF Yuan, Zhaoquan
   Xu, Changsheng
   Sang, Jitao
   Yan, Shuicheng
   Hossain, M. Shamim
TI Learning Feature Hierarchies: A Layer-Wise Tag-Embedded Approach
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Auto-encoder; deep learning; hierarchical feature learning; social tags
ID CLASSIFICATION; DICTIONARY; MULTIPLE; MODELS
AB Feature representation learning is an important and fundamental task in multimedia and pattern recognition research. In this paper, we propose a novel framework to explore the hierarchical structure inside the images from the perspective of feature representation learning, which is applied to hierarchical image annotation. Different from the current trend in multimedia analysis of using pre-defined features or focusing on the end-task "flat" representation, we propose a novel layer-wise tag-embedded deep learning (LTDL) model to learn hierarchical features which correspond to hierarchical semantic structures in the tag hierarchy. Unlike most existing deep learning models, LTDL utilizes both the visual content of the image and the hierarchical information of associated social tags. In the training stage, the two kinds of information are fused in a bottom-up way. Supervised training and multi-modal fusion alternate in a layer-wise way to learn feature hierarchies. To validate the effectiveness of LTDL, we conduct extensive experiments for hierarchical image annotation on a large-scale public dataset. Experimental results show that the proposed LTDL can learn representative features with improved performances.
C1 [Yuan, Zhaoquan; Xu, Changsheng; Sang, Jitao] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yan, Shuicheng] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Hossain, M. Shamim] King Saud Univ, SWE Dept, Coll Comp & Informat Sci, Riyadh 11543, Saudi Arabia.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   University of Singapore; King Saud University
RP Yuan, ZQ (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
EM zqyuan@nlpr.ia.ac.cn; csxu@nlpr.ia.ac.cn; jtsang@nlpr.ia.ac.cn;
   eleyans@nus.edu.sg; mshossain@ksu.edu.sa
RI Guizani, Mohsen/AAX-4534-2021; Hossain, M. Shamim/K-1362-2014; Yan,
   Shuicheng/HCI-1431-2022; xu, cj/HJZ-3488-2023
OI Guizani, Mohsen/0000-0002-8972-8094; Hossain, M.
   Shamim/0000-0001-5906-9422; 
FU National Basic Research Program of China [2012CB316304]; National
   Natural Science Foundation of China [61225009, 61373122, 61303176,
   61402479, 61328205, 61432019]; Beijing Natural Science Foundation
   [4131004]; Singapore National Research Foundation under its
   International Research Centre@Singapore Funding Initiative; Deanship of
   Scientific Research at King Saud University [IRG 14-18]
FX This work was supported in part by the National Basic Research Program
   of China under Grant 2012CB316304, by the National Natural Science
   Foundation of China under Grant 61225009, Grant 61373122, Grant
   61303176, Grant 61402479, Grant 61328205, and Grant 61432019, by the
   Beijing Natural Science Foundation under Grant 4131004, by the Singapore
   National Research Foundation under its International Research
   Centre@Singapore Funding Initiative and administered by the IDM
   Programme Office, and by the Deanship of Scientific Research at King
   Saud University through the international research group Program IRG
   14-18. The associate editor coordinating the review of this manuscript
   and approving it for publication was Dr. Cees Snoek.
CR [Anonymous], 2006, ADV NEURAL INF PROCE
   [Anonymous], P 2008 IEEE C COMP V
   [Anonymous], ACM T MULTIMEDIA C S
   [Anonymous], 2006, Data Analysis using Regression and Multilevel/Hierarchical Models, DOI DOI 10.1017/CBO9780511790942
   [Anonymous], 2010, THEORY APPL ONTOLOGY
   [Anonymous], 2013, 31 INT C MACH LEARN
   [Anonymous], 2011, P ICML
   [Anonymous], 2017, COMMUN ACM, DOI DOI 10.1145/3065386
   [Anonymous], 2007, LARGE SCALE KERNEL M
   [Anonymous], 2006, Proceedings of the 23rd International Conference on Machine Learning
   Bengio Y., 2007, Advances in neural information processing systems, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Blei DM, 2004, ADV NEUR IN, V16, P17
   Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993
   Botvinick MM, 2008, TRENDS COGN SCI, V12, P201, DOI 10.1016/j.tics.2008.02.009
   Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P31
   Chen YQ, 2001, IEEE IMAGE PROC, P385, DOI 10.1109/ICIP.2001.959034
   Coates A., 2011, P 14 INT C ART INT S, V15, P215
   Collet C, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL III, P979, DOI 10.1109/ICIP.1996.560989
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deselaers T, 2011, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR.2011.5995474
   Evgeniou T, 2003, IEEE T KNOWL DATA EN, V15, P911, DOI 10.1109/TKDE.2003.1209008
   Fawcett T., 2004, MACH LEARN, V31, P1
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Hand DJ, 2001, MACH LEARN, V45, P171, DOI 10.1023/A:1010920819831
   He Q, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P491, DOI 10.1137/1.9781611972771.50
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Horster E., 2008, Proceedings of the 16th ACM international conference on Multimedia, P643
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Hotho A., 2002, KUNSTL INTELL, V16, P48
   Irie G, 2013, PROC CVPR IEEE, P329, DOI 10.1109/CVPR.2013.49
   Jia D, 2011, PROC CVPR IEEE, P785, DOI 10.1109/CVPR.2011.5995516
   Jitao Sang, 2012, Journal of Multimedia, V7, P9, DOI 10.4304/jmm.7.1.9-20
   Katsurai M, 2014, IEEE T MULTIMEDIA, V16, P1059, DOI 10.1109/TMM.2014.2306655
   Kavukcuoglu K., 2010, Advances in neural information processing systems, P1
   Koskela M, 2007, IEEE T MULTIMEDIA, V9, P912, DOI 10.1109/TMM.2007.900137
   Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LEVITT JB, 1994, J NEUROPHYSIOL, V71, P2517, DOI 10.1152/jn.1994.71.6.2517
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Marszalek Marcin, 2007, CVPR '07. IEEE Conference on Computer Vision and Pattern Recognition, P1
   Naphade M, 2006, IEEE MULTIMEDIA, V13, P86, DOI 10.1109/MMUL.2006.63
   Osindero S, 2006, NEURAL COMPUT, V18, P381, DOI 10.1162/089976606775093936
   Qixing Huang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P1953, DOI 10.1109/CVPR.2011.5995571
   Ranzato MA., 2007, CVPR, DOI [10.1109/cvpr.2007.383157, 10.1109/CVPR.2007.383157]
   Roig Gemma, 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P657, DOI 10.1109/FG.2011.5771328
   Schulz H, 2012, KUNSTL INTELL, V26, P357, DOI 10.1007/s13218-012-0198-z
   Shen L, 2013, PROC CVPR IEEE, P383, DOI 10.1109/CVPR.2013.56
   Silla CN, 2011, DATA MIN KNOWL DISC, V22, P31, DOI 10.1007/s10618-010-0175-9
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vincent P., 2008, INT C MACH LEARN ICM, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Wang KY, 2013, IEEE I CONF COMP VIS, P2088, DOI 10.1109/ICCV.2013.261
   Zhang H., 2007, P 15 INT C MULTIMEDI, P273
   Zhou N, 2014, IEEE T PATTERN ANAL, V36, P715, DOI 10.1109/TPAMI.2013.189
   Zhou N, 2012, PROC CVPR IEEE, P3490, DOI 10.1109/CVPR.2012.6248091
NR 60
TC 7
Z9 7
U1 1
U2 20
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD JUN
PY 2015
VL 17
IS 6
BP 816
EP 827
DI 10.1109/TMM.2015.2417777
PG 12
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CI1TM
UT WOS:000354527500005
DA 2024-07-18
ER

PT J
AU Lindner, A
   Süsstrunk, S
AF Lindner, Albrecht
   Suesstrunk, Sabine
TI Semantic-Improved Color Imaging Applications: It Is All About Context
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Color naming; color palette; enhancement; image processing; semantic
   gap; semantics
AB Multimedia data with associated semantics is omnipresent in today's social online platforms in the form of keywords, user comments, and so forth. This article presents a statistical framework designed to infer knowledge in the imaging domain from the semantic domain. Note that this is the reverse direction of common computer vision applications. The framework relates keywords to image characteristics using a statistical significance test. It scales to millions of images and hundreds of thousands of keywords. We demonstrate the usefulness of the statistical framework with three color imaging applications: 1) semantic image enhancement: re-render an image in order to adapt it to its semantic context; 2) color naming: find the color triplet for a given color name; and 3) color palettes: find a palette of colors that best represents a given arbitrary semantic context and that satisfies established harmony constraints.
C1 [Lindner, Albrecht; Suesstrunk, Sabine] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne
RP Lindner, A (corresponding author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.
EM albrecht.lindner@alumni.epfl.ch; sabine.susstrunk@epfl.ch
RI Süsstrunk, Sabine/I-2466-2013
FU Oce Print Logic Technologies (a Canon Company)
FX Manuscript received October 23, 2014; revised February 01, 2015;
   accepted February 23, 2015. Date of publication March 04, 2015; date of
   current version April 15, 2015. This work was supported in part by Oce
   Print Logic Technologies (a Canon Company). The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Mr. Jiebo Luo.
CR [Anonymous], 1908, BIOMETRIKA, V6, P1
   [Anonymous], THESIS EPFL LAUSANNE
   [Anonymous], P SPIE
   [Anonymous], P ACM SIGGRAPH
   Ciocca G, 2007, LECT NOTES COMPUT SC, V4733, P686
   Deselaers T, 2011, PROC CVPR IEEE, P1777, DOI 10.1109/CVPR.2011.5995474
   Hitchcock F. L., 1941, J MATH PHYS, V20, P224, DOI DOI 10.1002/SAPM1941201224
   HODGES JL, 1963, ANN MATH STAT, V34, P598, DOI 10.1214/aoms/1177704172
   Huiskes Mark J., 2008, Proceedings of the 1st ACM international conference on Multimedia information retrieval, P39, DOI DOI 10.1145/1460096.1460104
   Jones MJ, 2002, INT J COMPUT VISION, V46, P81, DOI 10.1023/A:1013200319198
   Kang SB, 2010, PROC CVPR IEEE, P1799, DOI 10.1109/CVPR.2010.5539850
   Kolmogorov A., 1933, Giorn Dell'inst Ital Degli Att, V4, P83, DOI DOI 10.12691/AJAMS-1-1-2
   Laput G.P., 2013, P SIGCHI C HUM FACT, P2185, DOI [DOI 10.1145/2470654.2481301, 10.1145/2470654.2481301]
   MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491
   Matsuda Y., 1995, COLOR DESIGN
   Meier BJ, 2004, IEEE COMPUT GRAPH, V24, P64, DOI 10.1109/MCG.2004.1297012
   Moroney N, 2003, PROC SPIE, V5008, P36, DOI 10.1117/12.472013
   Moser S, 2002, P SOC PHOTO-OPT INS, V4669, P259, DOI 10.1117/12.463430
   PLACKETT RL, 1983, INT STAT REV, V51, P59, DOI 10.2307/1402731
   Reinhard E, 2001, IEEE COMPUT GRAPH, V21, P34, DOI 10.1109/38.946629
   Shirdhonkar S., 2008, PROC IEEE C COMPUT V, P1
   SMIRNOV N, 1948, ANN MATH STAT, V19, P279, DOI 10.1214/aoms/1177730256
   van de Weijer J, 2009, IEEE T IMAGE PROCESS, V18, P1512, DOI 10.1109/TIP.2009.2019809
   WILCOXON F, 1945, BIOMETRICS BULL, V1, P80, DOI 10.1093/jee/39.2.269
   Zhuo SJ, 2011, PATTERN RECOGN, V44, P1852, DOI 10.1016/j.patcog.2011.03.009
NR 25
TC 4
Z9 6
U1 0
U2 4
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAY
PY 2015
VL 17
IS 5
BP 700
EP 710
DI 10.1109/TMM.2015.2410175
PG 11
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CG3AH
UT WOS:000353148300011
OA Green Submitted
DA 2024-07-18
ER

PT J
AU Tekin, C
   van der Schaar, M
AF Tekin, Cem
   van der Schaar, Mihaela
TI Contextual Online Learning for Multimedia Content Aggregation
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Content aggregation; distributed online learning; multi-armed bandits;
   social multimedia
AB The last decade has witnessed a tremendous growth in the volume as well as the diversity of multimedia content generated by a multitude of sources (news agencies, social media, etc.). Faced with a variety of content choices, consumers are exhibiting diverse preferences for content; their preferences often depend on the context in which they consume content as well as various exogenous events. To satisfy the consumers' demand for such diverse content, multimedia content aggregators (CAs) have emerged which gather content from numerous multimedia sources. A key challenge for such systems is to accurately predict what type of content each of its consumers prefers in a certain context, and adapt these predictions to the evolving consumers' preferences, contexts, and content characteristics. We propose a novel, distributed, online multimedia content aggregation framework, which gathers content generated by multiple heterogeneous producers to fulfill its consumers' demand for content. Since both the multimedia content characteristics and the consumers' preferences and contexts are unknown, the optimal content aggregation strategy is unknown a priori. Our proposed content aggregation algorithm is able to learn online what content to gather and how to match content and users by exploiting similarities between consumer types. We prove bounds for our proposed learning algorithms that guarantee both the accuracy of the predictions as well as the learning speed. Importantly, our algorithms operate efficiently even when feedback from consumers is missing or content and preferences evolve over time. Illustrative results highlight the merits of the proposed content aggregation system in a variety of settings.
C1 [Tekin, Cem; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
C3 University of California System; University of California Los Angeles
RP Tekin, C (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90095 USA.
EM cmtkn@ucla.edu; mihaela@ee.ucla.edu
RI Tekin, Cem/ADX-4585-2022
OI Tekin, Cem/0000-0003-4361-4021; van der schaar,
   Mihaela/0000-0003-3933-6049
FU NSF CNS [1016081]; AFOSR DDDAS; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1016081] Funding
   Source: National Science Foundation
FX This work is supported in part by the NSF CNS under Grant 1016081 and by
   AFOSR DDDAS. The associate editor coordinating the review of this
   manuscript and approving it for publication was Prof. Changsheng Xu.
CR Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352
   Bouneffouf D., 2012, Pacific-Asia Conference on Knowledge Discovery and Data Mining, P468
   Boutet A, 2013, P INT C NETW SYST MA, P253
   Chu Wei, 2011, P 14 INT C ARTIFICIA, P208
   Deshpande Y, 2012, ANN ALLERTON CONF, P1750, DOI 10.1109/Allerton.2012.6483433
   Dudik M., 2011, P  WORKSH 28 INT C M
   Gao J, 2007, IEEE DATA MINING, P143
   Hazan E, 2007, LECT NOTES COMPUT SC, V4539, P499, DOI 10.1007/978-3-540-72927-3_36
   Kohli P., 2013, AAAI
   Langford John., 2007, ADV NEURAL INFORM PR, V20, P1096
   Li L., 2010, P 19 INT C WORLD WID, DOI [10.1145/1772690.1772758, DOI 10.1145/1772690.1772758]
   Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344
   Lu T., 2010, P 13 INT C ARTIFICIA, V9, P485
   Masud MM, 2009, LECT NOTES ARTIF INT, V5782, P79, DOI 10.1007/978-3-642-04174-7_6
   Minku LL, 2010, IEEE T KNOWL DATA EN, V22, P730, DOI 10.1109/TKDE.2009.156
   Miyahara K., 2000, PRICAI 2000. Topics in Artificial Intelligence. 6th Pacific Rim International Conference on Artificial Intelligence. Proceedings (Lecture Notes in Artificial Intelligence Vol.1886), P679
   Mohan R, 1999, IEEE T MULTIMEDIA, V1, P104, DOI 10.1109/6046.748175
   Naaman M, 2012, MULTIMED TOOLS APPL, V56, P9, DOI 10.1007/s11042-010-0538-7
   Narasimhamurthy A, 2007, PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND APPLICATIONS, P384
   Ren SL, 2012, IEEE T MULTIMEDIA, V14, P1566, DOI 10.1109/TMM.2012.2217120
   Roy S. D., 2012, 2012 IEEE International Conference on Multimedia and Expo (ICME), P49, DOI 10.1109/ICME.2012.105
   Roy SD, 2013, IEEE T MULTIMEDIA, V15, P1255, DOI 10.1109/TMM.2013.2265079
   Sahoo N, 2012, MIS QUART, V36, P1329
   Saxena Mohit., 2008, NOSSDAV '08: Proceedings of the 18th International Workshop on Network and Operating Systems Support for Digital Audio and Video, P39
   Schranz M, 2005, INT FED INFO PROC, V186, P587
   Slivkins Aleksandrs, 2011, P 24 ANN C LEARN THE, P679
   Song SB, 2012, IEEE T MULTIMEDIA, V14, P1528, DOI 10.1109/TMM.2012.2217118
   Tekin Cem, 2015, CONTEXTUAL ONLINE LE
   van der Schaar M, 2013, ECON THEOR, V54, P211, DOI 10.1007/s00199-013-0744-4
   Xu CS, 2008, IEEE T MULTIMEDIA, V10, P421, DOI 10.1109/TMM.2008.917346
NR 30
TC 12
Z9 12
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD APR
PY 2015
VL 17
IS 4
BP 549
EP 561
DI 10.1109/TMM.2015.2403234
PG 13
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QH
UT WOS:000351586300008
OA Green Submitted, Green Published
DA 2024-07-18
ER

PT J
AU Yao, RX
   Liu, YW
   Liu, JX
   Zhao, PH
   Ci, S
AF Yao, Ruixiao
   Liu, Yanwei
   Liu, Jinxia
   Zhao, Pinghua
   Ci, Song
TI Utility-Based H.264/SVC Video Streaming Over Multi-Channel Cognitive
   Radio Networks
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Cognitive radio; flexible sensing-transmission scheme; H.264/SVC;
   multi-channel; utility-based transmission scheme; video quality
   prediction
ID SCALABLE VIDEO; MULTIMEDIA TRANSMISSION; SECONDARY USERS; PROTECTION;
   ALLOCATION; EXTENSION; QUALITY; HEVC
AB In a cognitive radio (CR) network, a secondary user (SU) with multiple interfaces is capable of accessing multiple CR channels in an opportunistic fashion. Therefore, the available channel resources may change dramatically, and the reliabilities of the multiple accessed CR channels are also time-varying. Video streaming in such a multi-channel CR network faces great challenges in guaranteeing the quality of the received video. To deal with these challenges, we adopt H.264/SVC encoded video as the source and firstly optimize the video streaming from the perspective of exploiting more channel resources for the SU by developing a flexible sensing-transmission scheme for opportunistic spectrum access (OSA). In this scheme, the primary user activity, channel sensing result, and channel sensing accuracy are all considered in reducing the unnecessary channel sensings and correspondingly extending the transmission duration for the SU. Based on the flexible sensing-transmission scheme, we then propose a utility-based H.264/SVC video transmission scheme to further improve the expected video quality at the receiver. Specifically, the network abstraction layer units (NALUs) in the SVC video are assigned utilities which accurately reflect their contributions to the video quality, and the total effective utility of expected received video is maximized through perfectly dispatching the NALUs over the multiple CR channels. Both analytical studies and experimental results validate the effectiveness and efficiency of the proposed method.
C1 [Yao, Ruixiao; Liu, Yanwei] Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
   [Yao, Ruixiao; Zhao, Pinghua; Ci, Song] Chinese Acad Sci, Inst Acoust, Beijing 100190, Peoples R China.
   [Liu, Jinxia] Zhejiang Wanli Univ, Ningbo 315100, Zhejiang, Peoples R China.
   [Ci, Song] Univ Nebraska, Dept Comp & Elect Engn, Lincoln, NE 68588 USA.
C3 Chinese Academy of Sciences; Institute of Information Engineering, CAS;
   Chinese Academy of Sciences; Institute of Acoustics, CAS; Zhejiang Wanli
   University; University of Nebraska System; University of Nebraska
   Lincoln
RP Liu, YW (corresponding author), Chinese Acad Sci, Inst Informat Engn, Beijing 100093, Peoples R China.
EM yaorxgg@163.com; liuyanwei@iie.ac.cn; li-ujinxia1969@126.com;
   zhaopinghua205@163.com; sci@engr.unl.edu
RI Liu, Jinxia/H-1794-2011; liu, yanwei/L-2453-2019; Ci, Song/R-8324-2019
FU NSFC [61102077, 61472388]; National Key Technology RD Program
   [2012BAH06B02]; Zhejiang Provincial Natural Science Foundation of China
   [LY13F010012]; public welfare projects of Zhejiang Province [2014C31072]
FX This work was supported in part by the NSFC under Grant 61102077 and
   Grant 61472388, the National Key Technology R&D Program under Grant
   2012BAH06B02, the Zhejiang Provincial Natural Science Foundation of
   China under Contract LY13F010012, and the public welfare projects of
   Zhejiang Province under Contract 2014C31072. The associate editor
   coordinating the review of this manuscript and approving it for
   publication was Prof. Pal Halvorsen. (Corresponding author: Yanwei Liu.)
CR Afifi W, 2011, IEEE INT SYMP DYNAM, P380, DOI 10.1109/DYSPAN.2011.5936227
   Amonou I, 2007, IEEE T CIRC SYST VID, V17, P1186, DOI 10.1109/TCSVT.2007.906870
   [Anonymous], 2012, P 4 ACM WORKSH MOB V
   [Anonymous], 2010, JSVM SOFTW MAN VER J
   Chen DW, 2009, FIFTEENTH ACM INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING (MOBICOM 2009), P13
   Chen J., 2014, P 18 M JCT VC, P1
   Cheng P, 2011, IEEE T COMMUN, V59, P1878, DOI 10.1109/TCOMM.2011.051711.100223
   Chou PA, 2006, IEEE T MULTIMEDIA, V8, P390, DOI 10.1109/TMM.2005.864313
   Cicalò S, 2014, IEEE T MULTIMEDIA, V16, P848, DOI 10.1109/TMM.2014.2300442
   Digham FF, 2007, IEEE T COMMUN, V55, P21, DOI 10.1109/TCOMM.2006.887483
   Engelman R., 2002, REPORT SPECTRUM EFFI
   Feldmann C, 2013, IEEE IMAGE PROC, P1583, DOI 10.1109/ICIP.2013.6738326
   Freris NM, 2013, IEEE ACM T NETWORK, V21, P469, DOI 10.1109/TNET.2012.2203608
   Görkemli B, 2010, IEEE IMAGE PROC, P4201, DOI 10.1109/ICIP.2010.5650771
   Guo C, 2009, P IEEE WIR COMM NETW, P1, DOI DOI 10.1109/PES.2009.5275742
   Hassan MT, 2013, 2013 IEEE 27TH INTERNATIONAL CONFERENCE ON ADVANCED INFORMATION NETWORKING AND APPLICATIONS WORKSHOPS (WAINA), P511, DOI 10.1109/WAINA.2013.76
   Hu DL, 2010, IEEE J SEL AREA COMM, V28, P434, DOI 10.1109/JSAC.2010.100414
   Huang SH, 2009, IEEE INFOCOM SER, P2295, DOI 10.1109/INFCOM.2009.5062155
   Huang XL, 2011, IEEE T MULTIMEDIA, V13, P748, DOI 10.1109/TMM.2011.2148701
   Jiang TG, 2012, IEEE J SEL AREA COMM, V30, P1215, DOI 10.1109/JSAC.2012.120807
   Kuipers F, 2010, LECT NOTES COMPUT SC, V6074, P216
   Lee WY, 2008, IEEE T WIREL COMMUN, V7, P3845, DOI 10.1109/T-WC.2008.070391
   Li MD, 2013, IEEE T MULTIMEDIA, V15, P1519, DOI 10.1109/TMM.2013.2267207
   Li S., 2010, P IEEE GLOBECOM, P1, DOI DOI 10.1109/CISE.2010.5677091
   Liu QW, 2005, IEEE T WIREL COMMUN, V4, P1142, DOI 10.1109/TWC.2005.847005
   Luo HY, 2011, IEEE T CIRC SYST VID, V21, P1040, DOI 10.1109/TCSVT.2011.2129810
   Maani E, 2010, IEEE T CIRC SYST VID, V20, P407, DOI 10.1109/TCSVT.2009.2035846
   Mitola J, 1999, IEEE PERS COMMUN, V6, P13, DOI 10.1109/98.788210
   Palaniappan R., 2010, 2010 Fourth Pacific-Rim Symposium on Image and Video Technology (PSIVT), P239, DOI 10.1109/PSIVT.2010.47
   Schwarz H, 2007, IEEE T CIRC SYST VID, V17, P1103, DOI 10.1109/TCSVT.2007.905532
   Sinha D, 2007, IEEE T COMPUT AID D, V26, P1522, DOI 10.1109/TCAD.2007.893544
   Song Xiao, 2008, 2008 22nd International Conference on Advanced Information Networking and Applications - Workshops, P896, DOI 10.1109/AINA.2008.149
   Sullivan GJ, 2012, IEEE T CIRC SYST VID, V22, P1649, DOI 10.1109/TCSVT.2012.2221191
   van der Schaar M, 2003, IEEE J SEL AREA COMM, V21, P1752, DOI 10.1109/JSAC.2003.815231
   Wang CL, 2012, IEEE WCNC, P1293, DOI 10.1109/WCNC.2012.6213977
   Wang S., 2010, P IEEE INFOCOM, P1, DOI [10.1109/INFCOM.2010.5461943, DOI 10.1109/INFCOM.2010.5461943]
   Yao RX, 2013, IEEE GLOB COMM CONF, P1681, DOI 10.1109/GLOCOM.2013.6831315
   Ye Y, 2014, IEEE MULTIMEDIA, V21, P58, DOI 10.1109/MMUL.2014.47
   Yu R., 2013, P IEEE INT C MULT EX, P1
   Zhao QC, 2008, IEEE T SIGNAL PROCES, V56, P785, DOI 10.1109/TSP.2007.907867
NR 40
TC 13
Z9 15
U1 0
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD MAR
PY 2015
VL 17
IS 3
BP 434
EP 449
DI 10.1109/TMM.2015.2394385
PG 16
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA CE1QB
UT WOS:000351585700014
DA 2024-07-18
ER

PT J
AU Nemoianu, ID
   Greco, C
   Cagnazzo, M
   Pesquet-Popescu, B
AF Nemoianu, Irina-Delia
   Greco, Claudio
   Cagnazzo, Marco
   Pesquet-Popescu, Beatrice
TI On a Hashing-Based Enhancement of Source Separation Algorithms Over
   Finite Fields With Network Coding Perspectives
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE Blind source separation; channel coding; Galois fields; independent
   component analysis; multimedia networking; network coding
ID INDEPENDENT COMPONENT ANALYSIS; ICA
AB Blind Source Separation (BSS) deals with the recovery of source signals from a set of observed mixtures, when little or no knowledge of the mixing process is available. BSS can find an application in the context of network coding, where relaying linear combinations of packets maximizes the throughput and increases the loss immunity. By relieving the nodes from the need to send the combination coefficients, the overhead cost is largely reduced. However, the scaling ambiguity of the technique and the quasi-uniformity of compressed media sources makes it unfit, at its present state, for multimedia transmission. In order to open new practical applications for BSS in the context of multimedia transmission, we have recently proposed to use a non-linear encoding to increase the discriminating power of the classical entropy-based separation methods. Here, we propose to append to each source a non-linear message digest, which offers an overhead smaller than a per-symbol encoding and that can be more easily tuned. Our results prove that our algorithm is able to provide high decoding rates for different media types such as image, audio, and video, when the transmitted messages are less than 1.5 kilobytes, which is typically the case in a realistic transmission scenario.
C1 [Nemoianu, Irina-Delia; Cagnazzo, Marco; Pesquet-Popescu, Beatrice] Telecom ParisTech, Inst Mines Telecom, CNRS LTCI, TSI Dept, F-75634 Paris, France.
   [Greco, Claudio] INRIA Rocquencourt, HIPERCOM Project Team, F-78153 Le Chesnay, France.
   [Greco, Claudio] CNRS, LSS Supelec, Div Telecoms & Reseaux, F-91192 Gif Sur Yvette, France.
C3 IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; Telecom
   Paris; Centre National de la Recherche Scientifique (CNRS); Universite
   Paris Saclay; Centre National de la Recherche Scientifique (CNRS)
RP Nemoianu, ID (corresponding author), Telecom ParisTech, Inst Mines Telecom, CNRS LTCI, TSI Dept, F-75634 Paris, France.
EM nemoianu@telecom-paristech.fr; greco@telecom-paris-tech.fr;
   cagnazzo@telecom-paristech.fr; pesquet@telecom-paristech.fr
RI Cagnazzo, Marco/AAZ-3881-2020
OI Cagnazzo, Marco/0000-0001-6731-3755
CR Ahlswede R, 2000, IEEE T INFORM THEORY, V46, P1204, DOI 10.1109/18.850663
   Bertoni G., 2011, NIST ROUND 3 UNPUB
   Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250
   Chou P.A., 2003, Proc. Annual Allerton Conference on Communication control and Computing, V41, P40
   Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1
   COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9
   Greco C, 2012, EUR SIGNAL PR CONF, P1915
   Gutch HW, 2012, SIGNAL PROCESS, V92, P1796, DOI 10.1016/j.sigpro.2011.10.003
   Gutch HW, 2010, LECT NOTES COMPUT SC, V6365, P645, DOI 10.1007/978-3-642-15995-4_80
   Ho T., 2003, P ANN ALLERTON C COM, V41, P11
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jafari M, 2009, IEEE INT SYMP INFO, P109, DOI 10.1109/ISIT.2009.5206041
   Koetter R, 2003, IEEE ACM T NETWORK, V11, P782, DOI 10.1109/TNET.2003.818197
   Li SZ, 2010, IEEE COMMUN LETT, V14, P749, DOI 10.1109/LCOMM.2010.08.092453
   Li SYR, 2003, IEEE T INFORM THEORY, V49, P371, DOI 10.1109/TIT.2002.807285
   Magli E, 2013, IEEE T MULTIMEDIA, V15, P1195, DOI 10.1109/TMM.2013.2241415
   Nemoianu I, 2013, INT CONF ACOUST SPEE, P1335, DOI 10.1109/ICASSP.2013.6637868
   Nemoianu ID, 2012, INT CONF ACOUST SPEE, P2309, DOI 10.1109/ICASSP.2012.6288376
   Thomos N, 2012, IEEE COMMUN LETT, V16, P1860, DOI 10.1109/LCOMM.2012.092812.121661
   Thomos N, 2011, IEEE T MULTIMEDIA, V13, P776, DOI 10.1109/TMM.2011.2111364
   Thomos N, 2010, IEEE T CIRC SYST VID, V20, P1834, DOI 10.1109/TCSVT.2010.2087830
   Vukobratovic Dejan, 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P280, DOI 10.1109/MMSP.2010.5662033
   Vukobratovic D, 2012, IEEE T COMMUN, V60, P1243, DOI 10.1109/TCOMM.2012.030712.100454
   Yeredor A, 2007, LECT NOTES COMPUT SC, V4666, P827
   Yeredor A, 2011, IEEE T INFORM THEORY, V57, P5342, DOI 10.1109/TIT.2011.2145090
NR 25
TC 2
Z9 2
U1 0
U2 5
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD NOV
PY 2014
VL 16
IS 7
BP 2011
EP 2024
DI 10.1109/TMM.2014.2341923
PG 14
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AT1UX
UT WOS:000344720300018
DA 2024-07-18
ER

PT J
AU Ding, K
   Liu, YH
AF Ding, Ke
   Liu, Yun-Hui
TI Sphere Image for 3-D Model Retrieval
SO IEEE TRANSACTIONS ON MULTIMEDIA
LA English
DT Article
DE 3-D model retrieval; sphere image; 3-D model descriptor; matching
ID OBJECT RETRIEVAL; SEARCH ENGINE; 3D; RECOGNITION
AB The view-based 3-D model retrieval system represents a 3-D model by its projected views. Most of the existing view-based 3-D model retrieval systems only analyze the features of the projected views, but not well consider the spatial arrangements of the viewpoints. Furthermore, most of these systems suffer from the high computational cost due to pairwise comparing the projected views of 3-D models. In this paper, we propose a new 3-D model descriptor called Sphere Image, which is defined as a collection of view features. A viewpoint of a 3-D model is regarded as a "pixel": (1) The position of the viewpoint is denoted as the coordinate of the "pixel". (2) The feature descriptor of the projected view is denoted as the value of the "pixel". We also propose a probabilistic graphical model for 3-D model matching, and develop a 3-D model retrieval system to test our approach. We have conducted experiments based on the SHape REtrieval Contest (SHREC) 2012 generic 3-D model date set and the SHREC2009 partial 3-D model data set. Experimental results indicate that our system outperforms some state-of-the-art 3-D model retrieval systems.
C1 [Ding, Ke; Liu, Yun-Hui] Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Hong Kong, Peoples R China.
C3 Chinese University of Hong Kong
RP Ding, K (corresponding author), Chinese Univ Hong Kong, Dept Mech & Automat Engn, Hong Kong, Hong Kong, Peoples R China.
EM kding@mae.cuhk.edu.hk; yhliu@mae.cuhk.edu.hk
RI lin, yuan/JXL-9592-2024
CR [Anonymous], P ACM INT C IM VID R
   [Anonymous], MULTIMEDIA TOOLS APP
   Ansary TF, 2007, IEEE T MULTIMEDIA, V9, P78, DOI 10.1109/TMM.2006.886359
   Assfalg J, 2007, IEEE T MULTIMEDIA, V9, P589, DOI 10.1109/TMM.2006.886271
   Boiman O, 2005, IEEE I CONF COMP VIS, P462
   Bustos B, 2005, ACM COMPUT SURV, V37, P345, DOI 10.1145/1118890.1118893
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Chen DY, 2003, COMPUT GRAPH FORUM, V22, P223, DOI 10.1111/1467-8659.00669
   Del Bimbo A, 2006, ACM T MULTIM COMPUT, V2, P20
   DING K., 2013, Computer Vision ACCV 2012, P536
   Ding K, 2012, INT C PATT RECOG, P601
   Dutafaci H., 2009, P EUR WORKSH 3D OBJ
   Eitz M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185527
   Felzenszwalb PF, 2000, PROC CVPR IEEE, P66, DOI 10.1109/CVPR.2000.854739
   Funkhouser T, 2003, ACM T GRAPHIC, V22, P83, DOI 10.1145/588272.588279
   Gao Y, 2014, IEEE T IND ELECTRON, V61, P2088, DOI 10.1109/TIE.2013.2262760
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P4290, DOI 10.1109/TIP.2012.2199502
   Gao Y, 2012, IEEE T IMAGE PROCESS, V21, P2269, DOI 10.1109/TIP.2011.2170081
   Gao Y, 2011, IEEE T MULTIMEDIA, V13, P1007, DOI 10.1109/TMM.2011.2160619
   Gao Y, 2011, SIGNAL PROCESS-IMAGE, V26, P39, DOI 10.1016/j.image.2010.10.006
   Gao Y, 2010, PATTERN RECOGN, V43, P1142, DOI 10.1016/j.patcog.2009.07.012
   Gong BQ, 2013, IEEE T MULTIMEDIA, V15, P369, DOI 10.1109/TMM.2012.2231059
   Hamerly G, 2004, ADV NEUR IN, V16, P281
   Jarvelin K., 2000, SIGIR Forum, V34, P41
   Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655
   Kazhdan M., 2003, Symposium on Geometry Processing, P156
   Li B., EUR WORKSH 3D OBJ RE
   Li B, 2010, LECT NOTES COMPUT SC, V5916, P185
   Liu YJ, 2013, IEEE T AUTOM SCI ENG, V10, P783, DOI 10.1109/TASE.2012.2228481
   Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410
   Mademlis A, 2008, IEEE T MULTIMEDIA, V10, P819, DOI 10.1109/TMM.2008.922790
   Ohbuchi Ryutarou, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P63, DOI 10.1109/ICCVW.2009.5457716
   OHBUCHI R., 2010, Proceedings of the ACM workshop on 3D object retrieval, P63
   Ohbuchi R, 2008, IEEE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS 2008, PROCEEDINGS, P93, DOI 10.1109/SMI.2008.4547955
   Papadakis P, 2010, INT J COMPUT VISION, V89, P177, DOI 10.1007/s11263-009-0281-6
   Saupe D., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P392
   Semechko A., 2012, UNIFORM SAMPLING SPH
   Shilane P, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P167, DOI 10.1109/smi.2004.1314504
   Tangelder JWH, 2004, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON SHAPE MODELING AND APPLICATIONS, P145, DOI 10.1109/SMI.2004.1314502
   Tatsuma A, 2009, VISUAL COMPUT, V25, P785, DOI 10.1007/s00371-008-0304-2
   VRANIC D.V., 2004, 3d model retrieval, P1
   Vranic DV, 2001, 2001 IEEE FOURTH WORKSHOP ON MULTIMEDIA SIGNAL PROCESSING, P293, DOI 10.1109/MMSP.2001.962749
   Wang M, 2013, IEEE T IMAGE PROCESS, V22, P1395, DOI 10.1109/TIP.2012.2231088
   ZAHN CT, 1972, IEEE T COMPUT, VC 21, P269, DOI 10.1109/TC.1972.5008949
   Zhouhui Lian, 2010, Proceedings of the Shape Modeling International (SMI 2010), P25, DOI 10.1109/SMI.2010.20
NR 45
TC 4
Z9 6
U1 1
U2 14
PU IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC
PI PISCATAWAY
PA 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA
SN 1520-9210
EI 1941-0077
J9 IEEE T MULTIMEDIA
JI IEEE Trans. Multimedia
PD AUG
PY 2014
VL 16
IS 5
BP 1369
EP 1376
DI 10.1109/TMM.2014.2314073
PG 8
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science; Telecommunications
GA AN0SZ
UT WOS:000340295600018
DA 2024-07-18
ER

EF